[
    {
        "title": "Original Research",
        "firstAuthor": "Shigang Gao",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "The rapid development and adoption of generative artificial intelligence (AI) tools in the art and design education landscape have introduced both opportunities and challenges. This timely study addresses the need to effectively inte-grate these tools into the classroom while considering ethical implications and the importance of prompt engineering. By examining the iterative process of refining original ideas through multiple iterations, verbal expansion, and the use of OpenAI\u2019s DALL-E2 for generating diverse visual outcomes, researchers gain insights into the potential benefits and pitfalls of these tools in an educational context. Students in the digital at case study were taught prompt engineering techniques and were tasked with crafting multiple prompts, focusing on refining their ideas over time. Participants demonstrated an increased understanding of the potential and limitations of generative AI tools and how to manipulate subject matter for more effective results. The iterative process encouraged students to explore and experiment with their creative ideas, leading to a deeper understanding of the possibilities offered by AI tools. Despite acknowledging the ethical concerns regarding copyright and the potential replacement of artists, students appreciated the value of generative AI tools for enhancing their sketchbooks and ideation process. Through prompt engineering and iterative processes, students developed a more detail-oriented approach to their work. The challenge of using AI-generated images as final products was conceptually intriguing, requiring further investigation and consideration of the prompts. This study highlights the potential benefits and challenges of integrating generative AI tools into art and design classrooms, emphasizing the importance of prompt engineering, iterative processes, and ethical considerations as these technologies continue to evolve.",
        "paperId": "039052a04d01dd07d7b46c292cd883e0e3dd9ffb"
    },
    {
        "title": "ChatGPT vs. Crowdsourcing vs. Experts: Annotating Open-Domain Conversations with Speech Functions",
        "firstAuthor": "Lidiia Ostyakova",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "This paper deals with the task of annotating open-domain conversations with speech functions. We propose a semi-automated method for annotating dialogs following the topic-oriented, multi-layered taxonomy of speech functions with the use of hierarchical guidelines using Large Language Models. These guidelines comprise simple questions about the topic and speaker change, sentence types, pragmatic aspects of the utterance, and examples that aid untrained annotators in understanding the taxonomy. We compare the results of dialog annotation performed by experts, crowdsourcing workers, and ChatGPT. To improve the performance of ChatGPT, several experiments utilising different prompt engineering techniques were conducted. We demonstrate that in some cases large language models can achieve human-like performance following a multi-step tree-like annotation pipeline on complex discourse annotation, which is usually challenging and costly in terms of time and money when performed by humans.",
        "paperId": "061d5b2ceb7e537c3c96d13f267c0cc22f8f96d3"
    },
    {
        "title": "RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model",
        "firstAuthor": "Yao Lu",
        "url": "https://arxiv.org/pdf/2308.05345",
        "dateSubmitted": "2023-08-10",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "Inspired by the recent success of large language models (LLMs) like ChatGPT, researchers start to explore the adoption of LLMs for agile hardware design, such as generating design RTL based on natural-language instructions. However, in existing works, their target designs are all relatively simple and in a small scale, and proposed by the authors themselves, making a fair comparison among different LLM solutions challenging. In addition, many prior works only focus on the design correctness, without evaluating the design qualities of generated design RTL. In this work, we propose an open-source benchmark named RTLLM, for generating design RTL with natural language instructions. To systematically evaluate the auto-generated design RTL, we summarized three progressive goals, named syntax goal, functionality goal, and design quality goal. This benchmark can automatically provide a quantitative evaluation of any given LLM-based solution. Furthermore, we propose an easy-to-use yet surprisingly effective prompt engineering technique named self-planning, which proves to significantly boost the performance of GPT-3.5 in our proposed benchmark.",
        "paperId": "079be8c8a93fc80274ff22251a3dac9804bec66a"
    },
    {
        "title": "Extracting Financial Data From Unstructured Sources: Leveraging Large Language Models",
        "firstAuthor": "Huaxia Li",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "This research addresses the challenge of extracting financial data from unstructured sources, a persistent issue for accounting researchers, investors, and regulators. Leveraging large language models (LLMs), this study develops a framework for automated financial data extraction from PDF-formatted files. Following the design science methodology, this research develops the framework through a series of text mining and prompt engineering techniques and further applies it to governmental annual reports in PDF format. Pilot test results indicate that the framework achieves a 100% accuracy rate within a short period of time when extracting key financial indicators. This study contributes to the evolving literature on applying LLMs in accounting and finance, while also providing a practical tool for both academic and industry applications.",
        "paperId": "08a1c56ae37c6bf6f38b554e26a31a251abe7807"
    },
    {
        "title": "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT",
        "firstAuthor": "Jules White",
        "url": "http://arxiv.org/pdf/2302.11382",
        "dateSubmitted": "2023-02-21",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.",
        "paperId": "08b85bce712168998004ee80ce4e475390413c74"
    },
    {
        "title": "More Samples or More Prompt Inputs? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering",
        "firstAuthor": "Bingsheng Yao",
        "url": null,
        "dateSubmitted": "2023-11-16",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "While most existing works on LLM prompt-engineering focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can't we design and leverage multiple prompt inputs together to further improve the LLM performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompt-engineering technique to produce the most confident prediction results by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with two SOTA LLMs (FlanT5-XL and Mistral-7B) on three NLI datasets (e-SNLI, Multi-NLI, and ANLI) illustrate that ICS can consistently enhance LLM's prediction performance and confidence. An ablation study suggests that a diversity-based ICS strategy may further improve LLM's performance, which sheds light on a new yet promising future research direction.",
        "paperId": "0ab79543d98e375b9de1354766c024e165cc2369"
    },
    {
        "title": "ChatGPT for Robotics: Design Principles and Model Abilities",
        "firstAuthor": "Sai Vemprala",
        "url": "https://arxiv.org/pdf/2306.17582",
        "dateSubmitted": "2023-02-20",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing users to interact with it primarily via natural language instructions. In addition to these studies, we introduce an open-sourced research tool called PromptCraft, which contains a platform where researchers can collaboratively upload and vote on examples of good prompting schemes for robotics applications, as well as a sample robotics simulator with ChatGPT integration, making it easier for users to get started with using ChatGPT for robotics.",
        "paperId": "0ba581718f294db1d7b3dbc159cc3d3380f74606"
    },
    {
        "title": "AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling",
        "firstAuthor": "Pivithuru Thejan Amarasinghe",
        "url": "https://arxiv.org/pdf/2309.13218",
        "dateSubmitted": "2023-09-22",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "Business optimisation is the process of finding and implementing efficient and cost-effective means of operation to bring a competitive advantage for businesses. Synthesizing problem formulations is an integral part of business optimisation which is centred around human expertise, thus with a high potential of becoming a bottleneck. With the recent advancements in Large Language Models (LLMs), human expertise needed in problem formulation can potentially be minimized using Artificial Intelligence (AI). However, developing a LLM for problem formulation is challenging, due to training data requirements, token limitations, and the lack of appropriate performance metrics in LLMs. To minimize the requirement of large training data, considerable attention has recently been directed towards fine-tuning pre-trained LLMs for downstream tasks, rather than training a LLM from scratch for a specific task. In this paper, we adopt this approach and propose an AI-Copilot for business optimisation by fine-tuning a pre-trained LLM for problem formulation. To address token limitations, we introduce modularization and prompt engineering techniques to synthesize complex problem formulations as modules that fit into the token limits of LLMs. In addition, we design performance evaluation metrics that are more suitable for assessing the accuracy and quality of problem formulations compared to existing evaluation metrics. Experiment results demonstrate that our AI-Copilot can synthesize complex and large problem formulations for a typical business optimisation problem in production scheduling.",
        "paperId": "13fafa40eb7b15813cdf6c2ead1e1032e7b085f0"
    },
    {
        "title": "Co-audit: tools to help humans double-check AI-generated content",
        "firstAuthor": "Andrew D. Gordon",
        "url": "https://arxiv.org/pdf/2310.01297",
        "dateSubmitted": "2023-10-02",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "Users are increasingly being warned to check AI-generated content for correctness. Still, as LLMs (and other generative models) generate more complex output, such as summaries, tables, or code, it becomes harder for the user to audit or evaluate the output for quality or correctness. Hence, we are seeing the emergence of tool-assisted experiences to help the user double-check a piece of AI-generated content. We refer to these as co-audit tools. Co-audit tools complement prompt engineering techniques: one helps the user construct the input prompt, while the other helps them check the output response. As a specific example, this paper describes recent research on co-audit tools for spreadsheet computations powered by generative models. We explain why co-audit experiences are essential for any application of generative AI where quality is important and errors are consequential (as is common in spreadsheet computations). We propose a preliminary list of principles for co-audit, and outline research challenges.",
        "paperId": "14dcafae548d578f6b8c683d0972531bc46423ca"
    },
    {
        "title": "Generative AI tools in art education: Exploring prompt engineering and iterative processes for enhanced creativity",
        "firstAuthor": "Peter Cotroneo",
        "url": null,
        "dateSubmitted": "2023-06-05",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "The rapid development and adoption of generative artificial intelligence (AI) tools in the art and design education landscape have introduced both opportunities and challenges. This timely study addresses the need to effectively integrate these tools into the classroom while considering ethical implications and the importance of prompt engineering. By examining the iterative process of refining original ideas through multiple iterations, verbal expansion, and the use of OpenAI\u2019s DALL-E2 for generating diverse visual outcomes, researchers gain insights into the potential benefits and pitfalls of these tools in an educational context. Students in the digital at case study were taught prompt engineering techniques and were tasked with crafting multiple prompts, focusing on refining their ideas over time. Participants demonstrated an increased understanding of the potential and limitations of generative AI tools and how to manipulate subject matter for more effective results. The iterative process encouraged students to explore and experiment with their creative ideas, leading to a deeper understanding of the possibilities offered by AI tools. Despite acknowledging the ethical concerns regarding copyright and the potential replacement of artists, students appreciated the value of generative AI tools for enhancing their sketchbooks and ideation process. Through prompt engineering and iterative processes, students developed a more detail-oriented approach to their work. The challenge of using AI-generated images as final products was conceptually intriguing, requiring further investigation and consideration of the prompts. This study highlights the potential benefits and challenges of integrating generative AI tools into art and design classrooms, emphasizing the importance of prompt engineering, iterative processes, and ethical considerations as these technologies continue to evolve.",
        "paperId": "1e39f3ca4aff09dccc3b951cdd355c7d8e7cbc2f"
    },
    {
        "title": "Solving and Generating NPR Sunday Puzzles with Large Language Models",
        "firstAuthor": "Jin Zhao",
        "url": "http://arxiv.org/pdf/2306.12255",
        "dateSubmitted": "2023-06-21",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "We explore the ability of large language models to solve and generate puzzles from the NPR Sunday Puzzle game show using PUZZLEQA, a dataset comprising 15 years of on-air puzzles. We evaluate four large language models using PUZZLEQA, in both multiple choice and free response formats, and explore two prompt engineering techniques to improve free response performance: chain-of-thought reasoning and prompt summarization. We find that state-of-the-art large language models can solve many PUZZLEQA puzzles: the best model, GPT-3.5, achieves 50.2% loose accuracy. However, in our few-shot puzzle generation experiment, we find no evidence that models can generate puzzles: GPT-3.5 generates puzzles with answers that do not conform to the generated rules. Puzzle generation remains a challenging task for future work.",
        "paperId": "1e5743366625128e225879dbcfb568f6b8f1bcdc"
    },
    {
        "title": "Towards Multimodal Computational Humanities. Using CLIP to Analyze Late-Nineteenth Century Magic Lantern Slides",
        "firstAuthor": "T. Smits",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "The introduction of the CLIP model signaled a breakthrough in multimodal deep learning. This paper examines whether CLIP can be fruitfully applied to a (binary) classification task in the Humanities. We focus on a historical collection of late-nineteenth century magic lantern slides from the Lucerna database. Based on the available metadata, we evaluate CLIP\u2019s performance on classifying slide images into \u2018exterior\u2019 and \u2018interior\u2019 categories. We compare the performance of several textual prompts for CLIP to two conventional mono-modal models (textual and visual) which we train and evaluate on the same stratified set of 5,244 magic lantern slides and their captions. We find that the textual and multimodal models achieve a respectable performance (\u223c0.80 accuracy) but are still outperformed by a vision model that was fine-tuned to the task (\u223c0.89). We flag three methodological issues that might arise from the application of CLIP in the (computational) humanities. First, the lack of (need for) labelled data makes it hard to inspect and/or interpret the performance of the model. Second, CLIP\u2019s zero-shot capability only allows for classification tasks to be simulated, which makes it doubtful if standard metrics can be used to compare its performance to text and/or image models. Third, the lack of effective prompt engineering techniques makes the performance of CLIP (highly) unstable.",
        "paperId": "2416687fad3ff0344201b76b7015579d24ddf712"
    },
    {
        "title": "Conceptual Design Generation Using Large Language Models",
        "firstAuthor": "Kevin Ma",
        "url": "http://arxiv.org/pdf/2306.01779",
        "dateSubmitted": "2023-05-30",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "Concept generation is a creative step in the conceptual design phase, where designers often turn to brainstorming, mindmapping, or crowdsourcing design ideas to complement their own knowledge of the domain. Recent advances in natural language processing (NLP) and machine learning (ML) have led to the rise of Large Language Models (LLMs) capable of generating seemingly creative outputs from textual prompts. The success of these models has led to their integration and application across a variety of domains, including art, entertainment, and other creative work. In this paper, we leverage LLMs to generate solutions for a set of 12 design problems and compare them to a baseline of crowdsourced solutions. We evaluate the differences between generated and crowdsourced design solutions through multiple perspectives, including human expert evaluations and computational metrics. Expert evaluations indicate that the LLM-generated solutions have higher average feasibility and usefulness while the crowdsourced solutions have more novelty. We experiment with prompt engineering and find that leveraging few-shot learning can lead to the generation of solutions that are more similar to the crowdsourced solutions. These findings provide insight into the quality of design solutions generated with LLMs and begins to evaluate prompt engineering techniques that could be leveraged by practitioners to generate higher-quality design solutions synergistically with LLMs.",
        "paperId": "29203f0b8b9be7fd70d99bf7390c6a78b68a9289"
    },
    {
        "title": "Prompt Engineering in Medical Education",
        "firstAuthor": "Thomas F. Heston",
        "url": "https://www.mdpi.com/2813-141X/2/3/19/pdf?version=1693479951",
        "dateSubmitted": "2023-08-31",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "Artificial intelligence-powered generative language models (GLMs), such as ChatGPT, Perplexity AI, and Google Bard, have the potential to provide personalized learning, unlimited practice opportunities, and interactive engagement 24/7, with immediate feedback. However, to fully utilize GLMs, properly formulated instructions are essential. Prompt engineering is a systematic approach to effectively communicating with GLMs to achieve the desired results. Well-crafted prompts yield good responses from the GLM, while poorly constructed prompts will lead to unsatisfactory responses. Besides the challenges of prompt engineering, significant concerns are associated with using GLMs in medical education, including ensuring accuracy, mitigating bias, maintaining privacy, and avoiding excessive reliance on technology. Future directions involve developing more sophisticated prompt engineering techniques, integrating GLMs with other technologies, creating personalized learning pathways, and researching the effectiveness of GLMs in medical education.",
        "paperId": "3159478fbc81e562c812b9d5dc1891271b21f0c4"
    },
    {
        "title": "Large Language Models as Data Preprocessors",
        "firstAuthor": "Haochen Zhang",
        "url": "https://arxiv.org/pdf/2308.16361",
        "dateSubmitted": "2023-08-30",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "Large Language Models (LLMs), typified by OpenAI's GPT series and Meta's LLaMA variants, have marked a significant advancement in artificial intelligence. Trained on vast amounts of text data, LLMs are capable of understanding and generating human-like text across a diverse range of topics. This study expands on the applications of LLMs, exploring their potential in data preprocessing, a critical stage in data mining and analytics applications. We delve into the applicability of state-of-the-art LLMs such as GPT-3.5, GPT-4, and Vicuna-13B for error detection, data imputation, schema matching, and entity matching tasks. Alongside showcasing the inherent capabilities of LLMs, we highlight their limitations, particularly in terms of computational expense and inefficiency. We propose an LLM-based framework for data preprocessing, which integrates cutting-edge prompt engineering techniques, coupled with traditional methods like contextualization and feature selection, to improve the performance and efficiency of these models. The effectiveness of LLMs in data preprocessing is evaluated through an experimental study spanning 12 datasets. GPT-4 emerged as a standout, achieving 100\\% accuracy or F1 score on 4 datasets, suggesting LLMs' immense potential in these tasks. Despite certain limitations, our study underscores the promise of LLMs in this domain and anticipates future developments to overcome current hurdles.",
        "paperId": "3e1ca026052d30e3b9677e363616fae23f6616df"
    },
    {
        "title": "Using ChatGPT Standard Prompt Engineering Techniques in Lesson Preparation: Role, Instructions and Seed-Word Prompts",
        "firstAuthor": "A. Spasic",
        "url": null,
        "dateSubmitted": "2023-06-29",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "The application of available natural language processing systems can have a significant impact on the education process. The primary aim of this research was to test the impact of three standard prompting techniques on the results obtained from ChatGPT. Generation of a lesson plan for programming for preschoolers was chosen as the task set for AI. The obtained results show that use of a standard prompting with additional defined roles and seed words can be useful in preparation of teaching units and lessons and it can be considered as a technique of teachers' choice.",
        "paperId": "44cbf7206f1dc9e0518c14c2f82b7e6cc0edd74c"
    },
    {
        "title": "Digital Commons@Lindenwood University Digital Commons@Lindenwood University",
        "firstAuthor": "James Hutson",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "The rapid development and adoption of generative artificial intelligence (AI) tools in the art and design education landscape have introduced both opportunities and challenges. This timely study addresses the need to effectively integrate these tools into the classroom while considering ethical implications and the importance of prompt engineering. By examining the iterative process of refining original ideas through multiple iterations, verbal expansion, and the use of OpenAI\u2019s DALL-E2 for generating diverse visual outcomes, researchers gain insights into the potential benefits and pitfalls of these tools in an educational context. Students in the digital at case study were taught prompt engineering techniques and were tasked with crafting multiple prompts, focusing on refining their ideas over time. Participants demonstrated an increased understanding of the potential and limitations of generative AI tools and how to manipulate subject matter for more effective results. The iterative process encouraged students to explore and experiment with their creative ideas, leading to a deeper understanding of the possibilities offered by AI tools. Despite acknowledging the ethical concerns regarding copyright and the potential replacement of artists, students appreciated the value of generative AI tools for enhancing their sketchbooks and ideation process. Through prompt engineering and iterative processes, students developed a more detail-oriented approach to their work. The challenge of using AI-generated images as final products was conceptually intriguing, requiring further investigation and consideration of the prompts. This study highlights the potential benefits and challenges of integrating generative AI tools into art and design classrooms, emphasizing the importance of prompt engineering, iterative processes, and ethical considerations as these technologies continue to evolve.",
        "paperId": "4748329434e481d5fb963e4a9c0759d33fd1bbf1"
    },
    {
        "title": "ChatGPT-Based Debate Game Application Utilizing Prompt Engineering",
        "firstAuthor": "Eunyul Lee",
        "url": null,
        "dateSubmitted": "2023-08-06",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "This paper1 focuses on the implementation of a debate game using ChatGPT, aiming to investigate the feasibility of incorporating large language models into the educational domain through prompt engineering. The study explores strategies to elicit desired outputs from the GPT model by employing the prompt engineering methodology, as provided by Microsoft. Specifically, the game implementation involves the customization of ChatGPT's responses to facilitate a natural progression of debates, varying levels of difficulty, and an evaluation system for assessing the quality of discourse. By leveraging the prompt engineering methodology, we demonstrate that providing specific instructions or case-based prompts improves the accuracy and relevance of ChatGPT's answers. The developed application targets teenagers, enabling them to engage in real-time debates with ChatGPT and enhance their literacy skills. Furthermore, the game fosters the development of logical reasoning, persuasive abilities, effective expression, active participation, and attentive listening while expressing personal opinions, ultimately fostering a sense of accomplishment. Moreover, through debate evaluation and personalized advice, ChatGPT is expected to recognize and address its shortcomings, thereby continuously improving its conversational capabilities. Overall, this research contributes to the understanding of how large language models can be harnessed in educational settings and underscores the potential benefits of prompt engineering techniques in optimizing the outputs of such models.",
        "paperId": "51630e94d13c6af1ce86aa0a654ded7d78e7e49f"
    },
    {
        "title": "Prompt Engineering or Fine Tuning: An Empirical Assessment of Large Language Models in Automated Software Engineering Tasks",
        "firstAuthor": "Jiho Shin",
        "url": null,
        "dateSubmitted": "2023-10-11",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "In this paper, we investigate the effectiveness of state-of-the-art LLM, i.e., GPT-4, with three different prompting engineering techniques (i.e., basic prompting, in-context learning, and task-specific prompting) against 18 fine-tuned LLMs on three typical ASE tasks, i.e., code generation, code summarization, and code translation. Our quantitative analysis of these prompting strategies suggests that prompt engineering GPT-4 cannot necessarily and significantly outperform fine-tuning smaller/older LLMs in all three tasks. For comment generation, GPT-4 with the best prompting strategy (i.e., task-specific prompt) had outperformed the first-ranked fine-tuned model by 8.33% points on average in BLEU. However, for code generation, the first-ranked fine-tuned model outperforms GPT-4 with best prompting by 16.61% and 28.3% points, on average in BLEU. For code translation, GPT-4 and fine-tuned baselines tie as they outperform each other on different translation tasks. To explore the impact of different prompting strategies, we conducted a user study with 27 graduate students and 10 industry practitioners. From our qualitative analysis, we find that the GPT-4 with conversational prompts (i.e., when a human provides feedback and instructions back and forth with a model to achieve best results) showed drastic improvement compared to GPT-4 with automatic prompting strategies. Moreover, we observe that participants tend to request improvements, add more context, or give specific instructions as conversational prompts, which goes beyond typical and generic prompting strategies. Our study suggests that, at its current state, GPT-4 with conversational prompting has great potential for ASE tasks, but fully automated prompt engineering with no human in the loop requires more study and improvement.",
        "paperId": "59e0e0c1aa06d51430792eb5d8308911a1b0110f"
    },
    {
        "title": "Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs",
        "firstAuthor": "Jiaoayan Chen",
        "url": "http://arxiv.org/pdf/2305.09858",
        "dateSubmitted": "2023-05-17",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between products or product types, which can be utilized in recommender systems. However, relation labeling in KGs remains a challenging task due to the dynamic nature of e-commerce domains and the associated cost of human labor. Recently, breakthroughs in Large Language Models (LLMs) have shown surprising results in numerous natural language processing tasks. In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data. We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation labeling tasks using just 1 to 5 labeled examples per relation. Additionally, we experiment with different prompt engineering techniques to examine their impact on model performance. Our results show that LLMs significantly outperform existing KG completion models in relation labeling for e-commerce KGs and exhibit performance strong enough to replace human labeling.",
        "paperId": "5e8dd82419f78025093acbec3ba2e345fff85d11"
    },
    {
        "title": "Generating Requirements Elicitation Interview Scripts with Large Language Models",
        "firstAuthor": "Binnur G\u00f6rer",
        "url": null,
        "dateSubmitted": "2023-09-01",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "Requirements elicitation interviews are the most popular requirements elicitation technique and an integral part of requirements engineering education. Good and bad interview scripts provide students with examples of applying the theory. Constructing an interview script requires technical knowledge, practical experience, and creativity. As a result, only a few educational interview scripts are available to the community. This paper explores automatically generating interview scripts with large language models through prompt engineering. Our contribution is two-fold: First, we present a graph representation of interactive interview scripts. Second, we apply prompt engineering techniques to generate business domain descriptions, linear scripts, and conversation pieces focused on certain types of mistakes. Our findings indicate that large language models face challenges in handling interview conversation graphs. However, we can enhance the quality of the generated interview scripts by decomposing the task into smaller components and refining the prompts to provide more precise instructions.",
        "paperId": "78b779bdd002a9a927adeca89d2f7650a1646029"
    },
    {
        "title": "The application of ChatGPT in healthcare progress notes: A commentary from a clinical and research perspective",
        "firstAuthor": "Josh Nguyen",
        "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/ctm2.1324",
        "dateSubmitted": "2023-07-01",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "ChatGPT, powered by one of the most advanced language processing systems, gained over 100 million users in just 2 months following its release in November 2022.1 This unprecedented popularity is likely due to its wide range of potential applications in fields, such as engineering, education and healthcare.2\u20134 The integration of artificial intelligence (AI)\u2014driven language models like ChatGPT has the potential to revolutionize documentation practices, streamline workflows, and ultimately lead to more efficient and patient-centred care,2 though the use of such tools is not without its challenges. Here, we outline the potential benefits and pitfalls of implementing AI-driven language models, such as ChatGPT, in the creation and management of healthcare progress notes using prompt engineering techniques. We provide recommendations for responsible and effective integration into clinical practice and priorities for future research. Healthcare clinicians spend 35% of their time documenting patient data, and evidence suggests the length of healthcare case notes has been increasing over time.5 Existing innovations, such as speech recognition technology, yield no clear benefit in timesaving or documentation quality.6 With the ability to coherently write logical and accurate text within a few seconds, ChatGPT has the potential to reduce time spent on tasks such as preparing healthcare progress notes, and might also enhance",
        "paperId": "86d84341fb7f6d3990acccf4a433147501b20934"
    },
    {
        "title": "Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering",
        "firstAuthor": "Angus Addlesee",
        "url": "https://arxiv.org/pdf/2308.15231",
        "dateSubmitted": "2023-08-29",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "This paper evaluates the extent to which current LLMs can capture task-oriented multi-party conversations (MPCs). We have recorded and transcribed 29 MPCs between patients, their companions, and a social robot in a hospital. We then annotated this corpus for multi-party goal-tracking and intent-slot recognition. People share goals, answer each other\u2019s goals, and provide other people\u2019s goals in MPCs - none of which occur in dyadic interactions. To understand user goals in MPCs, we compared three methods in zero-shot and few-shot settings: we fine-tuned T5, created pre-training tasks to train DialogLM using LED, and employed prompt engineering techniques with GPT-3.5-turbo, to determine which approach can complete this novel task with limited data. GPT-3.5-turbo significantly outperformed the others in a few-shot setting. The \u2018reasoning\u2019 style prompt, when given 7% of the corpus as example annotated conversations, was the best performing method. It correctly annotated 62.32% of the goal tracking MPCs, and 69.57% of the intent-slot recognition MPCs. A \u2018story\u2019 style prompt increased model hallucination, which could be detrimental if deployed in safety-critical settings. We conclude that multi-party conversations still challenge state-of-the-art LLMs.",
        "paperId": "8a1a8290f7d42b0ce60445a4c0130ef737b3ff69"
    },
    {
        "title": "LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation",
        "firstAuthor": "Christian Munley",
        "url": "https://arxiv.org/pdf/2310.04963",
        "dateSubmitted": "2023-10-08",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "Large language models (LLMs) are a new and powerful tool for a wide span of applications involving natural language and demonstrate impressive code generation abilities. In this paper, we explore the capabilitity of state-of-the-art LLMs, including closed-source options like OpenAI GPT-4 and open-source alternatives like Meta AI Codellama, to automatically generate tests and use these tests to validate and verify compiler implementations of a directive-based programming paradigm, OpenACC. Our approach entails exploring various prompt engineering techniques including a code template, retrieval-augmented generation (RAG) with code template, expressive prompt using RAG with code template, one-shot example, and RAG with one-shot example. This paper focusses on (a) exploring the capabilities of the latest LLMs for code generation, (b) investigating prompt and fine tuning methods, and (c) analyzing the outcome of LLMs generated tests",
        "paperId": "8c52b3bbe5897ba3f42b38c5bfc33bbd48f9a1f2"
    },
    {
        "title": "VOICE: Visual Oracle for Interaction, Conversation, and Explanation",
        "firstAuthor": "Donggang Jia",
        "url": "http://arxiv.org/pdf/2304.04083",
        "dateSubmitted": "2023-04-08",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "We present VOICE, a novel approach for connecting large language models' (LLM) conversational capabilities with interactive exploratory visualization. VOICE introduces several innovative technical contributions that drive our conversational visualization framework. Our foundation is a pack-of-bots that can perform specific tasks, such as assigning tasks, extracting instructions, and generating coherent content. We employ fine-tuning and prompt engineering techniques to tailor bots' performance to their specific roles and accurately respond to user queries, and a new prompt-based iterative scene-tree generation establishes a coupling with a structural model. Our text-to-visualization method generates a flythrough sequence matching the content explanation. Finally, 3D natural language interaction provides capabilities to navigate and manipulate the 3D models in real-time. The VOICE framework can receive arbitrary voice commands from the user and responds verbally, tightly coupled with corresponding visual representation with low latency and high accuracy. We demonstrate the effectiveness and high generalizability potential of our approach by applying it to two distinct domains: analyzing three 3D molecular models with multi-scale and multi-instance attributes, and showcasing its effectiveness on a cartographic map visualization. A free copy of this paper and all supplemental materials are available at https://osf.io/g7fbr/.",
        "paperId": "8ca384547bb4b21b7f38d478119bf3168eb9c9cd"
    },
    {
        "title": "Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing",
        "firstAuthor": "Walid Hariri",
        "url": "http://arxiv.org/pdf/2304.02017",
        "dateSubmitted": "2023-03-27",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "Large language models have revolutionized the field of artificial intelligence and have been used in various applications. Among these models, ChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI, it stands out as a powerful tool that has been widely adopted. ChatGPT has been successfully applied in numerous areas, including chatbots, content generation, language translation, personalized recommendations, and even medical diagnosis and treatment. Its success in these applications can be attributed to its ability to generate human-like responses, understand natural language, and adapt to different contexts. Its versatility and accuracy make it a powerful tool for natural language processing (NLP). However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns. This article provides a comprehensive overview of ChatGPT, its applications, advantages, and limitations. Additionally, the paper emphasizes the importance of ethical considerations when using this robust tool in real-world scenarios. Finally, This paper contributes to ongoing discussions surrounding artificial intelligence and its impact on vision and NLP domains by providing insights into prompt engineering techniques.",
        "paperId": "9e93ab728e3e174ec1492009055885a9123d434f"
    },
    {
        "title": "Simulating H.P. Lovecraft horror literature with the ChatGPT large language model",
        "firstAuthor": "Eduardo C. Garrido-Merch'an",
        "url": "http://arxiv.org/pdf/2305.03429",
        "dateSubmitted": "2023-05-05",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "In this paper, we present a novel approach to simulating H.P. Lovecraft's horror literature using the ChatGPT large language model, specifically the GPT-4 architecture. Our study aims to generate text that emulates Lovecraft's unique writing style and themes, while also examining the effectiveness of prompt engineering techniques in guiding the model's output. To achieve this, we curated a prompt containing several specialized literature references and employed advanced prompt engineering methods. We conducted an empirical evaluation of the generated text by administering a survey to a sample of undergraduate students. Utilizing statistical hypothesis testing, we assessed the students ability to distinguish between genuine Lovecraft works and those generated by our model. Our findings demonstrate that the participants were unable to reliably differentiate between the two, indicating the effectiveness of the GPT-4 model and our prompt engineering techniques in emulating Lovecraft's literary style. In addition to presenting the GPT model's capabilities, this paper provides a comprehensive description of its underlying architecture and offers a comparative analysis with related work that simulates other notable authors and philosophers, such as Dennett. By exploring the potential of large language models in the context of literary emulation, our study contributes to the body of research on the applications and limitations of these models in various creative domains.",
        "paperId": "a7d8a6d8c04bd4554da4219be0f9d3bf87e2e56b"
    },
    {
        "title": "Machine Translation of Folktales: small-data-driven and LLM-based approaches",
        "firstAuthor": "Olena Burda-Lassen",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "Can Large Language Models translate texts with rich cultural elements? How \u201ccultured\u201d are they? This paper provides an overview of an experiment in Machine Translation of Ukrainian folktales using Large Language Models (Open AI), Google Cloud Translation API, and Opus MT. After benchmarking their performance, we have fine-tuned an Opus MT model on a domain-specific small dataset specially created to translate folktales from Ukrainian to English. We have also tested various prompt engineering techniques on the new Open AI models to generate translations of our test dataset (folktale \u2018The Mitten\u2019) and have observed promising results. This research explores the importance of both small data and Large Language Models in Machine Learning, specifically in Machine Translation of literary texts, on the example of Ukrainian folktales.",
        "paperId": "bf5329fc5e2e41ed635f2ad13b39ceea05603517"
    },
    {
        "title": "How understanding large language models can inform their use in physics education",
        "firstAuthor": "Giulia Polverini",
        "url": null,
        "dateSubmitted": "2023-09-21",
        "keyWords": [
            "prompt engineering techniques"
        ],
        "abstract": "The paper aims to fulfil three main functions: (1) to serve as an introduction for the physics education community to the functioning of Large Language Models (LLMs), (2) to present a series of illustrative examples demonstrating how prompt-engineering techniques can impact LLMs performance on conceptual physics tasks and (3) to discuss potential implications of the understanding of LLMs and prompt engineering for physics teaching and learning. We first summarise existing research on the performance of a popular LLM-based chatbot (ChatGPT) on physics tasks. We then give a basic account of how LLMs work, illustrate essential features of their functioning, and discuss their strengths and limitations. Equipped with this knowledge, we discuss some challenges with generating useful output with ChatGPT-4 in the context of introductory physics, paying special attention to conceptual questions and problems. We then provide a condensed overview of relevant literature on prompt engineering and demonstrate through illustrative examples how selected prompt-engineering techniques can be employed to improve ChatGPT-4's output on conceptual introductory physics problems. Qualitatively studying these examples provides additional insights into ChatGPT's functioning and its utility in physics problem solving. Finally, we consider how insights from the paper can inform the use of LMMs in the teaching and learning of physics.",
        "paperId": "d651380f8c99f2522ead2d86d60cb4af4413abfa"
    }
]