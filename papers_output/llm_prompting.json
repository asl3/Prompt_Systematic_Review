[
    {
        "title": "Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification",
        "firstAuthor": "Shan Chen",
        "url": "http://arxiv.org/pdf/2304.02496",
        "dateSubmitted": "2023-04-05",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Recent advances in large language models (LLMs) have shown impressive ability in biomedical question-answering, but have not been adequately investigated for more specific biomedical applications. This study investigates the performance of LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical tasks beyond question-answering. Because no patient data can be passed to the OpenAI API public interface, we evaluated model performance with over 10000 samples as proxies for two fundamental tasks in the clinical domain - classification and reasoning. The first task is classifying whether statements of clinical and policy recommendations in scientific literature constitute health advice. The second task is causal relation detection from the biomedical literature. We compared LLMs with simpler models, such as bag-of-words (BoW) with logistic regression, and fine-tuned BioBERT models. Despite the excitement around viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks remained the best strategy. The simple BoW model performed on par with the most complex LLM prompting. Prompt engineering required significant investment.",
        "paperId": "020e473d8c987dcfb03fcfffeb87b17812447031"
    },
    {
        "title": "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT",
        "firstAuthor": "Jules White",
        "url": "http://arxiv.org/pdf/2302.11382",
        "dateSubmitted": "2023-02-21",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.",
        "paperId": "08b85bce712168998004ee80ce4e475390413c74"
    },
    {
        "title": "Protect Your Prompts: Protocols for IP Protection in LLM Applications",
        "firstAuthor": "M. V. Wyk",
        "url": "http://arxiv.org/pdf/2306.06297",
        "dateSubmitted": "2023-06-09",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "With the rapid adoption of AI in the form of large language models (LLMs), the potential value of carefully engineered prompts has become significant. However, to realize this potential, prompts should be tradable on an open market. Since prompts are, at present, generally economically non-excludable, by virtue of their nature as text, no general competitive market has yet been established. This note discusses two protocols intended to provide protection of prompts, elevating their status as intellectual property, thus confirming the intellectual property rights of prompt engineers, and potentially supporting the flourishing of an open market for LLM prompts.",
        "paperId": "08fd45ac85916b95f734cc75af8660cff73c33ca"
    },
    {
        "title": "More Samples or More Prompt Inputs? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering",
        "firstAuthor": "Bingsheng Yao",
        "url": null,
        "dateSubmitted": "2023-11-16",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "While most existing works on LLM prompt-engineering focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can't we design and leverage multiple prompt inputs together to further improve the LLM performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompt-engineering technique to produce the most confident prediction results by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with two SOTA LLMs (FlanT5-XL and Mistral-7B) on three NLI datasets (e-SNLI, Multi-NLI, and ANLI) illustrate that ICS can consistently enhance LLM's prediction performance and confidence. An ablation study suggests that a diversity-based ICS strategy may further improve LLM's performance, which sheds light on a new yet promising future research direction.",
        "paperId": "0ab79543d98e375b9de1354766c024e165cc2369"
    },
    {
        "title": "S3HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering",
        "firstAuthor": "Fangyu Lei",
        "url": "http://arxiv.org/pdf/2305.11725",
        "dateSubmitted": "2023-05-19",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Answering multi-hop questions over hybrid factual knowledge from the given text and table (TextTableQA) is a challenging task. Existing models mainly adopt a retriever-reader framework, which have several deficiencies, such as noisy labeling in training retriever, insufficient utilization of heterogeneous information over text and table, and deficient ability for different reasoning operations. In this paper, we propose a three-stage TextTableQA framework S3HQA, which comprises of retriever, selector, and reasoner. We use a retriever with refinement training to solve the noisy labeling problem. Then, a hybrid selector considers the linked relationships between heterogeneous data to select the most relevant factual knowledge. For the final stage, instead of adapting a reading comprehension module like in previous methods, we employ a generation-based reasoner to obtain answers. This includes two approaches: a row-wise generator and an LLM prompting generator (first time used in this task). The experimental results demonstrate that our method achieves competitive results in the few-shot setting. When trained on the full dataset, our approach outperforms all baseline methods, ranking first on the HybridQA leaderboard.",
        "paperId": "0ea6b7371017721d87f9c5b32b084bd1ca762532"
    },
    {
        "title": "ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models",
        "firstAuthor": "Mohi Reza",
        "url": "https://arxiv.org/pdf/2310.00117",
        "dateSubmitted": "2023-09-29",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art large language models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new versions without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly produce multiple variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text segments for rapid in-place comparisons using mouse-over interactions on a context toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p<0.001), enhances user perceptions of the revision process (d = 2.41, p<0.001) compared to a popular baseline workflow, and provides insights into how writers explore variations using LLMs.",
        "paperId": "0f71c1e2acf286951544d3bd9eb5d85acfba5af1"
    },
    {
        "title": "Bringing Context-Aware Completion Suggestions to Arbitrary Text Entry Interfaces",
        "firstAuthor": "Timothy J. Aveni",
        "url": null,
        "dateSubmitted": "2023-10-29",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large language models (LLMs) can predict \u201cobvious\u201d next steps that users will take in text entry fields, especially the tedious components of tasks like software engineering or email composition. These models are not only useful in large, unbroken text fields, however. We present OmniFill, a browser extension that detects text entry fields and offers \u201cautofill\u201d-style suggestions based on context from the browsing session. The system constructs an LLM prompt that includes three main components: (a) a description of the active tab\u2019s text fields and their current values, (b) information from the user\u2019s recent web browsing context, and (c) a history, if available, of the user\u2019s prior submissions to the web form (alongside those submissions\u2019 associated browsing context). Suggestions from the LLM\u2019s response are offered to the user to be automatically typed into each corresponding text field. We offer a motivating example of a time-saving interaction and discuss the broader utility of interface-agnostic LLM integrations.",
        "paperId": "0fa28ce9377753c0dde1173cf4229f0e52e62dc5"
    },
    {
        "title": "A ML-LLM pairing for better code comment classification",
        "firstAuthor": "Hanna Abi Akl",
        "url": null,
        "dateSubmitted": "2023-10-13",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "The\"Information Retrieval in Software Engineering (IRSE)\"at FIRE 2023 shared task introduces code comment classification, a challenging task that pairs a code snippet with a comment that should be evaluated as either useful or not useful to the understanding of the relevant code. We answer the code comment classification shared task challenge by providing a two-fold evaluation: from an algorithmic perspective, we compare the performance of classical machine learning systems and complement our evaluations from a data-driven perspective by generating additional data with the help of large language model (LLM) prompting to measure the potential increase in performance. Our best model, which took second place in the shared task, is a Neural Network with a Macro-F1 score of 88.401% on the provided seed data and a 1.5% overall increase in performance on the data generated by the LLM.",
        "paperId": "13f62693ab4483566dca1d818d0122a7b08eef98"
    },
    {
        "title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers",
        "firstAuthor": "Jon Saad-Falcon",
        "url": "https://arxiv.org/pdf/2303.00807",
        "dateSubmitted": "2023-03-01",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.",
        "paperId": "14d81c84662a1de7b5605a5a68bb0f63d6e293e5"
    },
    {
        "title": "In-Context Impersonation Reveals Large Language Models' Strengths and Biases",
        "firstAuthor": "Leonard Salewski",
        "url": "http://arxiv.org/pdf/2305.14930",
        "dateSubmitted": "2023-05-24",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "In everyday conversations, humans can take on different roles and adapt their vocabulary to their chosen roles. We explore whether LLMs can take on, that is impersonate, different roles when they generate text in-context. We ask LLMs to assume different personas before solving vision and language tasks. We do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise. In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration. In a language-based reasoning task, we find that LLMs impersonating domain experts perform better than LLMs impersonating non-domain experts. Finally, we test whether LLMs' impersonations are complementary to visual information when describing different categories. We find that impersonation can improve performance: an LLM prompted to be a bird expert describes birds better than one prompted to be a car expert. However, impersonation can also uncover LLMs' biases: an LLM prompted to be a man describes cars better than one prompted to be a woman. These findings demonstrate that LLMs are capable of taking on diverse roles and that this in-context impersonation can be used to uncover their hidden strengths and biases.",
        "paperId": "19c63eade265d8a47d160098d97194b3b83d3770"
    },
    {
        "title": "From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models",
        "firstAuthor": "Jiaxian Guo",
        "url": null,
        "dateSubmitted": "2022-12-21",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large language models (LLMs) have demonstrated excellent zero-shot generalization to new language tasks. However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily due to the modality disconnect and task disconnect between the LLM and VQA tasks. End-to-end training on multimodal data may bridge the disconnects, but is inflexible and computationally expensive. To address this issue, we propose Img2LLM, a plug-and-play module that provides LLM prompts to enable LLMs to perform zeroshot VQA tasks without end-to-end training. We develop LLM-agnostic models describe image content as exemplar question-answer pairs, which prove to be effective LLM prompts. Img2LLM offers the following benefits: 1) It achieves comparable or better performance than methods relying on end-to-end training. For example, we outperform Flamingo [3] by 5.6% on VQAv2. On the challenging A-OKVQA dataset, our method outperforms few-shot methods by as much as 20%. 2) It flexibly interfaces with a wide range of LLMs to perform VQA. 3) It eliminates the need to specialize LLMs using end-to-end finetuning and serve highly specialized LLMs to end users, thereby reducing cost. Code is available via the LAVIS [28] framework at https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa.",
        "paperId": "1a310b5d357a16c8d909cc5a5106ca1ae3e47ed1"
    },
    {
        "title": "ChatGPT for PLC/DCS Control Logic Generation",
        "firstAuthor": "H. Koziolek",
        "url": "https://arxiv.org/pdf/2305.15809",
        "dateSubmitted": "2023-05-25",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large language models (LLMs) providing generative AI have become popular to support software engineers in creating, summarizing, optimizing, and documenting source code. It is still unknown how LLMs can support control engineers using typical control programming languages in programming tasks. Researchers have explored GitHub CoPilot or DeepMind AlphaCode for source code generation but did not yet tackle control logic programming. A key contribution of this paper is an exploratory study, for which we created 100 LLM prompts in 10 representative categories to analyze control logic generation for of PLCs and DCS from natural language. We tested the prompts by generating answers with ChatGPT using the GPT-4 LLM. It generated syntactically correct IEC 61131-3 Structured Text code in many cases and demonstrated useful reasoning skills that could boost control engineer productivity. Our prompt collection is the basis for a more formal LLM benchmark to test and compare such models for control logic generation.",
        "paperId": "1c1b83df13de4334e48a4c2039bc7ddfa374c486"
    },
    {
        "title": "SayTap: Language to Quadrupedal Locomotion",
        "firstAuthor": "Yujin Tang",
        "url": "https://arxiv.org/pdf/2306.07580",
        "dateSubmitted": "2023-06-13",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large language models (LLMs) have demonstrated the potential to perform high-level planning. Yet, it remains a challenge for LLMs to comprehend low-level commands, such as joint angle targets or motor torques. This paper proposes an approach to use foot contact patterns as an interface that bridges human commands in natural language and a locomotion controller that outputs these low-level commands. This results in an interactive system for quadrupedal robots that allows the users to craft diverse locomotion behaviors flexibly. We contribute an LLM prompt design, a reward function, and a method to expose the controller to the feasible distribution of contact patterns. The results are a controller capable of achieving diverse locomotion patterns that can be transferred to real robot hardware. Compared with other design choices, the proposed approach enjoys more than 50% success rate in predicting the correct contact patterns and can solve 10 more tasks out of a total of 30 tasks. Our project site is: https://saytap.github.io.",
        "paperId": "1fc21645ccc8e99eb8162e5f91407148b7f77e3d"
    },
    {
        "title": "Multi-stage Large Language Model Correction for Speech Recognition",
        "firstAuthor": "Jie Pu",
        "url": null,
        "dateSubmitted": "2023-10-17",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "In this paper, we investigate the usage of large language models (LLMs) to improve the performance of competitive speech recognition systems. Different from traditional language models that focus on one single data domain, the rise of LLMs brings us the opportunity to push the limit of state-of-the-art ASR performance, and at the same time to achieve higher robustness and generalize effectively across multiple domains. Motivated by this, we propose a novel multi-stage approach to combine traditional language model re-scoring and LLM prompting. Specifically, the proposed method has two stages: the first stage uses a language model to re-score an N-best list of ASR hypotheses and run a confidence check; The second stage uses prompts to a LLM to perform ASR error correction on less confident results from the first stage. Our experimental results demonstrate the effectiveness of the proposed method by showing a 10% ~ 20% relative improvement in WER over a competitive ASR system -- across multiple test domains.",
        "paperId": "20e87ab2e4f14db2046588c6a3bdccd2c9664e1a"
    },
    {
        "title": "Supplementary - I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification",
        "firstAuthor": "Muhammad Ferjad Naeem",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "In this supplementary, we perform additional experiments on I2MVFormer to confirm our design choices. Moreover, we perform further analysis of our LLM prompting strategy to confirm that it is robust to the choice of LLM, the choice of the prompt and the choice of k-shot examples. Finally, we qualitatively analyze the LLM Views to confirm that each view provides complementary information about a class which allows for a highly discriminative zero-shot image classification model. The content of the supplementary is organized as follows.",
        "paperId": "2402965ec4b5487cb43b26765742e6e6fff9dca8"
    },
    {
        "title": "ProgPrompt: program generation for situated robot task planning using large language models",
        "firstAuthor": "Ishika Singh",
        "url": "https://link.springer.com/content/pdf/10.1007/s10514-023-10135-3.pdf",
        "dateSubmitted": "2023-08-28",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": null,
        "paperId": "24d836bbc35413d76c3c69cb30bfc0f1449f5207"
    },
    {
        "title": "MMHQA-ICL: Multimodal In-context Learning for Hybrid Question Answering over Text, Tables and Images",
        "firstAuthor": "Weihao Liu",
        "url": "https://arxiv.org/pdf/2309.04790",
        "dateSubmitted": "2023-09-09",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "In the real world, knowledge often exists in a multimodal and heterogeneous form. Addressing the task of question answering with hybrid data types, including text, tables, and images, is a challenging task (MMHQA). Recently, with the rise of large language models (LLM), in-context learning (ICL) has become the most popular way to solve QA problems. We propose MMHQA-ICL framework for addressing this problems, which includes stronger heterogeneous data retriever and an image caption module. Most importantly, we propose a Type-specific In-context Learning Strategy for MMHQA, enabling LLMs to leverage their powerful performance in this task. We are the first to use end-to-end LLM prompting method for this task. Experimental results demonstrate that our framework outperforms all baselines and methods trained on the full dataset, achieving state-of-the-art results under the few-shot setting on the MultimodalQA dataset.",
        "paperId": "27d6d02e24de259e3aa38e556a81f89ec505816e"
    },
    {
        "title": "LMCanvas: Object-Oriented Interaction to Personalize Large Language Model-Powered Writing Environments",
        "firstAuthor": "Tae Soo Kim",
        "url": "http://arxiv.org/pdf/2303.15125",
        "dateSubmitted": "2023-03-27",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large language models (LLMs) can enhance writing by automating or supporting specific tasks in writers' workflows (e.g., paraphrasing, creating analogies). Leveraging this capability, a collection of interfaces have been developed that provide LLM-powered tools for specific writing tasks. However, these interfaces provide limited support for writers to create personal tools for their own unique tasks, and may not comprehensively fulfill a writer's needs -- requiring them to continuously switch between interfaces during writing. In this work, we envision LMCanvas, an interface that enables writers to create their own LLM-powered writing tools and arrange their personal writing environment by interacting with\"blocks\"in a canvas. In this interface, users can create text blocks to encapsulate writing and LLM prompts, model blocks for model parameter configurations, and connect these to create pipeline blocks that output generations. In this workshop paper, we discuss the design for LMCanvas and our plans to develop this concept.",
        "paperId": "2cdff023cd4b185bb452f3c7399580db2d0fdfcd"
    },
    {
        "title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models",
        "firstAuthor": "Haonan Duan",
        "url": "http://arxiv.org/pdf/2305.15594",
        "dateSubmitted": "2023-05-24",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large language models (LLMs) are excellent in-context learners. However, the sensitivity of data contained in prompts raises privacy concerns. Our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs. To address this vulnerability, one could forego prompting and resort to fine-tuning LLMs with known algorithms for private gradient descent. However, this comes at the expense of the practicality and efficiency offered by prompting. Therefore, we propose to privately learn to prompt. We first show that soft prompts can be obtained privately through gradient descent on downstream data. However, this is not the case for discrete prompts. Thus, we orchestrate a noisy vote among an ensemble of LLMs presented with different prompts, i.e., a flock of stochastic parrots. The vote privately transfers the flock's knowledge into a single public prompt. We show that LLMs prompted with our private algorithms closely match the non-private baselines. For example, using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on the sst2 dataset with ($\\epsilon=0.147, \\delta=10^{-6}$)-differential privacy vs. 95.2% for the non-private baseline. Through our experiments, we also show that our prompt-based approach is easily deployed with existing commercial APIs.",
        "paperId": "2f2a430ba6c93bcfaf4818316ff8a27b1e034b1a"
    },
    {
        "title": "Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering",
        "firstAuthor": "Yike Wu",
        "url": "https://arxiv.org/pdf/2309.11206",
        "dateSubmitted": "2023-09-20",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Despite their competitive performance on knowledge-intensive tasks, large language models (LLMs) still have limitations in memorizing all world knowledge especially long tail knowledge. In this paper, we study the KG-augmented language model approach for solving the knowledge graph question answering (KGQA) task that requires rich world knowledge. Existing work has shown that retrieving KG knowledge to enhance LLMs prompting can significantly improve LLMs performance in KGQA. However, their approaches lack a well-formed verbalization of KG knowledge, i.e., they ignore the gap between KG representations and textual representations. To this end, we propose an answer-sensitive KG-to-Text approach that can transform KG knowledge into well-textualized statements most informative for KGQA. Based on this approach, we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task. Experiments on several KGQA benchmarks show that the proposed KG-to-Text augmented LLMs approach outperforms previous KG-augmented LLMs approaches regarding answer accuracy and usefulness of knowledge statements.",
        "paperId": "30f0abb793772c15f2cdfec97c994685348177c1"
    },
    {
        "title": "Better Patching Using LLM Prompting, via Self-Consistency",
        "firstAuthor": "Toufique Ahmed",
        "url": "https://arxiv.org/pdf/2306.00108",
        "dateSubmitted": "2023-05-31",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large Language models (LLMs) can be induced to solve non-trivial problems with \u201cfew-shot\u201d prompts including illustrative problem-solution examples. Now if the few-shots also include \u201cchain of thought\u201d ($\\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a \u201cexplained\u201d solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\\mathcal{S}-C$ (or even $\\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.",
        "paperId": "32426b96ff3c680125bde3b835bfa931288b8ade"
    },
    {
        "title": "Knowledge Crosswords: Geometric Reasoning over Structured Knowledge with Large Language Models",
        "firstAuthor": "Wenxuan Ding",
        "url": "https://arxiv.org/pdf/2310.01290",
        "dateSubmitted": "2023-10-02",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large language models (LLMs) are widely adopted in knowledge-intensive tasks and have achieved impressive performance thanks to their knowledge abilities. While LLMs have demonstrated outstanding performance on atomic or linear (multi-hop) QA tasks, whether they can reason in knowledge-rich scenarios with interweaving constraints remains an underexplored problem. In this work, we propose geometric reasoning over structured knowledge, where pieces of knowledge are connected in a graph structure and models need to fill in the missing information. Such geometric knowledge reasoning would require the ability to handle structured knowledge, reason with uncertainty, verify facts, and backtrack when an error occurs. We propose Knowledge Crosswords, a multi-blank QA dataset where each problem consists of a natural language question representing the geometric constraints of an incomplete entity network, where LLMs are tasked with working out the missing entities while meeting all factual constraints. Knowledge Crosswords contains 2,101 individual problems, covering various knowledge domains and further divided into three difficulty levels. We conduct extensive experiments to evaluate existing LLM prompting approaches on the Knowledge Crosswords benchmark. We additionally propose two new approaches, Staged Prompting and Verify-All, to augment LLMs' ability to backtrack and verify structured constraints. Our results demonstrate that while baseline approaches perform well on easier problems but struggle with hard ones, our proposed Verify-All outperforms other methods by a large margin and is more robust with hard problems. Further analysis reveals that LLMs' ability of geometric reasoning over structured knowledge is still far from robust or perfect, susceptible to confounders such as the order of options, certain structural patterns, assumption of existence of correct answer, and more.",
        "paperId": "33d944de189d6edf3a510ea195803a381c5a3bab"
    },
    {
        "title": "OmniFill: Domain-Agnostic Form Filling Suggestions Using Multi-Faceted Context",
        "firstAuthor": "Timothy J. Aveni",
        "url": null,
        "dateSubmitted": "2023-10-27",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Predictive suggestion systems offer contextually-relevant text entry completions. Existing approaches, like autofill, often excel in narrowly-defined domains but fail to generalize to arbitrary workflows. We introduce a conceptual framework to analyze the compound demands of a particular suggestion context, yielding unique opportunities for large language models (LLMs) to infer suggestions for a wide range of domain-agnostic form-filling tasks that were out of reach with prior approaches. We explore these opportunities in OmniFill, a prototype that collects multi-faceted context including browsing and text entry activity to construct an LLM prompt that offers suggestions in situ for arbitrary structured text entry interfaces. Through a user study with 18 participants, we found that OmniFill offered valuable suggestions and we identified four themes that characterize users' behavior and attitudes: an\"opportunistic scrapbooking\"approach; a trust placed in the system; value in partial success; and a need for visibility into prompt context.",
        "paperId": "35b45a852ff24e789f9406b96170cfcbbaed1781"
    },
    {
        "title": "OptiMUS: Optimization Modeling Using mip Solvers and large language models",
        "firstAuthor": "Ali AhmadiTeshnizi",
        "url": "https://arxiv.org/pdf/2310.06116",
        "dateSubmitted": "2023-10-09",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Optimization problems are pervasive across various sectors, from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers, as the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. We introduce OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve MILP problems from their natural language descriptions. OptiMUS is capable of developing mathematical models, writing and debugging solver code, developing tests, and checking the validity of generated solutions. To benchmark our agent, we present NLP4LP, a novel dataset of linear programming (LP) and mixed integer linear programming (MILP) problems. Our experiments demonstrate that OptiMUS solves nearly twice as many problems as a basic LLM prompting strategy. OptiMUS code and NLP4LP dataset are available at \\href{https://github.com/teshnizi/OptiMUS}{https://github.com/teshnizi/OptiMUS}",
        "paperId": "37a30e97ae043075f09738984a59991ff20ecf0c"
    },
    {
        "title": "GEAR: Augmenting Language Models with Generalizable and Efficient Tool Resolution",
        "firstAuthor": "Yining Lu",
        "url": "https://arxiv.org/pdf/2307.08775",
        "dateSubmitted": "2023-07-17",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Augmenting large language models (LLM) to use external tools enhances their performance across a variety of tasks. However, prior works over-rely on task-specific demonstration of tool use that limits their generalizability and computational cost due to making many calls to large-scale LLMs. We introduce GEAR, a computationally efficient query-tool grounding algorithm that is generalizable to various tasks that require tool use while not relying on task-specific demonstrations. GEAR achieves better efficiency by delegating tool grounding and execution to small language models (SLM) and LLM, respectively; while leveraging semantic and pattern-based evaluation at both question and answer levels for generalizable tool grounding. We evaluate GEAR on 14 datasets across 6 downstream tasks, demonstrating its strong generalizability to novel tasks, tools and different SLMs. Despite offering more efficiency, GEAR achieves higher precision in tool grounding compared to prior strategies using LLM prompting, thus improving downstream accuracy at a reduced computational cost. For example, we demonstrate that GEAR-augmented GPT-J and GPT-3 outperform counterpart tool-augmented baselines because of better tool use.",
        "paperId": "3bd83ff979f3c0e9470f23c360a18333593dc5a1"
    },
    {
        "title": "Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference",
        "firstAuthor": "Zachary Levonian",
        "url": "https://arxiv.org/pdf/2310.03184",
        "dateSubmitted": "2023-10-04",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "For middle-school math students, interactive question-answering (QA) with tutors is an effective way to learn. The flexibility and emergent capabilities of generative large language models (LLMs) has led to a surge of interest in automating portions of the tutoring process - including interactive QA to support conceptual discussion of mathematical concepts. However, LLM responses to math questions can be incorrect or mismatched to the educational context - such as being misaligned with a school's curriculum. One potential solution is retrieval-augmented generation (RAG), which involves incorporating a vetted external knowledge source in the LLM prompt to increase response quality. In this paper, we designed prompts that retrieve and use content from a high-quality open-source math textbook to generate responses to real student questions. We evaluate the efficacy of this RAG system for middle-school algebra and geometry QA by administering a multi-condition survey, finding that humans prefer responses generated using RAG, but not when responses are too grounded in the textbook content. We argue that while RAG is able to improve response quality, designers of math QA systems must consider trade-offs between generating responses preferred by students and responses closely matched to specific educational resources.",
        "paperId": "3dc1b657bf821b731c5ed0396823b67c10d54ba1"
    },
    {
        "title": "Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models",
        "firstAuthor": "Renat Aksitov",
        "url": "http://arxiv.org/pdf/2302.05578",
        "dateSubmitted": "2023-02-11",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Despite recent progress, it has been difficult to prevent semantic hallucinations in generative Large Language Models. One common solution to this is augmenting LLMs with a retrieval system and making sure that the generated output is attributable to the retrieved information. Given this new added constraint, it is plausible to expect that the overall quality of the output will be affected, for example, in terms of fluency. Can scaling language models help? Here we examine the relationship between fluency and attribution in LLMs prompted with retrieved evidence in knowledge-heavy dialog settings. Our experiments were implemented with a set of auto-metrics that are aligned with human preferences. They were used to evaluate a large set of generations, produced under varying parameters of LLMs and supplied context. We show that larger models tend to do much better in both fluency and attribution, and that (naively) using top-k retrieval versus top-1 retrieval improves attribution but hurts fluency. We next propose a recipe that could allow smaller models to both close the gap with larger models and preserve the benefits of top-k retrieval while avoiding its drawbacks.",
        "paperId": "3ed1c94ec4fdd2a9235afeb2d929fde965b1d723"
    },
    {
        "title": "Universal Fuzzing via Large Language Models",
        "firstAuthor": "Chun Xia",
        "url": "https://arxiv.org/pdf/2308.04748",
        "dateSubmitted": "2023-08-09",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input generation and mutation engine, which enables the approach to produce diverse and realistic inputs for any practically relevant language. To realize this potential, we present a novel autoprompting technique, which creates LLM prompts that are wellsuited for fuzzing, and a novel LLM-powered fuzzing loop, which iteratively updates the prompt to create new fuzzing inputs. We evaluate Fuzz4All on nine systems under test that take in six different languages (C, C++, Go, SMT2, Java and Python) as inputs. The evaluation shows, across all six languages, that universal fuzzing achieves higher coverage than existing, language-specific fuzzers. Furthermore, Fuzz4All has identified 76 bugs in widely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskit quantum computing platform, with 47 bugs already confirmed by developers as previously unknown.",
        "paperId": "4cc2d056365b8b5a58ce57f4b0bf7b20cfa2b6b7"
    },
    {
        "title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models",
        "firstAuthor": "Yen-Ting Lin",
        "url": "http://arxiv.org/pdf/2305.13711",
        "dateSubmitted": "2023-05-23",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios.",
        "paperId": "4f480bae3196dbbc27ab383bce33478ea963f9b3"
    },
    {
        "title": "Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction",
        "firstAuthor": "S. Carta",
        "url": "http://arxiv.org/pdf/2307.01128",
        "dateSubmitted": "2023-07-03",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "In the current digitalization era, capturing and effectively representing knowledge is crucial in most real-world scenarios. In this context, knowledge graphs represent a potent tool for retrieving and organizing a vast amount of information in a properly interconnected and interpretable structure. However, their generation is still challenging and often requires considerable human effort and domain expertise, hampering the scalability and flexibility across different application fields. This paper proposes an innovative knowledge graph generation approach that leverages the potential of the latest generative large language models, such as GPT-3.5, that can address all the main critical issues in knowledge graph building. The approach is conveyed in a pipeline that comprises novel iterative zero-shot and external knowledge-agnostic strategies in the main stages of the generation process. Our unique manifold approach may encompass significant benefits to the scientific community. In particular, the main contribution can be summarized by: (i) an innovative strategy for iteratively prompting large language models to extract relevant components of the final graph; (ii) a zero-shot strategy for each prompt, meaning that there is no need for providing examples for\"guiding\"the prompt result; (iii) a scalable solution, as the adoption of LLMs avoids the need for any external resources or human expertise. To assess the effectiveness of our proposed model, we performed experiments on a dataset that covered a specific domain. We claim that our proposal is a suitable solution for scalable and versatile knowledge graph construction and may be applied to different and novel contexts.",
        "paperId": "50bdea5132ef4b8cf25b0d9f3ac2ee0d09bf18cb"
    },
    {
        "title": "ROSGPT_Vision: Commanding Robots Using Only Language Models' Prompts",
        "firstAuthor": "Bilel Benjdira",
        "url": "https://arxiv.org/pdf/2308.11236",
        "dateSubmitted": "2023-08-22",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "In this paper, we argue that the next generation of robots can be commanded using only Language Models' prompts. Every prompt interrogates separately a specific Robotic Modality via its Modality Language Model (MLM). A central Task Modality mediates the whole communication to execute the robotic mission via a Large Language Model (LLM). This paper gives this new robotic design pattern the name of: Prompting Robotic Modalities (PRM). Moreover, this paper applies this PRM design pattern in building a new robotic framework named ROSGPT_Vision. ROSGPT_Vision allows the execution of a robotic task using only two prompts: a Visual and an LLM prompt. The Visual Prompt extracts, in natural language, the visual semantic features related to the task under consideration (Visual Robotic Modality). Meanwhile, the LLM Prompt regulates the robotic reaction to the visual description (Task Modality). The framework automates all the mechanisms behind these two prompts. The framework enables the robot to address complex real-world scenarios by processing visual data, making informed decisions, and carrying out actions automatically. The framework comprises one generic vision module and two independent ROS nodes. As a test application, we used ROSGPT_Vision to develop CarMate, which monitors the driver's distraction on the roads and makes real-time vocal notifications to the driver. We showed how ROSGPT_Vision significantly reduced the development cost compared to traditional methods. We demonstrated how to improve the quality of the application by optimizing the prompting strategies, without delving into technical details. ROSGPT_Vision is shared with the community (link: https://github.com/bilel-bj/ROSGPT_Vision) to advance robotic research in this direction and to build more robotic frameworks that implement the PRM design pattern and enables controlling robots using only prompts.",
        "paperId": "53e8d327e7ceda6f4efd321752da57edbaee6257"
    },
    {
        "title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",
        "firstAuthor": "Shubhra (Santu) Karmaker",
        "url": "http://arxiv.org/pdf/2305.11430",
        "dateSubmitted": "2023-05-19",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied. Indeed, we are yet to conduct comprehensive benchmarking studies with multiple LLMs that are exclusively focused on a complex task. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, the paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw more accurate conclusions about LLMs' performance on a specific complex task.",
        "paperId": "5645502d73c6907f1671923638773152e55bfb00"
    },
    {
        "title": "What's the Magic Word? A Control Theory of LLM Prompting",
        "firstAuthor": "Aman Bhargava",
        "url": "https://arxiv.org/pdf/2310.04444",
        "dateSubmitted": "2023-10-02",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\\epsilon$ controllability to characterize LLM steerability. We compute the $k-\\epsilon$ controllability of a panel of large language models, including Falcon-7b, Llama-7b, and Falcon-40b on 5000 WikiText causal language modeling tasks. Remarkably, we find that magic words of 10 tokens or less exist for over 97% of WikiText instances surveyed for each model.",
        "paperId": "57a4f8f69908d3474565d3cd6f58b1ca651ff673"
    },
    {
        "title": "Profit: Benchmarking Personalization and Robustness Trade-off in Federated Prompt Tuning",
        "firstAuthor": "Liam Collins",
        "url": "https://arxiv.org/pdf/2310.04627",
        "dateSubmitted": "2023-10-06",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "In many applications of federated learning (FL), clients desire models that are personalized using their local data, yet are also robust in the sense that they retain general global knowledge. However, the presence of data heterogeneity across clients induces a fundamental trade-off between personalization (i.e., adaptation to a local distribution) and robustness (i.e., not forgetting previously learned general knowledge). It is critical to understand how to navigate this personalization vs robustness trade-off when designing federated systems, which are increasingly moving towards a paradigm of fine-tuning large foundation models. Due to limited computational and communication capabilities in most federated settings, this foundation model fine-tuning must be done using parameter-efficient fine-tuning (PEFT) approaches. While some recent work has studied federated approaches to PEFT, the personalization vs robustness trade-off of federated PEFT has been largely unexplored. In this work, we take a step towards bridging this gap by benchmarking fundamental FL algorithms -- FedAvg and FedSGD plus personalization (via client local fine-tuning) -- applied to one of the most ubiquitous PEFT approaches to large language models (LLMs) -- prompt tuning -- in a multitude of hyperparameter settings under varying levels of data heterogeneity. Our results show that federated-trained prompts can be surprisingly robust when using a small learning rate with many local epochs for personalization, especially when using an adaptive optimizer as the client optimizer during federated training. We also demonstrate that simple approaches such as adding regularization and interpolating two prompts are effective in improving the personalization vs robustness trade-off in computation-limited settings with few local updates allowed for personalization.",
        "paperId": "61f46dbe000930877c5da4d8628c63ce1ce2df82"
    },
    {
        "title": "Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation",
        "firstAuthor": "Marjan Ghazvininejad",
        "url": "http://arxiv.org/pdf/2302.07856",
        "dateSubmitted": "2023-02-15",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large language models (LLMs) demonstrate remarkable machine translation (MT) abilities via prompting, even though they were not explicitly trained for this task. However, even given the incredible quantities of data they are trained on, LLMs can struggle to translate inputs with rare words, which are common in low resource or domain transfer scenarios. We show that LLM prompting can provide an effective solution for rare words as well, by using prior knowledge from bilingual dictionaries to provide control hints in the prompts. We propose a novel method, DiPMT, that provides a set of possible translations for a subset of the input words, thereby enabling fine-grained phrase-level prompted control of the LLM. Extensive experiments show that DiPMT outperforms the baseline both in low-resource MT, as well as for out-of-domain MT. We further provide a qualitative analysis of the benefits and limitations of this approach, including the overall level of controllability that is achieved.",
        "paperId": "64ce6ef1f5cf227bf2bf917c87273386ae16256f"
    },
    {
        "title": "System Report for CCL23-Eval Task 9: HUST1037 Explore Proper Prompt Strategy for LLM in MRC Task",
        "firstAuthor": "Xiao Liu",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "\u201cOur research paper delves into the Adversarial Robustness Evaluation for Chinese Gaokao Read-ing Comprehension (GCRC advRobust). While Chinese reading comprehension tasks havegained significant attention in recent years, previous methods have not proven effective for thischallenging dataset. We focus on exploring how prompt engineering can impact a model\u2019s read-ing comprehension ability. Through our experiments using ChatGLM, GPT3.5, and GPT4, wediscovered a correlation between prompt and LLM reading comprehension ability, and found thatprompt engineering improves the performance of each model. Our team submitted the results ofour system evaluation, which ranked first in three indexes and total scores.Keywords\u2014 LLM, Prompt, Chinese Reading Comprehension\u201d",
        "paperId": "693c3c1c54c1c65561a0a7628177b55a0ebad603"
    },
    {
        "title": "On-the-Fly Fusion of Large Language Models and Machine Translation",
        "firstAuthor": "Hieu T. Hoang",
        "url": null,
        "dateSubmitted": "2023-11-14",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "We propose the on-the-fly ensembling of a machine translation model with an LLM, prompted on the same task and input. We perform experiments on 4 language pairs (both directions) with varying data amounts. We find that a slightly weaker-at-translation LLM can improve translations of a NMT model, and ensembling with an LLM can produce better translations than ensembling two stronger MT models. We combine our method with various techniques from LLM prompting, such as in context learning and translation context.",
        "paperId": "69eeb4dfcc971b30119b9f0dcffdac3b4f9c1c98"
    },
    {
        "title": "Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models",
        "firstAuthor": "Ran Xu",
        "url": null,
        "dateSubmitted": "2023-11-01",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Clinical natural language processing requires methods that can address domain-specific challenges, such as complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation using LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 7 clinical NLP tasks and 16 datasets reveals that ClinGen consistently enhances performance across various tasks, effectively aligning the distribution of real datasets and significantly enriching the diversity of generated training instances. We will publish our code and all the generated data in \\url{https://github.com/ritaranx/ClinGen}.",
        "paperId": "69f0c3a693d5f7f1512f2fcb4104692e4ae36184"
    },
    {
        "title": "Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models",
        "firstAuthor": "Gabriel H. Sarch",
        "url": null,
        "dateSubmitted": "2023-10-23",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Pre-trained and frozen LLMs can effectively map simple scene re-arrangement instructions to programs over a robot's visuomotor functions through appropriate few-shot example prompting. To parse open-domain natural language and adapt to a user's idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short. In this paper, we introduce HELPER, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction or VLM description, and used as in-context prompt examples for LLM querying. The memory is expanded during deployment to include pairs of user's language and action plans, to assist future inferences and personalize them to the user's language and routines. HELPER sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD), with 1.7x improvement over the previous SOTA for TfD. Our models, code and video results can be found in our project's website: https://helper-agent-llm.github.io.",
        "paperId": "729fc01274cc26798654a318d1a95e73c61f99a3"
    },
    {
        "title": "SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models",
        "firstAuthor": "S. S. Kannan",
        "url": "https://arxiv.org/pdf/2309.10062",
        "dateSubmitted": "2023-09-18",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "In this work, we introduce SMART-LLM, an innovative framework designed for embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm. We create a benchmark dataset designed for validating the multi-robot task planning problem, encompassing four distinct categories of high-level instructions that vary in task complexity. Our evaluation experiments span both simulation and real-world scenarios, demonstrating that the proposed model can achieve promising results for generating multi-robot task plans. The experimental videos, code, and datasets from the work can be found at https://sites.google.com/view/smart-llm/.",
        "paperId": "755853c6b30f5a186131e23a63c68a3f2737068e"
    },
    {
        "title": "Federated Large Language Model: A Position Paper",
        "firstAuthor": "Chaochao Chen",
        "url": "https://arxiv.org/pdf/2307.08925",
        "dateSubmitted": "2023-07-18",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles faced by these solutions within the context of federated LLM.",
        "paperId": "7aad760762c4a10dfbc2d3391eb8bdb28c80b236"
    },
    {
        "title": "Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review",
        "firstAuthor": "Banghao Chen",
        "url": null,
        "dateSubmitted": "2023-10-23",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we gather information about the application of prompt engineering in such fields as education and programming, showing its transformative potential. This comprehensive survey aims to serve as a friendly guide for anyone venturing through the big world of LLMs and prompt engineering.",
        "paperId": "7d083d654f66f763302d8a5f0678beb753f6507b"
    },
    {
        "title": "Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion",
        "firstAuthor": "Jinheon Baek",
        "url": null,
        "dateSubmitted": "2023-11-10",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large Language Models (LLMs) excel at tackling various natural language tasks. However, due to the significant costs involved in re-training or fine-tuning them, they remain largely static and difficult to personalize. Nevertheless, a variety of applications could benefit from generations that are tailored to users' preferences, goals, and knowledge. Among them is web search, where knowing what a user is trying to accomplish, what they care about, and what they know can lead to improved search experiences. In this work, we propose a novel and general approach that augments an LLM with relevant context from users' interaction histories with a search engine in order to personalize its outputs. Specifically, we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web, which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is light-weight, since it only produces user-specific aggregate projections of interests and knowledge onto public knowledge graphs, and leverages existing search log infrastructure, thereby mitigating the privacy, compliance, and scalability concerns associated with building deep user profiles for personalization. We then validate our approach on the task of contextual query suggestion, which requires understanding not only the user's current search context but also what they historically know and care about. Through a number of experiments based on human evaluation, we show that our approach is significantly better than several other LLM-powered baselines, generating query suggestions that are contextually more relevant, personalized, and useful.",
        "paperId": "7f212f04edd558d0b81930d11022d1df57b6a0d8"
    },
    {
        "title": "SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning",
        "firstAuthor": "Yue Wu",
        "url": "http://arxiv.org/pdf/2305.15486",
        "dateSubmitted": "2023-05-24",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality of in-context\"reasoning\"induced by different forms of prompts under the setting of the Crafter open-world environment. Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training. Finally, we show the potential of games as a test bed for LLMs.",
        "paperId": "864cb3a725ae829cbfb675761cd2313897b1b7a8"
    },
    {
        "title": "Prompting a Large Language Model to Generate Diverse Motivational Messages: A Comparison with Human-Written Messages",
        "firstAuthor": "Samuel Rhys Cox",
        "url": "https://arxiv.org/pdf/2308.13479",
        "dateSubmitted": "2023-08-25",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large language models (LLMs) are increasingly capable and prevalent, and can be used to produce creative content. The quality of content is influenced by the prompt used, with more specific prompts that incorporate examples generally producing better results. On from this, it could be seen that using instructions written for crowdsourcing tasks (that are specific and include examples to guide workers) could prove effective LLM prompts. To explore this, we used a previous crowdsourcing pipeline that gave examples to people to help them generate a collectively diverse corpus of motivational messages. We then used this same pipeline to generate messages using GPT-4, and compared the collective diversity of messages from: (1) crowd-writers, (2) GPT-4 using the pipeline, and (3&4) two baseline GPT-4 prompts. We found that the LLM prompts using the crowdsourcing pipeline caused GPT-4 to produce more diverse messages than the two baseline prompts. We also discuss implications from messages generated by both human writers and LLMs.",
        "paperId": "8da6e4537122af618c36563caef5863f8728d789"
    },
    {
        "title": "AdaPlanner: Adaptive Planning from Feedback with Language Models",
        "firstAuthor": "Haotian Sun",
        "url": "http://arxiv.org/pdf/2305.16653",
        "dateSubmitted": "2023-05-26",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.",
        "paperId": "8e37dc1215681aa153a51c07078ba8befd6a6e01"
    },
    {
        "title": "Distilled Language Models are economically efficient for the enterprise. ...mostly.",
        "firstAuthor": "Kristen Howell",
        "url": "http://arxiv.org/pdf/2306.07402",
        "dateSubmitted": "2023-06-08",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. With their ability to handle large context windows, Large Language Models (LLMs) are a natural fit for this use case. However, their efficacy must be balanced with the cost of training and serving them. This paper assesses the practical cost and impact of LLMs for the enterprise as a function of the usefulness of the responses that they generate. We present a cost framework for evaluating an NLP model\u2019s utility for this use case and apply it to a single brand as a case study in the context of an existing agent assistance product. We compare three strategies for specializing an LLM \u2014 prompt engineering, fine-tuning, and knowledge distillation \u2014 using feedback from the brand\u2019s customer service agents. We find that the usability of a model\u2019s responses can make up for a large difference in inference cost for our case study brand, and we extrapolate our findings to the broader enterprise space.",
        "paperId": "8e5af286e461ad07625e43e17d4c69e8b16d9fbb"
    },
    {
        "title": "Large Language Models Enable Few-Shot Clustering",
        "firstAuthor": "Vijay Viswanathan",
        "url": "http://arxiv.org/pdf/2307.00524",
        "dateSubmitted": "2023-07-02",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user's intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model can amplify an expert's guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find incorporating LLMs in the first two stages can routinely provide significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.",
        "paperId": "8e8a1489bf4d782d2435cdeb93f7d1f165747c63"
    },
    {
        "title": "Why Johnny Can\u2019t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts",
        "firstAuthor": "J. Zamfirescu-Pereira",
        "url": "https://dl.acm.org/doi/pdf/10.1145/3544548.3581388",
        "dateSubmitted": "2023-04-19",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Pre-trained large language models (\u201cLLMs\u201d) like GPT-3 can engage in fluent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer LLM outputs (\u201cprompting\u201d) has emerged as an important design technique potentially accessible to non-AI-experts. Crafting effective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-AI-experts can successfully engage in \u201cend-user prompt engineering\u201d using a design probe\u2014a prototype LLM-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to effective prompt design. These findings have implications for non-AI-expert-facing LLM-based tool design and for improving LLM-and-prompt literacy among programmers and the public, and present opportunities for further research.",
        "paperId": "8f9e864fab09bbae4a46a2a62bb954db1a88eb3e"
    },
    {
        "title": "Extrinsically-Focused Evaluation of Omissions in Medical Summarization",
        "firstAuthor": "Elliot Schumacher",
        "url": null,
        "dateSubmitted": "2023-11-14",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "The goal of automated summarization techniques (Paice, 1990; Kupiec et al, 1995) is to condense text by focusing on the most critical information. Generative large language models (LLMs) have shown to be robust summarizers, yet traditional metrics struggle to capture resulting performance (Goyal et al, 2022) in more powerful LLMs. In safety-critical domains such as medicine, more rigorous evaluation is required, especially given the potential for LLMs to omit important information in the resulting summary. We propose MED-OMIT, a new omission benchmark for medical summarization. Given a doctor-patient conversation and a generated summary, MED-OMIT categorizes the chat into a set of facts and identifies which are omitted from the summary. We further propose to determine fact importance by simulating the impact of each fact on a downstream clinical task: differential diagnosis (DDx) generation. MED-OMIT leverages LLM prompt-based approaches which categorize the importance of facts and cluster them as supporting or negating evidence to the diagnosis. We evaluate MED-OMIT on a publicly-released dataset of patient-doctor conversations and find that MED-OMIT captures omissions better than alternative metrics.",
        "paperId": "a3e62fc612b65d9809f5ebb1c3269d98fc729aaa"
    },
    {
        "title": "PromptInfuser: How Tightly Coupling AI and UI Design Impacts Designers' Workflows",
        "firstAuthor": "S. Petridis",
        "url": null,
        "dateSubmitted": "2023-10-24",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Prototyping AI applications is notoriously difficult. While large language model (LLM) prompting has dramatically lowered the barriers to AI prototyping, designers are still prototyping AI functionality and UI separately. We investigate how coupling prompt and UI design affects designers' workflows. Grounding this research, we developed PromptInfuser, a Figma plugin that enables users to create semi-functional mockups, by connecting UI elements to the inputs and outputs of prompts. In a study with 14 designers, we compare PromptInfuser to designers' current AI-prototyping workflow. PromptInfuser was perceived to be significantly more useful for communicating product ideas, more capable of producing prototypes that realistically represent the envisioned artifact, more efficient for prototyping, and more helpful for anticipating UI issues and technical constraints. PromptInfuser encouraged iteration over prompt and UI together, which helped designers identify UI and prompt incompatibilities and reflect upon their total solution. Together, these findings inform future systems for prototyping AI applications.",
        "paperId": "b085564a9fdfac91a8f281f88ed9b0825c0aa91f"
    },
    {
        "title": "LPML: LLM-Prompting Markup Language for Mathematical Reasoning",
        "firstAuthor": "Ryutaro Yamauchi",
        "url": "https://arxiv.org/pdf/2309.13078",
        "dateSubmitted": "2023-09-21",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "In utilizing large language models (LLMs) for mathematical reasoning, addressing the errors in the reasoning and calculation present in the generated text by LLMs is a crucial challenge. In this paper, we propose a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL). We discovered that by prompting LLMs to generate structured text in XML-like markup language, we could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs. With our approach, LLMs can utilize Python computation to rectify errors within CoT. We applied our method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and demonstrated that combining CoT and Python REPL through the markup language enhances the reasoning capability of LLMs. Our approach enables LLMs to write the markup language and perform advanced mathematical reasoning using only zero-shot prompting.",
        "paperId": "b099104d1a065cbc1432af22e6085b1a44dbc839"
    },
    {
        "title": "ChainForge: An open-source visual programming environment for prompt engineering",
        "firstAuthor": "Ian Arawjo",
        "url": null,
        "dateSubmitted": "2023-10-29",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Prompt engineering for large language models (LLMs) is a critical to effectively leverage their capabilities. However, due to the inherent stochastic and opaque nature of LLMs, prompt engineering is far from an exact science. Crafting prompts that elicit the desired responses still requires a lot of trial and error to gain a nuanced understanding of a model\u2019s strengths and limitations for one\u2019s specific task context and target application. To support users in sensemaking around the outputs of LLMs, we create ChainForge, an open-source visual programming environment for prompt engineering. ChainForge is publicly available, both on the web (https://chainforge.ai) and as a locally installable Python package hosted on PyPI. We detail some features of ChainForge and how we iterated the design in response to internal and external feedback.",
        "paperId": "bb411609bbe11aa674e484507c989ad933b1e64c"
    },
    {
        "title": "Prompt Cache: Modular Attention Reuse for Low-Latency Inference",
        "firstAuthor": "In Gim",
        "url": null,
        "dateSubmitted": "2023-11-07",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "We present Prompt Cache, an approach for accelerating inference for large language models (LLM) by reusing attention states across different LLM prompts. Many input prompts have overlapping text segments, such as system messages, prompt templates, and documents provided for context. Our key insight is that by precomputing and storing the attention states of these frequently occurring text segments on the inference server, we can efficiently reuse them when these segments appear in user prompts. Prompt Cache employs a schema to explicitly define such reusable text segments, called prompt modules. The schema ensures positional accuracy during attention state reuse and provides users with an interface to access cached states in their prompt. Using a prototype implementation, we evaluate Prompt Cache across several LLMs. We show that Prompt Cache significantly reduce latency in time-to-first-token, especially for longer prompts such as document-based question answering and recommendations. The improvements range from 8x for GPU-based inference to 60x for CPU-based inference, all while maintaining output accuracy and without the need for model parameter modifications.",
        "paperId": "bc5c73c101da795cfa44e4ac7751cdedca9b6d93"
    },
    {
        "title": "Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation",
        "firstAuthor": "J. Mendoncca",
        "url": "https://arxiv.org/pdf/2308.16797",
        "dateSubmitted": "2023-08-31",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Despite significant research effort in the development of automatic dialogue evaluation metrics, little thought is given to evaluating dialogues other than in English. At the same time, ensuring metrics are invariant to semantically similar responses is also an overlooked topic. In order to achieve the desired properties of robustness and multilinguality for dialogue evaluation metrics, we propose a novel framework that takes advantage of the strengths of current evaluation models with the newly-established paradigm of prompting Large Language Models (LLMs). Empirical results show our framework achieves state of the art results in terms of mean Spearman correlation scores across several benchmarks and ranks first place on both the Robust and Multilingual tasks of the DSTC11 Track 4 \u201cAutomatic Evaluation Metrics for Open-Domain Dialogue Systems\u201d, proving the evaluation capabilities of prompted LLMs.",
        "paperId": "bcefc74b20649fd41ea05d87a3fa512d2559fc8d"
    },
    {
        "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models",
        "firstAuthor": "Ishika Singh",
        "url": "https://arxiv.org/pdf/2209.11302",
        "dateSubmitted": "2022-09-22",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io",
        "paperId": "c03fa01fbb9c77fe3d10609ba5f1dee33a723867"
    },
    {
        "title": "Improving Few-Shot Prompts with Relevant Static Analysis Products",
        "firstAuthor": "Toufique Ahmed",
        "url": "https://arxiv.org/pdf/2304.06815",
        "dateSubmitted": "2023-04-13",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large Language Models (LLM) are a new class of computation engines,\"programmed\"via prompt engineering. We are still learning how to best\"program\"these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously have a collection of semantics facts in mind when working on coding tasks. Mostly these are shallow, simple facts arising from a quick read. For a function, examples of facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc. One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them inherently capable of doing this simple level of\"code analysis\"and extracting such information, implicitly, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM's prompt with semantic facts explicitly, actually helps. Prior work shows that LLM performance on code summarization benefits from few-shot samples drawn either from the same-project or from examples found via information retrieval methods (such as BM25). While summarization performance has steadily increased since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization. We find that adding semantic facts actually does help! This approach improves performance in several different settings suggested by prior work, including for two different Large Language Models. In most cases, improvement nears or exceeds 2 BLEU; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU.",
        "paperId": "c2391a8c8e24a450f00810ecb441e26413ea3791"
    },
    {
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
        "firstAuthor": "Yann Dubois",
        "url": "https://arxiv.org/pdf/2305.14387",
        "dateSubmitted": "2023-05-22",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback. Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate eleven models on 10k pairs of real human feedback and show that rankings of models trained in AlpacaFarm match rankings of models trained on human data. As a demonstration of the research possible in AlpacaFarm, we find that methods that use a reward model can substantially improve over supervised fine-tuning and that our reference PPO implementation leads to a +10% improvement in win-rate against Davinci003. We release all components of AlpacaFarm at https://github.com/tatsu-lab/alpaca_farm.",
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa"
    },
    {
        "title": "ConstitutionMaker: Interactively Critiquing Large Language Models by Converting Feedback into Principles",
        "firstAuthor": "S. Petridis",
        "url": null,
        "dateSubmitted": "2023-10-24",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large language model (LLM) prompting is a promising new approach for users to create and customize their own chatbots. However, current methods for steering a chatbot's outputs, such as prompt engineering and fine-tuning, do not support users in converting their natural feedback on the model's outputs to changes in the prompt or model. In this work, we explore how to enable users to interactively refine model outputs through their feedback, by helping them convert their feedback into a set of principles (i.e. a constitution) that dictate the model's behavior. From a formative study, we (1) found that users needed support converting their feedback into principles for the chatbot and (2) classified the different principle types desired by users. Inspired by these findings, we developed ConstitutionMaker, an interactive tool for converting user feedback into principles, to steer LLM-based chatbots. With ConstitutionMaker, users can provide either positive or negative feedback in natural language, select auto-generated feedback, or rewrite the chatbot's response; each mode of feedback automatically generates a principle that is inserted into the chatbot's prompt. In a user study with 14 participants, we compare ConstitutionMaker to an ablated version, where users write their own principles. With ConstitutionMaker, participants felt that their principles could better guide the chatbot, that they could more easily convert their feedback into principles, and that they could write principles more efficiently, with less mental demand. ConstitutionMaker helped users identify ways to improve the chatbot, formulate their intuitive responses to the model into feedback, and convert this feedback into specific and clear principles. Together, these findings inform future tools that support the interactive critiquing of LLM outputs.",
        "paperId": "ccac57515a8fedc0631de58879f886e827e725ad"
    },
    {
        "title": "AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models",
        "firstAuthor": "Siqi Ouyang",
        "url": null,
        "dateSubmitted": "2023-05-24",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Recent large language models (LLMs) are promising for making decisions in grounded environments. However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment. Existing methods require either costly gradient computation or lengthy in-context demonstrations. In this paper, we propose AutoPlan, an approach to guide LLM-based agents to accomplish interactive decision-making tasks. AutoPlan augments the LLM prompt with a task-solving plan and optimizes it through iterative experience collection and reflection. Our experiments show that AutoPlan, though using no in-context demonstrations, achieves success rates on par with the baselines using human-written demonstrations on ALFWorld and even outperforms them by 8% on HotpotQA. The code is available at https://github.com/owaski/AutoPlan.",
        "paperId": "cecfd07a37923d51733ca556382788848804f9f2"
    },
    {
        "title": "HeaP: Hierarchical Policies for Web Actions using LLMs",
        "firstAuthor": "Paloma Sodhi",
        "url": "https://arxiv.org/pdf/2310.03720",
        "dateSubmitted": "2023-10-05",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in performing a range of instruction following tasks in few and zero-shot settings. However, teaching LLMs to perform tasks on the web presents fundamental challenges -- combinatorially large open-world tasks and variations across web interfaces. We tackle these challenges by leveraging LLMs to decompose web tasks into a collection of sub-tasks, each of which can be solved by a low-level, closed-loop policy. These policies constitute a shared grammar across tasks, i.e., new web tasks can be expressed as a composition of these policies. We propose a novel framework, Hierarchical Policies for Web Actions using LLMs (HeaP), that learns a set of hierarchical LLM prompts from demonstrations for planning high-level tasks and executing them via a sequence of low-level policies. We evaluate HeaP against a range of baselines on a suite of web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as live website interactions, and show that it is able to outperform prior works using orders of magnitude less data.",
        "paperId": "da0a170656a336f82fa8cf00289d1cc944d9b630"
    },
    {
        "title": "PromptInfuser: Bringing User Interface Mock-ups to Life with Large Language Models",
        "firstAuthor": "S. Petridis",
        "url": null,
        "dateSubmitted": "2023-04-19",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large Language Models have enabled novices without machine learning (ML) experience to quickly prototype ML functionalities with prompt programming. This paper investigates incorporating prompt-based prototyping into designing functional user interface (UI) mock-ups. To understand how infusing LLM prompts into UI mock-ups might affect the prototyping process, we conduct a exploratory study with five designers, and find that this capability might significantly speed up creating functional prototypes, inform designers earlier on how their designs will integrate ML, and enable user studies with functional prototypes earlier. From these findings, we built PromptInfuser, a Figma plugin for authoring LLM-infused mock-ups. PromptInfuser introduces two novel LLM-interactions: input-output, which makes content interactive and dynamic, and frame-change, which directs users to different frames depending on their natural language input. From initial observations, we find that PromptInfuser has the potential to transform the design process by tightly integrating UI and AI prototyping in a single interface.",
        "paperId": "dfefbadec889ab5ab265a35d5c5eb67183e8cc72"
    },
    {
        "title": "CauSE: Causal Search Engine for Understanding Contact-Center Conversations",
        "firstAuthor": "Anup Pattnaik",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Contact centers sit on multitude of conversational data that contains helpful information which can assist businesses to deliver better outcomes like improving customer experience. However, finding such information manually is hard. Towards this end, we propose CauSE, a causal search engine for understanding contact center conversations that assist in finding relevant answers to a question. Using topic modelling, the engine identifies themes within conversational contexts to help reason for the given question. To address the challenge of multiple topics in a single context, we divide the context into Elementary Discourse Units (EDUs) and perform topic modelling on EDUs to better identify coherent themes as topics. Subsequently, we employ a novel contrastive ranking algorithm to surface meaningful topics, and LLM-prompting to obtain descriptions for the topics. Our evaluations of the resultant topics and proof of value exercises demonstrate the strength of the proposed engine.",
        "paperId": "e1d633128d0094f98ddbaf81264105a129ed7361"
    },
    {
        "title": "Hallucination-minimized Data-to-answer Framework for Financial Decision-makers",
        "firstAuthor": "Sohini Roychowdhury",
        "url": null,
        "dateSubmitted": "2023-11-09",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large Language Models (LLMs) have been applied to build several automation and personalized question-answering prototypes so far. However, scaling such prototypes to robust products with minimized hallucinations or fake responses still remains an open challenge, especially in niche data-table heavy domains such as financial decision making. In this work, we present a novel Langchain-based framework that transforms data tables into hierarchical textual data chunks to enable a wide variety of actionable question answering. First, the user-queries are classified by intention followed by automated retrieval of the most relevant data chunks to generate customized LLM prompts per query. Next, the custom prompts and their responses undergo multi-metric scoring to assess for hallucinations and response confidence. The proposed system is optimized with user-query intention classification, advanced prompting, data scaling capabilities and it achieves over 90% confidence scores for a variety of user-queries responses ranging from {What, Where, Why, How, predict, trend, anomalies, exceptions} that are crucial for financial decision making applications. The proposed data to answers framework can be extended to other analytical domains such as sales and payroll to ensure optimal hallucination control guardrails.",
        "paperId": "e33e672fd04c2367a45b3271b36aac829451a468"
    },
    {
        "title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
        "firstAuthor": "Baolin Peng",
        "url": "http://arxiv.org/pdf/2302.12813",
        "dateSubmitted": "2023-02-24",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.",
        "paperId": "e5c72b92c48d68594b290c84a8904da7c8335554"
    },
    {
        "title": "Promptagator: Few-shot Dense Retrieval From 8 Examples",
        "firstAuthor": "Zhuyun Dai",
        "url": "http://arxiv.org/pdf/2209.11755",
        "dateSubmitted": "2022-09-23",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval tasks, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To amplify the power of a few examples, we propose Prompt-base Query Generation for Retriever (Promptagator), which leverages large language models (LLM) as a few-shot query generator, and creates task-specific retrievers based on the generated data. Powered by LLM's generalization ability, Promptagator makes it possible to create task-specific end-to-end retrievers solely based on a few examples {without} using Natural Questions or MS MARCO to train %question generators or dual encoders. Surprisingly, LLM prompting with no more than 8 examples allows dual encoders to outperform heavily engineered models trained on MS MARCO like ColBERT v2 by more than 1.2 nDCG on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 point nDCG improvement. Our studies determine that query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.",
        "paperId": "e86009d9f9b1cdf083a48d087552bc4153784451"
    },
    {
        "title": "SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting",
        "firstAuthor": "Xiaoying Zhang",
        "url": "http://arxiv.org/pdf/2305.09067",
        "dateSubmitted": "2023-05-15",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Building end-to-end task bots and maintaining their integration with new functionalities using minimal human efforts is a long-standing challenge in dialog research. Recently large language models (LLMs) have demonstrated exceptional proficiency in conversational engagement and adherence to instructions across various downstream tasks. In this work, we introduce SGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems effortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, we instruct fixed LLMs to generate appropriate responses on novel tasks, circumventing the need for training data. Specifically, SGP-TOD comprises three components: a LLM for engaging with users, a DST Prompter to aid the LLM with dialog state tracking, which is then used to retrieve database items, and a Policy Prompter to elicit proper responses adhering to the provided dialog policy. Experimental results on Multiwoz, RADDLE and STAR datasets show that our training-free strategy SGP-TOD, without any task-specific data, yields state-of-the-art (SOTA) zero-shot performance, greatly surpasses the few-shot approaches. In a domain-extension setting, SGP-TOD aptly adapts to new functionalities by merely adding supplementary schema rules. We make our code and data publicly available.",
        "paperId": "ec56f49bef8925dc8931cc261ab3aca4dd36ad2d"
    },
    {
        "title": "Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures",
        "firstAuthor": "Zhen Yue",
        "url": "http://arxiv.org/pdf/2306.05171",
        "dateSubmitted": "2023-06-08",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Traditional robot task planning methods face challenges when dealing with highly unstructured environments and complex tasks. We propose a task planning method that combines human expertise with an LLM and have designed an LLM prompt template, Think_Net_Prompt, with stronger expressive power to represent structured professional knowledge. We further propose a method to progressively decompose tasks and generate a task tree to reduce the planning volume for each task, and we have designed a strategy to decouple robot task planning. By dividing different planning entities and separating the task from the actual machine binding process, the task planning process becomes more flexible. Research results show that our method performs well in handling specified code formats, understanding the relationship between tasks and subtasks, and extracting parameters from text descriptions. However, there are also problems such as limited complexity of task logic handling, ambiguity in the quantity of parts and the precise location of assembly. Improving the precision of task description and cognitive structure can bring certain improvements. https://github.com/NOMIzy/Think_Net_Prompt",
        "paperId": "eebb4a3162c1251b51e50ccd83797babc5b776c0"
    },
    {
        "title": "Chat2VIS: Generating Data Visualizations via Natural Language Using ChatGPT, Codex and GPT-3 Large Language Models",
        "firstAuthor": "Paula Maddigan",
        "url": "https://ieeexplore.ieee.org/ielx7/6287639/10005208/10121440.pdf",
        "dateSubmitted": "2023-02-04",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "The field of data visualisation has long aimed to devise solutions for generating visualisations directly from natural language text. Research in Natural Language Interfaces (NLIs) has contributed towards the development of such techniques. However, the implementation of workable NLIs has always been challenging due to the inherent ambiguity of natural language, as well as in consequence of unclear and poorly written user queries which pose problems for existing language models in discerning user intent. Instead of pursuing the usual path of developing new iterations of language models, this study uniquely proposes leveraging the advancements in pre-trained large language models (LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directly into code for appropriate visualisations. This paper presents a novel system, Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrates how, with effective prompt engineering, the complex problem of language understanding can be solved more efficiently, resulting in simpler and more accurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMs together with the proposed prompts offer a reliable approach to rendering visualisations from natural language queries, even when queries are highly misspecified and underspecified. This solution also presents a significant reduction in costs for the development of NLI systems, while attaining greater visualisation inference abilities compared to traditional NLP approaches that use hand-crafted grammar rules and tailored models. This study also presents how LLM prompts can be constructed in a way that preserves data security and privacy while being generalisable to different datasets. This work compares the performance of GPT-3, Codex and ChatGPT across several case studies and contrasts the performances with prior studies.",
        "paperId": "ef3a38b9f15e9dcb5652cb3f86f19b845cdaaef7"
    },
    {
        "title": "Interpreting User Requests in the Context of Natural Language Standing Instructions",
        "firstAuthor": "Nikita Moghe",
        "url": null,
        "dateSubmitted": "2023-11-16",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Users of natural language interfaces, generally powered by Large Language Models (LLMs),often must repeat their preferences each time they make a similar request. To alleviate this, we propose including some of a user's preferences and instructions in natural language -- collectively termed standing instructions -- as additional context for such interfaces. For example, when a user states I'm hungry, their previously expressed preference for Persian food will be automatically added to the LLM prompt, so as to influence the search for relevant restaurants. We develop NLSI, a language-to-program dataset consisting of over 2.4K dialogues spanning 17 domains, where each dialogue is paired with a user profile (a set of users specific standing instructions) and corresponding structured representations (API calls). A key challenge in NLSI is to identify which subset of the standing instructions is applicable to a given dialogue. NLSI contains diverse phenomena, from simple preferences to interdependent instructions such as triggering a hotel search whenever the user is booking tickets to an event. We conduct experiments on NLSI using prompting with large language models and various retrieval approaches, achieving a maximum of 44.7% exact match on API prediction. Our results demonstrate the challenges in identifying the relevant standing instructions and their interpretation into API calls.",
        "paperId": "f2abeec1256f80970827d60f0151c7a19f2dbe7a"
    },
    {
        "title": "Generating Domain-Specific Programs for Diagram Authoring with Large Language Models",
        "firstAuthor": "Rijul Jain",
        "url": null,
        "dateSubmitted": "2023-10-22",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large language models (LLMs) can generate programs in general-purpose languages from prose descriptions, but are not trained on many domain-specific languages (DSLs). Diagram authoring with Penrose, a diagramming system using three DSLs, exemplifies the utility of DSL program generation with LLMs, which enables diagram creation from prose. We provide methods to conceptualize and evaluate the structures of one-shot LLM prompts to generate error-free DSL programs and implement Penrose diagram creation from prose using LLMs. We will evaluate our LLM prompt structures by testing prompt variations across different diagramming domains and plan to run a user study to assess the ease of LLM-augmented Penrose diagramming over other tools.",
        "paperId": "f367935cf035691aa9c3cdeebb956c6f4aa79232"
    },
    {
        "title": "PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine",
        "firstAuthor": "Chenrui Zhang",
        "url": "https://arxiv.org/pdf/2308.12033",
        "dateSubmitted": "2023-08-23",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Pompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinement. Moreover, to enhance stability of the prompt effect evaluation, we propose a novel prompt bagging method involving forward and backward thinking, which is superior to majority voting and is beneficial for both feedback and weight calculation in boosting. Extensive experiments demonstrate that our PREFER achieves state-of-the-art performance in multiple types of tasks by a significant margin. We have made our code publicly available.",
        "paperId": "f53a4f34757d1f237446b4d887d5323f2a17ed02"
    },
    {
        "title": "Empowering Private Tutoring by Chaining Large Language Models",
        "firstAuthor": "Yulin Chen",
        "url": "https://arxiv.org/pdf/2309.08112",
        "dateSubmitted": "2023-09-15",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Artificial intelligence has been applied in various aspects of online education to facilitate teaching and learning. However, few approaches has been made toward a complete AI-powered tutoring system. In this work, we explore the development of a full-fledged intelligent tutoring system powered by state-of-the-art large language models (LLMs), covering automatic course planning and adjusting, tailored instruction, and flexible quiz evaluation. To make the system robust to prolonged interaction and cater to individualized education, the system is decomposed into three inter-connected core processes-interaction, reflection, and reaction. Each process is implemented by chaining LLM-powered tools along with dynamically updated memory modules. Tools are LLMs prompted to execute one specific task at a time, while memories are data storage that gets updated during education process. Statistical results from learning logs demonstrate the effectiveness and mechanism of each tool usage. Subjective feedback from human users reveal the usability of each function, and comparison with ablation systems further testify the benefits of the designed processes in long-term interaction.",
        "paperId": "f7842099bbde74dc5aec70bb6af85b88de08ed13"
    },
    {
        "title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
        "firstAuthor": "Gurusha Juneja",
        "url": null,
        "dateSubmitted": "2023-10-21",
        "keyWords": [
            "llm prompting"
        ],
        "abstract": "Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with a solver LM (regarded as black-box) and guide it through subproblems, thereby rendering our method solver-agnostic. Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4. Additionally, we show that DaSLaM is not limited by the solver's capabilities as a function of scale; e.g., solver LMs with diverse sizes give significant performance improvement with our solver-agnostic decomposition technique. Exhaustive ablation studies evince the superiority of our modular finetuning technique over exorbitantly large decomposer LLMs, based on prompting alone.",
        "paperId": "f9465e71697cae802d66a66eb307f0a809773cd3"
    }
]