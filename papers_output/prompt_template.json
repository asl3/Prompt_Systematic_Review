[
    {
        "title": "Masked Prompt Learning for Formal Analogies beyond Words",
        "firstAuthor": "Liyan Wang",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Prompt learning, a recent thread in few-shot learning for pre-trained language models (PLMs), has been explored for completing word analogies in the extractive way. In this paper, we reformulate the analogy task as masked analogy completion task with the use of prompting to derive a generative model for analogies beyond words. We introduce a simple prompt-based fine-tuning paradigm for language modeling on answered prompts of analogies in the sequence-to-sequence framework. To convert discrete terms of analogies into linear sequences, we present a symbolic prompt template. The sequence-to-sequence model is fine-tuned to fill in the missing span of masked prompts deduced from different masking schemes on phrase analogies extracted from a small corpus. We analyze the out-of-distribution performance on sentence analogies which are unseen cases. Our experiments demonstrate that prompt-based fine-tuning with the objective of language modeling enables models to achieve significantly better performance on in-distribution cases than PLMs. Masked prompt learning with one-term masking exhibits the best out-of-distribution generalization on sentence analogies, with a difference of only 3 characters from references.",
        "paperId": "00541ef26fcdb73c4372b2f7fa9253524bc28c7c"
    },
    {
        "title": "Zero-shot information extraction from radiological reports using ChatGPT",
        "firstAuthor": "D. Hu",
        "url": "https://arxiv.org/pdf/2309.01398",
        "dateSubmitted": "2023-09-04",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Electronic health records contain an enormous amount of valuable information, but many are recorded in free text. Information extraction is the strategy to transform the sequence of characters into structured data, which can be employed for secondary analysis. However, the traditional information extraction components, such as named entity recognition and relation extraction, require annotated data to optimize the model parameters, which has become one of the major bottlenecks in building information extraction systems. With the large language models achieving good performances on various downstream NLP tasks without parameter tuning, it becomes possible to use large language models for zero-shot information extraction. In this study, we aim to explore whether the most popular large language model, ChatGPT, can extract useful information from the radiological reports. We first design the prompt template for the interested information in the CT reports. Then, we generate the prompts by combining the prompt template with the CT reports as the inputs of ChatGPT to obtain the responses. A post-processing module is developed to transform the responses into structured extraction results. We conducted the experiments with 847 CT reports collected from Peking University Cancer Hospital. The experimental results indicate that ChatGPT can achieve competitive performances for some extraction tasks compared with the baseline information extraction system, but some limitations need to be further improved.",
        "paperId": "0386711d1f9c4240ded4de56026ca18e475b507a"
    },
    {
        "title": "Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models",
        "firstAuthor": "Hendrik Strobelt",
        "url": "https://arxiv.org/pdf/2208.07852",
        "dateSubmitted": "2022-08-16",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases.",
        "paperId": "0392d58335ce674a70f5e58ac8c438de296a0e6a"
    },
    {
        "title": "On Codex Prompt Engineering for OCL Generation: An Empirical Study",
        "firstAuthor": "Seif Abukhalaf",
        "url": "https://arxiv.org/pdf/2303.16244",
        "dateSubmitted": "2023-03-29",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "The Object Constraint Language (OCL) is a declarative language that adds constraints and object query expressions to Meta-Object Facility (MOF) models. OCL can provide precision and conciseness to UML models. Nevertheless, the unfamiliar syntax of OCL has hindered its adoption by software practitioners. LLMs, such as GPT-3, have made significant progress in many NLP tasks, such as text generation and semantic parsing. Similarly, researchers have improved on the downstream tasks by fine-tuning LLMs for the target task. Codex, a GPT-3 descendant by OpenAI, has been fine-tuned on publicly available code from GitHub and has proven the ability to generate code in many programming languages, powering the AI-pair programmer Copilot. One way to take advantage of Codex is to engineer prompts for the target downstream task. In this paper, we investigate the reliability of the OCL constraints generated by Codex from natural language specifications. To achieve this, we compiled a dataset of 15 UML models and 168 specifications from various educational resources. We manually crafted a prompt template with slots to populate with the UML information and the target task in the prefix format to complete the template with the generated OCL constraint. We used both zero- and few-shot learning methods in the experiments. The evaluation is reported by measuring the syntactic validity and the execution accuracy metrics of the generated OCL constraints. Moreover, to get insight into how close or natural the generated OCL constraints are compared to human-written ones, we measured the cosine similarity between the sentence embedding of the correctly generated and human-written OCL constraints. Our findings suggest that by enriching the prompts with the UML information of the models and enabling few-shot learning, the reliability of the generated OCL constraints increases. Furthermore, the results reveal a close similarity based on sentence embedding between the generated OCL constraints and the human-written ones in the ground truth, implying a level of clarity and understandability in the generated OCL constraints by Codex.",
        "paperId": "0a0d6a98bd246a82aaaa9d33ec0eadf4ceae69dc"
    },
    {
        "title": "The Face of a Surgeon: An Analysis of Demographic Representation in Three Leading Artificial Intelligence Text-to-Image Generators",
        "firstAuthor": "R. Ali",
        "url": "https://www.medrxiv.org/content/medrxiv/early/2023/05/29/2023.05.24.23290463.full.pdf",
        "dateSubmitted": "2023-05-29",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Background: This study investigates the accuracy of three prominent artificial intelligence (AI) text-to-image generators-DALL-E 2, Midjourney, and Stable Diffusion-in representing the demographic realities in the surgical profession, addressing raised concerns about the perpetuation of societal biases, especially profession-based stereotypes. Methods: A cross-sectional analysis was conducted on 2,400 images generated across eight surgical specialties by each model. An additional 1,200 images were evaluated based on geographic prompts for three countries. Images were generated using a prompt template, \"A photo of the face of a [blank]\", with blank replaced by a surgical specialty. Geographic-based prompting was evaluated by specifying the most populous countries for three continents (United States, Nigeria, and China). Results: There was a significantly higher representation of female (average=35.8% vs. 14.7%, P<0.001) and non-white (average=37.4% vs. 22.8%, P<0.001) surgeons among trainees than attendings. DALL-E 2 reflected attendings' true demographics for female surgeons (15.9% vs. 14.7%, P=0.386) and non-white surgeons (22.6% vs. 22.8%, P=0.919) but underestimated trainees' representation for both female (15.9% vs. 35.8%, P<0.001) and non-white (22.6% vs. 37.4%, P<0.001) surgeons. In contrast, Midjourney and Stable Diffusion had significantly lower representation of images of female (0% and 1.8%, respectively) and non-white (0.5% and 0.6%, respectively) surgeons than DALL-E 2 or true demographics (all P<0.001). Geographic-based prompting increased non-white surgeon representation (all P<0.001), but did not alter female representation (P=0.779). Conclusions: While Midjourney and Stable Diffusion amplified societal biases by depicting over 98% of surgeons as white males, DALL-E 2 depicted more accurate demographics, although all three models underestimated trainee representation. These findings underscore the necessity for guardrails and robust feedback systems to prevent AI text-to-image generators from exacerbating profession-based stereotypes, and the importance of bolstering the representation of the evolving surgical field in these models' future training sets.",
        "paperId": "0c8cabcabd56ea48958bde8535a9da5ef5a7368c"
    },
    {
        "title": "Prompt template construction by Average Gradient Search with External Knowledge for aspect sentimental analysis",
        "firstAuthor": "Yongping Du",
        "url": null,
        "dateSubmitted": "2023-10-01",
        "keyWords": [
            "prompt template"
        ],
        "abstract": null,
        "paperId": "0ce77075003f646a3819b93ed0350a6c42ef12a8"
    },
    {
        "title": "Prompt scoring system for dialogue summarization using GPT-3",
        "firstAuthor": "George Prodan",
        "url": "https://www.techrxiv.org/articles/preprint/Prompt_scoring_system_for_dialogue_summarization_using_GPT-3/16652392/2/files/35289613.pdf",
        "dateSubmitted": "2021-09-23",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Recent results in language processing show that language models are capable of performing several natural language tasks without the need of supervised learning. A challenging task for pre-trained language models is dialogue summarization. One way of generating summaries is engineering prompt templates for few-shot training. However, a static approach of creating prompts leads to unreliable outcomes between different classes of dialogues. Focusing on the dialogues structure properties we propose a scoring system to improve the few-shot training performances. We build tuned prompts composed by the highest scored dialogue samples. Our evaluation based on ROUGE scores and human evaluation shows that there is an improvement for the experiments in which we use the score system. All experiments are performed within the framework of the GPT-3 API. We use different engines for comparison. Moreover, the human evaluation we conducted showed that the number of failures decreased by 11\\% after applying our scoring system.",
        "paperId": "0d08572c4ec1974478d6ba8467b6e7be30867a0d"
    },
    {
        "title": "MaPLe: Multi-modal Prompt Learning",
        "firstAuthor": "Muhammad Uzair Khattak",
        "url": "https://arxiv.org/pdf/2210.03117",
        "dateSubmitted": "2022-10-06",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Pre-trained vision-language (V-L) models such as CLIP have shown excellent generalization ability to downstream tasks. However, they are sensitive to the choice of input text prompts and require careful selection of prompt templates to perform well. Inspired by the Natural Language Processing (NLP) literature, recent CLIP adaptation approaches learn prompts as the textual inputs to fine-tune CLIP for downstream tasks. We note that using prompting to adapt representations in a single branch of CLIP (language or vision) is sub-optimal since it does not allow the flexibility to dynamically adjust both representation spaces on a downstream task. In this work, we propose Multi-modal Prompt Learning (MaPLe) for both vision and language branches to improve alignment between the vision and language representations. Our design promotes strong coupling between the vision-language prompts to ensure mutual synergy and discourages learning independent uni-modal solutions. Further, we learn separate prompts across different early stages to progressively model the stage-wise feature relationships to allow rich context learning. We evaluate the effectiveness of our approach on three representative tasks of generalization to novel classes, new target datasets and unseen domain shifts. Compared with the state-of-the-art method Co-CoOp, MaPLe exhibits favorable performance and achieves an absolute gain of 3.45% on novel classes and 2.72% on overall harmonic-mean, averaged over 11 diverse image recognition datasets. Our code and pre-trained models are available at https://github.com/muzairkhattak/multimodal-prompt-learning.",
        "paperId": "0d0dbfb1b315a43216020abaf74d289456198219"
    },
    {
        "title": "Estimating Uncertainty in Multimodal Foundation Models using Public Internet Data",
        "firstAuthor": "Shiladitya Dutta",
        "url": null,
        "dateSubmitted": "2023-10-15",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Foundation models are trained on vast amounts of data at scale using self-supervised learning, enabling adaptation to a wide range of downstream tasks. At test time, these models exhibit zero-shot capabilities through which they can classify previously unseen (user-specified) categories. In this paper, we address the problem of quantifying uncertainty in these zero-shot predictions. We propose a heuristic approach for uncertainty estimation in zero-shot settings using conformal prediction with web data. Given a set of classes at test time, we conduct zero-shot classification with CLIP-style models using a prompt template, e.g.,\"an image of a\", and use the same template as a search query to source calibration data from the open web. Given a web-based calibration set, we apply conformal prediction with a novel conformity score that accounts for potential errors in retrieved web data. We evaluate the utility of our proposed method in Biomedical foundation models; our preliminary results show that web-based conformal prediction sets achieve the target coverage with satisfactory efficiency on a variety of biomedical datasets.",
        "paperId": "0e1a7f453976d7aa74eed46686c943bc6e630b56"
    },
    {
        "title": "Demographic Representation in 3 Leading Artificial Intelligence Text-to-Image Generators.",
        "firstAuthor": "R. Ali",
        "url": null,
        "dateSubmitted": "2023-11-15",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Importance\nThe progression of artificial intelligence (AI) text-to-image generators raises concerns of perpetuating societal biases, including profession-based stereotypes.\n\n\nObjective\nTo gauge the demographic accuracy of surgeon representation by 3 prominent AI text-to-image models compared to real-world attending surgeons and trainees.\n\n\nDesign, Setting, and Participants\nThe study used a cross-sectional design, assessing the latest release of 3 leading publicly available AI text-to-image generators. Seven independent reviewers categorized AI-produced images. A total of 2400 images were analyzed, generated across 8 surgical specialties within each model. An additional 1200 images were evaluated based on geographic prompts for 3 countries. The study was conducted in May 2023. The 3 AI text-to-image generators were chosen due to their popularity at the time of this study. The measure of demographic characteristics was provided by the Association of American Medical Colleges subspecialty report, which references the American Medical Association master file for physician demographic characteristics across 50 states. Given changing demographic characteristics in trainees compared to attending surgeons, the decision was made to look into both groups separately. Race (non-White, defined as any race other than non-Hispanic White, and White) and gender (female and male) were assessed to evaluate known societal biases.\n\n\nExposures\nImages were generated using a prompt template, \"a photo of the face of a [blank]\", with the blank replaced by a surgical specialty. Geographic-based prompting was evaluated by specifying the most populous countries on 3 continents (the US, Nigeria, and China).\n\n\nMain Outcomes and Measures\nThe study compared representation of female and non-White surgeons in each model with real demographic data using \u03c72, Fisher exact, and proportion tests.\n\n\nResults\nThere was a significantly higher mean representation of female (35.8% vs 14.7%; P\u2009<\u2009.001) and non-White (37.4% vs 22.8%; P\u2009<\u2009.001) surgeons among trainees than attending surgeons. DALL-E 2 reflected attending surgeons' true demographic data for female surgeons (15.9% vs 14.7%; P\u2009=\u2009.39) and non-White surgeons (22.6% vs 22.8%; P\u2009=\u2009.92) but underestimated trainees' representation for both female (15.9% vs 35.8%; P\u2009<\u2009.001) and non-White (22.6% vs 37.4%; P\u2009<\u2009.001) surgeons. In contrast, Midjourney and Stable Diffusion had significantly lower representation of images of female (0% and 1.8%, respectively; P\u2009<\u2009.001) and non-White (0.5% and 0.6%, respectively; P\u2009<\u2009.001) surgeons than DALL-E 2 or true demographic data. Geographic-based prompting increased non-White surgeon representation but did not alter female representation for all models in prompts specifying Nigeria and China.\n\n\nConclusion and Relevance\nIn this study, 2 leading publicly available text-to-image generators amplified societal biases, depicting over 98% surgeons as White and male. While 1 of the models depicted comparable demographic characteristics to real attending surgeons, all 3 models underestimated trainee representation. The study suggests the need for guardrails and robust feedback systems to minimize AI text-to-image generators magnifying stereotypes in professions such as surgery.",
        "paperId": "118e8e2951082a10f50b68aea9d303c068660dbc"
    },
    {
        "title": "CoCoMo: Computational Consciousness Modeling for Generative and Ethical AI",
        "firstAuthor": "Edward Y. Chang",
        "url": "http://arxiv.org/pdf/2304.02438",
        "dateSubmitted": "2023-03-17",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "The CoCoMo model proposes a computational solution to the challenge of incorporating ethical and emotional intelligence considerations into AI systems, with the aim of creating AI agents that combine knowledge with compassion. To achieve this goal, CoCoMo prioritizes fairness, beneficence, non-maleficence, empathy, adaptability, transparency, and critical and exploratory thinking abilities. The model employs consciousness modeling, reinforcement learning, and prompt template formulation to support these desired traits. By incorporating ethical and emotional intelligence considerations, a generative AI model can potentially lead to improved fairness, reduced toxicity, and increased reliability.",
        "paperId": "12bad2032f3efa5a142d7dd25712960a4f9ca5a7"
    },
    {
        "title": "Global Constraints with Prompting for Zero-Shot Event Argument Classification",
        "firstAuthor": "Zizheng Lin",
        "url": "http://arxiv.org/pdf/2302.04459",
        "dateSubmitted": "2023-02-09",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Determining the role of event arguments is a crucial subtask of event extraction. Most previous supervised models leverage costly annotations, which is not practical for open-domain applications. In this work, we propose to use global constraints with prompting to effectively tackles event argument classification without any annotation and task-specific training. Specifically, given an event and its associated passage, the model first creates several new passages by prefix prompts and cloze prompts, where prefix prompts indicate event type and trigger span, and cloze prompts connect each candidate role with the target argument span. Then, a pre-trained language model scores the new passages, making the initial prediction. Our novel prompt templates can easily adapt to all events and argument types without manual effort. Next, the model regularizes the prediction by global constraints exploiting cross-task, cross-argument, and cross-event relations. Extensive experiments demonstrate our model\u2019s effectiveness: it outperforms the best zero-shot baselines by 12.5% and 10.9% F1 on ACE and ERE with given argument spans and by 4.3% and 3.3% F1, respectively, without given argument spans. We have made our code publicly available.",
        "paperId": "1467ced85b3ae2d695079a1557063a445c43988a"
    },
    {
        "title": "Paradigm Shift in Sustainability Disclosure Analysis: Empowering Stakeholders with CHATREPORT, a Language Model-Based Tool",
        "firstAuthor": "Jingwei Ni",
        "url": "http://arxiv.org/pdf/2306.15518",
        "dateSubmitted": "2023-06-27",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "This paper introduces a novel approach to enhance Large Language Models (LLMs) with expert knowledge to automate the analysis of corporate sustainability reports by benchmarking them against the Task Force for Climate-Related Financial Disclosures (TCFD) recommendations. Corporate sustainability reports are crucial in assessing organizations' environmental and social risks and impacts. However, analyzing these reports' vast amounts of information makes human analysis often too costly. As a result, only a few entities worldwide have the resources to analyze these reports, which could lead to a lack of transparency. While AI-powered tools can automatically analyze the data, they are prone to inaccuracies as they lack domain-specific expertise. This paper introduces a novel approach to enhance LLMs with expert knowledge to automate the analysis of corporate sustainability reports. We christen our tool CHATREPORT, and apply it in a first use case to assess corporate climate risk disclosures following the TCFD recommendations. CHATREPORT results from collaborating with experts in climate science, finance, economic policy, and computer science, demonstrating how domain experts can be involved in developing AI tools. We make our prompt templates, generated data, and scores available to the public to encourage transparency.",
        "paperId": "15dfb06dab3162f4bb7939b0e54c3b68c2b34cc4"
    },
    {
        "title": "A Unified Framework for Multi-intent Spoken Language Understanding with prompting",
        "firstAuthor": "Feifan Song",
        "url": "http://arxiv.org/pdf/2210.03337",
        "dateSubmitted": "2022-10-07",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Multi-intent Spoken Language Understanding has great potential for widespread implementation. Jointly modeling Intent Detection and Slot Filling in it provides a channel to exploit the correlation between intents and slots. However, current approaches are apt to formulate these two sub-tasks differently, which leads to two issues: 1) It hinders models from effective extraction of shared features. 2) Pretty complicated structures are involved to enhance expression ability while causing damage to the interpretability of frameworks. In this work, we describe a Prompt-based Spoken Language Understanding (PromptSLU) framework, to intuitively unify two sub-tasks into the same form by offering a common pre-trained Seq2Seq model. In detail, ID and SF are completed by concisely filling the utterance into task-specific prompt templates as input, and sharing output formats of key-value pairs sequence. Furthermore, variable intents are predicted first, then naturally embedded into prompts to guide slot-value pairs inference from a semantic perspective. Finally, we are inspired by prevalent multi-task learning to introduce an auxiliary sub-task, which helps to learn relationships among provided labels. Experiment results show that our framework outperforms several state-of-the-art baselines on two public datasets.",
        "paperId": "171412ef2410fad3f9a09238ad9e272c4e31aed4"
    },
    {
        "title": "PromptDA: Label-guided Data Augmentation for Prompt-based Few Shot Learners",
        "firstAuthor": "Canyu Chen",
        "url": "http://arxiv.org/pdf/2205.09229",
        "dateSubmitted": "2022-05-18",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Recent advances in large pre-trained language models (PLMs) lead to impressive gains on natural language understanding (NLU) tasks with task-specific fine-tuning. However, directly fine-tuning PLMs heavily relies on sufficient labeled training instances, which are usually hard to obtain. Prompt-based tuning on PLMs has shown to be powerful for various downstream few-shot tasks. Existing works studying prompt-based tuning for few-shot NLU tasks mainly focus on deriving proper label words with a verbalizer or generating prompt templates to elicit semantics from PLMs. In addition, conventional data augmentation strategies such as synonym substitution are also widely adopted in low-resource scenarios. However, the improvements they bring to prompt-based few-shot learning have been demonstrated to be marginal. Thus, an important research question arises as follows: how to design effective data augmentation methods for prompt-based few-shot tuning? To this end, considering the label semantics are essential in prompt-based tuning, we propose a novel label-guided data augmentation framework PromptDA, which exploits the enriched label semantic information for data augmentation. Extensive experiment results on few-shot text classification tasks show that our proposed framework achieves superior performances by effectively leveraging label semantics and data augmentation for natural language understanding.",
        "paperId": "176ec99005b5085d5d9a34fb770d75d34166c9f5"
    },
    {
        "title": "ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations",
        "firstAuthor": "Chunkit Chan",
        "url": "http://arxiv.org/pdf/2304.14827",
        "dateSubmitted": "2023-04-28",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "This paper aims to quantitatively evaluate the performance of ChatGPT, an interactive large language model, on inter-sentential relations such as temporal relations, causal relations, and discourse relations. Given ChatGPT's promising performance across various tasks, we conduct extensive evaluations on the whole test sets of 13 datasets, including temporal and causal relations, PDTB2.0-based and dialogue-based discourse relations, and downstream applications on discourse understanding. To achieve reliable results, we adopt three tailored prompt templates for each task, including the zero-shot prompt template, zero-shot prompt engineering (PE) template, and in-context learning (ICL) prompt template, to establish the initial baseline scores for all popular sentence-pair relation classification tasks for the first time. We find that ChatGPT exhibits strong performance in detecting and reasoning about causal relations, while it may not be proficient in identifying the temporal order between two events. It can recognize most discourse relations with existing explicit discourse connectives, but the implicit discourse relation still remains a challenging task. Meanwhile, ChatGPT performs poorly in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation.",
        "paperId": "186e96fe036927182ec963b63f9dd7f8ff650158"
    },
    {
        "title": "KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction",
        "firstAuthor": "Xiang Chen",
        "url": "https://arxiv.org/pdf/2104.07650",
        "dateSubmitted": "2021-04-15",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Extensive experimental results on five datasets with standard and low-resource settings demonstrate the effectiveness of our approach. Our code and datasets are available in GitHub1 for reproducibility.",
        "paperId": "1a2e90dff605dad7dbefeed121e6d295c7a77d62"
    },
    {
        "title": "Research on Chinese Short Text Classification Based on Prefix-vector Attention Template and Probabilistic Answer Set",
        "firstAuthor": "Baoshan Sun",
        "url": null,
        "dateSubmitted": "2022-12-01",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "When people tend to communicate in the network, a large amount of text data is generated. These text data have become an important way for people to obtain information. Nowadays, text classification through prompt learning has also become the focus of people\u2019s research. However, the effect of prompt learning is not ideal due to the immobilization of the prompt template and the single label words. In response to the above problems, this paper proposes a method of constructing a prefix vector attention template (PAP) and a probabilistic answer set. The prefix space vector is added in front of the input text sentence as a template for prompt learning, and using the Attention mechanism to connect the template with the input sentence to construct the input of the model sentence. For each label word, an answer space set is constructed, and the probability that the text belongs to this category is obtained in the form of the answer set probability. The experimental results show that the PAP + probabilistic answer set algorithm proposed in this paper is significantly better than other benchmark models in the text classification task, and can achieve better results.",
        "paperId": "1d8ff886d0ab8278da5c54ca6a59a9591f4240fe"
    },
    {
        "title": "Template-Free Prompting for Few-Shot Named Entity Recognition via Semantic-Enhanced Contrastive Learning.",
        "firstAuthor": "Kai He",
        "url": null,
        "dateSubmitted": "2023-09-26",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Prompt tuning has achieved great success in various sentence-level classification tasks by using elaborated label word mappings and prompt templates. However, for solving token-level classification tasks, e.g., named entity recognition (NER), previous research, which utilizes N-gram traversal for prompting all spans with all possible entity types, is time-consuming. To this end, we propose a novel prompt-based contrastive learning method for few-shot NER without template construction and label word mappings. First, we leverage external knowledge to initialize semantic anchors for each entity type. These anchors are simply appended with input sentence embeddings as template-free prompts (TFPs). Then, the prompts and sentence embeddings are in-context optimized with our proposed semantic-enhanced contrastive loss. Our proposed loss function enables contrastive learning in few-shot scenarios without requiring a significant number of negative samples. Moreover, it effectively addresses the issue of conventional contrastive learning, where negative instances with similar semantics are erroneously pushed apart in natural language processing (NLP)-related tasks. We examine our method in label extension (LE), domain-adaption (DA), and low-resource generalization evaluation tasks with six public datasets and different settings, achieving state-of-the-art (SOTA) results in most cases.",
        "paperId": "204f1c3490498eecb92bda6b4f8d664470ee90ab"
    },
    {
        "title": "DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines",
        "firstAuthor": "O. Khattab",
        "url": "https://arxiv.org/pdf/2310.03714",
        "dateSubmitted": "2023-10-05",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded\"prompt templates\", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot prompting (generally by over 25% and 65%, respectively) and pipelines with expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at https://github.com/stanfordnlp/dspy",
        "paperId": "2069aaaa281eb13bcd9330fc4d43f24f6b436a53"
    },
    {
        "title": "Visual Prompting for Adversarial Robustness",
        "firstAuthor": "Aochuan Chen",
        "url": "https://arxiv.org/pdf/2210.06284",
        "dateSubmitted": "2022-10-12",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "In this work, we leverage visual prompting (VP) to improve adversarial robustness of a fixed, pre-trained model at test time. Compared to conventional adversarial defenses, VP allows us to design universal (i.e., data-agnostic) input prompting templates, which have plug-and-play capabilities at test time to achieve desired model performance without introducing much computation overhead. Although VP has been successfully applied to improving model generalization, it remains elusive whether and how it can be used to defend against adversarial attacks. We investigate this problem and show that the vanilla VP approach is not effective in adversarial defense since a universal input prompt lacks the capacity for robust learning against sample-specific adversarial perturbations. To circumvent it, we propose a new VP method, termed Class-wise Adversarial Visual Prompting (C-AVP), to generate class-wise visual prompts so as to not only leverage the strengths of ensemble prompts but also optimize their interrelations to improve model robustness. Our experiments show that C-AVP outperforms the conventional VP method, with 2.1\u00d7 standard accuracy gain and 2\u00d7 robust accuracy gain. Compared to classical test-time defenses, C-AVP also yields a 42\u00d7 inference time speedup. Code is available at https://github.com/Phoveran/vp-for-adversarial-robustness.",
        "paperId": "20cb40199d03395d63615854863f9eda9c7863e2"
    },
    {
        "title": "A Few-shot Approach to Resume Information Extraction via Prompts",
        "firstAuthor": "Chengguang Gan",
        "url": "https://arxiv.org/pdf/2209.09450",
        "dateSubmitted": "2022-09-20",
        "keyWords": [
            "prompt template"
        ],
        "abstract": null,
        "paperId": "21bbbd300f73db57650dfbec6a8c2bbf5103a0e2"
    },
    {
        "title": "Using Large Language Models to Simulate Multiple Humans",
        "firstAuthor": "Gati Aher",
        "url": "https://arxiv.org/pdf/2208.10264",
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "We propose a method for using a large language model, such as GPT-3, to simulate responses of different humans in a given context. We test our method by attempting to repro- duce well-established economic, psycholinguistic, and social experiments. The method requires prompt templates for each experiment. Simulations are run by varying the (hypotheti-cal) subject details, such as name, and analyzing the text gen- erated by the language model. To validate our methodology, we use GPT-3 to simulate the Ultimatum Game , garden path sentences , risk aversion , and the Milgram Shock experiments. In order to address concerns of exposure to these studies in training data, we also evaluate simulations on novel variants of these studies. We show that it is possible to simulate re- sponses of different people and that their responses are consistent with prior human studies from the literature. Across all studies, the distributions generated by larger language models better align with prior experimental results, suggesting a trend that future language models may be used for even more faithful simulations of human responses. Our use of a lan- guage model for simulation is contrasted with anthropomor-phic views of a language model as having its own behavior.",
        "paperId": "21f377c5d89f85f2bd802f4f6abe1df4748ec07b"
    },
    {
        "title": "Rethinking the Event Coding Pipeline with Prompt Entailment",
        "firstAuthor": "C. Lefebvre",
        "url": "http://arxiv.org/pdf/2210.05257",
        "dateSubmitted": "2022-10-11",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "For monitoring crises, political events are extracted from the news. The large amount of unstructured full-text event descriptions makes a case-by-case analysis unmanageable, particularly for low-resource humanitarian aid organizations. This creates a demand to classify events into event types, a task referred to as event coding. Typically, domain experts craft an event type ontology, annotators label a large dataset and technical experts develop a supervised coding system. In this work, we propose PR-ENT, a new event coding approach that is more flexible and resource-efficient, while maintaining competitive accuracy: first, we extend an event description such as \u201cMilitary injured two civilians\u201d by a template, e.g. \u201cPeople were [Z]\u201d and prompt a pre-trained (cloze) language model to fill the slot Z. Second, we select suitable answer candidates Zstar = \u201cinjured\u201d, \u201churt\u201d... by treating the event description as premise and the filled templates as hypothesis in a textual entailment task. In a final step, the selected answer candidate can be mapped to its corresponding event type. This allows domain experts to draft the codebook directly as labeled prompts and interpretable answer candidates. This human-in-the-loop process is guided by our codebook design tool. We show that our approach is robust through several checks: perturbing the event description and prompt template, restricting the vocabulary and removing contextual information.",
        "paperId": "236375f49e3deb8ee7918c1f5e65175e453deb2e"
    },
    {
        "title": "PromptAttack: Prompt-based Attack for Language Models via Gradient Search",
        "firstAuthor": "Yundi Shi",
        "url": "http://arxiv.org/pdf/2209.01882",
        "dateSubmitted": "2022-09-05",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "As the pre-trained language models (PLMs) continue to grow, so do the hardware and data requirements for fine-tuning PLMs. Therefore, the researchers have come up with a lighter method called \\textit{Prompt Learning}. However, during the investigations, we observe that the prompt learning methods are vulnerable and can easily be attacked by some illegally constructed prompts, resulting in classification errors, and serious security problems for PLMs. Most of the current research ignores the security issue of prompt-based methods. Therefore, in this paper, we propose a malicious prompt template construction method (\\textbf{PromptAttack}) to probe the security performance of PLMs. Several unfriendly template construction approaches are investigated to guide the model to misclassify the task. Extensive experiments on three datasets and three PLMs prove the effectiveness of our proposed approach PromptAttack. We also conduct experiments to verify that our method is applicable in few-shot scenarios.",
        "paperId": "251269b9e16ab1da20cb57a669b2bfdbd0d1cd72"
    },
    {
        "title": "Prompt-based Zero-shot Relation Extraction with Semantic Knowledge Augmentation",
        "firstAuthor": "Jiaying Gong",
        "url": null,
        "dateSubmitted": "2021-12-08",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "In relation triplet extraction (RTE), recognizing unseen (new) relations for which there are no training instances is a challenging task. Efforts have been made to recognize unseen relations based on question-answering models or relation descriptions. However, these approaches miss the semantic information about connections between seen and unseen relations. In this paper, We propose a prompt-based model with semantic knowledge augmentation (ZS-SKA) to recognize unseen relations under the zero-shot setting. We present a new word-level analogy-based sentence translation rule and generate augmented instances with unseen relations from instances with seen relations using that new rule. We design prompts with weighted virtual label construction based on an external knowledge graph to integrate semantic knowledge information learned from seen relations. Instead of using the actual label sets in the prompt template, we construct weighted virtual label words. We learn the representations of both seen and unseen relations with augmented instances and prompts. We then calculate the distance between the generated representations using prototypical networks to predict unseen relations. Extensive experiments conducted on three public datasets FewRel, Wiki-ZSL, and NYT, show that ZS-SKA outperforms state-of-the-art methods under the zero-shot scenarios. Our experimental results also demonstrate the effectiveness and robustness of ZS-SKA.",
        "paperId": "28456f63013fcbb43766563584efc7a8cb8a0275"
    },
    {
        "title": "Prompt-based Zero-shot Relation Classification with Semantic Knowledge Augmentation",
        "firstAuthor": "Jiaying Gong",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "In relation classification, recognizing unseen 001 (new) relations for which there are no training 002 instances is a challenging task. We propose 003 a prompt-based model with semantic knowl- 004 edge augmentation (ZS-SKA) to recognize un- 005 seen relations under the zero-shot setting. We 006 present a new word-level sentence translation 007 rule and generate augmented instances with 008 unseen relations from instances with seen rela- 009 tions using that new rule. We design prompts 010 based on an external knowledge graph to inte- 011 grate semantic knowledge information learned 012 from seen relations. Instead of using the actual 013 label sets in the prompt template, we construct 014 weighted virtual label words. We learn the rep- 015 resentations of both seen and unseen relations 016 with augmented instances and prompts. We 017 then calculate the distance between the gen- 018 erated representations using prototypical net- 019 works to predict unseen relations. Extensive 020 experiments conducted on three public datasets 021 show that ZS-SKA outperforms state-of-the-art 022 methods under the zero-shot scenarios. Our 023 experimental results also demonstrate the effec- 024 tiveness and robustness of ZS-SKA. 025",
        "paperId": "2b7f49fdbaf1468ca80c366bf074b82879fa9130"
    },
    {
        "title": "Position-based Prompting for Health Outcome Generation",
        "firstAuthor": "Micheal Abaho",
        "url": "http://arxiv.org/pdf/2204.03489",
        "dateSubmitted": "2022-03-30",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Probing factual knowledge in Pre-trained Language Models (PLMs) using prompts has indirectly implied that language models (LMs) can be treated as knowledge bases. To this end, this phenomenon has been effective, especially when these LMs are fine-tuned towards not just data, but also to the style or linguistic pattern of the prompts themselves. We observe that satisfying a particular linguistic pattern in prompts is an unsustainable, time-consuming constraint in the probing task, especially because they are often manually designed and the range of possible prompt template patterns can vary depending on the prompting task. To alleviate this constraint, we propose using a position-attention mechanism to capture positional information of each word in a prompt relative to the mask to be filled, hence avoiding the need to re-construct prompts when the prompts\u2019 linguistic pattern changes. Using our approach, we demonstrate the ability of eliciting answers (in a case study on health outcome generation) to not only common prompt templates like Cloze and Prefix but also rare ones too, such as Postfix and Mixed patterns whose masks are respectively at the start and in multiple random places of the prompt. More so, using various biomedical PLMs, our approach consistently outperforms a baseline in which the default PLMs representation is used to predict masked tokens.",
        "paperId": "2c12d24c5ba5ad3bb3994635fcfcb9f8caac31d0"
    },
    {
        "title": "Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with Auxiliary Refined Knowledge",
        "firstAuthor": "Jinyuan Li",
        "url": null,
        "dateSubmitted": "2023-05-20",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Multimodal Named Entity Recognition (MNER) on social media aims to enhance textual entity prediction by incorporating image-based clues. Existing studies mainly focus on maximizing the utilization of pertinent image information or incorporating external knowledge from explicit knowledge bases. However, these methods either neglect the necessity of providing the model with external knowledge, or encounter issues of high redundancy in the retrieved knowledge. In this paper, we present PGIM -- a two-stage framework that aims to leverage ChatGPT as an implicit knowledge base and enable it to heuristically generate auxiliary knowledge for more efficient entity prediction. Specifically, PGIM contains a Multimodal Similar Example Awareness module that selects suitable examples from a small number of predefined artificial samples. These examples are then integrated into a formatted prompt template tailored to the MNER and guide ChatGPT to generate auxiliary refined knowledge. Finally, the acquired knowledge is integrated with the original text and fed into a downstream model for further processing. Extensive experiments show that PGIM outperforms state-of-the-art methods on two classic MNER datasets and exhibits a stronger robustness and generalization capability.",
        "paperId": "2c23a8c8b65c3dfe3bdbe93e60e04637fee48e2b"
    },
    {
        "title": "GraphPrompt: Biomedical Entity Normalization Using Graph-based Prompt Templates",
        "firstAuthor": "Jiayou Zhang",
        "url": "https://www.biorxiv.org/content/biorxiv/early/2021/12/01/2021.11.29.470486.full.pdf",
        "dateSubmitted": "2021-11-13",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Biomedical entity normalization unifies the language across biomedical experiments and studies, and further enables us to obtain a holistic view of life sciences. Current approaches mainly study the normalization of more standardized entities such as diseases and drugs, while disregarding the more ambiguous but crucial entities such as pathways, functions and cell types, hindering their real-world applications. To achieve biomedical entity normalization on these under-explored entities, we first introduce an expert-curated dataset OBO-syn encompassing 70 different types of entities and 2 million curated entity-synonym pairs. To utilize the unique graph structure in this dataset, we propose GraphPrompt, a promptbased learning approach that creates prompt templates according to the graphs. Graph-Prompt obtained 41.0% and 29.9% improvement on zero-shot and few-shot settings respectively, indicating the effectiveness of these graph-based prompt templates. We envision that our method GraphPrompt and OBO-syn dataset can be broadly applied to graph-based NLP tasks, and serve as the basis for analyzing diverse and accumulating biomedical data.",
        "paperId": "2d7a6a52264e8f875105cfb34c6c901bfd1f3229"
    },
    {
        "title": "MetricPrompt: Prompting Model as a Relevance Metric for Few-shot Text Classification",
        "firstAuthor": "Hongyuan Dong",
        "url": "https://arxiv.org/pdf/2306.08892",
        "dateSubmitted": "2023-06-15",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Prompting methods have shown impressive performance in a variety of text mining tasks and applications, especially few-shot ones. Despite the promising prospects, the performance of prompting model largely depends on the design of prompt template and verbalizer. In this work, we propose MetricPrompt, which eases verbalizer design difficulty by reformulating few-shot text classification task into text pair relevance estimation task. MetricPrompt adopts prompting model as the relevance metric, further bridging the gap between Pre-trained Language Model's (PLM) pre-training objective and text classification task, making possible PLM's smooth adaption. Taking a training sample and a query one simultaneously, MetricPrompt captures cross-sample relevance information for accurate relevance estimation. We conduct experiments on three widely used text classification datasets across four few-shot settings. Results show that MetricPrompt outperforms manual verbalizer and other automatic verbalizer design methods across all few-shot settings, achieving new state-of-the-art (SOTA) performance.",
        "paperId": "2e403ad2cd02409e1fdc15839da0a3f89886a990"
    },
    {
        "title": "ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing",
        "firstAuthor": "Ian Arawjo",
        "url": "https://arxiv.org/pdf/2309.09128",
        "dateSubmitted": "2023-09-17",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Evaluating outputs of large language models (LLMs) is challenging, requiring making -- and making sense of -- many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.",
        "paperId": "2ed64d90670177bf58cdce6bda04a48a8731a18f"
    },
    {
        "title": "Prompt Learning for News Recommendation",
        "firstAuthor": "Zizhuo Zhang",
        "url": "https://arxiv.org/pdf/2304.05263",
        "dateSubmitted": "2023-04-11",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Some recent news recommendation (NR) methods introduce a Pre-trained Language Model (PLM) to encode news representation by following the vanilla pre-train and fine-tune paradigm with carefully-designed recommendation-specific neural networks and objective functions. Due to the inconsistent task objective with that of PLM, we argue that their modeling paradigm has not well exploited the abundant semantic information and linguistic knowledge embedded in the pre-training process. Recently, the pre-train, prompt, and predict paradigm, called prompt learning, has achieved many successes in natural language processing domain. In this paper, we make the first trial of this new paradigm to develop a Prompt Learning for News Recommendation (Prompt4NR) framework, which transforms the task of predicting whether a user would click a candidate news as a cloze-style mask-prediction task. Specifically, we design a series of prompt templates, including discrete, continuous, and hybrid templates, and construct their corresponding answer spaces to examine the proposed Prompt4NR framework. Furthermore, we use the prompt ensembling to integrate predictions from multiple prompt templates. Extensive experiments on the MIND dataset validate the effectiveness of our Prompt4NR with a set of new benchmark results.",
        "paperId": "2ee1f98649ff27378fc341cae907eb89aba8fba4"
    },
    {
        "title": "Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations",
        "firstAuthor": "Junyeob Kim",
        "url": "http://arxiv.org/pdf/2205.12685",
        "dateSubmitted": "2022-05-25",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Despite recent explosion of interests in in-context learning, the underlying mechanism and the precise impact of the quality of demonstrations remain elusive.Intuitively, ground-truth labels should have as much impact in in-context learning (ICL) as supervised learning, but recent work reported that the input-label correspondence is significantly less important than previously thought.Intrigued by this counter-intuitive observation, we re-examine the importance of ground-truth labels in in-context learning.With the introduction of two novel metrics, namely Label-Correctness Sensitivity and Ground-truth Label Effect Ratio (GLER), we were able to conduct quantifiable analysis on the impact of ground-truth label demonstrations.Through extensive analyses, we find that the correct input-label mappings can have varying impacts on the downstream in-context learning performances, depending on the experimental configuration.Through additional studies, we identify key components, such as the verbosity of prompt templates and the language model size, as the controlling factor to achieve more noise-resilient ICL.",
        "paperId": "316206a2f89eb94ce02a81fba1dc304586f21b39"
    },
    {
        "title": "Research on Implicit Intent Recognition Method Based on Prompt Learning",
        "firstAuthor": "Shuhua Liu",
        "url": "https://www.researchsquare.com/article/rs-1891913/latest.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "As one of the core modules of the dialogue system, intent recognition plays an important role in human-computer interaction. Most of the existing intent recognition research is limited to simple, direct, and explicit intent recognitions. However, the natural human-computer interactions are \ufb02exible and diverse, and the expressions are often the euphemistic implicit intentions. Therefore, the implicit intent recognition brings new research challenges in this \ufb01eld. This paper pioneers a Chinese Implicit Intent Dataset CIID, which covers 7 common intents from di\ufb00erent \ufb01elds, and the data is the text containing the user\u2019s implicit intent. Based on this corpus, it is the \ufb01rst time prompt learning is employed for implicit intent recognition and by constructing a suitable prompt template, the model can get \u201drelevant hints\u201d to dig out the true intention of the user. Finally, this paper evaluates a range of classi\ufb01cation models on CIID dataset. Experimental results show that the recognition rate of the proposed model is 97.6%, and achieves the state-of-the-art recognition accuracy. Furthermore, since it is di\ufb03cult to collect the user\u2019s implicit intention data, this paper also explores the performance of these classi\ufb01cation models on the CIID dataset with few-shot settings, and the experimental results show when the training data is reduced to 4.7%, the recognition rate of the proposed model can still keep 92.4%, which is signi\ufb01cantly higher than other baseline models, the results further prove this proposed method is advanced and robust.",
        "paperId": "330009bebca2152592067c9616e0d86505d49e27"
    },
    {
        "title": "Low-Resource Multi-Granularity Academic Function Recognition Based on Multiple Prompt Knowledge",
        "firstAuthor": "Jiawei Liu",
        "url": "http://arxiv.org/pdf/2305.03287",
        "dateSubmitted": "2023-05-05",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Fine-tuning pre-trained language models (PLMs), e.g., SciBERT, generally requires large numbers of annotated data to achieve state-of-the-art performance on a range of NLP tasks in the scientific domain. However, obtaining the fine-tune data for scientific NLP task is still challenging and expensive. Inspired by recent advancement in prompt learning, in this paper, we propose the Mix Prompt Tuning (MPT), which is a semi-supervised method to alleviate the dependence on annotated data and improve the performance of multi-granularity academic function recognition tasks with a small number of labeled examples. Specifically, the proposed method provides multi-perspective representations by combining manual prompt templates with automatically learned continuous prompt templates to help the given academic function recognition task take full advantage of knowledge in PLMs. Based on these prompt templates and the fine-tuned PLM, a large number of pseudo labels are assigned to the unlabeled examples. Finally, we fine-tune the PLM using the pseudo training set. We evaluate our method on three academic function recognition tasks of different granularity including the citation function, the abstract sentence function, and the keyword function, with datasets from computer science domain and biomedical domain. Extensive experiments demonstrate the effectiveness of our method and statistically significant improvements against strong baselines. In particular, it achieves an average increase of 5% in Macro-F1 score compared with fine-tuning, and 6% in Macro-F1 score compared with other semi-supervised method under low-resource settings. In addition, MPT is a general method that can be easily applied to other low-resource scientific classification tasks.",
        "paperId": "35d2276749c2c31290d2ff410a305112e742da71"
    },
    {
        "title": "An Optimized Transfer Attack Framework Towards Multi-Modal Machine Learning",
        "firstAuthor": "Yinjie Zhang",
        "url": null,
        "dateSubmitted": "2022-10-28",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Deep neural networks (DNNs) have excelled at a wide range of tasks, including computer vision (CV), natural language processing (NLP), and speech recognition. However, past research has demonstrated that DNNs are vulnerable to adversarial examples, which are deliberately meant to trick models into making incorrect predictions by adding subtle perturbations into inputs. Adversarial examples create an exponential threat to multi-modal models that can accept a variety of inputs. By attacking substitute models, we provide a transferable attack framework. The suggested framework optimizes the attack process by modifying the prompt templates and simultaneously raising the attack on multiple inputs. Our experiments demonstrate that the proposed attack framework can significantly improve the success rate of transferable attacks, and adversarial examples are rarely noticed by humans. Meanwhile, experiments show that in transferable attacks, coarse-grained adversarial examples can achieve higher attack success rates than fine-grained ones, and the multi-modal models has some robustness against uni-modal attacks.",
        "paperId": "3a0cee573e19db7f3c561152199a3386adf6cb74"
    },
    {
        "title": "TEMPLATE SYSTEM FOR SECOND LANGUAGE AURAL COMPREHENSION",
        "firstAuthor": "Donna Mydlarski",
        "url": null,
        "dateSubmitted": "2013-01-14",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "With the introduction of the PROMPT template materials into the French, Italian, and Spanish classes at the Universities of Calgary and Guelph, Canada, it became evident that a system to help language students improve their listening skills was much needed. In response to this need, a template called DICTATE was developed to allow teachers to use the dictation format to practice auditory discrimination, aural comprehension, and orthography in the most efficient and effective way possible.",
        "paperId": "3a4d4d5ae9d31d0f7af823255745e52bbc61073a"
    },
    {
        "title": "Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages",
        "firstAuthor": "Zheng-Xin Yong",
        "url": "https://arxiv.org/pdf/2303.13592",
        "dateSubmitted": "2023-03-23",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The recent proliferation of Large Language Models (LLMs) compels one to ask: how capable are these systems in generating code-mixed data? In this paper, we explore prompting multilingual LLMs in a zero-shot manner to generate code-mixed data for seven languages in South East Asia (SEA), namely Indonesian, Malay, Chinese, Tagalog, Vietnamese, Tamil, and Singlish. We find that publicly available multilingual instruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of producing texts with phrases or clauses from different languages. ChatGPT exhibits inconsistent capabilities in generating code-mixed texts, wherein its performance varies depending on the prompt template and language pairing. For instance, ChatGPT generates fluent and natural Singlish texts (an English-based creole spoken in Singapore), but for English-Tamil language pair, the system mostly produces grammatically incorrect or semantically meaningless utterances. Furthermore, it may erroneously introduce languages not specified in the prompt. Based on our investigation, existing multilingual LLMs exhibit a wide range of proficiency in code-mixed data generation for SEA languages. As such, we advise against using LLMs in this context without extensive human checks.",
        "paperId": "3b27092740a489a63589cdcf40fad6a0e093daa0"
    },
    {
        "title": "LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking",
        "firstAuthor": "Zhenrui Yue",
        "url": null,
        "dateSubmitted": "2023-10-25",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Recently, large language models (LLMs) have exhibited significant progress in language understanding and generation. By leveraging textual features, customized LLMs are also applied for recommendation and demonstrate improvements across diverse recommendation scenarios. Yet the majority of existing methods perform training-free recommendation that heavily relies on pretrained knowledge (e.g., movie recommendation). In addition, inference on LLMs is slow due to autoregressive generation, rendering existing methods less effective for real-time recommendation. As such, we propose a two-stage framework using large language models for ranking-based recommendation (LlamaRec). In particular, we use small-scale sequential recommenders to retrieve candidates based on the user interaction history. Then, both history and retrieved items are fed to the LLM in text via a carefully designed prompt template. Instead of generating next-item titles, we adopt a verbalizer-based approach that transforms output logits into probability distributions over the candidate items. Therefore, the proposed LlamaRec can efficiently rank items without generating long text. To validate the effectiveness of the proposed framework, we compare against state-of-the-art baseline methods on benchmark datasets. Our experimental results demonstrate the performance of LlamaRec, which consistently achieves superior performance in both recommendation performance and efficiency.",
        "paperId": "3b3b1aba98388dead7c1cf964eff34de85b50af7"
    },
    {
        "title": "Do Language Models Learn about Legal Entity Types during Pretraining?",
        "firstAuthor": "Claire Barale",
        "url": null,
        "dateSubmitted": "2023-10-19",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Language Models (LMs) have proven their ability to acquire diverse linguistic knowledge during the pretraining phase, potentially serving as a valuable source of incidental supervision for downstream tasks. However, there has been limited research conducted on the retrieval of domain-specific knowledge, and specifically legal knowledge. We propose to explore the task of Entity Typing, serving as a proxy for evaluating legal knowledge as an essential aspect of text comprehension, and a foundational task to numerous downstream legal NLP applications. Through systematic evaluation and analysis and two types of prompting (cloze sentences and QA-based templates) and to clarify the nature of these acquired cues, we compare diverse types and lengths of entities both general and domain-specific entities, semantics or syntax signals, and different LM pretraining corpus (generic and legal-oriented) and architectures (encoder BERT-based and decoder-only with Llama2). We show that (1) Llama2 performs well on certain entities and exhibits potential for substantial improvement with optimized prompt templates, (2) law-oriented LMs show inconsistent performance, possibly due to variations in their training corpus, (3) LMs demonstrate the ability to type entities even in the case of multi-token entities, (4) all models struggle with entities belonging to sub-domains of the law (5) Llama2 appears to frequently overlook syntactic cues, a shortcoming less present in BERT-based architectures.",
        "paperId": "3bbd9a5a0fccf2e18041db9118fff7807501876c"
    },
    {
        "title": "Knowledge-Guided Prompt Learning for Few-Shot Text Classification",
        "firstAuthor": "Liangguo Wang",
        "url": "https://www.mdpi.com/2079-9292/12/6/1486/pdf?version=1679462243",
        "dateSubmitted": "2023-03-21",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Recently, prompt-based learning has shown impressive performance on various natural language processing tasks in few-shot scenarios. The previous study of knowledge probing showed that the success of prompt learning contributes to the implicit knowledge stored in pre-trained language models. However, how this implicit knowledge helps solve downstream tasks remains unclear. In this work, we propose a knowledge-guided prompt learning method that can reveal relevant knowledge for text classification. Specifically, a knowledge prompting template and two multi-task frameworks were designed, respectively. The experiments demonstrated the superiority of combining knowledge and prompt learning in few-shot text classification.",
        "paperId": "3cd05e8137676c8a7e488d7b621b8fb3f2f2a399"
    },
    {
        "title": "ALT: Towards Fine-grained Alignment between Language and CTR Models for Click-Through Rate Prediction",
        "firstAuthor": "Hangyu Wang",
        "url": null,
        "dateSubmitted": "2023-10-30",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Click-through rate (CTR) prediction plays as a core function module in various personalized online services. According to the data modality and input format, the models for CTR prediction can be mainly classified into two categories. The first one is the traditional CTR models that take as inputs the one-hot encoded ID features of tabular modality, which aims to capture the collaborative signals via feature interaction modeling. The second category takes as inputs the sentences of textual modality obtained by hard prompt templates, where pretrained language models (PLMs) are adopted to extract the semantic knowledge. These two lines of research generally focus on different characteristics of the same input data (i.e., textual and tabular modalities), forming a distinct complementary relationship with each other. Therefore, in this paper, we propose to conduct fine-grained feature-level Alignment between Language and CTR models (ALT) for CTR prediction. Apart from the common CLIP-like instance-level contrastive learning, we further design a novel joint reconstruction pretraining task for both masked language and tabular modeling. Specifically, the masked data of one modality (i.e., tokens or features) has to be recovered with the help of the other modality, which establishes the feature-level interaction and alignment via sufficient mutual information extraction between dual modalities. Moreover, we propose three different finetuning strategies with the option to train the aligned language and CTR models separately or jointly for downstream CTR prediction tasks, thus accommodating the varying efficacy and efficiency requirements for industrial applications. Extensive experiments on three real-world datasets demonstrate that ALT outperforms SOTA baselines, and is highly compatible for various language and CTR models.",
        "paperId": "3d2fd5d3be7ccbad738a0787d96e6fe4123f20cf"
    },
    {
        "title": "A Search for Prompts: Generating Structured Answers from Contracts",
        "firstAuthor": "Adam Roegiest",
        "url": null,
        "dateSubmitted": "2023-10-16",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "In many legal processes being able to action on the concrete implication of a legal question can be valuable to automating human review or signalling certain conditions (e.g., alerts around automatic renewal). To support such tasks, we present a form of legal question answering that seeks to return one (or more) fixed answers for a question about a contract clause. After showing that unstructured generative question answering can have questionable outcomes for such a task, we discuss our exploration methodology for legal question answering prompts using OpenAI's \\textit{GPT-3.5-Turbo} and provide a summary of insights. Using insights gleaned from our qualitative experiences, we compare our proposed template prompts against a common semantic matching approach and find that our prompt templates are far more accurate despite being less reliable in the exact response return. With some additional tweaks to prompts and the use of in-context learning, we are able to further improve the performance of our proposed strategy while maximizing the reliability of responses as best we can.",
        "paperId": "3f13324da3856cba6f66ccbb81faa0fed4e78b28"
    },
    {
        "title": "Do prompt positions really matter?",
        "firstAuthor": "Junyu Mao",
        "url": null,
        "dateSubmitted": "2023-05-23",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Prompt-based models have gathered a lot of attention from researchers due to their remarkable advancements in the fields of zero-shot and few-shot learning. Developing an effective prompt template plays a critical role. However, prior studies have mainly focused on prompt vocabulary selection or embedding initialization within a predefined template with the prompt position fixed. In this empirical study, we conduct the most comprehensive analysis to date of prompt position for diverse natural language process tasks. Our findings quantify the substantial impact prompt position has on model performance. We observe that the prompt position used in prior studies is often sub-optimal. These findings suggest prompt position optimisation as a valuable research direction to fill the gap in existing prompt engineering methodologies.",
        "paperId": "405b3b7f04380c87677735e471b8434616d2e229"
    },
    {
        "title": "UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?",
        "firstAuthor": "Dennis Aumiller",
        "url": "http://arxiv.org/pdf/2301.01764",
        "dateSubmitted": "2023-01-04",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Previous state-of-the-art models for lexical simplification consist of complex pipelines with several components, each of which requires deep technical knowledge and fine-tuned interaction to achieve its full potential. As an alternative, we describe a frustratingly simple pipeline based on prompted GPT-3 responses, beating competing approaches by a wide margin in settings with few training instances. Our best-performing submission to the English language track of the TSAR-2022 shared task consists of an \u201censemble\u201d of six different prompt templates with varying context levels. As a late-breaking result, we further detail a language transfer technique that allows simplification in languages other than English. Applied to the Spanish and Portuguese subset, we achieve state-of-the-art results with only minor modification to the original prompts. Aside from detailing the implementation and setup, we spend the remainder of this work discussing the particularities of prompting and implications for future work. Code for the experiments is available online at https://github.com/dennlinger/TSAR-2022-Shared-Task.",
        "paperId": "40fba1fc70e23abf9a3ea428f186dd44e57723fb"
    },
    {
        "title": "Grammar Correction for Multiple Errors in Chinese Based on Prompt Templates",
        "firstAuthor": "Zhici Wang",
        "url": "https://www.mdpi.com/2076-3417/13/15/8858/pdf?version=1690869487",
        "dateSubmitted": "2023-07-31",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Grammar error correction (GEC) is a crucial task in the field of Natural Language Processing (NLP). Its objective is to automatically detect and rectify grammatical mistakes in sentences, which possesses immense application research value. Currently, mainstream grammar-correction methods primarily rely on sequence labeling and text generation, which are two kinds of end-to-end methods. These methods have shown exemplary performance in areas with low error density but often fail to deliver satisfactory results in high-error density situations where multiple errors exist in a single sentence. Consequently, these methods tend to overcorrect correct words, leading to a high rate of false positives. To address this issue, we researched the specific characteristics of the Chinese grammar error correction (CGEC) task in high-error density situations. We proposed a grammar-correction method based on prompt templates. Firstly, we proposed a strategy for constructing prompt templates suitable for CGEC. This strategy transforms the CGEC task into a masked fill-in-the-blank task compatible with the masked language model BERT. Secondly, we proposed a method for dynamically updating templates, which incorporates already corrected errors into the template through dynamic updates to improve the template quality. Moreover, we used the phonetic and graphical resemblance knowledge from the confusion set as guiding information. By combining this with BERT\u2019s prediction results, the model can more accurately select the correct characters, significantly enhancing the accuracy of the model\u2019s prediction correction results. Our methods were validated through experiments on a public grammar-correction dataset. The results indicate that our method achieves higher correction performance and lower false correction rates in high-error density scenarios.",
        "paperId": "41103d218801da20683c50f137526c2929166583"
    },
    {
        "title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
        "firstAuthor": "Tom Kocmi",
        "url": "http://arxiv.org/pdf/2302.14520",
        "dateSubmitted": "2023-02-28",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "We describe GEMBA, a GPT-based metric for assessment of translation quality, which works both with a reference translation and without. In our evaluation, we focus on zero-shot prompting, comparing four prompt variants in two modes, based on the availability of the reference. We investigate seven versions of GPT models, including ChatGPT. We show that our method for translation quality assessment only works with GPT 3.5 and larger models. Comparing to results from WMT22\u2019s Metrics shared task, our method achieves state-of-the-art accuracy in both modes when compared to MQM-based human labels. Our results are valid on the system level for all three WMT22 Metrics shared task language pairs, namely English into German, English into Russian, and Chinese into English. This provides a first glimpse into the usefulness of pre-trained, generative large language models for quality assessment of translations. We publicly release all our code and prompt templates used for the experiments described in this work, as well as all corresponding scoring results, to allow for external validation and reproducibility.",
        "paperId": "4161ad2d2495d8af1d62dc5e71882bde642cd1c1"
    },
    {
        "title": "Decomposed Two-Stage Prompt Learning for Few-Shot Named Entity Recognition",
        "firstAuthor": "Feiyang Ye",
        "url": "https://www.mdpi.com/2078-2489/14/5/262/pdf?version=1682672547",
        "dateSubmitted": "2023-04-28",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Named entity recognition (NER) in a few-shot setting is an extremely challenging task, and most existing methods fail to account for the gap between NER tasks and pre-trained language models. Although prompt learning has been successfully applied in few-shot classification tasks, adapting to token-level classification similar to the NER task presents challenges in terms of time consumption and efficiency. In this work, we propose a decomposed prompt learning NER framework for few-shot settings, decomposing the NER task into two stages: entity locating and entity typing. In training, the location information of distant labels is used to train the entity locating model. A concise but effective prompt template is built to train the entity typing model. In inference, a pipeline approach is used to handle the entire NER task, which elegantly resolves time-consuming and inefficient problems. Specifically, a well-trained entity locating model is used to predict entity spans for each input. The input is then transformed using prompt templates, and the well-trained entity typing model is used to predict their types in a single step. Experimental results demonstrate that our framework outperforms previous prompt-based methods by an average of 2.3\u201312.9% in F1 score while achieving the best trade-off between accuracy and inference speed.",
        "paperId": "427d332ab3a1bdfb0c62c9f852e90dc2b2880546"
    },
    {
        "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems",
        "firstAuthor": "Yupeng Hou",
        "url": "http://arxiv.org/pdf/2305.08845",
        "dateSubmitted": "2023-05-15",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Recently, large language models (LLMs) (e.g. GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. To conduct our empirical study, we first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by the candidate generation model as candidates. We adopt a specific prompting approach to solving the ranking task by LLMs: we carefully design the prompting template by including the sequential interaction history, the candidate items, and the ranking instruction. We conduct extensive experiments on two widely-used datasets for recommender systems and derive several key findings for the use of LLMs in recommender systems. We show that LLMs have promising zero-shot ranking abilities, even competitive to or better than conventional recommendation models on candidates retrieved by multiple candidate generators. We also demonstrate that LLMs struggle to perceive the order of historical interactions and can be affected by biases like position bias, while these issues can be alleviated via specially designed prompting and bootstrapping strategies. The code to reproduce this work is available at https://github.com/RUCAIBox/LLMRank.",
        "paperId": "4683d3d6cb31111cf4499a199c0b036662b3eb32"
    },
    {
        "title": "Meta Learning for Domain Agnostic Soft Prompt",
        "firstAuthor": "Ming-Yen Chen",
        "url": null,
        "dateSubmitted": "2023-06-04",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "The prompt-based learning, as used in GPT-3, has become a popular approach to extract knowledge from a powerful pre-trained language model (PLM) for natural language understanding tasks. However, either applying the hard prompt for sentences by defining a collection of human-engineering prompt templates or directly optimizing the soft or continuous prompt with labeled data may not really generalize well for unseen domain data. To cope with this issue, this paper presents a new prompt-based unsupervised domain adaptation where the learned soft prompt is able to boost the frozen pre-trained language model to deal with the input tokens from unseen domains. Importantly, the meta learning and optimization is developed to carry out the domain agnostic soft prompt where the loss for masked language model is minimized. The experiments on multi-domain natural language understanding tasks show the merits of the proposed method.",
        "paperId": "4791aacf07f8ed425003d32ad3138416ffa44ae2"
    },
    {
        "title": "A Chinese Few-Shot Text Classification Method Utilizing Improved Prompt Learning and Unlabeled Data",
        "firstAuthor": "Tingkai Hu",
        "url": "https://www.mdpi.com/2076-3417/13/5/3334/pdf?version=1678093925",
        "dateSubmitted": "2023-03-06",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Insufficiently labeled samples and low-generalization performance have become significant natural language processing problems, drawing significant concern for few-shot text classification (FSTC). Advances in prompt learning have significantly improved the performance of FSTC. However, prompt learning methods typically require the pre-trained language model and tokens of the vocabulary list for model training, while different language models have different token coding structures, making it impractical to build effective Chinese prompt learning methods from previous approaches related to English. In addition, a majority of current prompt learning methods do not make use of existing unlabeled data, thus often leading to unsatisfactory performance in real-world applications. To address the above limitations, we propose a novel Chinese FSTC method called CIPLUD that combines an improved prompt learning method and existing unlabeled data, which are used for the classification of a small amount of Chinese text data. We used the Chinese pre-trained language model to build two modules: the Multiple Masks Optimization-based Prompt Learning (MMOPL) module and the One-Class Support Vector Machine-based Unlabeled Data Leveraging (OCSVM-UDL) module. The former generates prompt prefixes with multiple masks and constructs suitable prompt templates for Chinese labels. It optimizes the random token combination problem during label prediction with joint probability and length constraints. The latter, by establishing an OCSVM model in the trained text vector space, selects reasonable pseudo-label data for each category from a large amount of unlabeled data. After selecting the pseudo-label data, we mixed them with the previous few-shot annotated data to obtain brand new training data and then repeated the steps of the two modules as an iterative semi-supervised optimization process. The experimental results on the four Chinese FSTC benchmark datasets demonstrate that our proposed solution outperformed other prompt learning methods with an average accuracy improvement of 2.3%.",
        "paperId": "492a1334e243e94c321c162e05bc65f74cb91108"
    },
    {
        "title": "Can Language Models be Biomedical Knowledge Bases?",
        "firstAuthor": "Mujeen Sung",
        "url": "https://aclanthology.org/2021.emnlp-main.388.pdf",
        "dateSubmitted": "2021-09-15",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these LMs contain and how we can extract that knowledge, treating LMs as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge triples for probing biomedical LMs. We find that biomedical LMs with recently proposed probing methods can achieve up to 18.51% Acc@5 on retrieving biomedical knowledge. Although this seems promising given the task difficulty, our detailed analyses reveal that most predictions are highly correlated with prompt templates without any subjects, hence producing similar results on each relation and hindering their capabilities to be used as domain-specific KBs. We hope that BioLAMA can serve as a challenging benchmark for biomedical factual probing.",
        "paperId": "4c5f4ddc68be643fb34ea969bf2c105ff7538995"
    },
    {
        "title": "Implicit Sentiment Extraction Using Structure Generation with Sentiment Instructor Prompt Template",
        "firstAuthor": "YuXuan Liu",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": null,
        "paperId": "4fccd8c547d9ed3338e026725a8614df5cff2129"
    },
    {
        "title": "Prompt-Enhanced Self-supervised Representation Learning for Remote Sensing Image Understanding",
        "firstAuthor": "Mingming Zhang",
        "url": "https://arxiv.org/pdf/2310.00022",
        "dateSubmitted": "2023-09-28",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Learning representations through self-supervision on a large-scale, unlabeled dataset has proven to be highly effective for understanding diverse images, such as those used in remote sensing image analysis. However, remote sensing images often have complex and densely populated scenes, with multiple land objects and no clear foreground objects. This intrinsic property can lead to false positive pairs in contrastive learning, or missing contextual information in reconstructive learning, which can limit the effectiveness of existing self-supervised learning methods. To address these problems, we propose a prompt-enhanced self-supervised representation learning method that uses a simple yet efficient pre-training pipeline. Our approach involves utilizing original image patches as a reconstructive prompt template, and designing a prompt-enhanced generative branch that provides contextual information through semantic consistency constraints. We collected a dataset of over 1.28 million remote sensing images that is comparable to the popular ImageNet dataset, but without specific temporal or geographical constraints. Our experiments show that our method outperforms fully supervised learning models and state-of-the-art self-supervised learning methods on various downstream tasks, including land cover classification, semantic segmentation, object detection, and instance segmentation. These results demonstrate that our approach learns impressive remote sensing representations with high generalization and transferability.",
        "paperId": "52b6478b6f62beba0e604a54637d8987ebc21f6b"
    },
    {
        "title": "Evaluating BERT on cloud-edge time series forecasting and sentiment analysis via prompt learning",
        "firstAuthor": "Qizhi Li",
        "url": null,
        "dateSubmitted": "2022-12-01",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Existing pre-trained language models (PTLMs), like BERT, have shown their powerful ca-pabilities in many natural language processing tasks. In sequence analysis, such as time series forecasting, anomaly detection, and sentiment analysis, PTLMs have also achieved new state-of-the-art results. However, does this mean that PTLMs know sequence analysis? This paper explores whether BERT pre-trained on a large amount of data contains knowledge of sequence analysis. Specifically, we adopt prompt learning to see whether BERT will achieve good results on cloud-edge time series forecasting and sentiment analysis tasks. For the cloud-edge time series forecasting task, we give BERT some regular cloud-edge data and let it predict the features of the next time step; For the sentiment analysis task, we give BERT some sentence with sentiment and ask it what sentiment these sen-tences carry. Our experimental results reveal that: (1) BERT performs not well on the cloud-edge time series forecasting task, which means the logical reasoning of BERT is not good; (2) for sentiment analysis task, BERT with the prompt template performs poorly on both English and Chinese datasets; and (3) for sentiment analysis task, BERT appears to be more likely to perceive the text as carrying positive sentiment.",
        "paperId": "534909858fa7d4799d59bdcaf813f60036589f2c"
    },
    {
        "title": "Be-or-Not Prompt Enhanced Hard Negatives Generating For Memes Category Detection",
        "firstAuthor": "Jian Cui",
        "url": null,
        "dateSubmitted": "2023-07-01",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Memes are one of the most popular social media in online disinformation campaigns. Their creators often use a variety of rhetoric and psychological skills to achieve the purpose of misinformed audiences. These characteristics lead to the unsatisfactory performance of memes category detection tasks, such as predicting propaganda techniques, being harmful or not, and so on. To this end, we propose a novel memes category detection model via Be-or-Not Prompt Enhanced hard Negatives generating (BNPEN). Firstly, our BNPEN is reformulated into a contrastive learning-based image-text matching (ITM) task through category-padded prompt engineering. Secondly, we design the be-or-not prompt templates to keep the writing style of memes and create hard negative image-text pairs. Finally, our negatives generating can alleviate the negative-positive-coupling (NPC) effects in contrastive learning, thus improving the image-text matching quality. Conducted on two public datasets, experimental results show that our BNPEN is better than the off-the-shelf multi-modal learning models in terms of F1 and Accuracy measures.",
        "paperId": "54c57d23ce1b3e4d4bed15d2f9f4f665d4753ebd"
    },
    {
        "title": "Prompt position really matters in few-shot and zero-shot NLU tasks",
        "firstAuthor": "Junyu Mao",
        "url": "https://arxiv.org/pdf/2305.14493",
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Prompt-based models have made remarkable advancements in the fields of zero-shot and few-shot learning, attracting a lot of attention from researchers. Developing an effective prompt template plays a critical role. However, prior studies have mainly focused on prompt vocabulary selection or embedding initialization with the reserved prompt position fixed. In this empirical study, we conduct the most comprehensive analysis to date of prompt position option for natural language understanding tasks. Our findings quantify the substantial impact prompt position has on model performance. We observe that the prompt position used in prior studies is often sub-optimal for both zero-shot and few-shot settings. These findings suggest prompt position optimisation as an interesting research direction alongside the existing focus on prompt engineering.",
        "paperId": "56a9c96a29f4047be8465244576d731f0df2d9df"
    },
    {
        "title": "Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Dataset Augmented by ChatGPT",
        "firstAuthor": "Jiawei Zhang",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing multi-step logic reasoning, precise mathematical calculation and perception about the spatial and temporal factors. To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools. Specifically, we will investigate to teach Graph-ToolFormer to handle various graph data reasoning tasks in this paper, including both (1) very basic graph data loading and graph property reasoning tasks, ranging from simple graph order and size to the graph diameter and periphery, and (2) more advanced reasoning tasks on real-world graph data, such as bibliographic networks, protein molecules, sequential recommender systems, social networks and knowledge graphs. Technically, to build Graph-ToolFormer, we propose to hand-craft both the instruction and a small-sized of prompt templates for each of the graph reasoning tasks, respectively. Via in-context learning, based on such instructions and prompt template examples, we adopt ChatGPT to annotate and augment a larger graph reasoning statement dataset with the most appropriate calls of external API functions. Such augmented prompt datasets will be post-processed with selective filtering and used for fine-tuning existing pre-trained causal LLMs, such as the GPT-J, to teach them how to use graph reasoning tools in the output generation. To demonstrate the effectiveness of Graph-ToolFormer, we conduct some preliminary",
        "paperId": "56e403ba5941cf4053a52fb752426e4a9b0255fd"
    },
    {
        "title": "Dynamic Gesture Recognition Based on Three-Stream Coordinate Attention Network and Knowledge Distillation",
        "firstAuthor": "Shanshan Wan",
        "url": "https://ieeexplore.ieee.org/ielx7/6287639/10005208/10129842.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Gesture recognition has always been one of the important research directions in the field of computer vision. The dynamic gesture has the problems of complex backgrounds and many interference factors. The gesture recognition model based on deep learning usually has high computational cost and poor real-time performance. In addition, deep learning models are limited to recognizing existing categories in the training set and their performance largely depends on the amount of labeled data. To address the above problems, this paper presents a dynamic gesture recognition method named 3SCKI based on a three-stream coordinate attention (CA) network, knowledge distillation, and image-text contrastive learning. Specifically, 1) CA is utilized for feature fusion to make the model focus more on target gestures and reduce background interference, 2) traditional knowledge distillation loss is improved to reduce the amount of calculation and improve the real-time performance. Specifically, the guidance function is added to make the student network only learn the classification probability correctly identified by the teacher network, and 3) multi-granularity context prompt template integration method is proposed to construct an improved CLIP visual language model MG-CLIP. It aligns text and visual concepts from the image level to the object level to the part level. Through comparative learning of image features and text features, gesture classification is performed, enabling the model to identify image categories that have not appeared during the training phase. The proposed method is evaluated on the ChaLearn LAP large-scale isolated gesture dataset (IsoGD). The results show that our proposed method can obtain recognition rates of 65.87% on the validation set of IsoGD. For single mode data, 3SCKI can obtain the state-of-the-art recognition accuracy on RGB, Depth, and Optical Flow data (61.22%, 58.84%, and 50.30% of the validation set of IsoGD, respectively).",
        "paperId": "59872e5f75f3d452a595b472d08ced796b7c9cd8"
    },
    {
        "title": "DynaMaR: Dynamic Prompt with Mask Token Representation",
        "firstAuthor": "Xiaodi Sun",
        "url": "https://arxiv.org/pdf/2206.02982",
        "dateSubmitted": "2022-06-07",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Recent research has shown that large language models pretrained using unsupervised approaches can achieve significant performance improvement on many downstream tasks. Typically when adapting these language models to downstream tasks, like a classification or regression task, we employ a fine-tuning paradigm in which the sentence representation from the language model is input to a task-specific head; the model is then fine-tuned end-to-end. However, with the emergence of models like GPT-3, prompt-based fine-tuning has been proven to be a successful approach for few-shot tasks. Inspired by this work, we study discrete prompt technologies in practice. There are two issues that arise with the standard prompt approach. First, it can overfit on the prompt template. Second, it requires manual effort to formulate the downstream task as a language model problem. In this paper, we propose an improvement to prompt-based fine-tuning that addresses these two issues. We refer to our approach as DynaMaR -- Dynamic Prompt with Mask Token Representation. Results show that DynaMaR can achieve an average improvement of 10% in few-shot settings and improvement of 3.7% in data-rich settings over the standard fine-tuning approach on four e-commerce applications.",
        "paperId": "5d5b6b6c033c36a8b730042392cd29da84b67481"
    },
    {
        "title": "Few-Shot Table-to-Text Generation with Prompt-based Adapter",
        "firstAuthor": "Zhixin Guo",
        "url": "https://arxiv.org/pdf/2302.12468",
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "\u2014Pre-trained language models (PLMs) have made remarkable progress in table-to-text generation tasks. However, the topological gap between tabular data and text and the lack of domain-speci\ufb01c knowledge make it dif\ufb01cult for PLMs to produce faithful text, especially in real-world applications with limited resources. In this paper, we mitigate the above challenges by introducing a novel augmentation method: Prompt-based Adapter (PA), which targets table-to-text generation under few-shot conditions. The core insight design of the PA is to inject prompt templates for augmenting domain-speci\ufb01c knowledge and table-related representations into the model for bridging the structural gap between tabular data and descriptions through adapters. Such prompt-based knowledge augmentation method brings at least two bene\ufb01ts: (1) enables us to fully use the large amounts of unlabelled domain-speci\ufb01c knowledge, which can alleviate the PLMs\u2019 inherent shortcomings of lacking domain knowledge; (2) allows us to design different types of tasks supporting the generative challenge. Extensive experiments and analyses are conducted on three open-domain few-shot NLG datasets: human, song, and book. Compared to previous state-of-the-art approaches, our model achieves superior performance in terms of both \ufb02uency and accuracy as judged by human and automatic evaluations.",
        "paperId": "5fd951377b3a41ef80f9b15f617ce33d17118e47"
    },
    {
        "title": "IJCAI-ECAI Workshop \u201cInteractions between Analogical Reasoning and Machine Learning\u201d (IARML 2022)",
        "firstAuthor": "Kevin S. Chan",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Prompt learning, a recent thread in few-shot learning for pre-trained language models (PLMs), has been explored for completing word analogies in the extractive way. In this paper, we reformulate the analogy task as masked analogy completion task with the use of prompting to derive a generative model for analogies beyond words. We introduce a simple prompt-based fine-tuning paradigm for language modeling on answered prompts of analogies in the sequence-to-sequence framework. To convert discrete terms of analogies into linear sequences, we present a symbolic prompt template. The sequence-tosequence model is fine-tuned to fill in the missing span of masked prompts deduced from different masking schemes on phrase analogies extracted from a small corpus. We analyze the out-of-distribution performance on sentence analogies which are unseen cases. Our experiments demonstrate that promptbased fine-tuning with the objective of language modeling enables models to achieve significantly better performance on in-distribution cases than PLMs. Masked prompt learning with one-term masking exhibits the best out-of-distribution generalization on sentence analogies, with a difference of only 3 characters from references.",
        "paperId": "6203a2427fd682745e4e8f2585a659cfaaa3d5b1"
    },
    {
        "title": "Optimizing Continuous Prompts for Visual Relationship Detection by Affix-Tuning",
        "firstAuthor": "Shouguan Xiao",
        "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/09815128.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Visual relationship detection is crucial for understanding visual scenes and is widely used in many areas, including visual navigation, visual question answering, and machine trouble detection. Traditional detection methods often fuse multiple region modules, which takes considerable time and resources to train every module with extensive samples. As every module is independent, the computation process has difficulty achieving unity and lacks a higher level of logical reasonability. In response to the above problems, we propose a novel method of affix-tuning transformers for visual relationship detection tasks, which keeps transformer model parameters frozen and optimizes a small continuous task-specific vector. It not only makes the model unified and reduces the training cost but also maintains the common-sense reasonability without multiscale training. In addition, we design a vision-and-language sentence expression prompt template and train a few transformer model parameters for downstream tasks. Our method, Prompt Template and Affix-Tuning Transformers (PTAT), is evaluated on visual relationship detection and Visual Genome datasets. Finally, the results of the proposed method are close to or even higher than those of the state-of-the-art methods on some evaluation metrics.",
        "paperId": "63c3520050a70acdf564db80d26ab4dcd839d7e3"
    },
    {
        "title": "HybridPrompt: Bridging Language Models and Human Priors in Prompt Tuning for Visual Question Answering",
        "firstAuthor": "Zhiyuan Ma",
        "url": "https://ojs.aaai.org/index.php/AAAI/article/download/26569/26341",
        "dateSubmitted": "2023-06-26",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Visual Question Answering (VQA) aims to answer the natural language question about a given image by understanding multimodal content. However, the answer quality of most existing visual-language pre-training (VLP) methods is still limited, mainly due to: (1) Incompatibility. Upstream pre-training tasks are generally incompatible with downstream question answering tasks, which makes the knowledge from the language model not well transferable to downstream tasks, and greatly limits their performance in few-shot scenarios; (2) Under-fitting. They generally do not integrate human priors to compensate for universal knowledge from language models, so as to fit the challenging VQA problem and generate reliable answers. To address these issues, we propose HybridPrompt, a cloze- and verify-style hybrid prompt framework with bridging language models and human priors in prompt tuning for VQA. Specifically, we first modify the input questions into the cloze-style prompts to narrow the gap between upstream pre-training tasks and downstream VQA task, which ensures that the universal knowledge in the language model can be better transferred to subsequent human prior-guided prompt tuning. Then, we imitate the cognitive process of human brain to introduce topic and sample related priors to construct a dynamic learnable prompt template for human prior-guided prompt learning. Finally, we add fixed-length learnable free-parameters to further enhance the generalizability and scalability of prompt learning in the VQA model. Experimental results verify the effectiveness of HybridPrompt, showing that it achieves competitive performance against previous methods on widely-used VQAv2 dataset and obtains new state-of-the-art results. Our code is released at: https://github.com/zhizhi111/hybrid.",
        "paperId": "6470a35a46bbc8a844954af9fdf31e440d1aa289"
    },
    {
        "title": "CUP: Curriculum Learning based Prompt Tuning for Implicit Event Argument Extraction",
        "firstAuthor": "Jiaju Lin",
        "url": "https://arxiv.org/pdf/2205.00498",
        "dateSubmitted": "2022-05-01",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Implicit event argument extraction (EAE) aims to identify arguments that could scatter over the document. Most previous work focuses on learning the direct relations between arguments and the given trigger, while the implicit relations with long-range dependency are not well studied. Moreover, recent neural network based approaches rely on a large amount of labeled data for training, which is unavailable due to the high labelling cost. In this paper, we propose a Curriculum learning based Prompt tuning (CUP) approach, which resolves implicit EAE by four learning stages. The stages are defined according to the relations with the trigger node in a semantic graph, which well captures the long-range dependency between arguments and the trigger. In addition, we integrate a prompt-based encoder-decoder model to elicit related knowledge from pre-trained language models (PLMs) in each stage, where the prompt templates are adapted with the learning progress to enhance the reasoning for arguments. Experimental results on two well-known benchmark datasets show the great advantages of our proposed approach. In particular, we outperform the state-of-the-art models in both fully-supervised and low-data scenarios.",
        "paperId": "65d88194a902332b78dd5a7b919fa577bfa7ee9f"
    },
    {
        "title": "Multimodal Propaganda Detection Via Anti-Persuasion Prompt enhanced contrastive learning",
        "firstAuthor": "Jian Cui",
        "url": null,
        "dateSubmitted": "2023-06-04",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Propaganda, commonly used in memes disinformation, can influence the thinking of the audience and increase the reach of communication. Usually logical fallacy, as a kind of popular expression of memes, aims to create a logical reasonable illusion where the conclusion cannot be drawn with the use of correct logic rules. However, this characteristic inherent in memes leads to difficulties for classic multi-label classifiers to understand propagation techniques. To this end, we propose a novel propaganda detection model called Antipersuasion Prompt Enhanced Contrastive Learning (abbreviated as APCL). First, our APCL reformulates the multi-label classification task by leveraging the category words of propaganda technique based prompt engineering, which is converted into an image-text matching (ITM). Second, prompt engineering is designed with a persuasion prompt template and an anti-persuasion prompt template. The former is to build matched text-image pairs, and the latter is to form mismatched text-image pairs which fit the logical fallacy style of memes. Finally, the propagation technique is predicted based on the distances between the above two prompt templates enhanced texts and an input image. Experimental results on the memes dataset of SemEval2021 task 6 show that our APCL outperforms the state-of-the-art multimodal classification models in terms of F1 measures.",
        "paperId": "65e45895b9d9047b13d4f3ba58f4a386278d8cb0"
    },
    {
        "title": "CitePrompt: Using Prompts to Identify Citation Intent in Scientific Papers",
        "firstAuthor": "Avishek Lahiri",
        "url": "https://arxiv.org/pdf/2304.12730",
        "dateSubmitted": "2023-04-25",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Citations in scientific papers not only help us trace the intellectual lineage but also are a useful indicator of the scientific significance of the work. Citation intents prove beneficial as they specify the role of the citation in a given context. We present a tool Citeprompt which uses the hitherto unexplored approach of prompt learning for citation intent classification. We argue that with the proper choice of the pretrained language model, the prompt template, and the prompt verbalizer, we can not only get results that are better than or comparable to those obtained with the state-of-the-art methods but also do it with much less exterior information about the scientific document. We report state-of-the-art results on the ACL-ARC dataset, and also show significant improvement on the SciCite dataset over all baseline models except one. As suitably large labelled datasets for citation intent classification can be quite hard to find, in a first, we propose the conversion of this task to the few-shot and zero-shot settings. For the ACL-ARC dataset, we report a 53.86% F1 score for the zero-shot setting, which improves to 63.61% and 66.99% for the 5-shot and 10-shot settings respectively.",
        "paperId": "68ee8a53f0b1ff146194980337dd6d533b17c59b"
    },
    {
        "title": "Making Small Language Models Better Few-Shot Learners",
        "firstAuthor": "",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Large-scale language models coupled with 001 prompts have shown remarkable performance 002 on few-shot learning. However, through sys- 003 tematic experiments, we find that the few-shot 004 performance of small language models is poor, 005 and using prompts on them brings fewer im- 006 provements than on larger ones. In this paper, 007 we propose SMASH , an approach to improve 008 SMA ll language models\u2019 few- SH ot ability by 009 training on intermediate tasks before prompt- 010 based fine-tuning on downstream tasks. We de- 011 sign intermediate tasks for sentence-pair tasks 012 and single-sentence classification tasks by cre- 013 ating training examples with prompt templates 014 similar to downstream tasks using sentences 015 sampled from a large-scale unsupervised cor- 016 pus, and apply knowledge distillation to distill 017 from outputs of larger pre-trained models as 018 training objective. We conduct extensive ex- 019 periments and show that SMASH can make a 020 6-layer DistilRoBRETa-base achieve compa- 021 rable performance on few-shot datasets to a 022 12-layer RoBERTa-base at a low cost. 1 023",
        "paperId": "6995cfa818ddd1ba0b7eaf0564e4b080732f5000"
    },
    {
        "title": "Adapting Prompt for Few-shot Table-to-Text Generation",
        "firstAuthor": "Zhixin Guo",
        "url": null,
        "dateSubmitted": "2023-02-24",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Pretrained language models (PLMs) have made remarkable progress in table-to-text generation tasks. However, the lack of domain-specific knowledge makes it challenging to bridge the topological gap between tabular data and text, especially in real-world applications with limited resources. To mitigate the limitation of insufficient labeled data, we propose a novel framework: Adapt-Prompt-to-Generate (AdaPTGen). The core insight of AdaPTGen is to adapt prompt templates of domain-specific knowledge into the model, which brings at least three benefits: (1) it injects representation of normal table-related descriptions to bridge the topological gap between tabular data and texts; (2) it enables us to use large amounts of unlabeled domain-specific knowledge fully, which can alleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it allows us to design various tasks to explore the domain-specific knowledge. Extensive experiments and analyses are conducted on three open-domain few-shot natural language generation (NLG) data sets: Humans, Songs, and Books. Compared to previous state-of-the-art approaches, our model achieves superior performance in terms of both fluency and accuracy.",
        "paperId": "6ac0ad894e2af0bb2b0b3d8c2878faf41f77eb0b"
    },
    {
        "title": "Multi-label Few-shot ICD Coding as Autoregressive Generation with Prompt",
        "firstAuthor": "Zhichao Yang",
        "url": "https://arxiv.org/pdf/2211.13813",
        "dateSubmitted": "2022-11-24",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Automatic International Classification of Diseases (ICD) coding aims to assign multiple ICD codes to a medical note with an average of 3,000+ tokens. This task is challenging due to the high-dimensional space of multi-label assignment (155,000+ ICD code candidates) and the long-tail challenge - Many ICD codes are infrequently assigned yet infrequent ICD codes are important clinically. This study addresses the long-tail challenge by transforming this multi-label classification task into an autoregressive generation task. Specifically, we first introduce a novel pretraining objective to generate free text diagnoses and procedures using the SOAP structure, the medical logic physicians use for note documentation. Second, instead of directly predicting the high dimensional space of ICD codes, our model generates the lower dimension of text descriptions, which then infers ICD codes. Third, we designed a novel prompt template for multi-label classification. We evaluate our Generation with Prompt (GPsoap) model with the benchmark of all code assignment (MIMIC-III-full) and few shot ICD code assignment evaluation benchmark (MIMIC-III-few). Experiments on MIMIC-III-few show that our model performs with a marco F130.2, which substantially outperforms the previous MIMIC-III-full SOTA model (marco F1 4.3) and the model specifically designed for few/zero shot setting (marco F1 18.7). Finally, we design a novel ensemble learner, a cross-attention reranker with prompts, to integrate previous SOTA and our best few-shot coding predictions. Experiments on MIMIC-III-full show that our ensemble learner substantially improves both macro and micro F1, from 10.4 to 14.6 and from 58.2 to 59.1, respectively.",
        "paperId": "6b87c9700b8de4912fe7c361574640b5dc536ca9"
    },
    {
        "title": "DiffuGen: Adaptable Approach for Generating Labeled Image Datasets using Stable Diffusion Models",
        "firstAuthor": "Michael Shenoda",
        "url": "https://arxiv.org/pdf/2309.00248",
        "dateSubmitted": "2023-09-01",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Generating high-quality labeled image datasets is crucial for training accurate and robust machine learning models in the field of computer vision. However, the process of manually labeling real images is often time-consuming and costly. To address these challenges associated with dataset generation, we introduce\"DiffuGen,\"a simple and adaptable approach that harnesses the power of stable diffusion models to create labeled image datasets efficiently. By leveraging stable diffusion models, our approach not only ensures the quality of generated datasets but also provides a versatile solution for label generation. In this paper, we present the methodology behind DiffuGen, which combines the capabilities of diffusion models with two distinct labeling techniques: unsupervised and supervised. Distinctively, DiffuGen employs prompt templating for adaptable image generation and textual inversion to enhance diffusion model capabilities.",
        "paperId": "6c1a53c05f1b1a024af740df84e530d79400ab86"
    },
    {
        "title": "LLM-FuncMapper: Function Identification for Interpreting Complex Clauses in Building Codes via LLM",
        "firstAuthor": "Zhe Zheng",
        "url": "https://arxiv.org/pdf/2308.08728",
        "dateSubmitted": "2023-08-17",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "As a vital stage of automated rule checking (ARC), rule interpretation of regulatory texts requires considerable effort. However, interpreting regulatory clauses with implicit properties or complex computational logic is still challenging due to the lack of domain knowledge and limited expressibility of conventional logic representations. Thus, LLM-FuncMapper, an approach to identifying predefined functions needed to interpret various regulatory clauses based on the large language model (LLM), is proposed. First, by systematically analysis of building codes, a series of atomic functions are defined to capture shared computational logics of implicit properties and complex constraints, creating a database of common blocks for interpreting regulatory clauses. Then, a prompt template with the chain of thought is developed and further enhanced with a classification-based tuning strategy, to enable common LLMs for effective function identification. Finally, the proposed approach is validated with statistical analysis, experiments, and proof of concept. Statistical analysis reveals a long-tail distribution and high expressibility of the developed function database, with which almost 100% of computer-processible clauses can be interpreted and represented as computer-executable codes. Experiments show that LLM-FuncMapper achieve promising results in identifying relevant predefined functions for rule interpretation. Further proof of concept in automated rule interpretation also demonstrates the possibility of LLM-FuncMapper in interpreting complex regulatory clauses. To the best of our knowledge, this study is the first attempt to introduce LLM for understanding and interpreting complex regulatory clauses, which may shed light on further adoption of LLM in the construction domain.",
        "paperId": "6c4d35d67f843e7de6ec00c088e339b2237d222c"
    },
    {
        "title": "SAKP: A Korean Sentiment Analysis Model via Knowledge Base and Prompt Tuning",
        "firstAuthor": "Haiqiang Wen",
        "url": null,
        "dateSubmitted": "2023-05-26",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "With the help of pre-trained language models, tasks such as sentiment analysis and text classification have achieved good results. With the advent of prompt tuning, especially previous studies have shown that in the case of few data, the prompt tuning method has significant advantages over the general tuning method of additional classifiers. At present, there are relatively few studies on sentiment analysis of Korean Chinese texts.This paper proposes a low resource sentiment classification method based on pre-trained language models (PLMs) combined with prompt tuning. In this work, we chose to use the pre-trained language model KLUE and elaborated a Korean prompt template with an expanded knowledge base and filtering in the verbalizer section. We focus on collecting external knowledge and integrating it into the utterance to form a prompt tuning of knowledge to improve and stabilize the prompt tuning. Specifically, we use the K-means clustering algorithm to construct the label wordspace of the external knowledge base (kb) extended language, and use PLM itself to refine the extended labeled wordspace before using the extended labeled wordspace for prediction. A large number of experiments on the few-shot emotion classification task prove the effectiveness of knowledge prompt tuning.",
        "paperId": "6efdc83f69530169c87779cad8d2f72499b57ce4"
    },
    {
        "title": "FashionSAP: Symbols and Attributes Prompt for Fine-Grained Fashion Vision-Language Pre-Training",
        "firstAuthor": "Yunpeng Han",
        "url": "https://arxiv.org/pdf/2304.05051",
        "dateSubmitted": "2023-04-11",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Fashion vision-language pre-training models have shown efficacy for a wide range of downstream tasks. However, general vision-language pre-training models pay less attention to fine-grained domain features, while these features are important in distinguishing the specific domain tasks from general tasks. We propose a method for fine-grained fashion vision-language pre-training based on fashion Symbols and Attributes Prompt (FashionSAP) to model fine-grained multi-modalities fashion attributes and characteristics. Firstly, we propose the fashion symbols, a novel abstract fashion concept layer, to represent different fashion items and to generalize various kinds of fine- grained fashion features, making modelling fine-grained attributes more effective. Secondly, the attributes prompt method is proposed to make the model learn specific attributes of fashion items explicitly. We design proper prompt templates according to the format of fashion data. Comprehensive experiments are conducted on two public fashion benchmarks, i.e., FashionGen and FashionIQ, and FashionSAP gets SOTA performances for four popular fashion tasks. The ablation study also shows the proposed abstract fashion symbols, and the attribute prompt method enables the model to acquire fine-grained semantics in the fashion domain effectively. The obvious performance gains from FashionSAP provide a new baseline for future fashion task research.11The source code is available at https://github.com/hssip/FashionSAP",
        "paperId": "6f05be4a0045cee3575fb39e88fc361d96f2cc4f"
    },
    {
        "title": "Knowledge Base Construction from Pre-trained Language Models by Prompt learning",
        "firstAuthor": "Xiaomin Ning",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Pre-trained language models (LMs) have advanced the state-of-the-art for many semantic tasks and have also been proven effective for extracting knowledge from the models itself. Although several works have explored the capability of the LMs for constructing knowledge bases, including prompt learning, this potential has not yet been fully explored. In this work, we propose a method of extracting factual knowledge from LMs for given subject-relation pairs and explore the most effective strategy to generate blank object entities for each relation of triples. We design prompt templates for each relation using personal knowledge and the descriptive information available on the web such as WikiData. The probing approach of our proposed LMs is tested on the dataset provided by the International Semantic Web Conference (ISWC 2022) LM-KBC Challenge. To cope with the problem of varying performance for each relation, we designed a parameter selection strategy for each relation. Using the test dataset, we obtain an F1-score of 0.4935%, which is higher than the baseline of 31.08%.",
        "paperId": "6f0e7f51f0ceb317061a803ae5d5eb79ef668b4c"
    },
    {
        "title": "Making Pre-trained Language Models Better Learn Few-Shot Spoken Language Understanding in More Practical Scenarios",
        "firstAuthor": "Yufan Wang",
        "url": "https://aclanthology.org/2023.findings-acl.853.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Most previous few-shot Spoken Language Understanding (SLU) models typically need to be trained on a set of data-rich source domains and adapt to the target domain with a few examples. In this paper, we explore a more practical scenario for few-shot SLU, in which we only assume access to a pre-trained language model and a few labeled examples without any other source domain data. We concentrate on understanding how far the few-shot SLU could be pushed in this setting. To this end, we develop a prompt-based intent detection model in few-shot settings, which leverages the BERT original pre-training next sentence prediction task and the prompt template to detect the user\u2019s intent. For slot filling, we propose an approach of reconstructing slot labels, which reduces the training complexity by reducing the number of slot labels in few-shot settings. To evaluate the few-shot SLU for a more practical scenario, we present two benchmarks, FewShotATIS and FewShotSNIPS. And a dynamic sampling strategy is designed to construct the two datasets according to the learning difficulty of each intent and slot. Experiments on FewShotATIS and FewShotSNIPS demonstrate that our proposed model achieves state-of-the-art performance.",
        "paperId": "6f3361fc0aa0be6a03d4f4af576d7bb056d7f905"
    },
    {
        "title": "UPER: Boosting Multi-Document Summarization with an Unsupervised Prompt-based Extractor",
        "firstAuthor": "Shangqing Tu",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Multi-Document Summarization (MDS) commonly employs the 2-stage extract-then-abstract paradigm, which first extracts a relatively short meta-document, then feeds it into the deep neural networks to generate an abstract. Previous work usually takes the ROUGE score as the label for training a scoring model to evaluate source documents. However, the trained scoring model is prone to under-fitting for low-resource settings, as it relies on the training data. To extract documents effectively, we construct prompting templates that invoke the underlying knowledge in Pre-trained Language Model (PLM) to calculate the document and keyword\u2019s perplexity, which can assess the document\u2019s semantic salience. Our unsupervised approach can be applied as a plug-in to boost other metrics for evaluating a document\u2019s salience, thus improving the subsequent abstract generation. We get positive results on 2 MDS datasets, 2 data settings, and 2 abstractive backbone models, showing our method\u2019s effectiveness. Our code is available at https://github.com/THU-KEG/UPER",
        "paperId": "6f785623450c17f4d4089dd812bc0de8bcfbb55c"
    },
    {
        "title": "Constraint-based Paraphrase Generation Model and it's Application on Message Extraction of Commodity Futures News",
        "firstAuthor": "Yintao Jia",
        "url": null,
        "dateSubmitted": "2022-10-27",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "The task of message extraction based on the Chinese futures domain is to extract the messages that may affect the futures market from the futures news or comment texts. In this work, we construct a Futures Messages Extraction Dataset (FMED) by extracting message-related entities from the Corpus for Entity and Relationship in Futures domain (CERF) and propose a message extraction model MGCC based on paraphrase generation. We define prompt templates for this task and use the pre-trained model mT5. In order to improve the model's performance, the constraint and the classifier are used to generate the target sentences. The final precision, recall and F1 values in the validation set are 0.732, 0.733 and 0.732, respectively.",
        "paperId": "713e2d5a1c880addd832545d121707de191dd1eb"
    },
    {
        "title": "Cross-Domain Reasoning via Template Filling",
        "firstAuthor": "Dheeraj Rajagopal",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "In this paper, we explore the ability of sequence to sequence models to perform cross-domain reasoning. Towards this, we present a prompt-template-\ufb01lling approach to enable sequence to sequence models to perform cross-domain reasoning. We also present a case-study with commonsense and health and well-being domains, where we study how prompt-template-\ufb01lling enables pretrained sequence to sequence models across domains. Our experiments across several pretrained encoder-decoder models show that cross-domain reasoning is challenging for current models. We also show an in-depth error analysis and avenues for future research for reasoning across domains 1 .",
        "paperId": "7393d2618c7478d937112865458862e8d5f10475"
    },
    {
        "title": "Can Visual Linguistic Models become Knowledge Bases: A Prompt Learning Framework for Size Perception",
        "firstAuthor": "Yi Zhang",
        "url": null,
        "dateSubmitted": "2022-12-08",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Empowering machines to understand our physical world should go beyond models with only natural language and models with only vision. Vision and language comprise a growing field of study that attempts to bridge the gap between natural language processing and computer vision communities by enabling models to learn visually grounded language. However, as an increasing number of pre-trained visual linguistic models focus on the alignment between visual regions and natural language, it is difficult to claim that these models capture certain properties of objects in their latent space, such as size. Inspired by recent trends in prompt learning, this study designed a prompt learning framework for two visual linguistic models, ViLBERT and ViLT, and used manually crafted prompt templates to evaluate the consistency of performance of these models in comparing the size of objects.",
        "paperId": "73982f448227113c27ec38cd77652ea395fc6953"
    },
    {
        "title": "RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction",
        "firstAuthor": "Yew Ken Chia",
        "url": "http://arxiv.org/pdf/2203.09101",
        "dateSubmitted": "2022-03-17",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve ZeroRTE, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (RelationPrompt). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot relation classification. Our code and data are available at github.com/declare-lab/RelationPrompt.",
        "paperId": "743dcf234cffd54c4e096a10a284dd81572b16ea"
    },
    {
        "title": "The utility of ChatGPT for cancer treatment information",
        "firstAuthor": "Shan Chen",
        "url": "https://www.medrxiv.org/content/medrxiv/early/2023/03/23/2023.03.16.23287316.full.pdf",
        "dateSubmitted": "2023-03-23",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "The use of large language models (LLMs) such as ChatGPT for medical question-answering is becoming increasingly popular. However, there are concerns that these models may generate and amplify medical misinformation. Because cancer patients frequently seek to educate themselves through online resources, some individuals will likely use ChatGPT to obtain cancer treatment information. This study evaluated the performance and robustness of ChatGPT in providing breast, prostate, and lung cancer treatment recommendations that align with National Comprehensive Cancer Network (NCCN) guidelines. Four prompt templates were created to explore how differences in how the query is posed impacts response. ChatGPT output was scored by 3 oncologists and a 4th oncologist adjudicated in cases of disagreement. ChatGPT provided at least one NCCN-concordant recommendation for 102/104 (98%) prompts. However, 35/102 (34.3%) of these also included a recommendation that was at least partially non-concordant with NCCN guidelines. Responses varied based on prompt type. In conclusion, ChatGPT did not perform well at reliably and robustly providing cancer treatment recommendations. Patients and clinicians should be aware of the limitations of ChatGPT and similar technologies for self-education.",
        "paperId": "763d953e671e2b6c6d0df2f5bc5472fb6ce074de"
    },
    {
        "title": "The Prompt Artists",
        "firstAuthor": "Minsuk Chang",
        "url": null,
        "dateSubmitted": "2023-03-22",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "This paper examines the art practices, artwork, and motivations of prolific users of the latest generation of text-to-image models. Through interviews, observations, and a user survey, we present a sampling of the artistic styles and describe the developed community of practice around generative AI. We find that: 1) artists hold the text prompt and the resulting image can be considered collectively as a form of artistic expression (prompts as art), and 2) prompt templates (prompts with \u201cslots\u201d for others to fill in with their own words) are developed to create generative art styles. We discover that the value placed by this community on unique outputs leads to artists seeking specialized vocabulary to produce distinctive art pieces (e.g., by reading architectural blogs to find phrases to describe images). We also find that some artists use \u201cglitches\u201d in the model that can be turned into artistic styles of their own right. From these findings, we outline specific implications for design regarding future prompting and image editing options.",
        "paperId": "7772380e6c6501c522974302389056a9c9320bf0"
    },
    {
        "title": "Investigating the Applicability of Self-Assessment Tests for Personality Measurement of Large Language Models",
        "firstAuthor": "Akshat Gupta",
        "url": "https://arxiv.org/pdf/2309.08163",
        "dateSubmitted": "2023-09-15",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "As large language models (LLM) evolve in their capabilities, various recent studies have tried to quantify their behavior using psychological tools created to study human behavior. One such example is the measurement of\"personality\"of LLMs using personality self-assessment tests. In this paper, we take three such studies on personality measurement of LLMs that use personality self-assessment tests created to study human behavior. We use the prompts used in these three different papers to measure the personality of the same LLM. We find that all three prompts lead very different personality scores. This simple test reveals that personality self-assessment scores in LLMs depend on the subjective choice of the prompter. Since we don't know the ground truth value of personality scores for LLMs as there is no correct answer to such questions, there's no way of claiming if one prompt is more or less correct than the other. We then introduce the property of option order symmetry for personality measurement of LLMs. Since most of the self-assessment tests exist in the form of multiple choice question (MCQ) questions, we argue that the scores should also be robust to not just the prompt template but also the order in which the options are presented. This test unsurprisingly reveals that the answers to the self-assessment tests are not robust to the order of the options. These simple tests, done on ChatGPT and Llama2 models show that self-assessment personality tests created for humans are not appropriate for measuring personality in LLMs.",
        "paperId": "781f4f7dd871c0eea0ce71692bcbc1283df6b550"
    },
    {
        "title": "PromptCL: Improving Event Representation via Prompt Template and Contrastive Learning",
        "firstAuthor": "Yubo Feng",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": null,
        "paperId": "7920be539d98e8154471e9943eb2e50a19054bc8"
    },
    {
        "title": "InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists",
        "firstAuthor": "Yulu Gan",
        "url": "https://arxiv.org/pdf/2310.00390",
        "dateSubmitted": "2023-09-30",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Recent advances in generative diffusion models have enabled text-controlled synthesis of realistic and diverse images with impressive quality. Despite these remarkable advances, the application of text-to-image generative models in computer vision for standard visual recognition tasks remains limited. The current de facto approach for these tasks is to design model architectures and loss functions that are tailored to the task at hand. In this paper, we develop a unified language interface for computer vision tasks that abstracts away task-specific design choices and enables task execution by following natural language instructions. Our approach involves casting multiple computer vision tasks as text-to-image generation problems. Here, the text represents an instruction describing the task, and the resulting image is a visually-encoded task output. To train our model, we pool commonly-used computer vision datasets covering a range of tasks, including segmentation, object detection, depth estimation, and classification. We then use a large language model to paraphrase prompt templates that convey the specific tasks to be conducted on each image, and through this process, we create a multi-modal and multi-task training dataset comprising input and output images along with annotated instructions. Following the InstructPix2Pix architecture, we apply instruction-tuning to a text-to-image diffusion model using our constructed dataset, steering its functionality from a generative model to an instruction-guided multi-task vision learner. Experiments demonstrate that our model, dubbed InstructCV, performs competitively compared to other generalist and task-specific vision models. Moreover, it exhibits compelling generalization capabilities to unseen data, categories, and user instructions.",
        "paperId": "819f477065088220a6f706cd9ef76dbcb4b4c134"
    },
    {
        "title": "PromptRGD: Prompt Learning with Relation-aware Gradient Denoising for Low-resource Relation Extraction",
        "firstAuthor": "Zihang Xu",
        "url": null,
        "dateSubmitted": "2022-12-17",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Relation extraction is a fundamental task to construct a knowledge graph, which aims to find the relation categories between two entities from a string of text input sequences. Considering the fact that high-quality labeled data is rare, a recent trend is low-resource setting. Existing works either utilize prompt-based learning method to achieve encouraging results for few-shot tasks by determining an appropriate prompt template, or leverage unlabeled data by generating pseudo labels. However, the manual design of the prompt template requires not only labor-intensive but also expert knowledge. Roughly utilizing a semi-supervised method to assign pseudo-label to unlabeled data will suffer from noise accumulation. To tackle these problems, we propose a novel semi-supervised prompt learning framework with relation-aware gradient denoising for low-resource relation extraction(PromptRGD). Firstly, using learnable template words and virtual labels for prompt learning, we introduce entity type and relation prior knowledge into prompt template construction. Secondly, given the relation-aware gradient similarity between labeled and unlabeled data, PromptRGD generates a pseudo label and then improves the quality of pseudo labels on unlabeled samples in a self-training fashion. The main experimental results and a series of analyses prove the effectiveness of PromptRGD.",
        "paperId": "824a6a995136ad7d703ef1afb7ee12d930bbe43b"
    },
    {
        "title": "Prompt Learning for Developing Software Exploits",
        "firstAuthor": "Xiaoming Ruan",
        "url": null,
        "dateSubmitted": "2023-08-04",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "A software exploit is a sequence of commands that exploits software vulnerabilities or security flaws, written either by security researchers as a Proof-Of-Concept (POC) threat or by malicious attackers for use in their operations. Writing exploits is a challenging task since it is time-consuming and costly. Pre-trained Language Models (PLMs) for code can benefit automatic exploit generation, achieving state-of-the-art performance. However, the typical paradigm for using these PLMs is fine-tuning, which leads to a significant gap between the language model pre-training and the target task fine-tuning process since they are in different forms. In this paper, we propose a prompt learning approach PT4Exploits based on the PLM, i.e., CodeT5 to automatically generate desired exploits by modifying the original English description inputs. More specifically, PT4Exploits can better elicit knowledge from the PLM by inserting trainable prompt tokens into the original input to construct the same form as the pre-training tasks. Experimental results show that our approach can significantly outperform baseline models on both automatic evaluation and human evaluation. Additionally, we conduct extensive experiments to investigate the performance of prompt learning on few-shot settings and the scenario of different prompt templates, which also show the competitive effectiveness of PT4Exploits.",
        "paperId": "84aeac4db19c56619a6929e2ee758f0150abf45c"
    },
    {
        "title": "Exploring Prompts in Few-Shot Cross-Linguistic Topic Classification Scenarios",
        "firstAuthor": "Zhipeng Zhang",
        "url": "https://www.mdpi.com/2076-3417/13/17/9944/pdf?version=1693814326",
        "dateSubmitted": "2023-09-02",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "In recent years, large-scale pretrained language models have become widely used in natural language processing tasks. On this basis, prompt learning has achieved excellent performance in specific few-shot classification scenarios. The core idea of prompt learning is to convert a downstream task into a masked language modelling task. However, different prompt templates can greatly affect the results, and finding an appropriate template is difficult and time-consuming. To this end, this study proposes a novel hybrid prompt approach, which combines discrete prompts and continuous prompts, to motivate the model to learn more semantic knowledge from a small number of training samples. By comparing the performance difference between discrete prompts and continuous prompts, we find that hybrid prompts achieve the best results, reaching a 73.82% F1 value in the test set. In addition, we analyze the effect of different virtual token lengths in continuous prompts and hybrid prompts in a few-shot cross-language topic classification scenario. The results demonstrate that there is a threshold for the length of virtual tokens, and too many virtual tokens decrease the performance of the model. It is better not to exceed the average length of the training set corpus. Finally, this paper designs a method based on vector similarity to explore the real meanings represented by virtual tokens. The experimental results show that the prompt automatically learnt from the virtual token has a certain correlation with the input text.",
        "paperId": "84b19d670228b024491127316072bcf5c0fd9ccb"
    },
    {
        "title": "Prompt-Learning for Cross-Lingual Relation Extraction",
        "firstAuthor": "Chiaming Hsu",
        "url": "https://arxiv.org/pdf/2304.10354",
        "dateSubmitted": "2023-04-20",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Relation Extraction (RE) is a crucial task in Information Extraction, which entails predicting relationships between entities within a given sentence. However, extending pre-trained RE models to other languages is challenging, particularly in real-world scenarios where Cross-Lingual Relation Extraction (XRE) is required. Despite recent advancements in Prompt-Learning, which involves transferring knowledge from Multilingual Pre-trained Language Models (PLMs) to diverse downstream tasks, there is limited research on the effective use of multilingual PLMs with prompts to improve XRE. In this paper, we present a novel XRE algorithm based on Prompt-Tuning, referred to as Prompt-Xre. To evaluate its effectiveness, we design and implement several prompt templates, including hard, soft, and hybrid prompts, and empirically test their performance on competitive multilingual PLMs, specifically mBART. Our extensive experiments, conducted on the low-resource ACE05 benchmark across multiple languages, demonstrate that our Prompt-Xre algorithm significantly outperforms both vanilla multilingual PLMs and other existing models, achieving state-of-the-art performance in XRE. To further show the generalization of our Prompt-XRE on larger data scales, we construct and release a new XRE dataset-WMTI7-EnZh XRE, containing 0.9M English-Chinese pairs extracted from WMT 2017 parallel corpus. Experiments on WMTI7-EnZh XRE also show the effectiveness of our Prompt-XRE against other competitive baselines. The code and newly constructed dataset are freely available at httus://2ithub.com/HSU-CHIA-MING/Promut-XRE.",
        "paperId": "850b8f31a1bb762544bd35163923784a664b315a"
    },
    {
        "title": "KUL@SMM4H\u201922: Template Augmented Adaptive Pre-training for Tweet Classification",
        "firstAuthor": "Sumam Francis",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "This paper describes models developed for the Social Media Mining for Health (SMM4H) 2022 shared tasks. Our team participated in the first subtask that classifies tweets with Adverse Drug Effect (ADE) mentions. Our best-performing model comprises of a template augmented task adaptive pre-training and further fine-tuning on target task data. Augmentation with random prompt templates increases the amount of task-specific data to generalize the LM to the target task domain. We explore 2 pre-training strategies: Masked language modeling (MLM) and Simple contrastive pre-training (SimSCE) and the impact of adding template augmentations with these pre-training strategies. Our system achieves an F1 score of 0.433 on the test set without using supplementary resources and medical dictionaries.",
        "paperId": "884bc45aab66eb6b36b967ca09ff88b57285c215"
    },
    {
        "title": "Supervision Is Not Education: The Dark Side of Remote Access to the Electronic Health Record.",
        "firstAuthor": "M. Fuglestad",
        "url": "https://www.jgme.org/doi/pdf/10.4300/JGME-D-17-00737.1",
        "dateSubmitted": "2017-12-01",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "T he Chinese symbol of yin and yang is a wellknown example of how opposite or contrary forces can be complementary and interdependent. There is probably no more polarizing topic than the recent introduction of the electronic health record (EHR) into medical education. Just as there are many potential benefits of the EHR, there are as many negative impacts. The EHR is a tool, and like all tools, it is neutral. The way in which the tool is used, however, determines whether the impact is positive or negative. In this issue of the Journal of Graduate Medical Education, Martin and colleagues have provided us with the first description of remote EHR utilization patterns in attending physicians. Additionally, they have brought to light a novel method of resident oversight that could be integrated into residency training. We applaud the authors for another wellexecuted investigation into the educational impact of the EHR. The current study, which demonstrates potential positive sides to remote access, also provides a balanced opportunity to consider some of the dark sides. The authors found that regardless of the number of years in practice, there is general acceptance of remote access to the EHR. This has the potential benefit of providing immediate patient care regardless of the physician\u2019s location. Nevertheless, this benefit needs to be balanced against the risks of making incorrect decisions based only on electronic data. Of note, a previous study, also conducted at the University of Chicago, found that residents extensively electronically research patients prior to actually examining them. This practice may inappropriately influence clinical decision-making and amplify clinical errors. An advantage of the EHR would be the immediate ability to take corrective actions or educate the residents, regardless of time of day. Unfortunately, Martin and colleagues found that the majority (75%) of EHR remote access was postcall, as compared with 54% while on call. This finding suggests that remote access is more frequently utilized for attending physician convenience in completing documentation, rather than providing immediate patient care and resident education. Finally, is there potential harm to the physicians who utilize remote EHR access? Martin and colleagues found that 75% of attending physicians remotely use the EHR for 60 to 90 minutes every day. Concern has previously been expressed that remote access to the EHR may contribute to physician burnout and negatively affect the well-being of the physician and the physician\u2019s family. It is important for the readers of Martin and colleagues\u2019 article to recognize the difference between supervision and education because it is easy to confuse them. Supervision involves the critical watching or directing of subordinates, whereas education is the process of imparting knowledge. The authors demonstrate this difference with the finding that 93% of attending physicians remotely discovered incorrect information provided by the residents, which resulted in changes in clinical decisions; yet only 50% of attending physicians recalled instances where these findings resulted in direct supervisory action from home in the form of telephone calls to cross-cover services. These 2 data points suggest that the previous dependency between the attending (teacher) and the resident (learner) is being lost. Before the implementation of the EHR, residents and attending physicians were dependent on each other. Residents were data collectors and were forced to distill what was most important. Likewise, attendings were forced to interact with residents to obtain the information. In this way, the resident had the opportunity to gain cognitive reinforcement between the raw data and the attending physician\u2019s decision-making. Unfortunately, the auto-filled data, prompts, templates, and copyand-paste functions of the EHR have nearly eliminated the requirement for residents and attending physicians to interact. In a theoretical model of remote EHR-based education, attending physicians would review residents\u2019 notes to understand their thought processes. After independently reviewing patient data, faculty could remotely evaluate at multiple points in time DOI: http://dx.doi.org/10.4300/JGME-D-17-00737.1",
        "paperId": "8b0b7541e2d55252ffeb3b99604001a5f809d28e"
    },
    {
        "title": "Large Language and Text-to-3D Models for Engineering Design Optimization",
        "firstAuthor": "Thiago Rios",
        "url": "https://arxiv.org/pdf/2307.01230",
        "dateSubmitted": "2023-07-03",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "The current advances in generative AI for learning large neural network models with the capability to produce essays, images, music and even 3D assets from text prompts create opportunities for a manifold of disciplines. In the present paper, we study the potential of deep text-to-3D models in the engineering domain, with focus on the chances and challenges when integrating and interacting with 3D assets in computational simulation-based design optimization. In contrast to traditional design optimization of 3D geometries that often searches for the optimum designs using numerical representations, such as B-Spline surface or deformation parameters in vehicle aerodynamic optimization, natural language challenges the optimization framework by requiring a different interpretation of variation operators while at the same time may ease and motivate the human user interaction. Here, we propose and realize a fully automated evolutionary design optimization framework using Shap-E, a recently published text-to-3D asset network by OpenAI, in the context of aerodynamic vehicle optimization. For representing text prompts in the evolutionary optimization, we evaluate (a) a bag-of-words approach based on prompt templates and Wordnet samples, and (b) a tokenisation approach based on prompt templates and the byte pair encoding method from GPT4. Our main findings from the optimizations indicate that, first, it is important to ensure that the designs generated from prompts are within the object class of application, i.e. diverse and novel designs need to be realistic, and, second, that more research is required to develop methods where the strength of text prompt variations and the resulting variations of the 3D designs share causal relations to some degree to improve the optimization.",
        "paperId": "8c2dbf98b75a01f7e93b68a9407f00b1728b66af"
    },
    {
        "title": "The Biases of Pre-Trained Language Models: An Empirical Study on Prompt-Based Sentiment Analysis and Emotion Detection",
        "firstAuthor": "Rui Mao",
        "url": null,
        "dateSubmitted": "2023-07-01",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Thanks to the breakthrough of large-scale pre-trained language model (PLM) technology, prompt-based classification tasks, e.g., sentiment analysis and emotion detection, have raised increasing attention. Such tasks are formalized as masked language prediction tasks which are in line with the pre-training objects of most language models. Thus, one can use a PLM to infer the masked words in a downstream task, then obtaining label predictions with manually defined label-word mapping templates. Prompt-based affective computing takes the advantages of both neural network modeling and explainable symbolic representations. However, there still remain many unclear issues related to the mechanisms of PLMs and prompt-based classification. We conduct a systematic empirical study on prompt-based sentiment analysis and emotion detection to study the biases of PLMs towards affective computing. We find that PLMs are biased in sentiment analysis and emotion detection tasks with respect to the number of label classes, emotional label-word selections, prompt templates and positions, and the word forms of emotion lexicons.",
        "paperId": "8eb9a8d756e93530eb35e9f0e26a2a0190c1dd7c"
    },
    {
        "title": "TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition",
        "firstAuthor": "Wei Xiang",
        "url": "http://arxiv.org/pdf/2305.10866",
        "dateSubmitted": "2023-05-18",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Implicit Discourse Relation Recognition (IDRR) aims at classifying the relation sense between two arguments without an explicit connective. Recently, the ConnPrompt~\\cite{Wei.X:et.al:2022:COLING} has leveraged the powerful prompt learning for IDRR based on the fusion of multi-prompt decisions from three different yet much similar connective prediction templates. Instead of multi-prompt ensembling, we propose to design auxiliary tasks with enlightened prompt learning for the IDRR task. Although an auxiliary task is not used to directly output final prediction, we argue that during the joint training some of its learned features can be useful to boost the main task. In light of such motivations, we propose a task enlightenment prompt learning model, called TEPrompt, to fuse learned features from three related tasks for IDRR. In particular, the TEPrompt contains three tasks, viz., Discourse Relation Recognition (DRR), Sense Semantics Classification (SSC) and Annotated Connective Prediction (ACP), each with a unique prompt template and an answer space. In the training phase, we jointly train three prompt learning tasks with shared argument representation. In the testing phase, we only take the DRR output with fused features as the final IDRR decision. Experiments with the same conditions have shown that the proposed TEPrompt outperforms the ConnPrompt. This can be attributed to the promoted decision features and language models benefited from joint-training of auxiliary tasks.",
        "paperId": "8eeb6cf85e6bf305fb761a6e6a22de20f09909de"
    },
    {
        "title": "Revisit Input Perturbation Problems for LLMs: A Unified Robustness Evaluation Framework for Noisy Slot Filling Task",
        "firstAuthor": "Guanting Dong",
        "url": null,
        "dateSubmitted": "2023-10-10",
        "keyWords": [
            "prompt template"
        ],
        "abstract": null,
        "paperId": "90022c80ea85a41d8d1a7765fd95824bf3a9830f"
    },
    {
        "title": "Supplementary material for Mask-free OVIS: Open-Vocabulary Instance Segmentation without Manual Mask Annotations",
        "firstAuthor": "VS Vibashan",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "COCO Caption vs Image-label pseudo-caption: Pseudo-caption generation: Since the pre-trained vision-language models are trained on full sentences, we need to feed the image-labels into a prompt template first, and use them to generate a pseudo-captions. Specifically, given image-labels [category-1,category-2...,category-n], we randomly sample a prompt from 63 prompt templates [1,6] and the pseudo-caption are generated as \u201d{Prompt-x} + {category-1 and category-2 and ... category-n}\u201d. For example, as shown in Fig. 1 bottom row the sampled prompts are \u201dA black and white photo of the {category}.\u201d and \u201dA photo of {category} in the scene.\u201d and the image-labels are \u201dzebra\u201d and \u201dgiraffe\u201d. Thus, the generated pseudo-captions are \u201dA black and white photo of the zebra and giraffe.\u201d and \u201dA photo of one zebra and giraffe in the scene.\u201d",
        "paperId": "907befcc0f2eb7a22a91f1a2829d0b823d340e4f"
    },
    {
        "title": "IIE-NLP-NUT at SemEval-2020 Task 4: Guiding PLM with Prompt Template Reconstruction Strategy for ComVE",
        "firstAuthor": "Luxi Xing",
        "url": "https://aclanthology.org/2020.semeval-1.42.pdf",
        "dateSubmitted": "2020-07-02",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "This paper introduces our systems for the first two subtasks of SemEval Task4: Commonsense Validation and Explanation. To clarify the intention for judgment and inject contrastive information for selection, we propose the input reconstruction strategy with prompt templates. Specifically, we formalize the subtasks into the multiple-choice question answering format and construct the input with the prompt templates, then, the final prediction of question answering is considered as the result of subtasks. Experimental results show that our approaches achieve significant performance compared with the baseline systems. Our approaches secure the third rank on both official test sets of the first two subtasks with an accuracy of 96.4 and an accuracy of 94.3 respectively.",
        "paperId": "94db2ba208a3ab2e469a5a65d6192f4dd04ef0bf"
    },
    {
        "title": "GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks",
        "firstAuthor": "Shivanshu Gupta",
        "url": null,
        "dateSubmitted": "2023-11-16",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Large language models (LLMs) have the ability to perform in-context learning (ICL) of new tasks by conditioning on prompts comprising a few task examples. This work studies the problem of selecting the best examples given a candidate pool to improve ICL performance on given a test input. Existing approaches either require training with feedback from a much larger LLM or are computationally expensive. We propose a novel metric, GistScore, based on Example Gisting, a novel approach for training example retrievers for ICL using an attention bottleneck via Gisting, a recent technique for compressing task instructions. To tradeoff performance with ease of use, we experiment with both fine-tuning gist models on each dataset and multi-task training a single model on a large collection of datasets. On 21 diverse datasets spanning 9 tasks, we show that our fine-tuned models get state-of-the-art ICL performance with 20% absolute average gain over off-the-shelf retrievers and 7% over the best prior methods. Our multi-task model generalizes well out-of-the-box to new task categories, datasets, and prompt templates with retrieval speeds that are consistently thousands of times faster than the best prior training-free method.",
        "paperId": "954b3be7d3800f9c3a78f38f5d6ae841bb1dc957"
    },
    {
        "title": "ALERT: Adapt Language Models to Reasoning Tasks",
        "firstAuthor": "Ping Yu",
        "url": "https://arxiv.org/pdf/2212.08286",
        "dateSubmitted": "2022-12-16",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context.To address this question, we introduce {pasted macro \u2018OUR\u2019}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. {pasted macro \u2018OUR\u2019}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using {pasted macro \u2018OUR\u2019}model we further investigate the role of finetuning. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.",
        "paperId": "95c11cc5820ba32c60d5f2671f6567b9914a4978"
    },
    {
        "title": "Advanced prompting as a catalyst: Empowering large language models in the management of gastrointestinal cancers",
        "firstAuthor": "J. Yuan",
        "url": "https://www.the-innovation.org/data/article/export-pdf?id=64db4fd54228a72545780714",
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Large Language Models' (LLMs) performance in healthcare can be significantly impacted by prompt engineering. However, the area of study remains relatively uncharted in gastrointestinal oncology until now. Our research delves into this unexplored territory, investigating the efficacy of varied prompting strategies, including simple prompts, templated prompts, in-context learning (ICL), and multi-round iterative questioning, for optimizing the performance of LLMs within a medical setting. We develop a comprehensive evaluation system to assess the performance of LLMs across multiple dimensions. This robust evaluation system ensures a thorough assessment of the LLMs' capabilities in the field of medicine. Our findings suggest a positive relationship between the comprehensiveness of the prompts and the LLMs' performance. Notably, the multi-round strategy, which is characterized by iterative question-and-answer rounds, consistently yields the best results. ICL, a strategy that capitalizes on interrelated contextual learning, also displays significant promise, surpassing the outcomes achieved with simpler prompts. The research underscores the potential of advanced prompt engineering and iterative learning approaches for boosting the applicability of LLMs in healthcare. We recommend that additional research be conducted to refine these strategies and investigate their potential integration, to truly harness the full potential of LLMs in medical applications.\n",
        "paperId": "995b2f650f55de6077b87db6dadb01cecd86dbd7"
    },
    {
        "title": "AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models",
        "firstAuthor": "J. H. Metzen",
        "url": "https://arxiv.org/pdf/2309.16414",
        "dateSubmitted": "2023-09-28",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Classifiers built upon vision-language models such as CLIP have shown remarkable zero-shot performance across a broad range of image classification tasks. Prior work has studied different ways of automatically creating descriptor sets for every class based on prompt templates, ranging from manually engineered templates over templates obtained from a large language model to templates built from random words and characters. Up until now, deriving zero-shot classifiers from the respective encoded class descriptors has remained nearly unchanged, i.e., classify to the class that maximizes cosine similarity between its averaged encoded class descriptors and the image encoding. However, weighing all class descriptors equally can be suboptimal when certain descriptors match visual clues on a given image better than others. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot classifiers. AutoCLIP tunes per-image weights to each prompt template at inference time, based on statistics of class descriptor-image similarities. AutoCLIP is fully unsupervised, has very low computational overhead, and can be easily implemented in few lines of code. We show that AutoCLIP outperforms baselines across a broad range of vision-language models, datasets, and prompt templates consistently and by up to 3 percent point accuracy.",
        "paperId": "99bd3e04b6b65abf3f03de69654059c3710d03e8"
    },
    {
        "title": "Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners",
        "firstAuthor": "Ningyu Zhang",
        "url": null,
        "dateSubmitted": "2021-08-30",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Large-scale pre-trained language models have contributed significantly to natural language processing by demonstrating remarkable abilities as few-shot learners. However, their effectiveness depends mainly on scaling the model parameters and prompt design, hindering their implementation in most real-world applications. This study proposes a novel pluggable, extensible, and efficient approach named DifferentiAble pRompT (DART), which can convert small language models into better few-shot learners without any prompt engineering. The main principle behind this approach involves reformulating potential natural language processing tasks into the task of a pre-trained language model and differentially optimizing the prompt template as well as the target label with backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any pre-trained language models; (ii) Extended to widespread classification tasks. A comprehensive evaluation of standard NLP tasks demonstrates that the proposed approach achieves a better few-shot performance. Code is available in https://github.com/zjunlp/DART.",
        "paperId": "9b56086e420ecb216f85d408a25264f640e46705"
    },
    {
        "title": "TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models",
        "firstAuthor": "Yue Huang",
        "url": "http://arxiv.org/pdf/2306.11507",
        "dateSubmitted": "2023-06-20",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Large Language Models (LLMs) such as ChatGPT, have gained significant attention due to their impressive natural language processing capabilities. It is crucial to prioritize human-centered principles when utilizing these models. Safeguarding the ethical and moral compliance of LLMs is of utmost importance. However, individual ethical issues have not been well studied on the latest LLMs. Therefore, this study aims to address these gaps by introducing a new benchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in three crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT examines toxicity in language models by employing toxic prompt templates derived from social norms. It then quantifies the extent of bias in models by measuring quantifiable toxicity values across different groups. Lastly, TrustGPT assesses the value of conversation generation models from both active value-alignment and passive value-alignment tasks. Through the implementation of TrustGPT, this research aims to enhance our understanding of the performance of conversation generation models and promote the development of language models that are more ethical and socially responsible.",
        "paperId": "9d81ec931b85d6c6cf3453126670cd7a30a689e7"
    },
    {
        "title": "Prompts of Large Language Model for Commanding Power Grid Operation",
        "firstAuthor": "Hanjiang Dong",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Large Language Models (LLMs) like ChatGPT can assist people\u2019s general workflows, where the prompt is necessary to inspire the potential of LLMs to solve problems from specified or professional domains like robotics. In the electrical engineering subject or the electric power utility industry, experienced operators and professional experts monitor power grid operation statuses and interact with the grid via human commands on the screen, and components in the grid execute the commands to keep the complex grid safe and economical operation. In this process, human experts edit commands to operate the corresponding software. Human commands are the natural language that the LLM can process. The power grid is composed of generation, transmission, distribution, and other components. Therefore, we redesign the human-computer interaction frame between practitioners and the grid via recurrent prompts to apply the LLM to generate computer programming instructions from the multi-step natural language commands. The programming instruction is executed on system components after being confirmed or revised by human experts, and the quality of generated programs will be gradually improved through human feedback. The idea of this study is originally inspired by studies on controlling individual robotic components by ChatGPT. In the future, we will apply the designed prompt templates to drive the general LLM to generate desired samples which could be used to train an LLM professional in the domain knowledge of electrical engineering to operate multiple types of software for power grid operators.",
        "paperId": "a0117209af5d7168d9e57318c916a9eb02289bb5"
    },
    {
        "title": "InstructionNER: A Multi-Task Instruction-Based Generative Framework for Few-shot NER",
        "firstAuthor": "Liwen Wang",
        "url": "http://arxiv.org/pdf/2203.03903",
        "dateSubmitted": "2022-03-08",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Recently, prompt-based methods have achieved significant performance in few-shot learning scenarios by bridging the gap between language model pre-training and fine-tuning for downstream tasks. However, existing prompt templates are mostly designed for sentence-level tasks and are inappropriate for sequence labeling objectives. To address the above issue, we propose a multi-task instruction-based generative framework, named InstructionNER, for low-resource named entity recognition. Specifically, we reformulate the NER task as a generation problem, which enriches source sentences with task-specific instructions and answer options, then inferences the entities and types in natural language. We further propose two auxiliary tasks, including entity extraction and entity typing, which enable the model to capture more boundary information of entities and deepen the understanding of entity type semantics, respectively. Experimental results show that our method consistently outperforms other baselines on five datasets in few-shot settings.",
        "paperId": "a29a0e679e626e8961dc217081eae2a6c63a15ad"
    },
    {
        "title": "PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using Visual Analytics for Large Language Models",
        "firstAuthor": "Aditi Mishra",
        "url": "http://arxiv.org/pdf/2304.01964",
        "dateSubmitted": "2023-04-04",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Large Language Models (LLMs) have gained widespread popularity due to their ability to perform ad-hoc Natural Language Processing (NLP) tasks with a simple natural language prompt. Part of the appeal for LLMs is their approachability to the general public, including individuals with no prior technical experience in NLP techniques. However, natural language prompts can vary significantly in terms of their linguistic structure, context, and other semantics. Modifying one or more of these aspects can result in significant differences in task performance. Non-expert users may find it challenging to identify the changes needed to improve a prompt, especially when they lack domain-specific knowledge and lack appropriate feedback. To address this challenge, we present PromptAid, a visual analytics system designed to interactively create, refine, and test prompts through exploration, perturbation, testing, and iteration. PromptAid uses multiple, coordinated visualizations which allow users to improve prompts by using the three strategies: keyword perturbations, paraphrasing perturbations, and obtaining the best set of in-context few-shot examples. PromptAid was designed through an iterative prototyping process involving NLP experts and was evaluated through quantitative and qualitative assessments for LLMs. Our findings indicate that PromptAid helps users to iterate over prompt template alterations with less cognitive overhead, generate diverse prompts with help of recommendations, and analyze the performance of the generated prompts while surpassing existing state-of-the-art prompting interfaces in performance.",
        "paperId": "a2c8d1c5470435176185bf891c76711a9b44808a"
    },
    {
        "title": "GraphPrompt: Graph-Based Prompt Templates for Biomedical Synonym Prediction",
        "firstAuthor": "Hanwen Xu",
        "url": "https://ojs.aaai.org/index.php/AAAI/article/download/26256/26028",
        "dateSubmitted": "2023-06-26",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "In the expansion of biomedical dataset, the same category may be labeled with different terms, thus being tedious and onerous to curate these terms. Therefore, automatically mapping synonymous terms onto the ontologies is desirable, which we name as biomedical synonym prediction task. Unlike biomedical concept normalization (BCN), no clues from context can be used to enhance synonym prediction, making it essential to extract graph features from ontology. We introduce an expert-curated dataset OBO-syn encompassing 70 different types of concepts and 2 million curated concept-term pairs for evaluating synonym prediction methods. We find BCN methods perform weakly on this task for not making full use of graph information. Therefore, we propose GraphPrompt, a prompt-based learning approach that creates prompt templates according to the graphs. GraphPrompt obtained 37.2% and 28.5% improvement on zero-shot and few-shot settings respectively, indicating the effectiveness of these graph-based prompt templates. We envision that our method GraphPrompt and OBO-syn dataset can be broadly applied to graph-based NLP tasks, and serve as the basis for analyzing diverse and accumulating biomedical data. All the data and codes are avalible at: https://github.com/HanwenXuTHU/GraphPrompt",
        "paperId": "a6628b4ac0b432659a0092add3eb371608d3e065"
    },
    {
        "title": "WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation",
        "firstAuthor": "Jongheon Jeong",
        "url": "https://arxiv.org/pdf/2303.14814",
        "dateSubmitted": "2023-03-26",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Visual anomaly classification and segmentation are vital for automating industrial quality inspection. The focus of prior research in the field has been on training custom models for each quality inspection task, which requires task-specific images and annotation. In this paper we move away from this regime, addressing zero-shot and few-normal-shot anomaly classification and segmentation. Recently CLIP, a vision-language model, has shown revolutionary generality with competitive zero-/few-shot performance in comparison to full-supervision. But CLIP falls short on anomaly classification and segmentation tasks. Hence, we propose window-based CLIP (WinCLIP) with (1) a compositional ensemble on state words and prompt templates and (2) efficient extraction and aggregation of window/patch/image-level features aligned with text. We also propose its few-normal-shot extension Win-CLIP+, which uses complementary information from normal images. In MVTec-AD (and VisA), without further tuning, WinCLIP achieves 91.8%/85.1% (78.1%/79.6%) AU-ROC in zero-shot anomaly classification and segmentation while WinCLIP + does 93.1%/95.2% (83.8%/96.4%) in 1-normal-shot, surpassing state-of-the-art by large margins.",
        "paperId": "aa207668318fec38d60b79f407fb64982e46fce9"
    },
    {
        "title": "A Prompt-based Few-shot Machine Reading Comprehension Model for Intelligent Bridge Management",
        "firstAuthor": "Luyi Zhang",
        "url": null,
        "dateSubmitted": "2022-10-21",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Bridge inspection reports are an important data source in the bridge management process, and they contain a large amount of fine-grained information. However, the research on machine reading comprehension (MRC) methods for this field is insufficient, and annotating large scale domain-specific corpus is time-consuming. This paper presented a novel prompt-based few-shot MRC approach for intelligent bridge management. The proposed model uses the pretrained model MacBERT as backbone. The prompt templates are designed based on some domain-specific heuristic rules. The experimental results show that our model outperforms the baseline models in different few-shot settings. The proposed model can provide technical support for the construction of automatic question answering system in the field of bridge management.",
        "paperId": "adad0b08dec960d0bc234bbc1b75c45dbfdf4732"
    },
    {
        "title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
        "firstAuthor": "Joonghoon Kim",
        "url": null,
        "dateSubmitted": "2023-11-07",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "This paper describes the DSBA submissions to the Prompting Large Language Models as Explainable Metrics shared task, where systems were submitted to two tracks: small and large summarization tracks. With advanced Large Language Models (LLMs) such as GPT-4, evaluating the quality of Natural Language Generation (NLG) has become increasingly paramount. Traditional similarity-based metrics such as BLEU and ROUGE have shown to misalign with human evaluation and are ill-suited for open-ended generation tasks. To address this issue, we explore the potential capability of LLM-based metrics, especially leveraging open-source LLMs. In this study, wide range of prompts and prompting techniques are systematically analyzed with three approaches: prompting strategy, score aggregation, and explainability. Our research focuses on formulating effective prompt templates, determining the granularity of NLG quality scores and assessing the impact of in-context examples on LLM-based evaluation. Furthermore, three aggregation strategies are compared to identify the most reliable method for aggregating NLG quality scores. To examine explainability, we devise a strategy that generates rationales for the scores and analyzes the characteristics of the explanation produced by the open-source LLMs. Extensive experiments provide insights regarding evaluation capabilities of open-source LLMs and suggest effective prompting strategies.",
        "paperId": "ae5767106f8e6b1d6a2da3992e3c4faaf6dee31c"
    },
    {
        "title": "Zero-Shot Relation Triple Extraction with Prompts for Low-Resource Languages",
        "firstAuthor": "Ayiguli Halike",
        "url": "https://www.mdpi.com/2076-3417/13/7/4636/pdf?version=1681110517",
        "dateSubmitted": "2023-04-06",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Although low-resource relation extraction is vital in knowledge construction and characterization, more research is needed on the generalization of unknown relation types. To fill the gap in the study of low-resource (Uyghur) relation extraction methods, we created a zero-shot with a quick relation extraction task setup. Each triplet extracted from an input phrase consists of the subject, relation type, and object. This paper suggests generating structured texts by urging language models to provide related instances. Our model consists of two modules: relation generator and relation and triplet extractor. We use the Uyghur relation prompt in the relation generator stage to generate new synthetic data. In the relation and triple extraction stage, we use the new data to extract the relation triplets in the sentence. We use multi-language model prompts and structured text techniques to offer a structured relation prompt template. This method is the first research that extends relation triplet extraction to a zero-shot setting for Uyghur datasets. Experimental results show that our method achieves a maximum weighted average F1 score of 47.39%.",
        "paperId": "b0068cb311068773968d74d377ea06e3c18fa5ce"
    },
    {
        "title": "Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification",
        "firstAuthor": "Han Wang",
        "url": "http://arxiv.org/pdf/2204.06305",
        "dateSubmitted": "2022-04-13",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Prompt-based learning (i.e., prompting) is an emerging paradigm for exploiting knowledge learned by a pretrained language model. In this paper, we propose Automatic Multi-Label Prompting (AMuLaP), a simple yet effective method to automatically select label mappings for few-shot text classification with prompting. Our method exploits one-to-many label mappings and a statistics-based algorithm to select label mappings given a prompt template. Our experiments demonstrate that AMuLaP achieves competitive performance on the GLUE benchmark without human effort or external resources.",
        "paperId": "b0f915c8e33afdf3829af71f189ddc34077dcc8e"
    },
    {
        "title": "Large Language Models can Share Images, Too!",
        "firstAuthor": "Young-Jun Lee",
        "url": null,
        "dateSubmitted": "2023-10-23",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "This paper explores the image-sharing capability of Large Language Models (LLMs), such as InstructGPT, ChatGPT, and GPT-4, in a zero-shot setting, without the help of visual foundation models. Inspired by the two-stage process of image-sharing in human dialogues, we propose a two-stage framework that allows LLMs to predict potential image-sharing turns and generate related image descriptions using our effective restriction-based prompt template. With extensive experiments, we unlock the \\textit{image-sharing} capability of LLMs in zero-shot prompting, with GPT-4 achieving the best performance. Additionally, we uncover the emergent \\textit{image-sharing} ability in zero-shot prompting, demonstrating the effectiveness of restriction-based prompts in both stages of our framework. Based on this framework, we augment the PhotoChat dataset with images generated by Stable Diffusion at predicted turns, namely PhotoChat++. To our knowledge, this is the first study to assess the image-sharing ability of LLMs in a zero-shot setting without visual foundation models. The source code and the dataset will be released after publication.",
        "paperId": "b1d2a29860e69c6ce9987ddefbe112feb1efa16a"
    },
    {
        "title": "A Cueing Strategy for Prompt Tuning in Relation Extraction",
        "firstAuthor": "",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Traditional relation extraction models predic- 001 t con\ufb01dence scores for each relation type 002 based on a condensed sentence representation. 003 In prompt tuning, prompt templates is used 004 to tune pre-trained language models (PLMs), 005 which outputs relation types as verbalized type 006 tokens. This strategy shows great potential 007 to support relation extraction because it is ef- 008 fective to take full use of rich knowledge in 009 PLMs. However, current prompt tuning mod- 010 els are directly implemented on a raw input. 011 It is weak to encode contextual features and 012 semantic dependencies of a relation instance. 013 In this paper, we designed a cueing strategy 014 which implants task speci\ufb01c cues into the in- 015 put. It controls the attention of prompt tun- 016 ing, which enable PLMs to learn task specif- 017 ic contextual features and semantic dependen- 018 cies of a relation instance. We evaluated our 019 method on two public datasets. Experiments 020 show great improvement. It exceeds state-of- 021 the-art performance by more than 4.8% and 022 1.4% in terms of F1-score on the SemEval cor- 023 pus and the ReTACRED corpus 1 . 024",
        "paperId": "b1d5d1774e7a9358c6549c808fba7203450acb21"
    },
    {
        "title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust",
        "firstAuthor": "Mrigank Raman",
        "url": "http://arxiv.org/pdf/2303.07320",
        "dateSubmitted": "2023-03-13",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token's hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations, such as word-level synonym substitutions. In this work, we demonstrate surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks. Rather than modifying the model (by appending an MLP head), MVP instead modifies the input (by appending a prompt template). Across three classification datasets, MVP improves performance against adversarial word-level synonym substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. By combining MVP with adversarial training, we achieve further improvements in robust accuracy while maintaining clean accuracy. Finally, we conduct ablations to investigate the mechanism underlying these gains. Notably, we find that the main causes of vulnerability of MLP can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized MLP parameters. Code is available at https://github.com/acmi-lab/mvp",
        "paperId": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c"
    },
    {
        "title": "CCPrompt: Counterfactual Contrastive Prompt-Tuning for Many-Class Classification",
        "firstAuthor": "Y. Li",
        "url": "https://arxiv.org/pdf/2211.05987",
        "dateSubmitted": "2022-11-11",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "With the success of the prompt-tuning paradigm in Natural Language Processing (NLP), various prompt templates have been proposed to further stimulate specific knowledge for serving downstream tasks, e.g., machine translation, text generation, relation extraction, and so on. Existing prompt templates are mainly shared among all training samples with the information of task description. However, training samples are quite diverse. The sharing task description is unable to stimulate the unique task-related information in each training sample, especially for tasks with the finite-label space. To exploit the unique task-related information, we imitate the human decision process which aims to find the contrastive attributes between the objective factual and their potential counterfactuals. Thus, we propose the \\textbf{C}ounterfactual \\textbf{C}ontrastive \\textbf{Prompt}-Tuning (CCPrompt) approach for many-class classification, e.g., relation classification, topic classification, and entity typing. Compared with simple classification tasks, these tasks have more complex finite-label spaces and are more rigorous for prompts. First of all, we prune the finite label space to construct fact-counterfactual pairs. Then, we exploit the contrastive attributes by projecting training instances onto every fact-counterfactual pair. We further set up global prototypes corresponding with all contrastive attributes for selecting valid contrastive attributes as additional tokens in the prompt template. Finally, a simple Siamese representation learning is employed to enhance the robustness of the model. We conduct experiments on relation classification, topic classification, and entity typing tasks in both fully supervised setting and few-shot setting. The results indicate that our model outperforms former baselines.",
        "paperId": "b7d643503f03dd0a23278932daa4fe01076e9ce6"
    },
    {
        "title": "EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus",
        "firstAuthor": "Cheng Li",
        "url": "https://arxiv.org/pdf/2307.11760",
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Large language models (LLMs) have achieved signi\ufb01cant performance in many \ufb01elds, such as reasoning, language understanding, and math problem-solving, and are regarded as an important step to arti\ufb01cial general intelligence (AGI). However, the sensitivity of LLMs to prompts remain a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. Our EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental re-sults demonstrate that our EmotionPrompt, using the same single prompt templates, signi\ufb01cantly out-performs the original prompt and Zero-shot-CoT in both zero-shot and few-shot settings on eight tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and Flan-T5-large. Furthermore, Emotion-Prompt was observed to improve both the truthfulness and informativeness. We believe that Emo-tionPrompt heralds a novel avenue for exploring interdisciplinary knowledge for interaction between humans and LLMs.",
        "paperId": "b8395045e0129b4152fdbd547467fe76be19471e"
    },
    {
        "title": "What Makes Pre-trained Language Models Better Zero-shot Learners?",
        "firstAuthor": "Jinghui Lu",
        "url": "https://aclanthology.org/2023.acl-long.128.pdf",
        "dateSubmitted": "2022-09-30",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Current methods for prompt learning in zero-shot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a real-world zero-shot scenario of practical relevance, no labelled data is available. Thus, we propose a simple yet effective method for screening reasonable prompt templates in zero-shot text classification: Perplexity Selection (Perplection). We hypothesize that language discrepancy can be used to measure the efficacy of prompt templates, and thereby develop a substantiated perplexity-based scheme allowing for forecasting the performance of prompt templates in advance. Experiments show that our method leads to improved prediction performance in a realistic zero-shot setting, eliminating the need for any labelled examples.",
        "paperId": "baf63d7cf115d674a8c8da3a3d789aa84521977a"
    },
    {
        "title": "A Dataset for Cross-Domain Reasoning via Template Filling",
        "firstAuthor": "",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "While several benchmarks exist for reasoning 001 tasks, reasoning across domains is an under- 002 explored area in NLP. Towards this, we present 003 a dataset and a prompt-template-filling ap- 004 proach to enable sequence to sequence mod- 005 els to perform cross-domain reasoning. We 006 also present a case-study with commonsense 007 and health and well-being domains, where 008 we study how prompt-template-filling en- 009 ables pretrained sequence to sequence models 010 across domains. Our experiments across sev- 011 eral pretrained encoder-decoder models show 012 that cross-domain reasoning is challenging for 013 current models. We also show an in-depth er- 014 ror analysis and avenues for future research for 015 reasoning across domains 1 . 016",
        "paperId": "bb359a433a67f9a1874e482145849d010318add8"
    },
    {
        "title": "A-C AP : Anticipation Captioning with Commonsense Knowledge",
        "firstAuthor": "I. Learning",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "The prompt learning was developed by NLP research [41, 42, 43]. It considers pre-trained language models such as BERT [10], as knowledge-based sources of useful information for downstream tasks. The key idea is to create a prompt (template) that can guide the pre-trained model through the adaptation process to a new task. It should be noted that the prompt format should be the same as the input format learned by the pre-trained model. Furthermore, the parameters of the pre-trained model are not updated during the training process; instead, we train the layers to learn prompt embeddings. The concept of prompt learning has recently been explored in computer vision [39, 40], where the context-word-generated prompt is converted into a set of learnable vectors and fed into a pre-trained vision-language model to solve downstream tasks. In our method, we use prompt learning in the same way as recent methods [39, 40]. We see that the key idea of VinVL [38] is the usage of concepts (object names), which allows better alignment between vision and language spaces, leading to the appearance of concepts in the caption. If we add forecasted concepts to the model, the model will be able to generate the caption based on the forecasted concepts. In our method, we combine detected and forecasted concepts to create the prompt. To this end, we change the VinVL\u2019s input to words\u2013(detected, forecasted)concepts\u2013 ROIs because the format of the prompt should be familiar to the pre-trained model (i.e., sequence of words\u2013concepts\u2013 ROIs). During the training time, by using cross-entropy loss, we update the graph neural network to learn the embeddings for the concepts to ensure that the pre-trained model can understand the prompt embeddings. After training, the pre-trained model can easily generate the desired captions from the input.",
        "paperId": "bc0cb4c753ab2ea51313355af848fc7a5c0d378d"
    },
    {
        "title": "Prompt Cache: Modular Attention Reuse for Low-Latency Inference",
        "firstAuthor": "In Gim",
        "url": null,
        "dateSubmitted": "2023-11-07",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "We present Prompt Cache, an approach for accelerating inference for large language models (LLM) by reusing attention states across different LLM prompts. Many input prompts have overlapping text segments, such as system messages, prompt templates, and documents provided for context. Our key insight is that by precomputing and storing the attention states of these frequently occurring text segments on the inference server, we can efficiently reuse them when these segments appear in user prompts. Prompt Cache employs a schema to explicitly define such reusable text segments, called prompt modules. The schema ensures positional accuracy during attention state reuse and provides users with an interface to access cached states in their prompt. Using a prototype implementation, we evaluate Prompt Cache across several LLMs. We show that Prompt Cache significantly reduce latency in time-to-first-token, especially for longer prompts such as document-based question answering and recommendations. The improvements range from 8x for GPU-based inference to 60x for CPU-based inference, all while maintaining output accuracy and without the need for model parameter modifications.",
        "paperId": "bc5c73c101da795cfa44e4ac7751cdedca9b6d93"
    },
    {
        "title": "PromptNER: Prompt Locating and Typing for Named Entity Recognition",
        "firstAuthor": "Yongliang Shen",
        "url": "http://arxiv.org/pdf/2305.17104",
        "dateSubmitted": "2023-05-26",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Prompt learning is a new paradigm for utilizing pre-trained language models and has achieved great success in many tasks. To adopt prompt learning in the NER task, two kinds of methods have been explored from a pair of symmetric perspectives, populating the template by enumerating spans to predict their entity types or constructing type-specific prompts to locate entities. However, these methods not only require a multi-round prompting manner with a high time overhead and computational cost, but also require elaborate prompt templates, that are difficult to apply in practical scenarios. In this paper, we unify entity locating and entity typing into prompt learning, and design a dual-slot multi-prompt template with the position slot and type slot to prompt locating and typing respectively. Multiple prompts can be input to the model simultaneously, and then the model extracts all entities by parallel predictions on the slots. To assign labels for the slots during training, we design a dynamic template filling mechanism that uses the extended bipartite graph matching between prompts and the ground-truth entities. We conduct experiments in various settings, including resource-rich flat and nested NER datasets and low-resource in-domain and cross-domain datasets. Experimental results show that the proposed model achieves a significant performance improvement, especially in the cross-domain few-shot setting, which outperforms the state-of-the-art model by +7.7% on average.",
        "paperId": "bd2c32285e8ad5b6e322391cca5d475de4f84169"
    },
    {
        "title": "HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing",
        "firstAuthor": "S. Sivarajkumar",
        "url": null,
        "dateSubmitted": "2022-03-09",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Developing clinical natural language systems based on machine learning and deep learning is dependent on the availability of large-scale annotated clinical text datasets, most of which are time-consuming to create and not publicly available. The lack of such annotated datasets is the biggest bottleneck for the development of clinical NLP systems. Zero-Shot Learning (ZSL) refers to the use of deep learning models to classify instances from new classes of which no training data have been seen before. Prompt-based learning is an emerging ZSL technique in NLP where we define task-based templates for different tasks. In this study, we developed a novel prompt-based clinical NLP framework called HealthPrompt and applied the paradigm of prompt-based learning on clinical texts. In this technique, rather than fine-tuning a Pre-trained Language Model (PLM), the task definitions are tuned by defining a prompt template. We performed an in-depth analysis of HealthPrompt on six different PLMs in a no-training-data setting. Our experiments show that HealthPrompt could effectively capture the context of clinical texts and perform well for clinical NLP tasks without any training data.",
        "paperId": "bf22ef16a6a912763780aea454198edc3e2bb3c9"
    },
    {
        "title": "Vision Encoders in Visual Question Answering",
        "firstAuthor": "Ryan R. Anderson",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Most existing methods that apply pretrained Visual Language Models (VLMs) to vision and language tasks do not sufficiently explore the effect of the format of their inputs on downstream performance. We show that utilising appropriate prompt formatting is a simple yet effective approach to improving the few-shot performance of VLMs that use relatively small language models on the Visual Question Answering (VQA) task. We format the inputs used to prompt a VLM using a modified text-only template from a closed-book question answering task that the language-model component of the VLM was pretrained on. By doing this, we explicitly align the VQA task with a task that this language model has already seen, enabling the VLM to leverage the similarities between the tasks, such as the answer-length distribution, when generating answers to the visual questions. In order to test our claims, we implement a simple architecture based on Frozen (Tsimpoukelli et al., 2021) and ClipCap (Mokady et al., 2021), whereby, through image captioning, the VLM learns to integrate powerful pretrained vision-only and language-only models via a relatively simple learnt mapping network. Furthermore, we contextualise our approach relative to existing work by presenting a unified view of VLMs. Our results show that explicit alignment enables our VLMs to achieve a significantly higher zero-shot (34.49% vs 20.89%) and best overall (40.39% vs 30.83%) VQA score on the VQA2.0 dataset (Goyal et al., 2017) than when the prompt template from Frozen (Tsimpoukelli et al., 2021) and Flamingo (Alayrac et al., 2022) is used. Furthermore, our zero-shot and best overall performance is better than Frozen\u2019s (34.49% vs 29.5% and 40.39% vs 38.2%, respectively) despite Frozen using a language model with more than double the number of parameters. Our code is available here.",
        "paperId": "c042d0169ea4f4e4490d406befb973ebab135e7e"
    },
    {
        "title": "CLIP model is an Efficient Continual Learner",
        "firstAuthor": "Vishal G. Thengane",
        "url": "http://arxiv.org/pdf/2210.03114",
        "dateSubmitted": "2022-10-06",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "The continual learning setting aims to learn new tasks over time without forgetting the previous ones. The literature reports several signi\ufb01cant efforts to tackle this problem with limited or no access to previous task data. Among such efforts, typical solutions offer sophisticated techniques involving memory replay, knowledge distillation, model regularization, and dynamic network expansion. The resulting methods have a retraining cost at each learning task, dedicated memory requirements, and setting-speci\ufb01c design choices. In this work, we show that a frozen CLIP (Contrastive Language-Image Pretraining) model offers as-tounding continual learning performance without any \ufb01ne-tuning (zero-shot eval-uation). We evaluate CLIP under a variety of settings including class-incremental, domain-incremental and task-agnostic incremental learning on \ufb01ve popular benchmarks (ImageNet-100 & 1K, CORe50, CIFAR-100, and TinyImageNet). Without any bells and whistles, the CLIP model outperforms the state-of-the-art continual learning approaches in majority of the settings. We show the effect on CLIP model\u2019s performance by varying text inputs with simple prompt templates. To the best of our knowledge, this is the \ufb01rst work to report the CLIP zero-shot performance in a continual setting. We advocate the use of this strong yet embarrass-ingly simple baseline for future comparisons in the continual learning tasks. Code is available at https://github.com/vgthengane/Continual-CLIP .",
        "paperId": "c1372b08e382030e905d1c8751a7794ee91e9d31"
    },
    {
        "title": "PRCBERT: Prompt Learning for Requirement Classification using BERT-based Pretrained Language Models",
        "firstAuthor": "Xianchang Luo",
        "url": "https://dl.acm.org/doi/pdf/10.1145/3551349.3560417",
        "dateSubmitted": "2022-10-10",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Software requirement classification is a longstanding and important problem in requirement engineering. Previous studies have applied various machine learning techniques for this problem, including Support Vector Machine (SVM) and decision trees. With the recent popularity of NLP technique, the state-of-the-art approach NoRBERT utilizes the pre-trained language model BERT and achieves a satisfactory performance. However, the dataset PROMISE used by the existing approaches for this problem consists of only hundreds of requirements that are outdated according to today\u2019s technology and market trends. Besides, the NLP technique applied in these approaches might be obsolete. In this paper, we propose an approach of prompt learning for requirement classification using BERT-based pretrained language models (PRCBERT), which applies flexible prompt templates to achieve accurate requirements classification. Experiments conducted on two existing small-size requirement datasets (PROMISE and NFR-Review) and our collected large-scale requirement dataset NFR-SO prove that PRCBERT exhibits moderately better classification performance than NoRBERT and MLM-BERT (BERT with the standard prompt template). On the de-labeled NFR-Review and NFR-SO datasets, Trans_PRCBERT (the version of PRCBERT which is fine-tuned on PROMISE) is able to have a satisfactory zero-shot performance with 53.27% and 72.96% F1-score when enabling a self-learning strategy.",
        "paperId": "c23e879454f8980424a261749f1f4b5797fef0a3"
    },
    {
        "title": "Distilling Task-specific Logical Rules from Large Pre-trained Models",
        "firstAuthor": "Tao Chen",
        "url": "http://arxiv.org/pdf/2210.02768",
        "dateSubmitted": "2022-10-06",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Logical rules, both transferable and explainable, are widely used as weakly supervised signals for many downstream tasks such as named entity tagging. To reduce the human effort of writing rules, previous researchers adopt an iterative approach to automatically learn logical rules from several seed rules. However, obtaining more seed rules can only be accomplished by extra human annotation with heavy costs. Limited by the size and quality of the seed rules, the model performance of previous systems is bounded. In this paper, we develop a novel framework STREAM to distill task-specific logical rules from large pre-trained models. Specifically, we borrow recent prompt-based language models as the knowledge expert to yield initial seed rules, and based on the formed high-quality instance pool that acts as an intermediary role, we keep teaching the expert to fit our task and learning task-specific logical rules. Experiments on three public named entity tagging benchmarks demonstrate the effectiveness of our proposed framework. With several predefined prompt templates, our system has gained significant improvements over previous state-of-the-art methods.",
        "paperId": "c2903ea606e409d49994c801bb5aab321f623e5c"
    },
    {
        "title": "A study on Prompt Design, Advantages and Limitations of ChatGPT for Deep Learning Program Repair",
        "firstAuthor": "Jialun Cao",
        "url": "http://arxiv.org/pdf/2304.08191",
        "dateSubmitted": "2023-04-17",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "ChatGPT has revolutionized many research and industrial fields. ChatGPT has shown great potential in software engineering to boost various traditional tasks such as program repair, code understanding, and code generation. However, whether automatic program repair (APR) applies to deep learning (DL) programs is still unknown. DL programs, whose decision logic is not explicitly encoded in the source code, have posed unique challenges to APR. While to repair DL programs, an APR approach needs to not only parse the source code syntactically but also needs to understand the code intention. With the best prior work, the performance of fault localization is still far less than satisfactory (only about 30\\%). Therefore, in this paper, we explore ChatGPT's capability for DL program repair by asking three research questions. (1) Can ChatGPT debug DL programs effectively? (2) How can ChatGPT's repair performance be improved by prompting? (3) In which way can dialogue help facilitate the repair? On top of that, we categorize the common aspects useful for prompt design for DL program repair. Also, we propose various prompt templates to facilitate the performance and summarize the advantages and disadvantages of ChatGPT's abilities such as detecting bad code smell, code refactoring, and detecting API misuse/deprecation.",
        "paperId": "c6808575096a6e4f3cbdc5f893384bc5a01cc6f8"
    },
    {
        "title": "Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner",
        "firstAuthor": "Zhengxiang Shi",
        "url": "https://arxiv.org/pdf/2305.01711",
        "dateSubmitted": "2023-05-02",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Language models (LMs) trained on vast quantities of unlabelled data have greatly advanced the field of natural language processing (NLP). In this study, we re-visit the widely accepted notion in NLP that continued pre-training LMs on task-related texts improves the performance of fine-tuning (FT) in downstream tasks. Through experiments on eight single-sentence tasks and eight sentence-pair tasks in both semi-supervised and fully-supervised settings, we find that conventional continued pre-training does not consistently provide benefits and can even be detrimental for sentence-pair tasks or when prompt-based FT is used. To tackle these issues, we propose Prompt-based Continued Pre-training (PCP), which combines the idea of instruction tuning with conventional continued pre-training. Our approach aims to improve the performance of prompt-based FT by presenting both task-related texts and prompt templates to LMs through unsupervised pre-training objectives before fine-tuning for the target task. Our empirical evaluations on 21 benchmarks demonstrate that the PCP consistently improves the performance of state-of-the-art prompt-based FT approaches (up to 20.1% absolute) in both semi-supervised and fully-supervised settings, even with only hundreds of unlabelled examples. Additionally, prompt-based FT with the PCP outperforms state-of-the-art semi-supervised approaches with greater simplicity, eliminating the need for an iterative process and extra data augmentation. Our further analysis explores the performance lower bound of the PCP and reveals that the advantages of PCP persist across different sizes of models and datasets.",
        "paperId": "c79852e9c9cc6734c9150847deb5449e489354ea"
    },
    {
        "title": "Prompting Large Language Model for Machine Translation: A Case Study",
        "firstAuthor": "Biao Zhang",
        "url": "http://arxiv.org/pdf/2301.07069",
        "dateSubmitted": "2023-01-17",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Research on prompting has shown excellent performance with little or even no supervised training across many tasks. However, prompting for machine translation is still under-explored in the literature. We fill this gap by offering a systematic study on prompting strategies for translation, examining various factors for prompt template and demonstration example selection. We further explore the use of monolingual data and the feasibility of cross-lingual, cross-domain, and sentence-to-document transfer learning in prompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as the testbed show that 1) the number and the quality of prompt examples matter, where using suboptimal examples degenerates translation; 2) several features of prompt examples, such as semantic similarity, show significant Spearman correlation with their prompting performance; yet, none of the correlations are strong enough; 3) using pseudo parallel prompt examples constructed from monolingual data via zero-shot prompting could improve translation; and 4) improved performance is achievable by transferring knowledge from prompt examples selected in other settings. We finally provide an analysis on the model outputs and discuss several problems that prompting still suffers from.",
        "paperId": "c879413103f8950bdd414c7f60a39bd7748c9be8"
    },
    {
        "title": "Grounded Theory and Collaborative Design Approach to Disability Storytelling on TikTok",
        "firstAuthor": "Morgan Lundy",
        "url": "https://iopn.library.illinois.edu/journals/aliseacp/article/download/1378/1102",
        "dateSubmitted": "2023-09-29",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "This developing dissertation explores the use of TikTok as a platform for individual and collective storytelling and information creation practices \u2013 within a specific online health community of people experiencing painful, invisible, and difficult to diagnose central sensitivity syndromes (CSSs) \u2013 to understand and support these embodied, creative, and collective information behaviors. Ongoing data collection indicates that people with CSSs are using TikTok affordances to tell and scaffold complex micro-stories about their expertise and social experiences of disability: by employing iconographic elements to make disability visual; intimate cinematography; audio, visual, and community-specific mimetic options; and platform-specific novel feature use.\nThe research design draws upon critical disabilities studies (CDS) sensitizing concepts. A constructivist grounded theory approach will be employed, by theoretically sampling TikTok micro-videos, their top comments, and, by the time of this presentation, conducting semi-structured interviews with CSS TikTok community members. This poster also discusses these preliminary results, as well as a novel initial sampling approach which addresses both the hashtag and algorithmic logics of the platform, and an implementation of feminist ethics of care in research methods.\nThen, three codesign workshops with individuals experiencing CSSs will develop creative storytelling materials that can be utilized in various contexts. These workshops promote the inclusion of disabled community members as co-designers and aim to co-design physical and digital storytelling resources such as prompts, templates, and TikTok features.These findings expand storytelling theory into the health domain, introduce and define algorithmically mediated online health communities, and promote critical disability studies perspectives in information science.",
        "paperId": "c8c63a9e67c65d78fdfa9ab7e6d94e23cd1ed3d1"
    },
    {
        "title": "Prompt Learning for Multi-modal COVID-19 Diagnosis",
        "firstAuthor": "Yangyang Yu",
        "url": null,
        "dateSubmitted": "2022-12-06",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "The outbreak of COVID-19 pandemic has spread rapidly and severely affected all aspects of human lives. Recent researches has shown artificial intelligence and deep learning based approaches have achieved successful results in detecting diseases. How to accurately and quickly detect COVID-19 has always been the core topic of research. In this paper, we propose a novel approach based on prompt learning for COVID-19 diagnosis. Different from the traditional \u201cpre-training, fine-tuning\u201d paradigm, we propose the prompt-based method that redefine the COVID-19 diagnosis as a masked predict task. Specifically, we adopt an attention mechanism to learn the multi-modal representation of medical image and text, and manually construct a cloze prompt template and a label word set. Selecting the label word corresponding to the maximum probability by pre-training language model. Finally, mapping the prediction results to the disease categories. Experimental results show that our proposed method obtains obvious improvement of 1.2% in terms of Mi-F1 score compared with the state-of-the-art methods.",
        "paperId": "c97f012003e1764f25382d7149ba09b1f9d79a65"
    },
    {
        "title": "LabelPrompt: Effective Prompt-based Learning for Relation Classification",
        "firstAuthor": "W. Zhang",
        "url": "https://arxiv.org/pdf/2302.08068",
        "dateSubmitted": "2023-02-16",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Recently, prompt-based learning has gained popularity across many natural language processing (NLP) tasks by reformulating them into a cloze-style format to better align pre-trained language models (PLMs) with downstream tasks. However, applying this approach to relation classification poses unique challenges. Specifically, associating natural language words that fill the masked token with semantic relation labels (\\textit{e.g.} \\textit{``org:founded\\_by}'') is difficult. To address this challenge, this paper presents a novel prompt-based learning method, namely LabelPrompt, for the relation classification task. Motivated by the intuition to ``GIVE MODEL CHOICES!'', we first define additional tokens to represent relation labels, which regard these tokens as the verbaliser with semantic initialisation and explicitly construct them with a prompt template method. Then, to mitigate inconsistency between predicted relations and given entities, we implement an entity-aware module with contrastive learning. Last, we conduct an attention query strategy within the self-attention layer to differentiates prompt tokens and sequence tokens. Together, these strategies enhance the adaptability of prompt-based learning, especially when only small labelled datasets is available. Comprehensive experiments on benchmark datasets demonstrate the superiority of our method, particularly in the few-shot scenario.",
        "paperId": "cb3379177c6e119dca0d32d41fa0c9b9fce172c8"
    },
    {
        "title": "MTPL-G2T: Graph-to-Text Generation Task Based on Mixed Template Prompt Learning",
        "firstAuthor": "Jianhe Cen",
        "url": null,
        "dateSubmitted": "2022-11-01",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "The Graph-to-Text(G2T) generation tasks are mainly done by pre-training and fine-tuning currently, but the drawback of fine-tuning is that it changes all parameters of the pre-trained model. In this paper, we aim to accomplish the text generation task through prompt learning so that no or a small number of model parameters can be changed. Also, we analyze the impact of three different prompt templates on the generation results. The results show that when the pre-trained language model is large (e.g., T5), prompt learning is competitive with finetuning, but the number of parameters that need to be modified for prompt learning is much smaller than for fine-tuning; meanwhile, compared with text templates and soft templates, using mixed prompt templates can make the model converge faster.",
        "paperId": "cb8387399966c88c3043657faa302ff56fc1307e"
    },
    {
        "title": "SAGA: Collaborative Storytelling with GPT-3",
        "firstAuthor": "Hanieh Shakeri",
        "url": null,
        "dateSubmitted": "2021-10-23",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "When friends live across different time zones, have incompatible work schedules, or have different levels of access to technology, synchronous communication becomes infeasible. To address this challenge, we developed a web application that allows friends to asynchronously collaborate creatively. In this application, multiple people can contribute to the writing of a story, told partially by a natural language AI system. By offloading some of the creative work to the AI, the human writers have the opportunity to also act as readers, being surprised by new events in the story. To gain preliminary insights into the experience of using this system, we conducted an informal pilot study over a span of 5 days. Through this process, we learned that storytelling with an AI system can encourage roleplay, it can be a cathartic experience, and it is curiosity-driven. Our recommendations for future research include (1) investigating new turn-taking strategies, and clearly communicating turns through the interface, (2) providing guidance for the prompt-writing process, perhaps through editable prompt templates, and (3) conducting a thorough evaluation of the system with friend groups of various sizes and timezones.",
        "paperId": "d05ba7fb84f60144d40b842c3824c00593b9c544"
    },
    {
        "title": "Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency",
        "firstAuthor": "Zhihan Liu",
        "url": "https://arxiv.org/pdf/2309.17382",
        "dateSubmitted": "2023-09-29",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call\"reason for future, act for now\"(\\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon (\"reason for future\"). At each step, the LLM agent takes the initial action of the planned trajectory (\"act for now\"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state. The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs to form an updated posterior of the unknown environment from the memory buffer (learning) and generate an optimal trajectory for multiple future steps that maximizes a value function (planning). The learning and planning subroutines are performed in an\"in-context\"manner to emulate the actor-critic update for MDPs. Our theoretical analysis proves that the novel combination of long-term reasoning and short-term acting achieves a $\\sqrt{T}$ regret. In particular, the regret bound highlights an intriguing interplay between the prior knowledge obtained through pretraining and the uncertainty reduction achieved by reasoning and acting. Our empirical validation shows that it outperforms various existing frameworks and achieves nearly perfect scores on a few benchmarks.",
        "paperId": "d3ca116177369bf6fbe27de64506a2f401aca996"
    },
    {
        "title": "LLM Powered Sim-to-real Transfer for Traffic Signal Control",
        "firstAuthor": "Longchao Da",
        "url": "https://arxiv.org/pdf/2308.14284",
        "dateSubmitted": "2023-08-28",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks aiming to provide efficient transportation and mitigate congestion waste. In recent, promising results have been attained by Reinforcement Learning (RL) methods through trial and error in simulators, bringing confidence in solving cities' congestion headaches. However, there still exist performance gaps when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the system dynamic difference between the training simulator and the real-world environments. The Large Language Models (LLMs) are trained on mass knowledge and proved to be equipped with astonishing inference abilities. In this work, we leverage LLMs to understand and profile the system dynamics by a prompt-based grounded action transformation. Accepting the cloze prompt template, and then filling in the answer based on accessible context, the pre-trained LLM's inference ability is exploited and applied to understand how weather conditions, traffic states, and road types influence traffic dynamics, being aware of this, the policies' action is taken and grounded based on realistic dynamics, thus help the agent learn a more realistic policy. We conduct experiments using DQN to show the effectiveness of the proposed PromptGAT's ability in mitigating the performance gap from simulation to reality (sim-to-real).",
        "paperId": "d40430275383ef8a453eefb693c44cbc686008e0"
    },
    {
        "title": "An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels",
        "firstAuthor": "Lisa P. Argyle",
        "url": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/035D7C8A55B237942FB6DBAD7CAA4E49/S1047198723000025a.pdf/div-class-title-out-of-one-many-using-language-models-to-simulate-human-samples-div.pdf",
        "dateSubmitted": "2022-03-21",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates without labeled examples and without direct access to the model. Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output. Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90% of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels.",
        "paperId": "d53e70d834243d3d8d4b621c0c52dfec26081155"
    },
    {
        "title": "Prompting Large Language Models With the Socratic Method",
        "firstAuthor": "Edward Y. Chang",
        "url": "https://arxiv.org/pdf/2303.08769",
        "dateSubmitted": "2023-02-17",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "This paper presents a systematic approach to using the Socratic method in developing prompt templates that effectively interact with large language models, including GPT-3. Various methods are examined, and those that yield precise answers and justifications while fostering creativity and imagination to enhance creative writing are identified. Techniques such as definition, elenchus, dialectic, maieutics, generalization, and counterfactual reasoning are discussed for their application in engineering prompt templates and their connections to inductive, deductive, and abductive reasoning. Through examples, the effectiveness of these dialogue and reasoning methods is demonstrated. An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively.",
        "paperId": "d7386e8859b22e05ce9c4a972613d4b1e1e44198"
    },
    {
        "title": "Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels",
        "firstAuthor": "Honglei Zhuang",
        "url": null,
        "dateSubmitted": "2023-10-21",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Zero-shot text rankers powered by recent LLMs achieve remarkable ranking performance by simply prompting. Existing prompts for pointwise LLM rankers mostly ask the model to choose from binary relevance labels like\"Yes\"and\"No\". However, the lack of intermediate relevance label options may cause the LLM to provide noisy or biased answers for documents that are partially relevant to the query. We propose to incorporate fine-grained relevance labels into the prompt for LLM rankers, enabling them to better differentiate among documents with different levels of relevance to the query and thus derive a more accurate ranking. We study two variants of the prompt template, coupled with different numbers of relevance levels. Our experiments on 8 BEIR data sets show that adding fine-grained relevance labels significantly improves the performance of LLM rankers.",
        "paperId": "da9b8b4073e6ad44b3da66e1e117cb1ddbf8836d"
    },
    {
        "title": "AnoVL: Adapting Vision-Language Models for Unified Zero-shot Anomaly Localization",
        "firstAuthor": "Hanqiu Deng",
        "url": "https://arxiv.org/pdf/2308.15939",
        "dateSubmitted": "2023-08-30",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Contrastive Language-Image Pre-training (CLIP) models have shown promising performance on zero-shot visual recognition tasks by learning visual representations under natural language supervision. Recent studies attempt the use of CLIP to tackle zero-shot anomaly detection by matching images with normal and abnormal state prompts. However, since CLIP focuses on building correspondence between paired text prompts and global image-level representations, the lack of patch-level vision to text alignment limits its capability on precise visual anomaly localization. In this work, we introduce a training-free adaptation (TFA) framework of CLIP for zero-shot anomaly localization. In the visual encoder, we innovate a training-free value-wise attention mechanism to extract intrinsic local tokens of CLIP for patch-level local description. From the perspective of text supervision, we particularly design a unified domain-aware contrastive state prompting template. On top of the proposed TFA, we further introduce a test-time adaptation (TTA) mechanism to refine anomaly localization results, where a layer of trainable parameters in the adapter is optimized using TFA's pseudo-labels and synthetic noise-corrupted tokens. With both TFA and TTA adaptation, we significantly exploit the potential of CLIP for zero-shot anomaly localization and demonstrate the effectiveness of our proposed methods on various datasets.",
        "paperId": "daa34ae46c82e6980ac1daaf2dd9716ef3718f21"
    },
    {
        "title": "Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing",
        "firstAuthor": "Yibo Wang",
        "url": "https://arxiv.org/pdf/2211.02483",
        "dateSubmitted": "2022-11-04",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "The explosion of e-commerce has caused the need for processing and analysis of product titles, like entity typing in product titles. However, the rapid activity in e-commerce has led to the rapid emergence of new entities, which is difficult for general entity typing. Besides, product titles in e-commerce have very different language styles from text data in general domain. In order to handle new entities in product titles and address the special language styles of product titles in e-commerce domain, we propose our textual entailment model with continuous prompt tuning based hypotheses and fusion embeddings for e-commerce entity typing. First, we reformulate entity typing into a textual entailment problem to handle new entities that are not present during training. Second, we design a model to automatically generate textual entailment hypotheses using a continuous prompt tuning method, which can generate better textual entailment hypotheses without manual design. Third, we utilize the fusion embeddings of BERT embedding and Char-acterBERT embedding to solve the problem that the language styles of product titles in e-commerce are different from that of general domain. To analyze the effect of each contribution, we compare the performance of entity typing and textual entailment model, and conduct ablation studies on continuous prompt tuning and fusion embeddings. We also evaluate the impact of different prompt template initialization for the continuous prompt tuning. We show our proposed model improves the average F1 score by around 2% compared to the baseline BERT entity typing model.",
        "paperId": "dd568e6838903ad7c381f13c1268c94c5db08b02"
    },
    {
        "title": "Sensitivity and Robustness of Large Language Models to Prompt Template in Japanese Text Classification Tasks",
        "firstAuthor": "Chengguang Gan",
        "url": null,
        "dateSubmitted": "2023-05-15",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Prompt engineering relevance research has seen a notable surge in recent years, primarily driven by advancements in pre-trained language models and large language models. However, a critical issue has been identified within this domain: the inadequate of sensitivity and robustness of these models towards Prompt Templates, particularly in lesser-studied languages such as Japanese. This paper explores this issue through a comprehensive evaluation of several representative Large Language Models (LLMs) and a widely-utilized pre-trained model(PLM). These models are scrutinized using a benchmark dataset in Japanese, with the aim to assess and analyze the performance of the current multilingual models in this context. Our experimental results reveal startling discrepancies. A simple modification in the sentence structure of the Prompt Template led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44. This observation underscores the fact that even the highly performance GPT-4 model encounters significant stability issues when dealing with diverse Japanese prompt templates, rendering the consistency of the model's output results questionable. In light of these findings, we conclude by proposing potential research trajectories to further enhance the development and performance of Large Language Models in their current stage.",
        "paperId": "de11dd9386518012fec7d6f564755b6e6cdbd241"
    },
    {
        "title": "A Practical Three-phase Approach To Fully Automated Programming Using System Decomposition And Coding Copilots",
        "firstAuthor": "Haoli Bai",
        "url": null,
        "dateSubmitted": "2022-09-23",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Very large-scale (VLS) deep learning models are capable of generating meaningful code snippets, yet the performance drops dramatically when the coding task becomes more complex. Although fully neural approaches have been proposed to solve this problem, the value of the application is still limited. In our work, we propose a neuro-symbolic approach that integrates the symbolic natures of programming and the existing neural language models. We divide a programming task into three phases: forming a hierarchical task composed of functions, completing each function, and fulfilling the corner cases. Because each phase can be completed by language models, the coding process can be fully automated. Our contribution is three-fold. Firstly, we show that with little help from humans, VLS language models are capable of completing non-trivial programming tasks. Secondly, we provide a number of empirical insights to create prompt templates that help the language models generate better code. Thirdly, compared to the existing approaches, our work provides a much more practical approach for programmers and researchers to follow. The generated programming project using our fully automated programming approach and part of the ablation study code are available at https://github.com/BiEchi/FAP.",
        "paperId": "e16b2de59f7397fec8eb3c6717abba5519cc055c"
    },
    {
        "title": "Large Language Models are Zero-Shot Reasoners",
        "firstAuthor": "Takeshi Kojima",
        "url": null,
        "dateSubmitted": "2022-05-24",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
        "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda"
    },
    {
        "title": "Mitigating Word Bias in Zero-shot Prompt-based Classifiers",
        "firstAuthor": "Adian Liusie",
        "url": "https://arxiv.org/pdf/2309.04992",
        "dateSubmitted": "2023-09-10",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Prompt-based classifiers are an attractive approach for zero-shot classification. However, the precise choice of the prompt template and label words can largely influence performance, with semantically equivalent settings often showing notable performance difference. This discrepancy can be partly attributed to word biases, where the classifier may be biased towards classes. To address this problem, it is possible to optimise classification thresholds on a labelled data set, however, this mitigates some of the advantages of prompt-based classifiers. This paper instead approaches this problem by examining the expected marginal probabilities of the classes. Here, probabilities are reweighted to have a uniform prior over classes, in an unsupervised fashion. Further, we draw a theoretical connection between the class priors and the language models' word prior, and offer the ability to set a threshold in a zero-resource fashion. We show that matching class priors correlates strongly with the oracle upper bound performance and demonstrate large consistent performance gains for prompt settings over a range of NLP tasks.",
        "paperId": "e7d21ad4da122bf1db19e4fda57bf94c1dfa24a4"
    },
    {
        "title": "DAPrompt: Deterministic Assumption Prompt Learning for Event Causality Identification",
        "firstAuthor": "Wei Xiang",
        "url": "https://arxiv.org/pdf/2307.09813",
        "dateSubmitted": "2023-07-19",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Event Causality Identification (ECI) aims at determining whether there is a causal relation between two event mentions. Conventional prompt learning designs a prompt template to first predict an answer word and then maps it to the final decision. Unlike conventional prompts, we argue that predicting an answer word may not be a necessary prerequisite for the ECI task. Instead, we can first make a deterministic assumption on the existence of causal relation between two events and then evaluate its rationality to either accept or reject the assumption. The design motivation is to try the most utilization of the encyclopedia-like knowledge embedded in a pre-trained language model. In light of such considerations, we propose a deterministic assumption prompt learning model, called DAPrompt, for the ECI task. In particular, we design a simple deterministic assumption template concatenating with the input event pair, which includes two masks as predicted events' tokens. We use the probabilities of predicted events to evaluate the assumption rationality for the final event causality decision. Experiments on the EventStoryLine corpus and Causal-TimeBank corpus validate our design objective in terms of significant performance improvements over the state-of-the-art algorithms.",
        "paperId": "e92f4ff44def2273d9fcb02921b257dcbe3c9626"
    },
    {
        "title": "ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction",
        "firstAuthor": "Jianghao Lin",
        "url": "https://arxiv.org/pdf/2310.09234",
        "dateSubmitted": "2023-10-13",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Click-through rate (CTR) prediction has become increasingly indispensable for various Internet applications. Traditional CTR models convert the multi-field categorical data into ID features via one-hot encoding, and extract the collaborative signals among features. Such a paradigm suffers from the problem of semantic information loss. Another line of research explores the potential of pretrained language models (PLMs) for CTR prediction by converting input data into textual sentences through hard prompt templates. Although semantic signals are preserved, they generally fail to capture the collaborative information (e.g., feature interactions, pure ID features), not to mention the unacceptable inference overhead brought by the huge model size. In this paper, we aim to model both the semantic knowledge and collaborative knowledge for accurate CTR estimation, and meanwhile address the inference inefficiency issue. To benefit from both worlds and close their gaps, we propose a novel model-agnostic framework (i.e., ClickPrompt), where we incorporate CTR models to generate interaction-aware soft prompts for PLMs. We design a prompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM has to recover the masked tokens based on the language context, as well as the soft prompts generated by CTR model. The collaborative and semantic knowledge from ID and textual features would be explicitly aligned and interacted via the prompt interface. Then, we can either tune the CTR model with PLM for superior performance, or solely tune the CTR model without PLM for inference efficiency. Experiments on four real-world datasets validate the effectiveness of ClickPrompt compared with existing baselines.",
        "paperId": "e96be7c55d139965b15bc0527d6d528b225f9a61"
    },
    {
        "title": "TaxoPrompt: A Prompt-based Generation Method with Taxonomic Context for Self-Supervised Taxonomy Expansion",
        "firstAuthor": "Hongyuan Xu",
        "url": "https://www.ijcai.org/proceedings/2022/0615.pdf",
        "dateSubmitted": "2022-07-01",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Taxonomies are hierarchical classifications widely exploited to facilitate downstream natural language processing tasks. The taxonomy expansion task aims to incorporate emergent concepts into the existing taxonomies. Prior works focus on modeling the local substructure of taxonomies but neglect the global structure. In this paper, we propose TaxoPrompt, a framework that learns the global structure by prompt tuning with taxonomic context. Prompt tuning leverages a template to formulate downstream tasks into masked language model form for better distributed semantic knowledge use. To further infuse global structure knowledge into language models, we enhance the prompt template by exploiting the taxonomic context constructed by a variant of the random walk algorithm. Experiments on seven public benchmarks show that our proposed TaxoPrompt is effective and efficient in automatically expanding taxonomies and achieves state-of-the-art performance.",
        "paperId": "ea2fb89403ea1cd6af000e761e2f72eb7c150607"
    },
    {
        "title": "B . Alternate Design Choices Prompt Initialization : Table 8",
        "firstAuthor": "",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Prompt Initialization: Table 8 shows the effect of prompt initialization on MaPLe. Best performance is achieved when the learnable prompts in the \ue000rst layer are initialized with the prompt \u2018a photo of a <category>\u2019 and rest of the layers are initialized randomly (row-3). Initializing prompts with a similar template in all layers leads to lower performance suggesting that this is redundant as these prompts learn hierarchically different contextual concepts in different layers (row-1). However, complete random initialization of prompts provides competitive performance (row-2). For implementation, if the number of learnable prompts M = #P are less than the total tokens of initial prompt template, we convert the former M word embeddings of template with learnable prompts and consider the rest of word embeddings of prompt template as \ue000xed and use all token embeddings (learnable prompts + \ue000xed word tokens) as input to text encoder.",
        "paperId": "ed0157de9b428109b51fad953f1c5dc42c74d060"
    },
    {
        "title": "Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures",
        "firstAuthor": "Zhen Yue",
        "url": "http://arxiv.org/pdf/2306.05171",
        "dateSubmitted": "2023-06-08",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Traditional robot task planning methods face challenges when dealing with highly unstructured environments and complex tasks. We propose a task planning method that combines human expertise with an LLM and have designed an LLM prompt template, Think_Net_Prompt, with stronger expressive power to represent structured professional knowledge. We further propose a method to progressively decompose tasks and generate a task tree to reduce the planning volume for each task, and we have designed a strategy to decouple robot task planning. By dividing different planning entities and separating the task from the actual machine binding process, the task planning process becomes more flexible. Research results show that our method performs well in handling specified code formats, understanding the relationship between tasks and subtasks, and extracting parameters from text descriptions. However, there are also problems such as limited complexity of task logic handling, ambiguity in the quantity of parts and the precise location of assembly. Improving the precision of task description and cognitive structure can bring certain improvements. https://github.com/NOMIzy/Think_Net_Prompt",
        "paperId": "eebb4a3162c1251b51e50ccd83797babc5b776c0"
    },
    {
        "title": "Focusing leadership through force field analysis: new variations on a venerable planning tool",
        "firstAuthor": "Randolph E. Schwering",
        "url": null,
        "dateSubmitted": "2003-11-01",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "This forwards a new version of a tool long used in planned change and organizational development efforts \u2013 force field analysis. Existing applications of this technique are critiqued in light of cognitive heuristics known to erode judgment and analytical performance in plan development. A cognitive prompting template is combined with the existing the force field analysis technique to mitigate these problems. As such, the revised technique represents a significant improvement over the traditional application of the force field tool as used by the OD practitioner. Following an overview of the theoretical underpinnings of the revised technique, a case example is offered to illustrate the technique as it was used in a real organization. Finally, practical facilitation guidelines are offered to help leaders and planners conduct force field analysis sessions in multi\u2010stakeholder change efforts.",
        "paperId": "eece5c6df36bcbd5b8441b1b3ce3947f18221718"
    },
    {
        "title": "TIAM - A Metric for Evaluating Alignment in Text-to-Image Generation",
        "firstAuthor": "P. Grimal",
        "url": "https://arxiv.org/pdf/2307.05134",
        "dateSubmitted": "2023-07-11",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "The progress in the generation of synthetic images has made it crucial to assess their quality. While several metrics have been proposed to assess the rendering of images, it is crucial for Text-to-Image (T2I) models, which generate images based on a prompt, to consider additional aspects such as to which extent the generated image matches the important content of the prompt. Moreover, although the generated images usually result from a random starting point, the influence of this one is generally not considered. In this article, we propose a new metric based on prompt templates to study the alignment between the content specified in the prompt and the corresponding generated images. It allows us to better characterize the alignment in terms of the type of the specified objects, their number, and their color. We conducted a study on several recent T2I models about various aspects. An additional interesting result we obtained with our approach is that image quality can vary drastically depending on the latent noise used as a seed for the images. We also quantify the influence of the number of concepts in the prompt, their order as well as their (color) attributes. Finally, our method allows us to identify some latent seeds that produce better images than others, opening novel directions of research on this understudied topic.",
        "paperId": "f7d57f223154965e6e5584d3a51561aaea7ca13b"
    },
    {
        "title": "The Limits of ChatGPT in Extracting Aspect-Category-Opinion-Sentiment Quadruples: A Comparative Analysis",
        "firstAuthor": "Xiancai Xu",
        "url": "https://arxiv.org/pdf/2310.06502",
        "dateSubmitted": "2023-10-10",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Recently, ChatGPT has attracted great attention from both industry and academia due to its surprising abilities in natural language understanding and generation. We are particularly curious about whether it can achieve promising performance on one of the most complex tasks in aspect-based sentiment analysis, i.e., extracting aspect-category-opinion-sentiment quadruples from texts. To this end, in this paper we develop a specialized prompt template that enables ChatGPT to effectively tackle this complex quadruple extraction task. Further, we propose a selection method on few-shot examples to fully exploit the in-context learning ability of ChatGPT and uplift its effectiveness on this complex task. Finally, we provide a comparative evaluation on ChatGPT against existing state-of-the-art quadruple extraction models based on four public datasets and highlight some important findings regarding the capability boundaries of ChatGPT in the quadruple extraction.",
        "paperId": "f84d6d6d58b836a64c4a96b062bfff769d08a595"
    },
    {
        "title": "AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models",
        "firstAuthor": "Jiarun Liu",
        "url": null,
        "dateSubmitted": "2023-10-20",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Large Language Models (LLMs) have emerged as promising agents for web navigation tasks, interpreting objectives and interacting with web pages. However, the efficiency of spliced prompts for such tasks remains underexplored. We introduces AllTogether, a standardized prompt template that enhances task context representation, thereby improving LLMs' performance in HTML-based web navigation. We evaluate the efficacy of this approach through prompt learning and instruction finetuning based on open-source Llama-2 and API-accessible GPT models. Our results reveal that models like GPT-4 outperform smaller models in web navigation tasks. Additionally, we find that the length of HTML snippet and history trajectory significantly influence performance, and prior step-by-step instructions prove less effective than real-time environmental feedback. Overall, we believe our work provides valuable insights for future research in LLM-driven web agents.",
        "paperId": "f875dd45ae6aa0935f05648f6c7873d9d9022807"
    },
    {
        "title": "Self-supervised Bidirectional Prompt Tuning for Entity-enhanced Pre-trained Language Model",
        "firstAuthor": "Jiaxin Zou",
        "url": null,
        "dateSubmitted": "2023-06-18",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "With the promotion of the pre-training paradigm, researchers are increasingly focusing on injecting external knowledge, such as entities and triplets from knowledge graphs, into pre-trained language models (PTMs) to improve their understanding and logical reasoning abilities. This results in significant improvements in natural language understanding and generation tasks and some level of interpretability. In this paper, we propose a novel two-stage entity knowledge enhancement pipeline for Chinese pre-trained models based on \u201cbidirectional\u201d prompt tuning. The pipeline consists of a \u201cforward\u201d stage, in which we construct fine-grained entity type prompt templates to boost PTMs injected with entity knowledge, and a \u201cbackward\u201d stage, where the trained templates are used to generate type-constrained context-dependent negative samples for contrastive learning. Experiments on six classification tasks in the Chinese Language Understanding Evaluation (CLUE) benchmark demonstrate that our approach significantly improves upon the baseline results in most datasets, particularly those that have a strong reliance on diverse and extensive knowledge.",
        "paperId": "fa9827c62ba9ca539a1a46f4bcbb87745bb2fefd"
    },
    {
        "title": "Aspect-based Sentiment Classification with Sequential Cross-modal Semantic Graph",
        "firstAuthor": "Yufen Huang",
        "url": "https://arxiv.org/pdf/2208.09417",
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Multi-modal aspect-based sentiment classi\ufb01cation (MABSC) is an emerging classi\ufb01cation task that aims to classify the sentiment of a given target such as a mentioned entity in data with different modalities. In typical multi-modal data with text and image, previous approaches do not make full use of the \ufb01ne-grained semantics of the image, especially in con-junction with the semantics of the text and do not fully con- sider modeling the relationship between \ufb01ne-grained image information and target, which leads to insuf\ufb01cient use of im- age and inadequate to identify \ufb01ne-grained aspects and opinions. To tackle these limitations, we propose a new frame- work SeqCSG including a method to construct sequential cross-modal semantic graphs and an encoder-decoder model. Speci\ufb01cally, we extract \ufb01ne-grained information from the original image, image caption, and scene graph, and regard them as elements of the cross-modal semantic graph as well as tokens from texts. The cross-modal semantic graph is rep- resented as a sequence with a multi-modal visible matrix indicating relationships between elements. In order to effec- tively utilize the cross-modal semantic graph, we propose an encoder-decoder method with a target prompt template. Ex- perimental results show that our approach outperforms existing methods and achieves the state-of-the-art on two stan- dard datasets MABSC. Further analysis demonstrates the effectiveness of each component and our model can implicitly learn the correlation between the target and \ufb01ne-grained information of the image.",
        "paperId": "fb7ed529fec665450925f9a75129cb69be83b67a"
    },
    {
        "title": "Let Me Check the Examples: Enhancing Demonstration Learning via Explicit Imitation",
        "firstAuthor": "Sirui Wang",
        "url": "http://arxiv.org/pdf/2209.00455",
        "dateSubmitted": "2022-08-31",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Demonstration learning aims to guide the prompt prediction by providing answered demonstrations in the few shot settings. Despite achieving promising results, existing work only concatenates the answered examples as demonstrations to the prompt template (including the raw context) without any additional operation, neglecting the prompt-demonstration dependencies. Besides, prior research found that randomly replacing the labels of demonstrations marginally hurts performance, illustrating that the model could not properly learn the knowledge brought by the demonstrations. Inspired by the human learning process, in this paper, we introduce Imitation DEMOnstration learning (Imitation-Demo) to strengthen demonstration learning via explicitly imitating human review behaviour, which includes: (1) contrastive learning mechanism to concentrate on similar demonstrations.(2) demonstration-label re-prediction method to consolidate known knowledge. Experiment results show that our proposed method achieves state-of-the-art performance on 5 out of 14 classification corpus. Further studies also prove that Imitation-Demo strengthens the associations between the prompt and demonstrations, which could provide the basis for exploring how demonstration learning works.",
        "paperId": "fdbdcc3a65dfd6f258c533fd12d58bbfcab15bc3"
    },
    {
        "title": "Prompt-Based Length Controlled Generation with Reinforcement Learning",
        "firstAuthor": "Renlong Jie",
        "url": "https://arxiv.org/pdf/2308.12030",
        "dateSubmitted": "2023-08-23",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising performance on a wide range of NLP tasks. Length controlled generation of LLMs emerges as an important topic, which enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can reduce the inference cost by limiting the length. Therefore, we propose a prompt-based length control method to achieve high-accuracy length controlled generation. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward models, which further enhances the length-control ability of LLMs by rewarding outputs that follows pre-defined control instruction. To enable rule-based inference, we also introduce standard prompt extractor to collect the standard control information from users' input. Experiments show that our method significantly improves the accuracy of prompt-based length control for summarization task on popular datasets like CNNDM and NYT. Both the standard prompt extractor and the RL-tuned model have show strong generalization ability to unseen control prompt templates.",
        "paperId": "fe583403c95c3e9b4148d6276f04bda5ace33660"
    },
    {
        "title": "Sensitivity and Robustness of Large Language Models to Prompt in Japanese",
        "firstAuthor": "Chengguang Gan",
        "url": "http://arxiv.org/pdf/2305.08714",
        "dateSubmitted": null,
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Prompt Engineering has gained signi\ufb01cant rel-evance in recent years, fueled by advance-ments in pre-trained and large language models. However, a critical issue has been iden-ti\ufb01ed within this domain: the lack of sensitivity and robustness of these models towards Prompt Templates, particularly in lesser-studied languages such as Japanese. This paper explores this issue through a comprehensive evaluation of several representative Large Language Models (LLMs) and a widely-utilized pre-trained model(PLM), T5. These models are scrutinized using a benchmark dataset in Japanese, with the aim to assess and analyze the performance of the current multilingual models in this context. Our experimental results reveal startling discrepancies. A simple modi\ufb01cation in the sentence structure of the Prompt Template led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44. This observation underscores the fact that even the highly performance GPT-4 model encoun-ters signi\ufb01cant stability issues when dealing with diverse Japanese prompt templates, ren-dering the consistency of the model\u2019s output results questionable. In light of these \ufb01ndings, we conclude by proposing potential research trajectories to further enhance the devel-opment and performance of Large Language Models in their current stage.",
        "paperId": "ff77cc047f5e7a2fdf8563d05e1ba4b383e859a4"
    },
    {
        "title": "LLM4DV: Using Large Language Models for Hardware Test Stimuli Generation",
        "firstAuthor": "Zixi Zhang",
        "url": "https://arxiv.org/pdf/2310.04535",
        "dateSubmitted": "2023-10-06",
        "keyWords": [
            "prompt template"
        ],
        "abstract": "Test stimuli generation has been a crucial but labor-intensive task in hardware design verification. In this paper, we revolutionize this process by harnessing the power of large language models (LLMs) and present a novel benchmarking framework, LLM4DV. This framework introduces a prompt template for interactively eliciting test stimuli from the LLM, along with four innovative prompting improvements to support the pipeline execution and further enhance its performance. We compare LLM4DV to traditional constrained-random testing (CRT), using three self-designed design-under-test (DUT) modules. Experiments demonstrate that LLM4DV excels in efficiently handling straightforward DUT scenarios, leveraging its ability to employ basic mathematical reasoning and pre-trained knowledge. While it exhibits reduced efficiency in complex task settings, it still outperforms CRT in relative terms. The proposed framework and the DUT modules used in our experiments will be open-sourced upon publication.",
        "paperId": "ff7f75989d125a3356fdb5ad76f504037cc27d5c"
    }
]