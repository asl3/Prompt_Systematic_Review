[
    {
        "title": "Teaching Arithmetic to Small Transformers",
        "firstAuthor": "Nayoung Lee",
        "url": "https://arxiv.org/pdf/2307.03381",
        "dateSubmitted": "2023-07-07",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and model scale. Additionally, we discuss length generalization challenges. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction objective for rapidly eliciting arithmetic capabilities.",
        "paperId": "002cfed5d4d9bf2fdaddb11d32f14751f2250e0c"
    },
    {
        "title": "Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations",
        "firstAuthor": "Swarnadeep Saha",
        "url": "https://arxiv.org/pdf/2211.07517",
        "dateSubmitted": "2022-11-14",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Recent work on explainable NLP has shown that few-shot prompting can enable large pre-trained language models (LLMs) to generate grammatical and factual natural language explanations for data labels. In this work, we study the connection between explainability and sample hardness by investigating the following research question \u2013 \u201cAre LLMs and humans equally good at explaining data labels for both easy and hard samples?\u201d We answer this question by first collecting human-written explanations in the form of generalizable commonsense rules on the task of Winograd Schema Challenge (Winogrande dataset). We compare these explanations with those generated by GPT-3 while varying the hardness of the test samples as well as the in-context samples. We observe that (1) GPT-3 explanations are as grammatical as human explanations regardless of the hardness of the test samples, (2) for easy examples, GPT-3 generates highly supportive explanations but human explanations are more generalizable, and (3) for hard examples, human explanations are significantly better than GPT-3 explanations both in terms of label-supportiveness and generalizability judgements. We also find that hardness of the in-context examples impacts the quality of GPT-3 explanations. Finally, we show that the supportiveness and generalizability aspects of human explanations are also impacted by sample hardness, although by a much smaller margin than models.",
        "paperId": "0040dac7a1bf7a1eeb01c86ddb993f331f35b158"
    },
    {
        "title": "P4E: Few-Shot Event Detection as Prompt-Guided Identification and Localization",
        "firstAuthor": "Sha Li",
        "url": null,
        "dateSubmitted": "2022-02-15",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "We propose P4E, an identify-and-localize event detection framework that integrates the best of few-shot prompting and structured prediction. Our framework decomposes event detection into an identification task and a localization task. For the identification task, which we formulate as multi-label classification, we leverage cloze-based prompting to align our objective with the pre-training task of language models, allowing our model to quickly adapt to new event types. We then employ an event type-agnostic sequence labeling model to localize the event trigger conditioned on the identification output. This heterogeneous model design allows P4E to quickly learn new event types without sacrificing the ability to make structured predictions. Our experiments demonstrate the effectiveness of our proposed design, and P4E shows superior performance for few-shot event detection on benchmark datasets FewEvent and MAVEN and comparable performance to SOTA for fully-supervised event detection on ACE.",
        "paperId": "019cb62ec0e5192d9950a21ead39afc4c70e8045"
    },
    {
        "title": "Controllable Generation of Dialogue Acts for Dialogue Systems via Few-Shot Response Generation and Ranking",
        "firstAuthor": "Angela Ramirez",
        "url": "https://arxiv.org/pdf/2307.14440",
        "dateSubmitted": "2023-07-26",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Dialogue systems need to produce responses that realize multiple types of dialogue acts (DAs) with high semantic fidelity. In the past, natural language generators (NLGs) for dialogue were trained on large parallel corpora that map from a domain-specific DA and its semantic attributes to an output utterance. Recent work shows that pretrained language models (LLMs) offer new possibilities for controllable NLG using prompt-based learning. Here we develop a novel few-shot overgenerate-and-rank approach that achieves the controlled generation of DAs. We compare eight few-shot prompt styles that include a novel method of generating from textual pseudo-references using a textual style transfer approach. We develop six automatic ranking functions that identify outputs with both the correct DA and high semantic accuracy at generation time. We test our approach on three domains and four LLMs. To our knowledge, this is the first work on NLG for dialogue that automatically ranks outputs using both DA and attribute accuracy. For completeness, we compare our results to fine-tuned few-shot models trained with 5 to 100 instances per DA. Our results show that several prompt settings achieve perfect DA accuracy, and near perfect semantic accuracy (99.81%) and perform better than few-shot fine-tuning.",
        "paperId": "03d8b1e78d124a561f3c2a67d3199472ee73228d"
    },
    {
        "title": "LAMBADA: Backward Chaining for Automated Reasoning in Natural Language",
        "firstAuthor": "Seyed Mehran Kazemi",
        "url": "http://arxiv.org/pdf/2212.13894",
        "dateSubmitted": "2022-12-20",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules, that are simply implemented by few-shot prompted LLM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.",
        "paperId": "03fb95e6be583ca954c3d00812a9e9a40f118e51"
    },
    {
        "title": "Skill-Based Few-Shot Selection for In-Context Learning",
        "firstAuthor": "Shengnan An",
        "url": "https://arxiv.org/pdf/2305.14210",
        "dateSubmitted": "2023-05-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "In-context learning is the paradigm that adapts large language models to downstream tasks by providing a few examples. Few-shot selection -- selecting appropriate examples for each test instance separately -- is important for in-context learning. In this paper, we propose Skill-KNN, a skill-based few-shot selection method for in-context learning. The key advantages of Skill-KNN include: (1) it addresses the problem that existing methods based on pre-trained embeddings can be easily biased by surface natural language features that are not important for the target task; (2) it does not require training or fine-tuning of any models, making it suitable for frequently expanding or changing example banks. The key insight is to optimize the inputs fed into the embedding model, rather than tuning the model itself. Technically, Skill-KNN generates the skill-based descriptions for each test case and candidate example by utilizing a pre-processing few-shot prompting, thus eliminating unimportant surface features. Experimental results across five cross-domain semantic parsing datasets and six backbone models show that Skill-KNN significantly outperforms existing methods.",
        "paperId": "04526876688e5a56106629229309fae272da1c79"
    },
    {
        "title": "EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning",
        "firstAuthor": "Rajasekhar Reddy Mekala",
        "url": "https://arxiv.org/pdf/2309.10687",
        "dateSubmitted": "2023-09-16",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Language models are achieving impressive performance on various tasks by aggressively adopting inference-time prompting techniques, such as zero-shot and few-shot prompting. In this work, we introduce EchoPrompt, a simple yet effective approach that prompts the model to rephrase its queries before answering them. EchoPrompt is adapted for both zero-shot and few-shot in-context learning with standard and chain-of-thought prompting. Experimental results show that EchoPrompt yields substantial improvements across all these settings for four families of causal language models. These improvements are observed across various numerical reasoning (e.g. GSM8K, SVAMP), reading comprehension (e.g. DROP), and logical reasoning (e.g. Coin Flipping) tasks. On average, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002 by 5% in numerical tasks and 13% in reading comprehension tasks. We investigate the factors contributing to EchoPrompt's effectiveness through ablation studies, which reveal that both the original query and the model-generated rephrased version are instrumental in its performance gains. Our empirical results indicate that EchoPrompt is an effective technique that enhances in-context learning performance. We recommend incorporating EchoPrompt into various baseline prompting strategies to achieve performance boosts.",
        "paperId": "04e838c16f3d1fb8d69d34fe0a0a92c59717875b"
    },
    {
        "title": "Unveiling the potential of large language models in generating semantic and cross-language clones",
        "firstAuthor": "Palash R. Roy",
        "url": "https://arxiv.org/pdf/2309.06424",
        "dateSubmitted": "2023-09-12",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Semantic and Cross-language code clone generation may be useful for code reuse, code comprehension, refactoring and benchmarking. OpenAI's GPT model has potential in such clone generation as GPT is used for text generation. When developers copy/paste codes from Stack Overflow (SO) or within a system, there might be inconsistent changes leading to unexpected behaviours. Similarly, if someone possesses a code snippet in a particular programming language but seeks equivalent functionality in a different language, a semantic cross-language code clone generation approach could provide valuable assistance.In this study, using SemanticCloneBench as a vehicle, we evaluated how well the GPT-3 model could help generate semantic and cross-language clone variants for a given fragment.We have comprised a diverse set of code fragments and assessed GPT-3s performance in generating code variants.Through extensive experimentation and analysis, where 9 judges spent 158 hours to validate, we investigate the model's ability to produce accurate and semantically correct variants. Our findings shed light on GPT-3's strengths in code generation, offering insights into the potential applications and challenges of using advanced language models in software development. Our quantitative analysis yields compelling results. In the realm of semantic clones, GPT-3 attains an impressive accuracy of 62.14% and 0.55 BLEU score, achieved through few-shot prompt engineering. Furthermore, the model shines in transcending linguistic confines, boasting an exceptional 91.25% accuracy in generating cross-language clones",
        "paperId": "073972fa0de48db1304509041e877e568c94e7de"
    },
    {
        "title": "QAmeleon: Multilingual QA with Only 5 Examples",
        "firstAuthor": "Priyanka Agrawal",
        "url": "https://arxiv.org/pdf/2211.08264",
        "dateSubmitted": "2022-11-15",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "The availability of large, high-quality datasets has been one of the main drivers of recent progress in question answering (QA). Such annotated datasets however are difficult and costly to collect, and rarely exist in languages other than English, rendering QA technology inaccessible to underrepresented languages. An alternative to building large monolingual training datasets is to leverage pre-trained language models (PLMs) under a few-shot learning setting. Our approach, QAmeleon, uses a PLM to automatically generate multilingual data upon which QA models are trained, thus avoiding costly annotation. Prompt tuning the PLM for data synthesis with only five examples per language delivers accuracy superior to translation-based baselines, bridges nearly 60% of the gap between an English-only baseline and a fully supervised upper bound trained on almost 50,000 hand labeled examples, and always leads to substantial improvements compared to fine-tuning a QA model directly on labeled examples in low resource settings. Experiments on the TyDiQA-GoldP and MLQA benchmarks show that few-shot prompt tuning for data synthesis scales across languages and is a viable alternative to large-scale annotation.",
        "paperId": "0783c214623c18f6a8ad96b8eaf4a67a382e68ee"
    },
    {
        "title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
        "firstAuthor": "Tushar Khot",
        "url": "http://arxiv.org/pdf/2210.02406",
        "dateSubmitted": "2022-10-05",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.",
        "paperId": "07955e96cbd778d0ae2a68f09d073b866dd84c2a"
    },
    {
        "title": "Improved Compositional Generalization by Generating Demonstrations for Meta-Learning",
        "firstAuthor": "Sam Spilsbury",
        "url": "http://arxiv.org/pdf/2305.13092",
        "dateSubmitted": "2023-05-22",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Meta-learning and few-shot prompting are viable methods to induce certain types of compositional behaviour. However, these methods can be very sensitive to the choice of support examples used. Choosing good supports from the training data for a given test query is already a difficult problem, but in some cases solving this may not even be enough. We consider a grounded language learning problem (gSCAN) where good support examples for certain test splits might not even exist in the training data, or would be infeasible to search for. We design an agent which instead generates possible supports which are relevant to the test query and current state of the world, then uses these supports via meta-learning to solve the test query. We show substantially improved performance on a previously unsolved compositional behaviour split without a loss of performance on other splits. Further experiments show that in this case, searching for relevant demonstrations even with an oracle function is not sufficient to attain good performance when using meta-learning.",
        "paperId": "088ba3cfb904ccd0aa1993a1e30c725b061aad7e"
    },
    {
        "title": "More Samples or More Prompt Inputs? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering",
        "firstAuthor": "Bingsheng Yao",
        "url": null,
        "dateSubmitted": "2023-11-16",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "While most existing works on LLM prompt-engineering focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can't we design and leverage multiple prompt inputs together to further improve the LLM performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompt-engineering technique to produce the most confident prediction results by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with two SOTA LLMs (FlanT5-XL and Mistral-7B) on three NLI datasets (e-SNLI, Multi-NLI, and ANLI) illustrate that ICS can consistently enhance LLM's prediction performance and confidence. An ablation study suggests that a diversity-based ICS strategy may further improve LLM's performance, which sheds light on a new yet promising future research direction.",
        "paperId": "0ab79543d98e375b9de1354766c024e165cc2369"
    },
    {
        "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
        "firstAuthor": "Yao Lu",
        "url": "https://aclanthology.org/2022.acl-long.556.pdf",
        "dateSubmitted": "2021-04-18",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are \u201cfantastic\u201d and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.",
        "paperId": "0adec918885dff698acf359988ed79a543157f80"
    },
    {
        "title": "Crowd Score: A Method for the Evaluation of Jokes using Large Language Model AI Voters as Judges",
        "firstAuthor": "Fabr\u00edcio G\u00f3es",
        "url": "http://arxiv.org/pdf/2212.11214",
        "dateSubmitted": "2022-12-21",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This paper presents the Crowd Score, a novel method to assess the funniness of jokes using large language models (LLMs) as AI judges. Our method relies on inducing different personalities into the LLM and aggregating the votes of the AI judges into a single score to rate jokes. We validate the votes using an auditing technique that checks if the explanation for a particular vote is reasonable using the LLM. We tested our methodology on 52 jokes in a crowd of four AI voters with different humour types: af\ufb01liative, self-enhancing, aggressive and self-defeating. Our results show that few-shot prompting leads to better results than zero-shot for the voting question. Personality induction showed that aggressive and self-defeating voters are signi\ufb01cantly more inclined to \ufb01nd more jokes funny of a set of aggressive/self-defeating jokes than the af\ufb01liative and self-enhancing voters. The Crowd Score follows the same trend as human judges by assigning higher scores to jokes that are also considered funnier by human judges. We believe that our methodology could be applied to other creative domains such as story, poetry, slogans, etc. It could both help the adoption of a \ufb02exible and accurate standard approach to compare different work in the CC community under a common metric and by minimizing human participation in assessing creative artefacts, it could accelerate the prototyping of creative artefacts and reduce the cost of hiring human participants to rate creative artefacts. 1",
        "paperId": "0ba5fb80d2c3ea3a8505415e32d954b4e4eea170"
    },
    {
        "title": "ART: Automatic multi-step reasoning and tool-use for large language models",
        "firstAuthor": "Bhargavi Paranjape",
        "url": "http://arxiv.org/pdf/2303.09014",
        "dateSubmitted": "2023-03-16",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multi-step reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks. ART is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.",
        "paperId": "0d42221038c05cee8443c5b5af838505ee137dc3"
    },
    {
        "title": "Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models",
        "firstAuthor": "Mirac Suzgun",
        "url": "https://arxiv.org/pdf/2205.11503",
        "dateSubmitted": "2022-05-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "We propose a method for arbitrary textual style transfer (TST)\u2014the task of transforming a text into any given style\u2014utilizing general-purpose pre-trained language models. Our method, Prompt-and-Rerank, is based on a mathematical formulation of the TST task, decomposing it into three constituent components: textual similarity, target style strength, and fluency. Our method uses zero-shot or few-shot prompting to obtain a set of candidate generations in the target style, and then re-ranks them according to the three components. Our method enables small pre-trained language models to perform on par with state-of-the-art large-scale models while using two orders of magnitude less compute and memory. We also investigate the effect of model size and prompt design (e.g., prompt paraphrasing and delimiter-pair choice) on style transfer quality across seven diverse textual style transfer datasets, finding, among other things, that delimiter-pair choice has a large impact on performance, and that models have biases on the direction of style transfer.",
        "paperId": "0d6bb585493e34975f0437faa3179db3a02f6ae8"
    },
    {
        "title": "Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models",
        "firstAuthor": "Varun Nair",
        "url": "http://arxiv.org/pdf/2305.05982",
        "dateSubmitted": "2023-05-10",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "A medical provider\u2019s summary of a patient visit serves several critical purposes, including clinical decision-making, facilitating hand-offs between providers, and as a reference for the patient. An effective summary is required to be coherent and accurately capture all the medically relevant information in the dialogue, despite the complexity of patient-generated language. Even minor inaccuracies in visit summaries (for example, summarizing \u201cpatient does not have a fever\u201d when a fever is present) can be detrimental to the outcome of care for the patient.This paper tackles the problem of medical conversation summarization by discretizing the task into several smaller dialogue-understanding tasks that are sequentially built upon. First, we identify medical entities and their affirmations within the conversation to serve as building blocks. We study dynamically constructing few-shot prompts for tasks by conditioning on relevant patient information and use GPT-3 as the backbone for our experiments. We also develop GPT-derived summarization metrics to measure performance against reference summaries quantitatively. Both our human evaluation study and metrics for medical correctness show that summaries generated using this approach are clinically accurate and outperform the baseline approach of summarizing the dialog in a zero-shot, single-prompt setting.",
        "paperId": "0f0a973c6457bcaf7255f891f9b34d658a0a84ae"
    },
    {
        "title": "Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning",
        "firstAuthor": "Mohamed Aghzal",
        "url": "https://arxiv.org/pdf/2310.03249",
        "dateSubmitted": "2023-10-05",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) have achieved remarkable success across a wide spectrum of tasks; however, they still face limitations in scenarios that demand long-term planning and spatial reasoning. To facilitate this line of research, in this work, we propose a new benchmark, termed $\\textbf{P}$ath $\\textbf{P}$lanning from $\\textbf{N}$atural $\\textbf{L}$anguage ($\\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by formulating ''path planning'' tasks that require an LLM to navigate to target locations while avoiding obstacles and adhering to constraints. Leveraging this benchmark, we systematically investigate LLMs including GPT-4 via different few-shot prompting methodologies and BART and T5 of various sizes via fine-tuning. Our experimental results show the promise of few-shot GPT-4 in spatial reasoning, when it is prompted to reason and act interleavedly, although it still fails to make long-term temporal reasoning. In contrast, while fine-tuned LLMs achieved impressive results on in-distribution reasoning tasks, they struggled to generalize to larger environments or environments with more obstacles.",
        "paperId": "107aa1e3b1ce604d953475baf98674e92a723bda"
    },
    {
        "title": "API-Assisted Code Generation for Question Answering on Varied Table Structures",
        "firstAuthor": "Yihan Cao",
        "url": null,
        "dateSubmitted": "2023-10-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation for structured tables as multi-index Pandas data frames, (2) uses Python as a powerful querying language, and (3) uses few-shot prompting to translate NL questions into Python programs, which are executable on Pandas data frames. Furthermore, to answer complex relational questions with extended program functionality and external knowledge, our framework allows customized APIs that Python programs can call. We experiment with four TableQA datasets that involve tables of different structures -- relational, multi-table, and hierarchical matrix shapes -- and achieve prominent improvements over past state-of-the-art systems. In ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM, and (2) demonstrate that our approach is modular and can incorporate additional APIs.",
        "paperId": "133777180e326dfa53523bf53b0a969bbdccb0ee"
    },
    {
        "title": "Learning Performance-Improving Code Edits",
        "firstAuthor": "Aman Madaan",
        "url": "http://arxiv.org/pdf/2302.07867",
        "dateSubmitted": "2023-02-15",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.",
        "paperId": "1786a2f9140ed7211b21302977de64e948b92308"
    },
    {
        "title": "Using ChatGPT with Confidence for Biodiversity-Related Information Tasks",
        "firstAuthor": "Michael Elliott",
        "url": "https://biss.pensoft.net/article/112926/download/pdf/",
        "dateSubmitted": "2023-09-19",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Recent advancements in conversational Artificial Intelligence (AI), such as OpenAI's Chat Generative Pre-Trained Transformer (ChatGPT), present the possibility of using large language models (LLMs) as tools for retrieving, analyzing, and transforming scientific information. We have found that ChatGPT (GPT 3.5) can provide accurate biodiversity knowledge in response to questions about species descriptions, occurrences, and taxonomy, as well as structure information according to data sharing standards such as Darwin Core. A rigorous evaluation of ChatGPT's capabilities in biodiversity-related tasks may help to inform viable use cases for today's LLMs in research and information workflows. In this work, we test the extent of ChatGPT's biodiversity knowledge, characterize its mistakes, and suggest how LLM-based systems might be designed to complete knowledge-based tasks with confidence.\n To test ChatGPT's biodiversity knowledge, we compiled a question-and-answer test set derived from Darwin Core records available in Integrated Digitized Biocollections (iDigBio). Each question focuses on one or more Darwin Core terms to test the model\u2019s ability to recall species occurrence information and its understanding of the standard. The test set covers a range of locations, taxonomic groups, and both common and rare species (defined by the number of records in iDigBio). The results of the tests will be presented. We also tested ChatGPT on generative tasks, such as creating species occurrence maps. A visual comparison of the maps with iDigBio data shows that for some species, ChatGPT can generate fairly accurate representationsof their geographic ranges (Fig. 1).\n ChatGPT's incorrect responses in our tests show several patterns of mistakes. First, responses can be self-conflicting. For example, when asked \"Does Acer saccharum naturally occur in Benton, Oregon?\", ChatGPT responded \"YES, Acer saccharum DOES NOT naturally occur in Benton, Oregon\". ChatGPT can also be misled by semantics in species names. For Rafinesquia neomexicana, the word \"neomexicana\" leads ChatGPT to believe that the species primarily occurs in New Mexico, USA. ChatGPT may also confuse species, such as when attempting to describe a lesser-known species (e.g., a rare bee) within the same genus as a better-known species. Other causes of mistakes include hallucination (Ji et al. 2023), memorization (Chang and Bergen 2023), and user deception (Li et al. 2023).\n Some mistakes may be avoided by prompt engineering, e.g., few-shot prompting (Chang and Bergen 2023) and chain-of-thought prompting (Wei et al. 2022). These techniques assist Large Language Models (LLMs) by clarifying expectations or by guiding recollection. However, such methods cannot help when LLMs lack required knowledge. In these cases, alternative approaches are needed.\n A desired reliability can be theoretically guaranteed if responses that contain mistakes are discarded or corrected. This requires either detecting or predicting mistakes. Sometimes mistakes can be ruled out by verifying responses with a trusted source. For example, a trusted specimen record might be found that corroborates the response. The difficulty, however, is finding such records programmatically; e.g., using iDigBio and Global Biodiversity Information Facility's (GBIF) search Application Programming Interfaces (APIs) requires specifying indexed terms that might not appear in an LLM's response. This presents a secondary problem for which LLMs may be well suited. Note that with presence-only data, it can be difficult to disprove presence claims or prove absence claims.\n Besides verification, mistakes may be predicted using probabilistic methods. Formulating mistake probabilities often relies on heuristics. For example, variability in a model\u2019s responses to a repeated query can be a sign of hallucination (Manakul et al. 2023). In practice, both probabilistic and verification methods may be needed to reach a desired reliability. LLM outputs that can be verified may be directly accepted (or discarded), while others are judged by estimating mistake probabilities. We will consider a set of heuristics and verification methods, and report empirical assessments of their impact on ChatGPT\u2019s reliability.",
        "paperId": "17abf939baa953dd69dfaa4c2af5719217102c11"
    },
    {
        "title": "PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models",
        "firstAuthor": "Minghua Liu",
        "url": "https://arxiv.org/pdf/2212.01558",
        "dateSubmitted": "2022-12-03",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Generalizable 3D part segmentation is important but challenging in vision and robotics. Training deep models via conventional supervised methods requires large-scale 3D datasets with fine-grained part annotations, which are costly to collect. This paper explores an alternative way for low-shot part segmentation of 3D point clouds by leveraging a pretrained image-language model, GLIP. which achieves superior performance on open-vocabulary 2D detection. We transfer the rich knowledge from 2D to 3D through GLIP-based part detection on point cloud rendering and a novel 2D-to-3D label lifting algorithm. We also utilize multi-view 3D priors and few-shot prompt tuning to boost performance significantly. Extensive evaluation on PartNet and PartNet-Mobility datasets shows that our method enables excellent zero-shot 3D part segmentation. Our few-shot version not only outperforms existing few-shot approaches by a large margin but also achieves highly competitive results compared to the fully supervised counterpart. Furthermore, we demonstrate that our method can be directly applied to iPhone-scanned point clouds without significant domain gaps.",
        "paperId": "18493a8c345cf6ca0a7f01a14afa0b9c72d1ff8f"
    },
    {
        "title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
        "firstAuthor": "David Vilar",
        "url": "http://arxiv.org/pdf/2211.09102",
        "dateSubmitted": "2022-11-16",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. We probe this ability in an in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT) performance among similarly-trained LLMs to date. We investigate various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor. Using optimized prompts, we revisit previous assessments of PaLM\u2019s MT capabilities with more recent test sets, modern MT metrics, and human evaluation, and find that its performance, while impressive, still lags that of state-of-the-art supervised systems. We conclude by providing an analysis of PaLM\u2019s MT output which reveals some interesting properties and prospects for future work.",
        "paperId": "197ba7bbfdbb052b0770088815c110774220f397"
    },
    {
        "title": "Can Language Models Understand Physical Concepts?",
        "firstAuthor": "Lei Li",
        "url": "http://arxiv.org/pdf/2305.14057",
        "dateSubmitted": "2023-05-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Language models~(LMs) gradually become general-purpose interfaces in the interactive and embodied world, where the understanding of physical concepts is an essential prerequisite. However, it is not yet clear whether LMs can understand physical concepts in the human world. To investigate this, we design a benchmark VEC that covers the tasks of (i) Visual concepts, such as the shape and material of objects, and (ii) Embodied Concepts, learned from the interaction with the world such as the temperature of objects. Our zero (few)-shot prompting results show that the understanding of certain visual concepts emerges as scaling up LMs, but there are still basic concepts to which the scaling law does not apply. For example, OPT-175B performs close to humans with a zero-shot accuracy of 85\\% on the material concept, yet behaves like random guessing on the mass concept. Instead, vision-augmented LMs such as CLIP and BLIP achieve a human-level understanding of embodied concepts. Analysis indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge. Inspired by this, we propose a distillation method to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up the parameters of LMs 134x. Our dataset is available at \\url{https://github.com/TobiasLee/VEC}",
        "paperId": "1caa2a29d3ca38d0e5111f4f9ae140727bb7d567"
    },
    {
        "title": "Contextual Biasing of Named-Entities with Large Language Models",
        "firstAuthor": "Chuanneng Sun",
        "url": "https://arxiv.org/pdf/2309.00723",
        "dateSubmitted": "2023-09-01",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This paper studies contextual biasing with Large Language Models (LLMs), where during second-pass rescoring additional contextual information is provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We propose to leverage prompts for a LLM without fine tuning during rescoring which incorporate a biasing list and few-shot examples to serve as additional information when calculating the score for the hypothesis. In addition to few-shot prompt learning, we propose multi-task training of the LLM to predict both the entity class and the next token. To improve the efficiency for contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we propose dynamic prompting, where we select the most likely class using the class tag prediction, and only use entities in this class as contexts for next token prediction. Word Error Rate (WER) evaluation is performed on i) an internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli dataset. Results indicate that biasing lists and few-shot examples can achieve 17.8% and 9.6% relative improvement compared to first pass ASR, and that multi-task training and dynamic prompting can achieve 20.0% and 11.3% relative WER improvement, respectively.",
        "paperId": "1ed5d06c4dc46e6a983597b740ab0a31d0ce22ad"
    },
    {
        "title": "MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning",
        "firstAuthor": "Bohan Li",
        "url": "http://arxiv.org/pdf/2304.09402",
        "dateSubmitted": "2023-04-19",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Prompt-based learning reformulates downstream tasks as cloze problems by combining the original input with a template. This technique is particularly useful in few-shot learning, where a model is trained on a limited amount of data. However, the limited templates and text used in few-shot prompt-based learning still leave significant room for performance improvement. Additionally, existing methods using model ensembles can constrain the model efficiency. To address these issues, we propose an augmentation method called MixPro, which augments both the vanilla input text and the templates through token-level, sentence-level, and epoch-level Mixup strategies. We conduct experiments on five few-shot datasets, and the results show that MixPro outperforms other augmentation baselines, improving model performance by an average of 5.08% compared to before augmentation.",
        "paperId": "1f0dfbbc13ac31de8709bbb4d0f6478aa1222cef"
    },
    {
        "title": "MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting",
        "firstAuthor": "Oscar Ma\u00f1as",
        "url": "http://arxiv.org/pdf/2210.07179",
        "dateSubmitted": "2022-10-13",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large pre-trained models have proved to be remarkable zero- and (prompt-based) few-shot learners in unimodal vision and language tasks. We propose MAPL, a simple and parameter-efficient method that reuses frozen pre-trained unimodal models and leverages their strong generalization capabilities in multimodal vision-language (VL) settings. MAPL learns a lightweight mapping between the representation spaces of unimodal models using aligned image-text data, and can generalize to unseen VL tasks from just a few in-context examples. The small number of trainable parameters makes MAPL effective at low-data and in-domain learning. Moreover, MAPL\u2019s modularity enables easy extension to other pre-trained models. Extensive experiments on several visual question answering and image captioning benchmarks show that MAPL achieves superior or competitive performance compared to similar methods while training orders of magnitude fewer parameters. MAPL can be trained in just a few hours using modest computational resources and public datasets. We release our code and pre-trained model weights at https://github.com/oscmansan/mapl.",
        "paperId": "1f86bf1e334200ec0481349255559fbfe7a33caa"
    },
    {
        "title": "MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models",
        "firstAuthor": "Kailai Yang",
        "url": null,
        "dateSubmitted": "2023-09-24",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "With the development of web technology, social media texts are becoming a rich source for automatic mental health analysis. As traditional discriminative methods bear the problem of low interpretability, the recent large language models have been explored for interpretable mental health analysis on social media, which aims to provide detailed explanations along with predictions. The results show that ChatGPT can generate approaching-human explanations for its correct classifications. However, LLMs still achieve unsatisfactory classification performance in a zero-shot/few-shot manner. Domain-specific finetuning is an effective solution, but faces 2 challenges: 1) lack of high-quality training data. 2) no open-source LLMs for interpretable mental health analysis were released to lower the finetuning cost. To alleviate these problems, we build the first multi-task and multi-source interpretable mental health instruction (IMHI) dataset on social media, with 105K data samples. The raw social media data are collected from 10 existing sources covering 8 mental health analysis tasks. We use expert-written few-shot prompts and collected labels to prompt ChatGPT and obtain explanations from its responses. To ensure the reliability of the explanations, we perform strict automatic and human evaluations on the correctness, consistency, and quality of generated data. Based on the IMHI dataset and LLaMA2 foundation models, we train MentalLLaMA, the first open-source LLM series for interpretable mental health analysis with instruction-following capability. We also evaluate the performance of MentalLLaMA on the IMHI evaluation benchmark with 10 test sets, where their correctness for making predictions and the quality of explanations are examined. The results show that MentalLLaMA approaches state-of-the-art discriminative methods in correctness and generates high-quality explanations.",
        "paperId": "1fa0a012a83348c02a113ec18e1a165e273402f2"
    },
    {
        "title": "DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines",
        "firstAuthor": "O. Khattab",
        "url": "https://arxiv.org/pdf/2310.03714",
        "dateSubmitted": "2023-10-05",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded\"prompt templates\", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot prompting (generally by over 25% and 65%, respectively) and pipelines with expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at https://github.com/stanfordnlp/dspy",
        "paperId": "2069aaaa281eb13bcd9330fc4d43f24f6b436a53"
    },
    {
        "title": "SPARSEFIT: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations",
        "firstAuthor": "Jesus Solano",
        "url": "http://arxiv.org/pdf/2305.13235",
        "dateSubmitted": "2023-05-22",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Explaining the decisions of neural models is crucial for ensuring their trustworthiness at deployment time. Using Natural Language Explanations (NLEs) to justify a model's predictions has recently gained increasing interest. However, this approach usually demands large datasets of human-written NLEs for the ground-truth answers, which are expensive and potentially infeasible for some applications. For models to generate high-quality NLEs when only a few NLEs are available, the fine-tuning of Pre-trained Language Models (PLMs) in conjunction with prompt-based learning recently emerged. However, PLMs typically have billions of parameters, making fine-tuning expensive. We propose SparseFit, a sparse few-shot fine-tuning strategy that leverages discrete prompts to jointly generate predictions and NLEs. We experiment with SparseFit on the T5 model and four datasets and compare it against state-of-the-art parameter-efficient fine-tuning techniques. We perform automatic and human evaluations to assess the quality of the model-generated NLEs, finding that fine-tuning only 6.8% of the model parameters leads to competitive results for both the task performance and the quality of the NLEs.",
        "paperId": "22cc280a0a239da49db0bec745b8fb3caa0e7a67"
    },
    {
        "title": "InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations",
        "firstAuthor": "Nils Feldhus",
        "url": "https://arxiv.org/pdf/2310.05592",
        "dateSubmitted": "2023-10-09",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, add new NLP-specific operations such as free-text rationalization, and illustrate its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel Adapter-based approach. We then conduct two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e. how objectively helpful dialogical explanations are for humans in figuring out the model's predicted label when it's not shown. We found rationalization and feature attribution were helpful in explaining the model behavior. Moreover, users could more reliably predict the model outcome based on an explanation dialogue rather than one-off explanations.",
        "paperId": "2522410b1cac0c14fa656a0aaeaff08bacb358a9"
    },
    {
        "title": "Multi-lingual Evaluation of Code Generation Models",
        "firstAuthor": "Ben Athiwaratkun",
        "url": "http://arxiv.org/pdf/2210.14868",
        "dateSubmitted": "2022-10-27",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "We present new benchmarks on evaluation code generation models: MBXP and Multilingual HumanEval, and MathQA-X. These datasets cover over 10 programming languages and are generated using a scalable conversion framework that transpiles prompts and test cases from the original Python datasets into the corresponding data in the target language. Using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of few-shot prompting to teach the model new languages, and zero-shot translation abilities even on mono-lingual settings. Furthermore, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks. Overall, our benchmarks represents a significant step towards a deeper understanding of language models' code generation abilities. We publicly release our code and datasets at https://github.com/amazon-research/mxeval.",
        "paperId": "2577d053f8aab912d29b424e1f09133d83740fd2"
    },
    {
        "title": "Dissecting In-Context Learning of Translations in GPTs",
        "firstAuthor": "Vikas Raunak",
        "url": null,
        "dateSubmitted": "2023-10-24",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Most of the recent work in leveraging Large Language Models (LLMs) such as GPT-3 for Machine Translation (MT) has focused on selecting the few-shot samples for prompting. In this work, we try to better understand the role of demonstration attributes for the in-context learning of translations through perturbations of high-quality, in-domain demonstrations. We find that asymmetric perturbation of the source-target mappings yield vastly different results. We show that the perturbation of the source side has surprisingly little impact, while target perturbation can drastically reduce translation quality, suggesting that it is the output text distribution that provides the most important learning signal during in-context learning of translations. We propose a method named Zero-Shot-Context to add this signal automatically in Zero-Shot prompting. We demonstrate that it improves upon the zero-shot translation performance of GPT-3, even making it competitive with few-shot prompted translations.",
        "paperId": "281f8411654a8bbca3b15dd95fdba42e03d6d666"
    },
    {
        "title": "S POTLIGHT : M OBILE UI U NDERSTANDING USING V ISION -L ANGUAGE M ODELS WITH A F OCUS",
        "firstAuthor": "Gang Li",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Mobile UI understanding is important for enabling various interaction tasks such as UI automation and accessibility. Previous mobile UI modeling often depends on the view hierarchy information of a screen, which directly provides the structural data of the UI, with the hope to bypass challenging tasks of visual modeling from screen pixels. However, view hierarchies are not always available, and are often corrupted with missing object descriptions or misaligned structure information. As a result, despite the use of view hierarchies could offer short-term gains, it may ultimately hinder the applicability and performance of the model. In this paper, we propose Spotlight, a vision-only approach for mobile UI understanding. Specifically, we enhance a vision-language model that only takes the screenshot of the UI and a region of interest on the screen\u2014the focus\u2014as the input. This general architecture of Spotlight is easily scalable and capable of performing a range of UI modeling tasks. Our experiments show that our model establishes SoTA results on several representative UI tasks and outperforms previous methods that use both screenshots and view hierarchies as inputs. Furthermore, we explore multi-task learning and few-shot prompting capacities of the proposed models, demonstrating promising results in the multi-task learning direction.",
        "paperId": "290400d20170480edb36399cafe0f2fad510c635"
    },
    {
        "title": "Towards using Few-Shot Prompt Learning for Automating Model Completion",
        "firstAuthor": "Meriem Ben Chaaben",
        "url": "https://arxiv.org/pdf/2212.03404",
        "dateSubmitted": "2022-12-07",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "We propose a simple yet a novel approach to improve completion in domain modeling activities. Our approach exploits the power of large language models by using few-shot prompt learning without the need to train or fine-tune those models with large datasets that are scarce in this field. We implemented our approach and tested it on the completion of static and dynamic domain diagrams. Our initial evaluation shows that such an approach is effective and can be integrated in different ways during the modeling activities.",
        "paperId": "2a99239f09e95f4dbccec572d66f4519206762f9"
    },
    {
        "title": "Few-shot Prompting Towards Controllable Response Generation",
        "firstAuthor": "Hsuan Su",
        "url": "http://arxiv.org/pdf/2206.03931",
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Much literature has shown that prompt-based learning is an ef\ufb01cient method to make use of the large pre-trained language model. Recent works also exhibit the possibility of steering a chatbot\u2019s output by plugging in an ap-propriate prompt. Gradient-based methods are often used to perturb the prompts. However, some language models are not even available to the public. In this work, we \ufb01rst explored the combination of prompting and reinforcement learning (RL) to steer models\u2019 generation without accessing any of the models\u2019 parameters. Second, to reduce the training effort and enhance the generalizability to the unseen task, we apply multi-task learning to make the model learn to generalize to new tasks better. The experiment results show that our proposed method can successfully control several state-of-the-art (SOTA) dialogue models without accessing their parameters. Furthermore, the model demonstrates the strong ability to quickly adapt to an unseen task in fewer steps than the baseline model.",
        "paperId": "308a59020d320f620f34f96c9ecdc187baff9fa1"
    },
    {
        "title": "Better Patching Using LLM Prompting, via Self-Consistency",
        "firstAuthor": "Toufique Ahmed",
        "url": "https://arxiv.org/pdf/2306.00108",
        "dateSubmitted": "2023-05-31",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large Language models (LLMs) can be induced to solve non-trivial problems with \u201cfew-shot\u201d prompts including illustrative problem-solution examples. Now if the few-shots also include \u201cchain of thought\u201d ($\\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a \u201cexplained\u201d solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\\mathcal{S}-C$ (or even $\\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.",
        "paperId": "32426b96ff3c680125bde3b835bfa931288b8ade"
    },
    {
        "title": "\u201cCovid vaccine is against Covid but Oxford vaccine is made at Oxford!\u201d Semantic Interpretation of Proper Noun Compounds",
        "firstAuthor": "Keshav Kolluru",
        "url": "http://arxiv.org/pdf/2210.13039",
        "dateSubmitted": "2022-10-24",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Proper noun compounds, e.g., \u201cCovid vaccine\u201d, convey information in a succinct manner (a \u201cCovid vaccine\u201d is a \u201cvaccine that immunizes against the Covid disease\u201d). These are commonly used in short-form domains, such as news headlines, but are largely ignored in information-seeking applications. To address this limitation, we release a new manually annotated dataset, ProNCI, consisting of 22.5K proper noun compounds along with their free-form semantic interpretations. ProNCI is 60 times larger than prior noun compound datasets and also includes non-compositional examples, which have not been previously explored. We experiment with various neural models for automatically generating the semantic interpretations from proper noun compounds, ranging from few-shot prompting to supervised learning, with varying degrees of knowledge about the constituent nouns. We find that adding targeted knowledge, particularly about the common noun, results in performance gains of upto 2.8%. Finally, we integrate our model generated interpretations with an existing Open IE system and observe an 7.5% increase in yield at a precision of 85%. The dataset and code are available at https://github.com/dair-iitd/pronci.",
        "paperId": "33285e02758788b681754d283df20971fef6e31f"
    },
    {
        "title": "Workshop On Large Language Models' Interpretability and Trustworthiness (LLMIT)",
        "firstAuthor": "Tulika Saha",
        "url": "https://dl.acm.org/doi/pdf/10.1145/3583780.3615311",
        "dateSubmitted": "2023-10-21",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs), when scaled from millions to billions of parameters, have been demonstrated to exhibit the so-called 'emergence' effect, in that they are not only able to produce semantically correct and coherent text, but are also able to adapt themselves surprisingly well with small changes in contexts supplied as inputs (commonly called prompts). Despite producing semantically coherent and potentially relevant text for a given context, LLMs are vulnerable to yield incorrect information. This misinformation generation, or the so-called hallucination problem of an LLM, gets worse when an adversary manipulates the prompts to their own advantage, e.g., generating false propaganda to disrupt communal harmony, generating false information to trap consumers with target consumables etc. Not only does the consumption of an LLM-generated hallucinated content by humans pose societal threats, such misinformation, when used as prompts, may lead to detrimental effects for in-context learning (also known as few-shot prompt learning). With reference to the above-mentioned problems of LLM usage, we argue that it is necessary to foster research on topics related to not only identifying misinformation from LLM-generated content, but also to mitigate the propagation effects of this generated misinformation on downstream predictive tasks thus leading to more robust and effective leveraging in-context learning.",
        "paperId": "339a9ef4187c33e9cc7ee867c7ce98a1f6f43735"
    },
    {
        "title": "Fairness-guided Few-shot Prompting for Large Language Models",
        "firstAuthor": "Huan Ma",
        "url": "http://arxiv.org/pdf/2303.13217",
        "dateSubmitted": "2023-03-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context learning. We perform comprehensive experiments with state-of-the-art mainstream models such as GPT-3 on various downstream tasks. Our results indicate that our method can enhance the model's in-context learning performance in an effective and interpretable manner.",
        "paperId": "3436ff7a1dd4c6547ba78968d3eec2545a6dccb9"
    },
    {
        "title": "Training Large-scale Foundation Models on Emerging AI Chips",
        "firstAuthor": "Aashiq Muhamed",
        "url": null,
        "dateSubmitted": "2023-08-04",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Foundation models such as ChatGPT and GPT-4 have garnered significant interest from both academia and industry due to their emergent capabilities, such as few-shot prompting, multi-step reasoning, instruction following, and model calibration. Such capabilities were previously only attainable with specially designed models, such as those using knowledge graphs, but can now be achieved on a much larger scale with foundation models. As the capabilities of foundation models have increased, so too have their sizes at a rate much faster than Moore's law. For example, the BERT large model was initially released as a 334M model in 2018, and by 2023, the largest GPT-4 models are estimated to range between 200-300B, representing an increase of three orders of magnitude in just five years. The training of foundation models requires massive computing power. For instance, training a BERT model on a single state-of-the-art GPU machine with multi-A100 chips can take several days, while training GPT-3 models on a large multi-instance GPU cluster can take several months to complete the estimated 3 X 1023 flops. This tutorial provides an overview of the latest progress in supporting foundation model training and inference with new AI chips. It reviews progress on the modeling side, with an emphasis on the transformer architecture, and presents the system architecture supporting training and serving foundation models. This includes programming language frameworks such as PyTorch and Tensorflow, graph compilers, 3D parallelisms, and accelerators such as the GPU H100, TPU, and Trainium. Finally, the tutorial presents our experience of training foundation models using different systems.",
        "paperId": "35245c451dd4c397528aa6ab6ec4fc158d311869"
    },
    {
        "title": "Large Language Model Augmented Narrative Driven Recommendations",
        "firstAuthor": "Sheshera Mysore",
        "url": "https://arxiv.org/pdf/2306.02250",
        "dateSubmitted": "2023-06-04",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Narrative-driven recommendation (NDR) presents an information access problem where users solicit recommendations with verbose descriptions of their preferences and context, for example, travelers soliciting recommendations for points of interest while describing their likes/dislikes and travel circumstances. These requests are increasingly important with the rise of natural language-based conversational interfaces for search and recommendation systems. However, NDR lacks abundant training data for models, and current platforms commonly do not support these requests. Fortunately, classical user-item interaction datasets contain rich textual data, e.g., reviews, which often describe user preferences and context \u2013 this may be used to bootstrap training for NDR models. In this work, we explore using large language models (LLMs) for data augmentation to train NDR models. We use LLMs for authoring synthetic narrative queries from user-item interactions with few-shot prompting and train retrieval models for NDR on synthetic queries and user-item interaction data. Our experiments demonstrate that this is an effective strategy for training small-parameter retrieval models that outperform other retrieval and LLM baselines for narrative-driven recommendation.",
        "paperId": "3566e1245bfc90096fe0cdb8b18674da6519c8d6"
    },
    {
        "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT",
        "firstAuthor": "Ce Zhou",
        "url": "http://arxiv.org/pdf/2302.09419",
        "dateSubmitted": "2023-02-18",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence.",
        "paperId": "3599a236f285af48782fc30b1341d13ec7320735"
    },
    {
        "title": "Multilingual Social Media Text Generation and Evaluation with Few-Shot Prompting",
        "firstAuthor": "Mack Blackburn",
        "url": "https://aclanthology.org/2022.gem-1.39.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This work adapts large language models to generate multilingual social media text that meets several objectives simultaneously: topic relevance, author style consistency, and reply validity. Leveraging existing online information behavior simulators, which currently only forecast activities but not content, our approach comprised of generalizable prompt formation and efficient evaluation to produce a believable, personalized, and responsive synthetic social network. According to some preliminary experiments, our multi-objective prompt formation and automatic evaluation/selection methods are able to yield a significant number of high-quality synthetic texts according to both standardized and trained metrics.",
        "paperId": "36731d3f9809535d5f57cc5cd610d92428a50716"
    },
    {
        "title": "Language Model Crossover: Variation through Few-Shot Prompting",
        "firstAuthor": "Elliot Meyerson",
        "url": "https://arxiv.org/pdf/2302.12170",
        "dateSubmitted": "2023-02-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This paper pursues the insight that language models naturally enable an intelligent variation operator similar in spirit to evolutionary crossover. In particular, language models of sufficient scale demonstrate in-context learning, i.e. they can learn from associations between a small number of input patterns to generate outputs incorporating such associations (also called few-shot prompting). This ability can be leveraged to form a simple but powerful variation operator, i.e. to prompt a language model with a few text-based genotypes (such as code, plain-text sentences, or equations), and to parse its corresponding output as those genotypes' offspring. The promise of such language model crossover (which is simple to implement and can leverage many different open-source language models) is that it enables a simple mechanism to evolve semantically-rich text representations (with few domain-specific tweaks), and naturally benefits from current progress in language models. Experiments in this paper highlight the versatility of language-model crossover, through evolving binary bit-strings, sentences, equations, text-to-image prompts, and Python code. The conclusion is that language model crossover is a promising method for evolving genomes representable as text.",
        "paperId": "3841234dd49250c4fcbba79eed6593d3b57932c1"
    },
    {
        "title": "MathAttack: Attacking Large Language Models Towards Math Solving Ability",
        "firstAuthor": "Zihao Zhou",
        "url": "https://arxiv.org/pdf/2309.01686",
        "dateSubmitted": "2023-09-04",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "With the boom of Large Language Models (LLMs), the research of solving Math Word Problem (MWP) has recently made great progress. However, there are few studies to examine the security of LLMs in math solving ability. Instead of attacking prompts in the use of LLMs, we propose a MathAttack model to attack MWP samples which are closer to the essence of security in solving math problems. Compared to traditional text adversarial attack, it is essential to preserve the mathematical logic of original MWPs during the attacking. To this end, we propose logical entity recognition to identify logical entries which are then frozen. Subsequently, the remaining text are attacked by adopting a word-level attacker. Furthermore, we propose a new dataset RobustMath to evaluate the robustness of LLMs in math solving ability. Extensive experiments on our RobustMath and two another math benchmark datasets GSM8K and MultiAirth show that MathAttack could effectively attack the math solving ability of LLMs. In the experiments, we observe that (1) Our adversarial samples from higher-accuracy LLMs are also effective for attacking LLMs with lower accuracy (e.g., transfer from larger to smaller-size LLMs, or from few-shot to zero-shot prompts); (2) Complex MWPs (such as more solving steps, longer text, more numbers) are more vulnerable to attack; (3) We can improve the robustness of LLMs by using our adversarial samples in few-shot prompts. Finally, we hope our practice and observation can serve as an important attempt towards enhancing the robustness of LLMs in math solving ability. We will release our code and dataset.",
        "paperId": "3886f3bd2a0af9e75bf9fa5b7db4224969dbf346"
    },
    {
        "title": "Multistage Collaborative Knowledge Distillation from Large Language Models",
        "firstAuthor": "Jiachen Zhao",
        "url": null,
        "dateSubmitted": "2023-11-15",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "We study semi-supervised sequence prediction tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from a prompted LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we propose a new distillation method, multistage collaborative knowledge distillation from an LLM (MCKD), for such tasks. MCKD first prompts an LLM using few-shot in-context learning to produce pseudolabels for unlabeled data. Then, at each stage of distillation, a pair of students are trained on disjoint partitions of the pseudolabeled data. Each student subsequently produces new and improved pseudolabels for the unseen partition to supervise the next round of student(s) with. We show the benefit of multistage cross-partition labeling on two constituency parsing tasks. On CRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches the performance of supervised finetuning with 500 examples and outperforms the prompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively.",
        "paperId": "39f0d1b894130852ee9f39a5df58905a09645c81"
    },
    {
        "title": "FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models",
        "firstAuthor": "Liwen Zhang",
        "url": "https://arxiv.org/pdf/2308.09975",
        "dateSubmitted": "2023-08-19",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) have demonstrated exceptional performance in various natural language processing tasks, yet their efficacy in more challenging and domain-specific tasks remains largely unexplored. This paper presents FinEval, a benchmark specifically designed for the financial domain knowledge in the LLMs. FinEval is a collection of high-quality multiple-choice questions covering Finance, Economy, Accounting, and Certificate. It includes 4,661 questions spanning 34 different academic subjects. To ensure a comprehensive model performance evaluation, FinEval employs a range of prompt types, including zero-shot and few-shot prompts, as well as answer-only and chain-of-thought prompts. Evaluating state-of-the-art Chinese and English LLMs on FinEval, the results show that only GPT-4 achieved an accuracy close to 70% in different prompt settings, indicating significant growth potential for LLMs in the financial domain knowledge. Our work offers a more comprehensive financial knowledge evaluation benchmark, utilizing data of mock exams and covering a wide range of evaluated LLMs.",
        "paperId": "3b88526a0f0337e3a6b632b4af8fd0882eb4b470"
    },
    {
        "title": "Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning",
        "firstAuthor": "Xiangyu Peng",
        "url": "http://arxiv.org/pdf/2210.12587",
        "dateSubmitted": "2022-10-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Prompt tuning approaches, which learn task-specific soft prompts for a downstream task conditioning on frozen pre-trained models, have attracted growing interest due to its parameter efficiency. With large language models and sufficient training data, prompt tuning performs comparably to full-model tuning. However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning. In this work, we focus on improving the few-shot performance of prompt tuning by transferring knowledge from soft prompts of source tasks. Recognizing the good generalization capabilities of ensemble methods in low-data regime, we first experiment and show that a simple ensemble of model predictions based on different source prompts, outperforms existing multi-prompt knowledge transfer approaches such as source prompt fusion in the few-shot setting. Motivated by this observation, we further investigate model ensembles and propose Sample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the contribution of each source model for each target sample separately when ensembling source model outputs. Through this way, SESoM inherits the superior generalization of model ensemble approaches and simultaneously captures the sample-specific competence of each source prompt. We conduct experiments across a diverse set of eight NLP tasks using models of different scales (T5-{base, large, XL}) and find that SESoM consistently outperforms the existing models of the same as well as larger parametric scale by a large margin.",
        "paperId": "3d7d385d9ee75a286e8da27f7d3cf9f12651c899"
    },
    {
        "title": "An Early Evaluation of GPT-4V(ision)",
        "firstAuthor": "Yang Wu",
        "url": null,
        "dateSubmitted": "2023-10-25",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "In this paper, we evaluate different abilities of GPT-4V including visual understanding, language understanding, visual puzzle solving, and understanding of other modalities such as depth, thermal, video, and audio. To estimate GPT-4V's performance, we manually construct 656 test instances and carefully evaluate the results of GPT-4V. The highlights of our findings are as follows: (1) GPT-4V exhibits impressive performance on English visual-centric benchmarks but fails to recognize simple Chinese texts in the images; (2) GPT-4V shows inconsistent refusal behavior when answering questions related to sensitive traits such as gender, race, and age; (3) GPT-4V obtains worse results than GPT-4 (API) on language understanding tasks including general language understanding benchmarks and visual commonsense knowledge evaluation benchmarks; (4) Few-shot prompting can improve GPT-4V's performance on both visual understanding and language understanding; (5) GPT-4V struggles to find the nuances between two similar images and solve the easy math picture puzzles; (6) GPT-4V shows non-trivial performance on the tasks of similar modalities to image, such as video and thermal. Our experimental results reveal the ability and limitations of GPT-4V and we hope our paper can provide some insights into the application and research of GPT-4V.",
        "paperId": "3df2b2a90ed3e3db6abc6c0d36165d1778cdcb37"
    },
    {
        "title": "EvoPrompting: Language Models for Code-Level Neural Architecture Search",
        "firstAuthor": "Angelica Chen",
        "url": "https://arxiv.org/pdf/2302.14838",
        "dateSubmitted": "2023-02-28",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.",
        "paperId": "411b16add23976ffcdf6422f932453f6ebcca119"
    },
    {
        "title": "Code as Policies: Language Model Programs for Embodied Control",
        "firstAuthor": "Jacky Liang",
        "url": "https://arxiv.org/pdf/2209.07753",
        "dateSubmitted": "2022-09-16",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (\u2018faster\u2019) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io",
        "paperId": "41531594d7e0f3b2e138ae43e0a0f6e24a9b014c"
    },
    {
        "title": "Towards More Generalizable and Accurate Sentence Classification in Medical Abstracts with Less Data",
        "firstAuthor": "Yan Hu",
        "url": "https://www.researchsquare.com/article/rs-2026270/latest.pdf",
        "dateSubmitted": "2023-08-30",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": null,
        "paperId": "437b0c6218bd06a0730e1b29829fa299e72b4a0b"
    },
    {
        "title": "Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models",
        "firstAuthor": "Cheng-Yu Hsieh",
        "url": "https://arxiv.org/pdf/2308.00675",
        "dateSubmitted": "2023-08-01",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage. Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen. Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide. As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable. Our work provides an alternative to demonstrations: tool documentation. We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations. We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities. First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts. Second, on a newly collected realistic tool-use dataset with hundreds of available tool APIs, we show that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation. Third, we highlight the benefits of tool documentations by tackling image generation and video tracking using just-released unseen state-of-the-art models as tools. Finally, we highlight the possibility of using tool documentation to automatically enable new applications: by using nothing more than the documentation of GroundingDino, Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the just-released Grounded-SAM and Track Anything models.",
        "paperId": "446fb5dead075a1a08862662738f462e9a0e91c8"
    },
    {
        "title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",
        "firstAuthor": "Aman Madaan",
        "url": "http://arxiv.org/pdf/2209.07686",
        "dateSubmitted": "2022-09-16",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "The past decade has witnessed dramatic gains in natural language processing and an unprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (CoT) prompting. Specifically, CoT pushes the performance of large language models in a few-shot setup by augmenting the prompts with intermediate steps. Despite impressive results across various tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of CoT-based few-shot prompting mechanisms in large language models. We first systematically identify and define the key components of a prompt: symbols, patterns, and text. Then, we devise and conduct an exhaustive set of experiments across four different tasks, by querying the model with counterfactual prompts where only one of these components is altered. Our experiments across three models (PaLM, GPT-3, and CODEX) reveal several surprising findings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success of CoT. Second, our results conclude that the primary role of intermediate steps may not be to facilitate learning how to solve a task. The intermediate steps are rather a beacon for the model to realize what symbols to replicate in the output to form a factual answer. Further, text imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis reveals that a symbiotic relationship between text and patterns explains the success of few-shot prompting: text helps extract commonsense from the question to help patterns, and patterns enforce task understanding and direct text generation.",
        "paperId": "4988b3d378b79eb8669112620baf1ff4e3e536fd"
    },
    {
        "title": "Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data",
        "firstAuthor": "Xuhai Xu",
        "url": null,
        "dateSubmitted": "2023-07-26",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present the first comprehensive evaluation of multiple LLMs, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4, on various mental health prediction tasks via online text data. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for the mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on the mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.",
        "paperId": "4bc2f5ac65da81568e93cc7a295199d9bf10b19b"
    },
    {
        "title": "Large Language Model Distillation Doesn't Need a Teacher",
        "firstAuthor": "A. Jha",
        "url": "http://arxiv.org/pdf/2305.14864",
        "dateSubmitted": "2023-05-24",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Knowledge distillation trains a smaller student model to match the output distribution of a larger teacher to maximize the end-task performance under computational constraints. However, existing literature on language model distillation primarily focuses on compressing encoder-only models that are then specialized by task-specific supervised finetuning. We need to rethink this setup for more recent large language models with tens to hundreds of billions of parameters. Task-specific finetuning is impractical at this scale, and model performance is often measured using zero/few-shot prompting. Thus, in this work, we advocate for task-agnostic zero-shot evaluated distillation for large language models without access to end-task finetuning data. We propose a teacher-free task-agnostic distillation method, which uses a truncated version of the larger model for initialization, and continues pretraining this model using a language modeling objective. Our teacher-free method shines in a distillation regime where it is infeasible to fit both the student and teacher into the GPU memory. Despite its simplicity, our method can effectively reduce the model size by 50\\%, matching or outperforming the vanilla distillation method on perplexity and accuracy on 13 zero-shot end-tasks while being 1.5x computationally efficient.",
        "paperId": "4d382b6e6b1be8a50e35516822c39e336c7c117e"
    },
    {
        "title": "Revisiting non-English Text Simplification: A Unified Multilingual Benchmark",
        "firstAuthor": "Michael Joseph Ryan",
        "url": "http://arxiv.org/pdf/2305.15678",
        "dateSubmitted": "2023-05-25",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MultiSim with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot cross-lingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplifications outperforming fine-tuned models in most languages. We validate these findings through human evaluation.",
        "paperId": "4e1a4d6804c7983c659feb7e41d49ad8c21aaa43"
    },
    {
        "title": "PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents",
        "firstAuthor": "Simeng Sun",
        "url": "http://arxiv.org/pdf/2305.14564",
        "dateSubmitted": "2023-05-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions that require complex reasoning over long narrative texts. PEARL outperforms zero-shot and chain-of-thought prompting on this dataset, and ablation experiments show that each stage of PEARL is critical to its performance. Overall, PEARL is a first step towards leveraging LLMs to reason over long documents.",
        "paperId": "4ee96f0757e517928590a2300af5d40ba768a5a7"
    },
    {
        "title": "Extraction of Atypical Aspects from Customer Reviews: Datasets and Experiments with Language Models",
        "firstAuthor": "Smita Nannaware",
        "url": null,
        "dateSubmitted": "2023-11-05",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "A restaurant dinner may become a memorable experience due to an unexpected aspect enjoyed by the customer, such as an origami-making station in the waiting area. If aspects that are atypical for a restaurant experience were known in advance, they could be leveraged to make recommendations that have the potential to engender serendipitous experiences, further increasing user satisfaction. Although relatively rare, whenever encountered, atypical aspects often end up being mentioned in reviews due to their memorable quality. Correspondingly, in this paper we introduce the task of detecting atypical aspects in customer reviews. To facilitate the development of extraction models, we manually annotate benchmark datasets of reviews in three domains - restaurants, hotels, and hair salons, which we use to evaluate a number of language models, ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5 to zero-shot and few-shot prompting of GPT-3.5.",
        "paperId": "4f94d8c270c57ea697d3d9e15796cba8352347a6"
    },
    {
        "title": "Continued Pretraining for Better Zero- and Few-Shot Promptability",
        "firstAuthor": "Zhaofeng Wu",
        "url": "http://arxiv.org/pdf/2210.10258",
        "dateSubmitted": "2022-10-19",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Recently introduced language model prompting methods can achieve high accuracy in zero- and few-shot settings while requiring few to no learned task-specific parameters. Nevertheless, these methods still often trail behind full model finetuning. In this work, we investigate if a dedicated continued pretraining stage could improve \u201cpromptability\u201d, i.e., zero-shot performance with natural language prompts or few-shot performance with prompt tuning. We reveal settings where existing continued pretraining methods lack promptability. We also identify current methodological gaps, which we fill with thorough large-scale experiments. We demonstrate that a simple recipe, continued pretraining that incorporates a trainable prompt during multi-task learning, leads to improved promptability in both zero- and few-shot settings compared to existing methods, up to 31% relative. On the other hand, we find that continued pretraining using MAML-style meta-learning, a method that directly optimizes few-shot promptability, yields subpar performance. We validate our findings with two prompt tuning methods, and, based on our results, we provide concrete recommendations to optimize promptability for different use cases.",
        "paperId": "53868a2a4caea7afc487ef08993372b186fb2ddb"
    },
    {
        "title": "Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning",
        "firstAuthor": "Hongfu Liu",
        "url": "https://arxiv.org/pdf/2310.08923",
        "dateSubmitted": "2023-10-13",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large Language models (LLMs) possess the capability to engage In-context Learning (ICL) by leveraging a few demonstrations pertaining to a new downstream task as conditions. However, this particular learning paradigm suffers from high instability stemming from substantial variances induced by factors such as the input distribution of selected examples, their ordering, and prompt formats. In this work, we demonstrate that even when all these factors are held constant, the random selection of examples still results in high variance. Consequently, we aim to explore the informative ability of data examples by quantifying the Information Gain (IG) obtained in prediction after observing a given example candidate. Then we propose to sample those with maximum IG. Additionally, we identify the presence of template bias, which can lead to unfair evaluations of IG during the sampling process. To mitigate this bias, we introduce Calibration Before Sampling strategy. The experimental results illustrate that our proposed method can yield an average relative improvement of 14.3% across six classification tasks using three LLMs.",
        "paperId": "53addc28b106440a3c306b2cff8e259ad63d6d53"
    },
    {
        "title": "Programming without a Programming Language: Challenges and Opportunities for Designing Developer Tools for Prompt Programming",
        "firstAuthor": "Alexander J. Fiannaca",
        "url": null,
        "dateSubmitted": "2023-04-19",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Existing tools for writing prompts for language models (known as \u201cprompt programming\u201d) provide little support to prompt programmers. Consequently, as prompts become more complex with the addition of multiple input/output examples (\u201cfew-shot\u201d prompts), they can be hard to read, understand, and edit. In this work, we observe that prompts are often used to solve complex problems, but lack the strict grammar of a traditional programming language. We describe methods for extracting the semantically meaningful structure of natural language prompts (e.g., regions of the prompt representing a preamble or input/output examples) in the absence of a rigid formal grammar, and demonstrate a range of editor features that can leverage this information to assist prompt programmers. Finally, we relate initial feedback from design probe explorations with a set of domain experts and provide insights to help guide the development of future prompt editors.",
        "paperId": "55368d21ee06ece0939009d4d370a0c1883dcab8"
    },
    {
        "title": "FOLIO: Natural Language Reasoning with First-Order Logic",
        "firstAuthor": "Simeng Han",
        "url": "http://arxiv.org/pdf/2209.00840",
        "dateSubmitted": "2022-09-02",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "We present FOLIO, a human-annotated, open-domain, and logically complex and diverse dataset for reasoning in natural language (NL), equipped with first order logic (FOL) annotations. FOLIO consists of 1,435 examples (unique conclusions), each paired with one of 487 sets of premises which serve as rules to be used to deductively reason for the validity of each conclusion. The logical correctness of premises and conclusions is ensured by their parallel FOL annotations, which are automatically verified by our FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO automatically constitute a new NL-FOL translation dataset using FOL as the logical form. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models (BERT, RoBERTa) and few-shot prompting on large language models (GPT-NeoX, OPT, GPT-3, Codex). For NL-FOL translation, we experiment with GPT-3 and Codex. Our results show that one of the most capable Large Language Model (LLM) publicly available, GPT-3 davinci, achieves only slightly better than random results with few-shot prompting on a subset of FOLIO, and the model is especially bad at predicting the correct truth values for False and Unknown conclusions. Our dataset and code are available at https://github.com/Yale-LILY/FOLIO.",
        "paperId": "5581bf85386737bd3378eec68189759a05280bea"
    },
    {
        "title": "Low-Parameter Federated Learning with Large Language Models",
        "firstAuthor": "Jing Jiang",
        "url": "https://arxiv.org/pdf/2307.13896",
        "dateSubmitted": "2023-07-26",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "We study few-shot Natural Language Understanding (NLU) tasks with Large Language Models (LLMs) in federated learning (FL) scenarios. It is a challenging task due to limited labeled data and communication capacities in FL, especially with mobile devices. Recent studies show LLMs can be prompted to perform few-shot NLU tasks like sentiment analysis and arithmetic reasoning. However, the huge sizes of LLMs result in high computation and communication costs, making classical FL schemes impractical. To address these challenges, we propose Low-Parameter Federated Learning (LP-FL). LP-FL combines few-shot prompt learning from LLMs with efficient communication and federating techniques. Our approach enables federated clients to assign soft labels to unlabeled data using gradually learned knowledge from the global model. Through iterative soft-label assigning, we continually expand the labeled set during the FL process. Additionally, to reduce computation and communication costs, LP-FL utilizes the Low-Rank Adaptation (LoRA) technique for compact learnable parameter construction, efficient local model fine-tuning, and affordable global model federation. LP-FL consistently outperforms Full-Parameter Federated Learning (FP-FL) in sentiment analysis tasks across various FL settings. Its resistance to overfitting allows LP-FL to equal or surpass centralized training in few-shot scenarios.",
        "paperId": "573dad7b2fca7ce72a7f0daf681391d96379ebe3"
    },
    {
        "title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
        "firstAuthor": "Hongxin Zhang",
        "url": "https://arxiv.org/pdf/2307.02485",
        "dateSubmitted": "2023-07-05",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated impressive planning abilities in single-agent embodied tasks across various domains. However, their capacity for planning and communication in multi-agent cooperation remains unclear, even though these are crucial skills for intelligent embodied agents. In this paper, we present a novel framework that utilizes LLMs for multi-agent cooperation and tests it in various embodied environments. Our framework enables embodied agents to plan, communicate, and cooperate with other embodied agents or humans to accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs, such as GPT-4, can surpass strong planning-based methods and exhibit emergent effective communication using our framework without requiring fine-tuning or few-shot prompting. We also discover that LLM-based agents that communicate in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for embodied AI and lays the foundation for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.",
        "paperId": "587352c3b95c90de6d37f061c8e117f42be0b575"
    },
    {
        "title": "DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks in the Physical World",
        "firstAuthor": "Cai Kang",
        "url": "http://arxiv.org/pdf/2306.09124",
        "dateSubmitted": "2023-06-15",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Adversarial attacks, particularly patch attacks, pose significant threats to the robustness and reliability of deep learning models. Developing reliable defenses against patch attacks is crucial for real-world applications, yet current research in this area is not satisfactory. In this paper, we propose DIFFender, a novel defense method that leverages a text-guided diffusion model to defend against adversarial patches. DIFFender includes two main stages: patch localization and patch restoration. In the localization stage, we find and exploit an intriguing property of the diffusion model to effectively identify the locations of adversarial patches. In the restoration stage, we employ the diffusion model to reconstruct the adversarial regions in the images while preserving the integrity of the visual content. Importantly, these two stages are carefully guided by a unified diffusion model, thus we can utilize the close interaction between them to improve the whole defense performance. Moreover, we propose a few-shot prompt-tuning algorithm to fine-tune the diffusion model, enabling the pre-trained diffusion model to easily adapt to the defense task. We conduct extensive experiments on the image classification and face recognition tasks, demonstrating that our proposed method exhibits superior robustness under strong adaptive attacks and generalizes well across various scenarios, diverse classifiers, and multiple patch attack methods.",
        "paperId": "5b8f647e1a02cace17ed0896eb9b37be9a9fa45e"
    },
    {
        "title": "Are Prompt-based Models Clueless?",
        "firstAuthor": "Pride Kavumba",
        "url": "https://arxiv.org/pdf/2205.09295",
        "dateSubmitted": "2022-05-19",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Finetuning large pre-trained language models with a task-specific head has advanced the state-of-the-art on many natural language understanding benchmarks. However, models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets.Prompting has reduced the data requirement by reusing the language model head and formatting the task input to match the pre-training objective. Therefore, it is expected that few-shot prompt-based models do not exploit superficial cues.This paper presents an empirical examination of whether few-shot prompt-based models also exploit superficial cues.Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues. While the models perform well on instances with superficial cues, they often underperform or only marginally outperform random accuracy on instances without superficial cues.",
        "paperId": "5cac32f85cb25299122c39b8b73eb01fdee710b6"
    },
    {
        "title": "The Rise of AI Language Pathologists: Exploring Two-level Prompt Learning for Few-shot Weakly-supervised Whole Slide Image Classification",
        "firstAuthor": "Linhao Qu",
        "url": "http://arxiv.org/pdf/2305.17891",
        "dateSubmitted": "2023-05-29",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This paper introduces the novel concept of few-shot weakly supervised learning for pathology Whole Slide Image (WSI) classification, denoted as FSWC. A solution is proposed based on prompt learning and the utilization of a large language model, GPT-4. Since a WSI is too large and needs to be divided into patches for processing, WSI classification is commonly approached as a Multiple Instance Learning (MIL) problem. In this context, each WSI is considered a bag, and the obtained patches are treated as instances. The objective of FSWC is to classify both bags and instances with only a limited number of labeled bags. Unlike conventional few-shot learning problems, FSWC poses additional challenges due to its weak bag labels within the MIL framework. Drawing inspiration from the recent achievements of vision-language models (V-L models) in downstream few-shot classification tasks, we propose a two-level prompt learning MIL framework tailored for pathology, incorporating language prior knowledge. Specifically, we leverage CLIP to extract instance features for each patch, and introduce a prompt-guided pooling strategy to aggregate these instance features into a bag feature. Subsequently, we employ a small number of labeled bags to facilitate few-shot prompt learning based on the bag features. Our approach incorporates the utilization of GPT-4 in a question-and-answer mode to obtain language prior knowledge at both the instance and bag levels, which are then integrated into the instance and bag level language prompts. Additionally, a learnable component of the language prompts is trained using the available few-shot labeled data. We conduct extensive experiments on three real WSI datasets encompassing breast cancer, lung cancer, and cervical cancer, demonstrating the notable performance of the proposed method in bag and instance classification. All codes will be made publicly accessible.",
        "paperId": "5d0fd58a69be946096516d723b4bf6326ebb319e"
    },
    {
        "title": "MEAL: Stable and Active Learning for Few-Shot Prompting",
        "firstAuthor": "Abdullatif K\u00f6ksal",
        "url": "http://arxiv.org/pdf/2211.08358",
        "dateSubmitted": "2022-11-15",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Few-shot classification has made great strides due to foundation models that, through priming and prompting, are highly effective few-shot learners. However, this approach has high variance both across different sets of few shots (data selection) and across different finetuning runs (run variability). This is problematic not only because it impedes the fair comparison of different approaches, but especially because it makes few-shot learning too unreliable for many real-world applications. To alleviate these issues, we make two contributions for more stable and effective few-shot learning: First, we propose novel ensembling methods and show that they substantially reduce run variability. Second, we introduce a new active learning (AL) criterion for data selection and present the first AL-based approach specifically tailored towards prompt-based learning. In our experiments, we show that our combined method, MEAL (Multiprompt finetuning and prediction Ensembling with Active Learning), improves overall performance of prompt-based finetuning by 2.3 points on five diverse tasks.",
        "paperId": "5df5ebcaed745a5252b4fae64dc1d7ca90e68ff6"
    },
    {
        "title": "ConsPrompt: Easily Exploiting Contrastive Samples for Few-shot Prompt Learning",
        "firstAuthor": "Jinta Weng",
        "url": "https://arxiv.org/pdf/2211.04118",
        "dateSubmitted": "2022-11-08",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Prompt learning recently become an effective linguistic tool to motivate the PLMs\u2019 knowledge on few-shot-setting tasks. However, studies have shown the lack of robustness still exists in prompt learning, since suitable initialization of continuous prompt and expert-\ufb01rst manual prompt are essential in \ufb01ne-tuning process. What is more, human also utilize their comparative ability to motivate their existing knowledge for distinguishing different examples. Motivated by this, we explore how to use contrastive samples to strengthen prompt learning. In detail, we \ufb01rst propose our model ConsPrompt combining with prompt encoding network, contrastive sampling module, and contrastive scoring module. Subsequently, two sampling strategies, similarity-based and label-based strategies, are introduced to realize dif-ferential contrastive learning. The effectiveness of proposed ConsPrompt is demonstrated in \ufb01ve different few-shot learning tasks and shown the similarity-based sampling strategy is more effective than label-based in combining contrastive learning. Our results also ex-hibits the state-of-the-art performance and robustness in different few-shot settings, which proves that the ConsPrompt could be assumed as a better knowledge probe to motivate PLMs. As far as we could reach, this is the \ufb01rst work exploring how to use contrastive learning approach and suitable contrastive samples to enhance prompt-based \ufb01ne-tuning.",
        "paperId": "5e3675bdbe898cb28a0fc3c2f72a578a97fe64bb"
    },
    {
        "title": "Can GPT-3 Perform Statutory Reasoning?",
        "firstAuthor": "Andrew Blair-Stanek",
        "url": "https://arxiv.org/pdf/2302.06100",
        "dateSubmitted": "2023-02-13",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Statutory reasoning is the task of reasoning with facts and statutes, which are rules written in natural language by a legislature. It is a basic legal skill. In this paper we explore the capabilities of the most capable GPT-3 model, text-davinci-003, on an established statutory-reasoning dataset called SARA. We consider a variety of approaches, including dynamic few-shot prompting, chain-of-thought prompting, and zero-shot prompting. While we achieve results with GPT-3 that are better than the previous best published results, we also identify several types of clear errors it makes. We investigate why these errors happen. We discover that GPT-3 has imperfect prior knowledge of the actual U.S. statutes on which SARA is based. More importantly, we create simple synthetic statutes, which GPT-3 is guaranteed not to have seen during training. We find GPT-3 performs poorly at answering straightforward questions about these simple synthetic statutes.",
        "paperId": "5f5253fb15ac382e96ade0335baf1cfaa240fb1d"
    },
    {
        "title": "Explainable Verbal Reasoner Plus (EVR+): A Natural Language Reasoning Framework that Supports Diverse Compositional Reasoning",
        "firstAuthor": "Zhengzhong Liang",
        "url": "http://arxiv.org/pdf/2305.00061",
        "dateSubmitted": "2023-04-28",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Languages models have been successfully applied to a variety of reasoning tasks in NLP, yet the language models still suffer from compositional generalization. In this paper we present Explainable Verbal Reasoner Plus (EVR+), a reasoning framework that enhances language models' compositional reasoning ability by (1) allowing the model to explicitly generate and execute symbolic operators, and (2) allowing the model to decompose a complex task into several simpler ones in a flexible manner. Compared with its predecessor Explainable Verbal Reasoner (EVR) and other previous approaches adopting similar ideas, our framework supports more diverse types of reasoning such as nested loops and different types of recursion. To evaluate our reasoning framework, we build a synthetic dataset with five tasks that require compositional reasoning. Results show that our reasoning framework can enhance the language model's compositional generalization performance on the five tasks, using a fine-tuned language model. We also discussed the possibility and the challenges to combine our reasoning framework with a few-shot prompted language model.",
        "paperId": "5f88b907cb6b79ce22e826832f05c0471ecb095e"
    },
    {
        "title": "On Bilingual Lexicon Induction with Large Language Models",
        "firstAuthor": "Yaoyiran Li",
        "url": null,
        "dateSubmitted": "2023-10-21",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a range of typologically diverse languages. Our work is the first to demonstrate strong BLI capabilities of text-to-text mLLMs. The results reveal that few-shot prompting with in-context examples from nearest neighbours achieves the best performance, establishing new state-of-the-art BLI scores for many language pairs. We also conduct a series of in-depth analyses and ablation studies, providing more insights on BLI with (m)LLMs, also along with their limitations.",
        "paperId": "6036f424468a5be5dd9b427ae266b72cb8468b5f"
    },
    {
        "title": "Language Models as Knowledge Bases for Visual Word Sense Disambiguation",
        "firstAuthor": "Anastasia Kritharoula",
        "url": "https://arxiv.org/pdf/2310.01960",
        "dateSubmitted": "2023-10-03",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Visual Word Sense Disambiguation (VWSD) is a novel challenging task that lies between linguistic sense disambiguation and fine-grained multimodal retrieval. The recent advancements in the development of visiolinguistic (VL) transformers suggest some off-the-self implementations with encouraging results, which however we argue that can be further improved. To this end, we propose some knowledge-enhancement techniques towards improving the retrieval performance of VL transformers via the usage of Large Language Models (LLMs) as Knowledge Bases. More specifically, knowledge stored in LLMs is retrieved with the help of appropriate prompts in a zero-shot manner, achieving performance advancements. Moreover, we convert VWSD to a purely textual question-answering (QA) problem by considering generated image captions as multiple-choice candidate answers. Zero-shot and few-shot prompting strategies are leveraged to explore the potential of such a transformation, while Chain-of-Thought (CoT) prompting in the zero-shot setting is able to reveal the internal reasoning steps an LLM follows to select the appropriate candidate. In total, our presented approach is the first one to analyze the merits of exploiting knowledge stored in LLMs in different ways to solve WVSD.",
        "paperId": "61bbdbf481a6d3519c22513ebe8d6c3cd381851e"
    },
    {
        "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
        "firstAuthor": "Mirac Suzgun",
        "url": "http://arxiv.org/pdf/2210.09261",
        "dateSubmitted": "2022-10-17",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",
        "paperId": "663a41c866d49ce052801fbc88947d39764cad29"
    },
    {
        "title": "Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis",
        "firstAuthor": "Hongyi Zheng",
        "url": null,
        "dateSubmitted": "2023-11-01",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Recent advances in prompt engineering enable large language models (LLMs) to solve multi-hop logical reasoning problems with impressive accuracy. However, there is little existing work investigating the robustness of LLMs with few-shot prompting techniques. Therefore, we introduce a systematic approach to test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic perturbations. We include perturbations at multiple levels of abstractions (e.g. lexical perturbations such as typos, and semantic perturbations such as the inclusion of intermediate reasoning steps in the questions) to conduct behavioral analysis on the LLMs. Throughout our experiments, we find that models are more sensitive to certain perturbations such as replacing words with their synonyms. We also demonstrate that increasing the proportion of perturbed exemplars in the prompts improves the robustness of few-shot prompting methods.",
        "paperId": "675e079cc3c11f9234f8f70bab9f763911b97955"
    },
    {
        "title": "FireAct: Toward Language Agent Fine-tuning",
        "firstAuthor": "Baian Chen",
        "url": "https://arxiv.org/pdf/2310.05915",
        "dateSubmitted": "2023-10-09",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Recent efforts have augmented language models (LMs) with external tools or environments, leading to the development of language agents that can reason and act. However, most of these agents rely on few-shot prompting techniques with off-the-shelf LMs. In this paper, we investigate and argue for the overlooked direction of fine-tuning LMs to obtain language agents. Using a setup of question answering (QA) with a Google search API, we explore a variety of base LMs, prompting methods, fine-tuning data, and QA tasks, and find language agents are consistently improved after fine-tuning their backbone LMs. For example, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4 leads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct, a novel approach to fine-tuning LMs with trajectories from multiple tasks and prompting methods, and show having more diverse fine-tuning data can further improve agents. Along with other findings regarding scaling effects, robustness, generalization, efficiency and cost, our work establishes comprehensive benefits of fine-tuning LMs for agents, and provides an initial set of experimental designs, insights, as well as open questions toward language agent fine-tuning.",
        "paperId": "67daf8c4fe1958d20ebdf95c2a36dd490c73836f"
    },
    {
        "title": "Natural Language Decomposition and Interpretation of Complex Utterances",
        "firstAuthor": "Harsh Jhamtani",
        "url": "http://arxiv.org/pdf/2305.08677",
        "dateSubmitted": "2023-05-15",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Natural language interfaces often require supervised data to translate user requests into programs, database queries, or other structured intent representations. During data collection, it can be difficult to anticipate and formalize the full range of user needs -- for example, in a system designed to handle simple requests (like $\\textit{find my meetings tomorrow}$ or $\\textit{move my meeting with my manager to noon})$, users may also express more elaborate requests (like $\\textit{swap all my calls on Monday and Tuesday}$). We introduce an approach for equipping a simple language-to-code model to handle complex utterances via a process of hierarchical natural language decomposition. Our approach uses a pre-trained language model to decompose a complex utterance into a sequence of smaller natural language steps, then interprets each step using the language-to-code model. To test our approach, we collect and release DeCU -- a new NL-to-program benchmark to evaluate Decomposition of Complex Utterances. Experiments show that the proposed approach enables the interpretation of complex utterances with almost no complex training data, while outperforming standard few-shot prompting approaches.",
        "paperId": "68040213e9a83408cdc491ed3e235b52b537eed1"
    },
    {
        "title": "PAL: Program-aided Language Models",
        "firstAuthor": "Luyu Gao",
        "url": "http://arxiv.org/pdf/2211.10435",
        "dateSubmitted": "2022-11-18",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\"few-shot prompting\"). Much of this success can be attributed to prompting methods such as\"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .",
        "paperId": "6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7"
    },
    {
        "title": "Prompted LLMs as Chatbot Modules for Long Open-domain Conversation",
        "firstAuthor": "Gibbeum Lee",
        "url": "http://arxiv.org/pdf/2305.04533",
        "dateSubmitted": "2023-05-08",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for creating high-quality conversational agents without the need for fine-tuning. Our method utilizes pre-trained large language models (LLMs) as individual modules for long-term consistency and flexibility, by using techniques such as few-shot prompting, chain-of-thought (CoT), and external memory. Our human evaluation results show that MPC is on par with fine-tuned chatbot models in open-domain conversations, making it an effective solution for creating consistent and engaging chatbots.",
        "paperId": "700da3f3758e053c379f905bebee261ba69f1073"
    },
    {
        "title": "Prompting GPT-3 To Be Reliable",
        "firstAuthor": "Chenglei Si",
        "url": "http://arxiv.org/pdf/2210.09150",
        "dateSubmitted": "2022-10-17",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.",
        "paperId": "711d5e8ddbb840ad31a9ffa3d38590603ba69a92"
    },
    {
        "title": "SciFix: Outperforming GPT3 on Scientific Factual Error Correction",
        "firstAuthor": "D. Ashok",
        "url": null,
        "dateSubmitted": "2023-05-24",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Due to the prohibitively high cost of creating error correction datasets, most Factual Claim Correction methods rely on a powerful verification model to guide the correction process. This leads to a significant drop in performance in domains like scientific claims, where good verification models do not always exist. In this work, we introduce SciFix, a scientific claim correction system that does not require a verifier but can outperform existing methods by a considerable margin -- achieving correction accuracy of 84% on the SciFact dataset, 77% on SciFact-Open and 72% on the CovidFact dataset, compared to next best accuracies of 7%, 5%, and 15% on the same datasets respectively. Our method leverages the power of prompting with LLMs during training to create a richly annotated dataset that can be used for fully supervised training and regularization. We additionally use a claim-aware decoding procedure to improve the quality of corrected claims. Our method outperforms the very LLM that was used to generate the annotated dataset -- with Few-Shot Prompting on GPT3.5 achieving 58%, 61%, and 64% on the respective datasets, a consistently lower correction accuracy, despite using nearly 800 times as many parameters as our model.",
        "paperId": "716178841e169f5c02a1fd5da241825699501248"
    },
    {
        "title": "Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions",
        "firstAuthor": "Skyler Wu",
        "url": "https://arxiv.org/pdf/2307.13339",
        "dateSubmitted": "2023-07-25",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Chain-of-thought (CoT) prompting has been shown to empirically improve the accuracy of large language models (LLMs) on various question answering tasks. While understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment. We address this question by leveraging gradient-based feature attribution methods which produce saliency scores that capture the influence of input tokens on model output. Specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens. Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to question perturbations and variations in model output.",
        "paperId": "71d68782c3da41b77866c2fd0cb65726f60b3af1"
    },
    {
        "title": "Understanding How Model Size Affects Few-shot Instruction Prompting",
        "firstAuthor": "Ayrton San Joaquin",
        "url": "https://arxiv.org/pdf/2212.01907",
        "dateSubmitted": "2022-12-04",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large Language Models are affected by the phenomena of memorizing and forgetting their training data. But how do these vary by model size? We work towards this question by investigating how the model size affects the model's ability to discriminate a word's meaning in a given context. We introduce a dataset called DeltaWords, which evaluates a model's ability to follow instructions to select a sentence which replaces the target word with its antonym. We show a weak inverse scaling trend, where task accuracy degrades as model size increase, under extremely few-shot prompting regimes. We show that increasing the number of examples tend to disproportionately benefit larger models than smaller models.",
        "paperId": "72491b96d8a614d1a9a099707d44593d4b5a8f49"
    },
    {
        "title": "SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models",
        "firstAuthor": "S. S. Kannan",
        "url": "https://arxiv.org/pdf/2309.10062",
        "dateSubmitted": "2023-09-18",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "In this work, we introduce SMART-LLM, an innovative framework designed for embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm. We create a benchmark dataset designed for validating the multi-robot task planning problem, encompassing four distinct categories of high-level instructions that vary in task complexity. Our evaluation experiments span both simulation and real-world scenarios, demonstrating that the proposed model can achieve promising results for generating multi-robot task plans. The experimental videos, code, and datasets from the work can be found at https://sites.google.com/view/smart-llm/.",
        "paperId": "755853c6b30f5a186131e23a63c68a3f2737068e"
    },
    {
        "title": "Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?",
        "firstAuthor": "Cheng-En Wu",
        "url": "https://arxiv.org/pdf/2307.11978",
        "dateSubmitted": "2023-07-22",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Vision-language models such as CLIP learn a generic text-image embedding from large-scale training data. A vision-language model can be adapted to a new classification task through few-shot prompt tuning. We find that such a prompt tuning process is highly robust to label noises. This intrigues us to study the key reasons contributing to the robustness of the prompt tuning paradigm. We conducted extensive experiments to explore this property and find the key factors are: 1) the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples; 2) the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification. Further, we demonstrate that noisy zero-shot predictions from CLIP can be used to tune its own prompt, significantly enhancing prediction accuracy in the unsupervised setting. The code is available at https://github.com/CEWu/PTNL.",
        "paperId": "757ca326783abad4cb5fa96bd6151de212386d7b"
    },
    {
        "title": "Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models",
        "firstAuthor": "Haoyu Gao",
        "url": "https://arxiv.org/pdf/2309.12940",
        "dateSubmitted": "2023-09-22",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Task-oriented dialogue (TOD) systems facilitate users in executing various activities via multi-turn dialogues, but Large Language Models (LLMs) often struggle to comprehend these intricate contexts. In this study, we propose a novel\"Self-Explanation\"prompting strategy to enhance the comprehension abilities of LLMs in multi-turn dialogues. This task-agnostic approach requires the model to analyze each dialogue utterance before task execution, thereby improving performance across various dialogue-centric tasks. Experimental results from six benchmark datasets confirm that our method consistently outperforms other zero-shot prompts and matches or exceeds the efficacy of few-shot prompts, demonstrating its potential as a powerful tool in enhancing LLMs' comprehension in complex dialogue tasks.",
        "paperId": "75ce9634d281cc12cbe434f86c737df8e10796fa"
    },
    {
        "title": "Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models",
        "firstAuthor": "Emily Reif",
        "url": "https://arxiv.org/pdf/2305.11364",
        "dateSubmitted": "2023-05-19",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) can be used to generate smaller, more refined datasets via few-shot prompting for benchmarking, fine-tuning or other use cases. However, understanding and evaluating these datasets is difficult, and the failure modes of LLM-generated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically. We present LinguisticLens, a novel inter-active visualization tool for making sense of and analyzing syntactic diversity of LLM-generated datasets. LinguisticLens clusters text along syntactic, lexical, and semantic axes. It supports hierarchical visualization of a text dataset, allowing users to quickly scan for an overview and inspect individual examples. The live demo is available at shorturl.at/zHOUV.",
        "paperId": "7655f05cd394da6cb0f707068203c9ff05d8f05a"
    },
    {
        "title": "CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models",
        "firstAuthor": "Hossein Hajipour",
        "url": null,
        "dateSubmitted": "2023-02-08",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) for automatic code generation have achieved breakthroughs in several programming tasks. Their advances in competition-level programming problems have made them an essential pillar of AI-assisted pair programming, and tools such as GitHub Copilot have emerged as part of the daily programming workflow used by millions of developers. The training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure. While these models have been extensively assessed for their ability to produce functionally correct programs, there remains a lack of comprehensive investigations and benchmarks addressing the security aspects of these models. In this work, we propose a method to systematically study the security issues of code language models to assess their susceptibility to generating vulnerable code. To this end, we introduce the first approach to automatically find generated code that contains vulnerabilities in black-box code generation models. To achieve this, we present an approach to approximate inversion of the black-box code generation models based on few-shot prompting. We evaluate the effectiveness of our approach by examining code language models in generating high-risk security weaknesses. Furthermore, we establish a collection of diverse non-secure prompts for various vulnerability scenarios using our method. This dataset forms a benchmark for evaluating and comparing the security weaknesses in code language models.",
        "paperId": "7677e9ead35f8f2578acbf5ad15fb330c83762c4"
    },
    {
        "title": "ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT",
        "firstAuthor": "Fatemeh Nazary",
        "url": "https://arxiv.org/pdf/2308.09731",
        "dateSubmitted": "2023-08-17",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This study presents an innovative approach to the application of large language models (LLMs) in clinical decision-making, focusing on OpenAI's ChatGPT. Our approach introduces the use of contextual prompts-strategically designed to include task description, feature description, and crucially, integration of domain knowledge-for high-quality binary classification tasks even in data-scarce scenarios. The novelty of our work lies in the utilization of domain knowledge, obtained from high-performing interpretable ML models, and its seamless incorporation into prompt design. By viewing these ML models as medical experts, we extract key insights on feature importance to aid in decision-making processes. This interplay of domain knowledge and AI holds significant promise in creating a more insightful diagnostic tool. Additionally, our research explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT with traditional supervised ML models in different data conditions, we aim to provide insights into the effectiveness of prompt engineering strategies under varied data availability. In essence, this paper bridges the gap between AI and healthcare, proposing a novel methodology for LLMs application in clinical decision support systems. It highlights the transformative potential of effective prompt design, domain knowledge integration, and flexible learning approaches in enhancing automated decision-making.",
        "paperId": "793eb805800c4af0b06260079e178efa0377b9d7"
    },
    {
        "title": "Natural Instructions: Benchmarking Generalization to New Tasks from Natural Language Instructions",
        "firstAuthor": "Swaroop Mishra",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Can we enable NLP models to appropriately respond to instructional prompts and conse-quently generalize to new tasks? To study this question, we leverage the existing NLP datasets and the instructions that were used to crowdsource them to create N ATURAL - I NSTRUCTIONS , a dataset of instructions and task-speci\ufb01c input/output data. This dataset consists of 61 distinct language instructions and about 600 k task instances, and is used to evaluate existing state-of-the-art language-models (LMs) in addressing new tasks by few-shot prompting of GPT3 and \ufb01ne-tuning BART. Our analysis indicates that: (a) the existing models indeed bene\ufb01t from instructions and hence, show improved generalization to new tasks; (b) while models like GPT-3 generally bene\ufb01t from instructions, the extent of their gains varies across different \ufb01elds of instructions and also depends on the task being solved; (c) generalization to unseen tasks in N ATURAL -I NSTRUCTIONS remains far from perfect for the state-of-the-art, indicating signi\ufb01cant room for more progress in this direc-tion. 1",
        "paperId": "7b486a6eac4b46cf39e41c97b25ea22c5d27a883"
    },
    {
        "title": "Transferring Procedural Knowledge across Commonsense Tasks",
        "firstAuthor": "Yifan Jiang",
        "url": "https://arxiv.org/pdf/2304.13867",
        "dateSubmitted": "2023-04-26",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Stories about everyday situations are an essential part of human communication, motivating the need to develop AI agents that can reliably understand these stories. Despite the long list of supervised methods for story completion and procedural understanding, current AI has no mechanisms to automatically track and explain procedures in unseen stories. To bridge this gap, we study the ability of AI models to transfer procedural knowledge to novel narrative tasks in a transparent manner. We design LEAP: a comprehensive framework that integrates state-of-the-art modeling architectures, training regimes, and augmentation strategies based on both natural and synthetic stories. To address the lack of densely annotated training data, we devise a robust automatic labeler based on few-shot prompting to enhance the augmented data. Our experiments with in- and out-of-domain tasks reveal insights into the interplay of different architectures, training regimes, and augmentation strategies. LEAP's labeler has a clear positive impact on out-of-domain datasets, while the resulting dense annotation provides native explainability.",
        "paperId": "7beec352ac2597c3cd3dc7aceb2f8cd068b72d15"
    },
    {
        "title": "Exploring The Landscape of Distributional Robustness for Question Answering Models",
        "firstAuthor": "Anas Awadalla",
        "url": "http://arxiv.org/pdf/2210.12517",
        "dateSubmitted": "2022-10-22",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "We conduct a large empirical evaluation to investigate the landscape of distributional robustness in question answering. Our investigation spans over 350 models and 16 question answering datasets, including a diverse set of architectures, model sizes, and adaptation methods (e.g., fine-tuning, adapter tuning, in-context learning, etc.). We find that, in many cases, model variations do not affect robustness and in-distribution performance alone determines out-of-distribution performance. Moreover, our findings indicate that i) zero-shot and in-context learning methods are more robust to distribution shifts than fully fine-tuned models; ii) few-shot prompt fine-tuned models exhibit better robustness than few-shot fine-tuned span prediction models; iii) parameter-efficient and robustness enhancing training methods provide no significant robustness improvements. In addition, we publicly release all evaluations to encourage researchers to further analyze robustness trends for question answering models.",
        "paperId": "7cf4f8cb8b4a373d869e785b79160dda7a49a250"
    },
    {
        "title": "Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review",
        "firstAuthor": "Banghao Chen",
        "url": null,
        "dateSubmitted": "2023-10-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we gather information about the application of prompt engineering in such fields as education and programming, showing its transformative potential. This comprehensive survey aims to serve as a friendly guide for anyone venturing through the big world of LLMs and prompt engineering.",
        "paperId": "7d083d654f66f763302d8a5f0678beb753f6507b"
    },
    {
        "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
        "firstAuthor": "Miles Turpin",
        "url": "http://arxiv.org/pdf/2305.04388",
        "dateSubmitted": "2023-05-07",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs -- e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always\"(A)\"-- which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations supporting those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. CoT is promising for explainability, but our results highlight the need for targeted efforts to evaluate and improve explanation faithfulness.",
        "paperId": "7dc928f41e15f65f1267bd87b0fcfcc7e715cb56"
    },
    {
        "title": "ZARA: Improving Few-Shot Self-Rationalization for Small Language Models",
        "firstAuthor": "Wei-Lin Chen",
        "url": "http://arxiv.org/pdf/2305.07355",
        "dateSubmitted": "2023-05-12",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Language models (LMs) that jointly generate end-task answers as well as free-text rationales are known as self-rationalization models. Recent works demonstrate great performance gain for self-rationalization by few-shot prompting LMs with rationale-augmented exemplars. However, the ability to benefit from explanations only emerges with large-scale LMs, which have poor accessibility. In this work, we explore the less-studied setting of leveraging explanations for small LMs to improve few-shot self-rationalization. We first revisit the relationship between rationales and answers. Inspired by the implicit mental process of how human beings assess explanations, we present a novel approach, Zero-shot Augmentation of Rationale-Answer pairs (ZARA), to automatically construct pseudo-parallel data for self-training by reducing the problem of plausibility judgement to natural language inference. Experimental results show ZARA achieves SOTA performance on the FEB benchmark, for both the task accuracy and the explanation metric. In addition, we conduct human and quantitative evaluation validating ZARA's ability to automatically identify plausible and accurate rationale-answer pairs.",
        "paperId": "7df3595bdb4003589e8ca1757cc39ec03a39a2ff"
    },
    {
        "title": "Natural Language to Code Generation in Interactive Data Science Notebooks",
        "firstAuthor": "Pengcheng Yin",
        "url": "http://arxiv.org/pdf/2212.09248",
        "dateSubmitted": "2022-12-19",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1078 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions. Arcade is publicly available at https://github.com/google-research/arcade-nl2code/.",
        "paperId": "815c6ca281536d18ec0eb408b6e46e72a0826163"
    },
    {
        "title": "Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models",
        "firstAuthor": "Jimmy Wei",
        "url": "http://arxiv.org/pdf/2304.13835",
        "dateSubmitted": "2023-04-26",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.",
        "paperId": "82beb8a86d438e85a134182128d47607b1b04004"
    },
    {
        "title": "Data-to-text Generation for Severely Under-Resourced Languages with GPT-3.5: A Bit of Help Needed from Google Translate (WebNLG 2023)",
        "firstAuthor": "Michela Lorandi",
        "url": "https://arxiv.org/pdf/2308.09957",
        "dateSubmitted": "2023-08-19",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "LLMs are great at tasks involving English which dominates in their training data. We explore their ability to address tasks involving languages that are severely under-represented in their training data. More specifically, we do this in the context of data-to-text generation for Irish, Maltese, Welsh and Breton. During the prompt-engineering phase we tested GPT-3.5 and~4 with a range of prompt types and formats on a small sample of example input/output pairs. We then fully evaluated the two most promising prompts in two scenarios: (i) direct generation into the under-resourced languages, and (ii) generation into English followed by translation into the under-resourced languages. We find that few-shot prompting works better for direct generation into under-resourced languages, but that the difference disappears when pivoting via English. The few-shot + translation system variants were submitted to the WebNLG 2023 shared task where they outperformed all other systems by substantial margins in all languages on all automatic metrics. We conclude that good performance can be achieved with state-of-the-art LLMs out-of-the box for under-resourced languages. However, best results (for Welsh) of BLEU 25.12, ChrF++ 0.55, and TER 0.64 are well below the lowest ranked English system at WebNLG\u201920 with BLEU 0.391, ChrF++ 0.579, and TER 0.564.",
        "paperId": "842f79c5acab440f8d7a592201738a3e854a5186"
    },
    {
        "title": "Towards Legally Enforceable Hate Speech Detection for Public Forums",
        "firstAuthor": "Chunyan Luo",
        "url": "http://arxiv.org/pdf/2305.13677",
        "dateSubmitted": "2023-05-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Hate speech causes widespread and deep-seated societal issues. Proper enforcement of hate speech laws is key for protecting groups of people against harmful and discriminatory language. However, determining what constitutes hate speech is a complex task that is highly open to subjective interpretations. Existing works do not align their systems with enforceable definitions of hate speech, which can make their outputs inconsistent with the goals of regulators. This research introduces a new perspective and task for enforceable hate speech detection centred around legal definitions, and a dataset annotated on violations of eleven possible definitions by legal experts. Given the challenge of identifying clear, legally enforceable instances of hate speech, we augment the dataset with expert-generated samples and an automatically mined challenge set. We experiment with grounding the model decision in these definitions using zero-shot and few-shot prompting. We then report results on several large language models (LLMs). With this task definition, automatic hate speech detection can be more closely aligned to enforceable laws, and hence assist in more rigorous enforcement of legal protections against harmful speech in public forums.",
        "paperId": "895f3c9e452ae51fb02786de424ce6d2bba11c3b"
    },
    {
        "title": "USB: A Unified Summarization Benchmark Across Tasks and Domains",
        "firstAuthor": "Kundan Krishna",
        "url": "http://arxiv.org/pdf/2305.14296",
        "dateSubmitted": "2023-05-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "An abundance of datasets exist for training and evaluating models on the task of summary generation.However, these datasets are often derived heuristically, and lack sufficient annotations to support research into all aspects of summarization, such as evidence extraction and controllable summarization. We introduce a benchmark comprising 8 tasks that require multi-dimensional understanding of summarization, e.g., surfacing evidence for a summary, assessing its correctness, and gauging its relevance to different topics. We compare various methods on this benchmark and discover that on multiple tasks, moderately-sized fine-tuned models consistently outperform much larger few-shot prompted language models. For factuality related tasks, we also evaluate existing heuristics to create training data and find that training on them performs worse than training on $20\\times$ less human-labeled data. Our benchmark consists of data from 6 different domains, allowing us to study cross-domain performance of trained models. We find that for some tasks, the amount of training data matters more than the domain where it comes from, while for other tasks training specifically on data from the target domain, even if limited, is more beneficial. Our work fulfills the need for a well-annotated summarization benchmark with diverse tasks, and provides useful insights about the impact of the quality, size and domain of training data.",
        "paperId": "8ab27849799286459465d2262f926354093b20a9"
    },
    {
        "title": "NL2Color: Refining Color Palettes for Charts with Natural Language.",
        "firstAuthor": "Chuhan Shi",
        "url": null,
        "dateSubmitted": "2023-10-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Choice of color is critical to creating effective charts with an engaging, enjoyable, and informative reading experience. However, designing a good color palette for a chart is a challenging task for novice users who lack related design expertise. For example, they often find it difficult to articulate their abstract intentions and translate these intentions into effective editing actions to achieve a desired outcome. In this work, we present NL2Color, a tool that allows novice users to refine chart color palettes using natural language expressions of their desired outcomes. We first collected and categorized a dataset of 131 triplets, each consisting of an original color palette of a chart, an editing intent, and a new color palette designed by human experts according to the intent. Our tool employs a large language model (LLM) to substitute the colors in original palettes and produce new color palettes by selecting some of the triplets as few-shot prompts. To evaluate our tool, we conducted a comprehensive two-stage evaluation, including a crowd-sourcing study (N=71) and a within-subjects user study (N=12). The results indicate that the quality of the color palettes revised by NL2Color has no significantly large difference from those designed by human experts. The participants who used NL2Color obtained revised color palettes to their satisfaction in a shorter period and with less effort.",
        "paperId": "8e7741eaa2ed1821fe51111826b4dae87bb9356d"
    },
    {
        "title": "Grounding Language with Visual Affordances over Unstructured Data",
        "firstAuthor": "Oier Mees",
        "url": "https://arxiv.org/pdf/2210.01911",
        "dateSubmitted": "2022-10-04",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills. However, in practice, learning multi-task, language-conditioned robotic skills typically requires large-scale data collection and frequent human intervention to reset the environment or help correcting the current policies. In this work, we propose a novel approach to efficiently learn general-purpose language-conditioned robot skills from unstructured, offline and reset-free data in the real world by exploiting a self-supervised visuo-lingual affordance model, which requires annotating as little as 1% of the total data with language. We evaluate our method in extensive experiments both in simulated and real-world robotic tasks, achieving state-of-the-art performance on the challenging CALVIN benchmark and learning over 25 distinct visuomotor manipulation tasks with a single policy in the real world. We find that when paired with LLMs to break down abstract natural language instructions into subgoals via few-shot prompting, our method is capable of completing long-horizon, multi-tier tasks in the real world, while requiring an order of magnitude less data than previous approaches. Code and videos are available at http://hulc2.cs.uni-freiburg.de.",
        "paperId": "8f84dcbad8cd3b5b4d9229c56bc95f24be859a35"
    },
    {
        "title": "Dialogue Generation Conditional on Predefined Stories: Preliminary Results",
        "firstAuthor": "Chiaki Miyazaki",
        "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10213400.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This paper introduces dialogue generation conditional on predefined stories, which will be called Story2Dialogue. As a starting point for the task, this paper presents benchmark performances using simple but modern baseline methods along with error analysis. The experimental results show that few-shot prompting using large-scale pre-trained language models outperforms human effort in some objective evaluation metrics, but the quality of the generated dialogues is far inferior to humans\u2019 creations in terms of suitability as entertainment content and semantic equivalence to the input story. Regarding suitability as a movie script, human evaluators preferred automatically generated dialogues over those created by human writers in only 20% to 29% of cases. As for semantic equivalence to the input story, 75% to 80% of the automatically generated dialogues were found to be semantically insufficient. The error analysis shows that around 80% of the semantically insufficient dialogues lacked information from the given stories and, conversely, irrelevant utterances (including undefined continuations of conversations) were added in 20% to 26% of the dialogues.",
        "paperId": "8f959ab933f209a3430855c6380b2221ab471a1a"
    },
    {
        "title": "BpHigh at SemEval-2023 Task 7: Can Fine-tuned Cross-encoders Outperform GPT-3.5 in NLI Tasks on Clinical Trial Data?",
        "firstAuthor": "Bhavish Pahwa",
        "url": "https://aclanthology.org/2023.semeval-1.266.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Many nations and organizations have begun collecting and storing clinical trial records for storage and analytical purposes so that medical and clinical practitioners can refer to them on a centralized database over the internet and stay updated with the current clinical information. The amount of clinical trial records have gone through the roof, making it difficult for many medical and clinical practitioners to stay updated with the latest information. To help and support medical and clinical practitioners, there is a need to build intelligent systems that can update them with the latest information in a byte-sized condensed format and, at the same time, leverage their understanding capabilities to help them make decisions. This paper describes our contribution to SemEval 2023 Task 7: Multi-evidence Natural Language Inference for Clinical Trial Data (NLI4CT). Our results show that there is still a need to build domain-specific models as smaller transformer-based models can be finetuned on that data and outperform foundational large language models like GPT-3.5. We also demonstrate how the performance of GPT-3.5 can be increased using few-shot prompting by leveraging the semantic similarity of the text samples and the few-shot train snippets. We will also release our code and our models on open source hosting platforms, GitHub and HuggingFace.",
        "paperId": "90af40a1c1601ade4dcb024b24d946b1661c093d"
    },
    {
        "title": "Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis",
        "firstAuthor": "Chang Liu",
        "url": "https://arxiv.org/pdf/2308.11224",
        "dateSubmitted": "2023-08-22",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large Language Models (LLMs) have garnered considerable interest within both academic and industrial. Yet, the application of LLMs to graph data remains under-explored. In this study, we evaluate the capabilities of four LLMs in addressing several analytical problems with graph data. We employ four distinct evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification. Our results show that: 1) LLMs effectively comprehend graph data in natural language and reason with graph topology. 2) GPT models can generate logical and coherent results, outperforming alternatives in correctness. 3) All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity. 5) GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities. Notably, GPT-4 has demonstrated the capacity to rectify responses from GPT-3.5-turbo and its own previous iterations. The code is available at: https://github.com/Ayame1006/LLMtoGraph.",
        "paperId": "927fc7652e033c9eb17296df087e3e6491112bb0"
    },
    {
        "title": "Revisiting Relation Extraction in the era of Large Language Models",
        "firstAuthor": "Somin Wadhwa",
        "url": "http://arxiv.org/pdf/2305.05003",
        "dateSubmitted": "2023-05-08",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Relation extraction (RE) is the core NLP task of inferring semantic relationships between entities from text. Standard supervised RE techniques entail training modules to tag tokens comprising entity spans and then predict the relationship between them. Recent work has instead treated the problem as a sequence-to-sequence task, linearizing relations between entities as target strings to be generated conditioned on the input. Here we push the limits of this approach, using larger language models (GPT-3 and Flan-T5 large) than considered in prior work and evaluating their performance on standard RE tasks under varying levels of supervision. We address issues inherent to evaluating generative approaches to RE by doing human evaluations, in lieu of relying on exact matching. Under this refined evaluation, we find that: (1) Few-shot prompting with GPT-3 achieves near SOTA performance, i.e., roughly equivalent to existing fully supervised models; (2) Flan-T5 is not as capable in the few-shot setting, but supervising and fine-tuning it with Chain-of-Thought (CoT) style explanations (generated via GPT-3) yields SOTA results. We release this model as a new baseline for RE tasks.",
        "paperId": "97782a67971c4ff1a74bf07e82fe20b2c4bf86c4"
    },
    {
        "title": "The ADAIO System at the BEA-2023 Shared Task: Shared Task Generating AI Teacher Responses in Educational Dialogues",
        "firstAuthor": "Adaeze Adigwe",
        "url": "http://arxiv.org/pdf/2306.05360",
        "dateSubmitted": "2023-06-08",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This paper presents the ADAIO team\u2019s system entry in the Building Educational Applications (BEA) 2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues. The task aims to assess the performance of state-of-the-art generative models as AI teachers in producing suitable responses within a student-teacher dialogue. Our system comprises evaluating various baseline models using OpenAI GPT-3 and designing diverse prompts to prompt the OpenAI models for teacher response generation. After the challenge, our system achieved second place by employing a few-shot prompt-based approach with the OpenAI text-davinci-003 model. The results highlight the few-shot learning capabilities of large-language models, particularly OpenAI\u2019s GPT-3, in the role of AI teachers.",
        "paperId": "97d9d728f924c1f6cc085844136a481cac07c4b0"
    },
    {
        "title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement",
        "firstAuthor": "Zhiheng Xi",
        "url": "http://arxiv.org/pdf/2305.14497",
        "dateSubmitted": "2023-05-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Prompting methods such as Chain-of-Thought (CoT) have shed new light on enhancing the reasoning capabilities of large language models, and researchers have extensively explored the generation process of rationales and answers. However, they have overlooked the potential challenges posed by the poor quality of reasoning problems, which may influence the reasoning performance significantly. In this work, we propose Self-Polish (SP), a novel method that facilitates the model's problem-solving process by prompting them to progressively refine the given problems to be more comprehensible and solvable. Specifically, the method teaches models to eliminate irrelevant information, rearrange the logic structure and organize local conditions into new ones parallelly. SP is orthogonal to all other prompting methods, making it convenient to integrate with state-of-the-art techniques for further improvement. We conduct thorough experiments on five benchmarks to illustrate the effectiveness of the proposed method. For example, with Text-davinci-003, our method boosts the performance of standard few-shot prompting by $8.0\\%$ on GSM8K and $17.8\\%$ on MultiArith; it also improves the performance of CoT by $6.0\\%$ on GSM8K and $6.0\\%$ on MathQA, respectively. Furthermore, our method also showcases impressive performance on robustness evaluation.",
        "paperId": "9a9b1e2968302eb882870537d4af6e2c722dfd1a"
    },
    {
        "title": "Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning",
        "firstAuthor": "Duarte M. Alves",
        "url": null,
        "dateSubmitted": "2023-10-20",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) are a promising avenue for machine translation (MT). However, current LLM-based MT systems are brittle: their effectiveness highly depends on the choice of few-shot examples and they often require extra post-processing due to overgeneration. Alternatives such as finetuning on translation instructions are computationally expensive and may weaken in-context learning capabilities, due to overspecialization. In this paper, we provide a closer look at this problem. We start by showing that adapter-based finetuning with LoRA matches the performance of traditional finetuning while reducing the number of training parameters by a factor of 50. This method also outperforms few-shot prompting and eliminates the need for post-processing or in-context examples. However, we show that finetuning generally degrades few-shot performance, hindering adaptation capabilities. Finally, to obtain the best of both worlds, we propose a simple approach that incorporates few-shot examples during finetuning. Experiments on 10 language pairs show that our proposed approach recovers the original few-shot capabilities while keeping the added benefits of finetuning.",
        "paperId": "9af2b40a6d9edeb3d53d7e612018bdbff993ffd2"
    },
    {
        "title": "Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus",
        "firstAuthor": "Gang Li",
        "url": "http://arxiv.org/pdf/2209.14927",
        "dateSubmitted": "2022-09-29",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Mobile UI understanding is important for enabling various interaction tasks such as UI automation and accessibility. Previous mobile UI modeling often depends on the view hierarchy information of a screen, which directly provides the structural data of the UI, with the hope to bypass challenging tasks of visual modeling from screen pixels. However, view hierarchies are not always available, and are often corrupted with missing object descriptions or misaligned structure information. As a result, despite the use of view hierarchies could offer short-term gains, it may ultimately hinder the applicability and performance of the model. In this paper, we propose Spotlight, a vision-only approach for mobile UI understanding. Specifically, we enhance a vision-language model that only takes the screenshot of the UI and a region of interest on the screen -- the focus -- as the input. This general architecture of Spotlight is easily scalable and capable of performing a range of UI modeling tasks. Our experiments show that our model establishes SoTA results on several representative UI tasks and outperforms previous methods that use both screenshots and view hierarchies as inputs. Furthermore, we explore multi-task learning and few-shot prompting capacities of the proposed models, demonstrating promising results in the multi-task learning direction.",
        "paperId": "9b9fb973e5d3b413baa90648d9eb0743bd889747"
    },
    {
        "title": "Large Language Model Prompt Chaining for Long Legal Document Classification",
        "firstAuthor": "Dietrich Trautmann",
        "url": "https://arxiv.org/pdf/2308.04138",
        "dateSubmitted": "2023-08-08",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Prompting is used to guide or steer a language model in generating an appropriate response that is consistent with the desired outcome. Chaining is a strategy used to decompose complex tasks into smaller, manageable components. In this study, we utilize prompt chaining for extensive legal document classification tasks, which present difficulties due to their intricate domain-specific language and considerable length. Our approach begins with the creation of a concise summary of the original document, followed by a semantic search for related exemplar texts and their corresponding annotations from a training corpus. Finally, we prompt for a label - based on the task - to assign, by leveraging the in-context learning from the few-shot prompt. We demonstrate that through prompt chaining, we can not only enhance the performance over zero-shot, but also surpass the micro-F1 score achieved by larger models, such as ChatGPT zero-shot, using smaller models.",
        "paperId": "9bf587d032e3764720cccd5beaf941f5c32880bc"
    },
    {
        "title": "MindAgent: Emergent Gaming Interaction",
        "firstAuthor": "Ran Gong",
        "url": "https://arxiv.org/pdf/2309.09971",
        "dateSubmitted": "2023-09-18",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large Language Models (LLMs) have the capacity of performing complex scheduling in a multi-agent system and can coordinate these agents into completing sophisticated tasks that require extensive collaboration. However, despite the introduction of numerous gaming frameworks, the community has insufficient benchmarks towards building general multi-agents collaboration infrastructure that encompass both LLM and human-NPCs collaborations. In this work, we propose a novel infrastructure - MindAgent - to evaluate planning and coordination emergent capabilities for gaming interaction. In particular, our infrastructure leverages existing gaming framework, to i) require understanding of the coordinator for a multi-agent system, ii) collaborate with human players via un-finetuned proper instructions, and iii) establish an in-context learning on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new gaming scenario and related benchmark that dispatch a multi-agent collaboration efficiency and supervise multiple agents playing the game simultaneously. We conduct comprehensive evaluations with new auto-metric CoS for calculating the collaboration efficiency. Finally, our infrastructure can be deployed into real-world gaming scenarios in a customized VR version of CUISINEWORLD and adapted in existing broader Minecraft gaming domain. We hope our findings on LLMs and the new infrastructure for general-purpose scheduling and coordination can help shed light on how such skills can be obtained by learning from large language corpora.",
        "paperId": "9c01786f8195d53ad3902fc8d0872784b059adf3"
    },
    {
        "title": "OLaLa: Ontology Matching with Large Language Models",
        "firstAuthor": "S. Hertling",
        "url": null,
        "dateSubmitted": "2023-11-07",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Ontology (and more generally: Knowledge Graph) Matching is a challenging task where information in natural language is one of the most important signals to process. With the rise of Large Language Models, it is possible to incorporate this knowledge in a better way into the matching pipeline. A number of decisions still need to be taken, e.g., how to generate a prompt that is useful to the model, how information in the KG can be formulated in prompts, which Large Language Model to choose, how to provide existing correspondences to the model, how to generate candidates, etc. In this paper, we present a prototype that explores these questions by applying zero-shot and few-shot prompting with multiple open Large Language Models to different tasks of the Ontology Alignment Evaluation Initiative (OAEI). We show that with only a handful of examples and a well-designed prompt, it is possible to achieve results that are en par with supervised matching systems which use a much larger portion of the ground truth.",
        "paperId": "9c392d4532e588f04f94de2ede26d7d6bafe6271"
    },
    {
        "title": "A Study on the Effectiveness of Large Language Models for Translation with Markup",
        "firstAuthor": "Raj Dabre",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "In this paper we evaluate the utility of large language models (LLMs) for translation of text with markup in which the most important and challenging aspect is to correctly transfer markup tags while ensuring that the content, both, inside and outside tags is correctly translated. While LLMs have been shown to be effective for plain text translation, their effectiveness for structured document translation is not well understood. To this end, we experiment with BLOOM and BLOOMZ, which are open-source multilingual LLMs, using zero, one and few-shot prompting, and compare with a domain-specific in-house NMT system using a detag-and-project approach for markup tags. We observe that LLMs with in-context learning exhibit poorer translation quality compared to the domain-specific NMT system, however, they are effective in transferring markup tags, especially the large BLOOM model (176 billion parameters). This is further confirmed by our human evaluation which also reveals the types of errors of the different tag transfer techniques. While LLM-based approaches come with the risk of losing, hallucinating and corrupting tags, they excel at placing them correctly in the translation.",
        "paperId": "9cdffd99a388473f619ae7e166d32dc48e716b00"
    },
    {
        "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
        "firstAuthor": "Xi Ye",
        "url": null,
        "dateSubmitted": "2022-05-06",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",
        "paperId": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be"
    },
    {
        "title": "LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections",
        "firstAuthor": "M. J. Mirza",
        "url": "http://arxiv.org/pdf/2305.18287",
        "dateSubmitted": "2023-05-29",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Recently, large-scale pre-trained Vision and Language (VL) models have set a new state-of-the-art (SOTA) in zero-shot visual classification enabling open-vocabulary recognition of potentially unlimited set of categories defined as simple language prompts. However, despite these great advances, the performance of these zeroshot classifiers still falls short of the results of dedicated (closed category set) classifiers trained with supervised fine tuning. In this paper we show, for the first time, how to reduce this gap without any labels and without any paired VL data, using an unlabeled image collection and a set of texts auto-generated using a Large Language Model (LLM) describing the categories of interest and effectively substituting labeled visual instances of those categories. Using our label-free approach, we are able to attain significant performance improvements over the zero-shot performance of the base VL model and other contemporary methods and baselines on a wide variety of datasets, demonstrating absolute improvement of up to 11.7% (3.8% on average) in the label-free setting. Moreover, despite our approach being label-free, we observe 1.3% average gains over leading few-shot prompting baselines that do use 5-shot supervision.",
        "paperId": "a04883d1d780b438de6c127caf7ebe3d9233e193"
    },
    {
        "title": "Small Language Models Improve Giants by Rewriting Their Outputs",
        "firstAuthor": "Giorgos Vernikos",
        "url": "http://arxiv.org/pdf/2305.13514",
        "dateSubmitted": "2023-05-22",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) have demonstrated impressive few-shot learning capabilities, but they often underperform compared to fine-tuned models on challenging tasks. Furthermore, their large size and restricted access only through APIs make task-specific fine-tuning impractical. Moreover, LLMs are sensitive to different aspects of prompts (e.g., the selection and order of demonstrations) and can thus require time-consuming prompt engineering. In this light, we propose a method to correct LLM outputs without relying on their weights. First, we generate a pool of candidates by few-shot prompting an LLM. Second, we refine the LLM-generated outputs using a smaller model, the LM-corrector (LMCor), which is trained to rank, combine and rewrite the candidates to produce the final target output. Our experiments demonstrate that even a small LMCor model (250M) substantially improves the few-shot performance of LLMs (62B) across diverse tasks. Moreover, we illustrate that the LMCor exhibits robustness against different prompts, thereby minimizing the need for extensive prompt engineering. Finally, we showcase that the LMCor can be seamlessly integrated with different LLMs at inference time, serving as a plug-and-play module to improve their performance.",
        "paperId": "a21de70160c91dcf9b1e7a93fbb32f4b2687860a"
    },
    {
        "title": "Language Models can be Logical Solvers",
        "firstAuthor": "Jiazhan Feng",
        "url": null,
        "dateSubmitted": "2023-11-10",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decision-making. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit reasoning capabilities, but complex logical reasoning remains a challenge. The state-of-the-art, solver-augmented language models, use LLMs to parse natural language logical questions into symbolic representations first and then adopt external logical solvers to take in the symbolic representations and output the answers. Despite their impressive performance, any parsing errors will inevitably result in the failure of the execution of the external logical solver and no answer to the logical questions. In this paper, we introduce LoGiPT, a novel language model that directly emulates the reasoning processes of logical solvers and bypasses the parsing errors by learning to strict adherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly constructed instruction-tuning dataset derived from revealing and refining the invisible reasoning process of deductive solvers. Experimental results on two public deductive reasoning datasets demonstrate that LoGiPT outperforms state-of-the-art solver-augmented LMs and few-shot prompting methods on competitive LLMs like ChatGPT or GPT-4.",
        "paperId": "a2ccffe67a4ccfb10279dc3f0167fe65ae01e471"
    },
    {
        "title": "STREET: A Multi-Task Structured Reasoning and Explanation Benchmark",
        "firstAuthor": "D. Ribeiro",
        "url": "http://arxiv.org/pdf/2302.06729",
        "dateSubmitted": "2023-02-13",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "We introduce STREET, a unified multi-task and multi-domain natural language reasoning and explanation benchmark. Unlike most existing question-answering (QA) datasets, we expect models to not only answer questions, but also produce step-by-step structured explanations describing how premises in the question are used to produce intermediate conclusions that can prove the correctness of a certain answer. We perform extensive evaluation with popular language models such as few-shot prompting GPT-3 and fine-tuned T5. We find that these models still lag behind human performance when producing such structured reasoning steps. We believe this work will provide a way for the community to better train and test systems on multi-step reasoning and explanations in natural language.",
        "paperId": "a3a241e9397fe29b37f96cb5e8f4b8bebed3d3da"
    },
    {
        "title": "Large Language Models as Tax Attorneys: A Case Study in Legal Capabilities Emergence",
        "firstAuthor": "John J. Nay",
        "url": "http://arxiv.org/pdf/2306.07075",
        "dateSubmitted": "2023-06-12",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Better understanding of Large Language Models' (LLMs) legal analysis abilities can contribute to improving the efficiency of legal services, governing artificial intelligence, and leveraging LLMs to identify inconsistencies in law. This paper explores LLM capabilities in applying tax law. We choose this area of law because it has a structure that allows us to set up automated validation pipelines across thousands of examples, requires logical reasoning and maths skills, and enables us to test LLM capabilities in a manner relevant to real-world economic lives of citizens and companies. Our experiments demonstrate emerging legal understanding capabilities, with improved performance in each subsequent OpenAI model release. We experiment with retrieving and utilising the relevant legal authority to assess the impact of providing additional legal context to LLMs. Few-shot prompting, presenting examples of question-answer pairs, is also found to significantly enhance the performance of the most advanced model, GPT-4. The findings indicate that LLMs, particularly when combined with prompting enhancements and the correct legal texts, can perform at high levels of accuracy but not yet at expert tax lawyer levels. As LLMs continue to advance, their ability to reason about law autonomously could have significant implications for the legal profession and AI governance.",
        "paperId": "a6a0963fcf21ed47a2616ca3980f8f4f21e6d5ad"
    },
    {
        "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes",
        "firstAuthor": "Cheng-Yu Hsieh",
        "url": "https://arxiv.org/pdf/2305.02301",
        "dateSubmitted": "2023-05-03",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100% of the dataset. We release the code at: https://github.com/google-research/distilling-step-by-step .",
        "paperId": "aad167be3c902388ea625da4117fcae4325b8b7d"
    },
    {
        "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm",
        "firstAuthor": "Laria Reynolds",
        "url": "https://arxiv.org/pdf/2102.07350",
        "dateSubmitted": "2021-02-15",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models\u2019 novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. We discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.",
        "paperId": "ac3cdb50606f7770eef8e4cd951840a4f71287a0"
    },
    {
        "title": "SPEAK YOUR MIND: INTRODUCING APTLY, THE SOFTWARE PLATFORM THAT TURNS IDEAS INTO WORKING APPS",
        "firstAuthor": "David Y.J. Kim",
        "url": null,
        "dateSubmitted": "2022-11-01",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "MIT Aptly is a tool that uses the technology of large language models to automatically generate mobile apps from written or spoken natural language descriptions. Similar to Github\u2019s Copilot, it is based on OpenAI\u2019s Codex, a specially tuned version of GPT-3. Aptly lets people create programs without requiring any use of coding or knowledge of programming. For example, one can tell Aptly by speaking or typing: Make an app with a text box, a list of six languages and a button that says \u201ctranslate.\u201d When the button is clicked, translate the text into the selected language and show the translation. The result is a complete functioning app for Android or iPhone. The app has a field for user input and six buttons labeled English, Spanish, French, German, Italian, Japanese. Pressing one of the buttons translates the input to the corresponding language. Aptly\u2019s app generation is more than just a syntactic transformation of the input text. Aptly draws upon a large body of code with which it has been trained to provide a context for its app creation. In the above example, Aptly has independently chosen the six languages, something that was not specified in the input text. As most large language models do, Aptly\u2019s performance depends on the input given to OpenAI\u2019s Codex. These inputs are referred to as prompts . Aptly crafts a prompt by providing a set of example pairs (a textual description of an example app and its corresponding code) along with the description of the desired app. Such prompt engineering is referred to as few-shot prompts. In order to optimize Aptly\u2019s performance, when selecting example pairs, we choose the ones that are semantically close to the description of the desired app. The prospect of no-code platforms is currently sparking considerable ferment in enterprises concerned with professional programming careers. Similarly, Aptly poses challenges for research in computational thinking education for K-12 students. Much of the present-day curriculum emphasizes implementing computational artifacts using text-based coding with Python or block-based coding with Scratch or App Inventor. What will be the foundations for that curriculum when tools like Aptly are common and the transition from ideas to running programs can be accomplished automatically? This presentation will demonstrate Aptly\u2019s preliminary performance and review its implementation, which incorporates OpenAI Codex, Amazon Alexa and MIT App Inventor.",
        "paperId": "afd834af31f043e7c1d348c6a51d299d029dafca"
    },
    {
        "title": "Complex Reasoning in Natural Languag",
        "firstAuthor": "Wenting Zhao",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Teaching machines to reason over texts has been a long-standing goal of natural language processing (NLP). To this end, researchers have designed a diverse set of complex reasoning tasks that involve compositional reasoning, knowledge retrieval, grounding, commonsense reasoning, etc. A standard choice for building systems that perform a desired type of reasoning is to fine-tune a pretrained language model (LM) on specific downstream tasks. However, recent research has demonstrated that such a straightforward approach is often brittle. For example, Elazar et al. (2021) and Branco et al. (2021) show that, on question-answering (QA) tasks, similar performance can be achieved with questions removed from the inputs. Min et al. (2019), Chen and Durrett (2019), and Tang et al. (2021) show that models trained on multi-hop QA do not generalize to answer single-hop questions. The reasoning capabilities of these models thus remain at a surface level, i.e., exploiting data patterns. Consequently, augmenting LMs with techniques that make them robust and effective becomes an active research area. We will start the tutorial by providing an overview of complex reasoning tasks where the standard application of pretrained language models fails. This tutorial then reviews recent promising directions for tackling these tasks. Specifically, we focus on the following groups of approaches that explicitly consider problem structures: (1) knowledge-augmented methods, where the knowledge is either incorporated during fine-tuning or pretraining; (2) few-shot prompting methods, which effectively guide the models to follow instructions; (3) neuro-symbolic methods, which produce explicit intermediate representations; and, (4) rationale-based methods, one of the most popular forms of the neuro-symbolic methods, which highlight subsets of input as explanations for individual model predictions.",
        "paperId": "b30511210a20eed85f6c6b27a5b825eac3fd5153"
    },
    {
        "title": "Does GPT-3 Grasp Metaphors? Identifying Metaphor Mappings with Generative Language Models",
        "firstAuthor": "Lennart Wachowiak",
        "url": "https://aclanthology.org/2023.acl-long.58.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Conceptual metaphors present a powerful cognitive vehicle to transfer knowledge structures from a source to a target domain. Prior neural approaches focus on detecting whether natural language sequences are metaphoric or literal. We believe that to truly probe metaphoric knowledge in pre-trained language models, their capability to detect this transfer should be investigated. To this end, this paper proposes to probe the ability of GPT-3 to detect metaphoric language and predict the metaphor\u2019s source domain without any pre-set domains. We experiment with different training sample configurations for fine-tuning and few-shot prompting on two distinct datasets. When provided 12 few-shot samples in the prompt, GPT-3 generates the correct source domain for a new sample with an accuracy of 65.15% in English and 34.65% in Spanish. GPT\u2019s most common error is a hallucinated source domain for which no indicator is present in the sentence. Other common errors include identifying a sequence as literal even though a metaphor is present and predicting the wrong source domain based on specific words in the sequence that are not metaphorically related to the target domain.",
        "paperId": "b31fb03a86cd44860f1c38e5c7032d9aed10d2f2"
    },
    {
        "title": "The Potential and Pitfalls of using a Large Language Model such as ChatGPT or GPT-4 as a Clinical Assistant",
        "firstAuthor": "Jingqing Zhang",
        "url": "https://arxiv.org/pdf/2307.08152",
        "dateSubmitted": "2023-07-16",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Recent studies have demonstrated promising performance of ChatGPT and GPT-4 on several medical domain tasks. However, none have assessed its performance using a large-scale real-world electronic health record database, nor have evaluated its utility in providing clinical diagnostic assistance for patients across a full range of disease presentation. We performed two analyses using ChatGPT and GPT-4, one to identify patients with specific medical diagnoses using a real-world large electronic health record database and the other, in providing diagnostic assistance to healthcare workers in the prospective evaluation of hypothetical patients. Our results show that GPT-4 across disease classification tasks with chain of thought and few-shot prompting can achieve performance as high as 96% F1 scores. For patient assessment, GPT-4 can accurately diagnose three out of four times. However, there were mentions of factually incorrect statements, overlooking crucial medical findings, recommendations for unnecessary investigations and overtreatment. These issues coupled with privacy concerns, make these models currently inadequate for real world clinical use. However, limited data and time needed for prompt engineering in comparison to configuration of conventional machine learning workflows highlight their potential for scalability across healthcare applications.",
        "paperId": "b3d6fec3f1a878b0c612f0ffed820b045c2c46d8"
    },
    {
        "title": "Do GPTs Produce Less Literal Translations?",
        "firstAuthor": "Vikas Raunak",
        "url": "http://arxiv.org/pdf/2305.16806",
        "dateSubmitted": "2023-05-26",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating sentences that contain idiomatic expressions.",
        "paperId": "b4170009de40c1c46adea6a314734434ecd4b0dc"
    },
    {
        "title": "ADELT: Transpilation Between Deep Learning Frameworks",
        "firstAuthor": "Linyuan Gong",
        "url": "http://arxiv.org/pdf/2303.03593",
        "dateSubmitted": "2023-03-07",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "We propose Adversarial DEep Learning Transpiler (ADELT) for source-to-source transpilation between deep learning frameworks. Unlike prior approaches, we decouple the transpilation of code skeletons and the mapping of API keywords (an API function name or a parameter name). ADELT transpile code skeletons using few-shot prompting on big language models. Based on contextual embeddings extracted by a BERT for code, we train aligned API embeddings in a domain-adversarial setup, upon which we generate a dictionary for keyword translation. The model is trained on our unlabeled DL corpus from web crawl data, without using any hand-crafted rules and parallel data. Our method outperforms state-of-the-art transpilers on multiple transpilation pairs including PyTorch-Keras and PyTorch-MXNet by 15.9pts and 12.0pts in exact match scores respectively.",
        "paperId": "b6bea98ca29267acbebca6cdf64eb07a5671e000"
    },
    {
        "title": "Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models",
        "firstAuthor": "Ratish Puduppully",
        "url": "http://arxiv.org/pdf/2305.13085",
        "dateSubmitted": "2023-05-22",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This study investigates machine translation between related languages i.e., languages within the same family that share linguistic characteristics such as word order and lexical similarity. Machine translation through few-shot prompting leverages a small set of translation pair examples to generate translations for test sentences. This procedure requires the model to learn how to generate translations while simultaneously ensuring that token ordering is maintained to produce a fluent and accurate translation. We propose that for related languages, the task of machine translation can be simplified by leveraging the monotonic alignment characteristic of such languages. We introduce DecoMT, a novel approach of few-shot prompting that decomposes the translation process into a sequence of word chunk translations. Through automatic and human evaluation conducted on multiple related language pairs across various language families, we demonstrate that our proposed approach of decomposed prompting surpasses multiple established few-shot baseline approaches. For example, DecoMT outperforms the strong few-shot prompting BLOOM model with an average improvement of 8 chrF++ scores across the examined languages.",
        "paperId": "b6e5855b6a4e425ba251a93516f2bccffe5ba403"
    },
    {
        "title": "Prompt a Robot to Walk with Large Language Models",
        "firstAuthor": "Yen-Jen Wang",
        "url": "https://arxiv.org/pdf/2309.09969",
        "dateSubmitted": "2023-09-18",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) pre-trained on vast internet-scale data have showcased remarkable capabilities across diverse domains. Recently, there has been escalating interest in deploying LLMs for robotics, aiming to harness the power of foundation models in real-world settings. However, this approach faces significant challenges, particularly in grounding these models in the physical world and in generating dynamic robot motions. To address these issues, we introduce a novel paradigm in which we use few-shot prompts collected from the physical environment, enabling the LLM to autoregressively generate low-level control commands for robots without task-specific fine-tuning. Experiments across various robots and environments validate that our method can effectively prompt a robot to walk. We thus illustrate how LLMs can proficiently function as low-level feedback controllers for dynamic motion control even in high-dimensional robotic systems. The project website and source code can be found at: https://prompt2walk.github.io/ .",
        "paperId": "b70075b496c1f519093884945be5670c32cbceed"
    },
    {
        "title": "Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment Analysis",
        "firstAuthor": "Md. Arid Hasan",
        "url": "https://arxiv.org/pdf/2308.10783",
        "dateSubmitted": "2023-08-21",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "The rapid expansion of the digital world has propelled sentiment analysis into a critical tool across diverse sectors such as marketing, politics, customer service, and healthcare. While there have been significant advancements in sentiment analysis for widely spoken languages, low-resource languages, such as Bangla, remain largely under-researched due to resource constraints. Furthermore, the recent unprecedented performance of Large Language Models (LLMs) in various applications highlights the need to evaluate them in the context of low-resource languages. In this study, we present a sizeable manually annotated dataset encompassing 33,605 Bangla news tweets and Facebook comments. We also investigate zero- and few-shot in-context learning with several language models, including Flan-T5, GPT-4, and Bloomz, offering a comparative analysis against fine-tuned models. Our findings suggest that monolingual transformer-based models consistently outperform other models, even in zero and few-shot scenarios. To foster continued exploration, we intend to make this dataset and our research tools publicly available to the broader research community. In the spirit of further research, we plan to make this dataset and our experimental resources publicly accessible to the wider research community.",
        "paperId": "bc70af9248d210663edf22e5fc84ca9313c697b0"
    },
    {
        "title": "Jurassic is (almost) All You Need: Few-Shot Meaning-to-Text Generation for Open-Domain Dialogue",
        "firstAuthor": "Lena Reed",
        "url": "https://arxiv.org/pdf/2110.08094",
        "dateSubmitted": "2021-10-15",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": null,
        "paperId": "bd16d3d5f4f237bd76395b17e56cde3f01a41584"
    },
    {
        "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
        "firstAuthor": "Tu Vu",
        "url": "https://arxiv.org/pdf/2310.03214",
        "dateSubmitted": "2023-10-05",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.",
        "paperId": "be177300487b6d0f25e6cade9a31900454b13281"
    },
    {
        "title": "Designing an Effective Prompt for Biomechanics Research using ChatGPT and Open-Source Models: A Human-in-the-Loop Approach",
        "firstAuthor": "",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": ": This project aims to develop a protocol for generating accurate prompts in ChatGPT to facilitate the development and analysis of biomechanical models. The goal is to leverage free software and packages, such as OpenSim and Google Colab, along with ChatGPT, to generate Python code based on the API of the biomechanical models. While this framework focuses on ChatGPT, OpenSim, and Colab, it can be extended to other Language Models (LMs), biomechanical models, and Integrated Development (ID) environments. The framework begins by iteratively refining the prompt through a series of interactions with ChatGPT, leveraging techniques like few-shot prompting and fact-checking. The end result is a well-crafted prompt that generates detailed Python code, easily executable in Colab, to obtain specific biomechanical outputs, such as the center of mass. By sharing these models with the community, this research aims to enhance our understanding of human and animal biomechanics, prevent injuries, and improve overall performance.",
        "paperId": "c0a389f3282cc6b33d9bf129fb1a4a211647969b"
    },
    {
        "title": "Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering",
        "firstAuthor": "Zixian Huang",
        "url": "http://arxiv.org/pdf/2306.04508",
        "dateSubmitted": "2023-06-07",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Whereas the recent emergence of large language models (LLMs) like ChatGPT has exhibited impressive general performance, it still has a large gap with fully-supervised models on specific tasks such as multi-span question answering. Previous researches found that in-context learning is an effective approach to exploiting LLM, by using a few task-related labeled data as demonstration examples to construct a few-shot prompt for answering new questions. A popular implementation is to concatenate a few questions and their correct answers through simple templates, informing LLM of the desired output. In this paper, we propose a novel way of employing labeled data such that it also informs LLM of some undesired output, by extending demonstration examples with feedback about answers predicted by an off-the-shelf model, e.g., correct, incorrect, or incomplete. Experiments on three multi-span question answering datasets as well as a keyphrase extraction dataset show that our new prompting strategy consistently improves LLM's in-context learning performance.",
        "paperId": "c1647923704251875f4160e91b59afbbdc58483e"
    },
    {
        "title": "Majority Rule: better patching via Self-Consistency",
        "firstAuthor": "Toufique Ahmed",
        "url": "https://arxiv.org/pdf/2306.00108",
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "\u2014Large Language models (LLMs) can be induced to solve non-trivial problems with \u201cfew-shot\u201d prompts including illustrative problem-solution examples. Now if the few-shots also include \u201cchain of thought\u201d ( C oT ) explanations, which are of the form problem-explanation-solution , LLMs will generate a \u201cexplained\u201d solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ( S - C ) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations ) tend to be even more likely to be correct!Unfortunately, the use of this highly-performant S - C (or even C oT ) approach in software engineering settings is hampered by the lack of explanations ; most software datasets lack explanations. In this paper, we describe an application of the S - C approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.",
        "paperId": "c1a3dc24a2677b2c8a69ffd336b2112e1aa705b6"
    },
    {
        "title": "Improving Few-Shot Prompts with Relevant Static Analysis Products",
        "firstAuthor": "Toufique Ahmed",
        "url": "https://arxiv.org/pdf/2304.06815",
        "dateSubmitted": "2023-04-13",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large Language Models (LLM) are a new class of computation engines,\"programmed\"via prompt engineering. We are still learning how to best\"program\"these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously have a collection of semantics facts in mind when working on coding tasks. Mostly these are shallow, simple facts arising from a quick read. For a function, examples of facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc. One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them inherently capable of doing this simple level of\"code analysis\"and extracting such information, implicitly, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM's prompt with semantic facts explicitly, actually helps. Prior work shows that LLM performance on code summarization benefits from few-shot samples drawn either from the same-project or from examples found via information retrieval methods (such as BM25). While summarization performance has steadily increased since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization. We find that adding semantic facts actually does help! This approach improves performance in several different settings suggested by prior work, including for two different Large Language Models. In most cases, improvement nears or exceeds 2 BLEU; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU.",
        "paperId": "c2391a8c8e24a450f00810ecb441e26413ea3791"
    },
    {
        "title": "Towards Zero-Label Language Learning",
        "firstAuthor": "Zirui Wang",
        "url": null,
        "dateSubmitted": "2021-09-19",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This paper explores zero-label learning in Natural Language Processing (NLP), whereby no human-annotated data is used anywhere during training and models are trained purely on synthetic data. At the core of our framework is a novel approach for better leveraging the powerful pretrained language models. Specifically, inspired by the recent success of few-shot inference on GPT-3, we present a training data creation procedure named Unsupervised Data Generation (UDG), which leverages few-shot prompts to synthesize high-quality training data without real human annotations. Our method enables zero-label learning as we train task-specific models solely on the synthetic data, yet we achieve better or comparable results from strong baseline models trained on human-labeled data. Furthermore, when mixed with labeled data, our approach serves as a highly effective data augmentation procedure, achieving new state-of-the-art results on the SuperGLUE benchmark.",
        "paperId": "c2a79e2a65b721d4de5f6d4806323174b9f8f393"
    },
    {
        "title": "LLM Based Generation of Item-Description for Recommendation System",
        "firstAuthor": "Arkadeep Acharya",
        "url": null,
        "dateSubmitted": "2023-09-14",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "The description of an item plays a pivotal role in providing concise and informative summaries to captivate potential viewers and is essential for recommendation systems. Traditionally, such descriptions were obtained through manual web scraping techniques, which are time-consuming and susceptible to data inconsistencies. In recent years, Large Language Models (LLMs), such as GPT-3.5, and open source LLMs like Alpaca have emerged as powerful tools for natural language processing tasks. In this paper, we have explored how we can use LLMs to generate detailed descriptions of the items. To conduct the study, we have used the MovieLens 1M dataset comprising movie titles and the Goodreads Dataset consisting of names of books and subsequently, an open-sourced LLM, Alpaca, was prompted with few-shot prompting on this dataset to generate detailed movie descriptions considering multiple features like the names of the cast and directors for the ML dataset and the names of the author and publisher for the Goodreads dataset. The generated description was then compared with the scraped descriptions using a combination of Top Hits, MRR, and NDCG as evaluation metrics. The results demonstrated that LLM-based movie description generation exhibits significant promise, with results comparable to the ones obtained by web-scraped descriptions.",
        "paperId": "c47ba62dd18c70aafa04a8e04e105f624090217c"
    },
    {
        "title": "Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models",
        "firstAuthor": "Gangwoo Kim",
        "url": null,
        "dateSubmitted": "2023-10-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Questions in open-domain question answering are often ambiguous, allowing multiple interpretations. One approach to handling them is to identify all possible interpretations of the ambiguous question (AQ) and to generate a long-form answer addressing them all, as suggested by Stelmakh et al., (2022). While it provides a comprehensive response without bothering the user for clarification, considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge. To cope with the challenge, we propose a novel framework, Tree of Clarifications (ToC): It recursively constructs a tree of disambiguations for the AQ -- via few-shot prompting leveraging external knowledge -- and uses it to generate a long-form answer. ToC outperforms existing baselines on ASQA in a few-shot setup across the metrics, while surpassing fully-supervised baselines trained on the whole training set in terms of Disambig-F1 and Disambig-ROUGE. Code is available at https://github.com/gankim/tree-of-clarifications.",
        "paperId": "c49fd6cac5382cdbc2bc31be195e42bc28dc615d"
    },
    {
        "title": "Benchmarking Arabic AI with Large Language Models",
        "firstAuthor": "Ahmed Abdelali",
        "url": "http://arxiv.org/pdf/2305.14982",
        "dateSubmitted": "2023-05-24",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "With large Foundation Models (FMs), language technologies (AI in general) are entering a new paradigm: eliminating the need for developing large-scale task-specific datasets and supporting a variety of tasks through set-ups ranging from zero-shot to few-shot learning. However, understanding FMs capabilities requires a systematic benchmarking effort by comparing FMs performance with the state-of-the-art (SOTA) task-specific models. With that goal, past work focused on the English language and included a few efforts with multiple languages. Our study contributes to ongoing research by evaluating FMs performance for standard Arabic NLP and Speech processing, including a range of tasks from sequence tagging to content classification across diverse domains. We start with zero-shot learning using GPT-3.5-turbo, Whisper, and USM, addressing 33 unique tasks using 59 publicly available datasets resulting in 96 test setups. For a few tasks, FMs performs on par or exceeds the performance of the SOTA models but for the majority it under-performs. Given the importance of prompt for the FMs performance, we discuss our prompt strategies in detail and elaborate on our findings. Our future work on Arabic AI will explore few-shot prompting, expand the range of tasks, and investigate additional open-source models.",
        "paperId": "c5fa70db839fd05b1111f3586a601d8a93e78d0c"
    },
    {
        "title": "Internet-augmented language models through few-shot prompting for open-domain question answering",
        "firstAuthor": "Angeliki Lazaridou",
        "url": "https://arxiv.org/pdf/2203.05115",
        "dateSubmitted": "2022-03-10",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "In this work, we aim to capitalize on the unique few-shot capabilities of large-scale language models (LSLMs) to overcome some of their challenges with respect to grounding to factual and up-to-date information. Motivated by semi-parametric language models (LMs), which ground their decisions in external retrieved evidence, we use few-shot prompting to learn to condition LMs on information returned from the web using Google Search, a broad and constantly updated knowledge source. Our approach does not involve fine-tuning or learning additional parameters, thus making it applicable to any LM, offering therefore a strong baseline. Indeed, we find that LMs conditioned on the web surpass performance of closed-book models of similar, or even larger, model sizes in open-domain question answering. Finally, we find that increasing the inference-time compute of models, achieved via using multiple retrieved evidences to generate multiple answers followed by a reranking stage that uses scores generated by the same LMs, leads to better performance and alleviates lower performance of smaller few-shot LMs. All in all, our findings suggest that it might be beneficial to slow down the race towards the biggest model and instead shift attention towards finding more effective ways to use models, including but not limited to, better prompting or increasing inference-time compute.",
        "paperId": "c70eb74e09c41e8fcc71dd59e3b4d631f657f7cd"
    },
    {
        "title": "Investigating the Perception of the Future in GPT-3, -3.5 and GPT-4",
        "firstAuthor": "Diana Kozachek",
        "url": null,
        "dateSubmitted": "2023-06-19",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This study explores the potential of GPT-3, GPT-3.5, and GPT-4, in generating human-like future scenarios to investigate each model's ability to perceive time. The methodology combines a coding-based experiment and an expert survey. The investigation involves fine- and prompt-tuning GPT-3, prompt-tuning GPT-3.5, and few-shot prompting GPT-4 with human-made future scenarios. The models and output are quantitatively and qualitatively analyzed. The survey invited practitioners from fields of foresight and futurology, AI, and NLP to assess whether differences in output can be identified. This study's findings suggest that GPT-3 and GPT-4 generated scenarios are difficult to distinguish from human-made ones, while GPT-3.5 performed more poorly. Yet none of the models can differentiate time horizons and their respective effects on the future from each other. And while no one knows the shape of things to come, this lack of understanding of a core concept of life invites future investigations.",
        "paperId": "c78cd6d6280961511ac73ce64a788cc632fe5550"
    },
    {
        "title": "An Evaluation of Log Parsing with ChatGPT",
        "firstAuthor": "Van-Hoang Le",
        "url": "https://arxiv.org/pdf/2306.01590",
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "\u2014Software logs play an essential role in ensuring the reliability and maintainability of large-scale software systems, as they are often the sole source of runtime information. Log parsing, which converts raw log messages into structured data, is an important initial step towards downstream log analytics. In recent studies, ChatGPT, the current cutting-edge large language model (LLM), has been widely applied to a wide range of software engineering tasks. However, its performance in automated log parsing remains unclear. In this paper, we evaluate ChatGPT\u2019s ability to undertake log parsing by addressing two research questions. (1) Can ChatGPT effectively parse logs? (2) How does ChatGPT perform with different prompting methods? Our results show that ChatGPT can achieve promising results for log parsing with appropriate prompts, especially with few-shot prompting. Based on our findings, we outline several challenges and opportunities for ChatGPT-based log parsing.",
        "paperId": "c7f0c31bd260ccafd6995350f30707b3cf03ce9e"
    },
    {
        "title": "Unleashing the Power of ChatGPT for Translation: An Empirical Study",
        "firstAuthor": "Yuan Gao",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "The recently released ChatGPT has demonstrated surprising abilities in natural language understanding and natural language generation. Machine translation is an important and extensively studied task in the \ufb01eld of natural language processing, which heavily relies on the abilities of language understanding and generation. Thus, in this paper, we explore how to assist machine translation with ChatGPT. We adopt several translation prompts on a wide range of translations. Our experimental results show that ChatGPT with designed translation prompts can achieve comparable or better performance over professional translation systems for high-resource language translations but lags behind signi\ufb01cantly on low-resource translations. We further evaluate the translation quality using multiple references, and ChatGPT achieves superior performance compared to the professional systems. We also conduct experiments on domain-speci\ufb01c translations, the \ufb01nal results show that ChatGPT is able to comprehend the provided domain keyword and adjust accordingly to output proper translations. At last, we perform few-shot prompts that show consistent improvement across different base prompts. Our work provides empirical evidence that ChatGPT still has great potential in translations.",
        "paperId": "c853c8aeaabadfde86a97d35867b0f5778700dd7"
    },
    {
        "title": "Is ChatGPT a Good Recommender? A Preliminary Study",
        "firstAuthor": "Junling Liu",
        "url": "http://arxiv.org/pdf/2304.10149",
        "dateSubmitted": "2023-04-20",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Recommendation systems have witnessed significant advancements and have been widely used over the past decades. However, most traditional recommendation methods are task-specific and therefore lack efficient generalization ability. Recently, the emergence of ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. Nonetheless, the application of ChatGPT in the recommendation domain has not been thoroughly investigated. In this paper, we employ ChatGPT as a general-purpose recommendation model to explore its potential for transferring extensive linguistic and world knowledge acquired from large-scale corpora to recommendation scenarios. Specifically, we design a set of prompts and evaluate ChatGPT's performance on five recommendation scenarios. Unlike traditional recommendation methods, we do not fine-tune ChatGPT during the entire evaluation process, relying only on the prompts themselves to convert recommendation tasks into natural language tasks. Further, we explore the use of few-shot prompting to inject interaction information that contains user potential interest to help ChatGPT better understand user needs and interests. Comprehensive experimental results on Amazon Beauty dataset show that ChatGPT has achieved promising results in certain tasks and is capable of reaching the baseline level in others. We conduct human evaluations on two explainability-oriented tasks to more accurately evaluate the quality of contents generated by different models. And the human evaluations show ChatGPT can truly understand the provided information and generate clearer and more reasonable results. We hope that our study can inspire researchers to further explore the potential of language models like ChatGPT to improve recommendation performance and contribute to the advancement of the recommendation systems field.",
        "paperId": "ca7bd64d372e3bcb3f4633ca4a20291ff57de3c3"
    },
    {
        "title": "Problematic Webpage Identification: A Trilogy of Hatespeech, Search Engines and GPT",
        "firstAuthor": "Ojasvin Sood",
        "url": "https://aclanthology.org/2023.woah-1.13.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "In this paper, we introduce a fine-tuned transformer-based model focused on problematic webpage classification to identify webpages promoting hate and violence of various forms. Due to the unavailability of labelled problematic webpage data, first we propose a novel webpage data collection strategy which leverages well-studied short-text hate speech datasets. We have introduced a custom GPT-4 few-shot prompt annotation scheme taking various webpage features to label the prohibitively expensive webpage annotation task. The resulting annotated data is used to build our problematic webpage classification model. We report the accuracy (87.6% F1-score) of our webpage classification model and conduct a detailed comparison of it against other state-of-the-art hate speech classification model on problematic webpage identification task. Finally, we have showcased the importance of various webpage features in identifying a problematic webpage.",
        "paperId": "cb9c917af837d016b5977b9f158a713e1318e039"
    },
    {
        "title": "Legal Prompting: Teaching a Language Model to Think Like a Lawyer",
        "firstAuthor": "Fang Yu",
        "url": "http://arxiv.org/pdf/2212.01326",
        "dateSubmitted": "2022-12-02",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models that are capable of zero or few-shot prompting approaches have given rise to the new research area of prompt engineering. Recent advances showed that for example Chain-of-Thought (CoT) prompts can improve arithmetic or common sense tasks significantly. We explore how such approaches fare with legal reasoning tasks and take the COLIEE entailment task based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning approaches. Our findings show that while CoT prompting and fine-tuning with explanations approaches show improvements, the best results are produced by prompts that are derived from specific legal reasoning techniques such as IRAC (Issue, Rule, Application, Conclusion). Based on our experiments we improve the 2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best system of 0.6789 accuracy with an accuracy of 0.7431.",
        "paperId": "cc43306e22dbfd5bc35251ab8c8ba37e4fc2a1b3"
    },
    {
        "title": "Towards Expert Systems for Improved Customer Services Using ChatGPT as an Inference Engine",
        "firstAuthor": "C. P. Ezenkwu",
        "url": "https://rgu-repository.worktribe.com/preview/1987218/EZENKWU%202023%20Towards%20expert%20systems%20%28AAM%29.pdf",
        "dateSubmitted": "2023-07-14",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "By harnessing both implicit and explicit customer data, companies can develop a more comprehensive understanding of their consumers, leading to better customer engagement and experience, and improved loyalty. As a result, businesses have embraced many AI technologies, including chatbots, sentiment analysis, voice assistants, predictive analytics, and natural language processing, within customer services and e-commerce. The arrival of ChatGPT, a state-of-the-art deep learning model trained with general knowledge in mind, has brought about a paradigm shift in how companies approach AI applications. However, given that most business problems are bespoke and require specialised domain expertise, ChatGPT needs to be aligned with the requisite task-oriented ability to solve these issues. This paper presents an iterative procedure that incorporates expert system development process models and prompt engineering, in the design of descriptive knowledge and few-shot prompts, as are necessary for ChatGPT-powered expert systems applications within customer services. Furthermore, this paper explores potential application areas for ChatGPT-powered expert systems in customer services, presenting opportunities for their effective utilisation in the business sector.",
        "paperId": "cc5869343d670c801512de910ab3bf0ca7bc5c4a"
    },
    {
        "title": "Utilizing Language Models to Expand Vision-Based Commonsense Knowledge Graphs",
        "firstAuthor": "Navid Rezaei",
        "url": "https://www.mdpi.com/2073-8994/14/8/1715/pdf?version=1660727694",
        "dateSubmitted": "2022-08-17",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "The introduction and ever-growing size of the transformer deep-learning architecture have had a tremendous impact not only in the field of natural language processing but also in other fields. The transformer-based language models have contributed to a renewed interest in commonsense knowledge due to the abilities of deep learning models. Recent literature has focused on analyzing commonsense embedded within the pre-trained parameters of these models and embedding missing commonsense using knowledge graphs and fine-tuning. We base our current work on the empirically proven language understanding of very large transformer-based language models to expand a limited commonsense knowledge graph, initially generated only on visual data. The few-shot-prompted pre-trained language models can learn the context of an initial knowledge graph with less bias than language models fine-tuned on a large initial corpus. It is also shown that these models can offer new concepts that are added to the vision-based knowledge graph. This two-step approach of vision mining and language model prompts results in the auto-generation of a commonsense knowledge graph well equipped with physical commonsense, which is human commonsense gained by interacting with the physical world. To prompt the language models, we adapted the chain-of-thought method of prompting. To the best of our knowledge, it is a novel contribution to the domain of the generation of commonsense knowledge, which can result in a five-fold cost reduction compared to the state-of-the-art. Another contribution is assigning fuzzy linguistic terms to the generated triples. The process is end to end in the context of knowledge graphs. It means the triples are verbalized to natural language, and after being processed, the results are converted back to triples and added to the commonsense knowledge graph.",
        "paperId": "cc7df8fa3b642269531c25af065c2cc78e5000e0"
    },
    {
        "title": "Query2doc: Query Expansion with Large Language Models",
        "firstAuthor": "Liang Wang",
        "url": "https://arxiv.org/pdf/2303.07678",
        "dateSubmitted": "2023-03-14",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.",
        "paperId": "ccc772d88c231275f24c4fac9b28bbe0942e1107"
    },
    {
        "title": "How to Design Translation Prompts for ChatGPT: An Empirical Study",
        "firstAuthor": "Yuan Gao",
        "url": "http://arxiv.org/pdf/2304.02182",
        "dateSubmitted": "2023-04-05",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "The recently released ChatGPT has demonstrated surprising abilities in natural language understanding and natural language generation. Machine translation relies heavily on the abilities of language understanding and generation. Thus, in this paper, we explore how to assist machine translation with ChatGPT. We adopt several translation prompts on a wide range of translations. Our experimental results show that ChatGPT with designed translation prompts can achieve comparable or better performance over commercial translation systems for high-resource language translations. We further evaluate the translation quality using multiple references, and ChatGPT achieves superior performance compared to commercial systems. We also conduct experiments on domain-specific translations, the final results show that ChatGPT is able to comprehend the provided domain keyword and adjust accordingly to output proper translations. At last, we perform few-shot prompts that show consistent improvement across different base prompts. Our work provides empirical evidence that ChatGPT still has great potential in translations.",
        "paperId": "cd77ea482d9245f3fcaeb670261a00c3fb5cabbd"
    },
    {
        "title": "Passive learning of active causal strategies in agents and language models",
        "firstAuthor": "Andrew Kyle Lampinen",
        "url": "https://arxiv.org/pdf/2305.16183",
        "dateSubmitted": "2023-05-25",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training. We then show that strategies for causal intervention and exploitation can be generalized from passive data even in a more complex environment with high-dimensional observations, with the support of natural language explanations. Explanations can even allow passive learners to generalize out-of-distribution from perfectly-confounded training data. Finally, we show that language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing examples of experimentation, together with explanations and reasoning. These results highlight the surprising power of passive learning of active causal strategies, and may help to understand the behaviors and capabilities of language models.",
        "paperId": "ce0154d9251f67c262512b6e598f3aa3ba9fe9a4"
    },
    {
        "title": "Are GPT-3 Models Pragmatic Reasoners?",
        "firstAuthor": "",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "While large language models (LLMs) have shown incredible performances over natural language understanding (NLU) tasks, a question that remains to be solved is whether they are pragmatic listeners that can reason over ambiguous scenarios and act optimally. In this project, we test the pragmatic reasoning capabilities of GPT-3 models in ambiguous scenarios under the Rational Speech Act (RSA) framework in a reference game setting. Testing the models with zero-shot, few-shot, and fewshot chain-of-thought (CoT) prompting, we find that although few-shot prompting produces little performance improvement, and indeed often worsens model performance, few-shot CoT prompting dramatically improves the performance of davinci models. 1 Key Information to include \u2022 Mentor: Zhengxuan Wuwuzhengx@stanford.edu \u2022 External Collaborators (if you have any): N/A \u2022 Sharing project: N/A",
        "paperId": "d042e7f8b93720be3454a6c7c6cee9265e9535b4"
    },
    {
        "title": "NAISTeacher: A Prompt and Rerank Approach to Generating Teacher Utterances in Educational Dialogues",
        "firstAuthor": "Justin Vasselli",
        "url": "https://aclanthology.org/2023.bea-1.63.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This paper presents our approach to the BEA 2023 shared task of generating teacher responses in educational dialogues, using the Teacher-Student Chatroom Corpus. Our system prompts GPT-3.5-turbo to generate initial suggestions, which are then subjected to reranking. We explore multiple strategies for candidate generation, including prompting for multiple candidates and employing iterative few-shot prompts with negative examples. We aggregate all candidate responses and rerank them based on DialogRPT scores. To handle consecutive turns in the dialogue data, we divide the task of generating teacher utterances into two components: teacher replies to the student and teacher continuations of previously sent messages. Through our proposed methodology, our system achieved the top score on both automated metrics and human evaluation, surpassing the reference human teachers on the latter.",
        "paperId": "d0482bd01de9d0912acf4e5338c7799eba4b9360"
    },
    {
        "title": "SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data",
        "firstAuthor": "Ruoxi Sun",
        "url": null,
        "dateSubmitted": "2023-11-06",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Text-to-SQL aims to automate the process of generating SQL queries on a database from natural language text. In this work, we propose\"SQLPrompt\", tailored to improve the few-shot prompting capabilities of Text-to-SQL for Large Language Models (LLMs). Our methods include innovative prompt design, execution-based consistency decoding strategy which selects the SQL with the most consistent execution outcome among other SQL proposals, and a method that aims to improve performance by diversifying the SQL proposals during consistency selection with different prompt designs (\"MixPrompt\") and foundation models (\"MixLLMs\"). We show that \\emph{SQLPrompt} outperforms previous approaches for in-context learning with few labeled data by a large margin, closing the gap with finetuning state-of-the-art with thousands of labeled data.",
        "paperId": "d1e10a539cc83d3df1b612fa098ceea1be63cc29"
    },
    {
        "title": "Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries",
        "firstAuthor": "Noel Ngu",
        "url": "https://arxiv.org/pdf/2308.11189",
        "dateSubmitted": "2023-08-22",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Error prediction in large language models often relies on domain-specific information. In this paper, we present measures for quantification of error in the response of a large language model based on the diversity of responses to a given prompt - hence independent of the underlying application. We describe how three such measures - based on entropy, Gini impurity, and centroid distance - can be employed. We perform a suite of experiments on multiple datasets and temperature settings to demonstrate that these measures strongly correlate with the probability of failure. Additionally, we present empirical results demonstrating how these measures can be applied to few-shot prompting, chain-of-thought reasoning, and error detection.",
        "paperId": "d4fc988c6510420a5290dfe8d1a991ca4878d696"
    },
    {
        "title": "Log Parsing: How Far Can ChatGPT Go?",
        "firstAuthor": "Van-Hoang Le",
        "url": "https://arxiv.org/pdf/2306.01590",
        "dateSubmitted": "2023-06-02",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Software logs play an essential role in ensuring the reliability and maintainability of large-scale software systems, as they are often the sole source of runtime information. Log parsing, which converts raw log messages into structured data, is an important initial step towards downstream log analytics. In recent studies, ChatGPT, the current cutting-edge large language model (LLM), has been widely applied to a wide range of software engineering tasks. However, its performance in automated log parsing remains unclear. In this paper, we evaluate ChatGPT's ability to undertake log parsing by addressing two research questions. (1) Can ChatGPT effectively parse logs? (2) How does ChatGPT perform with different prompting methods? Our results show that ChatGPT can achieve promising results for log parsing with appropriate prompts, especially with few-shot prompting. Based on our findings, we outline several challenges and opportunities for ChatGPT-based log parsing.",
        "paperId": "d589c49e1cd1dd3b994dcac01b4c6e7fb8eef161"
    },
    {
        "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing",
        "firstAuthor": "S. Sivarajkumar",
        "url": "https://arxiv.org/pdf/2309.08008",
        "dateSubmitted": "2023-09-14",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduced two new types of prompts, namely heuristic prompting and ensemble prompting. We evaluated the performance of these prompts on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted zero-shot prompting with few-shot prompting, and provide novel insights and guidelines for prompt engineering for LLMs in clinical NLP. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative AI, and we hope that it will inspire and inform future research in this area.",
        "paperId": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc"
    },
    {
        "title": "GPT-3 for Few-Shot Dialogue State Tracking",
        "firstAuthor": "Nicholas Pezzotti",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "GPT-3 (Brown et al., 2020) has attracted considerable attention due to its superior performance across a wide range of Natural Language Processing (NLP) tasks, especially with its powerful and versatile in-context few-shot learning ability. That is, it has been shown that by carefully crafting a prompt, consisting of a few labelled examples followed by an unlabelled example, GPT\u20193 is able to do few-shot sentiment classification, three-digit arithmetic and much more. We seek to evaluate its performance on a novel and notably more complicated task: few-shot Dialogue State Tracking (DST). We propose a few-shot prompting framework that selects in-context examples based on similarity which outperforms the original random in-context selection framework. We also review and formalise the two types of completion strategies employed by previous literature, which we name constrained and unconstrained, and propose a third \"semi-constrained\" completion strategy, which is particularly well adapted for DST. Additionally, we propose a prompt ensembling technique that reliably outperforms individual models. Furthermore, we are the first, to the best of our knowledge, to fine-tune GPT-3 for the task of few-shot DST, showing that it reliably outperforms its GPT-2 counterpart. Furthermore, we seek to synthesise and formalise the largely heterogeneous body of previous work on prompt programming and in-context learning for GPT-3. In an attempt to contribute to the understanding of the strengths, weaknesses and inner-working of GPT-3, we perform numerous ablative studies that validate and confute previous in-context learning empirical findings: mainly, we find that natural language instructions in the prompt have little impact on performance, larger language models do not always induce higher downstream performance and that GPT-3 is highly sensitive to the order and number of the in-context examples.",
        "paperId": "d66e80224cda0c1d5a4c1be3798df6a6bfe3713c"
    },
    {
        "title": "Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning",
        "firstAuthor": "Zhanming Jie",
        "url": "http://arxiv.org/pdf/2305.18170",
        "dateSubmitted": "2023-05-29",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Chain-of-thought (CoT) prompting with large language models has proven effective in numerous natural language processing tasks, but designing prompts that generalize well to diverse problem types can be challenging, especially in the context of math word problem (MWP) solving. Additionally, it is common to have a large amount of training data that have a better diversity coverage but CoT annotations are not available, which limits the use of supervised learning techniques. To address these issues, we investigate two approaches to leverage the training data in a few-shot prompting scenario: dynamic program prompting and program distillation. Our approach is largely inspired by Gao et al., (2022), where they proposed to replace the CoT with the programs as the intermediate reasoning step. Such a prompting strategy allows us to accurately verify the answer correctness through program execution in MWP solving. Our dynamic program prompting involves annotating the training data by sampling correct programs from a large language model, while program distillation involves adapting a smaller model to the program-annotated training data. Our experiments on three standard MWP datasets demonstrate the effectiveness of these approaches, yielding significant improvements over previous baselines for prompting and fine-tuning. Our results suggest that leveraging a large amount of training data can improve the generalization ability of prompts and boost the performance of fine-tuned small models in MWP solving.",
        "paperId": "d75d11d2c89c01cd284383546ae057cb827dc272"
    },
    {
        "title": "ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment",
        "firstAuthor": "Tarek Naous",
        "url": null,
        "dateSubmitted": "2023-05-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "We present a systematic study and comprehensive evaluation of large language models for automatic multilingual readability assessment. In particular, we construct ReadMe++, a multilingual multi-domain dataset with human annotations of 9757 sentences in Arabic, English, French, Hindi, and Russian collected from 112 different data sources. ReadMe++ offers more domain and language diversity than existing readability datasets, making it ideal for benchmarking multilingual and non-English language models (including mBERT, XLM-R, mT5, Llama-2, GPT-4, etc.) in the supervised, unsupervised, and few-shot prompting settings. Our experiments reveal that models fine-tuned on ReadMe++ outperform those trained on single-domain datasets, showcasing superior performance on multi-domain readability assessment and cross-lingual transfer capabilities. We also compare to traditional readability metrics (such as Flesch-Kincaid Grade Level and Open Source Metric for Measuring Arabic Narratives), as well as the state-of-the-art unsupervised metric RSRS (Martinc et al., 2021). We will make our data and code publicly available at: https://github.com/tareknaous/readme.",
        "paperId": "d7cc84e25f1fd8cfb0582be3856002ffd8ccfa54"
    },
    {
        "title": "Boosted Prompt Ensembles for Large Language Models",
        "firstAuthor": "Silviu Pitis",
        "url": "http://arxiv.org/pdf/2304.05970",
        "dateSubmitted": "2023-04-12",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Methods such as chain-of-thought prompting and self-consistency have pushed the frontier of language model reasoning performance with no additional training. To further improve performance, we propose a prompt ensembling method for large language models, which uses a small dataset to construct a set of few shot prompts that together comprise a ``boosted prompt ensemble''. The few shot examples for each prompt are chosen in a stepwise fashion to be ``hard'' examples on which the previous step's ensemble is uncertain. We show that this outperforms single-prompt output-space ensembles and bagged prompt-space ensembles on the GSM8k and AQuA datasets, among others. We propose both train-time and test-time versions of boosted prompting that use different levels of available annotation and conduct a detailed empirical study of our algorithm.",
        "paperId": "dca6c3927ade6481a1ae080f5c24decbfeced1be"
    },
    {
        "title": "Bootstrapping Multilingual Semantic Parsers using Large Language Models",
        "firstAuthor": "Abhijeet Awasthi",
        "url": "http://arxiv.org/pdf/2210.07313",
        "dateSubmitted": "2022-10-13",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Despite cross-lingual generalization demonstrated by pre-trained multilingual models, the translate-train paradigm of transferring English datasets across multiple languages remains to be a key mechanism for training task-specific multilingual models. However, for many low-resource languages, the availability of a reliable translation service entails significant amounts of costly human-annotated translation pairs. Further, translation services may continue to be brittle due to domain mismatch between task-specific input text and general-purpose text used for training translation models. For multilingual semantic parsing, we demonstrate the effectiveness and flexibility offered by large language models (LLMs) for translating English datasets into several languages via few-shot prompting. Through extensive comparisons on two public datasets, MTOP and MASSIVE, spanning 50 languages and several domains, we show that our method of translating data using LLMs outperforms a strong translate-train baseline on 41 out of 50 languages. We study the key design choices that enable more effective multilingual data translation via prompted LLMs.",
        "paperId": "dda0f7f086fc875d583604f8b0cf4a8678bc4de4"
    },
    {
        "title": "Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting",
        "firstAuthor": "Benjamin Yan",
        "url": null,
        "dateSubmitted": "2023-10-26",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Automatically generated reports from medical images promise to improve the workflow of radiologists. Existing methods consider an image-to-report modeling task by directly generating a fully-fledged report from an image. However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports. To address this, we propose a two-step approach for radiology report generation. First, we extract the content from an image; then, we verbalize the extracted content into a report that matches the style of a specific radiologist. For this, we leverage RadGraph -- a graph representation of reports -- together with large language models (LLMs). In our quantitative evaluations, we find that our approach leads to beneficial performance. Our human evaluation with clinical raters highlights that the AI-generated reports are indistinguishably tailored to the style of individual radiologist despite leveraging only a few examples as context.",
        "paperId": "e31a820dd9324791ab294f89528455ac380d0d87"
    },
    {
        "title": "MDC at BioLaySumm Task 1: Evaluating GPT Models for Biomedical Lay Summarization",
        "firstAuthor": "Oisn Turbitt",
        "url": "https://aclanthology.org/2023.bionlp-1.65.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This paper presents our approach to the BioLaySumm Task 1 shared task, held at the BioNLP 2023 Workshop. The effective communication of scientific knowledge to the general public is often limited by the technical language used in research, making it difficult for non-experts to comprehend. To address this issue, lay summaries can be used to explain research findings to non-experts in an accessible form. We conduct an evaluation of autoregressive language models, both general and specialized for the biomedical domain, to generate lay summaries from biomedical research article abstracts. Our findings demonstrate that a GPT-3.5 model combined with a straightforward few-shot prompt produces lay summaries that achieve significantly relevance and factuality compared to those generated by a fine-tuned BioGPT model. However, the summaries generated by the BioGPT model exhibit better readability. Notably, our submission for the shared task achieved 1st place in the competition.",
        "paperId": "e4e65df11e4d063199c6035004be2b28c3e2f82f"
    },
    {
        "title": "Prompt2Model: Generating Deployable Models from Natural Language Instructions",
        "firstAuthor": "Vijay Viswanathan",
        "url": "https://arxiv.org/pdf/2308.12261",
        "dateSubmitted": "2023-08-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.",
        "paperId": "e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43"
    },
    {
        "title": "Leveraging Large Language Models for Mental Health Prediction via Online Text Data",
        "firstAuthor": "Xuhai Xu",
        "url": "https://arxiv.org/pdf/2307.14385",
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "The recent technology boost of large language models (LLMs) has empowered a variety of applications. However, there is very little research on understanding and improving LLMs\u2019 capability for the mental health domain. In this work, we present the first comprehensive evaluation of multiple LLMs, including Alpaca, Alpaca-LoRA, and GPT-3.5, on various mental health prediction tasks via online text data. We conduct a wide range of experiments, covering zero-shot prompting, few-shot prompting, and instruction finetuning. The results indicate the promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned model, Mental-Alpaca, outperforms GPT-3.5 (25 times bigger) by 16.7% on balanced accuracy and performs on par with the state-of-the-art task-specific model. We summarize our findings into a set of action guidelines for future researchers, engineers, and practitioners on how to empower LLMs with better mental health domain knowledge and become an expert in mental health prediction tasks.",
        "paperId": "ea284d2045672daf44deffa3f0b7ce154630424c"
    },
    {
        "title": "SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization",
        "firstAuthor": "Yash Mathur",
        "url": "http://arxiv.org/pdf/2306.17384",
        "dateSubmitted": "2023-06-30",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Medical dialogue summarization is challenging due to the unstructured nature of medical conversations, the use of medical terminologyin gold summaries, and the need to identify key information across multiple symptom sets. We present a novel system for the Dialogue2Note Medical Summarization tasks in the MEDIQA 2023 Shared Task. Our approach for sectionwise summarization (Task A) is a two-stage process of selecting semantically similar dialogues and using the top-k similar dialogues as in-context examples for GPT-4. For full-note summarization (Task B), we use a similar solution with k=1. We achieved 3rd place in Task A (2nd among all teams), 4th place in Task B Division Wise Summarization (2nd among all teams), 15th place in Task A Section Header Classification (9th among all teams), and 8th place among all teams in Task B. Our results highlight the effectiveness of few-shot prompting for this task, though we also identify several weaknesses of prompting-based approaches. We compare GPT-4 performance with several finetuned baselines. We find that GPT-4 summaries are more abstractive and shorter. We make our code publicly available.",
        "paperId": "ebb3d299213bae89b5d302cc3dfc36573ec83956"
    },
    {
        "title": "Pachinko: Patching Interpretable QA Models through Natural Language Feedback",
        "firstAuthor": "Chaitanya Malaviya",
        "url": null,
        "dateSubmitted": "2023-11-16",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Eliciting feedback from end users of NLP models can be beneficial for improving models. However, how should we present model responses to users so they are most amenable to be corrected from user feedback? Further, what properties do users value to understand and trust responses? We answer these questions by analyzing the effect of rationales generated by QA models to support their answers. We specifically consider decomposed question-answering models that first extract an intermediate rationale based on a context and a question and then use solely this rationale to answer the question. A rationale outlines the approach followed by the model to answer the question. Our work considers various formats of these rationales that vary according to well-defined properties of interest. We sample these rationales from large language models using few-shot prompting for two reading comprehension datasets, and then perform two user studies. In the first one, we present users with incorrect answers and corresponding rationales of various formats and ask them to provide natural language feedback to revise the rationale. We then measure the effectiveness of this feedback in patching these rationales through in-context learning. The second study evaluates how well different rationale formats enable users to understand and trust model answers, when they are correct. We find that rationale formats significantly affect how easy it is (1) for users to give feedback for rationales, and (2) for models to subsequently execute this feedback. In addition to influencing critiquablity, certain formats significantly enhance user reported understanding and trust of model outputs.",
        "paperId": "ed6476f36f17cfdaa35efcbc7cb35e5c3bb7ef3f"
    },
    {
        "title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
        "firstAuthor": "Ruochen Zhang",
        "url": "http://arxiv.org/pdf/2305.14235",
        "dateSubmitted": "2023-05-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current\"multilingualism\"in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.",
        "paperId": "eda54452d8a8a412c2a985ef11572cb468906b1f"
    },
    {
        "title": "MultiQG-TI: Towards Question Generation from Multi-modal Sources",
        "firstAuthor": "Zichao Wang",
        "url": "https://arxiv.org/pdf/2307.04643",
        "dateSubmitted": "2023-07-07",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "We study the new problem of automatic question generation (QG) from multi-modal sources containing images and texts, significantly expanding the scope of most of the existing work that focuses exclusively on QG from only textual sources. We propose a simple solution for our new problem, called MultiQG-TI, which enables a text-only question generator to process visual input in addition to textual input. Specifically, we leverage an image-to-text model and an optical character recognition model to obtain the textual description of the image and extract any texts in the image, respectively, and then feed them together with the input texts to the question generator. We only fine-tune the question generator while keeping the other components fixed. On the challenging ScienceQA dataset, we demonstrate that MultiQG-TI significantly outperforms ChatGPT with few-shot prompting, despite having hundred-times less trainable parameters. Additional analyses empirically confirm the necessity of both visual and textual signals for QG and show the impact of various modeling choices. Code is available at https://anonymous.4open.science/r/multimodal-QG-47F2/",
        "paperId": "ef4b604fca0c62dcd0d5caf7ca24ad74e285632d"
    },
    {
        "title": "Product Information Extraction using ChatGPT",
        "firstAuthor": "Alexander Brinkmann",
        "url": "http://arxiv.org/pdf/2306.14921",
        "dateSubmitted": "2023-06-23",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Structured product data in the form of attribute/value pairs is the foundation of many e-commerce applications such as faceted product search, product comparison, and product recommendation. Product offers often only contain textual descriptions of the product attributes in the form of titles or free text. Hence, extracting attribute/value pairs from textual product descriptions is an essential enabler for e-commerce applications. In order to excel, state-of-the-art product information extraction methods require large quantities of task-specific training data. The methods also struggle with generalizing to out-of-distribution attributes and attribute values that were not a part of the training data. Due to being pre-trained on huge amounts of text as well as due to emergent effects resulting from the model size, Large Language Models like ChatGPT have the potential to address both of these shortcomings. This paper explores the potential of ChatGPT for extracting attribute/value pairs from product descriptions. We experiment with different zero-shot and few-shot prompt designs. Our results show that ChatGPT achieves a performance similar to a pre-trained language model but requires much smaller amounts of training data and computation for fine-tuning.",
        "paperId": "f00e7326baa9600e46b3a8e7077dc3a349f90a01"
    },
    {
        "title": "\"You Are An Expert Linguistic Annotator\": Limits of LLMs as Analyzers of Abstract Meaning Representation",
        "firstAuthor": "Allyson Ettinger",
        "url": null,
        "dateSubmitted": "2023-10-26",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) show amazing proficiency and fluency in the use of language. Does this mean that they have also acquired insightful linguistic knowledge about the language, to an extent that they can serve as an\"expert linguistic annotator\"? In this paper, we examine the successes and limitations of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning structure, focusing on the Abstract Meaning Representation (AMR; Banarescu et al. 2013) parsing formalism, which provides rich graphical representations of sentence meaning structure while abstracting away from surface forms. We compare models' analysis of this semantic structure across two settings: 1) direct production of AMR parses based on zero- and few-shot prompts, and 2) indirect partial reconstruction of AMR via metalinguistic natural language queries (e.g.,\"Identify the primary event of this sentence, and the predicate corresponding to that event.\"). Across these settings, we find that models can reliably reproduce the basic format of AMR, and can often capture core event, argument, and modifier structure -- however, model outputs are prone to frequent and major errors, and holistic analysis of parse acceptability shows that even with few-shot demonstrations, models have virtually 0% success in producing fully accurate parses. Eliciting natural language responses produces similar patterns of errors. Overall, our findings indicate that these models out-of-the-box can capture aspects of semantic structure, but there remain key limitations in their ability to support fully accurate semantic analyses or parses.",
        "paperId": "f120490d06d1d30c389ed60b634b8bf69cd64efd"
    },
    {
        "title": "Large Language Models for User Interest Journeys",
        "firstAuthor": "Konstantina Christakopoulou",
        "url": "http://arxiv.org/pdf/2305.15498",
        "dateSubmitted": "2023-05-24",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Large language models (LLMs) have shown impressive capabilities in natural language understanding and generation. Their potential for deeper user understanding and improved personalized user experience on recommendation platforms is, however, largely untapped. This paper aims to address this gap. Recommender systems today capture users' interests through encoding their historical activities on the platforms. The generated user representations are hard to examine or interpret. On the other hand, if we were to ask people about interests they pursue in their life, they might talk about their hobbies, like I just started learning the ukulele, or their relaxation routines, e.g., I like to watch Saturday Night Live, or I want to plant a vertical garden. We argue, and demonstrate through extensive experiments, that LLMs as foundation models can reason through user activities, and describe their interests in nuanced and interesting ways, similar to how a human would. We define interest journeys as the persistent and overarching user interests, in other words, the non-transient ones. These are the interests that we believe will benefit most from the nuanced and personalized descriptions. We introduce a framework in which we first perform personalized extraction of interest journeys, and then summarize the extracted journeys via LLMs, using techniques like few-shot prompting, prompt-tuning and fine-tuning. Together, our results in prompting LLMs to name extracted user journeys in a large-scale industrial platform demonstrate great potential of these models in providing deeper, more interpretable, and controllable user understanding. We believe LLM powered user understanding can be a stepping stone to entirely new user experiences on recommendation platforms that are journey-aware, assistive, and enabling frictionless conversation down the line.",
        "paperId": "f834aed32f5531bfa426faab71878c549572500e"
    },
    {
        "title": "Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning",
        "firstAuthor": "Prasetya Ajie Utama",
        "url": "https://aclanthology.org/2021.emnlp-main.713.pdf",
        "dateSubmitted": "2021-09-09",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Recent prompt-based approaches allow pretrained language models to achieve strong performances on few-shot finetuning by reformulating downstream tasks as a language modeling problem. In this work, we demonstrate that, despite its advantages on low data regimes, finetuned prompt-based models for sentence pair classification tasks still suffer from a common pitfall of adopting inference heuristics based on lexical overlap, e.g., models incorrectly assuming a sentence pair is of the same meaning because they consist of the same set of words. Interestingly, we find that this particular inference heuristic is significantly less present in the zero-shot evaluation of the prompt-based model, indicating how finetuning can be destructive to useful knowledge learned during the pretraining. We then show that adding a regularization that preserves pretraining weights is effective in mitigating this destructive tendency of few-shot finetuning. Our evaluation on three datasets demonstrates promising improvements on the three corresponding challenge datasets used to diagnose the inference heuristics.",
        "paperId": "fccce60283729934467877f0730317c3e9fcc61e"
    },
    {
        "title": "Ecologically Valid Explanations for Label Variation in NLI",
        "firstAuthor": "Nan-Jiang Jiang",
        "url": null,
        "dateSubmitted": "2023-10-20",
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "Human label variation, or annotation disagreement, exists in many natural language processing (NLP) tasks, including natural language inference (NLI). To gain direct evidence of how NLI label variation arises, we build LiveNLI, an English dataset of 1,415 ecologically valid explanations (annotators explain the NLI labels they chose) for 122 MNLI items (at least 10 explanations per item). The LiveNLI explanations confirm that people can systematically vary on their interpretation and highlight within-label variation: annotators sometimes choose the same label for different reasons. This suggests that explanations are crucial for navigating label interpretations in general. We few-shot prompt large language models to generate explanations but the results are inconsistent: they sometimes produces valid and informative explanations, but it also generates implausible ones that do not support the label, highlighting directions for improvement.",
        "paperId": "fcf4dcd27ae9a26c0719d493f7b90d1ec3d620ea"
    },
    {
        "title": "Generating Dialog Responses with Specified Grammatical Items for Second Language Learning",
        "firstAuthor": "Yuki Okano",
        "url": "https://aclanthology.org/2023.bea-1.16.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "few-shot prompt"
        ],
        "abstract": "This paper proposes a new second language learning task of generating a response including specified grammatical items. We consider two approaches: 1) fine-tuning a pre-trained language model (DialoGPT) by reinforcement learning and 2) providing a few-shot prompt to a large language model (GPT-3). For reinforcement learning, we examine combinations of three reward functions that consider grammatical items, diversity, and fluency. Our experiments confirm that both approaches can generate responses including the specified grammatical items and that it is crucial to consider fluency rather than diversity as the reward function.",
        "paperId": "fffd5378bdfb5a2d4bfc3ff9d2ce30f77d716e9f"
    }
]