[
    {
        "title": "Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks",
        "firstAuthor": "Melanie Mitchell",
        "url": null,
        "dateSubmitted": "2023-11-14",
        "keyWords": [
            "one-shot prompting"
        ],
        "abstract": "We explore the abstract reasoning abilities of text-only and multimodal versions of GPT-4, using the ConceptARC benchmark [10], which is designed to evaluate robust understanding and reasoning with core-knowledge concepts. We extend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed, one-shot prompting (rather than simple, zero-shot prompts) with text versions of ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4, on zero- and one-shot prompts using image versions of the simplest tasks. Our experimental results support the conclusion that neither version of GPT-4 has developed robust abstraction abilities at humanlike levels.",
        "paperId": "024646623aa733df2bb752aa3bb3e76d691cab11"
    },
    {
        "title": "Prompt-based Extraction of Social Determinants of Health Using Few-shot Learning",
        "firstAuthor": "Giridhar Kaushik Ramachandran",
        "url": "http://arxiv.org/pdf/2306.07170",
        "dateSubmitted": "2023-06-12",
        "keyWords": [
            "one-shot prompting"
        ],
        "abstract": "Social determinants of health (SDOH) documented in the electronic health record through unstructured text are increasingly being studied to understand how SDOH impacts patient health outcomes. In this work, we utilize the Social History Annotation Corpus (SHAC), a multi-institutional corpus of de-identified social history sections annotated for SDOH, including substance use, employment, and living status information. We explore the automatic extraction of SDOH information with SHAC in both standoff and inline annotation formats using GPT-4 in a one-shot prompting setting. We compare GPT-4 extraction performance with a high-performing supervised approach and perform thorough error analyses. Our prompt-based GPT-4 method achieved an overall 0.652 F1 on the SHAC test set, similar to the 7th best-performing system among all teams in the n2c2 challenge with SHAC.",
        "paperId": "386bd4d25043516f076ea7b2296a1ebec84f43ce"
    },
    {
        "title": "DePlot: One-shot visual language reasoning by plot-to-table translation",
        "firstAuthor": "Fangyu Liu",
        "url": "http://arxiv.org/pdf/2212.10505",
        "dateSubmitted": "2022-12-20",
        "keyWords": [
            "one-shot prompting"
        ],
        "abstract": "Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than>28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA on human-written queries from the task of chart QA.",
        "paperId": "4d3a49d1439a0b8fbb0e9f588970ad0f1d70dec8"
    },
    {
        "title": "GEE! Grammar Error Explanation with Large Language Models",
        "firstAuthor": "Yixiao Song",
        "url": null,
        "dateSubmitted": "2023-11-16",
        "keyWords": [
            "one-shot prompting"
        ],
        "abstract": "Grammatical error correction tools are effective at correcting grammatical errors in users' input sentences but do not provide users with \\textit{natural language} explanations about their errors. Such explanations are essential for helping users learn the language by gaining a deeper understanding of its grammatical rules (DeKeyser, 2003; Ellis et al., 2006). To address this gap, we propose the task of grammar error explanation, where a system needs to provide one-sentence explanations for each grammatical error in a pair of erroneous and corrected sentences. We analyze the capability of GPT-4 in grammar error explanation, and find that it only produces explanations for 60.2% of the errors using one-shot prompting. To improve upon this performance, we develop a two-step pipeline that leverages fine-tuned and prompted large language models to perform structured atomic token edit extraction, followed by prompting GPT-4 to generate explanations. We evaluate our pipeline on German and Chinese grammar error correction data sampled from language learners with a wide range of proficiency levels. Human evaluation reveals that our pipeline produces 93.9% and 98.0% correct explanations for German and Chinese data, respectively. To encourage further research in this area, we will open-source our data and code.",
        "paperId": "63cfdf83529f25f4058bb1bf0c96311ffea5600e"
    },
    {
        "title": "DS4DH at MEDIQA-Chat 2023: Leveraging SVM and GPT-3 Prompt Engineering for Medical Dialogue Classification and Summarization",
        "firstAuthor": "Boya Zhang",
        "url": "https://access.archive-ouverte.unige.ch/access/metadata/290c4289-0017-45ec-baa9-ff2fdd7948f9/download",
        "dateSubmitted": "2023-06-12",
        "keyWords": [
            "one-shot prompting"
        ],
        "abstract": "This paper presents the results of the Data Science for Digital Health (DS4DH) group in the MEDIQA-Chat Tasks at ACL-ClinicalNLP 2023. Our study combines the power of a classical machine learning method, Support Vector Machine, for classifying medical dialogues, along with the implementation of one-shot prompts using GPT-3.5. We employ dialogues and summaries from the same category as prompts to generate summaries for novel dialogues. Our findings exceed the average benchmark score, offering a robust reference for assessing performance in this field.",
        "paperId": "cd902673a9396b63fdaf2cf7e0e1ce25cc3c545c"
    },
    {
        "title": "Short Answer Grading Using One-shot Prompting and Text Similarity Scoring Model",
        "firstAuthor": "Su-Youn Yoon",
        "url": "http://arxiv.org/pdf/2305.18638",
        "dateSubmitted": "2023-05-29",
        "keyWords": [
            "one-shot prompting"
        ],
        "abstract": "In this study, we developed an automated short answer grading (ASAG) model that provided both analytic scores and final holistic scores. Short answer items typically consist of multiple sub-questions, and providing an analytic score and the text span relevant to each sub-question can increase the interpretability of the automated scores. Furthermore, they can be used to generate actionable feedback for students. Despite these advantages, most studies have focused on predicting only holistic scores due to the difficulty in constructing dataset with manual annotations. To address this difficulty, we used large language model (LLM)-based one-shot prompting and a text similarity scoring model with domain adaptation using small manually annotated dataset. The accuracy and quadratic weighted kappa of our model were 0.67 and 0.71 on a subset of the publicly available ASAG dataset. The model achieved a substantial improvement over the majority baseline.",
        "paperId": "d1aa858644154af50e36860e6761ae52ae655bd3"
    }
]