[
    {
        "title": "Active and Student-Centered Teaching and Learning Methods in International Studies at the University of Miami",
        "firstAuthor": "J. Twichell",
        "url": null,
        "dateSubmitted": "2020-01-31",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This paper explores the value of student-centered \u201cin context\u201d learning in IS programs. It evaluates application of and key benefits of different hands-on activities such as debates, geopolitical simulations, negotiations and role play. Building on the existing literature, it describes the active learning techniques we have adopted for introductory undergraduate IS courses at the University of Miami. The debates and simulations, which find students working collaboratively in teams, are structured to meet clear learning outcomes. These techniques concurrently provide a student-centered classroom setting that we find conducive to more effective learning of International Relations theory and to greater understanding of complex global problems. The paper further explores how to best relate these student-centered teaching strategies with \u201coutside the classroom\u201d capstone and internship experiences. We envision our paper promoting dialogue consistent with the conference workshop themes and objectives.",
        "paperId": "001cb5ec7271303be069b3f7d2e60c3242faa87c"
    },
    {
        "title": "Black-Box Tuning for Language-Model-as-a-Service",
        "firstAuthor": "Tianxiang Sun",
        "url": null,
        "dateSubmitted": "2022-01-10",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Extremely large pre-trained language models (PTMs) such as GPT-3 are usually released as a service. It allows users to design task-specific prompts to query the PTMs through some black-box APIs. In such a scenario, which we call Language-Model-as-a-Service (LMaaS), the gradients of PTMs are usually unavailable. Can we optimize the task prompts by only accessing the model inference APIs? This paper proposes the black-box tuning framework to optimize the continuous prompt prepended to the input text via derivative-free optimization. Instead of optimizing in the original high-dimensional prompt space, which is intractable for traditional derivative-free optimization, we perform optimization in a randomly generated subspace due to the low intrinsic dimensionality of large PTMs. The experimental results show that the black-box tuning with RoBERTa on a few labeled samples not only significantly outperforms manual prompt and GPT-3's in-context learning, but also surpasses the gradient-based counterparts, i.e., prompt tuning and full model tuning.",
        "paperId": "002c58077a1f1b296468b117230a1199e91f35c2"
    },
    {
        "title": "Exploring Example Selection for Few-shot Text-to-SQL Semantic Parsing",
        "firstAuthor": "Emmanouil Antonios",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "We study example selection methods for few- 001 shot text-to-SQL tasks with unseen databases. 002 Annotating natural language questions with cor- 003 responding SQL queries is expensive, but we 004 can use abundant unlabeled questions to effi- 005 ciently select examples to annotate and then 006 use them to adapt models. Many previous 007 works only randomly sample a few instances 008 for few-shot learning, but this random selec- 009 tion is not sufficient to select representative and 010 informative examples that provide specific do- 011 main knowledge. We thus explore methods to 012 efficiently choose annotation examples. We 013 identify two important factors: the diversity of 014 selected instances and the dissimilarity to the 015 source training data if any. A diverse training 016 set contains more domain knowledge, while 017 dissimilar examples are selected to fill in the 018 domain gap between the source and target. We 019 show that our best example selection approach 020 substantially improves few-shot text-to-SQL 021 performance in both finetuning using T5 and 022 in-context learning with Codex: average execu- 023 tion accuracy gains of 8.7% and 4.3% over ran- 024 dom selection. Our extensive analysis demon- 025 strates the importance of the similarity metric 026 and the embedding method for example repre- 027 sentations. We also find that effective example 028 selection reduces syntax errors on the target 029 domains. Our results encourage future work to 030 further explore example selection for efficient 031 adaptation of text-to-SQL models. 1 032",
        "paperId": "002e6af90e7ae36fad7723356ad2f5dd880c2e90"
    },
    {
        "title": "ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning",
        "firstAuthor": "Jingyuan Selena She",
        "url": "http://arxiv.org/pdf/2305.19426",
        "dateSubmitted": "2023-05-30",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "A number of recent benchmarks seek to assess how well models handle natural language negation. However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had truly learned how negation morphemes semantically scope. To fill these analytical gaps, we present the Scoped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six examples with up to two negations where either zero, one, or both negative morphemes affect the NLI label. We use ScoNe-NLI to assess fine-tuning and in-context learning strategies. We find that RoBERTa and DeBERTa models solve ScoNe-NLI after many shot fine-tuning. For in-context learning, we test the latest InstructGPT models and find that most prompt strategies are not successful, including those using step-by-step reasoning. To better understand this result, we extend ScoNe with ScoNe-NLG, a sentence completion test set that embeds negation reasoning in short narratives. Here, InstructGPT is successful, which reveals the model can correctly reason about negation, but struggles to do so on NLI examples outside of its core pretraining regime.",
        "paperId": "00337d7d7fc679cb4959aead3204b0fb9c6a5ef2"
    },
    {
        "title": "Education to Prepare Health Professionals for Rural Practice",
        "firstAuthor": "Marg Adams",
        "url": "https://journal.spera.asn.au/index.php/AIJRE/article/download/349/694",
        "dateSubmitted": "2023-03-23",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Rural and remote communities are challenged by an incongruous combination of poorer health and deficits of health workers. Health professionals working in rural and remote practice contexts are largely educated with standardised curriculum content designed for urban-dominant systems, even though non-urban populations account for approximately half of the global population. Education is one strategy considered pivotal to recruitment and retention of health care workers in rural areas, yet has received less research attention than strategies such as rural background, incentive schemes and clinical placements.\nPeer reviewed literature published in English between 2011-2021 were obtained, guided by PRISMA-P guidelines (Shamseer, 2015). The aim of this review was to examine how health professionals are currently being prepared for rural practice.\nFollowing key health database searches 189 relevant articles were retrieved, of which 26 articles met the inclusion criteria for final analysis. This review identified a heavy reliance upon standardised curricula delivered via clinical and interprofessional placements with little attention to specific rural curriculum content or pedagogic strategies tailored to rural and remote practice. Three themes were developed from the literature: Context (\u2018learning to think differently\u2019, \u2018relationships\u2019, and \u2018health leadership\u2019), Curriculum and Pedagogy (\u2018rural clinical placements\u2019 and \u2018interprofessional education\u2019).\nThere is a paucity of educational design and evaluation research to assess the value of education strategies that prepare health professionals to work in rural places. Identifying key rural pedagogic strategies can support curriculum content, experiences and assessment that provide health professionals with the competence, confidence and skills to sustain careers in rural and remote practice.",
        "paperId": "00350d89e91f2e39e06c2be2567cfe8e9f8952eb"
    },
    {
        "title": "Cerebellar Contribution to Context Processing in Extinction Learning and Recall",
        "firstAuthor": "Dae-In Chang",
        "url": null,
        "dateSubmitted": "2015-04-12",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "006d81854738423f0f429c977c2bd8478d229a1d"
    },
    {
        "title": "PRODIGY: Enabling In-context Learning Over Graphs",
        "firstAuthor": "Qian Huang",
        "url": "http://arxiv.org/pdf/2305.12600",
        "dateSubmitted": "2023-05-21",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In-context learning is the ability of a pretrained model to adapt to novel and diverse downstream tasks by conditioning on prompt examples, without optimizing any parameters. While large language models have demonstrated this ability, how in-context learning could be performed over graphs is unexplored. In this paper, we develop \\textbf{Pr}etraining \\textbf{O}ver \\textbf{D}iverse \\textbf{I}n-Context \\textbf{G}raph S\\textbf{y}stems (PRODIGY), the first pretraining framework that enables in-context learning over graphs. The key idea of our framework is to formulate in-context learning over graphs with a novel \\emph{prompt graph} representation, which connects prompt examples and queries. We then propose a graph neural network architecture over the prompt graph and a corresponding family of in-context pretraining objectives. With PRODIGY, the pretrained model can directly perform novel downstream classification tasks on unseen graphs via in-context learning. We provide empirical evidence of the effectiveness of our framework by showcasing its strong in-context learning performance on tasks involving citation networks and knowledge graphs. Our approach outperforms the in-context learning accuracy of contrastive pretraining baselines with hard-coded adaptation by 18\\% on average across all setups. Moreover, it also outperforms standard finetuning with limited data by 33\\% on average with in-context learning.",
        "paperId": "0088c9f4d50706c7ab71efa13bcb4b42cf2058e2"
    },
    {
        "title": "Context learning in Okapi",
        "firstAuthor": "A. Goker",
        "url": "http://repository.bilkent.edu.tr/bitstream/11693/25682/1/Context%20learning%20in%20Okapi.pdf",
        "dateSubmitted": "1997-03-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "A user who makes repeated use of a retrieval system may be assumed to have a context which is common to successive uses (even if the immediate need differs). An IR system which could make use of this context may be better able to match the specific need. A machine\u2010learning approach to inferring the user\u2019s context is described, and the results of an evaluation experiment are given. There appears to be scope for IR systems to operate in this way.",
        "paperId": "00892b60f3adfd37c9a7f765e17328425872a14f"
    },
    {
        "title": "Improved compressive tracker via local context learning",
        "firstAuthor": "Yong Zhang",
        "url": null,
        "dateSubmitted": "2014-07-28",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "00923e05147636b98bf716e2dc47437b4dc6fd7c"
    },
    {
        "title": "OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples",
        "firstAuthor": "Ryuto Koike",
        "url": "https://arxiv.org/pdf/2307.11729",
        "dateSubmitted": "2023-07-21",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors lack robustness against attacks: they degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, a malicious user might attempt to deliberately evade the detectors based on detection results, but this has not been assumed in previous studies. In this paper, we propose OUTFOX, a framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output. In this framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are harder to detect, while the detector uses the adversarially generated essays as examples for in-context learning to learn to detect essays from a strong attacker. Experiments in the domain of student essays show that the proposed detector improves the detection performance on the attacker-generated texts by up to +41.3 points in F1-score. Furthermore, the proposed detector shows a state-of-the-art detection performance: up to 96.9 points in F1-score, beating existing detectors on non-attacked texts. Finally, the proposed attacker drastically degrades the performance of detectors by up to -57.0 points F1-score, massively outperforming the baseline paraphrasing method for evading detection.",
        "paperId": "0095acc4f2c3255cf38fdf844003c97858adb418"
    },
    {
        "title": "NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers",
        "firstAuthor": "Kai Shen",
        "url": "http://arxiv.org/pdf/2304.09116",
        "dateSubmitted": "2023-04-18",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2.",
        "paperId": "00c367427d9135209d84008e6cb5e90f0adba881"
    },
    {
        "title": "Mathematics teacher preparation examined in an international context: learning from the Teacher Education and Development Study in Mathematics (TEDS-M) and beyond",
        "firstAuthor": "Yeping Li",
        "url": null,
        "dateSubmitted": "2012-05-19",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "00eeb0cea0da28e4d7d78c04f5927cfa76415636"
    },
    {
        "title": "A second chance for a first impression: Sensitivity to cumulative input statistics for lexically guided perceptual learning",
        "firstAuthor": "Christina Y. Tzeng",
        "url": "https://link.springer.com/content/pdf/10.3758/s13423-020-01840-6.pdf",
        "dateSubmitted": "2021-01-14",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "00fb10f6fef53a805b6cb2d8bcbda939612e9bb4"
    },
    {
        "title": "The UCSB Confluent Education Program:Its Essence and Demise",
        "firstAuthor": "S. Shapiro",
        "url": null,
        "dateSubmitted": "1997-07-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This article presents historical, political, and empirical perspectives on Confluent Education, a 25-year program at the University of California, Santa Barbara. It interprets the program's essence and regards its demise as a result of a clash of academic and professional subcultures. Confluent Education, a humanistic orientation, is seen as a relatively high-context learning community but one surrounded by a hostile and/or indifferent low-context larger academic and community environment.",
        "paperId": "012c4fe1724022ac3cd94262c599430c67b33fb1"
    },
    {
        "title": "Few-shot In-context Learning on Knowledge Base Question Answering",
        "firstAuthor": "Tianle Li",
        "url": "http://arxiv.org/pdf/2305.01750",
        "dateSubmitted": "2023-05-02",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even outperform the state-of-the-art trained models. On GrailQA and WebQSP, our model is also on par with other fully-trained models. We believe KB-BINDER can serve as an important baseline for future research. We plan to release all the code and data. Our code is available at https://github.com/ltl3A87/KB-BINDER.",
        "paperId": "0139e689add40a61c9454674edac4e93702aa5fc"
    },
    {
        "title": "TART: A plug-and-play Transformer module for task-agnostic reasoning",
        "firstAuthor": "K. Bhatia",
        "url": "http://arxiv.org/pdf/2306.07536",
        "dateSubmitted": "2023-06-13",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large language models (LLMs) exhibit in-context learning abilities which enable the same model to perform several tasks without any task-specific training. In contrast, traditional adaptation approaches, such as fine-tuning, modify the underlying models for each specific task. In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples. While most existing approaches (e.g., prompt engineering) focus on the LLM's learned representations to patch this performance gap, our analysis actually reveal that LLM representations contain sufficient information to make good predictions. As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks. This raises an intriguing question: Are LLMs actually capable of learning how to reason in a task-agnostic manner? We answer this in the affirmative and propose TART which generically improves an LLM's reasoning abilities using a synthetically trained Transformer-based reasoning module. TART trains this reasoning module in a task-agnostic manner using only synthetic logistic regression tasks and composes it with an arbitrary real-world pre-trained model without any additional training. With a single inference module, TART improves performance across different model families (GPT-Neo, Pythia, BLOOM), model sizes (100M - 6B), tasks (14 NLP binary classification tasks), and even across different modalities (audio and vision). Additionally, on the RAFT Benchmark, TART improves GPT-Neo (125M)'s performance such that it outperforms BLOOM (176B), and is within 4% of GPT-3 (175B). Our code and models are available at https://github.com/HazyResearch/TART .",
        "paperId": "014c00319cb23c6322ea5218049661a4ce222946"
    },
    {
        "title": "The meaning of BMus studies for early career graduates : a phenomenological study",
        "firstAuthor": "Ellr\u00e9 Phyllis Jacobs",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This study is a phenomenological investigation with the aim of understanding the meaning early career graduates ascribe to their BMus studies in the work environment. Open-ended interviews were conducted with fourteen participants and thematic data analysis was used to make sense of the data. The study provides knowledge of the experiences of BMus graduates to inform music academia on the practical needs existing within musicians\u2019 work environment. When assessing the relevance of knowledge, skills and competencies during BMus studies from a work perspective, the participants mentioned learning efficiencies experienced such as gaining a \u2018musical foundation\u2019, which included \u2018learning a musical language\u2019, gaining \u2018an advantage of understanding music\u2019, \u2018musical knowledge to draw from\u2019, and developing \u2018a musical perspective\u2019. The reasons provided by the participants for the perceived value of subjects, classes and projects referred to were: knowledge, skills and competencies that were \u2018reusable\u2019, \u2018enriched personhood\u2019, and \u2018enhanced musical ability\u2019, as well as learning that allowed participants to \u2018integrate, transfer or relate information to different contexts\u2019. Learning deficiencies voiced by participants from a work perspective emphasised the need for \u2018more practical application of knowledge\u2019, \u2018contextual understanding of knowledge\u2019, \u2018in-service training\u2019, \u2018a more supportive musical environment\u2019, \u2018more music-making opportunities\u2019, as well as \u2018career guidance\u2019, \u2018music technology skills\u2019, and \u2018small business management skills\u2019. The essence of the study revealed through the emergent themes of the participants\u2019 experiences in the workplace was \u2018Lifelong learning\u2019 and \u2018Variety within career portfolios\u2019, and this in turn enabled the participants to experience \u2018Financial sustainability\u2019.",
        "paperId": "0192b186a249dd61e3c1ec61062a9a95a69112dc"
    },
    {
        "title": "Explore Spurious Correlations at the Concept Level in Language Models for Text Classification",
        "firstAuthor": "Yuhang Zhou",
        "url": null,
        "dateSubmitted": "2023-11-15",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Language models (LMs) have gained great achievement in various NLP tasks for both fine-tuning and in-context learning (ICL) methods. Despite its outstanding performance, evidence shows that spurious correlations caused by imbalanced label distributions in training data (or exemplars in ICL) lead to robustness issues. However, previous studies mostly focus on word- and phrase-level features and fail to tackle it from the concept level, partly due to the lack of concept labels and subtle and diverse expressions of concepts in text. In this paper, we first use the LLM to label the concept for each text and then measure the concept bias of models for fine-tuning or ICL on the test data. Second, we propose a data rebalancing method to mitigate the spurious correlations by adding the LLM-generated counterfactual data to make a balanced label distribution for each concept. We verify the effectiveness of our mitigation method and show its superiority over the token removal method. Overall, our results show that there exist label distribution biases in concepts across multiple text classification datasets, and LMs will utilize these shortcuts to make predictions in both fine-tuning and ICL methods.",
        "paperId": "01efb3fd2d3ae4b5f4389c916c94f2c6d9c11b81"
    },
    {
        "title": "The State of Intent Detection in the Era of Large Autoregressive Language Models",
        "firstAuthor": "",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In-context learning (ICL) using large pre001 trained autoregressive language models (LLMs, 002 e.g. GPT-3) has demonstrated effective clas003 sification performance at a variety of natural 004 language tasks. Using LLMs for intent detec005 tion is challenging due to the large label space 006 and limited context window, such that it is diffi007 cult to fit a sufficient number of examples in the 008 prompt to allow the use of in-context learning. 009 In this paper, dense retrieval is used to bypass 010 this limitation, giving the model only a par011 tial view of the full label space. We show that 012 retriever-augmented large language models are 013 an effective way to tackle intent detection, by014 passing context window limitations effectively 015 through the retrieval mechanism. Comparing 016 the LLaMA and OPT model families at differ017 ent scales, we set new state of the art perfor018 mance in the few-shot setting with zero training 019 for two of the three intent classification datasets 020 that we consider, while achieving competitive 021 results on the third one. This work demon022 strates that the Retriever+ICL framework is a 023 strong zero-training competitor to fine-tuned in024 tent detection approaches. In addition, a small 025 study on the number of examples provided at 026 different model scales is done, showing that 027 larger models are needed to make effective use 028 of more examples in-prompt. 029",
        "paperId": "024908a72e2a7eeb59cc4183c028de19207cac40"
    },
    {
        "title": "Pre-Attentive, Context-Specific Representation of Fear Memory in the Auditory Cortex of Rat",
        "firstAuthor": "A. Funamizu",
        "url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0063655&type=printable",
        "dateSubmitted": "2013-05-06",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Neural representation in the auditory cortex is rapidly modulated by both top-down attention and bottom-up stimulus properties, in order to improve perception in a given context. Learning-induced, pre-attentive, map plasticity has been also studied in the anesthetized cortex; however, little attention has been paid to rapid, context-dependent modulation. We hypothesize that context-specific learning leads to pre-attentively modulated, multiplex representation in the auditory cortex. Here, we investigate map plasticity in the auditory cortices of anesthetized rats conditioned in a context-dependent manner, such that a conditioned stimulus (CS) of a 20-kHz tone and an unconditioned stimulus (US) of a mild electrical shock were associated only under a noisy auditory context, but not in silence. After the conditioning, although no distinct plasticity was found in the tonotopic map, tone-evoked responses were more noise-resistive than pre-conditioning. Yet, the conditioned group showed a reduced spread of activation to each tone with noise, but not with silence, associated with a sharpening of frequency tuning. The encoding accuracy index of neurons showed that conditioning deteriorated the accuracy of tone-frequency representations in noisy condition at off-CS regions, but not at CS regions, suggesting that arbitrary tones around the frequency of the CS were more likely perceived as the CS in a specific context, where CS was associated with US. These results together demonstrate that learning-induced plasticity in the auditory cortex occurs in a context-dependent manner.",
        "paperId": "02575c39ccf77e8c850b2e13e59831e24f403a36"
    },
    {
        "title": "I Felt Safe to be a Child, I Wanted to Learn",
        "firstAuthor": "Ann Higgins",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "025ce48f1f23413aef6df2252816768ef3345dd4"
    },
    {
        "title": "Quantifying and Transferring Contextual Information in Object Detection",
        "firstAuthor": "Weishi Zheng",
        "url": "https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/2539/2/ZHENGQuantifyingand2012POST.pdf",
        "dateSubmitted": "2012-04-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Context is critical for reducing the uncertainty in object detection. However, context modeling is challenging because there are often many different types of contextual information coexisting with different degrees of relevance to the detection of target object(s) in different images. It is therefore crucial to devise a context model to automatically quantify and select the most effective contextual information for assisting in detecting the target object. Nevertheless, the diversity of contextual information means that learning a robust context model requires a larger training set than learning the target object appearance model, which may not be available in practice. In this work, a novel context modeling framework is proposed without the need for any prior scene segmentation or context annotation. We formulate a polar geometric context descriptor for representing multiple types of contextual information. In order to quantify context, we propose a new maximum margin context (MMC) model to evaluate and measure the usefulness of contextual information directly and explicitly through a discriminant context inference method. Furthermore, to address the problem of context learning with limited data, we exploit the idea of transfer learning based on the observation that although two categories of objects can have very different visual appearance, there can be similarity in their context and/or the way contextual information helps to distinguish target objects from nontarget objects. To that end, two novel context transfer learning models are proposed which utilize training samples from source object classes to improve the learning of the context model for a target object class based on a joint maximum margin learning framework. Experiments are carried out on PASCAL VOC2005 and VOC2007 data sets, a luggage detection data set extracted from the i-LIDS data set, and a vehicle detection data set extracted from outdoor surveillance footage. Our results validate the effectiveness of the proposed models for quantifying and transferring contextual information, and demonstrate that they outperform related alternative context models.",
        "paperId": "026e18e6a9286f23c6b0ff5f7e351816a543eb51"
    },
    {
        "title": "Adaptive spatio-temporal context learning for visual tracking",
        "firstAuthor": "Yaqin Zhang",
        "url": null,
        "dateSubmitted": "2019-02-14",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "ABSTRACT In recent years, a spatio-temporal context (STC) algorithm has attracted the attention of scholars, due to the algorithm makes full use of the information of the target background. Although the STC algorithm achieve tracking at the real-time, but there is still a need to improve the tracking capability when the target is occluded or the size of the target changes. In this paper, we presented an adaptive spatio-temporal context learning for visual tracking (AFSTC). Firstly, in order to accurately describe the appearance of the target, we integrate Histogram of Oriented Gradient (HOG) and Colour-naming (CN) features. And then we use the average difference between two adjacent frames to adjust the learning rate of update model for adaptive tracking. Finally, we adjust parameters of scale update strategy to achieve the competitive results on accuracy and robustness. We perform experiments on the Online Tracking Benchmark (OTB) 2015 dataset. Our tracker achieves a 13% relative gain in distance precision compared to the traditional STC algorithm. Moreover, although the speed of our tracker reduces, but it reaches 129.99 frames per second (FPS) and can still achieve tracking at the real-time.",
        "paperId": "02d9c6a3d1347c6dac66732d99c41e3c1ad8bc77"
    },
    {
        "title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP",
        "firstAuthor": "O. Khattab",
        "url": "http://arxiv.org/pdf/2212.14024",
        "dateSubmitted": "2022-12-28",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple\"retrieve-then-read\"pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120%, 8-39%, and 80-290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp",
        "paperId": "03532123ccffae8d411264320e8a5ae2b6eddea0"
    },
    {
        "title": "Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection",
        "firstAuthor": "Costas Mavromatis",
        "url": null,
        "dateSubmitted": "2023-10-30",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large Language Models (LLMs) can adapt to new tasks via in-context learning (ICL). ICL is efficient as it does not require any parameter updates to the trained LLM, but only few annotated examples as input for the LLM. In this work, we investigate an active learning approach for ICL, where there is a limited budget for annotating examples. We propose a model-adaptive optimization-free algorithm, termed AdaICL, which identifies examples that the model is uncertain about, and performs semantic diversity-based example selection. Diversity-based sampling improves overall effectiveness, while uncertainty sampling improves budget efficiency and helps the LLM learn new information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage problem, that dynamically adapts based on the model's feedback and can be approximately solved via greedy algorithms. Extensive experiments on nine datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy points over SOTA (7.7% relative improvement), is up to 3x more budget-efficient than performing annotations uniformly at random, while it outperforms SOTA with 2x fewer ICL examples.",
        "paperId": "03613effe356d2a8815f899027d6a5868822fd93"
    },
    {
        "title": "In-Context Analogical Reasoning with Pre-Trained Language Models",
        "firstAuthor": "Xiaoyang Hu",
        "url": "http://arxiv.org/pdf/2305.17626",
        "dateSubmitted": "2023-05-28",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven\u2019s Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abstraction over task features, finding that higher-level abstractions further strengthen PLMs\u2019 analogical reasoning. Our detailed analysis reveals insights on the role of model complexity, in-context learning, and prior knowledge in solving RPM tasks.",
        "paperId": "0366177b44ed13d86b9d704a3a82ea3750e5abed"
    },
    {
        "title": "Transformative learning experiences among international postgraduate students in the classroom contexts",
        "firstAuthor": "S. Sivagnanam",
        "url": null,
        "dateSubmitted": "2016-10-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This study was carried out to identify transformative learning experiences among international postgraduate students in UTM. This study was conducted to explore and understand how the international postgraduate students experience transformative learning in the classroom contexts. Learning experience that went through by the international postgraduate students would give positive impact towards teaching and learning process in public Universities in Malaysia. Meaning perspective is related to student interaction with the environment and learning styles among international postgraduate students. Transformative learning theory is the main theory used in the framework of this study, in addition to the theory of self-directed learning, socio-cultural and constructive. All four of these theories are important in understanding and exploring the perspectives of meaning and significance scheme of respondents in the context of learning in the classroom. Therefore, to explore and understand this research in depth, qualitative method which is in-depth interview (semi-structured) being used by the researcher. Phenomenological approach and case studies are also used to understand the transformative learning among postgraduate students were selected using purposive sampling. Thematic analysis was used to analyze the transcripts of the eight respondents correctly. The results showed that participants can change the structure of meaning (meaning old structure) they are old to the new (revised new meaning structure) through information technology, in-class discussions and physical environment of the classroom. This study provides a deep understanding of the phenomenon of international students abroad. However, due to the limitations of the research, future studies related to this topic need to explore more by upcoming researchers.",
        "paperId": "036d0d1b8c132c118c6d637189949928b5087e85"
    },
    {
        "title": "Modern Languages (MOL)",
        "firstAuthor": "",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "MOL 210 | CLASSICAL MYTHOLOGY | 4 quarter hours (Undergraduate) This course is an interdisciplinary blend of the classical traditions of myth, literature and philosophy, concentrating on myth. It endeavors to place Classical literature into its historical, social and cultural contexts. Students will learn significant myths and the names and functions of the most important characters in them. They will investigate how the ancients used traditional narratives and images to explore, explain and experiment with ideas about themselves and their surroundings in those contexts. Learning about how myth is variously interpreted (as for example by means of theories of myth-interpretation) as well as basic methods of literary criticism (e.g., analysis of language, content, structure, etc.), students will employ ways to identify, understand and interpret the different types of communication present in myth-literature. As they move chronologically through the ancient world students will observe how myths change to reflect differing individual and collective concerns, as well as the specific interests (and so the significant aspects of discourse) of authors and audiences as conditions change. Students will also learn about literary genres, including poetry, tragedy and prose, their properties and distinctions and discover how genres also reflect audiences and times. Students will read and evaluate modern views of myth in order to understand how myth is open to multiple interpretations and upon what sounder bases myth is to be interpreted. While analyzing myths' diversity in various forms of literature, students will come to comprehend what mythology meant to the ancients, as well as those facets and ingredients of myth that are universal, enduring, and meaningful today. MOL 211 | ANCIENT GREEK AND ROMAN EPIC | 4 quarter hours (Undergraduate) This course centers upon the Homeric Iliad and Odyssey and Vergil's Aeneid and endeavors to place these epic poems into their historical, social, and cultural contexts. Students will learn the definition of epic as a literary genre and discover how this genre evolved to reflect audiences and times. They will learn the components of epic language, in particular, literary devices and structural features (e.g., formulas, nested stories, epic similes). They will be able to describe the plots of the three epics and know the mainand mid-level human characters, gods, and goddesses. They will be able to define and better understand the meanings of \"hero\" and \"heroism.\" Students will be able to express mature appreciation for the epics as whole works. Learning how the epics are variously interpreted as well as basic methods of literary criticism (e.g., analysis of language, content, structure, etc.), students will employ these as ways to understand and interpret the poems. As they read, learn and evaluate modern views of the epics, students will also acquire better means to distinguish critically between views and interpretations. A strong emphasis in this class will be upon the vital connections between past and present, and how students can become more aware of and understand important lasting concepts such as heroism, leadership, self-definition, etc. Finally, students will relate these stories to modern story-telling in order to understand how the heroes of the ancient Greeks live with us today. In short, they will interpret what epic poetry offered ancient listeners and what it has to teach modern readers. The classical tradition is rich with meaning and significance, even to modern 21st century adults and this class will not only be an exploration of the culture and instruction of the ancient world through epic, but an investigation of what classic motifs remain with us today.",
        "paperId": "03945ff519dfb96e924f7abccb40746c8a792fff"
    },
    {
        "title": "Empowering MultiModal Models\u2019 In-Context Learning Ability through Large Language Models",
        "firstAuthor": "Wenjuan Han",
        "url": null,
        "dateSubmitted": "2023-07-28",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Pretrained visual-language models (VLMs) have made progress in developing multimodal models to improve various tasks. However, they lack reasoning and in-context learning ability. Building on the success of large language models (LLMs) in general-purple NLP tasks, researchers anticipate that the VLM should also have the same strong reasoning and ICL ability through specific techniques, for example benefiting from LLMs. To boost VLMs to solve vision-language problems via few-shot exemplars, we suggest a vision-language model, called MIC1.",
        "paperId": "03d10eb9f1e03a059b41167a672af203419b0b7d"
    },
    {
        "title": "Blended learning model for enhancing entrepreneurial skills among women",
        "firstAuthor": "Navnath Bhagchand Tupe",
        "url": null,
        "dateSubmitted": "2018-04-13",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This theoretical article is devoted to the creation of Blended Learning Model [BLM] which aims at providing a learning environment for enriching entrepreneurial skills among women.\u00a0 Product Development process has been employed for developing BLM. Self-instructional strategies are also applied to design the learning situation in the BLM. Face to face and online mode of learning are effectively blended in the BLM which includes 70% learning through online and only 30% happens in face to face mode. There is a scientific course alignment that has been established in different aspects of the BLM such as the aims & objectives, commitments of learner, content and context, learning outcomes and human interactions, interactions with content, scenarios based learning for creating contextual learning environment, etc. Self-governed learning activities have been developed for the purpose of effective learning. Maximum teaching-learning activities were carried out through a computer-mediated online platform and some of them were provided through the face to face mode of learning. The development process of BLM has been described in detailed in present paper.",
        "paperId": "03edbe54183437d9df11bcf0d646059718903354"
    },
    {
        "title": "Impaired spatial learning in the APPSwe\u2003+\u2003PSEN1\u0394E9 bigenic mouse model of Alzheimer\u2019s disease",
        "firstAuthor": "R. Reiserer",
        "url": null,
        "dateSubmitted": "2007-02-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Mice co\u2010expressing the Swedish amyloid precursor protein mutation (APPSwe) and exon 9 deletion (\u0394E9) of the PSEN1 gene begin to develop amyloid plaques at 6\u20137 months of age. We demonstrate here a spatial learning deficit in 7\u2010month\u2010old APPSwe\u2003+\u2003PSEN1\u0394E9 bigenic mice using an adaptation of the Barnes maze. Mice were first trained on a cued target followed by a hidden\u2010target condition. Although bigenic mice quickly learned the cued\u2010target version of the task, they were significantly impaired when switched to the hidden\u2010target version. In contrast, a separate group of double\u2010transgenic mice trained first on the spatial hidden\u2010target version of the task were unimpaired relative to wild\u2010type controls. We propose that processes such as general rule learning, context learning and exploratory habituation exert a greater influence when the testing environment is novel and overshadow the spatial memory deficit in naive bigenic mice. However, when cued\u2010target training is conducted first, these processes habituate and the spatial learning deficit is unmasked. Seven\u2010month\u2010old APPSwe\u2003+\u2003PSEN1\u0394E9 mice were unimpaired on tests of memory that did not involve learning the rules governing spatial associations.",
        "paperId": "043f4dcebe8b9ffb723e8216b67b074b0af66d37"
    },
    {
        "title": "Skill-Based Few-Shot Selection for In-Context Learning",
        "firstAuthor": "Shengnan An",
        "url": "https://arxiv.org/pdf/2305.14210",
        "dateSubmitted": "2023-05-23",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In-context learning is the paradigm that adapts large language models to downstream tasks by providing a few examples. Few-shot selection -- selecting appropriate examples for each test instance separately -- is important for in-context learning. In this paper, we propose Skill-KNN, a skill-based few-shot selection method for in-context learning. The key advantages of Skill-KNN include: (1) it addresses the problem that existing methods based on pre-trained embeddings can be easily biased by surface natural language features that are not important for the target task; (2) it does not require training or fine-tuning of any models, making it suitable for frequently expanding or changing example banks. The key insight is to optimize the inputs fed into the embedding model, rather than tuning the model itself. Technically, Skill-KNN generates the skill-based descriptions for each test case and candidate example by utilizing a pre-processing few-shot prompting, thus eliminating unimportant surface features. Experimental results across five cross-domain semantic parsing datasets and six backbone models show that Skill-KNN significantly outperforms existing methods.",
        "paperId": "04526876688e5a56106629229309fae272da1c79"
    },
    {
        "title": "Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning",
        "firstAuthor": "Xindi Wang",
        "url": "https://arxiv.org/pdf/2307.15411",
        "dateSubmitted": "2023-07-28",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large language models (LLMs) have shown remarkable capacity for in-context learning (ICL), where learning a new task from just a few training examples is done without being explicitly pre-trained. However, despite the success of LLMs, there has been little understanding of how ICL learns the knowledge from the given prompts. In this paper, to make progress toward understanding the learning behaviour of ICL, we train the same LLMs with the same demonstration examples via ICL and supervised learning (SL), respectively, and investigate their performance under label perturbations (i.e., noisy labels and label imbalance) on a range of classification tasks. First, via extensive experiments, we find that gold labels have significant impacts on the downstream in-context performance, especially for large language models; however, imbalanced labels matter little to ICL across all model sizes. Second, when comparing with SL, we show empirically that ICL is less sensitive to label perturbations than SL, and ICL gradually attains comparable performance to SL as the model size increases.",
        "paperId": "0456cd227edb95e596e3915ebcfd1133bcc8d725"
    },
    {
        "title": "LooGLE: Can Long-Context Language Models Understand Long Contexts?",
        "firstAuthor": "Jiaqi Li",
        "url": null,
        "dateSubmitted": "2023-11-08",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large language models (LLMs), despite their impressive performance in various language tasks, are typically limited to processing texts within context-window size. This limitation has spurred significant research efforts to enhance LLMs' long-context understanding with high-quality long-sequence benchmarks. However, prior datasets in this regard suffer from shortcomings, such as short context length compared to the context window of modern LLMs; outdated documents that have data leakage problems; and an emphasis on short dependency tasks rather than long dependency tasks. In this paper, we present LooGLE, a Long Context Generic Language Evaluation benchmark for LLMs' long context understanding. LooGLE features relatively new documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning diverse domains. Human annotators meticulously crafted more than 1,100 high-quality question-answer pairs to meet the long dependency requirements. These pairs underwent thorough cross-validation, yielding the most precise assessment of LLMs' long dependency capabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed key findings: (i) commercial models outperformed open-sourced models; (ii) LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks; (iii) in-context learning and chaining thoughts offered only marginal improvements; (iv) retrieval-based techniques demonstrated substantial benefits for short question-answering, while strategies for extending context window length had limited impact on long context understanding. As such, LooGLE not only provides a systematic and comprehensive evaluation schema on long-context LLMs, but also sheds light on future development of enhanced models towards\"true long-context understanding\".",
        "paperId": "0484f05b6b3c11a9344cb623649ae867f172046f"
    },
    {
        "title": "Communal Versus Individual Learning of a Math-Estimation Task: African American Children and the Culture of Learning Contexts",
        "firstAuthor": "Eric A. Hurley",
        "url": "http://www.pages.pomona.edu/~eah04747/documents/commvsindiv05.pdf",
        "dateSubmitted": "2005-11-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The authors compared the performance of 78 African American 5th-grade students who studied a math-estimation task in one of two learning contexts. Learning contexts differed in the degree to which they afforded the expression of communalism. ANCOVA confirmed that posttest performance was best for students who studied in the high communal-learning context. The findings support A. W. Boykin's (1994) contention that the cultural context of learning can be a critical mediator of children's performance.",
        "paperId": "04996c547a000a2406a8149f6a14457412be9d4e"
    },
    {
        "title": "Developing a Cloud-Based Mobile Learning Adoption Model to Promote Sustainable Education",
        "firstAuthor": "Naim Ahmad",
        "url": "https://www.mdpi.com/2071-1050/12/8/3126/pdf?version=1586786486",
        "dateSubmitted": "2020-04-13",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Education plays a very significant role in the context of sustainability. As the world population is growing, providing education through the traditional classroom setting is not sufficient and not feasible to extend learning in professional life. Therefore, modern technology-mediated learning paradigms such as mobile learning are becoming increasingly popular. Mobile learning is said to integrate multiple contexts, learning types, mobilities and communications. As information and communications technology (ICT) plays a vital role in the delivery of mobile learning services, it is very essential to adopt sustainable IT resources to keep it viable. Cloud computing offers a range of affordable, scalable and on-demand solutions. This paper attempts to model important critical success factors (CSFs) in the area of cloud-based mobile learning using the interpretive structural modeling (ISM) technique. ISM helps in identifying the hierarchical inter-relationships between the variables of study with the help of experts in the field. Finally, Matrice d\u2019Impacts Crois\u00e9s-Multiplication Appliqu\u00e9e \u00e1 un Classement (MICMAC) analysis is employed to classify the variables into dependent and independent variables. Management support has been identified as most rudimentary among sixteen CSFs identified through a literature review to establish a distinguished relative advantage. Further, the paper discusses the theoretical underpinning of all the constructs. This study will help organizations to implement mobile learning in sustainable ways.",
        "paperId": "04b6c8443dae4e89f7ecf67da69c238b8e1c8d76"
    },
    {
        "title": "Teaching Vocabulary in All Classrooms",
        "firstAuthor": "Camille L. Z. Blachowicz",
        "url": null,
        "dateSubmitted": "1995-11-09",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Chapter 1 Vocabulary in the Classroom: A Theoretical and Practical Perspective *What Does the Research Tell Us About Vocabulary? *What Does It Mean to Know a Word? *What Is Effective Vocabulary Instruction? Chapter 2 Learning Vocabulary from Context *Learning from Context *Encouraging Informal Word Learning *Learning to Problem-Solve with Context *Using Contextual Methods to Present New Vocabulary Chapter 3 Integrating Vocabulary and Reading Strategy Instruction *Developing Strategic Reading *Vocabulary in Strategic Reading Instruction *Classroom Examples Chapter 4 Learning Vocabulary in Literature-Based Reading Instruction *Literature-Based Reading Instruction *Figurative Language Chapter 5 Learning Vocabulary in the Content Areas *Teaching New Meanings for Known Words *Teaching New Words for New Concepts *Teaching New Words for Known Concepts Chapter 6 Using Dictionaries and Other References *The Nature of Definitions and Defining *The Nature of Dictionaries *Understanding How to Use a Dictionary *Using Dictionaries in the Classroom *Knowing Other Resources for Word Learning Chapter 7 Assessing Vocabulary Knowledge *Assessment for Instruction *Standardized Measures of Vocabulary *Diagnosis for Special Needs Chapter 8 Vocabulary Instruction for Diversity: English Language Learners and Struggling Readers *Making Connections Through Topic Relatedness *Making Connections Through Word Relatedness *Making Connections with Imagery *Making Connections: English-Language Learners Chapter 9 Vocabulary and Spelling Instruction Using Structural Anaysis *Morphological Connections *Spelling and Morphemic Analysis *Etymology Chapter 10 Wordplay in the Classroom *Why Do WordPlay in the Classroom? *Using Books About Words *Using Riddles, Jokes, and Puns *Using Word Games *Using Art for WordPlay *Using Drama *Using Puzzles *Using Computers Appendices A School Dictionaries B Books for Expanding Vocabulary and Wordplay C Computer Programs with a Vocabulary Focus (Including Publishers' Recommended Levels) D Vocabulary/Word Games References Name Index Subject Index",
        "paperId": "04c207fb8a78bb66ef41a765cf96818da627889f"
    },
    {
        "title": "Visual context learning based on textual knowledge for image-text retrieval",
        "firstAuthor": "Yuzhuo Qin",
        "url": null,
        "dateSubmitted": "2022-05-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "04e09327b31b7a3509e82f18b49bc9f80ff3d315"
    },
    {
        "title": "EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning",
        "firstAuthor": "Rajasekhar Reddy Mekala",
        "url": "https://arxiv.org/pdf/2309.10687",
        "dateSubmitted": "2023-09-16",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Language models are achieving impressive performance on various tasks by aggressively adopting inference-time prompting techniques, such as zero-shot and few-shot prompting. In this work, we introduce EchoPrompt, a simple yet effective approach that prompts the model to rephrase its queries before answering them. EchoPrompt is adapted for both zero-shot and few-shot in-context learning with standard and chain-of-thought prompting. Experimental results show that EchoPrompt yields substantial improvements across all these settings for four families of causal language models. These improvements are observed across various numerical reasoning (e.g. GSM8K, SVAMP), reading comprehension (e.g. DROP), and logical reasoning (e.g. Coin Flipping) tasks. On average, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002 by 5% in numerical tasks and 13% in reading comprehension tasks. We investigate the factors contributing to EchoPrompt's effectiveness through ablation studies, which reveal that both the original query and the model-generated rephrased version are instrumental in its performance gains. Our empirical results indicate that EchoPrompt is an effective technique that enhances in-context learning performance. We recommend incorporating EchoPrompt into various baseline prompting strategies to achieve performance boosts.",
        "paperId": "04e838c16f3d1fb8d69d34fe0a0a92c59717875b"
    },
    {
        "title": "The Construction of a Customized Medical Corpus for Assisting Chinese Clinicians in English Research Article Writing",
        "firstAuthor": "Xiaowen Wang",
        "url": null,
        "dateSubmitted": "2016-10-15",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "053cdfd02208cac299696e68eb705256e82e9ca0"
    },
    {
        "title": "MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations",
        "firstAuthor": "Arkil Patel",
        "url": null,
        "dateSubmitted": "2023-10-18",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Humans possess a remarkable ability to assign novel interpretations to linguistic expressions, enabling them to learn new words and understand community-specific connotations. However, Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly. Therefore, it is crucial for LLMs to learn novel interpretations in-context. In this paper, we systematically analyse the ability of LLMs to acquire novel interpretations using in-context learning. To facilitate our study, we introduce MAGNIFICo, an evaluation suite implemented within a text-to-SQL semantic parsing framework that incorporates diverse tokens and prompt settings to simulate real-world complexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a surprisingly robust capacity for comprehending novel interpretations from natural language descriptions as well as from discussions within long conversations. Nevertheless, our findings also highlight the need for further improvements, particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example. Additionally, our analysis uncovers the semantic predispositions in LLMs and reveals the impact of recency bias for information presented in long contexts.",
        "paperId": "0577ca1b6f8d9cddbad7f76ea7f82dc71b5af043"
    },
    {
        "title": "HPA-Net: Hierarchical and Parallel Aggregation Network for Context Learning in Stereo Matching",
        "firstAuthor": "Wei Chen",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "058e11d5c1a93234c355042b6050e63c2f65585f"
    },
    {
        "title": "Distributed Model of Collicular and Cerebellar Function during Saccades",
        "firstAuthor": "L. Optican",
        "url": "http://lsr-web.net/Assets/NEIPages/LanceOptican/pdf/optican_quaia_ANYAS_02.pdf",
        "dateSubmitted": "2002-04-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Abstract: How does the brain tell the eye where to go? Classical models of rapid eye movements are lumped control systems that compute analogs of physical signals such as desired eye displacement, instantaneous error, and motor drive. Components of these lumped models do not correspond well with anatomical and physiological data. We have developed a more brain\u2010like, distributed model (called a neuromimetic model), in which the superior colliculus (SC) and cerebellum (CB) play novel roles, using information about the desired target and the movement context to generate saccades. It suggests that the SC is neither sensory nor motor; rather it encodes the desired sensory consequence of the saccade in retinotopic coordinates. It also suggests a non\u2010computational scheme for motor control by the cerebellum, based on context learning and a novel spatial mechanism, the pilot map. The CB learns to use contextual information to initialize the pilot signal that will guide the saccade to its goal. The CB monitors feedback information to steer and stop the saccade, and thus replaces the classical notion of a displacement integrator. One consequence of this model is that no desired eye movement signal is encoded explicitly in the brain; rather it is distributed across activity in both the SC and CB. Another is that the transformation from spatially coded sensory information to temporally coded motor information is implicit in the velocity feedback loop around the CB. No explicit spatial\u2010to\u2010temporal transformation with a normalization step is needed.",
        "paperId": "05b2532e46cb4b0512cbb3a67a1284684d3ef732"
    },
    {
        "title": "GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information",
        "firstAuthor": "Qiao Jin",
        "url": null,
        "dateSubmitted": "2023-04-19",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "While large language models (LLMs) have been successfully applied to various tasks, they still face challenges with hallucinations. Augmenting LLMs with domain-specific tools such as database utilities can facilitate easier and more precise access to specialized knowledge. In this paper, we present GeneGPT, a novel method for teaching LLMs to use the Web APIs of the National Center for Biotechnology Information (NCBI) for answering genomics questions. Specifically, we prompt Codex to solve the GeneTuring tests with NCBI Web APIs by in-context learning and an augmented decoding algorithm that can detect and execute API calls. Experimental results show that GeneGPT achieves state-of-the-art performance on eight tasks in the GeneTuring benchmark with an average score of 0.83, largely surpassing retrieval-augmented LLMs such as the new Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as well as GPT-3 (0.16) and ChatGPT (0.12). Our further analyses suggest that: (1) API demonstrations have good cross-task generalizability and are more useful than documentations for in-context learning; (2) GeneGPT can generalize to longer chains of API calls and answer multi-hop questions in GeneHop, a novel dataset introduced in this work; (3) Different types of errors are enriched in different tasks, providing valuable insights for future improvements.",
        "paperId": "05e003a34148d4663734d3f39deefa0979d2a0e6"
    },
    {
        "title": "Teacher reflections on implementing a learning cycle in EFL writing classes",
        "firstAuthor": "Judith Runnels",
        "url": null,
        "dateSubmitted": "2020-08-11",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This action research study began with classroom observations of a learning cycle informed English as a Foreign Language writing class (referred to as the first learning context) for the purposes of creating a general how-to guide for implementing a learning cycle within a writing course. The guide was then implemented in a writing skills class in a different educational context (referred to as the second learning context). A learning cycle was introduced to help learners become more accustomed to peer-editing, giving peer feedback, performing self-assessments and being more critical of their own work. It was found that the learning cycle functioned very differently in the second learning context and not entirely as intended, despite modifications that were made to account for differences between the two learning contexts. Teacher reflections revealed that differences between the reasons for using a learning cycle, assumptions about the similarities between learning contexts (the two courses and their content), decisions regarding changes to the second contexts\u2019 learning materials, differences in student population and other unforeseen differences affected how the learning cycle operated. Critical interactions with sample performance writing texts, the provision or collaborative development of assessment criteria and feedback prompts for peer-editing, materials which support reflection on each task and at the end of the course, and additional class time spent on reflective discussion are all identified as key components of a learning cycle when used in an EFL writing class. The reflections also revealed that learning cycles can have utility when applied to contexts vastly different to those from where they were developed. Recommendations and suggested supporting resources for teachers interested in implementing learning cycles within their own contexts are provided.",
        "paperId": "05fd0b19d67f8d6f28ef5ebbf2293b607f7ee2d8"
    },
    {
        "title": "Att skapa en yrkesidentitet : En studie om organisationers strategier f\u00f6r nyanst\u00e4llda",
        "firstAuthor": "Joseph Wahlund",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The purpose of this study is to examine how certain organisations enable new employees to develop a professional identity. Professional identity is in this sense defined in the study as a social identity that is established and consolidated in regard to a specific profession. The basis of this study is the socio-cultural perspective in which all human action and thought is seen as situated in a certain social context. Learning is viewed in this perspective as an ongoing social and active process that bestows meaning to substance, through which knowledge is attained by interaction with others. The collection of empirical data is grounded in the holding of semi-structured interviews of seven HR-representatives from various organisations. This data is then transcribed and assessed through the use of codification and thematic ascription. The various themes through which data is assessed are social interaction, shared responsibility, introduction to work-specifications, and the influence of corporate culture on an organisation. This study concludes that the organisations in question actively work to promote an environment in which the establishing of professional identities by new employees is practicable when mentorship holds a central role in this socialising process.",
        "paperId": "0619d851436e6b81f4b547714b246d388b511766"
    },
    {
        "title": "Developing teachers\u2019 and teacher educators\u2019 professional identity in changing contexts",
        "firstAuthor": "K. Livingston",
        "url": null,
        "dateSubmitted": "2016-08-07",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "As I indicated in my last editorial, the title of the 41st Annual Conference of the Association for Teacher Education in Europe (ATEE) was \u2018Educating the best teachers: a challenge for teacher education\u2019. Discussions among the conference delegates following keynote addresses, parallel sessions and in the research community meetings concerned a range of issues including innovation in schools and teacher education. Questions arising in these discussions were \u2013 what characteristics do the \u2018best\u2019 teachers have? and how do teacher education programmes enable the development of the \u2018best\u2019 teachers? Many of the discussions were about the identification of characteristics of a teacher\u2019s identity that enable and promote innovation. The topic of teacher identity is not new and much has already been written and said about it. Indeed a search of back issues of this journal will reveal that teacher identity has been the focus of many articles over the last decade. Why then are we still talking about teacher identity? The discussions about identity that I participated in during the conference with delegates from different countries, highlighted that teacher identity is not easily described or fixed. It evolves as contexts, learning and teaching approaches and student/teacher relationships change \u2013 our understanding about who we are as teachers\u2019 changes. Some teachers feel comfortable about re-forming their identity as a teacher and others find it a very difficult process. Discussions inevitably led to further questions about the role of teacher education in the development of teachers\u2019 professional identity. In particular how to support the development of professional identities that enable teachers to take an inquiring, reflective and collaborative approach to learning and teaching in school. Fred Korthagen, one of the keynote speakers at the conference, focused his presentation on developing understanding of the complexity of the relationships between personal and professional identity and the influences of the environment in which we work. If teacher education is to be successful in supporting teachers in identity shifts, we must be willing to re-form our own identities as teacher educators in changing educational contexts. This requires self-reflection and a deeper understanding of the relationships between who we are teachers and teacher educations and our values and beliefs about education in general and learning and teaching approaches in particular. It also requires a better understanding of the implications of our way of being in the classroom for the decisions we make about what we teach, our pedagogical choices and our relationships with students. While the articles in this issue do not all necessarily consider identity specifically, all the authors in some way consider teacher reflection, influences on pedagogical approaches, collaborative relationships or the development of a better understanding who we are as teachers from different perspectives. Wall and Hall, the authors of the first article in this issue, discuss data gathered as part of a longitudinal collaborative study conducted with teachers in England from schools and further education colleges. The article concerns the relationship between the pedagogies used by teachers to develop their students\u2019 metacognition and their own learning and metacognitive knowledge and skills. K\u00f6nig and Pflanzl also explore teachers\u2019 pedagogy in the second article. Their particular focus is on the relationship between what is identified in the article as teachers\u2019 general pedagogical knowledge and the quality of student instruction. In the article, the authors discuss",
        "paperId": "06525dec2c2bdf8a3e8ac2070a1e42cdc049b3be"
    },
    {
        "title": "Humanizing secondary school contexts: learnings from Aotearoa New Zealand and Peru Latin America",
        "firstAuthor": "Fickel Lh",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "068a5369006d542c007133546a44a105302e5cdd"
    },
    {
        "title": "An Approach of Context Ontology for Robust Face Recognition Against Illumination Variations",
        "firstAuthor": "M. RezaulBashar",
        "url": "http://eprints.usq.edu.au/2911/2/ICICT2007.pdf",
        "dateSubmitted": "2007-03-07",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This paper proposes a face recognition method that is robust against image variations due to arbitrary lighting condition. Though many researches have been carried out on face recognition system, however; there exist some limitations such as illumination, pose, alignment, occlusion, etc. This paper presents a context ontology model making a robust face recognition system on different illumination situations. Our proposed system works on two phases: environmental context ontology building (modelling) and recognition using context ontology. Context ontology is built using context acquisition, context learning and context categorization. The recognition approach is implemented on illumination variant face recognition that takes identified context as input and performs recognition with usual process such as preprocessing, feature extraction, learning, and recognition. We have tested the recognition performance of our proposed model with an international standard FERET face database (our produced synthesized FERET images) and we have achieved a success rate of more than 92%.",
        "paperId": "069b872a0db0840efbb2e7f6b7b6c4b01b0b3839"
    },
    {
        "title": "Word Embedding-based Context-sensitive Network Flow Payload Anomaly Detection",
        "firstAuthor": "Yizhou Li",
        "url": null,
        "dateSubmitted": "2021-07-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Payload anomaly detection can discover malicious behaviors hidden in network packets. It is hard to handle payload due to its various possible characters and complex semantic context, and thus identifying abnormal payload is also a non-trivial task. Prior art only uses the n-gram language model to extract features, which directly leads to ultra-high-dimensional feature space and also fails to capture the context semantics fully. Accordingly, this paper proposes a word embedding-based context-sensitive network flow payload anomaly detection method (termed WECAD). First, WECAD obtains the initial feature representation of the payload through the word embedding-based method. Then, we propose a corpus pruning algorithm, which applies the cosine similarity clustering and frequency distribution to prune inconsequential characters. We only keep the essential characters to reduce the calculation space. Subsequently, we propose a context learning algorithm. It employs the co-occurrence matrix transformation technology and introduces the backward step size to consider the order relationship of essential characters. Comprehensive experiments on real-world intrusion detection datasets validate the effectiveness of our method.",
        "paperId": "06ac8e47c81276aea9b1767306dfc838df6634f7"
    },
    {
        "title": "Computer-Assisted Explorations in Mathematics: Pedagogical Adaptations across the Atlantic",
        "firstAuthor": "H. Miller",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "These questions will be explored through an analysis of a pair of mathematics courses \u2013 the Project Laboratory in Mathematics at MIT and CATAM, Computer Assisted Teaching of All Mathematics, at Cambridge University \u2013 developed under a grant from the Cambridge MIT Institute. They share certain features \u2013 a focus on the application of computation in the understanding of mathematical contexts, learning activities which are very open-ended relative to their respective institutional standards, and some specific content \u2013 but differ markedly in others.",
        "paperId": "06c22f494c234da28a3656e99781425b45a50831"
    },
    {
        "title": "Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners",
        "firstAuthor": "Hyunsoo Cho",
        "url": "http://arxiv.org/pdf/2212.10873",
        "dateSubmitted": "2022-12-21",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Through in-context learning (ICL), large-scale language models are effective few-shot learners without additional model fine-tuning. However, the ICL performance does not scale well with the number of available training sample as it is limited by the inherent input length constraint of the underlying language model. Meanwhile, many studies have revealed that language models are also powerful feature extractors, allowing them to be utilized in a black-box manner and enabling the linear probing paradigm, where lightweight discriminators are trained on top of the pre-extracted input representations. This paper proposes prompt-augmented linear probing (PALP), a hybrid of linear probing and ICL, which leverages the best of both worlds. PALP inherits the scalability of linear probing and the capability of enforcing language models to derive more meaningful representations via tailoring input into a more conceivable form. Throughout in-depth investigations on various datasets, we verified that PALP significantly closes the gap between ICL in the data-hungry scenario and fine-tuning in the data-abundant scenario with little training overhead, potentially making PALP a strong alternative in a black-box scenario.",
        "paperId": "06edda0310b4ec7c5012d012349252a3a77521b6"
    },
    {
        "title": "GPT-4 Vision on Medical Image Classification - A Case Study on COVID-19 Dataset",
        "firstAuthor": "Ruibo Chen",
        "url": null,
        "dateSubmitted": "2023-10-27",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This technical report delves into the application of GPT-4 Vision (GPT-4V) in the nuanced realm of COVID-19 image classification, leveraging the transformative potential of in-context learning to enhance diagnostic processes.",
        "paperId": "06ee879ef6759d4590627bb35d8a17ed070f2de6"
    },
    {
        "title": "STEM: Teaching Space Science of Extraterrestrial Development and Defense",
        "firstAuthor": "Ronald H. Freeman",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The 2013 Next Generation Science Standards required core science curricula to include for all K-12 grades, the subject area space science. Students learning space science contributes to a science literacy that will correlate in real time to the evolving human-crewed exploration and human-robotic extraterrestrial space development. In contrast to learning space science ideas first, students will discover relevancy in a context learning approach of space. World-wide use of space systems is broken down into three user communities: Military, Civil, and Commercial. This paper describes the operations of each user community\u2019s mission in which space science ideas are embedded. The paper further outlines the needed activities (and progress) for developing notional architectures user communities will extend to the lunar surface and Mars planet in-situ resource utilization and communications.",
        "paperId": "06fc22044fb81f83d2cd68d2c32a052bd279cd89"
    },
    {
        "title": "ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games",
        "firstAuthor": "Ruoyao Wang",
        "url": "http://arxiv.org/pdf/2305.14879",
        "dateSubmitted": "2023-05-24",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In this work, we investigate the capacity of language models to generate explicit, interpretable, and interactive world models of scientific and common-sense reasoning tasks. We operationalize this as a task of generating text games, expressed as hundreds of lines of Python code. To facilitate this task, we introduce ByteSized32 (Code: github.com/cognitiveailab/BYTESIZED32), a corpus of 32 reasoning-focused text games totaling 20k lines of Python code. We empirically demonstrate that GPT-4 can use these games as templates for single-shot in-context learning, successfully producing runnable games on unseen topics in 28% of cases. When allowed to self-reflect on program errors, game runnability substantially increases to 57%. While evaluating simulation fidelity is labor-intensive, we introduce a suite of automated metrics to assess game fidelity, technical validity, adherence to task specifications, and winnability, showing a high degree of agreement with expert human ratings. We pose this as a challenge task to spur further development at the juncture of world modeling and code generation.",
        "paperId": "070b91f80ac118b910c1d2ab5be9f65f685979fe"
    },
    {
        "title": "Les styles d'apprentissage des \u00e9tudiants de la facult\u00e9 de m\u00e9decine de Sousse (Tunisie) Learning styles of medical student in the faculty of medicine of Sousse (Tunisia)",
        "firstAuthor": "M. E. Ghardallou",
        "url": "https://www.pedagogie-medicale.org/articles/pmed/pdf/2013/04/pmed120065.pdf",
        "dateSubmitted": "2013-08-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Context: Learning style appears to play a significant role in choosing a successful learning approach. The teachers' role is to identify and adapt the teaching techniques to their student's styles. Goal: Describing the learning styles of medical students at the faculty of medicine of Sousse (Tunisia). Subjects and methods: A descriptive cross-sectional study was conducted among five cohorts of students from the faculty of medicine of Sousse during RECHERCHE ET PERSPECTIVES",
        "paperId": "071b6a340678c8ed1ee2346c3becd23f8268341a"
    },
    {
        "title": "Exploring Diverse In-Context Configurations for Image Captioning",
        "firstAuthor": "Xu Yang",
        "url": "http://arxiv.org/pdf/2305.14800",
        "dateSubmitted": "2023-05-24",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "After discovering that Language Models (LMs) can be good in-context few-shot learners, numerous strategies have been proposed to optimize in-context sequence configurations. Recently, researchers in Vision-Language (VL) domains also develop their few-shot learners, while they only use the simplest way, i.e., randomly sampling, to configure in-context image-text pairs. In order to explore the effects of varying configurations on VL in-context learning, we devised four strategies for image selection and four for caption assignment to configure in-context image-text pairs for image captioning. Here Image Captioning is used as the case study since it can be seen as the visually-conditioned LM. Our comprehensive experiments yield two counter-intuitive but valuable insights, highlighting the distinct characteristics of VL in-context learning due to multi-modal synergy, as compared to the NLP case.",
        "paperId": "0744783bbefc12b2b1383bed137e8a80061274b7"
    },
    {
        "title": "Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners",
        "firstAuthor": "Jihyeon Janel Lee",
        "url": "https://arxiv.org/pdf/2307.14856",
        "dateSubmitted": "2023-07-27",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In-context learning, which offers substantial advantages over fine-tuning, is predominantly observed in decoder-only models, while encoder-decoder (i.e., seq2seq) models excel in methods that rely on weight updates. Recently, a few studies have demonstrated the feasibility of few-shot learning with seq2seq models; however, this has been limited to tasks that align well with the seq2seq architecture, such as summarization and translation. Inspired by these initial studies, we provide a first-ever extensive experiment comparing the in-context few-shot learning capabilities of decoder-only and encoder-decoder models on a broad range of tasks. Furthermore, we propose two methods to more effectively elicit in-context learning ability in seq2seq models: objective-aligned prompting and a fusion-based approach. Remarkably, our approach outperforms a decoder-only model that is six times larger and exhibits significant performance improvements compared to conventional seq2seq models across a variety of settings. We posit that, with the right configuration and prompt design, seq2seq models can be highly effective few-shot learners for a wide spectrum of applications.",
        "paperId": "07bc02bd16f6fe78a7ea3bb8d966fcc6e3893195"
    },
    {
        "title": "PITCH PROCESSING IN MUSIC AND SPEECH",
        "firstAuthor": "B. Tillmann",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "INTRODUCTION A highly-debated question is to what extent music and language share processing components. Beyond syntax and temporal structure processing, one studied aspect is pitchprocessing in a given domain and across domains (e.g., [1]). Pitch processing is crucial in music. For example, in Western tonal music, it is a form-bearing dimension (next to temporal structures). Pitch processing is also crucial in speech, notably for discriminating questions and statements, as well as for communicating emotional expressions. This is valid for non-tone intonation languages (e.g., English, French) and tone languages (e.g., Mandarin, Thai, Vietnamese). However, for tone languages, pitch processing is even more crucial as pitch information is used for communicating word meaning. Tone languages comprise 70% of the world\u2019s languages and are spoken by more than 50% of the world\u2019s population. In these languages, tone variations (comprising predominantly fundamental frequency (F0) height and contour parameters) at the syllabic level have the same effect on word meaning as do vowel and consonant variations in non-tone languages. For example, the syllable /ma/ combined with different tones (e.g., tones describing contours of rather constant level, rising dipping or falling patterns in Mandarin) represents different lexical items. Interestingly, music and language differ in the size of the pitch differences that are relevant for each of the systems (speech intonation versus musical structures). For speech intonation of non-tonal languages, F0 variations, in particular those indicating statements and questions, are typically coarse (up to more than 12 semitones1 for the pitch rise of the final word in a question; e.g., [2]). For music (as in most research, we are referring here mostly to the Western tonal system), however, the pitch variations are typically more fine-grained (1 or 2 semitones; see [3]). In tone languages, the range of F0 variations can be as small as in music of the Western tonal system or larger depending on the tones and the tone languages. The present paper proposes an overview of research that investigates pitch processing by considering cognitive processes (related to context, learning, memory and/or knowledge) for both music and language material. While extensive research has focussed on pitch processing in musical material as well as the influence of musical expertise (e.g., comparing musicians and nonmusicians), research investigating pitch processing in tone language material for either musicians, nonmusicians or tone language speakers (or both), provides complementary information about underlying mechanisms. Furthermore, investigating deficits (such as in amusia) provides further insights into the potential domain-specificity or domaingenerality of pitch processing mechanisms.",
        "paperId": "07e2470d13433d5239a9ad92b0f5daca4aa8c0ba"
    },
    {
        "title": "Context-aware attention network for image recognition",
        "firstAuthor": "Jiaxu Leng",
        "url": null,
        "dateSubmitted": "2019-06-18",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "08636d6365ce71690bd9d88f295f1cc7eb654f41"
    },
    {
        "title": "Assessing Translation capabilities of Large Language Models involving English and Indian Languages",
        "firstAuthor": "Vandan Mujadia",
        "url": null,
        "dateSubmitted": "2023-11-15",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. In this work, our aim is to explore the multilingual capabilities of large language models by using machine translation as a task involving English and 22 Indian languages. We first investigate the translation capabilities of raw large language models, followed by exploring the in-context learning capabilities of the same raw models. We fine-tune these large language models using parameter efficient fine-tuning methods such as LoRA and additionally with full fine-tuning. Through our study, we have identified the best performing large language model for the translation task involving LLMs, which is based on LLaMA. Our results demonstrate significant progress, with average BLEU scores of 13.42, 15.93, 12.13, 12.30, and 12.07, as well as CHRF scores of 43.98, 46.99, 42.55, 42.42, and 45.39, respectively, using 2-stage fine-tuned LLaMA-13b for English to Indian languages on IN22 (conversational), IN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets. Similarly, for Indian languages to English, we achieved average BLEU scores of 14.03, 16.65, 16.17, 15.35 and 12.55 along with chrF scores of 36.71, 40.44, 40.26, 39.51, and 36.20, respectively, using fine-tuned LLaMA-13b on IN22 (conversational), IN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets. Overall, our findings highlight the potential and strength of large language models for machine translation capabilities, including for languages that are currently underrepresented in LLMs.",
        "paperId": "088617a8862cfa372a62070916e88a5f10e7690b"
    },
    {
        "title": "The Wicked Problem of Climate Change: A New Approach Based on Social Mess and Fragmentation",
        "firstAuthor": "Jiazhe Sun",
        "url": "https://www.mdpi.com/2071-1050/8/12/1312/pdf?version=1481631764",
        "dateSubmitted": "2016-12-13",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The 21st century has been the warmest period on record since 1880, making the problem of climate change a central issue in the global political arena. While most approaches to climate change emphasize setting and imposing thresholds for greenhouse gas emissions, this paper argues that the issue of climate change and its solutions should be viewed in a more dynamic and complex way, involving social messes and the fragmentation of industries and organizations. In this context, learning models can offer a starting point to understand the reasons why organizations engage in certain types of corporate environmental strategies with regard to climate change, and can help in the search for solutions to the problem of climate change.",
        "paperId": "089a6f242d657a52a7b65caf521460ea78150fbd"
    },
    {
        "title": "CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation",
        "firstAuthor": "Aswanth Kumar",
        "url": null,
        "dateSubmitted": "2023-05-23",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large language models have demonstrated the capability to perform on machine translation when the input is prompted with a few examples (in-context learning). Translation quality depends on various features of the selected examples, such as their quality and relevance, but previous work has predominantly focused on individual features in isolation. In this paper, we propose a general framework for combining different features influencing example selection. We learn a regression model, CTQ Scorer (Contextual Translation Quality), that selects examples based on multiple features in order to maximize the translation quality. On multiple language pairs and language models, we show that CTQ Scorer helps significantly outperform random selection as well as strong single-factor baselines reported in the literature. We also see an improvement of over 2.5 COMET points on average with respect to a strong BM25 retrieval-based baseline.",
        "paperId": "08ac959a8c235917453bcad207d16b30d6e3ca2f"
    },
    {
        "title": "Literacy learning in the early years",
        "firstAuthor": "C. Barratt-Pugh",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Acknowledgements Introduction The socio-cultural context of literacy learning Linking literacy learning across different contexts Learning about words, sounds and letters Play and literacy learning Children's literature and literacy learning Information communication technologies and literacy learning Negotiating critical literacies with young children Literacies in more than one language Monitoring young children's literacy learning References Index.",
        "paperId": "08b72ad671a7a66475232d8cd4363f271c586655"
    },
    {
        "title": "Semi-supervised siamese network using self-supervision under scarce annotation improves class separability and robustness to attack",
        "firstAuthor": "G. B. Cavallari",
        "url": null,
        "dateSubmitted": "2021-10-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Self-supervised learning approaches were shown to benefit feature learning by training models under a pretext task. In this context, learning from limited data can be tackled using a combination of semi-supervised learning and self-supervision. In this paper we combine the traditional supervised learning paradigm with the rotation prediction self-supervised task, that are used simultaneously to train a siamese model with a joint loss function and shared weights. In particular, we are interested in the case in which the proportion of labeled with respect to unlabeled data is small. We investigate the effectiveness of a compact feature space obtained after training under such limited annotation scenario, in terms of linear class separability and under attack. The study includes images from multiple domains, such as natural images (STL-10 dataset), products (Fashion-MNIST dataset) and biomedical images (Malaria dataset). We show that in scenarios where we have only a few labeled data the model augmented with a self-supervised task can take advantage of the unlabeled data to improve the learned representation in terms of the linear discrimination, as well as allowing learning even under attack. Also, we discuss the choices in terms of self-supervision and cases of failure considering the different datasets.",
        "paperId": "09085b5e7a514192535a6cd9218b74af356e3f85"
    },
    {
        "title": "The Development of Math Learning Tools for Elementary Based on 2013 Curriculum in Coastal Area",
        "firstAuthor": "Zuhri D",
        "url": "https://jes.ejournal.unri.ac.id/index.php/JES/article/download/7940/6598",
        "dateSubmitted": "2020-01-27",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "K13 has not been implemented optimally, one of them is caused by the inability to develop learning tools that apply the scientific approach. Student-environment contexts rich in learning resources can not be utilized by teachers. This condition encourages us to conduct a study in developing K13-based elementary school educational devices by utilizing coastal area contexts. Learning tools developed are lesson plans, learning media and teaching materials. The purpose of this study is to produce lesson plans, learning media and teaching materials that are scientifically designed coastal areas are valid so feasible to use. In the development of these learning tools, the researcher uses a 4-D framework model. The research data was collected using a validated questionnaire. Validation results indicate that: (1) the validity test of lesson plans are in a very valid category; (2) validity test of valid category media; and (3) the validity test of the teaching materials is in a very valid category.",
        "paperId": "091533b5e09ca31911691baaf364c5c76cbfbbdd"
    },
    {
        "title": "Mindfulness and Technology: A Blended Learning",
        "firstAuthor": "Pramila Bakhati",
        "url": "https://journalajess.com/index.php/AJESS/article/download/793/1559",
        "dateSubmitted": "2022-12-22",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Children in the 21st century have no choice to be isolated from the digital world as they almost grow up with devices from an early stage. The availability of internet service and digitization\u00a0expanded the access to children for learning opportunities, even those who belong to the remote area of developing countries. Since internet services have connected people globally, creating many opportunities and adding new challenges to young children, such as cyber bullying, lack of physical exercise, and many others. Despite the numerous advantages of Information, Communication, and Technology (ICT), experts worry that children spend most of their time with devices contributing to\u00a0digital dependency\u00a0and\u00a0screen addiction. It has changed the children's way of life, thoughts, and experiences, as these issues are the subject of public debate of overuse of the internet increased the threat to their well-being. One of the significant risks is that children are facing mental health problems, including anxiety, depression, and lack of inner peace. Thus, the education system should rethink balancing digital learning and real-world context learning in the 21st-century education system. The present paper discusses how young children are affected by digital dependency and how mindfulness support mitigating these issues by adopting blended learning of technology and mindfulness in the teaching-learning process.",
        "paperId": "091a486de6ab4f34887983d7514b9c7c657766dc"
    },
    {
        "title": "Weighted Part Context Learning for Visual Tracking",
        "firstAuthor": "Guibo Zhu",
        "url": null,
        "dateSubmitted": "2015-09-16",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Context information is widely used in computer vision for tracking arbitrary objects. Most of the existing studies focus on how to distinguish the object of interest from background or how to use keypoint-based supporters as their auxiliary information to assist them in tracking. However, in most cases, how to discover and represent both the intrinsic properties inside the object and the surrounding context is still an open problem. In this paper, we propose a unified context learning framework that can effectively capture spatiotemporal relations, prior knowledge, and motion consistency to enhance tracker's performance. The proposed weighted part context tracker (WPCT) consists of an appearance model, an internal relation model, and a context relation model. The appearance model represents the appearances of the object and the parts. The internal relation model utilizes the parts inside the object to directly describe the spatiotemporal structure property, while the context relation model takes advantage of the latent intersection between the object and background regions. Then, the three models are embedded in a max-margin structured learning framework. Furthermore, prior label distribution is added, which can effectively exploit the spatial prior knowledge for learning the classifier and inferring the object state in the tracking process. Meanwhile, we define online update functions to decide when to update WPCT, as well as how to reweight the parts. Extensive experiments and comparisons with the state of the arts demonstrate the effectiveness of the proposed method.",
        "paperId": "09255414dac54b5168977d2bc871b85cb88eee50"
    },
    {
        "title": "Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations",
        "firstAuthor": "Xinxi Lyu",
        "url": "http://arxiv.org/pdf/2212.09865",
        "dateSubmitted": "2022-12-19",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Although large language models can be prompted for both zero- and few-shot learning, performance drops significantly when no demonstrations are available. In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap by constructing pseudo-demonstrations for a given test input using a raw text corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the nearest neighbors to the test input from the corpus and pairing them with random task labels, and (2) applying a set of techniques to reduce the amount of direct copying the model does from the resulting demonstrations. Evaluation on nine classification datasets shows that Z-ICL outperforms previous zero-shot methods by a significant margin, and is on par with in-context learning with labeled training data in the few-shot setting. Overall, Z-ICL provides a significantly higher estimate of the zero-shot performance levels of a model, and supports future efforts to develop better pseudo-demonstrations that further improve zero-shot results.",
        "paperId": "0942bd8fad71282994ff4e9a779c09745da68edc"
    },
    {
        "title": "Cleft lip and palate in context: Learning from, and adding to, the sociological literature on long-term conditions",
        "firstAuthor": "R. Abualfaraj",
        "url": null,
        "dateSubmitted": "2018-07-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Cleft lip and palate is a common congenital anomaly affecting males and females. While there is psychological research on cleft lip and palate, there is relatively little research exploring the social context of cleft lip and palate and the experiences of living with the condition on a daily basis. Drawing on common themes emerging from sociological work which have explored the experiences of people living with long-term conditions (uncertainty, social relations, self-esteem and self-image and biomedical concerns), we argue that these themes can be used to help elucidate the experiences of people living with cleft lip and palate. Within this framework, the findings of a qualitative study exploring the experiences of people living with cleft lip and palate are presented. The results suggest that all four themes can be found within the accounts of people living with cleft lip and palate, and there are many commonalities between the experiences of these people and those living with other long-term conditions. Conversely there are interesting areas of divergence. Unlike most long-term conditions, cleft lip and palate is not degenerative and treatment means symptoms will reduce over time. This is reflected in narratives around \u2018normality\u2019 as the endpoint of the care pathway. In addition, prenatal diagnosis means that the vast majority of participants within this study were born into, and grew up within, the care pathway. This has implications for the way in which cleft lip and palate is understood and the provision of information, relationships with members of the care team over time and the temporal and contextualised impact of cleft lip and palate on social relations and the self.",
        "paperId": "0945e673021048515040e966e59dbc7c363a99c4"
    },
    {
        "title": "Crafting In-context Examples according to LMs' Parametric Knowledge",
        "firstAuthor": "Yoonsang Lee",
        "url": null,
        "dateSubmitted": "2023-11-16",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In-context learning has been applied to knowledge-rich tasks such as question answering. In such scenarios, in-context examples are used to trigger a behaviour in the language model: namely, it should surface information stored in its parametric knowledge. We study the construction of in-context example sets, with a focus on the parametric knowledge of the model regarding in-context examples. We identify 'known' examples, where models can correctly answer from its parametric knowledge, and 'unknown' ones. Our experiments show that prompting with 'unknown' examples decreases the performance, potentially as it encourages hallucination rather than searching its parametric knowledge. Constructing an in-context example set that presents both known and unknown information performs the best across diverse settings. We perform analysis on three multi-answer question answering datasets, which allows us to further study answer set ordering strategies based on the LM's knowledge about each answer. Together, our study sheds lights on how to best construct in-context example sets for knowledge-rich tasks.",
        "paperId": "09744a25e57d7d1b95b35acd4ce9fda42e19c673"
    },
    {
        "title": "Context, learning, and extinction.",
        "firstAuthor": "S. Gershman",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "A. Redish et al. (2007) proposed a reinforcement learning model of context-dependent learning and extinction in conditioning experiments, using the idea of \"state classification\" to categorize new observations into states. In the current article, the authors propose an interpretation of this idea in terms of normative statistical inference. They focus on renewal and latent inhibition, 2 conditioning paradigms in which contextual manipulations have been studied extensively, and show that online Bayesian inference within a model that assumes an unbounded number of latent causes can characterize a diverse set of behavioral results from such manipulations, some of which pose problems for the model of Redish et al. Moreover, in both paradigms, context dependence is absent in younger animals, or if hippocampal lesions are made prior to training. The authors suggest an explanation in terms of a restricted capacity to infer new causes.",
        "paperId": "097ade72f43a979e652ed5a6aa801f5e8dfb9066"
    },
    {
        "title": "Algebra in context, learning and assessment",
        "firstAuthor": "K. Gravemeijer",
        "url": null,
        "dateSubmitted": "2002-07-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "097ccb1712ea525a5496780ec7e3f0337a2f00f9"
    },
    {
        "title": "Complementary Explanations for Effective In-Context Learning",
        "firstAuthor": "Xi Ye",
        "url": "http://arxiv.org/pdf/2211.13892",
        "dateSubmitted": "2022-11-25",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two different factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By perturbing explanations on three controlled tasks, we show that both factors contribute to the effectiveness of explanations. We further study how to form maximally effective sets of explanations for solving a given test query. We find that LLMs can benefit from the complementarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the in-context learning performance across three real-world tasks on multiple LLMs.",
        "paperId": "097dc73d5d422b3c09286e72d16b2561ae5fb395"
    },
    {
        "title": "Distance education for tobacco reduction with Inuit frontline health workers",
        "firstAuthor": "R. Collins",
        "url": "https://www.tandfonline.com/doi/pdf/10.3402/ijch.v72i0.21078?needAccess=true&role=button",
        "dateSubmitted": "2013-01-31",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Background Tobacco reduction is a major priority in Canadian Inuit communities. However, many Inuit frontline health workers lacked the knowledge, confidence and support to address the tobacco epidemic. Given vast distances, high costs of face-to-face training and previous successful pilots using distance education, this method was chosen for a national tobacco reduction course. Objective To provide distance education about tobacco reduction to at least 25 frontline health workers from all Inuit regions of Canada. Design Promising practices globally were assessed in a literature survey. The National Inuit Tobacco Task Group guided the project. Participants were selected from across Inuit Nunangat. They chose a focus from a \u201cmenu\u201d of 6 course options, completed a pre-test to assess individual learning needs and chose which community project(s) to complete. Course materials were mailed, and trainers provided intensive, individualized support through telephone, fax and e-mail. The course ended with an open-book post-test. Follow-up support continued for several months post-training. Results Of the 30 participants, 27 (90%) completed the course. The mean pre-test score was 72% (range: 38\u201398%). As the post-test was done using open books, everyone scored 100%, with a mean improvement of 28% (range: 2\u201362%). Conclusions Although it was often challenging to contact participants through phone, a distance education approach was very practical in a northern context. Learning is more concrete when it happens in a real-life context. As long as adequate support is provided, we recommend individualized distance education to others working in circumpolar regions.",
        "paperId": "09820dc5fc011df8efe7c35811384e93633d20d1"
    },
    {
        "title": "Neural Machine Translation Models Can Learn to be Few-shot Learners",
        "firstAuthor": "Raphael Reinauer",
        "url": "https://arxiv.org/pdf/2309.08590",
        "dateSubmitted": "2023-09-15",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The emergent ability of Large Language Models to use a small number of examples to learn to perform in novel domains and tasks, also called in-context learning (ICL). In this work, we show that a much smaller model can be trained to perform ICL by fine-tuning towards a specialized training objective, exemplified on the task of domain adaptation for neural machine translation. With this capacity for ICL, the model can take advantage of relevant few-shot examples to adapt its output towards the domain. We compare the quality of this domain adaptation to traditional supervised techniques and ICL with a 40B-parameter Large Language Model. Our approach allows efficient batch inference on a mix of domains and outperforms state-of-the-art baselines in terms of both translation quality and immediate adaptation rate, i.e. the ability to reproduce a specific term after being shown a single example.",
        "paperId": "09a85806442373f167e45eaf662a7914df048b10"
    },
    {
        "title": "Real-time Context-aware learning System for IoT Applications",
        "firstAuthor": "Bhaskar Das",
        "url": "https://arxiv.org/pdf/1810.11295",
        "dateSubmitted": "2018-10-26",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This paper introduces a real-time context-aware learning system that runs on mobile devices, collects data from the sensors, learns about the user-defined context, makes predictions in real-time, and manage IoT devices accordingly. However, the computational power of the mobile devices makes it challenging to run machine-learning algorithms with acceptable accuracy. Hence, existing works implement machine-learning algorithms on the server and transmit the results to the mobile devices. Although the context-aware predictions made by the server are more accurate than their mobile counterpart is it heavily depends on the network connection for the delivery of the results to the devices, which negatively affects real-time context learning. Therefore, in this paper, we propose a context-learning algorithm for mobile devices, which is less demanding on the computational resources and maintains the accuracy of the prediction by updating itself from the learning parameters obtained from the server periodically. Experimental results show that the proposed lightweight context-learning algorithm can achieve mean accuracy up to 97.51% while mean execution time requires only 11ms.",
        "paperId": "09b2b63747b8550749ce486e2e71aa842841c406"
    },
    {
        "title": "Nonparametric Bayesian Context Learning for Buried Threat Detection",
        "firstAuthor": "Christopher R. Ratto",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Abstract : This dissertation addresses the problem of detecting buried explosive threats (i.e. landmines and improvised explosive devices) with ground-penetrating radar (GPR) and hyperspectral imaging (HSI) across widely-varying environmental conditions. Automated detection of buried objects with GPR and HSI is particularly difficult due to the sensitivity of sensor phenomenology to variations in local environmental conditions. Past approaches have attempted to mitigate the effects of ambient factors by designing statistical detection and classification algorithms to be invariant to such conditions. An alternative approach to improving detection performance is to consider exploiting differences in sensor behavior across environments rather than mitigating them, and treat changes in the background data as a possible source of supplemental information for the task of classifying targets and non-targets. This approach is referred to as context-dependent learning. Although past researchers have proposed context-based approaches to detection and decision fusion, the definition of context used in this work differs from those used in the past. In this work, context is motivated by the physical state of the world from which an observation is made, and not from properties of the observation itself. The proposed context-dependent learning technique therefore utilized additional features that characterize soil properties from the sensor background, and a variety of nonparametric models were proposed for clustering these features into individual contexts. The number of contexts was assumed to be unknown a priori, and was learned via Bayesian inference using Dirichlet process priors. The learned contextual information was then exploited by an ensemble on classifiers trained for classifying targets in each of the learned contexts.",
        "paperId": "09de980aa0407a9c32e992404f3bdf2169509a26"
    },
    {
        "title": "Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task",
        "firstAuthor": "Mohsen Tabasi",
        "url": "https://aclanthology.org/2022.acl-short.36.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "As a recent development in few-shot learning, prompt-based techniques have demonstrated promising potential in a variety of natural language processing tasks. However, despite proving competitive on most tasks in the GLUE and SuperGLUE benchmarks, existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context (WiC) dataset. Specifically, none of the existing few-shot approaches (including the in-context learning of GPT-3) can attain a performance that is meaningfully different from the random baseline.Trying to fill this gap, we propose a new prompting technique, based on similarity metrics, which boosts few-shot performance to the level of fully supervised methods. Our simple adaptation shows that the failure of existing prompt-based techniques in semantic distinction is due to their improper configuration, rather than lack of relevant knowledge in the representations. We also show that this approach can be effectively extended to other downstream tasks for which a single prompt is sufficient.",
        "paperId": "0a0e48c469b124c9a03d4bc841311f59424e97f2"
    },
    {
        "title": "Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER",
        "firstAuthor": "Dong-Ho Lee",
        "url": "https://aclanthology.org/2022.acl-long.192.pdf",
        "dateSubmitted": "2021-10-16",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Recent advances in prompt-based learning have shown strong results on few-shot text classification by using cloze-style templates.Similar attempts have been made on named entity recognition (NER) which manually design templates to predict entity types for every text span in a sentence. However, such methods may suffer from error propagation induced by entity span detection, high cost due to enumeration of all possible text spans, and omission of inter-dependencies among token labels in a sentence. Here we present a simple demonstration-based learning method for NER, which lets the input be prefaced by task demonstrations for in-context learning. We perform a systematic study on demonstration strategy regarding what to include (entity examples, with or without surrounding context), how to select the examples, and what templates to use. Results on in-domain learning and domain adaptation show that the model\u2019s performance in low-resource settings can be largely improved with a suitable demonstration strategy (e.g., a 4-17% improvement on 25 train instances). We also find that good demonstration can save many labeled examples and consistency in demonstration contributes to better performance.",
        "paperId": "0a2ac054c533314c0659f3b139388527df0d42f3"
    },
    {
        "title": "Prompting Language Models for Linguistic Structure",
        "firstAuthor": "Terra Blevins",
        "url": "http://arxiv.org/pdf/2211.07830",
        "dateSubmitted": "2022-11-15",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns. To test this, we present a structured prompting approach for linguistic structured prediction tasks, allowing us to perform zero- and few-shot sequence tagging with autoregressive PLMs. We evaluate this approach on part-of-speech tagging, named entity recognition, and sentence chunking, demonstrating strong few-shot performance in all cases. We also find that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus, structured prompting can also retrieve linguistic structure with arbitrary labels. These findings indicate that the in-context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data.",
        "paperId": "0a67a5e3f4125445ed84f2db3c92429010aad68a"
    },
    {
        "title": "\u0e01\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e40\u0e21\u0e34\u0e19\u0e2a\u0e48\u0e07\u0e40\u0e2a\u0e23\u0e34\u0e21\u0e2a\u0e38\u0e02\u0e20\u0e32\u0e1e\u0e43\u0e19\u0e2a\u0e16\u0e32\u0e19\u0e1b\u0e23\u0e30\u0e01\u0e2d\u0e1a\u0e01\u0e32\u0e23\u0e43\u0e19\u0e01\u0e32\u0e23\u0e02\u0e31\u0e1a\u0e40\u0e04\u0e25\u0e37\u0e48\u0e2d\u0e19\u0e23\u0e30\u0e1a\u0e1a\u0e2a\u0e38\u0e02\u0e0a\u0e38\u0e21\u0e0a\u0e19 \u0e2a\u0e39\u0e48\u0e23\u0e30\u0e1a\u0e1a\u0e2a\u0e38\u0e02\u0e20\u0e32\u0e1e\u0e2d\u0e33\u0e40\u0e20\u0e2d \u0e40\u0e04\u0e23\u0e37\u0e2d\u0e02\u0e48\u0e32\u0e22\u0e1e\u0e22\u0e32\u0e1a\u0e32\u0e25 \u0e20\u0e32\u0e04\u0e15\u0e30\u0e27\u0e31\u0e19\u0e2d\u0e2d\u0e01 \u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28\u0e44\u0e17\u0e22",
        "firstAuthor": "\u0e1e\u0e19\u0e32\u0e23\u0e31\u0e15\u0e19\u0e4c \u0e40\u0e08\u0e19\u0e08\u0e1a",
        "url": null,
        "dateSubmitted": "2019-12-02",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The purpose of this evaluation research was to examine the strengthening health \u2013 promoting workplaces using a community - based participatory (CBPR) approach to enhance Eastern District Health System (DHS) in Thailand under the CIP model. This study focused on the primary health care and community health network in DHS at the 1st, 2nd, 3rd in public health region 6. The study was carried out during October 2016 -\u00a0 September 2018. The participants were recruited using purposive sampling. Quantitative data were analyzed using descriptive statistics and qualitative data were analyzed using content analysis. The results revealed that 3 districts and 2 workplaces succeeded in developing community health and health promotion (promotion behaviors) among 40 community nurses, 100 participants in 16 interdisciplinary teams and 72 community health networks. \nThe result of this study suggests that the approaches towards primary care system development and health, and community include: 1) the development of the potential primary care nurses in health promotion; 2) the encouragement of health promotion based on the community context learning; 3) the development process with tools \u201cfour main and three sub - components\u201d in order to analyze problem and foundation, and communicate with community; 4) the encouragement of the local government and public sector for integrating in the common goal; and 5) sharing and learning among healthcare providers in community health networks. The result attracts healthcare providers\u2019 interest to enhance the competency in community nurses. Nurses, healthcare providers, and community health networks should be encouraged and supported based on the context of community to create a sustainable health system.",
        "paperId": "0a6aa4785272fb304b5a33e19fcdbd29b469a5e5"
    },
    {
        "title": "Fine-tune Language Models to Approximate Unbiased In-context Learning",
        "firstAuthor": "Timothy Chu",
        "url": "https://arxiv.org/pdf/2310.03331",
        "dateSubmitted": "2023-10-05",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In-context learning (ICL) is an astonishing emergent ability of large language models (LLMs). By presenting a prompt that includes multiple input-output pairs as examples and introducing a new query input, models can generate the corresponding output. However, the performance of models heavily relies on the quality of the input prompt when implementing in-context learning. Biased or imbalanced input prompts can significantly degrade the performance of language models. To address this issue, we introduce a reweighted algorithm called RICL (Reweighted In-context Learning). This algorithm fine-tunes language models using an unbiased validation set to determine the optimal weight for each input-output example to approximate unbiased in-context learning. Furthermore, we also introduce a low-cost reweighted algorithm, a linear optimal weight approximation algorithm called LARICL (Linear Approximation of Reweighted In-context Learning). This algorithm requires minimal training cost while providing effective results. We prove the convergence of our algorithm and validate its performance through experiments conducted on a numerical dataset. The experimental findings reveal a substantial improvement in comparison to benchmarks including the performance of casual prompt-based in-context learning and the performance of a classic fine-tuning method.",
        "paperId": "0a9030dd6cc2d438509f7568c1081d58c2523d5c"
    },
    {
        "title": "Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning",
        "firstAuthor": "Yuchen Yang",
        "url": "https://arxiv.org/pdf/2310.04782",
        "dateSubmitted": "2023-10-07",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In recent years, large-scale language models (LLMs) have gained attention for their impressive text generation capabilities. However, these models often face the challenge of\"hallucination,\"which undermines their reliability. In this study, we introduce an uncertainty-aware in-context learning framework to empower the model to enhance or reject its output in response to uncertainty. Human-defined methods for estimating uncertainty typically assume that\"uncertainty is lower when the model's response is correct compared to when it is incorrect.\"However, setting a precise threshold to distinguish correctness is challenging. Therefore, we introduce uncertainty information as an intermediary variable that implicitly influences the model's behavior. Our innovative uncertainty-aware in-context learning framework involves fine-tuning the LLM using a calibration dataset. Our aim is to improve the model's responses by filtering out answers with high uncertainty while considering the model's knowledge limitations. We evaluate the model's knowledge by examining multiple responses to the same question for the presence of a correct answer. When the model lacks relevant knowledge, the response should indicate that the question cannot be answered. Conversely, when the model has relevant knowledge, the response should provide the correct answer. Extensive experiments confirm the effectiveness of our framework, leading to two key findings. First, the logit output values of the LLM partly reflect inherent uncertainty. Second, our model autonomously recognizes uncertainty, resulting in improved responses.",
        "paperId": "0aa5940fda7c994675d08c41eca2a6909eb6d205"
    },
    {
        "title": "Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning",
        "firstAuthor": "Jinyuan Wang",
        "url": null,
        "dateSubmitted": "2023-10-20",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In open-domain question-answering (ODQA), most existing questions require single-hop reasoning on commonsense. To further extend this task, we officially introduce open-domain multi-hop reasoning (ODMR) by answering multi-hop questions with explicit reasoning steps in open-domain setting. Recently, large language models (LLMs) have found significant utility in facilitating ODQA without external corpus. Furthermore, chain-of-thought (CoT) prompting boosts the reasoning capability of LLMs to a greater extent with manual or automated paradigms. However, existing automated methods lack of quality assurance, while manual approaches suffer from limited scalability and poor diversity, hindering the capabilities of LLMs. In this paper, we propose Self-prompted Chain-of-Thought (SP-CoT), an automated framework to mass-produce high quality CoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation pipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT selection and self-prompted inference via in-context learning. Extensive experiments on four multi-hop question-answering benchmarks show that our proposed SP-CoT not only significantly surpasses the previous SOTA methods on large-scale (175B) LLMs, but also nearly doubles the zero-shot performance of small-scale (13B) LLMs. Further analysis reveals the remarkable capability of SP-CoT to elicit direct and concise intermediate reasoning steps by recalling $\\sim$50\\% of intermediate answers on MuSiQue-Ans dataset.",
        "paperId": "0aaf7a76507248d80f65b6a49e200d2370bcb2c9"
    },
    {
        "title": "Unsupervised context learning in natural language processing",
        "firstAuthor": "J. Scholtes",
        "url": null,
        "dateSubmitted": "1991-07-08",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "By generalizing over contextual information, excellent results were obtained in connectionist language processing. Normally, these contexts are added manually to the system or deducted by using a supervised learning algorithm. A recurrent self-organizing model, capable of deriving the context from scratch, is presented. Syntactic features and structures are learned in a unsupervised way from flat sentences. By generalizing over the words as well as the sentences, simple semantics can be derived. The model forms a two-layer extension of the Kohonen feature map, provided with additional recurrent fibers which are responsible for the automatic determination of word contexts, thus resulting in an unsupervised recurrent learning algorithm. After a formal description of the model, the experimental results are presented.<<ETX>>",
        "paperId": "0ab65471608ea13f1970e20d4a2e2c77784d9a20"
    },
    {
        "title": "More Samples or More Prompt Inputs? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering",
        "firstAuthor": "Bingsheng Yao",
        "url": null,
        "dateSubmitted": "2023-11-16",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "While most existing works on LLM prompt-engineering focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can't we design and leverage multiple prompt inputs together to further improve the LLM performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompt-engineering technique to produce the most confident prediction results by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with two SOTA LLMs (FlanT5-XL and Mistral-7B) on three NLI datasets (e-SNLI, Multi-NLI, and ANLI) illustrate that ICS can consistently enhance LLM's prediction performance and confidence. An ablation study suggests that a diversity-based ICS strategy may further improve LLM's performance, which sheds light on a new yet promising future research direction.",
        "paperId": "0ab79543d98e375b9de1354766c024e165cc2369"
    },
    {
        "title": "Language learning and teaching \u2013 theory and practice",
        "firstAuthor": "Celce-Murcia Marianne",
        "url": null,
        "dateSubmitted": "1992-07-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "To provide some perspective on current issues and challenges concerning the role of grammar in language teaching, the article reviews some methodological trends of the past 25 years. When, and to what extent, one should teach grammar to language learners is a controversial issue. The paper proposes a decision-making strategy for resolving this controversy, based on learner and instructional variables. Then taking Canale and Swain's (1980) model of communicative competence, which views grammatical competence as one component of communicative competence, the paper argues that grammar instruction is part of language teaching. In this new role, grammar interacts with meaning, social function, or discourse \u2014 or a combination of these - rather than standing alone as an autonomous system to be learned for its own sake. After addressing feedback and correction in terms of research and pedagogical techniques, the article concludes with a survey of options for integrating grammar instruction into a communicative curriculum and with a reformulation of the role of grammar in language teaching. This paper details research carried out to examine individual differences in strategy use by adult second language learners, via both the Descriptive Test of Language Skills-Reading Comprehension Test (Forms A and B) and the Textbook Reading Profile. The aim was to evaluate the extent to which reading comprehension processes used during a standardised test relate to actual academic reading processes. The research involved 28 Spanish LI participants enrolled on a university-level ESL programme; when undertaking the test and the textbook reading exercises the testees used 'think aloud' protocol procedures and verbal reporting to explain which strategies they deployed to process the texts. Pritchard's inventory of 47 reading process strategies was used to provide a basis for the researchers' subsequent categorisation of the participant's taped responses. The results [tabular data] seemed to imply a tangible relationship between the intensity/orches-tration of strategy use and improved achievement on the reading comprehension measures described. Three tested case studies are also outlined, including that of a high scorer who was consistently able to identify when comprehension had failed. The data suggested though, that there is no single set of 'successful' processing strategies, as other factors such as interest, motivation and learning style are doubtless operationally significant but difficult to define or analyse. The superior control of cognitive processing dem-onstrated by children in the early stages of additive bilingualism may enhance symbolic reasoning abili-ties. The developmental interdependence of LI and L2 may allow additive-bilingual children to main-tain normal native-language development. This study examined the development of a Grade 2 additive-bilingual (Spanish-immersion) programme class as compared to a monolingual classroom on measures of non-verbal The purpose of this study was to compare three learning strategies - differentiated according to Craik and Lockhart's ' depths of processing' theory - for ESL vocabulary. Six intact ESL classes at two levels of proficiency were divided into three treatment groups (keyword, semantic, and key-word-semantic). These Arabic-speaking students then received four days of instruction. Both recognition and cued-recall instruments were used to measure effects both one day and nine days after treatment. Cued-recall results immediately after treatment revealed that the keyword method facilitated vocabularly acquisition for lower-pro-ficiency students. The delayed results for both the recognition and cued-recall tests suggested that the combined keyword-semantic strategy increased re-tention above the other strategies. Possible applications of these findings are discussed. this research was to contrast the acquisition of temporal systems in LI and tutored L2 learners. The research focused on the distinction between absolute and relative temporal location: latter time. An a sentence\u2014picture-matching adults learning ones and that for LI, digit span was more anxiety-provoking than was vocabulary. These results are interpreted in terms of the deficits created by anxiety during the cognitive processing of L2 stimuli. that a range of strategies may be used for learning vocabulary, each involving liabilities as well as assets. Students need aware of the range so as develop flexibility in responses unfamiliar This study was undertaken to address theoretical claims regarding the importance of negotiated interaction to the comprehension of second-language (L2) input through a comparison of three different interactional behaviours of L2 learners in a classroom context. Three groups of L2 learners were asked to carry out their teacher's directions to a comprehension task: eight Negotiators, who were encouraged to negotiate by requesting clarification, repetition, and confirmation of the directions; eight Observers, who were not permitted to interact with the teacher, but could watch and listen as the Negotiators did this; and eight Listeners, who carried out the task away from the other two groups by listening to a text of the directions which had been generated through negotiation. Results of the study revealed comparable comprehension scores for each of the three subject groups. Moreover, follow-up analyses suggested that individual subjects whose level of comprehension development was at or above the level of their classmates could comprehend the direction input whether they engaged in negotiation, observed negotiation, or listened to the text of negotiated input. However, for subjects at lower developmental levels of comprehension, direct participation in negotiation was the most effective means to facilitate comprehension of the direction input. This article presents a critical synthesis of French research into the teaching of oral skills at pre-primary and primary level published between 1970 and 1989 inclusive. It centres on two themes which make up the main objectives of oral teaching: the development of linguistic competence and the development of communicative competence. In the case of the former, the teaching is based mainly on permeation and analysis which aim to enlarge the mastery of the language. In the case of latter, it can be brought about in various ways: a functional approach, a strictly communicative approach, and a mixed communicative approach which is envisaged in a process of liberating \u2014 structuring, of object-ivisation or appropriation. The critique proposes essentially to call into question the pedagogic basis of some of the approaches, to show the limits of their efficiency and to draw out the main theoretical issues which confront research in this field. The paper evaluates the contribution of the different methodological procedures and principles offered by William Labov and John Gumperz, with particular reference to the problem of observer effect in community-level investigations of linguistic min-orities. While Labov's work seems generally to have fallen somewhat out of favour in recent years, and was in any event not devised for bilingual com-munities, it still offers researchers a clear set of replicable and adaptable procedures. Gumperz's procedures are on the other hand more clearly suitable for sociolinguistic research in bilingual minority communities, but error index as a factor in the evaluation of performance in text analysis widespread; guidelines of some Lander for the of A-level (Abitur) examinations in order to standardise scores as much as possible. This investigation, however, shows that at least in cases where responsibility for the content and correction of such examinations is decentralised, the assessment and evaluation of correct language use may also vary widely, even if the error index method is applied. The reservations which have been expressed in discussions in the field about the reliability of the error index are thus confirmed. This article examines the causes of errors made by pupils of French as a foreign language. The author has developed a lexical model with the help of which words are analysed according to the semantic relations which connect them. To this end, the word used incorrectly by the pupil is described by its relationship with the correct word being aimed at. The essential aim of vocabulary learning is to develop in the pupil an ever greater sensitivity to meaning, so that semantic frontiers are adjusted to semantically-related words which are to be learned. and recognises the influence of the test method and test-taker characteristics on test performance, (b) applications of more soph-isticated measurement and statistical tools, and (c) the development of' communicative' language tests that incorporate principles of 'communicative' teaching. After reviewing these advances, describes an interactional model of test performance that includes two components, ability and test method. ability consists of knowledge and metacognitive strategies, whereas test method includes characteristics of the environment, rubric, input, expected response, and relationship between input and expected response. Two aspects of authenticity are derived from this model. The situational authenticity of a given test task depends on the relationship between its test method characteristics and the features of a specific use situation, while authenticity pertains to the to which it invokes the test taker's language The application of this definition of authenticity to test development is discussed. scoring instrument ESL writing assessment contexts. Learning 41, 3 (1991), 337-73. This study investigated the validity of using a multiple-trait scoring procedure to obtain communicative writing profiles of the writing performance of adult non-native English speakers in assessment contexts different from that for which the instrument was designed. Transf",
        "paperId": "0ada4b1f56fa68df29091ad6a4db5c5fe5d58c04"
    },
    {
        "title": "How Do In-Context Examples Affect Compositional Generalization?",
        "firstAuthor": "Shengnan An",
        "url": "http://arxiv.org/pdf/2305.04835",
        "dateSubmitted": "2023-05-08",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Compositional generalization\u2013understanding unseen combinations of seen primitives\u2013is an essential reasoning capability in human intelligence.The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning\u2013the prevailing few-shot paradigm based on large language models\u2013exhibits compositional generalization.In this paper, we present CoFe, a test suite to investigate in-context compositional generalization.We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization.We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple.Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus.We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm.",
        "paperId": "0ae12d63f77f40b430f17c791a5191ff5fee5086"
    },
    {
        "title": "Grief in family and cultural context: learning from Latino families.",
        "firstAuthor": "Ester R. Shapiro",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The following article offers an integrative theory of family development in sociocultural context, which critically examines the goodness of fit between the grief experiences of culturally diverse families and prevailing North American grief practices. This model suggests that many Latino families, even when they are themselves from a variety of national backgrounds with somewhat different sociopolitical histories and cultural practices, offer an approach to death and grief that can enhance developmental outcomes in family bereavement. These qualities include a focus on family and extended family relationships as developmental resources; an appreciation of the spiritual and psychological continuity between the living and the dead; and an appreciation of the need to keep working on relationships, even after a death, so as to create new, more optimal shared understandings of family past, present, and future. The bereavement story of the Ruiz family, who migrated from rural Puerto Rico to the United States, serves to illustrate the ways that integration of Latino family experience into mental health models of grief can help expand understanding and improve bereavement outcomes for all families.",
        "paperId": "0ae815295ea71c29f5bd55467b0a34fd93bd453b"
    },
    {
        "title": "Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers",
        "firstAuthor": "Stephanie C. Y. Chan",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large transformer-based language models are able to perform few-shot learning (also known as in-context learning), without having been explicitly trained for it. We hypothesized that speci\ufb01c distributional properties of natural language might drive this emergent phenomenon, as these characteristics might lead to a kind of interpolation between few-shot meta-training (designed to elicit rapid few-shot learning) and standard supervised training (designed to elicit gradual in-weights learning). We also hypothesized that these distributional properties could lead to emergent few-shot learning in domains outside of language. Inspired by this idea, we ran a series of experiments on a standard image-based few-shot dataset. We discovered that a number of data properties did indeed promote the emergence of few-shot learning in transformer models. All of these properties are present in natural language \u2013 burstiness, long-tailedness, and many-to-one or one-to-many label mappings. The data in\ufb02uenced whether models were biased towards either few-shot learning vs. memorizing information in their weights; models could generally perform well at only one or the other. However, we discovered that an additional distributional property could allow the two capabilities to co-exist in the same model \u2013 a skewed, Zip\ufb01an distribution over classes \u2013 which occurs in language as well. Notably, training data that could elicit few-shot learning in transformers were unable to elicit few-shot learning in recurrent models. In sum, we \ufb01nd that few-shot learning emerges only from applying the right architecture to the right data distribution; neither component is su\ufb03cient on its own. However, if we train on skewed distributions, there is a sweet spot where both few-shot learning and in-weights memorization can be maintained at a high level in the same model (Zipf exponent = 1, for this particular training regime). Intriguingly, a Zipf exponent of 1 corresponds approximately to the skew in many natural languages. Rare items from training are never memorized (performance is at chance for all Zipf exponents) (e).",
        "paperId": "0b18c6f168e1a6d2f16eaa317747748c5c4c5b63"
    },
    {
        "title": "Alternative Pattern-making 3D Design Software",
        "firstAuthor": "Lida Aflatoony",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The purpose of this project was to explore alternative design software that allows 3D designs to easily convert into 2D patterns for apparel production. The 3D design software programs explored for this project include CAD software Rhinoceros (Rhino) and 3D Studio Max (3DsMax). In comparison to software such as Optitex, Modaris 3D Lectra, and other pattern-making software, these alternatives may be more accessible to design students, present a user-friendly interface, allow a more abstract approach in design creativity, and provide the interdisciplinary value beneficial for projects that may evolve into 3D printing. Although learning the professional pattern-making software is essential for apparel design students entering the apparel industry, these software programs, however, are not accessible to many students outside of their institution or working context. Learning alternative design software assists students to not being detached from the technology while being outside of the didactic or industry circumstances.",
        "paperId": "0b351630c43841c936133c632c78c75e8eff83dd"
    },
    {
        "title": "Increasing Probability Mass on Answer Choices Does Not Always Improve Accuracy",
        "firstAuthor": "Sarah Wiegreffe",
        "url": null,
        "dateSubmitted": "2023-05-24",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "When pretrained language models (LMs) are applied to discriminative tasks such as multiple-choice questions, they place probability mass on vocabulary tokens that aren't among the given answer choices. Spreading probability mass across multiple surface forms with identical meaning (such as\"bath\"and\"bathtub\") is thought to cause an underestimation of a model's true performance, referred to as the\"surface form competition\"(SFC) hypothesis. This has motivated the introduction of various probability normalization methods. However, many core questions remain unanswered. How do we measure SFC? Are there direct ways of reducing it, and does doing so improve task performance? We propose a mathematical formalism for SFC which allows us to quantify and bound its impact for the first time. We identify a simple method for reducing it -- namely, increasing probability mass on the given answer choices by a) including them in the prompt and b) using in-context learning with even just one example. We show this method eliminates the impact of SFC in the majority of instances. Our experiments on three diverse datasets and six LMs reveal several additional surprising findings. For example, both normalization and prompting methods for reducing SFC can be ineffective or even detrimental to task performance for some LMs. We conclude with practical insights for effectively prompting LMs for multiple-choice tasks.",
        "paperId": "0b9541209442dc8192b3df60efb3bb59e953ff63"
    },
    {
        "title": "High beta rhythm amplitude in olfactory learning signs a well-consolidated and non-flexible behavioral state",
        "firstAuthor": "Nicolas Fourcaud-Trocm\u00e9",
        "url": "https://www.nature.com/articles/s41598-019-56340-y.pdf",
        "dateSubmitted": "2019-12-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "0be03b330ded309e82845d24f31b217a8312240e"
    },
    {
        "title": "A Survey on Context Learning",
        "firstAuthor": "Guangxu Xun",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Learning semantics based on context information has been researched in many research areas for decades. Context information can not only be directly used as the input data, but also sometimes used as auxiliary knowledge to improve existing models. This survey aims at providing a structured and comprehensive overview of the research on context learning. We summarize and group the existing literature into four categories, Explicit Analysis, Implicit Analysis, Neural Network Models, and Composite Models, based on the underlying techniques adopted by them. For each category, we talk about the basic idea and techniques, and also introduce how context information is utilized as the model input or incorporated into the model to enhance the performance or extend the domain of application as auxiliary knowledge. In addition, we discuss the advantages and disadvantages of each model from both the technical and practical point of view.",
        "paperId": "0c0a778e6fdf7e36b1750c533dcc916f86608607"
    },
    {
        "title": "User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue",
        "firstAuthor": "Sam Davidson",
        "url": "https://arxiv.org/pdf/2309.13233",
        "dateSubmitted": "2023-09-23",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "One of the major impediments to the development of new task-oriented dialogue (TOD) systems is the need for human evaluation at multiple stages and iterations of the development process. In an effort to move toward automated evaluation of TOD, we propose a novel user simulator built using recently developed large pretrained language models (LLMs). In order to increase the linguistic diversity of our system relative to the related previous work, we do not fine-tune the LLMs used by our system on existing TOD datasets; rather we use in-context learning to prompt the LLMs to generate robust and linguistically diverse output with the goal of simulating the behavior of human interlocutors. Unlike previous work, which sought to maximize goal success rate (GSR) as the primary metric of simulator performance, our goal is a system which achieves a GSR similar to that observed in human interactions with TOD systems. Using this approach, our current simulator is effectively able to interact with several TOD systems, especially on single-intent conversational goals, while generating lexically and syntactically diverse output relative to previous simulators that rely upon fine-tuned models. Finally, we collect a Human2Bot dataset of humans interacting with the same TOD systems with which we experimented in order to better quantify these achievements.",
        "paperId": "0c110794ae91b4c165b0de3ff11fc841e2455bdb"
    },
    {
        "title": "Canonical Image Selection by Visual Context Learning",
        "firstAuthor": "Wen-gang Zhou",
        "url": null,
        "dateSubmitted": "2010-08-23",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Canonical image selection is to select a subset of photos that best summarize a photo collection. In this paper, we define the canonical image as those that contain most important and distinctive visual words. We propose to use visual context learning to discover visual word significance and develop Weighted Set Coverage algorithm to select canonical images containing distinctive visual words. Experiments with web image datasets demonstrate that the canonical images selected by our approach are not only representatives of the collected photos, but also exhibit a diverse set of views with minimal redundancy.",
        "paperId": "0c4138df382b115ce8ce1efb93c1f312603f7ca2"
    },
    {
        "title": "A novel passive forgery detection algorithm for video region duplication",
        "firstAuthor": "Lichao Su",
        "url": null,
        "dateSubmitted": "2017-05-09",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "0c6422a6b160961beefed275c83d96add8151be2"
    },
    {
        "title": "Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA",
        "firstAuthor": "Dhruv Agarwal",
        "url": null,
        "dateSubmitted": "2023-11-14",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "We present BYOKG, a universal question-answering (QA) system that can operate on any knowledge graph (KG), requires no human-annotated training data, and can be ready to use within a day -- attributes that are out-of-scope for current KGQA systems. BYOKG draws inspiration from the remarkable ability of humans to comprehend information present in an unseen KG through exploration -- starting at random nodes, inspecting the labels of adjacent nodes and edges, and combining them with their prior world knowledge. In BYOKG, exploration leverages an LLM-backed symbolic agent that generates a diverse set of query-program exemplars, which are then used to ground a retrieval-augmented reasoning procedure to predict programs for arbitrary questions. BYOKG is effective over both small- and large-scale graphs, showing dramatic gains in QA accuracy over a zero-shot baseline of 27.89 and 58.02 F1 on GrailQA and MetaQA, respectively. On GrailQA, we further show that our unsupervised BYOKG outperforms a supervised in-context learning method, demonstrating the effectiveness of exploration. Lastly, we find that performance of BYOKG reliably improves with continued exploration as well as improvements in the base LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1 on a sub-sampled zero-shot split of GrailQA.",
        "paperId": "0cb30be84a534399bebc6f4f53a3585b1eb48231"
    },
    {
        "title": "Collaborative Problem-Solving in the Cross-Border Context: Learning from Paired Local Communities along the Russian Border",
        "firstAuthor": "E. Mikhailova",
        "url": null,
        "dateSubmitted": "2018-07-03",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "ABSTRACT The article investigates governance structure and information sharing as managerial tools utilized by borderland communities to solve local problems coordinately. Focusing on three case studies of transfrontier intermunicipal cooperation on the Russian-Norwegian, Russian-Finnish and Russian-Chinese borders gave a chance to illustrate that selected instruments provide heterogeneous results in the cross-border context in terms of dependence on socio-economic and cultural circumstances of each locus and in the process of public value creation. Testing hypotheses revealed that governance structures of adjacent border municipalities tend to adjust to each other regardless the milieu, as well as majority of local mass media, tend to initiate collaboration with similar organizations across the border. However, these initiatives frequently remain unsuccessful as information sharing is a region-specific variable that relies on local communication culture, understanding of mass media mission and information production. Applying a ranging technique allowed visualization of carried out research.",
        "paperId": "0cf32a4cdfeeca19c5df3098d9f29ea87601ffba"
    },
    {
        "title": "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System",
        "firstAuthor": "Yunfan Gao",
        "url": "http://arxiv.org/pdf/2303.14524",
        "dateSubmitted": "2023-03-25",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually also hinder their broad deployment in real-world systems. To address these limitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building conversational recommender systems by converting user profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning, which also makes the recommendation process more interactive and explainable. What's more, within the Chat-Rec framework, user's preferences can transfer to different products for cross-domain recommendations, and prompt-based injection of information into LLMs can also handle the cold-start scenarios with new items. In our experiments, Chat-Rec effectively improve the results of top-k recommendations and performs better in zero-shot rating prediction task. Chat-Rec offers a novel approach to improving recommender systems and presents new practical scenarios for the implementation of AIGC (AI generated content) in recommender system studies.",
        "paperId": "0cfdd655100055f234fd23ebecd915504b8e00e3"
    },
    {
        "title": "In-Context Learning for Few-Shot Molecular Property Prediction",
        "firstAuthor": "Christopher Fifty",
        "url": "https://arxiv.org/pdf/2310.08863",
        "dateSubmitted": "2023-10-13",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In-context learning has become an important approach for few-shot learning in Large Language Models because of its ability to rapidly adapt to new tasks without fine-tuning model parameters. However, it is restricted to applications in natural language and inapplicable to other domains. In this paper, we adapt the concepts underpinning in-context learning to develop a new algorithm for few-shot molecular property prediction. Our approach learns to predict molecular properties from a context of (molecule, property measurement) pairs and rapidly adapts to new properties without fine-tuning. On the FS-Mol and BACE molecular property prediction benchmarks, we find this method surpasses the performance of recent meta-learning algorithms at small support sizes and is competitive with the best methods at large support sizes.",
        "paperId": "0d09c569477457c32637f9e866727cc4623b1165"
    },
    {
        "title": "MaPLe: Multi-modal Prompt Learning",
        "firstAuthor": "Muhammad Uzair Khattak",
        "url": "https://arxiv.org/pdf/2210.03117",
        "dateSubmitted": "2022-10-06",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Pre-trained vision-language (V-L) models such as CLIP have shown excellent generalization ability to downstream tasks. However, they are sensitive to the choice of input text prompts and require careful selection of prompt templates to perform well. Inspired by the Natural Language Processing (NLP) literature, recent CLIP adaptation approaches learn prompts as the textual inputs to fine-tune CLIP for downstream tasks. We note that using prompting to adapt representations in a single branch of CLIP (language or vision) is sub-optimal since it does not allow the flexibility to dynamically adjust both representation spaces on a downstream task. In this work, we propose Multi-modal Prompt Learning (MaPLe) for both vision and language branches to improve alignment between the vision and language representations. Our design promotes strong coupling between the vision-language prompts to ensure mutual synergy and discourages learning independent uni-modal solutions. Further, we learn separate prompts across different early stages to progressively model the stage-wise feature relationships to allow rich context learning. We evaluate the effectiveness of our approach on three representative tasks of generalization to novel classes, new target datasets and unseen domain shifts. Compared with the state-of-the-art method Co-CoOp, MaPLe exhibits favorable performance and achieves an absolute gain of 3.45% on novel classes and 2.72% on overall harmonic-mean, averaged over 11 diverse image recognition datasets. Our code and pre-trained models are available at https://github.com/muzairkhattak/multimodal-prompt-learning.",
        "paperId": "0d0dbfb1b315a43216020abaf74d289456198219"
    },
    {
        "title": "Church Ethics and Its Organizational Context: Learning from the Sex Abuse Scandal in the Catholic Church; Common Calling: The Laity & Governance of the Catholic Church",
        "firstAuthor": "G. Vigna",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "0e0e95ff50a2458009070dbae67b8ed447588df5"
    },
    {
        "title": "METFormer: A Motion Enhanced Transformer for Multiple Object Tracking",
        "firstAuthor": "Jianjun Gao",
        "url": null,
        "dateSubmitted": "2023-05-21",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Multiple object tracking (MOT) is an important task in computer vision, especially video analytics. Transformer-based methods are emerging approaches using both tracking and detection queries. However, motion modeling in existing transformer-based methods lacks effective association capability. Thus, this paper introduces a new METFormer model, a Motion Enhanced TransFormer-based tracker with a novel global-local motion context learning technique to mitigate the lack of motion information in existing transformer-based methods. The global-local motion context learning technique first centers on difference-guided global motion learning to obtain temporal information from adjacent frames. Based on global motion, we leverage context-aware local object motion modelling to study motion patterns and enhance the feature representation for individual objects. Experimental results on the benchmark MOT17 dataset show that our proposed method can surpass the state-of-the-art Trackformer [21] by 1.8% on IDF1 and 21.7% on ID Switches under public detection settings.",
        "paperId": "0e20859b6f937f5e30b36c01116df5a8e7be2828"
    },
    {
        "title": "POSSIBILITIES OF USING THEORY AND TECHNOLOGIES CONTEXT LEARNING IN LANGUAGE PREPARATION FUTURE LAW ENFORCEMENT AGENTS",
        "firstAuthor": "O. Mamonova",
        "url": null,
        "dateSubmitted": "2008-12-15",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This article is devoted to the peculiarities of theory and technologies of contextual teaching, possibilities of their introduction into the process of training and upbringing in a higher educational establishment of MIA of Ukraine. Also some experience of teaching Ukrainian and foreign languages for future law enforcers according to the contextual model of teaching is given; it helps to improve the quality of their training and upbringing.",
        "paperId": "0e487912d36c66d6ffd0ea791f5fd371994b47b7"
    },
    {
        "title": "Compression of Clustered Ship Trajectories for Context Learning and Anomaly Detection",
        "firstAuthor": "David S\u00e1nchez Pedroche",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "0e542cbe888195ec1c20d26f421479312ed64144"
    },
    {
        "title": "An analysis of learning analytics in personalised learning",
        "firstAuthor": "B. Wong",
        "url": null,
        "dateSubmitted": "2022-06-04",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "0e79080bf48c1833de7bf5a54417d396e0e68129"
    },
    {
        "title": "A Theory of Emergent In-Context Learning as Implicit Structure Induction",
        "firstAuthor": "Michael Hahn",
        "url": "http://arxiv.org/pdf/2303.07971",
        "dateSubmitted": "2023-03-14",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Scaling large language models (LLMs) leads to an emergent capacity to learn in-context from example demonstrations. Despite progress, theoretical understanding of this phenomenon remains limited. We argue that in-context learning relies on recombination of compositional operations found in natural language data. We derive an information-theoretic bound showing how in-context learning abilities arise from generic next-token prediction when the pretraining distribution has sufficient amounts of compositional structure, under linguistically motivated assumptions. A second bound provides a theoretical justification for the empirical success of prompting LLMs to output intermediate steps towards an answer. To validate theoretical predictions, we introduce a controlled setup for inducing in-context learning; unlike previous approaches, it accounts for the compositional nature of language. Trained transformers can perform in-context learning for a range of tasks, in a manner consistent with the theoretical results. Mirroring real-world LLMs in a miniature setup, in-context learning emerges when scaling parameters and data, and models perform better when prompted to output intermediate steps. Probing shows that in-context learning is supported by a representation of the input's compositional structure. Taken together, these results provide a step towards theoretical understanding of emergent behavior in large language models.",
        "paperId": "0ea7fc93d4947d9024ccaa202987a2070683bc1f"
    },
    {
        "title": "Strengthening Content With Context: Experience With Alternative Ways of Learning",
        "firstAuthor": "P. Cunha",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "As the social, political, economical and technical transformations of our world uncover limitations of traditional lecture-based higher education, there is a rising interest in ways of learning that break away from the delivery of information paradigm towards knowledge creation approaches capable of fostering self-learning, responsibility, relational skills, communication, activity and team work. This search lets us revisit approaches that were being used in more specific contexts but that now fit the requirements of a broader reality. Learning contracts and portfolios are two such examples. We report on the use we have made of these learning instruments over the past four years, across dissimilar courses: two B.Sc. and one M.Sc. \u2013 one on advanced IT, another on management, and another that was research-oriented. Different numbers of students were involved in each course, and we have achieved different levels of success. We provide insights from our experience and discuss merits and problems of applying these approaches, persuaded that readers wishing to venture into these avenues may share the rewards and avoid the pitfalls. Index Terms Content, Context, Learning Contracts, Portfolios.",
        "paperId": "0ea8ffd31422c3d9e0b21d1e20ceb199450f19b3"
    },
    {
        "title": "Object tracking via Spatio-Temporal Context learning based on multi-feature fusion in stationary scene",
        "firstAuthor": "Yunfei Cheng",
        "url": null,
        "dateSubmitted": "2017-10-24",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "A robust algorithm is proposed for tracking object in dynamic challenges including illumination change, pose variation, and occlusion in stationary scene. To cope with these factors, the Spatio-Temporal Context learning based on Multifeature (MSTC) is integrated within a fusion framework. Different from the original Spatio-Temporal Context learning (STC) algorithm which exploits the low-level features (i.e. image intensity and position) from the target and its surrounding regions, our approach utilize the high-level features like Histogram of Oriented Gradient (HOG) and low-level features for tracker interaction and selection for robust tracking performance in decision level. Experimental results on benchmark datasets demonstrate that the proposed algorithm performs robustly and favorably against the original algorithm.",
        "paperId": "0eb2745f9675a52e35d011028cd0f719bf656be1"
    },
    {
        "title": "The positive impacts of interactive whiteboards on student learning outcomes in FE colleges, and the conditions under which outcomes can be maximised.",
        "firstAuthor": "Bronwen Maxwell",
        "url": null,
        "dateSubmitted": "2007-09-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This paper draws from a wider study on the use and impact of ICT within FE colleges. The research questions addressed are: what is it about the ways interactive whiteboards (iWBs) are being used that produce positive impacts on student outcomes, and what institutional and personal factors determine which teachers use iWBs effectively? Multiple case-studies of 6 colleges were designed using a new framework for classifying e-learning uses (ELUs) according to the learning context, learning objectives and the types of software and activities being used. Tutors\u2019 beliefs in the efficacy of iWB use, their intentions for use, teaching style and pedagogical skills, and the subject taught all affected the ways in which iWB were deployed, and in particular the degree of multimedia and pedagogic interactivity. Tutors who made a lot of use of iWBs were in colleges where the leadership vision prioritised ICT within teaching and learning. The strongest impact on student outcomes occurred where iWBs were used in a variety of ways, use was appropriate for the subject, and congruent with the teachers' purposes and intentions for students' learning. Tutors who made little use of iWBs tended to be in colleges where the emphasis on management of learning was stronger than on supporting pedagogic development, and/or they were unaware of the potential of iWBs particularly in relation to their subject.",
        "paperId": "0ed611112f7645266f6cdcbfd212428101e84e76"
    },
    {
        "title": "Visual Context Learning with Big Data Analytics",
        "firstAuthor": "Chandrashekar Mayanka",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "0ed63405d688a8eb87c48c88da319c0a2887761d"
    },
    {
        "title": "Conclusion & Outlook",
        "firstAuthor": "Tobias Schwandt",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "0edced0ad38108db16d6e8a5b1cd29d3a576dd13"
    },
    {
        "title": "Parallel Context Windows Improve In-Context Learning of Large Language Models",
        "firstAuthor": "Nir Ratner",
        "url": "https://arxiv.org/pdf/2212.10947",
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "For applications that require processing large amounts of text at inference time, Large Language Models (LLMs) are handicapped by their limited context windows, which are typically 2048 tokens. In-context learning, an emergent phenomenon in LLMs in sizes above a certain parameter threshold, constitutes one signi\ufb01cant example because it can only leverage training examples that \ufb01t into the context window. Existing efforts to address the context window limitation involve training specialized architectures, which tend to be smaller than the sizes in which in-context learning manifests due to the memory footprint of processing long texts. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training . The key to the approach is to carve a long context into chunks (\u201cwindows\u201d) that \ufb01t within the architecture, re-strict the attention mechanism to apply only within each window, and re-use the positional embeddings among the windows. We test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substan-tial improvements for tasks with diverse input and output spaces. Our results motivate further investigation of Parallel Context Windows as a method for applying off-the-shelf LLMs in other settings that require long text sequences.",
        "paperId": "0eedbc38bc215fdbe4e5bcde8aeac08fb3ce9f44"
    },
    {
        "title": "A Framework for Studying the Learning Outcomes of Chinese Outbound Group Tourists",
        "firstAuthor": "P. Pearce",
        "url": null,
        "dateSubmitted": "2011-10-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This article uses existing studies to build a rich framework that depicts the learning outcomes of Chinese outbound group tourists.Key themes used to build the framework include perspectives on learning from literature in psychology, previous research on traveler learning in Western contexts, learning through traveling in Chinese culture, and key characteristics of the Chinese outbound travel market.Influential determining forces are identified and potential learning is depicted as (a) no learning, (b) orientation awareness,(c) the acquisition of basic facts, (d) the enhancement of skills, and (e) the learning of values, new views, and reflections on Chinese society. It is proposed that there are variable patterns of learning outcomes among tourists depending on the mix of forces shaping their individual circumstances and travel experiences.",
        "paperId": "0ef8f8079acfdd7018fe5cba810bb31e003ab56c"
    },
    {
        "title": "Promoting LGBT Equality in the EFL classroom",
        "firstAuthor": "M. Gallardo",
        "url": null,
        "dateSubmitted": "2018-06-18",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This paper defends the importance of introducing and dealing with LGBT (Lesbian, Gay, Bisexual and Transexual) issues during an EFL (English as a Foreign Language) classroom. Such issues are key in promoting the value of equality within a language education context. Learning and teaching English is essential for the European Union's structure, and for this reason the classes are full of multicultural backgrounds. This dissertation will introduce the groundbreaking innovative inclusion of sexual and gender identity issues whilst learning a foreign language in its theoretical aspect, and also exposing a teaching proposal that allows for this connection between the English language education to take place in the most natural way. The main focus is promoting the equality for the LGBT community, therefore reducing the discrimination based on sexual orientation or gender identity that is such a serious issue for LGBT youth.",
        "paperId": "0efe3ffd8180e3c5fc9b7f72cd65f09d256b0431"
    },
    {
        "title": "Long-term retention of vocabulary after keyword and context learning.",
        "firstAuthor": "M. McDaniel",
        "url": null,
        "dateSubmitted": "1987-03-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "0f203bbe65e71e3e6764e9fb5fbccc8c24f97026"
    },
    {
        "title": "Cross-border alliances and\u00a0strategic games",
        "firstAuthor": "I. Tlemsani",
        "url": "https://www.emerald.com/insight/content/doi/10.1108/JWAM-04-2023-0034/full/pdf?title=cross-border-alliances-and-strategic-games",
        "dateSubmitted": "2023-07-11",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "PurposeThis empirical research examined the factors and conditions that contribute to the success of international strategic learning alliances. The study aimed to provide organisations with evidence-based insights and recommendations that can help them to create more effective and sustainable partnerships and to leverage collaborative learning to drive innovation and growth. The examination is performed using game theory as a mathematical framework to analyse the interaction of the decision-makers, where one alliance's decision is contingent on the decision made by others in the partnership. There are 20 possible games out of 120 outcomes that can be grouped into four different types; each type has been divided into several categories.Design/methodology/approachThe research methodology included secondary and primary data collection using empirical data, the Delphi technique for obtaining qualitative data, a research questionnaire for collecting quantitative data and computer simulation (1,000 cases, network resources and cooperative game theory). The key variables collected and measured when analysing a strategic alliance were identified, grouped and mapped into the developed model.FindingsMost respondents ranked reputation and mutual benefits in Type 1 games relatively high, averaging 4.1 and 3.85 of a possible 5. That is significantly higher than net transfer benefits, ranked at 0.61. The a priori model demonstrate that Type 1 games are the most used in cooperative games and in-game distribution, 40% of all four types of games. This is also confirmed by the random landscape model, approximately 50%. The results of the empirical data in a combination of payoff characteristics for Type 1 games show that joint and reputation benefits are critical for the success of cooperation.Practical implicationsResearch on cross-border learning alliances has several implications. Managerial implications can help managers to understand the challenges and benefits of engaging in these activities. They can use this knowledge to develop strategies to improve the effectiveness of their cross-border learning alliances. Practical implications, the development of game theory and cross-border models can be applied in effective decision-making in a variety of complex contexts. Learning alliances have important policy implications, particularly in trade, investment and innovation. Policymakers must consider the potential benefits and risks of these collaborations and develop policies that encourage and support them while mitigating potential negative impacts.Originality/valueInternational learning alliances have become a popular strategy for firms seeking to gain access to new knowledge, capabilities and markets in foreign countries. The originality of this research lies in its ability to contribute to the understanding of the dynamics and outcomes of these complex relationships in a novel and meaningful way.",
        "paperId": "0f3955b4e753c5acf67afb0d93fdac8c6eb78cd7"
    },
    {
        "title": "Are Human-generated Demonstrations Necessary for In-context Learning?",
        "firstAuthor": "Rui Li",
        "url": "https://arxiv.org/pdf/2309.14681",
        "dateSubmitted": "2023-09-26",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understanding, and code generation benchmarks, show that SEC, which does not require hand-crafted demonstrations, significantly outperforms the zero-shot learning strategy, and achieves comparable results to ICL with hand-crafted demonstrations. This demonstrates that, for many tasks, contemporary LLMs possess a sufficient level of competence to exclusively depend on their own capacity for decision making, removing the need for external training data. Code is available at https://github.com/ruili33/SEC.",
        "paperId": "0f45608ddc01b3e192f3490330f4c4b8de074f79"
    },
    {
        "title": "Honest Students from Untrusted Teachers: Learning an Interpretable Question-Answering Pipeline from a Pretrained Language Model",
        "firstAuthor": "Jacob Eisenstein",
        "url": "http://arxiv.org/pdf/2210.02498",
        "dateSubmitted": "2022-10-05",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Explainable question answering systems should produce not only accurate answers but also rationales that justify their reasoning and allow humans to check their work. But what sorts of rationales are useful and how can we train systems to produce them? We propose a new style of rationale for open-book question answering, called \\emph{markup-and-mask}, which combines aspects of extractive and free-text explanations. In the markup phase, the passage is augmented with free-text markup that enables each sentence to stand on its own outside the discourse context. In the masking phase, a sub-span of the marked-up passage is selected. To train a system to produce markup-and-mask rationales without annotations, we leverage in-context learning. Specifically, we generate silver annotated data by sending a series of prompts to a frozen pretrained language model, which acts as a teacher. We then fine-tune a smaller student model by training on the subset of rationales that led to correct answers. The student is\"honest\"in the sense that it is a pipeline: the rationale acts as a bottleneck between the passage and the answer, while the\"untrusted\"teacher operates under no such constraints. Thus, we offer a new way to build trustworthy pipeline systems from a combination of end-task annotations and frozen pretrained language models.",
        "paperId": "0f4ab3fe492ececbfd38be9682047371e2e9b8c6"
    },
    {
        "title": "Development of a contextualised MALL research framework based on L2 Chinese empirical study",
        "firstAuthor": "Anat Cohen",
        "url": null,
        "dateSubmitted": "2018-03-31",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "ABSTRACT Contextualised mobile assisted language learning (MALL) has been known for its potential in language learning rooted in social constructivism theories. However, a consistent approach to constituents of contextualised MALL, in addition to a quantitative tool to evaluate it, is missing in reported case studies. The present research, drawing upon literature context definitions, presents a research framework for analysing, designing and evaluating contextualised MALL. Real world and real life context variables were analysed and the potential influence of the country (target and non-target) and the mobile activities language learning orientation (generic/dedicated) on contextualised MALL was suggested. Empirical data collected from 53 L2 Chinese students in Taiwan and Israel, encompassing 296 types of MALL activities performed by students, was used to develop the contextualised MALL model as well as an index for measuring real world and real life context learning. This measuring index was subsequently established in a combined top-down and bottom-up process, using pre-defined context literature augmented with students\u2019 stories. At the end of the research procedure a quantitatively operative evaluation tool was developed. Real world was measured by the amount of content activity in relation to the place, typical or non-typical objects of the place and typical situations at the place. Real life was measured by the degree to which other tools assisted in other core activities whose purpose was not learning. The paper presents the research framework, the developed model and index with examples illustrating their application. They are offered for the research and practice community to use.",
        "paperId": "0f9611ade60190750aefb5c43dcd0bacbe076dc3"
    },
    {
        "title": "Active Inference: A Process Theory",
        "firstAuthor": "Karl J. Friston",
        "url": "https://openaccess.city.ac.uk/id/eprint/16683/1/NECO_a_00912.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This article describes a process theory based on active inference and belief propagation. Starting from the premise that all neuronal processing (and action selection) can be explained by maximizing Bayesian model evidence\u2014or minimizing variational free energy\u2014we ask whether neuronal responses can be described as a gradient descent on variational free energy. Using a standard (Markov decision process) generative model, we derive the neuronal dynamics implicit in this description and reproduce a remarkable range of well-characterized neuronal phenomena. These include repetition suppression, mismatch negativity, violation responses, place-cell activity, phase precession, theta sequences, theta-gamma coupling, evidence accumulation, race-to-bound dynamics, and transfer of dopamine responses. Furthermore, the (approximately Bayes\u2019 optimal) behavior prescribed by these dynamics has a degree of face validity, providing a formal explanation for reward seeking, context learning, and epistemic foraging. Technically, the fact that a gradient descent appears to be a valid description of neuronal activity means that variational free energy is a Lyapunov function for neuronal dynamics, which therefore conform to Hamilton\u2019s principle of least action.",
        "paperId": "0fdd66fcf6b9bc04428e337df66fd510a659eb2f"
    },
    {
        "title": "Gamified Learning Experiences",
        "firstAuthor": "Ole Goethe",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "0ffb067d41623d481a7162c8cb0cb20804d0fd21"
    },
    {
        "title": "Resilience in a Protected Area: Prospects for Fathom Five National Marine Park, Lake Huron, Canada",
        "firstAuthor": "S. Parker",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Introduction Building or maintaining resilience within a protected area is increasingly cited as a means to achieve long-term conservation goals in the face of climate change and other human impacts (e.though there is an established body of ecological and social\u2013ecological knowledge related to resilience concepts, in application it is still conceptually and methodologically early in its development. Within this paper, we explore the applicability of a resilience-based approach to planning and management by using Fathom Five National Marine Park as a study area. Resilience is a system property that describes the capacity to cope with disturbance and remain within the same regime, essentially retaining defining structures, functions, and feedbacks (Walker and Salt 2012). Furthermore, to support resilience in a protected area context, learning, cross-scale linkages, and adaptability are needed (Berkes et al. 2003; Fazey et al. 2007; Francis 2008). Resilient systems are more diverse, flexible, and prepared for change and uncertainty (Hughes et al. 2005). Resilience is founded on non-equilibrium dynamics, where systems can transition to alternate states and where system behavior and progression is described within an adaptive cycle involving phases of collapse, renewal, growth, and conservation (Holling and Gunderson 2002). Whereas a traditional management approach may focus on maintaining historic conditions (e.g., composition and abundance of native species) or promoting system efficiency (e.g., maximum sustainable yield, single stable state), a resilience-based approach focuses more on the desired system regime and maintaining functional and response diversity (Table 1 and Text Box 1) (Folke et al. 2004; Chapin et al. 2010). Resilience itself is neither inherently good nor bad. As noted by those studying degraded systems, being locked in an undesirable state due to high resilience would be perceived as bad (Carpenter et al. 2001). Thus, in managing for resilience there rests a",
        "paperId": "1017e8a66a0adac91553bea6c372a0fcb6b6b382"
    },
    {
        "title": "The Effect of Thinking Actively in a Social Context and Creative Problem-Solving Learning Models on Divergent-Thinking Skills Viewed from Adversity Quotient",
        "firstAuthor": "Muna Fauziah",
        "url": "https://www.eu-jer.com/core.php?ajax=count&link=EU-JER_9_2_537.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This research aims to find out: (1) the more effective learning model on students' divergent-thinking skills; (2) the better adversity quotient on students' divergent-thinking skills; (3) the better adversity quotient to improve students' divergent-thinking skills in each learning model; and (4) the better learning model to improve students' divergent-thinking skills in each adversity quotient. This research uses a quantitative approach with a quasi-experimental type. The fifth-grade students were selected as the research subjects. This research was carried out at the public elementary schools in Laweyan District, Surakarta, Indonesia. Test and questionnaire techniques were used to collect data. The data analysis was performed with the analysis prerequisite, hypothesis, and multiple-comparison tests. The results showed that the learning model and adversity quotient have an influence on divergentthinking skills; for each adversity quotient, the thinking actively in a social context learning model is better than the creative problem solving and direct instruction learning model; the creative problem solving learning model is better than the direct instruction learning model; and adversity quotient of the climbers is better than that of the campers and the adversity quotient of the campers is better than that of the quitters in each learning model.",
        "paperId": "101b5c30706f1caebad38bd7cb8e03013eddd220"
    },
    {
        "title": "Collaborating with language models for embodied reasoning",
        "firstAuthor": "Ishita Dasgupta",
        "url": "http://arxiv.org/pdf/2302.00763",
        "dateSubmitted": "2023-02-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.",
        "paperId": "102e4c860e39a2bfd7bf3f03b9ad69aac7bf3b5f"
    },
    {
        "title": "Contextual Attention Network for Emotional Video Captioning",
        "firstAuthor": "Peipei Song",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This paper investigates an emerging and challenging task\u2014emotional video captioning. Formally, given a video, the task aims to not only describe the factual content of the video, but also discover the emotional clues in the video. We propose a novel Contextual Attention Network (CANet), which recognizes and describes the fact and emotion in the video by semantic-rich context learning. To be specific, at each time step, we first extract visual and textual features from both input video and previously generated words. Then, we apply the attention mechanism to these features to capture informative contexts for captioning. We train the CANet model with the joint optimization of cross-entropy loss <inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {L}_{CE}$</tex-math></inline-formula> and contrastive loss <inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {L}_{CL}$</tex-math></inline-formula>, where <inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {L}_{CE}$</tex-math></inline-formula> constrains the semantics of the generated sentence to be close to human annotation and <inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {L}_{CL}$</tex-math></inline-formula> encourages discriminative representation learning from positive and negative pairs of video and caption. Experiments on two emotional video captioning datasets (i.e., EmVidCap and EmVidCap-S) demonstrate the superiority of CANet compared to the state-of-the-art approaches.",
        "paperId": "1044f72ca15ef8c20eb676cdd4fd88957d59d637"
    },
    {
        "title": "Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering",
        "firstAuthor": "Keheng Wang",
        "url": "https://arxiv.org/pdf/2308.13259",
        "dateSubmitted": "2023-08-25",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown impressive reasoning ability in various downstream tasks. Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA. To alleviate this issue, we propose a framework called Knowledge-Driven Chain-of-Thought (KD-CoT) to verify and modify reasoning traces in CoT via interaction with external knowledge, and thus overcome the hallucinations and error propagation. Concretely, we formulate the CoT rationale process of LLMs into a structured multi-round QA format. In each round, LLMs interact with a QA system that retrieves external knowledge and produce faithful reasoning traces based on retrieved precise answers. The structured CoT reasoning of LLMs is facilitated by our developed KBQA CoT collection, which serves as in-context learning demonstrations and can also be utilized as feedback augmentation to train a robust retriever. Extensive experiments on WebQSP and ComplexWebQuestion datasets demonstrate the effectiveness of proposed KD-CoT in task-solving reasoning generation, which outperforms the vanilla CoT ICL with an absolute success rate of 8.0% and 5.1%. Furthermore, our proposed feedback-augmented retriever outperforms the state-of-the-art baselines for retrieving knowledge, achieving significant improvement in Hit and recall performance. Our code and data are released on https://github.com/AdelWang/KD-CoT/tree/main.",
        "paperId": "10955e63aa49fab146267949f8ebc9ebe8275183"
    },
    {
        "title": "Head gesture recognition in intelligent interfaces: the role of context in improving recognition",
        "firstAuthor": "Louis-Philippe Morency",
        "url": null,
        "dateSubmitted": "2006-01-29",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Acknowledging an interruption with a nod of the head is a natural and intuitive communication gesture which can be performed without significantly disturbing a primary interface activity. In this paper we describe vision-based head gesture recognition techniques and their use for common user interface commands. We explore two prototype perceptual interface components which use detected head gestures for dialog box confirmation and document browsing, respectively. Tracking is performed using stereo-based alignment, and recognition proceeds using a trained discriminative classifier. An additional context learning component is described, which exploits interface context to obtain robust performance. User studies with prototype recognition components indicate quantitative and qualitative benefits of gesture-based confirmation over conventional alternatives.",
        "paperId": "10aa1fe216a3af33bcaa9c4a349294db4a7db2b3"
    },
    {
        "title": "An Explanation of In-context Learning as Implicit Bayesian Inference",
        "firstAuthor": "Sang Michael Xie",
        "url": null,
        "dateSubmitted": "2021-11-03",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.",
        "paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90"
    },
    {
        "title": "Dinamika Metode Pembelajaran Bahasa Arab",
        "firstAuthor": "Mustafa A. Mustafa",
        "url": null,
        "dateSubmitted": "2021-01-05",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The method is one of the very important components of learning Arabic. It cannot be denied that the existence of methods greatly influences learning. One of the success factors of a learning process is a good method. The method is certainly supported by the competence of an educator in carrying it out. This paper aims to convey the dynamics of learning methods in a comprehensive manner. Some classical and modern methods that cannot be separated from learning Arabic, that is grammar and translation method, direct method, reading method, audio-lingual method. All of these methods are still quite relevant to learning Arabic in the contemporary context. Learning Arabic cannot be separated from the translation process with the appropriate language rules. The same is true for the direct method, reading dan practice of listening and speaking the language.",
        "paperId": "10da8835588d0a176b3fc04be677d231eba81498"
    },
    {
        "title": "Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning",
        "firstAuthor": "Namrata Shivagunde",
        "url": "http://arxiv.org/pdf/2303.16445",
        "dateSubmitted": "2023-03-29",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Language model probing is often used to test specific capabilities of models. However, conclusions from such studies may be limited when the probing benchmarks are small and lack statistical power. In this work, we introduce new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500) inspired by psycholinguistic studies. We dramatically extend existing NEG-136 and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44 sentence pairs to 750 each. We also create another version of extended negation dataset (NEG-1500-SIMP-TEMP), created using template-based generation. It consists of 770 sentence pairs. We evaluate 22 models on the extended datasets, seeing model performance dip 20-57% compared to the original smaller benchmarks. We observe high levels of negation sensitivity in models like BERT and ALBERT demonstrating that previous findings might have been skewed due to smaller test sets. Finally, we observe that while GPT3 has generated all the examples in ROLE-1500 is only able to solve 24.6% of them during probing. The datasets and code are available on $\\href{https://github.com/text-machine-lab/extending_psycholinguistic_dataset}{Github}$.",
        "paperId": "10e5d2f2e8a59686567ae0d2d0ce741cea9e9ea3"
    },
    {
        "title": "Content and context in knowledge production: a critical review of doctoral supervision literature",
        "firstAuthor": "Wendy Bastalich",
        "url": null,
        "dateSubmitted": "2017-07-03",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "With the massification of higher degrees, the efficiency gaze has fixed on students and supervisors, or on their relationship, as the \u2018problem\u2019 to be managed, in need of administrative regulation, skill improvement or perhaps emotional management. This critical review of a selection of higher education journal articles on doctoral supervision published in the past 20 years within the UK, Australia, Sweden and the Netherlands aims to summarise what we have learnt about \u2018the problem of supervision\u2019 to date, and to suggest possible ways forward in light of this within the changing doctoral education climate. The review observes four distinct conceptual frames that prescribe how research education is thought in these contexts, each taking in a specific understanding of what constitutes \u2018good supervision\u2019, with implicit relations drawn between academics, doctoral candidates, academic developers and government. The review highlights the importance of the challenge mounted to the conception of supervisors as distant masters with sole responsibility for research outcomes. At the same time, the article argues that a de-contextualised, psychological lens dominates educational thought about research education and innovation, pointing to the need for a greater emphasis on content and context learning within future research and practice around doctoral education.",
        "paperId": "10f3c5e344deb338a06e7520b9d2d0d12f66f4c8"
    },
    {
        "title": "In-Context Learning with Iterative Demonstration Selection",
        "firstAuthor": "Chengwei Qin",
        "url": null,
        "dateSubmitted": "2023-10-15",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Leveraging the merits of both dimensions, we propose Iterative Demonstration Selection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is accompanied by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including commonsense reasoning, question answering, topic classification, and sentiment analysis, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.",
        "paperId": "112597088d25b21b3ad2d77f107b816e6b5bb36c"
    },
    {
        "title": "Learning to Teach in post-apartheid South Africa - 'Student Teachers' Encounters with Initial Teacher Education",
        "firstAuthor": "Y. Sayed",
        "url": null,
        "dateSubmitted": "2018-11-30",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Teacher education programmes seek to provide student teachers with the knowledge and expertise to provide qualtiy teaching and learning in a diverse and challenging school context. Learning to Teach in post-apartheid South Africa: Student Teachers' Encounters with Initial Teacher Education addresses the complexities of teacher education programmes in preparing students to teach. It adds to the knowledge about teacher education, contributing critical understanding of education and the schooling system. The book provides important insights to deepen researchers, academics, teacher education providers, policy-makers, and students' understanding of the importance to address equity, redress, and quality in South African educaiton in a post-apartheid era. This book further helps to build student teachers' capacities to work creatively and to become active and critical agents of transformation. It ultimately outlines the challenges face in designing and delivering successful Inital Teacher Education programmes, and the impact this has on delivering equitable and qualtiy education.",
        "paperId": "11373aad931e46c7261edd439909900a106a3c62"
    },
    {
        "title": "Study on the interpretability of poetic expressions in ChatGPT through in-context learning: Focusing on Fable Poetry in Yoon Gon-gang\u2019s \u201cAnimal Poems\u201d",
        "firstAuthor": "Taehyeong Kim",
        "url": null,
        "dateSubmitted": "2023-08-31",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In 2023, as artificial intelligence(AI) moves beyond the realm of imitation and into the realm of creation, Korean literature is also entering the era of \u201ccreative AI,\u201d with Slitscope\u2019s SIA releasing a book of poetry, \u201c\uc2dc\ub97c \uc4f0\ub294 \uc774\uc720(Why I Write Poetry).\u201d However, the time and cost required to reach such a level is quite burdensome for the general public to attempt. \nTherefore, this study focuses on in-context learning of conversational A.I. that has already been trained through deep learning to explore the possibility of learning about A.I.\u2019s allegory. In particular, by using ChatGPT as a research object, we can gain research advantages in terms of accessibility and functionality over previous studies on the model. \nIn order to study the learning potential of ChatGPT in an in-context learning environment, this study consists of a structure of (1) pre-learning verification, (2) output request based on pre-learning, (3) related information input, and (4) verification re-execution of (2) after learning, and examines the possibility of conversational AI learning about poetic alliteration. \nThe poetry texts that are input as learning information in the above process are allegorical poetry texts extracted from Yoon Gon-gang\u2019s \u300e\u52d5\u7269\u8a69\u96c6\u300f(Hansung Book Co., 1938) because it was judged to have value as the first Korean poetry collection with a single animal theme, which was approached and interpreted from the perspective of satirical and allegorical poetry at the time and even now. \nThis study determines the learning potential of literary rhetoric for ChatGPT through the education and output process based on Yoon Gon-kang\u2019s poems, and considers its utilization in various aspects such as education, creation, and education in the future. This will be an exploration of possibilities to solidify the position of literature in the era of artificial intelligence that will become a reality in the future.",
        "paperId": "1181c2409935ddb84fcd9cf836dc7bb5c149c9ab"
    },
    {
        "title": "A Serious Game to Self-Regulate Heart Rate Variability as a Technique to Manage Arousal Level Through Cardiorespiratory Biofeedback: Development and Pilot Evaluation Study",
        "firstAuthor": "Tony Estrella",
        "url": "https://games.jmir.org/2023/1/e46351/PDF",
        "dateSubmitted": "2023-02-10",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Background Heart rate variability biofeedback (HRVB) is an established intervention for increasing heart rate variability (HRV) in the clinical context. Using this technique, participants become aware of their HRV through real-time feedback and can self-regulate it. Objective The aim of this study was 2-fold: first, to develop a serious game that applies the HRVB technique to teach participants to self-regulate HRV and, second, to test the app with participants in a pilot study. Methods An HRVB app called the FitLab Game was developed for this study. To play the game, users must move the main character up and down the screen, avoiding collisions with obstacles. The wavelength that users must follow to avoid these obstacles is based on the user\u2019s basal heart rate and changes in instantaneous heart rate. To test the FitLab Game, a total of 16 participants (mean age 23, SD 0.69 years) were divided into a control group (n=8) and an experimental group (n=8). A 2 \u00d7 2 factorial design was used in each session. Participants in the experimental condition were trained in breathing techniques. Results Changes in the frequency and time domain parameters of HRV and the game\u2019s performance features were evaluated. Significant changes in the average RR intervals and root mean square of differences between adjacent RR intervals (RMSSD) were found between the groups (P=.02 and P=.04, respectively). Regarding performance, both groups showed a tendency to increase the evaluated outcomes from baseline to the test condition. Conclusions The results may indicate that playing different levels leads to an improvement in the game\u2019s final score by repeated training. The tendency of changes in HRV may reflect a higher activation of the mental system of attention and control in the experimental group versus the control group. In this context, learning simple, voluntary strategies through a serious game can aid the improvement of self-control and arousal management. The FitLab Game appears to be a promising serious game owing to its ease of use, high engagement, and enjoyability provided by the instantaneous feedback.",
        "paperId": "121cccf3d509566ea5d7bcea3b600b787eb2938d"
    },
    {
        "title": "\u0423\u0447\u0438\u0442\u0435\u043b\u044c \u0432 \u0441\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u043e-\u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u043d\u043e\u043c \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0448\u043a\u043e\u043b\u044b",
        "firstAuthor": "\u041c\u0430\u0447\u0435\u0445\u0438\u043d\u0430 \u041e\u043b\u044c\u0433\u0430 \u041d\u0438\u043a\u043e\u043b\u0430\u0435\u0432\u043d\u0430",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The article deals with the problems of organization of social and pedagogical interaction of the subjects of educational process. It defines the category of social context space of modern school, and the conditions for its development, among which a special role belongs to context-oriented educational technologies considered by the author from the standpoint of the context learning theory",
        "paperId": "12920752c39a4868ddc976a8e19ee5bbcb092740"
    },
    {
        "title": "Effects of keyword on long-term retention: Help or hindrance?",
        "firstAuthor": "Alvin Y. Wang",
        "url": null,
        "dateSubmitted": "1995-09-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Three experiments assessed the long-term effectiveness of the keyword mnemonic relative to a nonmnemonic (i.e., semantic-context) learning strategy. Following incidental-learning instructions, cued recall was assessed either immediately or after a 2-day delay. The keyword mnemonic produced superior immediate performance relative to the semantic-context strategy. However, after 2 days, there was a marked reversal in performance, with higher levels of delayed recall associated with semantic-context learning. This pattern of findings was obtained when obscure English words (Experiment 1) and second-language vocabulary (Experiment 2) were the learning stimuli. When practice frequencies were manipulated (Experiment 3), increased opportunities for study were more likely to boost the long-term retention of keyword learners compared with semantic-context learners. The implication is that keyword-based memories are especially fragile over time and will benefit from repeated testing and rehearsal.",
        "paperId": "129ed0d609f2b5d905dedd089c088e35fac633a5"
    },
    {
        "title": "Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous Mixture of LMs",
        "firstAuthor": "Young-Suk Lee",
        "url": null,
        "dateSubmitted": "2023-10-21",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Using in-context learning (ICL) for data generation, techniques such as Self-Instruct (Wang et al., 2023) or the follow-up Alpaca (Taori et al., 2023) can train strong conversational agents with only a small amount of human supervision. One limitation of these approaches is that they resort to very large language models (around 175B parameters) that are also proprietary and non-public. Here we explore the application of such techniques to language models that are much smaller (around 10B--40B parameters) and have permissive licenses. We find the Self-Instruct approach to be less effective at these sizes and propose new ICL methods that draw on two main ideas: (a) Categorization and simplification of the ICL templates to make prompt learning easier for the LM, and (b) Ensembling over multiple LM outputs to help select high-quality synthetic examples. Our algorithm leverages the 175 Self-Instruct seed tasks and employs separate pipelines for instructions that require an input and instructions that do not. Empirical investigations with different LMs show that: (1) Our proposed method yields higher-quality instruction tuning data than Self-Instruct, (2) It improves performances of both vanilla and instruction-tuned LMs by significant margins, and (3) Smaller instruction-tuned LMs generate more useful outputs than their larger un-tuned counterparts. Our codebase is available at https://github.com/IBM/ensemble-instruct.",
        "paperId": "12aa2b1e9556c20752e37e8b18d0e396c0cea1c5"
    },
    {
        "title": "Learning Applications based on Semantic Web Technologies",
        "firstAuthor": "M. Palm\u00e9r",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The interplay between learning and technology is a growing field that is often referred to as Technology Enhanced Learning (TEL). Within this context, learning applications are software components ...",
        "paperId": "12ac19445eca7cd376a11e5099bf8bf2aa991e82"
    },
    {
        "title": "Digital Natives\u2019 Use of Web 2.0 Tools in Learning Foreign Language: A Case Study",
        "firstAuthor": "H. Bozna",
        "url": null,
        "dateSubmitted": "2020-06-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In the information age, depending on the ubiquitousness of information and digitalization, learners\u2019 learning methods and approaches have changed rapidly and profoundly. Web 2.0 tools and recent technologies have facilitated people\u2019s lives as well as their teaching and learning environments. The generation called \u201cdigital natives\u201d live addictively to Web 2.0 and digital media tools. This generation with a perfect command of Web 2.0 tools can reach boundless information and interact with people around the world. In this context, learning a foreign language has become vital for communication and a common language (lingua franca) has become indispensable in this globalized world. Widespread use of Web 2.0 tools in foreign language learning enables both learners and teachers to interact and access information in a short time in and out of class. Accordingly, this study aims to determine digital natives\u2019 levels of using Web 2.0 tools in learning foreign languages under the Connectivism Theory and Cognitive Theory of Multimedia. It\u2019s anticipated that the findings of the research will enable both face to face and distance education-based institutions to learn more about digital natives and their learning styles. In this case study, data collection was completed through semi-structured oral interviews, observations, and document analysis. Parallel with the information in the literature review; the results of this study show that digital natives use Web 2.0 tools quite often and they are ambitious and practical in generating content and sharing their contents via connections.\ufeff",
        "paperId": "12de86c9f537782f27464a79219434eed23a347f"
    },
    {
        "title": "Taken out of context: On measuring situational awareness in LLMs",
        "firstAuthor": "Lukas Berglund",
        "url": "https://arxiv.org/pdf/2309.00667",
        "dateSubmitted": "2023-09-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "We aim to better understand the emergence of `situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations. At test time, we assess whether the model can pass the test. To our surprise, we find that LLMs succeed on this out-of-context reasoning task. Their success is sensitive to the training setup and only works when we apply data augmentation. For both GPT-3 and LLaMA-1, performance improves with model size. These findings offer a foundation for further empirical study, towards predicting and potentially controlling the emergence of situational awareness in LLMs. Code is available at: https://github.com/AsaCooperStickland/situational-awareness-evals.",
        "paperId": "135ae2ea7a2c966815e85a232469a0a14b4d8d67"
    },
    {
        "title": "Deer, Dissension, and Dialogue: A University-Community Collaboration in Public Deliberation.",
        "firstAuthor": "Wynne Wright",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Michigan State University embarked upon an initiative to explore deliberative dialogue as a tool for addressing community-based contested issues in agriculture and natural resources. Our goal is to assess the extent to which deliberative dialogue can help \u201cbridge the divides\u201d among citizens and professionals and fulfill the land-grant mission. In this article, I explore the strengths and impediments to this practice by examining the growing\u2014and for many, unwelcome\u2014population of whitetailed deer in an urban community. I discovered that deliberative dialogue can help resolve social tensions and invigorate civic life as people\u2014working in conjunction with communitybased and university professionals\u2014consider complex issues. The three primary lessons drawn from this collaboration focus on the importance of context, learning, and the role of science. This article concludes with a discussion of how this collaborative approach can become part of the culture of university-community relations.",
        "paperId": "1374a88d744df195a2dac24c0f4d728df8ba9dd3"
    },
    {
        "title": "Contrastive Learning for Prompt-based Few-shot Language Learners",
        "firstAuthor": "Yiren Jian",
        "url": "http://arxiv.org/pdf/2205.01308",
        "dateSubmitted": "2022-05-03",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The impressive performance of GPT-3 using natural language prompts and in-context learning has inspired work on better fine-tuning of moderately-sized models under this paradigm. Following this line of work, we present a contrastive learning framework that clusters inputs from the same class for better generality of models trained with only limited examples. Specifically, we propose a supervised contrastive framework that clusters inputs from the same class under different augmented \u201cviews\u201d and repel the ones from different classes. We create different \u201cviews\u201d of an example by appending it with different language prompts and contextual demonstrations. Combining a contrastive loss with the standard masked language modeling (MLM) loss in prompt-based few-shot learners, the experimental results show that our method can improve over the state-of-the-art methods in a diverse set of 15 language tasks. Our framework makes minimal assumptions on the task or the base model, and can be applied to many recent methods with little modification.",
        "paperId": "139bfd354431c4541004c5de5052a63e1f94130e"
    },
    {
        "title": "The use of data analytics technique in learning management system to develop fashion design skills and technology acceptance",
        "firstAuthor": "Abdellah Ibrahim Mohammed Elfeky",
        "url": null,
        "dateSubmitted": "2021-06-24",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "ABSTRACT Data analytics technique in education provides unparalleled opportunities in the era of e-learning platforms to support students. In this context, learning management system (LMS) as a platform allows recording the learner's activity recorded in order to explore data unique types coming from the academic context in order to improve the learning experience. Therefore, this research aims to reveal the impact of using data analytics technique in LMS to develop the fashion design skills and technology acceptance. The research was applied to students at Home Economics Department who were randomly divided into two equal experimental groups. An extended Technology Acceptance Model (TAM) and a developed product evaluation card were utilized as main research instruments. The empirical findings showed that data analytics technique in LMS had significant and positive influence on the enhancement of students' fashion design skills in the functional, esthetic and creative aspects. Findings also revealed robust supporting evidence for the suggested research model as data analytics technique had a major positive direct impact on the perceived usefulness and perceived ease of use of LMS. In addition, it had positive indirect impact on students' attitude and their behavioral intention to use LMS, that mediated by perceived usefulness of LMS.",
        "paperId": "13bf410a450a5520ddae9fb210b5ae4ecaa81fae"
    },
    {
        "title": "Entity Matching using Large Language Models",
        "firstAuthor": "R. Peeters",
        "url": null,
        "dateSubmitted": "2023-10-17",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity. Entity Matching is a central step in most data integration pipelines and an enabler for many e-commerce applications which require to match products offers from different vendors. State-of-the-art entity matching methods often rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using large language models (LLMs) for entity matching as a less domain-specific training data reliant and more robust alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2 which can be run locally. We evaluate these models in a zero-shot scenario as well as a scenario where task-specific training data is available. We compare different prompt designs as well as the prompt sensitivity of the models in the zero-shot scenario. We investigate (i) the selection of in-context demonstrations, (ii) the generation of matching rules, as well as (iii) fine-tuning GPT3.5 in the second scenario using the same pool of training data across the different approaches. Our experiments show that GPT4 without any task-specific training data outperforms fine-tuned PLMs (RoBERTa and Ditto) on three out of five benchmark datasets reaching F1 scores around 90%. The experiments with in-context learning and rule generation show that all models beside of GPT4 benefit from these techniques (on average 5.9% and 2.2% F1), while GPT4 does not need such additional guidance in most cases...",
        "paperId": "13c2ae7831c0f1579bc8c6f1a31c9aa8689e24a8"
    },
    {
        "title": "Deeply Felt Affect: The Emergence of Valence in Deep Active Inference",
        "firstAuthor": "C. Hesp",
        "url": "https://direct.mit.edu/neco/article-pdf/33/2/398/1896849/neco_a_01341.pdf",
        "dateSubmitted": "2019-12-03",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The positive-negative axis of emotional valence has long been recognized as fundamental to adaptive behavior, but its origin and underlying function have largely eluded formal theorizing and computational modeling. Using deep active inference, a hierarchical inference scheme that rests on inverting a model of how sensory data are generated, we develop a principled Bayesian model of emotional valence. This formulation asserts that agents infer their valence state based on the expected precision of their action model\u2014an internal estimate of overall model fitness (\u201csubjective fitness\u201d). This index of subjective fitness can be estimated within any environment and exploits the domain generality of second-order beliefs (beliefs about beliefs). We show how maintaining internal valence representations allows the ensuing affective agent to optimize confidence in action selection preemptively. Valence representations can in turn be optimized by leveraging the (Bayes-optimal) updating term for subjective fitness, which we label affective charge (AC). AC tracks changes in fitness estimates and lends a sign to otherwise unsigned divergences between predictions and outcomes. We simulate the resulting affective inference by subjecting an in silico affective agent to a T-maze paradigm requiring context learning, followed by context reversal. This formulation of affective inference offers a principled account of the link between affect, (mental) action, and implicit metacognition. It characterizes how a deep biological system can infer its affective state and reduce uncertainty about such inferences through internal action (i.e., top-down modulation of priors that underwrite confidence). Thus, we demonstrate the potential of active inference to provide a formal and computationally tractable account of affect. Our demonstration of the face validity and potential utility of this formulation represents the first step within a larger research program. Next, this model can be leveraged to test the hypothesized role of valence by fitting the model to behavioral and neuronal responses.",
        "paperId": "13fc63ff6dcd58893dd2383d29e4b9c4d0fb2ebf"
    },
    {
        "title": "Few-shot Learning with Multilingual Language Models",
        "firstAuthor": "Xi Victoria Lin",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language tasks without fine-tuning. While these models are known to be able to jointly represent many different languages, their training data is dom-inated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 translation directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We present a detailed analysis of where the model suc-ceeds and fails, showing in particular that it enables cross-lingual in-context learning on some tasks, while there is still room for improvement on surface form robustness and adaptation to tasks that do not have a natural cloze form. Finally, we evaluate our models in social value tasks such as hate speech detection in 5 languages and find it has limitations similar to comparably sized GPT-3 models.",
        "paperId": "1403e6b9adf7712c35ae56327d52fe54603b87e1"
    },
    {
        "title": "Adult mathematics and everyday life: building bridges and facilitating learning 'transfer'",
        "firstAuthor": "Jeff Evans",
        "url": "https://link.springer.com/content/pdf/10.1007%2F0-306-47221-X_16.pdf",
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "1423bf401a96ce69cd28702c0bec386225828430"
    },
    {
        "title": "Pedagogical Design: Bridging Learning Theory and Learning Analytics",
        "firstAuthor": "Kazem Banihashem",
        "url": "https://cjlt.ca/index.php/cjlt/article/download/27959/20524",
        "dateSubmitted": "2021-08-09",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Which learning analytics (LA) approach might be the best choice for your teaching and learning context? Learning analytics as a field of research and application seeks to collect, analyze, report, and interpret educational data with the goal of improving teaching and learning. But hasty adoption of learning analytics tools and methods that are simply convenient, promoted or available risks allowing learning analytics to \u2018drive the pedagogical bus\u2019. In this paper, we propose that careful reflection on pedagogical design choices and the learning theory that underpins them can and should inform selection of relevant learning analytics tools and approaches. We broadly review established learning theories and the implications of each for pedagogical design; for each design approach we offer examples of learning analytics most clearly aligned with the theoretical perspectives on learning and knowledge that have shaped it. Moreover, we argue that careful consideration of the learning theory underpinning the pragmatics of pedagogical design choices should guide LA implementation, and help educators and designers avoid the risk of gathering data on, and measuring outcomes for, activities that are not relevant to their pedagogical design or goals.",
        "paperId": "1435285872ad1bf4f4977d519f7cd2471b64f367"
    },
    {
        "title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers",
        "firstAuthor": "Stephanie C. Y. Chan",
        "url": "https://arxiv.org/pdf/2205.05055",
        "dateSubmitted": "2022-04-22",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having large numbers of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. In sum, our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and how future work might encourage both in-context and in-weights learning in domains beyond language.",
        "paperId": "146e9e1238ff6caf18f0bd936ffcfbe1e65d2afd"
    },
    {
        "title": "Low-Resource Authorship Style Transfer: Can Non-Famous Authors Be Imitated?",
        "firstAuthor": "Ajay Patel",
        "url": null,
        "dateSubmitted": "2022-12-18",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Authorship style transfer involves altering text to match the style of a target author whilst preserving the original meaning. Existing unsupervised approaches like STRAP have largely focused on style transfer to target authors with many examples of their writing style in books, speeches, or other published works. This high-resource training data requirement (often greater than 100,000 words) makes these approaches primarily useful for style transfer to published authors, politicians, or other well-known figures and authorship styles, while style transfer to non-famous authors has not been well-studied. We introduce the \\textit{low-resource authorship style transfer} task, a more challenging class of authorship style transfer where only a limited amount of text in the target author's style may exist. In our experiments, we specifically choose source and target authors from Reddit and style transfer their Reddit posts, limiting ourselves to just 16 posts (on average ~500 words) of the target author's style. Style transfer accuracy is typically measured by how often a classifier or human judge will classify an output as written by the target author. Recent authorship representations models excel at authorship identification even with just a few writing samples, making automatic evaluation of this task possible for the first time through evaluation metrics we propose. Our results establish an in-context learning technique we develop as the strongest baseline, though we find current approaches do not yet achieve mastery of this challenging task. We release our data and implementations to encourage further investigation.",
        "paperId": "1476e47bd1a4cd82dd71e0321d2012e371e6b3da"
    },
    {
        "title": "Visual In-Context Learning for Few-Shot Eczema Segmentation",
        "firstAuthor": "Neelesh Kumar",
        "url": "https://arxiv.org/pdf/2309.16656",
        "dateSubmitted": "2023-09-28",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Automated diagnosis of eczema from digital camera images is crucial for developing applications that allow patients to self-monitor their recovery. An important component of this is the segmentation of eczema region from such images. Current methods for eczema segmentation rely on deep neural networks such as convolutional (CNN)-based U-Net or transformer-based Swin U-Net. While effective, these methods require high volume of annotated data, which can be difficult to obtain. Here, we investigate the capabilities of visual in-context learning that can perform few-shot eczema segmentation with just a handful of examples and without any need for retraining models. Specifically, we propose a strategy for applying in-context learning for eczema segmentation with a generalist vision model called SegGPT. When benchmarked on a dataset of annotated eczema images, we show that SegGPT with just 2 representative example images from the training dataset performs better (mIoU: 36.69) than a CNN U-Net trained on 428 images (mIoU: 32.60). We also discover that using more number of examples for SegGPT may in fact be harmful to its performance. Our result highlights the importance of visual in-context learning in developing faster and better solutions to skin imaging tasks. Our result also paves the way for developing inclusive solutions that can cater to minorities in the demographics who are typically heavily under-represented in the training data.",
        "paperId": "14795c7f2e8129f6bd6994b9c8c04bb23b23c483"
    },
    {
        "title": "Leveraging Code to Improve In-context Learning for Semantic Parsing",
        "firstAuthor": "Ben Bogin",
        "url": null,
        "dateSubmitted": "2023-11-16",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In-context learning (ICL) is an appealing approach for semantic parsing due to its few-shot nature and improved generalization. However, learning to parse to rare domain-specific languages (DSLs) from just a few demonstrations is challenging, limiting the performance of even the most capable LLMs. In this work, we improve the effectiveness of ICL for semantic parsing by (1) using general-purpose programming languages such as Python instead of DSLs, and (2) augmenting prompts with a structured domain description that includes, e.g., the available classes and functions. We show that both these changes significantly improve accuracy across three popular datasets. Combined, they lead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositional split), nearly closing the performance gap between easier i.i.d.\\ and harder compositional splits when used with a strong model, and reducing the need for a large number of demonstrations. We find that the resemblance of the target parse language to general-purpose code is a more important factor than the language's popularity in pre-training corpora. Our findings provide an improved methodology for building semantic parsers in the modern context of ICL with LLMs.",
        "paperId": "1486a3310e945b69f3dcf605f09e777187a0189d"
    },
    {
        "title": "Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model",
        "firstAuthor": "Xiaolin Chen",
        "url": "https://dl.acm.org/doi/pdf/10.1145/3606368",
        "dateSubmitted": "2022-07-16",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Text response generation for multimodal task-oriented dialog systems, which aims to generate the proper text response given the multimodal context, is an essential yet challenging task. Although existing efforts have achieved compelling success, they still suffer from two pivotal limitations: 1) overlook the benefit of generative pre-training, and 2) ignore the textual context related knowledge. To address these limitations, we propose a novel dual knowledge-enhanced generative pretrained language model for multimodal task-oriented dialog systems (DKMD), consisting of three key components: dual knowledge selection, dual knowledge-enhanced context learning, and knowledge-enhanced response generation. To be specific, the dual knowledge selection component aims to select the related knowledge according to both textual and visual modalities of the given context. Thereafter, the dual knowledge-enhanced context learning component targets seamlessly integrating the selected knowledge into the multimodal context learning from both global and local perspectives, where the cross-modal semantic relation is also explored. Moreover, the knowledge-enhanced response generation component comprises a revised BART decoder, where an additional dot-product knowledge-decoder attention sub-layer is introduced for explicitly utilizing the knowledge to advance the text response generation. Extensive experiments on a public dataset verify the superiority of the proposed DKMD over state-of-the-art competitors.",
        "paperId": "14c6082e408e9cf34747e3a49f3bd9fd2c06240a"
    },
    {
        "title": "SALM: Speech-augmented Language Model with In-context Learning for Speech Recognition and Translation",
        "firstAuthor": "Zhehuai Chen",
        "url": null,
        "dateSubmitted": "2023-10-13",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "We present a novel Speech Augmented Language Model (SALM) with {\\em multitask} and {\\em in-context} learning capabilities. SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions. The unified SALM not only achieves performance on par with task-specific Conformer baselines for Automatic Speech Recognition (ASR) and Speech Translation (AST), but also exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST. Moreover, {\\em speech supervised in-context training} is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models. Proposed model is open-sourced via NeMo toolkit.",
        "paperId": "1533c2cbfa402f7f6caf0d126be7864795afbe9d"
    },
    {
        "title": "Larger language models do in-context learning differently",
        "firstAuthor": "Jerry W. Wei",
        "url": "http://arxiv.org/pdf/2303.03846",
        "dateSubmitted": "2023-03-07",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings. We investigate two setups-ICL with flipped labels and ICL with semantically-unrelated labels-across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo/bar instead of negative/positive), thereby forcing language models to learn the input-label mappings shown in in-context exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input-label mappings, but more of the former.",
        "paperId": "154493f69d7db3d49da0e51df0192c6ad5f1724a"
    },
    {
        "title": "Women in crisis? : how young Greek women navigate 'emerging adulthood' following the effects of the 2008 economic crisis.",
        "firstAuthor": "Ioulia Kazana",
        "url": null,
        "dateSubmitted": "2018-10-31",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This study focuses upon Greek women aged in their twenties and thirties, examining how they have experienced \u2018emerging adulthood\u2019 amidst the post-2008 social and economic crisis. Despite several commentaries charting the social consequences of the Greek crisis, few have examined exclusively on young women. This thesis is among the first to demonstrate the gendered effects of the Greek crisis. Based on in-depth interviews with 36 young women in Thessaloniki and Athens, the study assesses how young women negotiate \u2018emerging adulthood\u2019, by examining certain attributes of the crisis, combined with Greece\u2019s unique cultural fabric. The thesis examines how traditional markers of adulthood, such as having a job, acquiring accommodation, establishing stable romantic relations and forming families have been considerably curtailed due to the effects of the crisis. \nThe findings of the thesis are positioned around three major themes; firstly, the importance of education and work for young women during emerging adulthood. Due to a reduction in labour market opportunities in medium-high skilled work, young middle-class women have found themselves facing considerably curtailed employment prospects. The study examines how young women negotiate these challenging employment contexts, learning to find ways of coping within these situations. Secondly, with most young women forced to live with parents, the thesis examines the ways these living situations provide both a safety net, but also a hinderance to their sense of autonomy and independence. Finally, the thesis explores how young middle-class women in Greece negotiate love and intimacy under conditions of financial hardships and a general context of uncertainties and insecurities. The thesis concludes with the argument that significant social uncertainties and repeated experiences of personal injustice and social strain have resulted in resignation - an accepted state of their life events with few alternatives and hopes of positive change.",
        "paperId": "1548b0688a6a172405baf9a5be428f2789e486ac"
    },
    {
        "title": "Personalised learning environments (part 2): a conceptual model for construction",
        "firstAuthor": "S. Syed-Khuzzan",
        "url": null,
        "dateSubmitted": "2009-01-30",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Purpose \u2013 The purpose of this paper is to present a conceptual model for a PLE prototype, specifically incorporating learning styles for the UK construction industry.Design/methodology/approach \u2013 The initial research methodology approach adopted for this paper embraced the distillation of core research material gathered from a detailed literature review. The literature review encompassed the needs and importance of developing a PLE prototype, and used as a context learning styles for the UK construction industry. A qualitative approach was used in this research, as this was considered more suitable for studying social and cultural phenomena. This paper explores the relationship between pedagogy and technology in the context of the design and implementation of a PLE. The implementation framework for the PLE adopted the principles of the \u201cCollaborative System Design\u201d approach as identified by the Advanced Distributed Learning (ADL) Initiative Guidelines.Findings \u2013 This paper describes the development phases...",
        "paperId": "155f113f7ca5423a01e1d04b8b8df6abd60883f1"
    },
    {
        "title": "The Relationship between Iranian Efl Learners' Emotional Quotient (Eq) and Their Language Proficiency",
        "firstAuthor": "H. Hosseinpoor",
        "url": null,
        "dateSubmitted": "2015-06-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "1. IntroductionAs a general concept, Intelligence consists of social and emotional factors in addition to the cognitive factors (Wechsler, 1989). Brown (2007) states that -the management of core emotions controls efficient mental and cognitive processing\" (p. 109). He contends that, based on Goleman (1995), the emotional mind is far quicker than the rational mind, springing into action without even pausing to think what it is doing. Goleman (1995) emphasized that, intelligence quotient (IQ) contributes about 20 percent to the factors that determine life success, the rest is related to other factors such as emotional intelligence (EI). EI is defined as \"the cooperative combination of intelligence and emotion\" (Mayer, Salovey, & Caruso, 2004, p. 197). EQ, as an approximately new behavioral model, makes us capable of perceiving, using, understanding, and managing our emotions (Salovey & Mayer, 1990). Research has indicated that EQ plays more important role than IQ in life success and education (Goleman, 1995; Salovey & Mayer, 1990). Many studies come to the conclusion that EQ is significant for work settings (Carmeli, 2003), and classrooms (Petrides, Frederickson, & Furnham, 2004), enhances performance in interviewing (Fox & Spector, 2000), and contextual performance (Carmeli, 2003) (as cited in Pishghadam, 2009). As Human beings, we are very emotional. Emotions dominate all our thoughts, actions and reflections. In fact, we are influenced by our emotion (Brown, 2007).In spite of the lack of attention to emotions in teaching contexts, learning and teacher training courses, recent studies on EQ made plenty of changes. Recent studies have demonstrated that EQ is significant for classrooms (Petrides et al., 2004), and it may affect academic achievement in various ways. Put it differently, as language is a social behavior and involves the external expression of emotions, affective and social language learning strategies also play significant roles in determining one's academic success (Petrides et al., 2004).Goleman (1995) asserts that EQ is a group of acquired skills and competencies which predict positive outcomes at home with one's family, in school, and at work. People who have them are healthier, less depressed, more productive at work, and have better relationships. In his belief, EQ is defined as the ability to love and be loved by friends, partner and family members.Goleman (1995) proposed that nearly 80% of the difference among people in different forms of success which is not accounted for by IQ and similar tests could be explained by other features that constitute EQ. Goleman defines EQ as including -abilities such as being able to motivate oneself and persist in the face of frustration, to control impulses and delay gratification; to regulate one's moods and keep distress from swapping the ability to think; to emphasize and to hope\" (p. 34) . Later, he reformulated his first definition of EQ and divided EQ into 25 different emotional competencies, among them political awareness, service orientation, self-confidence, consciousness, and achievement drive (Goleman, 1998).Recently, the effect of EQ on academic success has received considerable attention in education (Elias, Arnold, & Hussey, 2003). Nevertheless, as Brackett and Katulak (2007) assert, just few studies have been carried out to investigate EQ in English as a Foreign/Second Language (ESL/EFL) contexts, given the idea that the EQ serves both internal mechanisms and external environment in language learning process (Goleman, 2000).Researchers have recently started to explore the effects of EQ in the educational context and few studies have concentrated on the overall impacts of EQ on second or foreign language learning. These few studies have only been associated with particular aspects as management, self-esteem, anxiety, strategy use, or motivation. Moreover, other studies about intelligence have investigated the relationship between IQ and different aspects of language learning. \u2026",
        "paperId": "1562c1478aaf04c4a44f0ec02fa9e89dacf8a88f"
    },
    {
        "title": "A process model for information retrieval context learning and knowledge discovery",
        "firstAuthor": "Harvey S. Hyman",
        "url": null,
        "dateSubmitted": "2015-04-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "158625c7b0178ab34504244a5f1fffd8c6faf6dd"
    },
    {
        "title": "Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models",
        "firstAuthor": "Junpeng Li",
        "url": null,
        "dateSubmitted": "2023-11-13",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Document-level Relation Extraction (DocRE), which aims to extract relations from a long context, is a critical challenge in achieving fine-grained structural comprehension and generating interpretable document representations. Inspired by recent advances in in-context learning capabilities emergent from large language models (LLMs), such as ChatGPT, we aim to design an automated annotation method for DocRE with minimum human effort. Unfortunately, vanilla in-context learning is infeasible for document-level relation extraction due to the plenty of predefined fine-grained relation types and the uncontrolled generations of LLMs. To tackle this issue, we propose a method integrating a large language model (LLM) and a natural language inference (NLI) module to generate relation triples, thereby augmenting document-level relation datasets. We demonstrate the effectiveness of our approach by introducing an enhanced dataset known as DocGNRE, which excels in re-annotating numerous long-tail relation types. We are confident that our method holds the potential for broader applications in domain-specific relation type definitions and offers tangible benefits in advancing generalized language semantic comprehension.",
        "paperId": "159712301120bbc88f90848260d2bf846ba97203"
    },
    {
        "title": "Education and Unemployment in the Knowledge-Based Society",
        "firstAuthor": "Mihai Andronie",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "ABSTRACT.In the knowledge-based society when changes are fast, social and economic development depend on the quality of human resources development (HRD). Educated persons are more productive and have more opportunities to play an active role in the development of companies. According to the Global labor full report June 2012, carried out by McKinsey Global Institute, until 2030 the labor market will face unbalances: there will be a deficit of qualified workers of 13%, while the number of unqualified workers will increase with 10% as related to the request on the labor market. In the given context, learning towards specialization represents a need for preventing disparity between the competences requested by the market and the one held by workers. Due to the information technology environment, many e-learning opportunities appeared, enabling specialization on the labor market. The studies carried out show that in terms of unemployment there are some differences between the persons with higher education studies and those without higher education studies and the implications of long term unemployment are too major in order to neglect long life learning. The paper proposes an analysis of the situation of education and unemployment in the knowledge-based society, when new technologies and online learning create opportunities, but also the need for employees to improve their abilities in order to adapt to the constantly changing labor market.Keywords: unemployment; e-learning; specialized learning; education level1. Previous ResearchNumerous previous researches have been conducted in the field of unemployment and in the field of education, researches that were used as a base for the present study related to the use of education as a long term solution for preventing unemployment. Among the studies were used as bibliographies for the present paper are: The MGI - Global labor full report, June 2012; The Global Slavery Index 2013; The National Statistics Institute reports relating unemployment from 2008 to 2012; Long-term unemployment, the new challenge for many countries, International Labor Organization, 2013; Long-term unemployment, World Bank; Unemployment statistics, European Commission, Eurostat; Consequences of Long-Term Unemployment, Austin Nichols, Josh Mitchell, and Stephan Lindner, July 2013; New Skills for New Jobs, European Commission, Employment, Social Affairs and Inclusion; Americans Want Cost Cuts, Employer Help to Fund Education, Gallup 2013; Linkedln Still Rules As The Top Job Search Technology Tool, Survey Says, Susan Adams, 2013; Job Seeker Survey Job-Hunt.org; HR and Recruiting Professionals Survey Job Board Doctor, April 2013; Toward knowledge societies. UNESCO World Report, United Nations Educational, Scientific and Cultural Organization (2005), Conde-sur-Noireau; Societatea informafionala \u00a7i a cunoacterii, vectorii societafii cunoacterii, Mihai Draganescu, Romanian Academy.2. Disparities on the Labor MarketThe current society, where internet provides access to a lot of information, transforms knowledge into intangible assets which managers must use in order to be successful. Innovation, a feature of knowledge-based society, determines quick technological changes and all these will generate a different need for labor force. Thus, the institutions which provide education will have to be receptive to the constantly changing requests.1The Global labor full report June 2012 estimates that by 2030 the labor market will face the following disparities:* Lack of qualified workers reaching up to 38-40 million, representing 13% of the request for qualified work;* An average of 90-95 million unqualified workers worldwide, representing 10% of the request for unqualified labor force;* An average of 90-95 million unqualified labor force worldwide;* Lack of 45 million workers with mid-level studies in the emerging economies, representing 15% of the request. \u2026",
        "paperId": "15979e5cdf22aee0ef4138266d5ddea428721dd3"
    },
    {
        "title": "Inside Learning Centers.",
        "firstAuthor": "M. Cosgrove",
        "url": null,
        "dateSubmitted": "1992-08-31",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Learning centers are areas in a classroom, such as a corner in the room, that define a specific focus or afford a unique learning opportunity. Learning takes place when students reinforce skills by using the skills in interesting, meaningful, relevant, and social contexts. Learning centers can aid this reinforcement by giving students opportunities to participate in thought-provoking activities and by stimulating curiosity within a cooperative setting. Learning centers serve both a curriculum-centered and a child-centered approach to learning. Learning center activities can be organized by skills, novel interests, or integrated themes. Thematic centers are usually the most popular for teachers and students alike. Many center activities are game-like in nature, focusing on luck rather than ability. All center activities should include an objective, a set of directions, and a means of evaluation. Space availability, time constraints, and student movement all will affect the organization of learning centers. Learning centers offer another way to incorporate portfolio assessment into the classroom. The teacher must also carefully evaluate the effectiveness of the individual learning centers. Thus, with organization and creativity, learning centers can be a valuable addition to classrooms. (HB) *********************************************************************** Reproductions supplied by EDRS are the best that can be made from the original document. *********************************************************************** ;,,;:s\"..,1 1; ; 17,1, t,.+.Z.1r*'A; E.\" ...ktd-riF r. PY 1H1 U.S DEPARTMENT OF EDUCATION CIIIrre of Ent al onal PeE,arch and improvement EDUCATIONAL RESOURCES INFORMATION aNTERIERICr L7 Th.s cioc.rneni has Deen reproduced as 'eCe.ved Isom me oerscr, or organ nation wrgmar.N.1 Mnor changes have see\" nacle 10 .morose reproduclrOn po.ols or sten or opo,ons slated 1.45 clOCu Tent do not noceSSat t5 'en,eSeM oftto,at OEM P05.1.0^ INSIDE LEARNING CENTERS Maryellen S. Cosgrove, Ph.D. All I really need to know about how to live and what to do and how to be I learned in kindergarten. These are the things I have learned:",
        "paperId": "15a78c90d5de326aeee20b99d53033b0699d898f"
    },
    {
        "title": "The role of context in forming young learners' attitudes and motivation to learning French",
        "firstAuthor": "L. Courtney",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Despite the wealth of valuable information that has been generated by motivation studies to date, there are certain limitations in the common approaches. Quantitative and psychometric approaches to motivation research that have dominated in recent decades provided epiphenomenal descriptions of learner motivation within different contexts. However, these approaches assume homogeneity within a given group and often mask the variation between learners within the same, and different, contexts. Although these studies have provided empirical data to form and validate theoretical constructs, they have failed to recognise learners as individual \u2018people\u2019 that interact with their context. Learning context has become increasingly explicit in motivation studies, (see Coleman et al. 2007 and Housen et al. 2011), however it is generally considered as a background variable which is pre-existing and external to the individual. Stemming from the recent \u2018social turn\u2019 (Block 2003) in SLA research from a more cognitive-linguistic perspective to a more context-specific view of language learning, there has been an upsurge in demand for a greater focus on the \u2018person in context\u2019 in motivation research (Ushioda 2011). This paper reports on the findings of a longitudinal study of young English learners of French as they transition from primary to secondary school. Over 12 months, the study employed a mixed-method approach in order to gain an in-depth understanding of how the learners\u2019 context influenced attitudes to language learning. The questionnaire results show that whilst the learners displayed some consistent and stable motivational traits over the 12 months, there were significant differences for learners within different contexts in terms of their attitudes to the language classroom and their levels of self-confidence. A subsequent examination of the qualitative focus group data provided an insight into how and why these attitudes were formed and emphasised the dynamic and complex interplay between learners and their context.",
        "paperId": "15aed4a15bc045d43f0aeac10bde169046caa0ee"
    },
    {
        "title": "Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals",
        "firstAuthor": "Yanai Elazar",
        "url": null,
        "dateSubmitted": "2023-11-16",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The inevitable appearance of spurious correlations in training datasets hurts the generalization of NLP models on unseen data. Previous work has found that datasets with paired inputs are prone to correlations between a specific part of the input (e.g., the hypothesis in NLI) and the label; consequently, models trained only on those outperform chance. Are these correlations picked up by models trained on the full input data? To address this question, we propose a new evaluation method, Counterfactual Attentiveness Test (CAT). CAT uses counterfactuals by replacing part of the input with its counterpart from a different example (subject to some restrictions), expecting an attentive model to change its prediction. Using CAT, we systematically investigate established supervised and in-context learning models on ten datasets spanning four tasks: natural language inference, reading comprehension, paraphrase detection, and visual&language reasoning. CAT reveals that reliance on such correlations is mainly data-dependent. Surprisingly, we find that GPT3 becomes less attentive with an increased number of demonstrations, while its accuracy on the test data improves. Our results demonstrate that augmenting training or demonstration data with counterfactuals is effective in improving models' attentiveness. We show that models' attentiveness measured by CAT reveals different conclusions from solely measuring correlations in data.",
        "paperId": "15ceaf9932a3283ce06c06caaec0a35726df3c0a"
    },
    {
        "title": "What students who perform in \u201csecondary roles\u201d can learn from scenario training in vocational education",
        "firstAuthor": "David Sj\u00f6berg",
        "url": null,
        "dateSubmitted": "2019-04-25",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Context: Learning through scenario training and live simulation in vocational education is generally regarded as an effective tool for developing professional knowledge. However, previous research has largely overlooked the learning of students in secondary roles in scenario training. The objective of this study is to explore learning for students who act in secondary roles during scenario training in vocational educational settings.\u00a0 \nMethod: The studied case entails scenario training for police students in a Swedish police education programme. A case study design, which included both participant observation and a questionnaire, was used. The analytic lens applied was inspired by practice theory and focused on how structural and situational arrangements of the training activity affect learning.\u00a0 \nResults: Our findings show that students who act in secondary roles learn from their scenario training experiences, but this learning often is overlooked in the design of training activities. Due to the structural arrangements of training activities, learning emerged as students in secondary roles were tasked to support the primary participants in relation to their learning objectives. In addition, it emerged in how students in secondary roles used previous scenario training experiences in relation to the current scenario and its learning objectives. Examples of learning from situational arrangements emerged as students in secondary roles formulated and provided feedback to primary participants and through informal discussions and reflection processes. Learning also emerged as students in secondary roles embodied the \u201cother\u201d during scenario training, something that provided the students with new perspectives on police encounters.\u00a0 \nConclusions: We theorize and extract three dimensions for how learning emerges in this case for secondary participants. It emerges through embodying the \u201cother\u201d, in students\u2019 sensory experiences, and through reconstruction of knowledge through repetition. However, our findings also show that learning for students in secondary roles can be improved through mindful set-up and design. Based on the findings, our article provides a discussion and suggestions on how scenario training can be planned and set-up to develop professional knowledge for students in secondary roles.\u00a0",
        "paperId": "15d00d22ed9e65858f87f1946bc5674153a3af2a"
    },
    {
        "title": "A Cross-grade Comparison to Examine the Context Effect on the Relationships Among Family Resources, School Climate, Learning Participation, Science Attitude, and Science Achievement Based on TIMSS 2003 in Taiwan",
        "firstAuthor": "Shin-Feng Chen",
        "url": null,
        "dateSubmitted": "2012-09-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This study aimed to examine whether the relationships among family resources, school climate, learning participation, science attitude, and science achievement are different between primary school students and junior high school students within one educational system. The subjects included 4,181 Grade 4 students and 5,074 Grade 8 students who participated in TIMSS 2003 in Taiwan. Using structural equation modeling, the results showed that family resources had significant positive effects for both groups of learners. Furthermore, a context effect for the structural relationship between school climate, learning participation, and science achievement was revealed. In the primary school context, Grade 4 students who perceived positive school climate participated in school activities more actively, and had better science performance. However, in the secondary school context, learning participation had a negative impact and led to lower science achievement. The implications about this result in relation to the characteristics of the two educational contexts in Taiwan were further discussed.",
        "paperId": "15e412bc7f3d7d603d459bd613aaf2987d978708"
    },
    {
        "title": "In-Context Learning User Simulators for Task-Oriented Dialog Systems",
        "firstAuthor": "Silvia Terragni",
        "url": "http://arxiv.org/pdf/2306.00774",
        "dateSubmitted": "2023-06-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This paper presents a novel application of large language models in user simulation for task-oriented dialog systems, specifically focusing on an in-context learning approach. By harnessing the power of these models, the proposed approach generates diverse utterances based on user goals and limited dialog examples. Unlike traditional simulators, this method eliminates the need for labor-intensive rule definition or extensive annotated data, making it more efficient and accessible. Additionally, an error analysis of the interaction between the user simulator and dialog system uncovers common mistakes, providing valuable insights into areas that require improvement. Our implementation is available at https://github.com/telepathylabsai/prompt-based-user-simulator.",
        "paperId": "15fcd80193d1c446bc3d37fcc30f5475b9ebd5b0"
    },
    {
        "title": "Percepci\u00f3n de los profesores de sexto b\u00e1sico frente a la motivaci\u00f3n de la escritura",
        "firstAuthor": "Luisa Soto",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "En el contexto escolar el aprendizaje de la escritura supone un desafio complejo para los\nestudiantes. En esos procesos el docente tiene un papel clave como agente facilitador de\nconocimientos, habilidades y actitudes positivas hacia las tareas de escritura que los\nestudiantes producen en el aula. En el articulo se presentan los resultados de un estudio\ncualitativo en el que se analizaron las percepciones de 10 docentes de sexto basico respecto\na las estrategias que utilizan para motivar la escritura entre los estudiantes.; In the school context, learning to write is a complex challenge for students. In these\nprocesses, the teacher has a key role as a facilitator of knowledge, skills and positive\nattitudes towards the writing tasks that students produce in the classroom. The article\npresents the results of a qualitative study that analyzed the perceptions of 10 sixth grade\nteachers regarding the strategies they use to motivate writing among students.",
        "paperId": "16197f87a1d372c121159db78c3b5c66018fa284"
    },
    {
        "title": "Touch imprint cytology in tumor tissue banks for the confirmation of neoplastic cellularity and for DNA extraction.",
        "firstAuthor": "A. Mangia",
        "url": null,
        "dateSubmitted": "2008-06-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "CONTEXT\nLearning the characteristics of frozen tissue samples stored in tumor banks for biological studies remains a problem.\n\n\nOBJECTIVE\nTo assess the use of touch imprint cytology on fresh tissue samples as a rapid and reliable method of determining the presence and quantity of neoplastic cells before freezing.\n\n\nDESIGN\nTouch imprint cytology was performed on 259 specimens of operable breast cancer. Touch imprints were prepared from fresh tissue specimens before freezing samples for storage. Each tumor sample was imprinted on a glass slide and stained with hematoxylin-eosin. Tumor cellularity was quantified as negative, poor, moderate, or rich.\n\n\nRESULTS\nA significant correlation was found between samples with a tumor size greater than 2 cm and high tumor cellularity (P = .03; chi(2) test). Furthermore, 35% of ductal tumors showed higher tumor cellularity compared with lobular tumors (P < .001; chi(2) test). No association was found between lymph node status and tumor grade. When samples for which more than 2 imprints were available were examined, tumor cellularity among imprints of the same sample showed an overall agreement of 0.67 (P < .001; kappa statistic). It was also determined that the higher the cellularity, the higher the agreement. Our data also showed concordance of 0.87 (P < .001; kappa statistic) between touch imprint cytology imprints and histologic sections from contiguous tumor. Moreover, 11 randomly selected samples underwent DNA extraction, polymerase chain reaction, and sequencing to verify the feasibility of DNA analyses. We found that DNA from touch imprint cytology was amplifiable and suitable for direct sequencing.\n\n\nCONCLUSIONS\nTouch imprint cytology may represent an important step in the quality control of tumor cellularity of breast cancer specimens designed to be stored in tumor biobanks and a valid method for assessing the suitability of such tissue for further biomorphologic and biomolecular applications.",
        "paperId": "161eb704ecd1d6abfc9f93244fb46d95d730003d"
    },
    {
        "title": "Standard NER Tagging Scheme for Big Data Healthcare Analytics Built on Unified Medical Corpora",
        "firstAuthor": "Sarah Shafqat",
        "url": "https://ojs.istp-press.com/jait/article/download/127/141",
        "dateSubmitted": "2022-08-22",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The motivation for this research comes from the gap found in discovering the common ground for medical context learning through analytics for different purposes of\u00a0 diagnosing, recommending, prescribing or treating patients for uniform phenotype features from patients\u2019 profile. Authors of this paper while searching for possible solutions for medical context learning found that unified corpora tagged with medical nomenclature was missing to train the analytics for medical context learning. Therefore, here we demonstrated a mechanism to come up with uniform NER (Named Entity Recognition) tagged medical corpora that is fed with 14407 endocrine patients\u2019 dataset in CSV format diagnosed with DM and comorbidity diseases. The other corpus is of ICD-10-CM coding scheme in text format taken from www.icd10data.com. ICD-10-CM corpus is to be tagged for understanding the medical context with uniformity for which we are conducting different experiments using common NLP techniques and frameworks like; TensorFlow, Keras, LSTM, and Bi-LSTM. \nIn our preliminary experiments albeit label sets in form of (instance, label) pair were tagged with Sequential() model formed on TensorFlow.Keras and Bi-LSTM NLP algorithms. The maximum accuracy achieved for model validation was 0.8846.",
        "paperId": "163c6c4455c69ae4754d6d1be6f860db824eb8e2"
    },
    {
        "title": "An Application of Context-Learning in a Goal-Seeking Neural Network",
        "firstAuthor": "ThomasE . Portegys",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "An important function of many organisms is the ability to use contextual information in order to increase the probability of achieving goals. For example, a street address has a particular meaning only in the context of the city it is in. In this paper, predisposing conditions that influence future outcomes are learned by a goal-seeking neural network called Mona. A maze problem is used as a context-learning exercise. At the beginning of the maze, an initial door choice forms a context that must be remembered until the end of the maze, where the same door must be chosen again in order to reach a goal. Mona must learn these door associations and the intervening path through the maze. Movement is accomplished by expressing responses to the environment. The goalseeking effectiveness of the neural network in a variety of maze complexities is measured.",
        "paperId": "165cd493e9f23dd94c9977c73a038f3e293197f3"
    },
    {
        "title": "Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction",
        "firstAuthor": "Ashish Sharma",
        "url": "http://arxiv.org/pdf/2305.02466",
        "dateSubmitted": "2023-05-04",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "A proven therapeutic technique to overcome negative thoughts is to replace them with a more hopeful \u201creframed thought.\u201d Although therapy can help people practice and learn this Cognitive Reframing of Negative Thoughts, clinician shortages and mental health stigma commonly limit people\u2019s access to therapy. In this paper, we conduct a human-centered study of how language models may assist people in reframing negative thoughts. Based on psychology literature, we define a framework of seven linguistic attributes that can be used to reframe a thought. We develop automated metrics to measure these attributes and validate them with expert judgements from mental health practitioners. We collect a dataset of 600 situations, thoughts and reframes from practitioners and use it to train a retrieval-enhanced in-context learning model that effectively generates reframed thoughts and controls their linguistic attributes. To investigate what constitutes a \u201chigh-quality\u201d reframe, we conduct an IRB-approved randomized field study on a large mental health website with over 2,000 participants. Amongst other findings, we show that people prefer highly empathic or specific reframes, as opposed to reframes that are overly positive. Our findings provide key implications for the use of LMs to assist people in overcoming negative thoughts.",
        "paperId": "16aacf48048ac128a07fe2c0761439e1d7211492"
    },
    {
        "title": "Memory Reconsolidation Mediates the Updating of Hippocampal Memory Content",
        "firstAuthor": "Jonathan L. C. Lee",
        "url": "https://www.frontiersin.org/articles/10.3389/fnbeh.2010.00168/pdf",
        "dateSubmitted": "2010-09-15",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The retrieval or reactivation of a memory places it into a labile state, requiring a process of reconsolidation to restabilize it. This retrieval-induced plasticity is a potential mechanism for the modification of the existing memory. Following previous data supportive of a functional role for memory reconsolidation in the modification of memory strength, here I show that hippocampal memory reconsolidation also supports the updating of contextual memory content. Using a procedure that separates the learning of pure context from footshock-motivated contextual fear learning, I demonstrate doubly dissociable hippocampal mechanisms of initial context learning and subsequent updating of the neutral contextual representation to incorporate the footshock. Contextual memory consolidation was dependent upon BDNF expression in the dorsal hippocampus, whereas the footshock modification of the contextual representation required the expression of Zif268. These mechanisms match those previously shown to be selectively involved in hippocampal memory consolidation and reconsolidation, respectively. Moreover, memory reactivation is a necessary step in modifying memory content, as inhibition of hippocampal synaptic protein degradation also prevented the footshock-mediated memory modification. Finally, dorsal hippocampal knockdown of Zif268 impaired the reconsolidation of the pure contextual memory only under conditions of weak context memory training, as well as failing to disrupt contextual freezing when a strong contextual fear memory is reactivated by further conditioning. Therefore, an adaptive function of the reactivation and reconsolidation process is to enable the updating of memory content.",
        "paperId": "16c58fe645ad68669c36b13d36b22a9b3ba3b098"
    },
    {
        "title": "\u00d5ppematerjali m\u00f5ju g\u00fcmnaasiumi\u00f5pilaste \u00f5pimotivatsioonile: \u201ePraktiline eesti keel teise keelena: B2, C1\u201d [The influence of learning material on students\u2019 motivation to learn: Practical Estonian as a second language. B2, C1.]",
        "firstAuthor": "Mare Kitsnik",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Learning motivation connected to a second language is affected by cultural context (the value of language studies, belief in the likelihood of success etc.) and educational context (the study programme, the teacher, using materials, the study group etc.). The article discusses the influence of one component of the educational context \u2013 learning materials \u2013 on the motivation to learn Estonian as a second language, based on the example of the Practical Estonian B2, C1 workbook. The workbook is a new example of study materials for Estonian as a second language for use in secondary schools, one goal of the creation of which was to increase students\u2019 motivation to learn. For this purpose, the following characteristics of motivating learning activities were taken into consideration when creating the workbook: the meaningfulness of the topics to students, the use of active learning methods, the authenticity of the texts and tasks, and suitably difficult challenges and support. The workbook was tested during its compilation in one secondary school class and, after its completion, as a homework component of the training of secondary school teachers of Estonian as a second language. Feedback was collected from the teachers in the form of diaries. The teachers felt that the students were most motivated by tasks carried out using active study methods that provided appropriate challenges (role play, playful vocabulary and grammar exercises, and a project task in the language environment). Students were motivated by topics that were meaning ful to them (both \u201cyouth\u201d topics and topics of overall human importance). The authentic reading tasks had an average level of motivation, while the motivation level of the authentic listening tasks was decreased by the fact that the students were not used to doing them. The students also liked tasks that provided the necessary support in a new way (learning phrases that enabled them to express language functions and practising process writing). Analysis of the feedback also indicated that teachers had room for improvement in regard to teaching methods: they were not always capable of presenting tasks in a motivating way and were not used to using active teaching methods, letting students make their own choices or giving students supportive and motivating feedback. Keywords meaningful topics, authentic texts and tasks, active learning, appropriate challenge and support",
        "paperId": "16d8dc44c1030c4ba4b99ef561ed25da23ffbb18"
    },
    {
        "title": "The effect of different levels of realism of context learning on the prescribing competencies of medical students during the clinical clerkship in internal medicine: an exploratory study",
        "firstAuthor": "J. Tichelaar",
        "url": null,
        "dateSubmitted": "2015-02-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "16e5d89ce574c0b80df1c9d44b00d0a992569319"
    },
    {
        "title": "Prerequisites for the training of high school students in the competence to carry out experiences in the humanities",
        "firstAuthor": "A. Cernei",
        "url": "https://revista.ust.md/index.php/acta_educatie/article/download/794/774",
        "dateSubmitted": "2022-11-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Today, the idea of moving from traditional training to research-based learning is being circulated more and more often - a premise for the formation of the competence to carry out experiences in the humanities - which aims at the organic integration of the student into the knowledge-based society. Being a didactic method based on experience, in its context the student learns to apply in practice the theoretical knowledge obtained in the classroom, acting pertinently, individually or in groups, to solve the problems identified in society. In this context, learning through research is the one that offers opportunities for the cumulative development of knowledge, their understanding, thus developing students' motivation and interest in science.",
        "paperId": "1711c9ec8e3008181c656a8878e823b836888d49"
    },
    {
        "title": "U.S. Sentencing Strategy in Context: Learning from the International Community",
        "firstAuthor": "Brian J. Ostrom",
        "url": null,
        "dateSubmitted": "1999-11-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "17375cc756ddbd9a1a92ba4572c3d87d33f74ca9"
    },
    {
        "title": "Artefacts Mediating Practices across Time and Space: Sociocultural Studies of Material Conditions for Learning and Remembering",
        "firstAuthor": "K. Hakkarainen",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The theme of this symposium is to explore the material conditions of learning and remembering from a sociocultural perspective. We do this in four different empirical contexts. Learning and remembering are understood as meaning-making processes that are dependent on and co-constituted by mediating tools that enable practices to extend across time and space. Our interests are precisely in what ways the \u201ctools\u201d people employ in these studies mediate activities of learning and remembering, and how they contribute to the organization of collective forms of knowing. We also address how we analyze the specific material features of tools that co-determine the unfolding of the activities. Overall focus and issues addressed The overall focus of this symposium is to explore the material conditions of learning and remembering from a sociocultural perspective. It is nowadays not uncommon to think of learning and remembering in terms of sociomaterially entangled and distributed practices, but this idea, in fact, has been around for quite some time. The basic notion in sociocultural studies of the material basis of learning and remembering is mediation, initially introduced by Vygotsky (1997). Vygotsky argued for situated studies of how humans interact with artefacts \u2013 technical as well as \u201cpsychological\u201d \u2013 as they engage in activities and practices. To make his idea of mediation salient, he used an analogy and argued that psychological tools restructure the capacities of the human mind (i.e. learning, remembering, coordination of actions and perceptions) in ways that are similar to how \u201ctechnical tools\u201d transform physical activities. This is the essence of the idea of tool-mediated action. The inclusion of a tool in the behavioral process, thus, reorganizes what Vygotsky (1997, p. 87) refers to as the \u201cinstrumental act\u201d. First, it \u201csets to work a number of new functions connected with the use and control of the given tool; second, abolishes and makes unnecessary a number of natural processes, whose work is [now] done by the tool; third, it modifies the course and the various aspects (intensity, duration, order, etc.) of all mental processes included in the instrumental act, replacing some functions with others, i.e., it recreates, reconstructs the whole structure of behavior just like a technical tool recreates the entire system of labor operations.\u201d (Vygotsky, 1997, p. 87) As many scholars have pointed out, the distinction between psychological and technical tools must be understood as an analogy, since in a sociocultural tradition the distinction between the material and the ideal/conceptual is not accepted. Cultural tools \u201care simultaneously ideal (conceptual) and material\u201d as Cole argues (1996, p. 117). Based on these ideas as a shared premise, this symposium will first present four empirical studies conducted in diverse settings. Then the contributors will discuss the situated processes of meaning-making that are salient in the activities studied, and they will explore how these are contingent on material and conceptual features of tools: CSCL 2015 Proceedings 593 \u00a9 ISLS 1. Precisely in what ways are the \u201ctools\u201d people employ in these studies mediating activities of learning, remembering, and how do they contribute to the organization of collective forms of knowing? 2. What are the specific material features inherent to those activities? Collective contribution to the discussion of these issues As a collective enterprise we will explore and discuss the different ways that the notion of mediation is relevant in our studies in order to make salient what could be referred to as the materiality of learning and remembering in situated activities. It should also be noted that mediating tools may extend or restrict people\u2019s participation, they may serve to support or challenge people\u2019s co-ordination and interaction, they may black-box certain processes people engage in, they may function as prosthetic devices that extend people\u2019s agency or as vehicles for entering into domains and practices that are entirely new. Significance of each contribution The contributions to this symposium accordingly share specific interests but they also differ with regard to empirical contexts and the kind of artefacts included in the activity. This will be fruitful for the discussion we wish to promote. In the first paper, by Ritella, Ligorio and Hakkarainen, design students\u2019 collaborative activity (as they engage in creating artefacts in their design project), and their meaning-making process are analyzed through the Bakhtinian concept of chronotope. A similar, and yet different, case is presented in the paper by Arnseth, Jornet and Krange. The study reported concerns student work on sustainable energy use in the context of science education. As the authors follow how students engage in meaning-making with discursive and nondiscursive artefacts in multi-modal learning environments, they use the analytical notion of a functional system (John-Steiner, Meehan, & Mahn, 1998). The third contribution, by Fauville, Lantz-Andersson and S\u00e4lj\u00f6, explores students\u2019 meaning-making as they engage with a mediating tool, a so-called carbon footprint calculator. This tool has been designed for people to conceptualize and understand the complexities of climate change, and, more specifically, the environmental consequences of their daily activities (travel, shopping, food intake etc.). Here the notion of psychological tool is drawn on to elaborate on the reflexive nature of engaging with personal behaviors through the lens of a powerful artifact. The final contribution, by Lundin and M\u00e4kitalo, shares important elements of the third contribution with respect to how the self-generated data from people\u2019s everyday life are visualized to facilitate learning to manage one\u2019s own life situation. The tool in this case is used in hypertension care (a common but invisible condition). The study explores the tensions that this mediating tool creates in the clinical encounter as patients draw on their own documentation and experience to frame and understand their health status. Building space-time frames and shared understanding in a media design task Giuseppe Ritella, Beatrice Ligorio and Kai Hakkarainen This study is aimed at investigating if and how the building of space-time frames is intertwined with the construction of a shared understanding of the task to be accomplished during a media design project course. The literature suggests that building a shared understanding of the task is crucial for CSCL (Dillenbourg et al., 2010; Rochelle & Teasley, 1995). Moreover, design tasks are considered as ill-structured or wicked, which require additional interpretative efforts for the building of a shared understanding. Nevertheless, we argue that a comprehensive understanding of the relationship between two crucial processes is missing: (a) building a shared understanding of the task and (b) framing the context of learning in space and time. In particular, we analyze students\u2019 interaction when the link between negotiation of space-time relations (chronotope) and shared interpretation of the collaborative task is explicitly articulated in students\u2019 activity. By looking at such moments, we shed light on the collective interpretation of the task as a holistic sense-making process regarding what the task is about (including subtasks), and the space-time organization (chronotope) of the activity, as it is mediated by multiple artifacts created and updated by students throughout the course. We conducted participant observation at a media design course where students worked in groups of 4-5 to develop a project, held at Metropolia University of Applied Sciences in Helsinki. Students worked in teams of 4-5 participants to develop a project. Students had to build a product or a service based on a problem presented by a representative of a company, who acted as their customer. The course lasted 16 weeks and the students worked together for ten hours per week. Many technological tools such as smart-boards, tablets, and notebooks, were available for them. Groups were free to negotiate and select the tools they considered appropriate at the different stages of the course, which gave them a relatively high degree of autonomy in the management of the collaboration. In this CSCL 2015 Proceedings 594 \u00a9 ISLS paper we will present the analysis of one group, composed by five students coming from different master programs. The principal method of data collection of this study was participant observation, involving also collection of audio and video records, which allowed documenting multi-level activity processes taking place while participants are engaged in technology-mediated learning (Goodwin, 2000). Six weeks were selected for participant observation: two weeks at the beginning of the course, two weeks in the middle and two weeks at end of the course. The rationale of this sampling was to follow the development of space-time management in the different phases of the course. Moreover, the collection of video records was complemented with screen records of computer-mediated activity whenever students used a smart board. Furthermore, we had access to most of the artifacts that students shared during the course. Artifacts and field notes were used as secondary data. The qualitative methods employed allowed triangulating various aspects of the emergent chronotopes and their development. The data analysis was organized in three steps:1) exploration of the data and preliminary interpretation; 2) selection and transcription of the data for in-depth qualitative analysis: we selected the clips in which (a) students were explicitly discussing their interpretation of the task; (2) the students were referring to space-time coordinates in their speech; (3) the students were taking decisions implicitly framing the task or the chronotope;3) qualitative video analysis of the 52 selected clips. Finally, two stimu",
        "paperId": "1795e3ac755ece724079599bfb020134189190f6"
    },
    {
        "title": "Estimating Large Language Model Capabilities without Labeled Test Data",
        "firstAuthor": "Harvey Yiyun Fu",
        "url": "http://arxiv.org/pdf/2305.14802",
        "dateSubmitted": "2023-05-24",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large Language Models (LLMs) have the impressive ability to perform in-context learning (ICL) from only a few examples, but the success of ICL varies widely from task to task. Thus, it is important to quickly determine whether ICL is applicable to a new task, but directly evaluating ICL accuracy can be expensive in situations where test data is expensive to annotate -- the exact situations where ICL is most appealing. In this paper, we propose the task of ICL accuracy estimation, in which we predict the accuracy of an LLM when doing in-context learning on a new task given only unlabeled test data for that task. To perform ICL accuracy estimation, we propose a method that trains a meta-model using LLM confidence scores as features. We compare our method to several strong accuracy estimation baselines on a new benchmark that covers 4 LLMs and 3 task collections. The meta-model improves over all baselines across 8 out of 12 settings and achieves the same estimation performance as directly evaluating on 40 collected labeled test examples per task. At the same time, no existing approach provides an accurate and reliable ICL accuracy estimation in every setting, highlighting the need for better ways to measure the uncertainty of LLM predictions.",
        "paperId": "17b4028bafc6d92a3a1702afecb023bfb798ba3b"
    },
    {
        "title": "A ChatGPT-based Model for User Book Rating Prediction",
        "firstAuthor": "YanFang Chen",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": ": [Purpose/significance] With the continuous development and change of Large Language Models (LLMs) repre-sented by ChatGPT, classical scenarios in many fields have been given new opportunities. At the same time, more and more researchers begin to focus on how to apply the intelligentness and technology of LLMs to existing scenarios, and analyze the challenges and opportunities brought by these technologies. [Method/process] This is the first time that LLM technology has been introduced into user book rating prediction, which is a typical application scenario in library and information science. We explored the feasibility of using LLM technology in user book rating by building a CUBR (ChatGPT-based model for User Book Rating Prediction) model based on ChatGPT. At the same time, this paper compares different evaluation schemes based on book rating task with existing classical recommendation models, discusses and gives the advantages and disadvantages of CUBR in predicting scenarios of user book scoring, and analyses the possible application opportunities of subsequent LLMs in other scenarios of book recommendation. [Result/conclusion] The experimental research in this paper shows that: (1) CUBR model can achieve good recommendation results on existing user book rating prediction tasks, especially when the target information to be recommended is less, such as one-shot, which performs close to or exceeds the current classical recommendation algorithm, and has strong generalization ability, which is suitable for cold-start recommendation. (2) With the increase of sample content prompted by a single user (e.g. from One-shot to Ten-shot), the predictive effect of CUBR will be significantly improved, indicating that CUBR has good real-time in-context learning ability. [Limitations] The scenarios studied in this paper are limited to the understanding and recommendation of users\u2019 book scoring preferences. In the future, we will try to apply and transform the existing large language model technology in more library and information science scenarios, and achieve better landing effects.",
        "paperId": "17b512e7570e591965172fc681655b73b7169f3b"
    },
    {
        "title": "Learning to Repair Transgressions: Toddlers' Social Learning of a Reparative Prosocial Act",
        "firstAuthor": "M. Donohue",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This study investigated children\u2019s social learning of prosocial behaviors in a transgressor context. Two-to three-year-olds (24-47 months, N = 54) saw videos of an adult help another adult in distress by performing a novel prosocial action. Children were then led to believe that they had transgressed to cause their parent\u2019s pain and sadness. It was hypothesized that children in the experimental condition who watched the video and then transgressed would be more likely to perform the novel action (imitation) and to display non-demonstrated prosocial behaviors (goal emulation) relative to children in two control conditions: (a) children who did not view the video but transgressed and (b) children who viewed the video but witnessed a neutral interaction. Children in the experimental condition were no more likely to imitate or emulate than children in the control conditions, suggesting that children have difficulty applying socially learned prosocial behaviors in a transgressor context. INDEX WORDS: Prosocial behaviors, Reparation, Social learning, Imitation, Goal emulation, Transgressor context LEARNING TO REPAIR TRANSGRESSIONS: TODDLERS\u2019 SOCIAL LEARNING OF A REPARATIVE PROSOCIAL ACT",
        "paperId": "1802e863c279115ba4489462176df12c4e9cf870"
    },
    {
        "title": "Dr.ICL: Demonstration-Retrieved In-context Learning",
        "firstAuthor": "Man Luo",
        "url": "http://arxiv.org/pdf/2305.14128",
        "dateSubmitted": "2023-05-23",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In-context learning (ICL), teaching a large language model (LLM) to perform a task with few-shot demonstrations rather than adjusting the model parameters, has emerged as a strong paradigm for using LLMs. While early studies primarily used a fixed or random set of demonstrations for all test queries, recent research suggests that retrieving semantically similar demonstrations to the input from a pool of available demonstrations results in better performance. This work expands the applicability of retrieval-based ICL approaches by demonstrating that even simple word-overlap similarity measures such as BM25 outperform randomly selected demonstrations. Furthermore, we extend the success of retrieval-based ICL to instruction-finetuned LLMs as well as Chain-of-Thought (CoT) prompting. For instruction-finetuned LLMs, we find that although a model has already seen the training data at training time, retrieving demonstrations from the training data at test time yields better results compared to using no demonstrations or random demonstrations. Last but not least, we train a task-specific demonstration retriever that outperforms off-the-shelf retrievers.",
        "paperId": "18143a4c2da37444e06feed04cc9efeb0856352d"
    },
    {
        "title": "Students with Limited or Interrupted Formal Education in US Classrooms",
        "firstAuthor": "A. DeCapua",
        "url": null,
        "dateSubmitted": "2010-06-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "182042b318f80326b5fa04b2d575cc0e69e1a326"
    },
    {
        "title": "ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations",
        "firstAuthor": "Chunkit Chan",
        "url": "http://arxiv.org/pdf/2304.14827",
        "dateSubmitted": "2023-04-28",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This paper aims to quantitatively evaluate the performance of ChatGPT, an interactive large language model, on inter-sentential relations such as temporal relations, causal relations, and discourse relations. Given ChatGPT's promising performance across various tasks, we conduct extensive evaluations on the whole test sets of 13 datasets, including temporal and causal relations, PDTB2.0-based and dialogue-based discourse relations, and downstream applications on discourse understanding. To achieve reliable results, we adopt three tailored prompt templates for each task, including the zero-shot prompt template, zero-shot prompt engineering (PE) template, and in-context learning (ICL) prompt template, to establish the initial baseline scores for all popular sentence-pair relation classification tasks for the first time. We find that ChatGPT exhibits strong performance in detecting and reasoning about causal relations, while it may not be proficient in identifying the temporal order between two events. It can recognize most discourse relations with existing explicit discourse connectives, but the implicit discourse relation still remains a challenging task. Meanwhile, ChatGPT performs poorly in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation.",
        "paperId": "186e96fe036927182ec963b63f9dd7f8ff650158"
    },
    {
        "title": "Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment",
        "firstAuthor": "Sky Ch-Wang",
        "url": "http://arxiv.org/pdf/2305.14492",
        "dateSubmitted": "2023-05-23",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Designing systems that can reason across cultures requires that they are grounded in the norms of the contexts in which they operate. However, current research on developing computational models of social norms has primarily focused on American society. Here, we propose a novel approach to discover and compare descriptive social norms across Chinese and American cultures. We demonstrate our approach by leveraging discussions on a Chinese Q&A platform (Zhihu) and the existing SocialChemistry dataset as proxies for contrasting cultural axes, align social situations cross-culturally, and extract social norms from texts using in-context learning. Embedding Chain-of-Thought prompting in a human-AI collaborative framework, we build a high-quality dataset of 3,069 social norms aligned with social situations across Chinese and American cultures alongside corresponding free-text explanations. To test the ability of models to reason about social norms across cultures, we introduce the task of explainable social norm entailment, showing that existing models under 3B parameters have significant room for improvement in both automatic and human evaluation. Further analysis of cross-cultural norm differences based on our dataset shows empirical alignment with the social orientations framework, revealing several situational and descriptive nuances in norms across these cultures.",
        "paperId": "18bd959aaa8a83b5b2192282224d700da7459857"
    },
    {
        "title": "Educaci\u00f3n, Crisis y Mercado: reconfiguraciones institucionales, organizaci\u00f3n y resistencia tras la crisis del 2001",
        "firstAuthor": "J. Tranier",
        "url": null,
        "dateSubmitted": "2014-02-02",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This work will address some issues regarding teaching and learning processes in Rosario, Argentina, in the context of its first major crisis that the country have faced at the dawn of the new century. It follows from the above that one of the main purposes would be try to analyze how schools have inhabited the teaching field through their own daily actions. Along with that, we will try to ask about what kind of forms of subjectivity have been taken place between students and teachers; and what kind of educational and pedagogical alternatives have been raised in order to try to repair the damaged social of the subject in the context learning.",
        "paperId": "18bf923cc017cd6eee09706ff4c5f92837dc2203"
    },
    {
        "title": "Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer",
        "firstAuthor": "Yongqiang Wang",
        "url": "https://arxiv.org/pdf/2309.07566",
        "dateSubmitted": "2023-09-14",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Direct speech-to-speech translation (S2ST) with discrete self-supervised representations has achieved remarkable accuracy, but is unable to preserve the speaker timbre of the source speech during translation. Meanwhile, the scarcity of high-quality speaker-parallel data poses a challenge for learning style transfer between source and target speech. We propose an S2ST framework with an acoustic language model based on discrete units from a self-supervised model and a neural codec for style transfer. The acoustic language model leverages self-supervised in-context learning, acquiring the ability for style transfer without relying on any speaker-parallel data, thereby overcoming the issue of data scarcity. By using extensive training data, our model achieves zero-shot cross-lingual style transfer on previously unseen source languages. Experiments show that our model generates translated speeches with high fidelity and style similarity. Audio samples are available at http://stylelm.github.io/ .",
        "paperId": "191b685607b1801a0bc536ae4bc56def49e210a0"
    },
    {
        "title": "A Competence of A Multiple Context Learning System : International Journal of General Systems",
        "firstAuthor": "B. MacDonald",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "193fb7a6a2f1b96478b1be6760de3dcca86597c2"
    },
    {
        "title": "FLIRT: Feedback Loop In-context Red Teaming",
        "firstAuthor": "Ninareh Mehrabi",
        "url": "https://arxiv.org/pdf/2308.04265",
        "dateSubmitted": "2023-08-08",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Warning: this paper contains content that may be inappropriate or offensive. As generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. Here we propose an automatic red teaming framework that evaluates a given model and exposes its vulnerabilities against unsafe and inappropriate content generation. Our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. We propose different in-context attack strategies to automatically learn effective and diverse adversarial prompts for text-to-image models. Our experiments demonstrate that compared to baseline approaches, our proposed strategy is significantly more effective in exposing vulnerabilities in Stable Diffusion (SD) model, even when the latter is enhanced with safety features. Furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text models, resulting in significantly higher toxic response generation rate compared to previously reported numbers.",
        "paperId": "19443d48399d4fe89a4b0a96917c50c6fd9c5af1"
    },
    {
        "title": "Towards Robust Named Entity Recognition via Temporal Domain Adaptation and Entity Context Understanding",
        "firstAuthor": "Oshin Agarwal",
        "url": "https://ojs.aaai.org/index.php/AAAI/article/download/21570/21319",
        "dateSubmitted": "2022-06-28",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Named Entity Recognition models perform well on benchmark datasets but fail to generalize well even in the same domain. The goal of my th esis is to quantify the degree of in-domain generalization in NER, probe models for entity name vs. context learning and finally improve their robustness, focusing on the recognition of ethnically diverse entities and new entities over time when the models are deployed.",
        "paperId": "19649d8cf6ed4493f9c600e85d634a0a40e5b10a"
    },
    {
        "title": "Ontogenetic differences in expressed fear of context following aversive conditioning",
        "firstAuthor": "P. Kraemer",
        "url": "https://link.springer.com/content/pdf/10.3758%2FBF03330450.pdf",
        "dateSubmitted": "1992-09-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "1967050fa4843a0c8f6b576f0b60f402e46b93cf"
    },
    {
        "title": "ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction",
        "firstAuthor": "Jiabang He",
        "url": "https://arxiv.org/pdf/2303.05063",
        "dateSubmitted": "2023-03-09",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated remarkable results in various natural language processing (NLP) tasks with in-context learning, which involves inference based on a few demonstration examples. Despite their successes in NLP tasks, no investigation has been conducted to assess the ability of LLMs to perform document information extraction (DIE) using in-context learning. Applying LLMs to DIE poses two challenges: the modality and task gap. To this end, we propose a simple but effective in-context learning framework called ICL-D3IE, which enables LLMs to perform DIE with different types of demonstration examples. Specifically, we extract the most difficult and distinct segments from hard training documents as hard demonstrations for benefiting all test instances. We design demonstrations describing relationships that enable LLMs to understand positional relationships. We introduce formatting demonstrations for easy answer extraction. Additionally, the framework improves diverse demonstrations by updating them iteratively. Our experiments on three widely used benchmark datasets demonstrate that the ICL-D3IE framework enables Davinci-003/ChatGPT to achieve superior performance when compared to previous pre-trained methods fine-tuned with full training in both the in-distribution (ID) setting and in the out-of-distribution (OOD) setting. Code is available at https://github.com/MAEHCM/ICL-D3IE.",
        "paperId": "197022486b2e2584302bd9b6442e44d15bf3e351"
    },
    {
        "title": "Social Influences on Policy Preferences: Conformity and Reactance",
        "firstAuthor": "Meirav Furth-Matzkin",
        "url": "https://scholarship.law.umn.edu/cgi/viewcontent.cgi?article=1103&context=mlr",
        "dateSubmitted": "2016-12-07",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Social norms have been used to nudge people toward specified outcomes in various domains. But can people be nudged to support, or to reject, proposed government policies? How do people\u2019s views change when they learn that the majority approves of a particular policy, or that the majority opposes it? To answer these questions, we conducted a series of experiments. We find that in important contexts, learning about the majority\u2019s opinion causes a significant shift toward support for or opposition to particular policies. At the same time, we find that when people\u2019s views are fixed and firm, they are unlikely to conform to the majority\u2019s view and that they might even show reactance. We show this pattern of results with respect to people\u2019s support for or opposition to governmental policies in a wide range of substantive areas \u2014 and also to the use of paternalistic tools, such as nudges or bans.",
        "paperId": "19a7c94c7d4baadc9d3e78d7748b682dde735d8a"
    },
    {
        "title": "Contextual cueing in co-active visual search: Joint action allows acquisition of task-irrelevant context",
        "firstAuthor": "Xuelian Zang",
        "url": "https://link.springer.com/content/pdf/10.3758/s13414-022-02470-x.pdf",
        "dateSubmitted": "2022-04-18",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "19b943aa9a8ca132e622e68f0a450a9da53e794f"
    },
    {
        "title": "Learning Objects to Suport the Teaching of Science",
        "firstAuthor": "A. M. Vasco",
        "url": null,
        "dateSubmitted": "2011-06-16",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The use of technology is bringing about changes in education. Teaching through the computer gives a position to acquire concepts about any field of knowledge. In this context, learning objects are presented as a possibility to enhance the procedure for implementing the curriculum because they are considered portions of content that can be reapplied to different audiences in different situations. This paper describes the principles that underlie the creation of a Learning Object Science and its application in order to contribute to the concepts of Soils of elementary education.",
        "paperId": "19fed2a28cb68386962454e8e9650b9d1a9fd38e"
    },
    {
        "title": "Extractive Summarization via ChatGPT for Faithful Summary Generation",
        "firstAuthor": "Haopeng Zhang",
        "url": "https://arxiv.org/pdf/2304.04193",
        "dateSubmitted": "2023-04-09",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Extractive summarization is a crucial task in natural language processing that aims to condense long documents into shorter versions by directly extracting sentences. The recent introduction of large language models has attracted significant interest in the NLP community due to its remarkable performance on a wide range of downstream tasks. This paper first presents a thorough evaluation of ChatGPT's performance on extractive summarization and compares it with traditional fine-tuning methods on various benchmark datasets. Our experimental analysis reveals that ChatGPT exhibits inferior extractive summarization performance in terms of ROUGE scores compared to existing supervised systems, while achieving higher performance based on LLM-based evaluation metrics. In addition, we explore the effectiveness of in-context learning and chain-of-thought reasoning for enhancing its performance. Furthermore, we find that applying an extract-then-generate pipeline with ChatGPT yields significant performance improvements over abstractive baselines in terms of summary faithfulness. These observations highlight potential directions for enhancing ChatGPT's capabilities in faithful summarization using two-stage approaches.",
        "paperId": "1a01c982aa20c1a1ad1ad94866e3197da99a52a2"
    },
    {
        "title": "LEARNERS AND TEACHERS\u2019 ATTITUDES TOWARD USING L1 IN ARABIC CLASSES: DOES CONTEXT MATTER?",
        "firstAuthor": "E. Y. Aly",
        "url": null,
        "dateSubmitted": "2020-02-28",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This study aims at exploring learners and teachers\u2019 attitudes toward using students\u2019 L1 in foreign language classes. Research has paid much attention to using L1 in English as a foreign and second language, but other languages such as Arabic is still under-researched, particularly when distinctive contexts of learning are involved. Thus, in addition to teachers vs. learners\u2019 attitudes, and beginner vs. intermediate learners\u2019 attitudes, the study compares two different contexts: learning Arabic in Arabic speaking (Cairo, Egypt) and non-Arabic speaking (Indiana \u2013 USA) countries. Through a triangulation of observation, questionnaires and interviews, the study attempts to explore the black box of attitudes and whether or not learners and teachers believe students\u2019 L1 is useful for Arabic classes. Among the questions the study attempts to answer is whether or not teachers and learners attach any value to L1 use away from its effectiveness, and whether or not the context of learning L2 affects teachers and learners\u2019 attitudes toward L1 use. Article visualizations:",
        "paperId": "1a123fe2b341f2a0c8dfa9bbdef5b2c8f69cb592"
    },
    {
        "title": "Enhancing Chinese Address Parsing in Low-Resource Scenarios through In-Context Learning",
        "firstAuthor": "Guangming Ling",
        "url": "https://www.mdpi.com/2220-9964/12/7/296/pdf?version=1690185528",
        "dateSubmitted": "2023-07-22",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Address parsing is a crucial task in natural language processing, particularly for Chinese addresses. The complex structure and semantic features of Chinese addresses present challenges due to their inherent ambiguity. Additionally, different task scenarios require varying levels of granularity in address components, further complicating the parsing process. To address these challenges and adapt to low-resource environments, we propose CapICL, a novel Chinese address parsing model based on the In-Context Learning (ICL) framework. CapICL leverages a sequence generator, regular expression matching, BERT semantic similarity computation, and Generative Pre-trained Transformer (GPT) modeling to enhance parsing accuracy by incorporating contextual information. We construct the sequence generator using a small annotated dataset, capturing distribution patterns and boundary features of address types to model address structure and semantics, which mitigates interference from unnecessary variations. We introduce the REB\u2013KNN algorithm, which selects similar samples for ICL-based parsing using regular expression matching and BERT semantic similarity computation. The selected samples, raw text, and explanatory text are combined to form prompts and inputted into the GPT model for prediction and address parsing. Experimental results demonstrate significant achievements of CapICL in low-resource environments, reducing dependency on annotated data and computational resources. Our model\u2019s effectiveness, adaptability, and broad application potential are validated, showcasing its positive impact in natural language processing and geographical information systems.",
        "paperId": "1a3715b07636c8396e9d722057c5b052cbf03920"
    },
    {
        "title": "Assessing Motivation to Individualize Reinforcement and Reinforcers for an Intelligent Tutor",
        "firstAuthor": "Elizabeth Lameier",
        "url": null,
        "dateSubmitted": "2017-07-09",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "1a373f05b2b32b2c008a5373fc7c4cd31903ac1b"
    },
    {
        "title": "Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations",
        "firstAuthor": "Lifan Yuan",
        "url": "http://arxiv.org/pdf/2306.04618",
        "dateSubmitted": "2023-06-07",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at \\url{https://github.com/lifan-yuan/OOD_NLP}.",
        "paperId": "1a55d16c14587edda62dc9c9ff09e0b531dd169c"
    },
    {
        "title": "Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models with Discriminators",
        "firstAuthor": "Giwon Hong",
        "url": "http://arxiv.org/pdf/2305.01579",
        "dateSubmitted": "2023-05-02",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Most existing retrieval-augmented language models (LMs) for question answering assume all retrieved information is factually correct. In this work, we study a more realistic scenario in which retrieved documents may contain misinformation, causing conflicts among them. We observe that the existing models are highly brittle to such information in both fine-tuning and in-context few-shot learning settings. We propose approaches to make retrieval-augmented LMs robust to misinformation by explicitly fine-tuning a discriminator or prompting to elicit discrimination capability in GPT-3. Our empirical results on open-domain question answering show that these approaches significantly improve LMs' robustness to knowledge conflicts. We also provide our findings on interleaving the fine-tuned model's decision with the in-context learning process, paving a new path to leverage the best of both worlds.",
        "paperId": "1a62bc8ed9732bcdb6893a11f5cf239640883f87"
    },
    {
        "title": "A Cultural Analysis of Ottoman Algeria (1516-1830): The North \u00e2\u20ac\u201cSouth Mediterranean Progress Gap",
        "firstAuthor": "Tarek Ladjal",
        "url": null,
        "dateSubmitted": "2014-10-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "A number of works have dealt with the socio-political and economic history of Algeria under the Ottoman protectorate; yet the intellectual and cultural life of this period remains poorly explored. We examine the question of \u00e2\u20ac\u02dcprogress\u00e2\u20ac\u2122 against the intellectual and religious life of Ottoman Algeria, analysing the reasons behind the negligent European intellectual influences upon Ottoman Algeria. We review pre-colonial Algeria\u00e2\u20ac\u2122s cultural and intellectual landscape in order to assess the reaction of Algerian society to European ideas originating in the French Revolution and the Enlightenment. Algeria\u00e2\u20ac\u2122s intellectual context, learning system, and the public practice of Sufism contributed significantly to building resistance to European intellectual infiltration and influence, while the European communities in Algeria played a marginal role in shaping Algeria\u00e2\u20ac\u2122s intellectual and cultural life. In spite of its inherent political and geostrategic advantages, Ottoman Algeria failed to achieve a balance between military power and politics in the Mediterranean region, and its own inherent cultural resources.",
        "paperId": "1a6650d8af7a84611d2586aeeaee186c9c3fe9d6"
    },
    {
        "title": "Robust Tracking via Color Names and Spatial Context Learning",
        "firstAuthor": "Guomei Chen",
        "url": null,
        "dateSubmitted": "2015-10-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Although visual tracking is a well-established problem in computer vision, it still remains a challenging task due to many disturbed factors such as appearance changes, illumination changes, partial or full occlusions, rotation, etc. Recently, spatio-temporal context (STC) learning algorithm has been proposed, which exploits the spatio-temporal relationships between the object of interest and its locally dense contexts. However, STC just simple models the spatial correlation between the center point of target and its surrounding regions with the simple low level features (image intensity and position), which ignores the importance of the appearance of the target. In our proposed algorithm, we add color names (CN) which represents the color appearance of the target into the intensity-based STC tracker, take the appearance of the object and the context into consideration simultaneously. Experimental results show that our tracker significantly outperforms several prior art approaches and the STC algorithm on various benchmark videos.",
        "paperId": "1a97578db8c241c1fa6d060b088478e3a33e00f5"
    },
    {
        "title": "Offender Supervision: new directions in theory, research and practice, edited by McNeill, F., Raynor, P. and Trotter, C.",
        "firstAuthor": "P. Raynor",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "1. Introduction: 'What's New and Exciting?', Fergus McNeill, Peter Raynor and Chris Trotter Part One: New directions in theory 2. Viewing Offender Assessment and Rehabilitation Through the Lens of the Risk-Needs-Responsivity Model, Jim Bonta and Don Andrews 3. Strengths and Risks: The Good Lives Model of Offender Rehabilitation, Tony Ward 4. The Desistance Paradigm in Correctional Practice: From Programs to Lives, Shadd Maruna and Thomas P. LeBel Part Two: Staff skills and effective offender supervision 5. Technology Transfer: The Importance of On-Going Clinical Supervision in Translating 'What Works' to Everyday Community Supervision, Guy Bourgon, Jim Bonta, Tanya Rugge and Leticia Gutierrez 6. Skills and strategies in probation supervision: the Jersey study, Peter Raynor, Pamela Ugwudike and Maurice Vanstone 7. Supervision skills in juvenile justice, Chris Trotter and Philippa Evans Part Three: Improving offender supervision 8. The Role of Risk, Needs and Strengths Assessment in Improving Supervision, Hazel Kemshall 9. Managing Chaos: Implementing Evidence Based Practices in Correctional Agencies, Faye S Taxman and Judith Sachwald 10. Can Structured Programmes Improve One-to-One Supervision? Pauline Durrance, Nigel Hosking and Nancy Thorburn 11. Beyond supervision: Judicial involvement in offender management, Gill McIvor Part Four: Significant others and social networks 12. It's relational: Integrating families into community corrections, Carol Shapiro and Margaret DiZerega 13. Justice for all: Family matters in offender supervision, Bas Vogelvang and Herman van Alphen 14. Working with families in criminal justice, Chris Trotter 15. Collaborating with the Community, Trained Volunteers and Faith Traditions: Building Social Capacity and Making Meaning to Support Desistance, Tom O'Connor and Brad Bogue Part Five: Offenders' compliance with supervision 16. Compliance with community penalties: the importance of interactional dynamics, Pamela Ugwudike 17. Case Management in Corrections: Evidence, Issues and Challenges, Shelley Turner 18. The Dynamics of Compliance with Offender Supervision, Gwen Robinson and Fergus McNeill 19. Exploring Community Service, Understanding Compliance, Trish McCulloch Part Six: Offender supervision in its contexts 20. The Socio-Political Context of Reforms in Probation Agencies: Impact on Adoption of Evidence-based Practices, Faye Taxman, Craig Henderson and Jennifer Lerch 21. Revising the National Outcomes and Standards for Criminal Justice Social Work Services in Scotland, Tim Chapman 22. The Purposes of Supervision: Practitioner and Policy Perspectives in England and Wales, John Deering 23. Pre-sentence Reports in England and Wales: Changing Discourses of Need, Risk and Quality, Loraine Gelsthorpe, Peter Raynor and Gwen Robinson 24. Supervision in historical context: Learning the lessons of (oral) history, Fergus McNeill 25. Electronic monitoring: Towards integration into offender management? Mike Nellis 26. Conclusion: Where are we now? Fergus McNeill, Peter Raynor and Chris Trotter",
        "paperId": "1a9c0a7f79fdaa673bafd7ae0376ace1089aef76"
    },
    {
        "title": "Retrieval-Augmented Chain-of-Thought in Semi-structured Domains",
        "firstAuthor": "Vaibhav Mavi",
        "url": null,
        "dateSubmitted": "2023-10-22",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Applying existing question answering (QA) systems to specialized domains like law and finance presents challenges that necessitate domain expertise. Although large language models (LLMs) have shown impressive language comprehension and in-context learning capabilities, their inability to handle very long inputs/contexts is well known. Tasks specific to these domains need significant background knowledge, leading to contexts that can often exceed the maximum length that existing LLMs can process. This study explores leveraging the semi-structured nature of legal and financial data to efficiently retrieve relevant context, enabling the use of LLMs for domain-specialized QA. The resulting system outperforms contemporary models and also provides useful explanations for the answers, encouraging the integration of LLMs into legal and financial NLP systems for future research.",
        "paperId": "1ab033093df6a92fd93d52d09fb6322bb6e306a5"
    },
    {
        "title": "Adversarial Demonstration Attacks on Large Language Models",
        "firstAuthor": "Jiong Wang",
        "url": "http://arxiv.org/pdf/2305.14950",
        "dateSubmitted": "2023-05-24",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs. As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it. To achieve it, we propose the transferable version of advICL, named Transferable-advICL. Our experiment shows that the adversarial demonstration generated by Transferable-advICL can successfully attack the unseen test input examples. We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.",
        "paperId": "1abfc211793c683972ded8d3268475e3ee7a88b0"
    },
    {
        "title": "Analyzing the Impact of Sequential Context Learning on the Transformer Based Korean Text Summarization Model",
        "firstAuthor": "Subin Kim",
        "url": null,
        "dateSubmitted": "2021-10-31",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "1ad2fe95f02a8e92ea9445c2e2830f7d316ee22b"
    },
    {
        "title": "Scalable Multi-Robot Collaboration with Large Language Models: Centralized or Decentralized Systems?",
        "firstAuthor": "Yongchao Chen",
        "url": "https://arxiv.org/pdf/2309.15943",
        "dateSubmitted": "2023-09-27",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "A flurry of recent work has demonstrated that pre-trained large language models (LLMs) can be effective task planners for a variety of single-robot tasks. The planning performance of LLMs is significantly improved via prompting techniques, such as in-context learning or re-prompting with state feedback, placing new importance on the token budget for the context window. An under-explored but natural next direction is to investigate LLMs as multi-robot task planners. However, long-horizon, heterogeneous multi-robot planning introduces new challenges of coordination while also pushing up against the limits of context window length. It is therefore critical to find token-efficient LLM planning frameworks that are also able to reason about the complexities of multi-robot coordination. In this work, we compare the task success rate and token efficiency of four multi-agent communication frameworks (centralized, decentralized, and two hybrid) as applied to four coordination-dependent multi-agent 2D task scenarios for increasing numbers of agents. We find that a hybrid framework achieves better task success rates across all four tasks and scales better to more agents. We further demonstrate the hybrid frameworks in 3D simulations where the vision-to-text problem and dynamical errors are considered. See our project website https://yongchao98.github.io/MIT-REALM-Multi-Robot/ for prompts, videos, and code.",
        "paperId": "1ad735714ad2e4ee5b94ce26c976e5ee5c7cde3b"
    },
    {
        "title": "Multi-strategy tracking based text detection in scene videos",
        "firstAuthor": "Ze-Yu Zuo",
        "url": null,
        "dateSubmitted": "2015-08-23",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Text detection and tracking in scene videos are important prerequisites for content-based video analysis and retrieval, wearable camera systems and mobile devices augmented reality translators. Here, we present a novel multi-strategy tracking based text detection approach in scene videos. In this approach, a state-of-the-art scene text detection module [1] is first used to detect text in each video frame. Then a multi-strategy text tracking technique is proposed, which uses tracking by detection, spatio-temporal context learning, and linear prediction to predict the candidate text location sequentially, and adaptively integrates and selects the best matching text block from the candidate blocks with a rule-based method. This multi-strategy tracking technique can combine the advantages of the three different tracking techniques and afterwards make remedies to the disadvantages of them. Experiments on a variety of scene videos show that our proposed approach is effective and robust to reduce false alarm and improve the accuracy of detection.",
        "paperId": "1b0507d7c1c7a4d60a68b48ed0bc5fbf55925393"
    },
    {
        "title": "Leading in Global-Glocal Missional Contexts: Learning from the Journey of the Wycliffe Global Alliance",
        "firstAuthor": "K. Franklin",
        "url": null,
        "dateSubmitted": "2017-08-17",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The journey of the Wycliffe Global Alliance (WGA) is an example of how some paradigm shifts are influencing leading in mission. Since Christianity is both an agent and product of globalization, its beliefs have spread from one source to another, crossing religious, linguistic and cultural contexts. As a result, there are polycentric or multiple centres of influence since Christianity has homes within a diversity of contexts. This carries with it various implications including how partnering in mission needs to be deconceptualized through greater emphasis on friendship. In order for this to happen as a missiological principle, third spaces may need to be created. Viewed against the backdrop of church and mission agency leadership, structures may be \u2018stuck in the Industrial Era\u2019 (Uhl-Bien et al., 2007: 298). Therefore, the stage is set for exploring how these and other themes influence leadership in God\u2019s mission.",
        "paperId": "1b16f2322b878ff5a837773fd30405bbc78c7193"
    },
    {
        "title": "Desituating Action: Digital Representation of Context",
        "firstAuthor": "J. Grudin",
        "url": "http://www.cs.cmu.edu/~jasonh/courses/ubicomp-sp2007/papers/06-HCI-DesituatingAction-ContextAware.pdf",
        "dateSubmitted": "2001-12-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Many psychological studies have shown that when we act, and especially when we interact, we consciously and unconsciously attend to context of many types. Sensors can pick up some but not all context that is acquired through our senses. Some context is lost, some is added, and captured context is presented in new ways. Digital aggregators and interpreters do not aggregate and interpret the same way we do. Missing or altered context disrupts our processing of information in ways that we may not recognize. To address the disruption we may use additional sensors to capture and deliver some of the missing context. Learning to handle these new conduits is then a further source of disruption, and on it can go. With greater knowledge of context, we can work and interact more efficiently, assuming that we can learn to take advantage of the information without being overwhelmed. However, converting contextual information to a digital format changes it in specific ways. Transient information becomes more permanent, local information is made available globally, and information that once spread slowly can spread much more quickly. The information can enable us to work more efficiently, but these changes in its nature have profound indirect effects. The potential loss of privacy is widely discussed, but other effects may be more significant. In particular, the loss of confinement and transience of information creates an environment that is fundamentally unnatural, in conflict with the one we evolved to live in.",
        "paperId": "1b44bf92389a0fc3f46b84c38b5c370e0aec34c0"
    },
    {
        "title": "EEDN: Enhanced Encoder-Decoder Network with Local and Global Context Learning for POI Recommendation",
        "firstAuthor": "Xinfeng Wang",
        "url": null,
        "dateSubmitted": "2023-07-18",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The point-of-interest (POI) recommendation predicts users' destinations, which might be of interest to users and has attracted considerable attention as one of the major applications in location-based social networks (LBSNs). Recent work on graph-based neural networks (GNN) or matrix factorization-based (MF) approaches has resulted in better representations of users and POIs to forecast users' latent preferences. However, they still suffer from the implicit feedback and cold-start problems of check-in data, as they cannot capture both local and global graph-based relations among users (or POIs) simultaneously, and the cold-start neighbors are not handled properly during graph convolution in GNN. In this paper, we propose an enhanced encoder-decoder network (EEDN) to exploit rich latent features between users, POIs, and interactions between users and POIs for POI recommendation. The encoder of EEDN utilizes a hybrid hypergraph convolution to enhance the aggregation ability of each graph convolution step and learns to derive more robust cold-start-aware user representations. In contrast, the decoder mines local and global interactions by both graph- and sequential-based patterns for modeling implicit feedback, especially to alleviate exposure bias. Extensive experiments in three public real-world datasets demonstrate that EEDN outperforms state-of-the-art methods. Our source codes and data are released at https://github.com/WangXFng/EEDN",
        "paperId": "1b960a8ff97627644fefe6c45414bc3a8315a8a5"
    },
    {
        "title": "Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation",
        "firstAuthor": "Jin-Fang Gao",
        "url": "https://arxiv.org/pdf/2305.07375",
        "dateSubmitted": "2023-05-12",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal explainer. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Thought (CoT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit causality rather than implicit causality, and performs better in sentences with lower event density and smaller lexical distance between events. The code is available on https://github.com/ArrogantL/ChatGPT4CausalReasoning .",
        "paperId": "1b9fc8268b392742ea43c2c017a767cf62386139"
    },
    {
        "title": "Prompting AI Art: An Investigation into the Creative Skill of Prompt Engineering",
        "firstAuthor": "J. Oppenlaender",
        "url": "http://arxiv.org/pdf/2303.13534",
        "dateSubmitted": "2023-03-13",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Humankind is entering a novel era of creativity - an era in which anybody can synthesize digital content. The paradigm under which this revolution takes place is prompt-based learning (or in-context learning). This paradigm has found fruitful application in text-to-image generation where it is being used to synthesize digital images from zero-shot text prompts in natural language for the purpose of creating AI art. This activity is referred to as prompt engineering - the practice of iteratively crafting prompts to generate and improve images. In this paper, we investigate prompt engineering as a novel creative skill for creating prompt-based art. In three studies with participants recruited from a crowdsourcing platform, we explore whether untrained participants could 1) recognize the quality of prompts, 2) write prompts, and 3) improve their prompts. Our results indicate that participants could assess the quality of prompts and respective images. This ability increased with the participants' experience and interest in art. Participants further were able to write prompts in rich descriptive language. However, even though participants were specifically instructed to generate artworks, participants' prompts were missing the specific vocabulary needed to apply a certain style to the generated images. Our results suggest that prompt engineering is a learned skill that requires expertise and practice. Based on our findings and experience with running our studies with participants recruited from a crowdsourcing platform, we provide ten recommendations for conducting experimental research on text-to-image generation and prompt engineering with a paid crowd. Our studies offer a deeper understanding of prompt engineering thereby opening up avenues for research on the future of prompt engineering. We conclude by speculating on four possible futures of prompt engineering.",
        "paperId": "1bc9974780230573bfe9f89789115cb4fbf8bfc6"
    },
    {
        "title": "Robust object tracking with active context learning",
        "firstAuthor": "W. Quan",
        "url": null,
        "dateSubmitted": "2015-10-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "1be4a4c6cd361b40b0c507041e6af0dcfe3183b1"
    },
    {
        "title": "Learning Styles and Learning to Read",
        "firstAuthor": "Vicki E. Snider",
        "url": null,
        "dateSubmitted": "1992-01-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This paper examines the application of learning styles to the teaching of reading. The learning styles approach is based on the premise that learning styles can be assessed and the results can be used to determine instructional methods. Viewed in a historical context, learning style is not a new educational trend, but an extension of the well-worn process approaches that have been largely discredited. The application of learning styles to the teaching of reading is critiqued in light of four factors: (a) inability to adequately assess learning styles, (I$ failure to acknowledge the necessity of phonics instruction for beginning readers, (c) failure to consider the nature of reading disabilities, and (d) lack of convincing research. This critique suggests that the use of learning styles to prescribe methods of reading instruction must be viewed with skebticism.",
        "paperId": "1c516c30869772c85ac8258e3becc529ae132a68"
    },
    {
        "title": "Deep Learning to Predict At-Risk Students\u2019 Achievement in a Preparatory-year English Courses",
        "firstAuthor": "Amnah Al-Sulami",
        "url": null,
        "dateSubmitted": "2023-01-23",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Predicting learners\u2019 final course achievement is most of the time based on the grades they get on their graded course activities. Thus, it is of great importance for both students and higher education institutions to detect risk instances which can be addressed by the academic institution to support students\u2019 success and academic advancement. In this context, Learning Analytics (LA), which represents learners\u2019 behavior inside Learning Management Systems (LMS), and Deep Learning (DL) techniques come into play as academic data, which can be used to predict learners\u2019 future achievements. It is not surprising that at-risk profiling becomes necessary when there are large numbers of students taking a preparatory course online, for example, where instructors fail to monitor their progress in real-time. Thus, the proposed study aims to utilize neural networks (vRNN, LSTM, and GRU); to build models that predict students\u2019 final grade by classifing them as pass or fail based on their assessment grades. In the training process, the three models, alongside a baseline Multilayer Perceptron (MLP) classifier, were trained on four datasets illustrating students\u2019 LMS activity and final grade results in a two-module English preparatory course in King Abdulaziz University (KAU). The datasets were collected during the first and second semesters of 2021. Results indicate that though all of the three DL models performed better than the MLP baseline, the GRU model achieved the highest classification accuracy on three datasets: (ELIA 103-1, 103-2, and 104-1) with the accuracy values of 92.21%, 97.75%, and 94.34%, respectively. On ELIA 104-2 dataset, both vRNN and LSTM achieved 99.89% accuracy. Considering the prediction of at-risk students, the three DL models achieved high recall values ranging from 65.38% to 99.79. %",
        "paperId": "1c9a7277ec1ecc57cd5f706476cb5c47d730d8c0"
    },
    {
        "title": "Explainable Depression Symptom Detection in Social Media",
        "firstAuthor": "Eliseo Bao Souto",
        "url": null,
        "dateSubmitted": "2023-10-20",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Users of social platforms often perceive these sites as supportive spaces to post about their mental health issues. Those conversations contain important traces about individuals' health risks. Recently, researchers have exploited this online information to construct mental health detection models, which aim to identify users at risk on platforms like Twitter, Reddit or Facebook. Most of these models are centred on achieving good classification results, ignoring the explainability and interpretability of the decisions. Recent research has pointed out the importance of using clinical markers, such as the use of symptoms, to improve trust in the computational models by health professionals. In this paper, we propose using transformer-based architectures to detect and explain the appearance of depressive symptom markers in the users' writings. We present two approaches: i) train a model to classify, and another one to explain the classifier's decision separately and ii) unify the two tasks simultaneously using a single model. Additionally, for this latter manner, we also investigated the performance of recent conversational LLMs when using in-context learning. Our natural language explanations enable clinicians to interpret the models' decisions based on validated symptoms, enhancing trust in the automated process. We evaluate our approach using recent symptom-based datasets, employing both offline and expert-in-the-loop metrics to assess the quality of the explanations generated by our models. The experimental results show that it is possible to achieve good classification results while generating interpretable symptom-based explanations.",
        "paperId": "1ca0b2975010c3804ee90be36292f2212443b8e2"
    },
    {
        "title": "\u0421\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u0435\u043b\u044c\u043d\u043e-\u043f\u0435\u0434\u0430\u0433\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e \u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0435 \u0441\u0442\u0443\u0434\u0435\u043d\u0442\u043e\u0432 \u044d\u043a\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0435\u0439",
        "firstAuthor": "\u041c\u0438\u0445\u0430\u0438\u043b \u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440\u043e\u0432\u0438\u0447 \u0420\u043e\u0434\u0438\u043e\u043d\u043e\u0432",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The paper presents an optimal structure of the study and application of mathematics by students of environmental specialities, based on the following principles: i) building on previously learned math topics, ii) context-learning, according to which mathematical modeling activities should be an appropriate component of a real professional environmental specialists, and iii) creative activity, involving not only direct implementation of well-known mathematical procedures, but also some elements of the environmental study. General mathematics is considered as a basic for other structures in course of learning of future ecologists. Applied issues are arisen towards the final stage of education. In parallel already learned mathematical knowledge and skills of students are specified, expanded and deepened. They play as the basis of the mathematical apparatus used directly in solving the environmental problems of different nature.",
        "paperId": "1ca0e67007ca5139cb4213d9f8b5f4929d97c17e"
    },
    {
        "title": "The effect of different levels of realism of context learning on the prescribing competencies of medical students during the clinical clerkship in internal medicine: an exploratory study",
        "firstAuthor": "J. Tichelaar",
        "url": null,
        "dateSubmitted": "2014-12-17",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "1ce649fdabe535c101243915546d3b92d0de9e14"
    },
    {
        "title": "Why Enrol Citizens in the Governance of Nanotechnology",
        "firstAuthor": "Alain Kaufmann",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "1cec6023bf5352a2adb4119c64870767f2b16da5"
    },
    {
        "title": "Transfer of Online Learning to Performance in Variable Application Environments",
        "firstAuthor": "R. Merkley",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "A variable learning application environment exists when learners of different organizations are presented with identical learning content, but are required to apply the newly learned competencies (knowledge, skills, abilities) in a workplace setting differently depending upon the organization\u2019s resources, the general environment, or other characteristics. The variability in learning application environments often requires learners and organizations to expend additional resources to develop the specific competencies required in a particular organizational setting. The context learning strategy instructional methodology (CLIM) attempts to integrate the specific competencies required by the learner\u2019s individual application environment into general competency training. The CLIM not only facilitates individual learning but enables the learner to learn specific job-task competencies for a particular organization. The CLIM design is presented and the first part of a two part study is presented to provide support for the design in an online learning training program.",
        "paperId": "1cecc1aa8192be18c026dd655d41b6e16f221d8c"
    },
    {
        "title": "Role of age, post-training consolidation, and conjunctive associations in the ontogeny of the context preexposure facilitation effect.",
        "firstAuthor": "S. Jablonski",
        "url": "https://europepmc.org/articles/pmc3447086?pdf=render",
        "dateSubmitted": "2012-11-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The context preexposure facilitation effect (CPFE) is a variant of contextual fear conditioning in which context learning and context-shock associations occur on separate occasions. The CPFE with an immediate shock emerges between Postnatal Day (PND) 17 and 24 in the rat and depends on hippocampal NMDA-receptor function in PND 24 rats (Schiffino et al. [2011] Neurobiology of Learning and Memory 95(2):190-198). This study investigated this ontogenetic effect further and reports three findings: First, the CPFE is absent on PND 19 but emerges modestly in rats given exposure on PND 21. Second, the absence of the CPFE on PND 17 does not reflect inability to consolidate the context-shock association established on the training day. Lastly, the CPFE on PND 24 requires exposure to the combined features of the context. These results are the first to show that the early development of contextual fear conditioning depends on conjunctive representations and that processes underlying the CPFE begin to emerge around PND 21 in the rat.",
        "paperId": "1d28254e11b22257943426f99e19430c736bf0aa"
    },
    {
        "title": "Using In-Context Learning to Improve Dialogue Safety",
        "firstAuthor": "Nicholas Meade",
        "url": "http://arxiv.org/pdf/2302.00871",
        "dateSubmitted": "2023-02-02",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, which often perpetuates social biases or stereotypes. We investigate a retrieval-based method for reducing bias and toxicity in responses from chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. We find our method performs competitively with strong baselines without requiring training. For instance, using automatic evaluation, we find our best fine-tuned baseline only generates safe responses to unsafe dialogue contexts from DiaSafety 4.04% more than our approach. Finally, we also propose a re-ranking procedure which can further improve response safeness.",
        "paperId": "1d75f8de31bf47ec46fa5586056420ec8bc97e86"
    },
    {
        "title": "Pedagogical content knowledge (PCK) of prospective biology teacher on respiratory system material to education for sustainable development",
        "firstAuthor": "E. Hartadiyati",
        "url": null,
        "dateSubmitted": "2020-03-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Pedagogical content knowledge is the blending of content knowledge and pedagogical knowledge (PCK) into a specialized knowledge for teaching of specific subjects. In this study, what is meant by PCK for education for sustainable Development is the integration of pedagogy knowledge, knowledge content and three aspects in sustainable development, namely social, economic and environmental together so as to create Education to achieve 17 Sustainable Development goals (SDGs). The purpose of the study was to find out preservice biology teacher\u2019 PCK to Education for Sustainable Development in teaching the respiratory systems. This study was used by two female of prospective biology teachers who were in their final year at the undergraduate level of Biology Education. Data was collected qualitatively through documents of teaching and learning plan and observation during the learning process. Prospective biology of teachers A and B shows the ability to learn respiratory systems that support education for sustainable development. This can be seen in the framework PCK, which is a component of perspectives on economic, social and environmental that is simultaneously and harmonious, material in curriculum context, learning startegy, evaluation and learning outcomes. Leads to the sustainability of present and future life, especially those related to the respiratory system. Learning by teacher A and B aligns with the achievement of goals of Sustainable Development.",
        "paperId": "1da22caec46b396fbfe973c7248e98deca131723"
    },
    {
        "title": "How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?",
        "firstAuthor": "Xin Xu",
        "url": "http://arxiv.org/pdf/2305.01555",
        "dateSubmitted": "2023-05-02",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in https://github.com/zjunlp/DeepKE/tree/main/example/llm.",
        "paperId": "1ddeb500dd88d4b860b32bec1e2a85f8a53910d6"
    },
    {
        "title": "Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash Detection with Large Language Model",
        "firstAuthor": "Zhe Liu",
        "url": null,
        "dateSubmitted": "2023-10-24",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Mobile applications have become a ubiquitous part of our daily life, providing users with access to various services and utilities. Text input, as an important interaction channel between users and applications, plays an important role in core functionality such as search queries, authentication, messaging, etc. However, certain special text (e.g., -18 for Font Size) can cause the app to crash, and generating diversified unusual inputs for fully testing the app is highly demanded. Nevertheless, this is also challenging due to the combination of explosion dilemma, high context sensitivity, and complex constraint relations. This paper proposes InputBlaster which leverages the LLM to automatically generate unusual text inputs for mobile app crash detection. It formulates the unusual inputs generation problem as a task of producing a set of test generators, each of which can yield a batch of unusual text inputs under the same mutation rule. In detail, InputBlaster leverages LLM to produce the test generators together with the mutation rules serving as the reasoning chain, and utilizes the in-context learning schema to demonstrate the LLM with examples for boosting the performance. InputBlaster is evaluated on 36 text input widgets with cash bugs involving 31 popular Android apps, and results show that it achieves 78% bug detection rate, with 136% higher than the best baseline. Besides, we integrate it with the automated GUI testing tool and detect 37 unseen crashes in real-world apps from Google Play.",
        "paperId": "1ded7b62f6946ab27390ffd3c2feb386abf4bebf"
    },
    {
        "title": "Board 411 - Research Abstract An Interprofessional Education Approach to the Care of Tracheostomy Patients with Speaking Valves: Learning Together through Hybrid Simulation (Submission #1381)",
        "firstAuthor": "A. Rudd",
        "url": null,
        "dateSubmitted": "2013-12-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Introduction/Background Historically, respiratory therapists (RT), speech language pathologists (SLP) and nurses (RN) are trained separately and function independently. Collaboration among these disciplines is essential for optimized patient care and patient communication, particularly for management of tracheostomy patients who need speaking valves. Students learning through interprofessional education (IPE) absorb more than the content, including developing an understanding of professional roles and backgrounds while practicing multi-disciplinary communication.1 Systemic reviews of IPE reflect that it fosters positive interactions and improves attitudes; however, findings are difficult to interpret because projects are diverse, measurement of findings varies, and data is lacking.2 The purpose of this study is to implement an IPE simulation involving RT, SLP and RN students to enhance content mastery related to the interdisciplinary care of tracheostomy patients with speaking valves. Interdisciplinary care is essential for this patient population because optimal levels of patient communication must be balanced with adequate physiologic stability. The following research question is guiding this study: Is there a difference in role understanding and knowledge of clinical indications related to speaking valves, as measured by a pre-test/post-test, after an interprofessional simulation experience with nursing, respiratory therapy and speech language-pathology students? The hypothesis is that there is enhanced understanding of roles and clinical knowledge after an IPE approach using simulation. Upon completion of this simulation, students were able to: 1) understand the roles of RTs, SLPs, and RNs in caring for patients with speaking valves, 2) explain the clinical indications and benefits of speaking valves, 3) demonstrate clinical skills using high fidelity patient simulation and 4) evaluate interdisciplinary collaboration and teamwork. Methods Participants included 90 traditional undergraduate RN students, 22 graduate SLP students and 24 undergraduate RT students. The IPE experience consisted of 3 phases: training, simulation, and debriefing. In the training phase, each class of students viewed a one hour video training module created and presented by Passy-Muir, Inc. Training included anatomical structures, tracheostomy care and indications, benefits and application of speaking valves. For the simulation phase, students were presented with a realistic acute care environment containing a high-fidelity simulator as the patient and standardized patient as the family member. Small groups comprised of students from each discipline worked together in a scenario that included evaluation of patient, tracheostomy care and speaking valve placement, with an emphasis on interprofessional teamwork and patient communication. For the debriefing phase, faculty facilitated student group discussion of their simulation experience. Students were asked to provide reflective evaluation of clinical performance and collaborative care. To measure student understanding of the roles of the disciplines, procedures for speaking valve initiation, and benefits of speaking valves, a pre and post-test was given. An attitudinal survey was used to measure students\u2019 feelings and perceptions toward IPE and IPE using simulation. Results One hundred eleven students participated in the simulation project. An overall net increase of 18% was attained on the knowledge pre and post-test scores with p.<.000. Qualitative statements made by the students indicate the students: increased the value of collaboration within the healthcare system; improved interdisciplinary collaboration/communication skills; increased understanding of the roles of the various healthcare disciplines; and increased knowledge/comfort related to managing a patient with a tracheostomy and speaking valve. Ninety-seven percent of the students wanted to participate in additional interdisciplinary simulations. Conclusion Results of this study support the hypothesis that there is enhanced understanding of roles and knowledge of clinical indications for speaking valves. An implication of this study is that it supports the integration of IPE in health professions education. Qualitative data and Results of the attitudinal survey indicate that students appreciate IPE and support the idea of additional simulations with other disciplines. References 1. Clark, P.G: What would a theory of interprofessional education look like? Some suggestions for developing a theoretical framework for teamwork training. Journal of Interprofessional Care 2006; 20:577-589 Interprofessional Education Collaborative (IPEC) Expert Panel. Core competencies for interprofessional collaborative practice. http://www.aacn.nche.edu/education-resources/ipecreport.pdf. Accessed July 2012\\. 2. Thistlewaite, J: Interprofessional education: A review of the context, learning, and the research agenda. Medical Education 2012; 46:58-70. Disclosures Pharmaxis Hill Rom, Pharmaxis, Teleflex Medical, Boehringer Ingleheim.",
        "paperId": "1df5e458dd99b8cde40d24880ee8bcd5eadf155d"
    },
    {
        "title": "Self-Prompting Large Language Models for Open-Domain QA",
        "firstAuthor": "Junlong Li",
        "url": "http://arxiv.org/pdf/2212.08635",
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Open-Domain Question Answering (ODQA) requires models to answer factoid questions with no context given. The common way for this task is to train models on a large-scale annotated dataset to retrieve related documents and generate answers based on these documents. In this paper, we show that the ODQA architecture can be dramatically simpli\ufb01ed by treating Large Language Models (LLMs) as a knowledge corpus and propose a Self-Prompting framework for LLMs to perform ODQA so as to eliminate the need for training data and external knowledge corpus. Concretely, we \ufb01rstly generate multiple pseudo QA pairs with background passages and one-sentence explanations for these QAs by prompting LLMs step by step and then leverage the generated QA pairs for in-context learning. Experimental results show our method surpasses previous state-of-the-art methods by +8.8 EM averagely on three widely-used ODQA datasets, and even achieves comparable performance with several retrieval-augmented \ufb01ne-tuned models.",
        "paperId": "1e122149779c644855d1cccca5d96135db0482cb"
    },
    {
        "title": "LayoutPrompter: Awaken the Design Ability of Large Language Models",
        "firstAuthor": "Jiawei Lin",
        "url": null,
        "dateSubmitted": "2023-11-11",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Conditional graphic layout generation, which automatically maps user constraints to high-quality layouts, has attracted widespread attention today. Although recent works have achieved promising performance, the lack of versatility and data efficiency hinders their practical applications. In this work, we propose LayoutPrompter, which leverages large language models (LLMs) to address the above problems through in-context learning. LayoutPrompter is made up of three key components, namely input-output serialization, dynamic exemplar selection and layout ranking. Specifically, the input-output serialization component meticulously designs the input and output formats for each layout generation task. Dynamic exemplar selection is responsible for selecting the most helpful prompting exemplars for a given input. And a layout ranker is used to pick the highest quality layout from multiple outputs of LLMs. We conduct experiments on all existing layout generation tasks using four public datasets. Despite the simplicity of our approach, experimental results show that LayoutPrompter can compete with or even outperform state-of-the-art approaches on these tasks without any model training or fine-tuning. This demonstrates the effectiveness of this versatile and training-free approach. In addition, the ablation studies show that LayoutPrompter is significantly superior to the training-based baseline in a low-data regime, further indicating the data efficiency of LayoutPrompter. Our project is available at https://github.com/microsoft/LayoutGeneration/tree/main/LayoutPrompter.",
        "paperId": "1e19a260e771a3d46dfb6a93954071f22b7b564d"
    },
    {
        "title": "In-Context Learning for MIMO Equalization Using Transformer-Based Sequence Models",
        "firstAuthor": "Matteo Zecchin",
        "url": null,
        "dateSubmitted": "2023-11-10",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large pre-trained sequence models, such as transformer-based architectures, have been recently shown to have the capacity to carry out in-context learning (ICL). In ICL, a decision on a new input is made via a direct mapping of the input and of a few examples from the given task, serving as the task's context, to the output variable. No explicit updates of model parameters are needed to tailor the decision to a new task. Pre-training, which amounts to a form of meta-learning, is based on the observation of examples from several related tasks. Prior work has shown ICL capabilities for linear regression. In this study, we leverage ICL to address the inverse problem of multiple-input and multiple-output (MIMO) equalization based on a context given by pilot symbols. A task is defined by the unknown fading channel and by the signal-to-noise ratio (SNR) level, which may be known. To highlight the practical potential of the approach, we allow for the presence of quantization of the received signals. We demonstrate via numerical results that transformer-based ICL has a threshold behavior, whereby, as the number of pre-training tasks grows, the performance switches from that of a minimum mean squared error (MMSE) equalizer with a prior determined by the pre-trained tasks to that of an MMSE equalizer with the true data-generating prior.",
        "paperId": "1e41cd665678de96fa5b6f1a7497b16ada5260f5"
    },
    {
        "title": "Language , tools and brain : The ontogeny and phylogeny of hierarchically organized sequential behavior",
        "firstAuthor": "J. Andreae",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "of the original article: During the first two years of human life a common neural substrate (roughly Broca's area) underlies the hierarchical organization of elements in the development of speech as well as the capacity to combine objects manually, including tool use. Subsequent cortical differentiation, beginning at age two, creates distinct, relatively modularized capacities for linguistic grammar and more complex combination of objects. An evolutionary homologue of the neural substrate for language production and manual action is hypothesized to have provided a foundation for the evolution of language before the divergence of the hominids and the great apes. Support comes from the discovery of a Broca's area homologue and related neural circuits in contemporary primates. In addition, chimpanzees have an identical constraint on hierarchical complexity in both tool use and symbol combination. Their performance matches that of the two-year-old child who has not yet developed the neural circuits for complex grammar and complex manual combination of objects. Associative learning and task complexity John H. Andreae and Shaun W. Ryan andreae@elec.canterbury.ac.nz; Department of Electrical and Electronic Engineering, University of Canterbury, Christchurch, New Zealand Greenfield (1991t), in her far-ranging target article, has used tree diagrams, common in linguistics, to compare hierarchical relations in language and object-manipulation tasks and to explain the developmental differentiation that occurs in Broca's area. In particular, she has related the nesting of cups by young children with the combination of words into simple sentences. Our work with a robot learning system suggests that there may be no simple relation between tree diagrams of task complexity and the neural implementation of the association of events. Also, we shall argue that a more basic reason for the early interaction of language and object manipulation may be that each is necessary for the other. It will further be suggested that they appear to separate at a later stage only because the secondary function (the language that assists object manipulation and the manipulation actions that assist language) becomes disconnected from its motor area. Previous commentators have already pointed out that not only are there difficulties with choosing an appropriate tree diagram for a task (Bickerton 1991; Greenfield 1991r; Tomasello 1991) but the demands of the task depend critically on processing load (Bloom 1991). The \"multiple context\" approach, which is used in our experiments with a robot learning system, suggests a more cautious interpretation of Greenfield's evidence for Broca's area. First, we must present a minimum of background information. The reader will not need more than a superficial understanding of the robot learning system in order to follow the argument. Full details are readily available in Andreae (1972-1991) and Andreae and MacDonald (1991). 1. Preliminaries. The design of our robot learning system is based on the assumption that different areas of the (robot) brain allow different types of events to be associated. The associations allowed by a particular area are prescribed by an (innate) production template, which has a context-template part and a predicted-event-type part. If in the robot's experience events occur with types corresponding to the types in the context template and these are immediately followed by an event of the predicted-event type, then a production is stored in long-term memory (LTM) comprising the events of the context and the event of the predicted-event type. In an example of this (taken from the 5-puzzle task mentioned below) the robot \"said\" the sound \"Position-1\" which is of type \"speech'; it then did an eyeaction \"LookAtGame' and sensed its eye position with the eyestimulus \"Eye-4,\" thus generating an event sequence \"Position-1 LookAtGame Eye-4.\" The next event was the robot saying the speech sound \"Macro-1-4,\" which is the name of the macro for putting a tile at position 4 into its correction position 1. Because the robot had among its 17 production templates the template \"I\" (the templates are named by letter) and this sequence of events (which were among those in short-term memory, STM) matches the template, the production 136 is stored in LTM: Events in STM matched the production template [I: . . speech .eye-action .eye-stimulus >> speech ], to specify the production [136: Position-1 LookAtGame Eye-4 \u00bb Macro-1-4] where 136 means the 36th production from template I and the symbol > > separates context from prediction. The event types in the template are preceded by dots. One dot means an event from the last action-stimulus step of the robot. Two dots mean an event from two steps back, and so on. In this case, when Macro-1-4 was in the current step (no dot), LookAtGame and Eye-4 were in the last step (one dot) and Position-1 was in the one-before-last step (two dots): all the events match the types and dots of the template. Notice that the context-template part of template I has three events and the oldest event has two dots. We will refer to the maximum number of dots of an event in a template as being the context's dot-depth. In some experiments, templates have events preceded by commas instead of dots. A comma indicates that the event is the previous event of its type, two commas the one previous to that, and so on. The maximum number of commas preceding an event of a context will be referred to as the comma-depth. Very roughly, the robot starts learning with no productions (tabula rasa) but a full complement of fixed production templates and enough built-in reflexes to enable it to get going. When the robot is unable to choose its own action either by reflex or by matching productions, a teacher helps. We have not experimented with the maturational effects that might be achieved by introducing templates gradually. When the context of a stored production matches the events held in STM, the predicted event of the stored production is either a candidate for action \u00a9 1994 Cambridge University Press 0140-525X194 $5.00+.00 357 Continuing Commentary selection or a possible stimulus from the environment, depending upon whether it is an action or a stimulus. These productionmatches provide the robot with (1) actions to preform and (2) predictions of how the environment will respond. Every new production stored is treated as a goal (novelty goal) until it is matched again. A network of transition probabilities between stored productions is used to calculate the expectations of reaching a goal via alternative actions and thus bias the robot's selection of actions. The robot constructs plans by predicting, alternatively, its own action(s) and the environment's response as stimuli, until the predictions fail or loop (no plan) or the sequence reaches a novelty goal (good plan). 2. Hierarchical tasks. An experiment carried out recently (Andreae 1990) is relevant to the Greenfield argument. The robot is taught the 5-puzzIe (a two-row version of the well-known 8-puzzle) as a task with 5 levels of subtask. In order to get tiles placed for consecutive positions of the board (level 1), the appropriate macro-operator (Korf 1985) has to be selected (level 2) for each position. Within each macro, the robot has to perform a series of rotations (level 3). Each rotation comprises a sequence of shifts (level 4). Each shift is a move-grab-move-release-move sequence (level 5). The robot has to remember where it is in each of the subtasks in order to leave the subtask and return to the higher level correctly. We discovered that 4 templates of depth 5 (independent of number of subtask levels), 12 templates of less depth, and one auxiliary action enable the robot to learn the complete task and without further learning to repeat it for any requested order of the tiles. The auxiliary action is called \"raise-eyebrows' and could be any action that is not a hand action or speech. The robot uses minimal \"speech' interleaved with the hand actions. Complete details with program and data are provided by Andreae (1990). An earlier experiment showed that a multiple context learning system could learn to mimic a Universal Turing Machine (UTM) including the extendable tape (MacDonald & Andreae 1981). This was done totally within the system and used 9 templates, of which only 4 had the maximum dot-depth of 2, and 4 had the maximum comma-depth of 1. Implementation of a multiple context system in the form of neural circuits has been outlined by Andreae (1977). We doubt that enough is known about neural circuitry to compare, for example, the ease of representing dot and comma events in a context. Unlike the situation with hierarchical tree diagrams, however, the multiple context description suggests precise electrical circuits (gates, comparators, and shift-registers) that can be translated into neural circuitry when more is known about the latter. For the time being, it is probably reasonable to expect that the complexity of neural circuitry would correspond roughly to the complexity of the electrical circuitry, which in turn depends on the dotand comma-depths. The main point we wish to make concerns the way task hierarchies are and led in the two experiments referred to above. In neither case is the multiple context domain-specific. In the 5-puzzle experiment, the multiple context allows the holding of temporary information and this is used for the subtaskinjj. In the UTM experiment, the multiple context simulates in memory an alterable and extendable tape for the UTM, into which any problem with any hierarchy can be entered. In both cases, an unlimited variety of task hierarchies can be accommodated, so they could each qualify as Greenfield's \"supramodal hierarchical processor\" in Broca's area. Their neural implementations would be very different, however, because of their different dotand comma-depths. We have to conclude that only a detail",
        "paperId": "1e9c3a3f8266d51c2aaabf7708081231f7142f67"
    },
    {
        "title": "Efficient multi-atlas abdominal segmentation on clinically acquired CT with SIMPLE context learning",
        "firstAuthor": "Zhoubing Xu",
        "url": "https://europepmc.org/articles/pmc4532551?pdf=render",
        "dateSubmitted": "2015-08-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "1ea7819e4540e496c5289f40d4c7529640a2ad0f"
    },
    {
        "title": "Boosting on the shoulders of giants in quantum device calibration",
        "firstAuthor": "A. Wozniakowski",
        "url": null,
        "dateSubmitted": "2020-05-13",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Traditional machine learning applications, such as optical character recognition, arose from the inability to explicitly program a computer to perform a routine task. In this context, learning algorithms usually derive a model exclusively from the evidence present in a massive dataset. Yet in some scientific disciplines, obtaining an abundance of data is an impractical luxury, however; there is an explicit model of the domain based upon previous scientific discoveries. Here we introduce a new approach to machine learning that is able to leverage prior scientific discoveries in order to improve generalizability over a scientific model. We show its efficacy in predicting the entire energy spectrum of a Hamiltonian on a superconducting quantum device, a key task in present quantum computer calibration. Our accuracy surpasses the current state-of-the-art by over $20\\%.$ Our approach thus demonstrates how artificial intelligence can be further enhanced by \"standing on the shoulders of giants.\"",
        "paperId": "1ebede2e8cb9831211a0377f7c7f088febf42d24"
    },
    {
        "title": "STC Tracking Algorithm Based on Kalman Filter",
        "firstAuthor": "Panqiao Chen",
        "url": "https://download.atlantis-press.com/article/25850281.pdf",
        "dateSubmitted": "2016-03-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "During object tracking, Fast tracking via Spatio-Temporal Context Learning which combines temporal correlation among sequential frames and spatial correlation between object and background can solve the problem of semi-occlusion, but not full-occlusion. Kalman Filter makes use of the predictive value and measurement to calculate the optimal state. This paper aims at solving the full-occlusion problems by combining the algorithm and Kalman Filter together. Experiments show that the improved STC can solve occlusion problems effectively.",
        "paperId": "1f89cff99ff3d6b3890a1933547be58523163746"
    },
    {
        "title": "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model",
        "firstAuthor": "Seongjin Shin",
        "url": "http://arxiv.org/pdf/2204.13509",
        "dateSubmitted": "2022-04-28",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning performance changes as the training corpus varies. Here, we investigate the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From our in-depth investigation, we introduce the following observations: (1) in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning, (2) in-context learning ability can emerge when a language model is trained on a combination of multiple corpora, even when each corpus does not result in in-context learning on its own, (3) pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in the few-shot setting, and (4) the relationship between language modeling (measured in perplexity) and in-context learning does not always correlate: e.g., low perplexity does not always imply high in-context few-shot learning performance.",
        "paperId": "1fafaccebc4a74898a74c606f846318c4c2c7536"
    },
    {
        "title": "Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment",
        "firstAuthor": "Eshaan Tanwar",
        "url": "http://arxiv.org/pdf/2305.05940",
        "dateSubmitted": "2023-05-10",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of ICL for cross-lingual text classification. We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy \u2014 Cross-lingual In-context Source Target Alignment (X-InSTA). With an injected coherence in the semantics of the input examples and a task-based alignment across the source and target languages, X-InSTA is able to outperform random prompt selection by a large margin across three different tasks using 44 different cross-lingual pairs.",
        "paperId": "1fb5a5298747b8c7d60f98640a543f20d42ab053"
    },
    {
        "title": "Cognitive development in infancy : a reader",
        "firstAuthor": "J. Oates",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The period of human development from birth to eighteen months holds a unique attraction for psychologists interested in cognitive development. It is not only that many look to these early stages for evidence of the origins of later abilities such as language, but also that questions about the interactions of environmental circumstances and genetic transmission often come to focus on infancy. This reader contains a selection of edited papers that present major findings of the last 20 years of infancy research and reflect currently active research lines. It includes readings on motor development, cultural context, learning, perception, concept formation, the object concept, early social behaviout and continuity. Prepared as a source book for a third-level Open University course, Cognitive Development, this volume will also be of interest to other people interested in learning more about the abilities and mental processes of infants, and about current research and theory in these areas. It will be particularly appropriate for use in undergraduate and gradute courses in psychology with a developmental context.",
        "paperId": "1fbf313c2121b2d472fd2bdc501dfe5960a1a7a3"
    },
    {
        "title": "Learning and Knowledge",
        "firstAuthor": "Armin Trost",
        "url": null,
        "dateSubmitted": "2019-10-19",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "1fc9f6a70495049c01ed585d28cb57414d7da274"
    },
    {
        "title": "Link-Context Learning for Multimodal LLMs",
        "firstAuthor": "Yan Tai",
        "url": "https://arxiv.org/pdf/2308.07891",
        "dateSubmitted": "2023-08-15",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\"from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes\"reasoning from cause and effect\"to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.",
        "paperId": "1fd31b74f5e1eeb67341982fd35a613c6fad10e0"
    },
    {
        "title": "A Structural Model for CASE Adoption Behavior",
        "firstAuthor": "Arun Rai",
        "url": null,
        "dateSubmitted": "1996-09-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The adoption rate of computer-aided software engineering (CASE) technology continues to be low among information systems departments (ISDs). Some ISDs have reported significant hurdles in propagating CASE usage, while documenting the advantages of the technology. We construct and empirically test a theoretical model to explain CASE adoption behavior. Factors considered include need pull (environmental instability of the ISD and performance gap of the ISD), technology push (internal experimentation and learning from external information sources), and the adoption context (top-management support for the IS function, CASE championship, training availability, and job/role rotation). A national survey of 2,700 ISDs resulted in 405 usable responses for the data analysis.Our analysis suggests a reasonable fit between the model and the data. The results indicate that the need-pull factors do not directly promote CASE adoption behavior. Performance deficit promotes CASE championship behavior while negatively affecting other elements of the adoption context. The instability of ISDs, where the very existence of the ISD may be in question, negatively affects all elements of the adoption context. Learning about CASE from external information sources directly promotes CASE adoption. Both technology push factors positively affect all four elements of the adoption context. Of the contextual elements, CASE training availability, CASE championship, and job/role rotation positively affect CASE adoption behavior. Top management support does not affect CASE adoption behavior, which suggests that such support may be more critical for postadoption stages of the diffusion process.",
        "paperId": "1ff4a310aa1ec251b91fd21a3b1e053de70b7e66"
    },
    {
        "title": "The vowel path: Learning about vowel representation in written Hebrew",
        "firstAuthor": "D. Ravid",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This study probes Hebrew-speaking children\u2019s knowledge about vowel representation by diacritics and by vowel letters in emergent literacy stages, and how this knowledge changes with formal instruction in first grade. Sixteen kindergartners and sixteen first graders were administered two reading tasks, two meta-linguistic explanation tasks, and two writing tasks involving vowels in different phonological and orthographic constructions and morphological roles. The results show that kindergartners already have some knowledge about writing consonants, and proceed to meaningful reading and writing in first grade through learning about writing vowels. Reading and writing syllables were easier than word reading and writing in kindergarten, and converged in first grade. Explaining vowel differences in words was better than in syllables, since children made use of the lexical context. Learning to represent morphemes at word final position by vowel letters emerged gradually in kindergarten, in the following order: H, standing for a and e, then W and Y, representing o, u and i. The development of vowel letters is shown to be dependent on a variety of considerations \u2014 orthographic, morphological, phonological, and perceptual. These results are discussed in the context of general theories about the consolidation of spelling.",
        "paperId": "200b397f6be8d73b347a22f8d6fafbefa2788f00"
    },
    {
        "title": "Boosting In-Context Learning with Factual Knowledge",
        "firstAuthor": "J. Wang",
        "url": "https://arxiv.org/pdf/2309.14771",
        "dateSubmitted": "2023-09-26",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In-Context Learning (ICL) over Large language models (LLMs) aims at solving previously unseen tasks by conditioning on a few training examples, eliminating the need for parameter updates and achieving competitive performance. In this paper, we demonstrate that factual knowledge is imperative for the performance of ICL in three core facets, i.e., the inherent knowledge learned in LLMs, the factual knowledge derived from the selected in-context examples, and the knowledge biases in LLMs for output generation. To unleash the power of LLMs in few-shot learning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further improve the performance of ICL: 1) injecting factual knowledge to LLMs during continual self-supervised pre-training, 2) judiciously selecting the examples with high knowledge relevance, and 3) calibrating the prediction results based on prior knowledge. We evaluate the proposed approaches on auto-regressive LLMs (e.g., GPT-style models) over multiple text classification and question answering tasks. Experimental results demonstrate that KICT substantially outperforms strong baselines, and improves by more than 13% and 7% of accuracy on text classification and question answering tasks, respectively.",
        "paperId": "20177a85f632a34d085bcf645507e461733fcc96"
    },
    {
        "title": "Central and peripheral vision loss differentially affects contextual cueing in visual search.",
        "firstAuthor": "F. Geringswald",
        "url": null,
        "dateSubmitted": "2015-04-13",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Visual search for targets in repeated displays is more efficient than search for the same targets in random distractor layouts. Previous work has shown that this contextual cueing is severely impaired under central vision loss. Here, we investigated whether central vision loss, simulated with gaze-contingent displays, prevents the incidental learning of contextual cues or the expression of learning, that is, the guidance of search by learned target-distractor configurations. Visual search with a central scotoma reduced contextual cueing both with respect to search times and gaze parameters. However, when the scotoma was subsequently removed, contextual cueing was observed in a comparable magnitude as for controls who had searched without scotoma simulation throughout the experiment. This indicated that search with a central scotoma did not prevent incidental context learning, but interfered with search guidance by learned contexts. We discuss the role of visuospatial working memory load as source of this interference. In contrast to central vision loss, peripheral vision loss was expected to prevent spatial configuration learning itself, because the restricted search window did not allow the integration of invariant local configurations with the global display layout. This expectation was confirmed in that visual search with a simulated peripheral scotoma eliminated contextual cueing not only in the initial learning phase with scotoma, but also in the subsequent test phase without scotoma.",
        "paperId": "20404115c658db62592990f139069727bf0beadc"
    },
    {
        "title": "Tacit knowledge transfer and spillover learning in ramp-ups",
        "firstAuthor": "P. Letmathe",
        "url": null,
        "dateSubmitted": "2019-12-06",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "With shortening product life cycles and an increasing number of product variants, manufacturing firms perform more production ramp-ups. In this context, learning is crucially important to quickly achieve high production process quality and stability. The paper aims to discuss this issue.,Through a laboratory experiment, this study analyzes spillover learning between consecutive ramp-ups and how this phenomenon is influenced by tacit knowledge transfer through observation and imitation.,The results prove the existence of spillover learning between consecutive ramp-ups. Moreover, they provide evidence how tacit knowledge transfer through observation and imitation enhances learning of new tasks in consecutive production ramp-ups.,Future research could focus on the specific psychological processes driving tacit knowledge transfer and spillover learning, a topic which is only touched upon in this paper.,The findings show that manufacturing firms should not only aim at reaching a steep learning curve during a single production ramp-up, but should also take into account the effects of spillover learning with regard to future production ramp-ups. Furthermore, the paper provides novel insights concerning the allocation of workers to production tasks with regard to previous experience when introducing new personnel and during ramp-up phases.,Previous evidence on the existence and characteristics of spillover learning in production ramp-up situations is not conclusive. This paper provides new and unambiguous insights by considering different organizational settings.",
        "paperId": "2068a5bbd349b6e5ccbd3087de322279ae5c31b4"
    },
    {
        "title": "Context Learning with the Self Organizing",
        "firstAuthor": "M. Varsta",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In this paper a Recurrent Self-Organizing Map (RSOM) algorithm is proposed for temporal sequence processing. The RSOM algorithm is close in nature to the Kohonen's Self-Organizing Map, except that in the RSOM context of the temporal sequence is involved both in the best matching unit nding and in the adaptation of the weight vectors of the map via an introduced recursive di erence equation associated for each unit of the map. The experimental results in the paper demonstrate that the RSOM is able to learn and distinguish temporal sequences, and that the RSOM algorithm can be utilized, for instance, in electroencephalogram (EEG) based epileptic activity detection.",
        "paperId": "208917995f36fd78409797bd43ffc8ccc4287ba4"
    },
    {
        "title": "Conversation Style Transfer using Few-Shot Learning",
        "firstAuthor": "Shamik Roy",
        "url": "https://arxiv.org/pdf/2302.08362",
        "dateSubmitted": "2023-02-16",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Conventional text style transfer approaches focus on sentence-level style transfer without considering contextual information, and the style is described with attributes (e.g., formality). When applying style transfer in conversations such as task-oriented dialogues, existing approaches suffer from these limitations as context can play an important role and the style attributes are often difficult to define in conversations. In this paper, we introduce conversation style transfer as a few-shot learning problem, where the model learns to perform style transfer by observing only a few example dialogues in the target style. We propose a novel in-context learning approach to solve the task with style-free dialogues as a pivot. Human evaluation shows that by incorporating multi-turn context, the model is able to match the target style while having better appropriateness and semantic correctness compared to utterance/sentence-level style transfer. Additionally, we show that conversation style transfer can also benefit downstream tasks. For example, in multi-domain intent classification tasks, the F1 scores improve after transferring the style of training data to match the style of the test data.",
        "paperId": "20ab124b3e1a5770c7816e2f78dfd5d64de447e0"
    },
    {
        "title": "Don\u2019t Judge an Object by Its Context: Learning to Overcome Contextual Bias",
        "firstAuthor": "Krishna Kumar Singh",
        "url": "https://arxiv.org/pdf/2001.03152",
        "dateSubmitted": "2020-01-09",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Existing models often leverage co-occurrences between objects and their context to improve recognition accuracy. However, strongly relying on context risks a model's generalizability, especially when typical co-occurrence patterns are absent. This work focuses on addressing such contextual biases to improve the robustness of the learnt feature representations. Our goal is to accurately recognize a category in the absence of its context, without compromising on performance when it co-occurs with context. Our key idea is to decorrelate feature representations of a category from its co-occurring context. We achieve this by learning a feature subspace that explicitly represents categories occurring in the absence of context along side a joint feature subspace that represents both categories and context. Our very simple yet effective method is extensible to two multi-label tasks -- object and attribute classification. On 4 challenging datasets, we demonstrate the effectiveness of our method in reducing contextual bias.",
        "paperId": "20b0462acebf2bf859d14a53a020b21448cfbebf"
    },
    {
        "title": "Detecting Cheapfakes using Self-Query Adaptive-Context Learning",
        "firstAuthor": "Kha-Luan Pham",
        "url": "https://dl.acm.org/doi/pdf/10.1145/3592571.3592972",
        "dateSubmitted": "2023-06-12",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Detecting Cheapfakes often requires identifying contextual changes in media resulting from misleading captions. Cheapfake media can be created either by manipulating the content using image or video editing software, or by altering the context of an image or video through misleading claims, without relying on any software. While previous research has shown promising results, these approaches are limited to the data used during training. To overcome this limitation, we propose a Self-Query Adaptive-Context Learning method that is flexible and capable of adapting to new contexts during inference by using image search engine queries to enrich its knowledge. By verifying the context of captions based on the collected information, our approach extends knowledge in a flexible manner. Despite achieving an experimental accuracy of 59.70% on the IEEE ICME 2023 Cheapfakes Challenge dataset, our work has opened up new avenues for detecting out-of-context misuses.",
        "paperId": "20b0846cb94c8405e2da3aaab933a3649b14a802"
    },
    {
        "title": "Semantic LO for customized e-learning : university education",
        "firstAuthor": "A. Polzonetti",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The purpose of this paper is to provide an awareness of the elements that should be considered in quality LO instructional design for e-learning systems, taking into account certain classifications to improve their management. To achieve this, we explain issues relating to the LO characteristics which help to improve their quality for suitable management. On this basis we suggest some issues to be taken into account in order to obtain a good LO design with some classifications that could be considered for an application profile in order to improve LO management. Finally we describes our approach to realize personalized e-Learning in the Semantic Web with the development of our Customized Context Learning framework.",
        "paperId": "20b292210266f7d66ee11bb14a0be7635cc70b0d"
    },
    {
        "title": "Exploring Multi-Modal Contextual Knowledge for Open-Vocabulary Object Detection",
        "firstAuthor": "Yifan Xu",
        "url": "https://arxiv.org/pdf/2308.15846",
        "dateSubmitted": "2023-08-30",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In this paper, we for the first time explore helpful multi-modal contextual knowledge to understand novel categories for open-vocabulary object detection (OVD). The multi-modal contextual knowledge stands for the joint relationship across regions and words. However, it is challenging to incorporate such multi-modal contextual knowledge into OVD. The reason is that previous detection frameworks fail to jointly model multi-modal contextual knowledge, as object detectors only support vision inputs and no caption description is provided at test time. To this end, we propose a multi-modal contextual knowledge distillation framework, MMC-Det, to transfer the learned contextual knowledge from a teacher fusion transformer with diverse multi-modal masked language modeling (D-MLM) to a student detector. The diverse multi-modal masked language modeling is realized by an object divergence constraint upon traditional multi-modal masked language modeling (MLM), in order to extract fine-grained region-level visual contexts, which are vital to object detection. Extensive experiments performed upon various detection datasets show the effectiveness of our multi-modal context learning strategy, where our approach well outperforms the recent state-of-the-art methods.",
        "paperId": "20b7ae9eef2ffe4bbf25c2f8b5d130b81aa9d366"
    },
    {
        "title": "Modeling the Impact of Travel Information on Activity\u2013Travel Rescheduling Decisions under Conditions of Travel Time Uncertainty",
        "firstAuthor": "Zhongwei Sun",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This study elaborates on a model system developed in earlier papers to predict the perceived value and use of travel information. The value of travel information is conceptualized as the extent to which the information allows the individual to make better activity\u2013travel scheduling decisions at the beginning of the day and during execution of the schedule. By taking the broader schedule context into account, the model is sensitive to the impact of information and decisions on the full activity\u2013travel pattern. Furthermore, the model includes Bayesian mechanisms to make sure that beliefs about travel times, and other uncertain events, and the credibility of the information source are updated each time information is received and the real travel time is experienced. This paper describes the results of numerical simulations conducted to illustrate the system and to derive theoretical implications from the model. The simulations show that the schedule context, learning, and expected information gain in combination determine the perceived value of information and that none of these factors can be ignored in the derivation of estimates of these values. A theoretical analysis further shows how decision trees can be pruned to reduce a potential problem of combinatorics.",
        "paperId": "20cf28ce0a510eb616b7eeae4a68486a3e075eb5"
    },
    {
        "title": "The enhancement of mathematical problem-solving skill and self-efficacy achievement through thinking actively in a social context learning model",
        "firstAuthor": "W. Septiyana",
        "url": null,
        "dateSubmitted": "2019-11-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Mathematical problem-solving skill becomes the main tool of learning mathematics to develop thinking habits and establishes students\u2019 self-efficacy in their skill to face problems. Thinking actively in a social context (TASC) learning model is expected to enhance this skill. This research was quasi experiment by non-equivalent control group design with seventh grade students in one of Bandung Junior High School as a research subject. The number of participants was 52 students. The instruments that used were problem solving skill essay and self-efficacy questionnaires. This research generated the enhancement of problem-solving skill students who receive TASC learning model are higher significantly than students who receive scientific approach learning. Looking back for the whole piece of learning is not already become a student habit at the evaluate stage. Less accuracy of the students in understanding part of the problem indicated errors resulted from inappropriate strategy.",
        "paperId": "20cf6346830882c67f73eaeb719311dff82e4006"
    },
    {
        "title": "Metadata quality in digital repositories: Empirical results from the cross\u2010domain transfer of a quality assurance process",
        "firstAuthor": "N. Palavitsinis",
        "url": null,
        "dateSubmitted": "2014-06-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Metadata quality presents a challenge faced by many digital repositories. There is a variety of proposed quality assurance frameworks applied in repositories that are deployed in various contexts. Although studies report that there is an improvement of the quality of the metadata in many of the applications, the transfer of a successful approach from one application context to another has not been studied to a satisfactory extent. This article presents the empirical results of the application of a metadata quality assurance process that has been developed and successfully applied in an educational context (learning repositories) to 2 different application contexts to compare results with the previous application and assess its generalizability. More specifically, it reports results from the adaptation and application of this process in a library context (institutional repositories) and in a cultural context (digital cultural repositories). Initial empirical findings indicate that content providers seem to be gaining a better understanding of metadata when the proposed process is put in place and that the quality of the produced metadata records increases.",
        "paperId": "20d429c4fcf476f8a1d6e525b09c0390f629006f"
    },
    {
        "title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
        "firstAuthor": "Taicheng Guo",
        "url": null,
        "dateSubmitted": "2023-05-27",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper, rather than pursuing state-of-the-art performance, we aim to evaluate capabilities of LLMs in a wide range of tasks across the chemistry domain. We identify three key chemistry-related capabilities including understanding, reasoning and explaining to explore in LLMs and establish a benchmark containing eight chemistry tasks. Our analysis draws on widely recognized datasets facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts. Our investigation found that GPT-4 outperformed other models and LLMs exhibit different competitive levels in eight chemistry tasks. In addition to the key findings from the comprehensive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs' performance across various chemistry tasks. The code and datasets used in this study are available at https://github.com/ChemFoundationModels/ChemLLMBench.",
        "paperId": "20d7965c0b282a0cd7f990e435d0f6bc9535bbc6"
    },
    {
        "title": "Language Modeling Is Compression",
        "firstAuthor": "Gr'egoire Del'etang",
        "url": "https://arxiv.org/pdf/2309.10668",
        "dateSubmitted": "2023-09-19",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively. Finally, we show that the prediction-compression equivalence allows us to use any compressor (like gzip) to build a conditional generative model.",
        "paperId": "21091f8133ab034baacb92fdb958e14989eb427f"
    },
    {
        "title": "ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?",
        "firstAuthor": "Michael Heck",
        "url": "http://arxiv.org/pdf/2306.01386",
        "dateSubmitted": "2023-06-02",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Recent research on dialog state tracking (DST) focuses on methods that allow few- and zero-shot transfer to new domains or schemas. However, performance gains heavily depend on aggressive data augmentation and fine-tuning of ever larger language model based architectures. In contrast, general purpose language models, trained on large amounts of diverse data, hold the promise of solving any kind of task without task-specific training. We present preliminary experimental results on the ChatGPT research preview, showing that ChatGPT achieves state-of-the-art performance in zero-shot DST. Despite our findings, we argue that properties inherent to general purpose models limit their ability to replace specialized systems. We further theorize that the in-context learning capabilities of such models will likely become powerful tools to support the development of dedicated dialog state trackers and enable dynamic methods.",
        "paperId": "214fbadc57e954e325dc055fee5ac0e224dfde11"
    },
    {
        "title": "Why Are Acquired Search-Guiding Context Memories Resistant to Updating?",
        "firstAuthor": "T. Geyer",
        "url": "https://www.frontiersin.org/articles/10.3389/fpsyg.2021.650245/pdf",
        "dateSubmitted": "2021-03-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Looking for goal-relevant objects in our various environments is one of the most ubiquitous tasks the human visual system has to accomplish (Wolfe, 1998). Visual search is guided by a number of separable selective-attention mechanisms that can be categorized as bottom-up driven \u2013 guidance by salient physical properties of the current stimuli \u2013 or top-down controlled \u2013 guidance by observers' \u201conline\u201d knowledge of search-critical object properties (e.g., Liesefeld and M\u00fcller, 2019). In addition, observers' expectations based on past experience also play also a significant role in goal-directed visual selection. Because sensory environments are typically stable, it is beneficial for the visual system to extract and learn the environmental regularities that are predictive of (the location of) the target stimulus. This perspective article is concerned with one of these predictive mechanisms: statistical context learning of consistent spatial patterns of target and distractor items in visual search. We review recent studies on context learning and its adaptability to incorporate consistent changes, with the aim to provide new directions to the study of processes involved in the acquisition of search-guiding context memories and their adaptation to consistent contextual changes \u2013 from a three-pronged, psychological, computational, and neurobiological perspective.",
        "paperId": "2157dff2dd857b24f89b3b79e70659a04dfc2c27"
    },
    {
        "title": "Multi-task transfer learning for biomedical machine reading comprehension",
        "firstAuthor": "Wenyang Guo",
        "url": null,
        "dateSubmitted": "2020-06-22",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Biomedical machine reading comprehension aims to extract the answer to the given question from complex biomedical passages, which requires the machine to have the ability to process strong comprehension on natural language. Recent progress has made on this task, but still severely restricted by the insufficient training data due to the domain-specific nature. To solve this problem, we propose a hierarchical question-aware context learning model trained by the multi-task transfer learning algorithm, which can capture the interaction between the question and the passage layer by layer, with multi-level embeddings to strengthen the ability of the language representation. The multi-task transfer learning algorithm leverages the advantages of different machine reading comprehension tasks to improve model generalisation and robustness, pre-training on multiple large-scale open-domain data sets and fine-tuning on the target-domain training set. Moreover, data augmentation is also adopted to create new training samples with various expressions. The public biomedical data set collected from PubMed provided by BioASQ is used to evaluate the model performance. The results show that our method is superior to the best recent solution and achieves a new state of the art.",
        "paperId": "21a1f9af151d5681492fa65442bdd7e153252a54"
    },
    {
        "title": "Social Presence Awareness for Knowledge Transformation in a Mobile Learning Environment",
        "firstAuthor": "R. Kekwaletswe",
        "url": null,
        "dateSubmitted": "2007-12-24",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Knowledge sharing occurs between humans, rather than being a human-computer process. Knowledge transformation is an outcome of individuals\u2019 knowledge-sharing experiences. Social interaction is central to both knowledge transformation and to learning. When learners intermingle, there may be a shift in knowledge due to the social interaction. Consequently, knowledge transformation is a social process that occurs when there is an interaction among learners. In a contact university, learners perform tasks in three locations: formal contexts, semi-formal contexts and informal contexts. Learning tasks are presumed to be constant but a mobile learner carries the tasks across different environments. However, as learners move across different contexts, they do not have access to the same social networks for sharing knowledge and learning experiences. The paper conceptualises a mobile learning environment that provides social presence awareness as a learner traverses different learning contexts. It highlights how through synchronous mobile instant messaging, social presence provides learners with continuous awareness of available social support, thus facilitating the on-demand and opportunistic sharing of knowledge.",
        "paperId": "21a675433ed522d3f8c7b12ae1ed1ed39da706c1"
    },
    {
        "title": "MultiCLU: Multi-stage Context Learning and Utilization for Storefront Accessibility Detection and Evaluation",
        "firstAuthor": "X. Wang",
        "url": null,
        "dateSubmitted": "2022-06-27",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In this work, a storefront accessibility image dataset is collected from Google street view and is labeled with three main objects for storefront accessibility: doors (for store entrances), doorknobs (for accessing the entrances) and stairs (for leading to the entrances). Then MultiCLU, a new multi-stage context learning and utilization approach, is proposed with the following four stages: Context in Labeling (CIL), Context in Training (CIT), Context in Detection (CID) and Context in Evaluation (CIE). The CIL stage automatically extends the label for each knob to include more local contextual information. In the CIT stage, a deep learning method is used to project the visual information extracted by a Faster R-CNN based object detector to semantic space generated by a Graph Convolutional Network. The CID stage uses the spatial relation reasoning between categories to refine the confidence score. Finally in the CIE stage, a new loose evaluation metric for storefront accessibility, especially for knob category, is proposed to efficiently help BLV users to find estimated knob locations. Our experiment results show that the proposed MultiCLU framework can achieve significantly better performance than the baseline detector using Faster R-CNN, with +13.4% on mAP and +15.8% on recall, respectively. Our new evaluation metric also introduces a new way to evaluate storefront accessibility objects, which could benefit BLV group in real life.",
        "paperId": "21be33adb4824ca8fb87be9919ed4ecf61062521"
    },
    {
        "title": "Supervised learning of sparse context reconstruction coefficients for data representation and classification",
        "firstAuthor": "Xuejie Liu",
        "url": "https://arxiv.org/pdf/1508.04221",
        "dateSubmitted": "2015-08-18",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "21c4e12ea4917752e809e6cca71f925c28c10a3b"
    },
    {
        "title": "Context memory formation requires activity-dependent protein degradation in the hippocampus",
        "firstAuthor": "Patrick K. Cullen",
        "url": "http://learnmem.cshlp.org/content/24/11/589.full.pdf",
        "dateSubmitted": "2017-11-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Numerous studies have indicated that the consolidation of contextual fear memories supported by an aversive outcome like footshock requires de novo protein synthesis as well as protein degradation mediated by the ubiquitin\u2013proteasome system (UPS). Context memory formed in the absence of an aversive stimulus by simple exposure to a novel environment requires de novo protein synthesis in both the dorsal (dHPC) and ventral (vHPC) hippocampus. However, the role of UPS-mediated protein degradation in the consolidation of context memory in the absence of a strong aversive stimulus has not been investigated. In the present study, we used the context preexposure facilitation effect (CPFE) procedure, which allows for the dissociation of context learning from context\u2013shock learning, to investigate the role of activity-dependent protein degradation in the dHPC and vHPC during the formation of a context memory. We report that blocking protein degradation with the proteasome inhibitor clasto-lactacystin \u03b2-lactone (\u03b2Lac) or blocking protein synthesis with anisomycin (ANI) immediately after context preexposure significantly impaired context memory formation. Additionally, we examined 20S proteasome activity at different time points following context exposure and saw that the activity of proteasomes in the dHPC increases immediately after stimulus exposure while the vHPC exhibits a biphasic pattern of proteolytic activity. Taken together, these data suggest that the requirement of increased proteolysis during memory consolidation is not driven by processes triggered by the strong aversive outcome (i.e., shock) normally used to support fear conditioning.",
        "paperId": "21cab1f05c63566d9d9faee864188e6132f146c2"
    },
    {
        "title": "On Compression of Machine-Derived Context Sets for Fusion of Multi-modal Sensor Data",
        "firstAuthor": "Nurali Virani",
        "url": null,
        "dateSubmitted": "2018-11-13",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "21e24ed83b952675c87a9d4d624008db9ff1d55e"
    },
    {
        "title": "Recurring Concepts and Meta-learning",
        "firstAuthor": "J. Gama",
        "url": null,
        "dateSubmitted": "2010-08-17",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This work addresses data stream mining from dynamic\nenvironments where the distribution underlying the observations\nmay change over time. In these contexts, learning algorithms\nmust be equipped with change detection mechanisms. Several\nmethods have been proposed able to detect and react to concept\ndrift. When a drift is signaled, most of the approaches use a\nforgetting mechanism, by releasing the current model, and start\nlearning a new decision model. Nevertheless, it is not rare for\nthe concepts from history to reappear, for example seasonal\nchanges. In this work we present a method that memorizes learnt\ndecision models whenever a concept drift is signaled. The\nsystem uses meta-learning techniques that characterize the\ndomain of applicability of previous learnt models. The\nmeta-learner can detect re-occurrence of contexts and take\npro-active actions by activating previous learnt models. The\nmain benefit of this approach is that the proposed meta-learner\nis capable of selecting similar historical concept, if there is\none, without the knowledge of true classes of examples.",
        "paperId": "2208963e0152bef71a0500d798b5a0f5982b0558"
    },
    {
        "title": "The User Preference Learning for Multi-agent Based on Neural Network in Ubiquitous Computing Environment",
        "firstAuthor": "Eungyeong Kim",
        "url": null,
        "dateSubmitted": "2008-03-26",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "222618597560558637eba7ea4ea28e875b6d65ac"
    },
    {
        "title": "Creating Constructivist Learning Environment: Role of \"Web 2.0\" Technology",
        "firstAuthor": "M. Paily",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "[Abstract]Contemporary educational practices encourage teaching practices grounded in the principles of constructivism. In a constructivist learning situation learners bring unique prior experiences and beliefs and knowledge is constructed uniquely and individually, in multiple ways, using a variety of tools, resources, and contexts. New developments in the area of Information and communication Technology (ICT) in general and \"Web 2.0\" in particular have provided variety of tools and resources for designing and delivering instruction based on the constructivist principles. The terms \"Web 2.0\" refer to Web-based utilities and technology tools that focus on social, collaborative, user-driven content and applications. These among other things include blogs, wikis, multimedia sharing services, content syndication, podcasting and content tagging services. This emerging technology which is characterized by greater functionality, interoperability and connectivity helps in knowledge creation through open communication and collaboration. The adoption level of these emerging web technologies is on the rise in academic settings. There are also multiple instructional design models based on constructivist pedagogy having the scope to integrate most of the \"Web 2.0\" technologies. This paper elaborates upon various \"Web 2.0\" tools and its integration in the designing process to create a constructivist learning environment. A sample \"Web 2.0\" integrated constructivist learning plan based on the 5E approach is also provided.[Keywords] constructivism; multimedia; web technology; constructivist learning environmentConstructivist Learning EnvironmentConstructivist theory has its roots in a number of disciplines, including philosophy, anthropology, psychology, sociology, and education. Entrenched in learning theories advanced by Dewey, Piaget, Vygotsky, Bruner, and Glasersfeld, the essential element of constructivism is active construction of new knowledge by the learner based on their experiences. In a constructivist learning situation learners bring unique prior knowledge and beliefs and knowledge is constructed uniquely and individually, in multiple ways, using a variety of tools, resources, and contexts. Learning is both an active and reflective process. The learners' knowledge structure is expanded through the process of assimilation and accommodation facilitated by multiple perspectives from more knowledgeable others. Though the meaning making takes place through social interaction and collaboration, learning is internally controlled and mediated by the learner.Murphy (1997, in the section of \"characteristics of constructivist learning and teaching\") based on the analysis of work by Jonassen (1991, 1994), Wilson and Cole (1991), Ernest (1995), Honebein (1996), and Vygotsky (1978) synthezised and summarized the characteristics of constructivist learning and teaching.These characteristics are as follows:1 . Multiple perspectives and representations of concepts and content are presented and encouraged.2. Goals and objectives are derived by the student or in negotiation with the teacher or system.3. Teachers serve in the role of guides, monitors, coaches, tutors and facilitators.4. Activities, opportunities, tools and environments are provided to encourage meta-cognition, selfanalysis -regulation, -reflection & -awareness.5. The student plays a central role in mediating and controlling learning.6. Learning situations, environments, skills, content and tasks are relevant, realistic, authentic and represent the natural complexities of the 'real world'.7. Primary sources of data are used in order to ensure authenticity and real-world complexity.8. Knowledge construction and not reproduction is emphasized.9. This construction takes place in individual contexts and through social negotiation, collaboration and experience.10. The learner's previous knowledge constructions, beliefs and attitudes are considered in the knowledge construction process. \u2026",
        "paperId": "2265a428584f8c82b3376cfc608fa27c24c8d0a7"
    },
    {
        "title": "The Effects ofMethods on EFL Vocabulary Learning : The Roles ofContexts , Collocations and Ttanslations fatsuo ANEZAKI",
        "firstAuthor": "",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The present study aims to clarify the effects of learning methods on EFL vocabulary learning. Three methods were compared, namely Centext Learnin& Collocation (COL) learning and Translation Learning, to learn ten a{ljectives, [riwo tests, an lrnmediate and a delayed test, were conducted. Halfofthe contexts given in the leaming phase were changed in the test phase to evaluate the effeets oftransfer 144 subjects participating in this tudy were divided into two proficiency leyels, advariced and weak. The results indicated that COL Learning and Translation Learr}ing perforrned better than Context Learning on the immediate test and that COL Learning performed better than Context Learning on the delayed test. The resuks suggest that actvanced leaniers in COL LearTiing scored higheir than those in Translation Learning and Context Learning and that weak learners in Translation Leaining and COL Learning scored higiier than those ln Context Learning.",
        "paperId": "2267804d247436c6c25d370a56102e9a9974bcb4"
    },
    {
        "title": "Differentially Private In-Context Learning",
        "firstAuthor": "Ashwinee Panda",
        "url": "https://arxiv.org/pdf/2305.01639",
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "An important question in deploying large language models (LLMs) is how to augment LLMs with private data. We propose Differentially Private In-context Learning (DP-ICL) to enable LLMs to adapt to new tasks while maintaining privacy guarantees. DP-ICL performs private inference by establishing a noisy consensus over an ensemble of exemplars using the Report-Noisy-Max mechanism. We evaluate DP-ICL on four benchmarks and find that it achieves comparable performance (< 2% degradation) with non-private ICL.",
        "paperId": "227dcfb8f289b9629997da8572cfa84a3a016e2e"
    },
    {
        "title": "A Situative Perspective on Developing Writing Pedagogy in a Teacher Professional Learning Community.",
        "firstAuthor": "S. Pella",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The bulk of current research on teacher professional development is focused on teacher learning in the context of teacher professional learning communities (PLCs). In teacher PLCs, groups of teachers meet regularly to increase their own learning and the learning of their students. Teacher PLCs offer a learning model in which, \"new ideas and strategies emerge, take root, and develop, and where competence can be truly cultivated and nurtured\" (Lieberman & Miller, 2008, p. 2). Findings from this research suggests that teacher PLCs can lead to long-term capacity development and gains in student achievement (DuFour & Eaker, 1998; Grossman, Wineburg, & Woolworth, 2001; Lieberman, & Miller, 2008; Lieberman, & Wood, 2003; McLaughlin & Talbert, 2006; Stoll, Bolam, McMahon, Wallace, & Thomas, 2006). Research on teacher professional development has recognized the nature of situated learning in the context of teacher PLCs (Putnam & Borko, 2000). Situative perspectives of teacher learning can provide a multi-focal research lens, affording the study of multiple units of analysis: the individuals, the community context, and the social interactions of teachers as they develop knowledge for teaching (Borko, 2004; Putnam & Borko, 2000). According to situated learning theory posited by Jean Lave (1996), as researchers approach the study of learning as a situated process, learning is not characterized exclusively in terms of knowledge acquisition or outcomes. Instead, by focusing on the interactions in and across particular social and physical contexts, learning is a process of social engagement or participation in a community of practice (Lave & Wenger, 1991). In this study, I focused on the situated nature of teacher learning in a PLC that was based on the lesson study model for teacher professional development. My primary units of analysis were the engagements of four middle school language arts teachers as they participated in a lesson study focused on teaching and learning writing. I defined engagements as participants' interactions with their own and each others' prior and locally shared experiences, forms of knowledge, and material resources. I selected this focus based on the situative analytic methods suggested by Lemke (1997) in his ecosocial systems model, where he suggests that the primary units of analysis are not things or people, but processes and practices. According to his views on situated cognition theory, Lemke (1997) posited that an ecosocial system includes not only humans in their situated physical environment, but also the social practices, meaning relations, and all interactions between humans and their material ecosystems. My focus on participants' engagements also included a widened lens through which I studied how participants interacted with the features of the locally adapted teacher PLC model. These multiple foci involved my use of an integrated theoretical approach that combined social learning theory, situated cognition, and the principles of constructivism. As suggested by Borko (2004), \"The ability to use multiple frameworks at the same time is a key strength of situative research perspectives\" (p. 8). By foregrounding and detailing participants' engagements, I sought to provide a fuller, more complex account of how this locally designed teacher PLC fostered transformations in teachers' perceptions and pedagogy. Research Questions This study addressed the following research questions: What is the nature of participants' situated engagements in their collaborative inquiry about teaching and learning writing? How did these engagements contribute to transformations in teacher perspectives and pedagogy? My findings are discussed in the following themes, which emerged from the data: (a) Participants synthesized their own and each others' prior knowledge, experiences, and resources from diverse theoretical frameworks in teaching and learning writing. \u2026",
        "paperId": "22916bb7dc514231bf901e8158834dc078cf7c20"
    },
    {
        "title": "Stock Broad-Index Trend Patterns Learning via Domain Knowledge Informed Generative Network",
        "firstAuthor": "Jingyi Gu",
        "url": "http://arxiv.org/pdf/2302.14164",
        "dateSubmitted": "2023-02-27",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Predicting the Stock movement attracts much attention from both industry and academia. Despite such significant efforts, the results remain unsatisfactory due to the inherently complicated nature of the stock market driven by factors including supply and demand, the state of the economy, the political climate, and even irrational human behavior. Recently, Generative Adversarial Networks (GAN) have been extended for time series data; however, robust methods are primarily for synthetic series generation, which fall short for appropriate stock prediction. This is because existing GANs for stock applications suffer from mode collapse and only consider one-step prediction, thus underutilizing the potential of GAN. Furthermore, merging news and market volatility are neglected in current GANs. To address these issues, we exploit expert domain knowledge in finance and, for the first time, attempt to formulate stock movement prediction into a Wasserstein GAN framework for multi-step prediction. We propose Index GAN, which includes deliberate designs for the inherent characteristics of the stock market, leverages news context learning to thoroughly investigate textual information and develop an attentive seq2seq learning network that captures the temporal dependency among stock prices, news, and market sentiment. We also utilize the critic to approximate the Wasserstein distance between actual and predicted sequences and develop a rolling strategy for deployment that mitigates noise from the financial market. Extensive experiments are conducted on real-world broad-based indices, demonstrating the superior performance of our architecture over other state-of-the-art baselines, also validating all its contributing components.",
        "paperId": "22954d41a130b023653b7255637bf58251ece492"
    },
    {
        "title": "Learning Space Management Nurul Islam Nature School (SANI) Jember to Cultivate Children's Spirituality",
        "firstAuthor": "Istifadah Istifadah",
        "url": "https://obsesi.or.id/index.php/obsesi/article/download/2669/pdf",
        "dateSubmitted": "2022-06-19",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Learning space is a fundamental need in education. The availability of space for educational institutions is a necessity to accommodate all learning activities. This is no exception for early childhood education. The purpose of this study was to describe and analyze the management of the learning space to foster the value of children's spirituality. The research was conducted at the Nurul Islam Islamic School (SANI) at the Nurul Islam Islamic Boarding School Jember. The study focused on the analysis of the learning space for early childhood based on the theory of \u00a0(Taylor et al., 2009), as well as several other relevant theories. Data was extracted through observation, interviews, documentation and photography and analyzed in three steps: data condensation, data presentation, and drawing conclusions or verification based on source triangulation. In this study, it was found that the management of the SANI learning space is a development of Taylor theory, namely the content, context, learning process, and spiritual values of the pesantren tradition in managing the SANI learning space",
        "paperId": "22ad938aba8803b64a486436a17a3e18326e6b11"
    },
    {
        "title": "Adult distance learners in distance education : a literature review",
        "firstAuthor": "Yunsheng Cao",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Distance education is a current reality creating new opportunities and challenges for educational institutions, offering students expanded choices in where, when, how and from whom they learn, and making education accessible to a large population, especially for adult learners. This paper intends to present and discuss some of the most current research on the issues which influence adult distance learners' learning process. And help the school administrators, distance course designers and distance instructors to truly understand adult distance learners and their learning needs; design an effective and efficient learning environment and experience; facilitate adult learners' learning, provide learners' support to enhance adult learners' success in the higher distance education. These issues include analyzing adult learners' characteristics. Two kinds of adult distance learners' characteristics are presented within this paper. They are demographic and situational characteristics, such as age, gender, cultural background, disability, location, and life roles, and affective characteristics, such as personality type, learning styles, and motivation. Some concerns about issues that impact on supporting adult distance learners' learning process and how to support adult distance learning are discussed. These issues include understanding adult learners and their learning needs, such as their knowledge, prior skills, experience, culture, context, learning patterns and styles. Strategies of supporting learning from the aspects of course planning and instruction, technology support and interaction and feedback are also presented in this paper. Additionally, this paper also addressed adult distance learner's responsibility as an important essence in their distance learning process.",
        "paperId": "22b6c576ad76064e28d27b29ba19e186b6d714a6"
    },
    {
        "title": "Writing in Foreign Language Contexts: Learning, Teaching, and Research, Rosa Manch\u00f3n (Ed.). Multilingual Matters, Bristol, UK (2009). 296 pp.",
        "firstAuthor": "J. Bar\u00f3n",
        "url": null,
        "dateSubmitted": "2010-03-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": null,
        "paperId": "2312c4819504d3397fc8a856bbff9b500b7717ec"
    },
    {
        "title": "Pol\u00edticas de supervivencia: el imperativo de subsistir en un centro universitario en una c\u00e1rcel de la Provincia de Buenos Aires",
        "firstAuthor": "Carolina Emilia Di Pr\u00f3spero",
        "url": null,
        "dateSubmitted": "2020-10-31",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "The daily development of activities in a University Center that is located within a prison is marked by the deployment of a variety of practices performed by the actors involved (students, teachers, agents) aimed at facilitating the articulation between some restrictive actions towards security and others that bid for the expansion of rights. This paper aims to present some aspects of the process of circulation of educational rights in confinement contexts: learning practices and since that, the agency of subsistence conditions. \nMethodologically, we use ethnography, prioritizing the description and analysis of activities of imprisoned students and the meanings that they attributed to them.",
        "paperId": "231d6a9a979d40d7f2400cacea50b40dabbb9453"
    },
    {
        "title": "Dynamic trees for streaming and massive data contexts",
        "firstAuthor": "C. Anagnostopoulos",
        "url": null,
        "dateSubmitted": "2012-01-26",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Data collection at a massive scale is becoming ubiquitous in a wide variety of settings, from vast offline databases to streaming real-time information. Learning algorithms deployed in such contexts must rely on single-pass inference, where the data history is never revisited. In streaming contexts, learning must also be temporally adaptive to remain up-to-date against unforeseen changes in the data generating mechanism. Although rapidly growing, the online Bayesian inference literature remains challenged by massive data and transient, evolving data streams. Non-parametric modelling techniques can prove particularly ill-suited, as the complexity of the model is allowed to increase with the sample size. In this work, we take steps to overcome these challenges by porting standard streaming techniques, like data discarding and downweighting, into a fully Bayesian framework via the use of informative priors and active learning heuristics. We showcase our methods by augmenting a modern non-parametric modelling framework, dynamic trees, and illustrate its performance on a number of practical examples. The end product is a powerful streaming regression and classification tool, whose performance compares favourably to the state-of-the-art.",
        "paperId": "235a911587fcaefd7eabfcbef501e8eabd64b7df"
    },
    {
        "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models",
        "firstAuthor": "Huiqiang Jiang",
        "url": "https://arxiv.org/pdf/2310.05736",
        "dateSubmitted": "2023-10-09",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss. Our code is available at https://aka.ms/LLMLingua.",
        "paperId": "2392b6d3a5cad9e5cf349169eaeee848266adf6a"
    },
    {
        "title": "Modular reservoir computing networks for imitation learning of multiple robot behaviors",
        "firstAuthor": "Tim Waegeman",
        "url": null,
        "dateSubmitted": "2009-12-15",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Autonomous mobile robots must accomplish tasks in unknown and noisy environments. In this context, learning robot behaviors in an imitation based approach would be desirable in the perspective of service robotics as well as of learning robots. In this work, we use Reservoir Computing (RC) for learning robot behaviors by demonstration. In RC, a randomly generated recurrent neural network, the reservoir, projects the input to a dynamic temporal space. The reservoir states are mapped into a readout output layer which is the solely part being trained using standard linear regression. In this paper, we use a two layered modular structure, where the first layer comprises two RC networks, each one for learning primitive behaviors, namely, obstacle avoidance and target seeking. The second layer is composed of one RC network for behavior combination and coordination. The hierarchical RC network learns by examples given by simple controllers which implement the primitive behaviors. We use a simulation model of the e-puck robot which has distance sensors and a camera that serves as input for our system. The experiments show that, after training, the robot learns to coordinate the Goal Seeking (GS) and the Object Avoidance (OA) behaviors in unknown environments, being able to capture targets and navigate efficiently.",
        "paperId": "23eea94464690e57f9e96b4940ee6b9c21e773b5"
    },
    {
        "title": "Efficacy of Focused Group Discussion on Knowledge and Practices Related to Menstruation among Adolescent Girls of Rural Areas of Rhtc of a Medical College: An Interventional Study",
        "firstAuthor": "P. Kokiwar",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Context: Learning about menstrual hygiene is vital part of health education for adolescent girls so that they can continue to work and maintain hygienic habits throughout their adult life. Aim: The aim is to study the efficacy of focused group discussion (FGD) in comparison to didactic lecture method (DL) on knowledge and practices related to menstruation among adolescent girls of rural areas. Materials and Methods: Community-based interventional study was carried out among 260 adolescent girls. Knowledge and practice were assessed with the help of questionnaire preintervention. During intervention, 130 girls in DL group were given DL and 130 girls in FGD received FGD. After 2 months, all 260 girls were contacted. Their knowledge and practices were assessed using same questionnaire. Statistical Analysis: Yates corrected Chi-square and Student's t-test was used. Results: Both methods were equally effective in increasing mean knowledge and practices score (P < 0.05). On comparison of mean postintervention scores between the two groups for knowledge, the FGD method was found to be superior (P < 0.05) but not for practices (P > 0.05). Conclusion: FGD was more effective than DL method for knowledge related to menstruation but not for practices.",
        "paperId": "24050e2934414f9bc6349f280b385ea3f7d92f28"
    },
    {
        "title": "Efficient abdominal segmentation on clinically acquired CT with SIMPLE context learning",
        "firstAuthor": "Zhoubing Xu",
        "url": "https://europepmc.org/articles/pmc4405802?pdf=render",
        "dateSubmitted": "2015-03-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Abdominal segmentation on clinically acquired computed tomography (CT) has been a challenging problem given the inter-subject variance of human abdomens and complex 3-D relationships among organs. Multi-atlas segmentation (MAS) provides a potentially robust solution by leveraging label atlases via image registration and statistical fusion. We posit that the efficiency of atlas selection requires further exploration in the context of substantial registration errors. The selective and iterative method for performance level estimation (SIMPLE) method is a MAS technique integrating atlas selection and label fusion that has proven effective for prostate radiotherapy planning. Herein, we revisit atlas selection and fusion techniques for segmenting 12 abdominal structures using clinically acquired CT. Using a re-derived SIMPLE algorithm, we show that performance on multi-organ classification can be improved by accounting for exogenous information through Bayesian priors (so called context learning). These innovations are integrated with the joint label fusion (JLF) approach to reduce the impact of correlated errors among selected atlases for each organ, and a graph cut technique is used to regularize the combined segmentation. In a study of 100 subjects, the proposed method outperformed other comparable MAS approaches, including majority vote, SIMPLE, JLF, and the Wolz locally weighted vote technique. The proposed technique provides consistent improvement over state-of-the-art approaches (median improvement of 7.0% and 16.2% in DSC over JLF and Wolz, respectively) and moves toward efficient segmentation of large-scale clinically acquired CT data for biomarker screening, surgical navigation, and data mining.",
        "paperId": "2412103ab8334a3dee8d0686c5bb10fb0eefd621"
    },
    {
        "title": "Learning Users Habits : The APE",
        "firstAuthor": "ProjectJean",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This paper proposes new results in the eld of software assistants helping users of interactive tools in the task of automatically performing repetitive tasks. We propose an innovative integration of such an assistant into an interactive programming environment. In this context, learning to recognize situations in which repetitive tasks occur is diicult because languages describing users actions are complex and because fast learning is mandatory. To achieve this goal we propose an agent-based software assistant that includes a new learning algorithm. The algorithm is incremental, has a very low training time and is able to handle diierent description languages. In the context of that application, it provides better results than related machine learning algorithms, making our assistant actually usable and eecient. Our software assistant is embodied into the ParcPlace VisualWorks (Smalltalk) programming environment and accessible through the net. Its architecture and algorithm could be adapted with beneet to any application involving human-computer interaction.",
        "paperId": "2422c0b109b6c566a538d7f64b87a1c19fdc9643"
    },
    {
        "title": "In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations",
        "firstAuthor": "Safoora Yousefi",
        "url": "https://arxiv.org/pdf/2310.00313",
        "dateSubmitted": "2023-09-30",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large language models (LLMs) exhibit remarkable performance improvement through in-context learning (ICL) by leveraging task-specific examples in the input. However, the mechanisms behind this improvement remain elusive. In this work, we investigate embeddings and attention representations in Llama-2 70B and Vicuna 13B. Specifically, we study how embeddings and attention change after in-context-learning, and how these changes mediate improvement in behavior. We employ neuroscience-inspired techniques, such as representational similarity analysis (RSA), and propose novel methods for parameterized probing and attention ratio analysis (ARA, measuring the ratio of attention to relevant vs. irrelevant information). We designed three tasks with a priori relationships among their conditions: reading comprehension, linear regression, and adversarial prompt injection. We formed hypotheses about expected similarities in task representations to investigate latent changes in embeddings and attention. Our analyses revealed a meaningful correlation between changes in both embeddings and attention representations with improvements in behavioral performance after ICL. This empirical framework empowers a nuanced understanding of how latent representations affect LLM behavior with and without ICL, offering valuable tools and insights for future research and practical applications.",
        "paperId": "2427527c1a1bc61b32c28a107192c3e22ed629bb"
    },
    {
        "title": "A intelligent english situated learning system based on signboard information",
        "firstAuthor": "Jin Il Kim",
        "url": null,
        "dateSubmitted": "2014-05-31",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Recently, the prevalence of high-performance mobile devices and development of IT technologies, including augmented reality technology and location information service, etc. has led to the formation of an educational environment allowing learners to practice context learning theory ideally, subsequently contributing to active progress of relevant studies. Therefore, it is difficult to expect leaners to have an interest and a commitment since it cannot connect a lot of information coming through the perspective of the learner in a real-world environment with situated learning in real time. Accordingly, this paper proposes a intelligent English situated learning system that can practice situation context learning more reasonably using a location-based service and a recognition technology that automatically recognizes text information on street signboards that are easily accessible in everyday life, yet provide learners with a lot of information. The proposed system provides learners with English conversation learning contents that can be used in the business sector related to trade name recognized through text information on street signboards from images captured by cameras.",
        "paperId": "243234a2d4fe1cf89b3f89c98fcfc750e2f3c9ca"
    },
    {
        "title": "T\u00fcrkiye\u2019nin Avrupa Birli\u011fi E\u011fitim ve Gen\u00e7likAplikasyonlar\u0131na Kat\u0131l\u0131m\u0131 S\u00fcrecinde Bilgi ve \u0130leti\u015fim Teknolojileri Deste\u011fiyle K\u00fclt\u00fcrel ve Akademik Ama\u00e7l\u0131 De\u011fi\u015fim Programlar\u0131n\u0131n \u0130ncelenmesi",
        "firstAuthor": "Rah\u015fan Ba\u011fc\u0131",
        "url": "https://doi.org/10.52096/usbd.7.31.07",
        "dateSubmitted": "2023-08-30",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In the process of social and scientific progress and change recorded since the beginning of human history, the effort to communicate, learn new things and transfer the acquired knowledge from generation to generation stands out as an ongoing dynamic action and effort. The search for what is different and new and change has become a part of human nature.Dec. The desire for change has not been limited to a single area throughout human history, but has made itself felt on many issues. This process extends from the past to the present and has entered the field of interest of more than one area and subject by not focusing on a single area. Communication and informatics are also one of the areas that attract the most attention among these fields. Our era is called an information age, and society is turning into an information society with this change. In this context, learning processes have also been influenced by communication and information technologies to the necessary extent. In the process of Turkey's accession to the European Union, the most important element of integration manifests itself in the field of education. In multidimensional research and investigations, \u201cErasmus Intensive Language Course (EILC)\u201d, which is carried out within the scope of Higher Education Erasmus Exchange Program with a conceptual and practical perspective on the use of information and communication technologies in terms of Turkey's participation in European Union Education and Youth programs, in other words, \u201cEuropean Language Passport\u201d for \u201cErasmus Intensive Language Course (EYDC)\u201d, in other words, \"European Language Passport\", Within the framework of the \u201cCouncil of Europe Common Criteria for Languages (CEFRL) Learning, Teaching and Evaluation Document\u201d of the \u201cEuropean Language Development File\u201d consisting of \u201cLanguage Learning History (Language Biography)\u201d and \u201cEuropean Language Achievement and Experience File\u201d, tools such as web-based self-assessment (diagnostic) tools have been created for Turkish taught as a foreign language based on the \u201cSwiss Self-Assessment Checklist Can Do Statements\u201d and e-course applications have been prepared for reading, listening and writing skills it is seen. Key Words: European Union, Exchange Programs, Education and Youth Applications",
        "paperId": "24399b85d4ef58fefbfac758dfa02e581cf99008"
    },
    {
        "title": "EXnet: Efficient In-context Learning for Data-less Text classification",
        "firstAuthor": "Debaditya Shome",
        "url": "http://arxiv.org/pdf/2305.14622",
        "dateSubmitted": "2023-05-24",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Large pre-trained language models (PLMs) have made significant progress in encoding world knowledge and spawned a new set of learning paradigms including zero-shot, few-shot, and in-context learning. Many language tasks can be modeled as a set of prompts (for example, is this text about geography?) and language models can provide binary answers, i.e., Yes or No. There is evidence to suggest that the next-word prediction used by many PLMs does not align well with zero-shot paradigms. Therefore, PLMs are fine-tuned as a question-answering system. In-context learning extends zero-shot learning by incorporating prompts and examples, resulting in increased task accuracy. Our paper presents EXnet, a model specifically designed to perform in-context learning without any limitations on the number of examples. We argue that in-context learning is an effective method to increase task accuracy, and providing examples facilitates cross-task generalization, especially when it comes to text classification tasks. With extensive experiments, we show that even our smallest model (15M parameters) generalizes to several unseen classification tasks and domains.",
        "paperId": "2447d22655803bfacb880f117cc34d2ac5ac7e74"
    },
    {
        "title": "A Computational Model of Context Processing",
        "firstAuthor": "C. Balkenius",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "A computational model of the context processing is presented. It is shown in computer simulations how a stable context representation can be learned from a dynamic sequence of attentional shifts between various stimuli in the environment. The mechanism can automatically create the required context representations, store memories of stimuli and bind them to locations. The model also shows how an explicit matching between expected and actual stimuli can be used for novelty detection. The novelty detection system is used to decide when new binding nodes should be created and when the context representation should be shifted from one context to another. The role of context in conditioning and habituation is illustrated in two simple simulations where context learning is combined with conditioning or habituation.",
        "paperId": "2458dd8280780f75efaa758d02d83866d884f9c8"
    },
    {
        "title": "Orthogonal Procrustes Analysis for Dictionary Learning in Sparse Linear Representation",
        "firstAuthor": "G. Grossi",
        "url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0169663&type=printable",
        "dateSubmitted": "2017-01-19",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In the sparse representation model, the design of overcomplete dictionaries plays a key role for the effectiveness and applicability in different domains. Recent research has produced several dictionary learning approaches, being proven that dictionaries learnt by data examples significantly outperform structured ones, e.g. wavelet transforms. In this context, learning consists in adapting the dictionary atoms to a set of training signals in order to promote a sparse representation that minimizes the reconstruction error. Finding the best fitting dictionary remains a very difficult task, leaving the question still open. A well-established heuristic method for tackling this problem is an iterative alternating scheme, adopted for instance in the well-known K-SVD algorithm. Essentially, it consists in repeating two stages; the former promotes sparse coding of the training set and the latter adapts the dictionary to reduce the error. In this paper we present R-SVD, a new method that, while maintaining the alternating scheme, adopts the Orthogonal Procrustes analysis to update the dictionary atoms suitably arranged into groups. Comparative experiments on synthetic data prove the effectiveness of R-SVD with respect to well known dictionary learning algorithms such as K-SVD, ILS-DLA and the online method OSDL. Moreover, experiments on natural data such as ECG compression, EEG sparse representation, and image modeling confirm R-SVD\u2019s robustness and wide applicability.",
        "paperId": "24630ffd8e75637737e98c586b5d1ee8d336fdb5"
    },
    {
        "title": "ICT Education: Socio-Learning Issues Faced by International Students",
        "firstAuthor": "S. Christian",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Internationalization of education industry has increased the international student intake amongst private and public funded education providers in western countries. However, international students are faced with many challenges in different educational and societal settings of the host country. This study examines a case involving the information and communication technology (ICT) education sector to identify some of the learning and social issues in an international student context. Learning issues relate to understanding of the host country\u2019s education framework structure and to application of subject related concepts to real world practice. Social issues relate to linguistic difficulties and cultural diversity in foreign countries. The study proposes to enhance the student\u2019 socio-learning experience by using a game based learning strategy aligned with the ICT course structure, to encourage student interactions by having more learning and social exchanges.",
        "paperId": "2475f433a11239f594f93eedd5fe8ce4ef0ebe45"
    },
    {
        "title": "Problem-posing with Multicultural Children\u2019s Literature: Developing Critical Early Childhood Curricula",
        "firstAuthor": "Elizabeth P. Quintero",
        "url": null,
        "dateSubmitted": "2004-08-20",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Problem-posing with Multicultural Children's Literature documents an ongoing qualitative study of early childhood teachers using a problem-posing method with multicultural children's literature. Grounded in critical theory, the text has been written for use in upper-division undergraduate- and graduate-level classes that study infants, toddlers, preschoolers, kindergartners, and students in grades one and two. The book uses examples from both early childhood and elementary teacher education students, and practicing teachers' work as they study critical literacy, multicultural children's literature, and integrated early childhood curriculum. This structure provides insights into guided research in child development, cultural and linguistic contexts, learning theory, strategies for teaching young children, family advocacy, and all related aspects of early childhood teacher education as the learners move through the activities.",
        "paperId": "247eba27fbce0b467d52188f67574dd5cd83550a"
    },
    {
        "title": "When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks",
        "firstAuthor": "Hao Peng",
        "url": null,
        "dateSubmitted": "2023-11-15",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "In-context learning (ICL) has become the default method for using large language models (LLMs), making the exploration of its limitations and understanding the underlying causes crucial. In this paper, we find that ICL falls short of handling specification-heavy tasks, which are tasks with complicated and extensive task specifications, requiring several hours for ordinary humans to master, such as traditional information extraction tasks. The performance of ICL on these tasks mostly cannot reach half of the state-of-the-art results. To explore the reasons behind this failure, we conduct comprehensive experiments on 18 specification-heavy tasks with various LLMs and identify three primary reasons: inability to specifically understand context, misalignment in task schema comprehension with humans, and inadequate long-text understanding ability. Furthermore, we demonstrate that through fine-tuning, LLMs can achieve decent performance on these tasks, indicating that the failure of ICL is not an inherent flaw of LLMs, but rather a drawback of existing alignment methods that renders LLMs incapable of handling complicated specification-heavy tasks via ICL. To substantiate this, we perform dedicated instruction tuning on LLMs for these tasks and observe a notable improvement. We hope the analyses in this paper could facilitate advancements in alignment methods enabling LLMs to meet more sophisticated human demands.",
        "paperId": "248c9663001cddba588709ac5fb67f2a549c01a0"
    },
    {
        "title": "Power Conservation in Cloud-Assisted Real-Time Context Learning System",
        "firstAuthor": "Jean-Franois Laplante",
        "url": null,
        "dateSubmitted": "2019-05-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Contextual information can be learned at the mobile devices, such as smartphones, in real-time from the sensors to provide better services to the user. The sensor data collection process, where the data is collected by various internal sensors or autonomous external sensors, incurs greater power consumption, depending upon the type of sensor and data capturing rate in the mobile devices. On the other hand, the learning process itself drains the battery and at the same time affects the accuracy of the learning due to the limited computational power of the mobile devices. These problems can be addressed by shifting the learning process to the cloud, which is however achieved at the cost of reducing the accuracy of the real-time solutions and incurs heavy bandwidth usage depending upon the context of the user. Therefore, we propose a cloud-based real-time context-learning system where the user of the system will get the predetermined service in real-time according to the user-determined context, which is learned from the related sensors while conserving a maximum amount of power compared to the standalone system or the cloud-based system. We have produced experimental results using a smartphone that illustrates that our system conserves 96.36% of power compared to the mobile-learning system while at the same time, the network data usage is 80% lower when compared to the cloud-based system. We have also showed that the proposed system works 76.17% and 94.81% faster compared to the mobile-learning and cloud-based system respectively.",
        "paperId": "249ed09967abdbcd66753a7feee6aa954af2fab1"
    },
    {
        "title": "An Empirical Study on Using Large Language Models for Multi-Intent Comment Generation",
        "firstAuthor": "Mingyang Geng",
        "url": "http://arxiv.org/pdf/2304.11384",
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Code comment generation aims at generating natural language descriptions for a code snippet to facilitate developers\u2019 program comprehension activities. Despite being studied for a long time, a bottleneck for existing approaches is that given a code snippet, they can only generate one comment while developers usually need to know information from diverse perspectives such as what is the functionality of this code snippet and how to use it. To tackle this limitation, this study empirically investigates the feasibility of utilizing large language models (LLMs) to generate comments that can fulfill developers\u2019 diverse intents. Our intuition is based on the facts that (1) the code and its pairwise comment are used during the pre-training process of LLMs to build the semantic connection between the natural language and programming language, and (2) comments in the real-world projects, which are collected for the pre-training, usually contain different developers\u2019 intents. We thus postulate that the LLMs can already understand the code from different perspectives after the pre-training. Indeed, experiments on two large-scale datasets demonstrate the rationale of our insights: by adopting the in-context learning paradigm and giving adequate prompts to the LLM (e.g., providing it with ten or more examples), the LLM can significantly outperform a state-of-the-art supervised learning approach on generating comments with multiple intents. Results also show that customized strategies for constructing the prompts and post-processing strategies for reranking the results can both boost the LLM\u2019s performances, which shed light on future research directions for using LLMs to achieve comment generation.",
        "paperId": "24a10e614ecd19b24bc8a5afc412f151553d20d6"
    },
    {
        "title": "Visual Context Learning with Big Data Analytics",
        "firstAuthor": "Mayanka Chandrashekar",
        "url": null,
        "dateSubmitted": "2016-12-01",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Understanding contextual information composed of both text and images is very useful for multimedia information processing. However, capturing such contexts is not trivial, especially while dealing with real datasets. Existing solutions such as using ontologies (e.g., WordNet) are mainly interested in individual terms, but they do not support identifying a group of terms that describe a specific context available at runtime. Within our knowledge, there are very limited solutions regarding the integration of contextual information from both images and text. Furthermore, existing solutions are not scalable due to the computationally intensive tasks and are prone to data sparsity. In this paper, we propose a semantic framework, called VisContextthat is based on a contextual model combined with images and text. The VisContext framework is based on the scalable pipeline that is composed of the primary components as follows: (i)Natural Language Processing (NLP), (ii) Feature extraction usingTerm Frequency-Inverse Document Frequency (TF-IDF), (iii)Feature association using unsupervised learning algorithms including K-Means clustering (KM) and Expectation-Maximization(EM) algorithms, iv) Validation of visual context models using supervised learning algorithms (Na\u00efve Bayes, Decision Trees, Random Forests). The proposed VisContext framework has been implemented with the Spark MLlib and CoreNLP. We have evaluated the effectiveness of the framework in visual understanding with three large datasets (IAPR, Flick3k, SBU) containing more than one million images and their annotations. The results are reported in the discovery of the contextual association of terms and images, image context visualization, and image classification based on contexts.",
        "paperId": "24aee34d1fb3cf5b1cddc2c6ef9259506be4e9c0"
    },
    {
        "title": "\ud074\ub798\uc2a4\ud305 \uae30\ubc18 \uc4f0\uae30\ud65c\ub3d9\uc774 \ucd08\ub4f1 \ud559\uc2b5\uc790\uc758 \uc601\uc5b4 \uc4f0\uae30\ub2a5\ub825 \ubc0f \uc815\uc758\uc801 \ud0dc\ub3c4\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5",
        "firstAuthor": "\uc2e0\ub3d9\uc775",
        "url": null,
        "dateSubmitted": null,
        "keyWords": [
            "in context learning"
        ],
        "abstract": "This study attempted to examine the effects of the application of Classting-based writing activities on EFL children``s writing abilities and affective attitude. It was conducted in a one-group experimental design in which 19 students of fifth grade were taught basic English writing skills as directed by a textbook for eight weeks and then the same students were taught the skills using a SNS-platform, Classting, for another eight weeks. The results of this study were as follows. First, teaching writing using Classting was more effective than teaching writing based on the textbook in improving the students`` writing skills, particularly using them in contexts, learning punctuation, exchanging feedback, and correcting errors. Second, although the two methods did not exhibit statistically significant differences in affective domains, the application of Classting was found to promote the students`` self-directed writing and increase their interest, confidence and feeling of achievement in writing in English. Third, Classting was found to provide the students with a place for real communication where they wrote for meaning negotiations and message exchange. Pedagogical implications were provided on the basis of the results of the study.",
        "paperId": "24b2b6dc54e79f46e7c72c2418d5ce5fa9e5dbdf"
    },
    {
        "title": "SALMON: Self-Alignment with Principle-Following Reward Models",
        "firstAuthor": "Zhiqing Sun",
        "url": "https://arxiv.org/pdf/2310.05910",
        "dateSubmitted": "2023-10-09",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principle-fOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RL-trained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.",
        "paperId": "24df244bf7a6e8c93c5f183d3f62d39c0f773c68"
    },
    {
        "title": "A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks",
        "firstAuthor": "Jacob D. Abernethy",
        "url": "http://arxiv.org/pdf/2305.17040",
        "dateSubmitted": "2023-05-26",
        "keyWords": [
            "in context learning"
        ],
        "abstract": "We study the phenomenon of \\textit{in-context learning} (ICL) exhibited by large language models, where they can adapt to a new learning task, given a handful of labeled examples, without any explicit parameter optimization. Our goal is to explain how a pre-trained transformer model is able to perform ICL under reasonable assumptions on the pre-training process and the downstream tasks. We posit a mechanism whereby a transformer can achieve the following: (a) receive an i.i.d. sequence of examples which have been converted into a prompt using potentially-ambiguous delimiters, (b) correctly segment the prompt into examples and labels, (c) infer from the data a \\textit{sparse linear regressor} hypothesis, and finally (d) apply this hypothesis on the given test example and return a predicted label. We establish that this entire procedure is implementable using the transformer mechanism, and we give sample complexity guarantees for this learning framework. Our empirical findings validate the challenge of segmentation, and we show a correspondence between our posited mechanisms and observed attention maps for step (c).",
        "paperId": "253c900b0569694d57e8f2904e330b51ae740fd8"
    }
]