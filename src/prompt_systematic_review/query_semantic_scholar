import pandas as pd
from keywords import keywords_list
import semantic_scholar_source
import time
import requests

def fetch_and_store_data(papers_per_keyword: int = 100):
    source = semantic_scholar_source.SemanticScholarSource()
    all_papers_data = []
    papers_count_per_keyword = {}

    for keywords in keywords_list:
        papers_fetched = 0
        offset = 0
        while papers_fetched < papers_per_keyword:
            try:
                papers_data = source.getPapers(keywords, count=100, offset=offset)
                if not papers_data:
                    break  # No more papers to fetch

                all_papers_data.extend(papers_data)
                papers_fetched += len(papers_data)
                offset += len(papers_data)  # Update offset for next batch

                if len(papers_data) < 100:  # Last batch
                    break
            except requests.exceptions.HTTPError as e:
                if e.response.status_code in [429, 504]:
                    print(f"Error (HTTP {e.response.status_code}) for keyword: {keywords}. Retrying...")
                    time.sleep(1.1)  # Wait for 1.1 seconds before retrying
                else:
                    print(f"Unhandled error (HTTP {e.response.status_code}) for keyword: {keywords}.")
                    break

        papers_count_per_keyword['; '.join(keywords)] = papers_fetched

    # Convert to a DataFrame for easier manipulation
    df = pd.DataFrame(all_papers_data)
    df.to_csv('semantic_scholar_data.csv', index=False)

    print(f"Total papers before cleaning: {len(df)}")
    for k, v in papers_count_per_keyword.items():
        print(f"Papers for '{k}': {v}")

def clean_duplicates():
    df = pd.read_csv('semantic_scholar_data.csv')
    df_cleaned = df.drop_duplicates(subset=['Title', 'First Author'])
    df_cleaned.to_csv('semantic_scholar_data_cleaned.csv', index=False)

    print(f"Total papers after cleaning: {len(df_cleaned)}")

fetch_and_store_data()
clean_duplicates()
