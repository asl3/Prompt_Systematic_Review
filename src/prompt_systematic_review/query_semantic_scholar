import pandas as pd
from keywords import keywords_list
import semantic_scholar_source
import time
import requests


def bulk_fetch_and_store_data():
    source = semantic_scholar_source.SemanticScholarSource()
    all_papers_data = []
    papers_count_per_keyword = {}

    for keyword_set in keywords_list:
        token = None
        query = ' OR '.join([f'"{keyword}"' for keyword in keyword_set])  # Ensure correct query construction
        total_papers_for_keyword_set = 0

        while True:
            try:
                response_data = source.bulkSearchPapers(query, token=token)
                papers_data = response_data.get('data', [])
                token = response_data.get('next', None)  # Continuation token for next batch

                if papers_data:
                    total_papers_for_keyword_set += len(papers_data)
                    all_papers_data.extend([{
                        'Title': paper.get("title"),
                        'First Author': paper.get("authors", [{}])[0].get("name", ""),
                        'Abstract': paper.get('abstract', '').replace('\n', ' '),
                        'TLDR': paper.get("tldr"),
                        'Open Access PDF URL': paper.get("openAccessPdf", {}).get("url", None)
                    } for paper in papers_data])

                if not token:  # No more data to fetch
                    break

            except requests.exceptions.HTTPError as e:
                if e.response.status_code in [429, 504]:
                    print(f"Rate limit hit or server timeout. Waiting before retrying...")
                    time.sleep(1.1)  # Back-off before retrying
                else:
                    print(f"Error for keyword set {keyword_set}: {e}")
                    break

        papers_count_per_keyword['; '.join(keyword_set)] = total_papers_for_keyword_set

    # Convert to DataFrame and save to CSV
    df = pd.DataFrame(all_papers_data)
    df.to_csv('semantic_scholar_bulk_data.csv', index=False)

    print(f"Total papers before cleaning: {len(df)}")
    for keywords, count in papers_count_per_keyword.items():
        print(f"Papers fetched for '{keywords}': {count}")

def fetch_and_store_data(papers_per_keyword: int = 100):
    source = semantic_scholar_source.SemanticScholarSource()
    all_papers_data = []
    papers_count_per_keyword = {}

    for keywords in keywords_list:
        papers_fetched = 0
        offset = 0
        while papers_fetched < papers_per_keyword:
            try:
                papers_data = source.getPapers(keywords, count=100, offset=offset)
                if not papers_data:
                    break  # No more papers to fetch

                all_papers_data.extend(papers_data)
                papers_fetched += len(papers_data)
                offset += len(papers_data)  # Update offset for next batch

                if len(papers_data) < 100:  # Last batch
                    break
            except requests.exceptions.HTTPError as e:
                if e.response.status_code in [429, 504]:
                    print(f"Error (HTTP {e.response.status_code}) for keyword: {keywords}. Retrying...")
                    time.sleep(1.1)  # Wait for 1.1 seconds before retrying
                else:
                    print(f"Unhandled error (HTTP {e.response.status_code}) for keyword: {keywords}.")
                    break

        papers_count_per_keyword['; '.join(keywords)] = papers_fetched

    # Convert to a DataFrame for easier manipulation
    df = pd.DataFrame(all_papers_data)
    df.to_csv('semantic_scholar_data.csv', index=False)

    print(f"Total papers before cleaning: {len(df)}")
    for k, v in papers_count_per_keyword.items():
        print(f"Papers for '{k}': {v}")

def clean_duplicates(bulk = False):
    if bulk:
        df = pd.read_csv('semantic_scholar_bulk_data.csv')
        df_cleaned = df.drop_duplicates(subset=['Title', 'First Author'])
        df_cleaned.to_csv('semantic_scholar_bulk_data_cleaned.csv', index=False)
    else:
        df = pd.read_csv('semantic_scholar_data.csv')
        df_cleaned = df.drop_duplicates(subset=['Title', 'First Author'])
        df_cleaned.to_csv('semantic_scholar_data_cleaned.csv', index=False)

    print(f"Total papers after cleaning: {len(df_cleaned)}")

def clean_against_previous_dataset():
    current_df = pd.read_csv('/Users/aayushgupta/Documents/GitHub/Prompt_Systematic_Review/data/semantic_scholar_data_cleaned.csv')
    previous_df = pd.read_csv("/Users/aayushgupta/Downloads/arxiv_papers.csv")

    current_df['Title'] = current_df['Title'].str.lower()
    previous_df['title'] = previous_df['title'].str.lower()

    unique_titles = ~current_df['Title'].isin(previous_df['title'])

    cleaned_df = current_df[unique_titles]

    cleaned_df.to_csv("/Users/aayushgupta/Documents/GitHub/Prompt_Systematic_Review/data/semantic_scholar_data_doubled_cleaned.csv", index=False)


clean_against_previous_dataset()
# fetch_and_store_data()
# bulk_fetch_and_store_data()
# clean_duplicates(bulk = True)
