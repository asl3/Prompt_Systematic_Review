{"id": "1104d766527dead44a40532e8a89444d9cef5c65", "abstract": "The misuse of large language models (LLMs) has garnered significant attention from the general public and LLM vendors. In response, efforts have been made to align LLMs with human values and intent use. However, a particular type of adversarial prompts, known as jailbreak prompt, has emerged and continuously evolved to bypass the safeguards and elicit harmful content from LLMs. In this paper, we conduct the first measurement study on jailbreak prompts in the wild, with 6,387 prompts collected from four platforms over six months. Leveraging natural language processing technologies and graph-based community detection methods, we discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from public platforms to private ones, posing new challenges for LLM vendors in proactive detection. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 46,800 samples across 13 forbidden scenarios. Our experiments show that current LLMs and safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify two highly effective jailbreak prompts which achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and they have persisted online for over 100 days. Our work sheds light on the severe and evolving threat landscape of jailbreak prompts. We hope our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.", "title": "do anything now characterizing and evaluating inthewild jailbreak prompts on large language models", "url": "https://arxiv.org/pdf/2308.03825", "tokenized_text": "misuse large_language large language llms garnered significant attention general public llm vendors response efforts align llms human values intent use particular type adversarial known jailbreak emerged continuously evolved bypass safeguards elicit harmful content llms paper conduct measurement study jailbreak_prompts jailbreak wild collected platforms months leveraging natural_language natural language processing technologies graph based community detection methods discover unique characteristics jailbreak_prompts jailbreak major attack strategies prompt_injection injection privilege escalation observe jailbreak_prompts jailbreak increasingly shift public platforms private ones posing new challenges llm vendors proactive detection assess potential harm caused jailbreak_prompts jailbreak create question set comprising samples 13 scenarios experiments current llms safeguards adequately defend jailbreak_prompts jailbreak scenarios particularly identify highly effective jailbreak_prompts jailbreak achieve attack success rates chatgpt gpt-3.5 gpt-4 online 100 days work sheds light severe evolving threat landscape jailbreak_prompts jailbreak hope study facilitate research community llm vendors promoting safer llms"}
{"id": "3c784cd3150a359e269c70cfbadd18774d66055d", "abstract": "Jailbreak vulnerabilities in Large Language Models (LLMs), which exploit meticulously crafted prompts to elicit content that violates service guidelines, have captured the attention of research communities. While model owners can defend against individual jailbreak prompts through safety training strategies, this relatively passive approach struggles to handle the broader category of similar jailbreaks. To tackle this issue, we introduce FuzzLLM, an automated fuzzing framework designed to proactively test and discover jailbreak vulnerabilities in LLMs. We utilize templates to capture the structural integrity of a prompt and isolate key features of a jailbreak class as constraints. By integrating different base classes into powerful combo attacks and varying the elements of constraints and prohibited questions, FuzzLLM enables efficient testing with reduced manual effort. Extensive experiments demonstrate FuzzLLM's effectiveness and comprehensiveness in vulnerability discovery across various LLMs.", "title": "fuzzllm a novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models", "url": "https://arxiv.org/pdf/2309.05274", "tokenized_text": "jailbreak vulnerabilities large_language large language llms exploit meticulously crafted elicit content service guidelines captured attention research communities defend individual jailbreak_prompts jailbreak safety training strategies relatively passive approach struggles handle broader category similar jailbreaks tackle issue introduce automated fuzzing framework designed proactively test discover jailbreak vulnerabilities llms utilize templates capture structural integrity key features jailbreak class constraints integrating different base classes powerful attacks varying elements constraints prohibited questions enables efficient testing reduced manual effort extensive_experiments extensive experiments demonstrate effectiveness comprehensiveness vulnerability discovery llms"}
{"id": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e", "abstract": "As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.", "title": "baseline defenses for adversarial attacks against aligned language models", "url": "https://arxiv.org/pdf/2309.00614", "tokenized_text": "large_language large language quickly ubiquitous critical understand security vulnerabilities recent_work recent work shows text optimizers produce jailbreaking bypass moderation alignment drawing rich body work adversarial machine_learning machine learning approach attacks questions threat practically useful domain baseline defense techniques perform new domain llm security differ computer_vision computer vision evaluate baseline defense strategies leading adversarial attacks llms discussing settings feasible effective particularly look types defenses detection perplexity based input preprocessing paraphrase adversarial training discuss white box box settings discuss robustness performance trade defenses considered find weakness existing discrete optimizers text combined relatively high costs optimization makes standard adaptive attacks challenging llms future_research future research needed uncover powerful optimizers developed strength filtering preprocessing defenses greater llms domain computer_vision computer vision"}
{"id": "ace98e1e58bcc364afbb2feff6d136232f5f47da", "abstract": "Considerable research efforts have been devoted to ensuring that large language models (LLMs) align with human values and generate safe text. However, an excessive focus on sensitivity to certain topics can compromise the model's robustness in following instructions, thereby impacting its overall performance in completing tasks. Previous benchmarks for jailbreaking LLMs have primarily focused on evaluating the safety of the models without considering their robustness. In this paper, we propose a benchmark that assesses both the safety and robustness of LLMs, emphasizing the need for a balanced approach. To comprehensively study text safety and output robustness, we introduce a latent jailbreak prompt dataset, each involving malicious instruction embedding. Specifically, we instruct the model to complete a regular task, such as translation, with the text to be translated containing malicious instructions. To further analyze safety and robustness, we design a hierarchical annotation framework. We present a systematic analysis of the safety and robustness of LLMs regarding the position of explicit normal instructions, word replacements (verbs in explicit normal instructions, target groups in malicious instructions, cue words for explicit normal instructions), and instruction replacements (different explicit normal instructions). Our results demonstrate that current LLMs not only prioritize certain instruction verbs but also exhibit varying jailbreak rates for different instruction verbs in explicit normal instructions. Code and data are available at https://github.com/qiuhuachuan/latent-jailbreak.", "title": "latent jailbreak a benchmark for evaluating text safety and output robustness of large language models", "url": "https://arxiv.org/pdf/2307.08487", "tokenized_text": "considerable research efforts devoted ensuring large_language large language llms align human values generate safe text excessive focus sensitivity certain topics compromise robustness following instructions impacting overall performance completing tasks previous benchmarks jailbreaking llms primarily focused evaluating safety considering robustness paper propose benchmark assesses safety robustness llms emphasizing need balanced approach comprehensively study text safety output robustness introduce latent jailbreak dataset involving malicious instruction embedding specifically instruct complete regular task translation text translated containing malicious instructions analyze safety robustness design hierarchical annotation framework present systematic analysis safety robustness llms position explicit normal instructions word replacements explicit normal instructions target groups malicious instructions words explicit normal instructions instruction replacements different explicit normal instructions results_demonstrate results demonstrate current llms prioritize certain instruction exhibit varying jailbreak rates different instruction explicit normal instructions code data available"}
{"id": "cd29c25c489562b409a60f83365f93f33ee1a0a1", "abstract": "Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments on open-source large language models, we demonstrate that RA-LLM can successfully defend against both state-of-the-art adversarial prompts and popular handcrafted jailbreaking prompts by reducing their attack success rates from nearly 100\\% to around 10\\% or less.", "title": "defending against alignmentbreaking attacks via robustly aligned llm", "url": "https://arxiv.org/pdf/2309.14348", "tokenized_text": "recently large_language large language llms significant advancements widely domains unfortunately rising concern llms misused generate harmful malicious content line research focused aligning llms human values preventing producing inappropriate content alignments usually vulnerable alignment breaking attacks adversarially optimized handcrafted jailbreaking work introduce robustly aligned llm ra llm defend potential alignment breaking attacks ra llm directly constructed existing aligned llm robust alignment checking function requiring expensive retraining fine tuning process original llm furthermore provide theoretical analysis ra llm verify effectiveness alignment breaking attacks real world experiments open source large_language large language demonstrate ra llm successfully defend state art adversarial popular handcrafted jailbreaking reducing attack success rates nearly 100\\%"}
{"id": "e64df7e9448f7a9a4cb5d22c21c460134c8646ac", "abstract": "The assessment of cybersecurity Capture-The-Flag (CTF) exercises involves participants finding text strings or ``flags'' by exploiting system vulnerabilities. Large Language Models (LLMs) are natural-language models trained on vast amounts of words to understand and generate text; they can perform well on many CTF challenges. Such LLMs are freely available to students. In the context of CTF exercises in the classroom, this raises concerns about academic integrity. Educators must understand LLMs' capabilities to modify their teaching to accommodate generative AI assistance. This research investigates the effectiveness of LLMs, particularly in the realm of CTF challenges and questions. Here we evaluate three popular LLMs, OpenAI ChatGPT, Google Bard, and Microsoft Bing. First, we assess the LLMs' question-answering performance on five Cisco certifications with varying difficulty levels. Next, we qualitatively study the LLMs' abilities in solving CTF challenges to understand their limitations. We report on the experience of using the LLMs for seven test cases in all five types of CTF challenges. In addition, we demonstrate how jailbreak prompts can bypass and break LLMs' ethical safeguards. The paper concludes by discussing LLM's impact on CTF exercises and its implications.", "title": "using large language models for cybersecurity capturetheflag challenges and certification questions", "url": "https://arxiv.org/pdf/2308.10443", "tokenized_text": "assessment cybersecurity capture exercises involves participants finding text strings flags exploiting system vulnerabilities large_language large language llms natural language_models language trained vast amounts words understand generate text perform challenges llms freely available students context exercises classroom raises concerns academic integrity educators understand llms capabilities modify teaching accommodate generative_ai generative ai assistance research investigates effectiveness llms particularly realm challenges questions evaluate popular llms openai chatgpt google_bard google bard microsoft bing assess llms question answering performance varying difficulty levels qualitatively study llms abilities solving challenges understand limitations report experience llms seven test_cases test cases types challenges addition demonstrate jailbreak_prompts jailbreak bypass break llms ethical safeguards paper concludes discussing llm impact exercises implications"}
{"id": "f3f23f7f9f5369aade19f20bc5d028cce7b9c9aa", "abstract": "The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively.", "title": "autodan generating stealthy jailbreak prompts on aligned large language models", "url": "https://arxiv.org/pdf/2310.04451", "tokenized_text": "aligned large_language large language llms powerful language understanding decision making tools created extensive alignment human feedback large remain susceptible jailbreak attacks adversaries manipulate elicit malicious outputs given aligned llms investigating jailbreak_prompts jailbreak lead delve limitations llms guide secure unfortunately existing jailbreak techniques suffer scalability issues attacks heavily rely manual crafting problems attacks depend token based algorithms generate semantically meaningless making susceptible detection basic perplexity testing light challenges intend answer question develop approach automatically generate stealthy jailbreak_prompts jailbreak paper introduce novel jailbreak attack aligned llms automatically generate stealthy jailbreak_prompts jailbreak carefully designed hierarchical genetic algorithm extensive evaluations demonstrate automates process preserving semantic demonstrates superior attack strength cross transferability cross sample universality compared baseline compare perplexity based defense methods bypass effectively"}
{"id": "fc50a6202e2f675604543c1ae4ef22ec74f61ad5", "abstract": "Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. Our study investigates three key research questions: (1) the number of different prompt types that can jailbreak LLMs, (2) the effectiveness of jailbreak prompts in circumventing LLM constraints, and (3) the resilience of ChatGPT against these jailbreak prompts. Initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. Subsequently, we assess the jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. Finally, we evaluate the resistance of ChatGPT against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. The study underscores the importance of prompt structures in jailbreaking LLMs and discusses the challenges of robust jailbreak prompt generation and prevention.", "title": "jailbreaking chatgpt via prompt engineering an empirical study", "url": "http://arxiv.org/pdf/2305.13860", "tokenized_text": "large_language large language llms like_chatgpt like chatgpt demonstrated vast potential introduce challenges related content constraints potential misuse study investigates key research questions number different types jailbreak llms effectiveness jailbreak_prompts jailbreak circumventing llm constraints resilience chatgpt jailbreak_prompts jailbreak initially develop classification analyze distribution existing identifying distinct patterns categories jailbreak_prompts jailbreak subsequently assess jailbreak capability chatgpt versions 3.5 utilizing dataset jailbreak questions prohibited scenarios finally evaluate resistance chatgpt jailbreak_prompts jailbreak finding consistently evade restrictions 40 use case scenarios study underscores importance structures jailbreaking llms discusses challenges robust jailbreak generation"}
{"id": "07955e96cbd778d0ae2a68f09d073b866dd84c2a", "abstract": "Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.", "title": "decomposed prompting a modular approach for solving complex tasks", "url": "http://arxiv.org/pdf/2210.02406", "tokenized_text": "shot_prompting shot surprisingly powerful way use large_language large language llms solve tasks approach struggles task complexity increases individual reasoning_steps reasoning steps task hard learn especially embedded complex tasks address propose decomposed new approach solve complex tasks decomposing simpler sub tasks delegated library based llms dedicated sub tasks modular structure allows optimized specific sub task decomposed necessary easily replaced effective trained symbolic functions desired flexibility modularity decomposed allows outperform prior_work prior work shot_prompting shot gpt3 symbolic reasoning tasks decompose sub tasks hard llms simpler solvable sub tasks complexity comes input length recursively decompose task task smaller inputs evaluate approach textual multi step reasoning tasks long context multi hop qa task effectively teach sub tasks separate sub tasks open domain multi hop qa incorporate symbolic information retrieval decomposition framework leading improved performance tasks datasets code available"}
{"id": "0afb64ce430c5f26752c8aed246ead6820b02049", "abstract": "Due to the modern relevance of blockchain technology, smart contracts present both substantial risks and benefits. Vulnerabilities within them can trigger a cascade of consequences, resulting in significant losses. Many current papers primarily focus on classifying smart contracts for malicious intent, often relying on limited contract characteristics, such as bytecode or opcode. This paper proposes a novel, two-layered framework: 1) classifying and 2) directly repairing malicious contracts. Slither's vulnerability report is combined with source code and passed through a pre-trained RandomForestClassifier (RFC) and Large Language Models (LLMs), classifying and repairing each suggested vulnerability. Experiments demonstrate the effectiveness of fine-tuned and prompt-engineered LLMs. The smart contract repair models, built from pre-trained GPT-3.5-Turbo and fine-tuned Llama-2-7B models, reduced the overall vulnerability count by 97.5% and 96.7% respectively. A manual inspection of repaired contracts shows that all retain functionality, indicating that the proposed method is appropriate for automatic batch classification and repair of vulnerabilities in smart contracts.", "title": "two timin' repairing smart contracts with a twolayered approach", "url": "https://arxiv.org/pdf/2309.07841", "tokenized_text": "modern relevance blockchain technology smart contracts present substantial risks benefits vulnerabilities trigger consequences resulting significant losses current papers primarily focus classifying smart contracts malicious intent relying limited contract characteristics paper_proposes paper proposes novel framework classifying directly repairing malicious contracts vulnerability report combined source_code source code passed pre trained large_language large language llms classifying repairing suggested vulnerability experiments demonstrate_the_effectiveness demonstrate effectiveness fine tuned engineered llms smart contract repair built pre trained gpt-3.5 turbo fine tuned llama-2 7b reduced overall vulnerability count respectively manual inspection repaired contracts shows retain functionality indicating proposed_method proposed method appropriate automatic batch classification repair vulnerabilities smart contracts"}
{"id": "0c110794ae91b4c165b0de3ff11fc841e2455bdb", "abstract": "One of the major impediments to the development of new task-oriented dialogue (TOD) systems is the need for human evaluation at multiple stages and iterations of the development process. In an effort to move toward automated evaluation of TOD, we propose a novel user simulator built using recently developed large pretrained language models (LLMs). In order to increase the linguistic diversity of our system relative to the related previous work, we do not fine-tune the LLMs used by our system on existing TOD datasets; rather we use in-context learning to prompt the LLMs to generate robust and linguistically diverse output with the goal of simulating the behavior of human interlocutors. Unlike previous work, which sought to maximize goal success rate (GSR) as the primary metric of simulator performance, our goal is a system which achieves a GSR similar to that observed in human interactions with TOD systems. Using this approach, our current simulator is effectively able to interact with several TOD systems, especially on single-intent conversational goals, while generating lexically and syntactically diverse output relative to previous simulators that rely upon fine-tuned models. Finally, we collect a Human2Bot dataset of humans interacting with the same TOD systems with which we experimented in order to better quantify these achievements.", "title": "user simulation with large language models for evaluating taskoriented dialogue", "url": "https://arxiv.org/pdf/2309.13233", "tokenized_text": "major development new task oriented dialogue tod systems need human evaluation multiple stages iterations development process effort automated evaluation tod propose_a_novel propose novel user simulator built recently developed large pretrained_language pretrained language llms order increase linguistic diversity system relative related previous work fine tune llms system existing tod datasets use context_learning context learning llms generate robust linguistically diverse output goal simulating behavior human unlike previous work sought maximize goal success_rate success rate primary metric simulator performance goal system achieves similar observed human interactions tod systems approach current simulator effectively able interact tod systems especially single intent conversational goals generating lexically syntactically diverse output relative previous simulators rely fine tuned finally collect dataset humans interacting tod systems experimented order better quantify achievements"}
{"id": "204fd6c5e247c477d607f507ee01d94a8dbd408f", "abstract": "The emergence of large language models (LLMs) has sparked significant interest in extending their remarkable language capabilities to speech. However, modality alignment between speech and text still remains an open problem. Current solutions can be categorized into two strategies. One is a cascaded approach where outputs (tokens or states) of a separately trained speech recognition system are used as inputs for LLMs, which limits their potential in modeling alignment between speech and text. The other is an end-to-end approach that relies on speech instruction data, which is very difficult to collect in large quantities. In this paper, we address these issues and propose the BLSP approach that Bootstraps Language-Speech Pre-training via behavior alignment of continuation writing. We achieve this by learning a lightweight modality adapter between a frozen speech encoder and an LLM, ensuring that the LLM exhibits the same generation behavior regardless of the modality of input: a speech segment or its transcript. The training process can be divided into two steps. The first step prompts an LLM to generate texts with speech transcripts as prefixes, obtaining text continuations. In the second step, these continuations are used as supervised signals to train the modality adapter in an end-to-end manner. We demonstrate that this straightforward process can extend the capabilities of LLMs to speech, enabling speech recognition, speech translation, spoken language understanding, and speech conversation, even in zero-shot cross-lingual scenarios.", "title": "blsp bootstrapping languagespeech pretraining via behavior alignment of continuation writing", "url": "https://arxiv.org/pdf/2309.00916", "tokenized_text": "emergence large_language large language llms sparked significant interest extending remarkable language capabilities speech modality alignment speech text remains open problem current solutions strategies cascaded approach outputs tokens states separately trained speech recognition system inputs llms limits potential modeling alignment speech text end end approach relies speech instruction data difficult collect large quantities paper address issues propose approach bootstraps pre-training behavior alignment continuation writing achieve learning lightweight modality adapter frozen speech encoder llm ensuring llm exhibits generation behavior regardless modality input speech segment training process divided steps step llm generate texts speech transcripts prefixes obtaining text second step supervised signals train modality adapter end end manner demonstrate straightforward process extend capabilities llms speech enabling speech recognition speech translation spoken language understanding speech conversation zero shot cross lingual scenarios"}
{"id": "243ac5656c4f8ed6e1eb757b7145fb12b837c166", "abstract": "Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperforms fine-tuning moderate-size pre-trained models specially designed for IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further conduct a series of in-depth analyses to demonstrate the merits of leveraging Code-LLMs for IE tasks.", "title": "codeie large code generation models are better fewshot information extractors", "url": "http://arxiv.org/pdf/2305.05711", "tokenized_text": "large_language large language llms pre trained massive corpora demonstrated impressive shot_learning shot learning ability nlp_tasks nlp tasks common practice task text text format generative llms natural_language natural language nl llms like gpt-3 prompted solve nontrivial perform information_extraction information extraction ie tasks nl llms output ie task usually structured hard converted plain text paper propose structured output form code instead natural_language natural language utilize generative llms code code llms codex perform ie tasks particular named_entity named entity recognition relation_extraction relation extraction contrast nl llms code llms aligned ie tasks designing code style formulating ie tasks code_generation code generation tasks experiment results seven benchmarks method consistently_outperforms consistently outperforms fine tuning moderate size pre trained specially designed ie tasks e.g. nl llms shot_settings shot settings conduct series depth analyses demonstrate merits leveraging code llms ie tasks"}
{"id": "24dd96da6f700f57132713aeb5e9b06905abab5d", "abstract": "Instructional videos are an excellent source for learning multimodal representations by leveraging video-subtitle pairs extracted with automatic speech recognition systems (ASR) from the audio signal in the videos. However, in contrast to human-annotated captions, both speech and subtitles naturally differ from the visual content of the videos and thus provide only noisy supervision for multimodal learning. As a result, large-scale annotation-free web video training data remains sub-optimal for training text-video models. In this work, we propose to leverage the capability of large language models (LLMs) to obtain fine-grained video descriptions aligned with videos. Specifically, we prompt an LLM to create plausible video descriptions based on ASR narrations of the video for a large-scale instructional video dataset. To this end, we introduce a prompting method that is able to take into account a longer text of subtitles, allowing us to capture context beyond a single sentence. To align the captions to the video temporally, we prompt the LLM to generate timestamps for each produced caption based on the subtitles. In this way, we obtain human-style video captions at scale without human supervision. We apply our method to the subtitles of the HowTo100M dataset, creating a new large-scale dataset, HowToCaption. Our evaluation shows that the resulting captions not only significantly improve the performance over many different benchmark datasets for text-video retrieval but also lead to a disentangling of textual narration from the audio, boosting performance in text-video-audio tasks.", "title": "howtocaption prompting llms to transform video annotations at scale", "url": "https://arxiv.org/pdf/2310.04900", "tokenized_text": "instructional videos excellent source learning multimodal representations leveraging video subtitle pairs extracted automatic speech recognition systems asr audio signal videos contrast human annotated captions speech subtitles naturally differ visual content videos provide noisy supervision multimodal learning result large scale annotation free web video training_data training data remains sub optimal training text video work propose leverage capability large_language large language llms obtain fine grained video descriptions aligned videos specifically llm create plausible video descriptions based asr video large scale instructional video dataset end introduce method able account longer text subtitles allowing capture context single sentence align captions video temporally llm generate produced caption based subtitles way obtain human style video captions scale human supervision apply method subtitles dataset creating new large scale dataset evaluation shows resulting captions significantly improve performance different benchmark_datasets benchmark datasets text video retrieval lead disentangling textual audio boosting performance text video audio tasks"}
{"id": "2bb4fe9bc10dbf1ea70135e52452f9f63bb10671", "abstract": "Large language models (LLMs) excel at implementing code from functionality descriptions but struggle with algorithmic problems that require not only implementation but also identification of the suitable algorithm. Moreover, LLM-generated programs lack guaranteed correctness and require human verification. To address these challenges, we propose ALGO, a framework that synthesizes Algorithmic programs with LLM-Generated Oracles to guide the generation and verify their correctness. ALGO first generates a reference oracle by prompting an LLM to exhaustively enumerate all the combinations of relevant variables. This oracle is then utilized to guide an arbitrary search strategy in exploring the algorithm space and to verify the synthesized algorithms. Our study shows that the LLM-generated oracles are correct for 88% of the cases. With the oracles as verifiers, ALGO can be integrated with any existing code generation model in a model-agnostic manner to enhance its performance. Experiments show that when equipped with ALGO, we achieve an 8x better one-submission pass rate over the Codex model and a 2.6x better one-submission pass rate over CodeT, the current state-of-the-art model on CodeContests. We can also get 1.3x better pass rate over the ChatGPT Code Interpreter on unseen problems. The problem set we used for testing, the prompts we used, the verifier and solution programs, and the test cases generated by ALGO are available at https://github.com/zkx06111/ALGO.", "title": "algo synthesizing algorithmic programs with generated oracle verifiers", "url": "http://arxiv.org/pdf/2305.14591", "tokenized_text": "large_language large language llms excel implementing code functionality descriptions struggle algorithmic problems require implementation identification suitable algorithm llm generated programs lack guaranteed correctness require human verification address challenges propose framework synthesizes algorithmic programs guide generation verify correctness generates reference oracle llm enumerate combinations relevant variables oracle utilized guide arbitrary search strategy exploring algorithm space verify synthesized algorithms study shows llm generated correct 88 cases verifiers integrated existing code_generation code generation agnostic manner enhance performance experiments equipped achieve 8x better submission pass rate codex better submission pass rate current state art better pass rate chatgpt code interpreter unseen problems problem set testing verifier solution programs test_cases test cases generated available"}
{"id": "2f75de70511fa9f5c7a1e7f61f2d7928d121adbf", "abstract": "Objective To develop soft prompt-based learning algorithms for large language models (LLMs), examine the shape of prompts, prompt-tuning using frozen/unfrozen LLMs, transfer learning, and few-shot learning abilities. Methods We developed a soft prompt-based LLM model and compared 4 training strategies including (1) fine-tuning without prompts; (2) hard-prompt with unfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt with frozen LLMs. We evaluated 7 pretrained LLMs using the 4 training strategies for clinical concept and relation extraction on two benchmark datasets. We evaluated the transfer learning ability of the prompt-based learning algorithms in a cross-institution setting. We also assessed the few-shot learning ability. Results and Conclusion When LLMs are unfrozen, GatorTron-3.9B with soft prompting achieves the best strict F1-scores of 0.9118 and 0.8604 for concept extraction, outperforming the traditional fine-tuning and hard prompt-based models by 0.6~3.1% and 1.2~2.9%, respectively; GatorTron-345M with soft prompting achieves the best F1-scores of 0.8332 and 0.7488 for end-to-end relation extraction, outperforming the other two models by 0.2~2% and 0.6~11.7%, respectively. When LLMs are frozen, small (i.e., 345 million parameters) LLMs have a big gap to be competitive with unfrozen models; scaling LLMs up to billions of parameters makes frozen LLMs competitive with unfrozen LLMs. For cross-institute evaluation, soft prompting with a frozen GatorTron-8.9B model achieved the best performance. This study demonstrates that (1) machines can learn soft prompts better than humans, (2) frozen LLMs have better few-shot learning ability and transfer learning ability to facilitate muti-institution applications, and (3) frozen LLMs require large models.", "title": "model tuning or prompt tuning a study of large language models for clinical concept and relation extraction", "url": "https://arxiv.org/pdf/2310.06239", "tokenized_text": "objective develop soft based learning algorithms large_language large language llms examine shape tuning frozen llms transfer learning shot_learning shot learning abilities methods developed soft based llm compared training strategies including fine tuning hard llms soft llms soft frozen llms evaluated pretrained llms training strategies clinical concept relation_extraction relation extraction benchmark_datasets benchmark datasets evaluated transfer learning ability based learning algorithms cross setting assessed shot_learning shot learning ability results conclusion llms soft achieves best strict f1 scores concept extraction outperforming traditional fine tuning hard based respectively soft achieves best f1 scores end end relation_extraction relation extraction outperforming respectively llms frozen small i.e. million parameters llms big gap competitive scaling llms billions parameters makes frozen llms competitive llms cross evaluation soft frozen achieved best performance study demonstrates machines learn soft better humans frozen llms better shot_learning shot learning ability transfer learning ability facilitate applications frozen llms require large"}
{"id": "370cea8b4220917f45a69358c0303df71f5063c7", "abstract": "Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think \u2013 retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum \u2013 probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.", "title": "thinksum probabilistic reasoning over sets using large language models", "url": "http://arxiv.org/pdf/2210.01293", "tokenized_text": "large_language large language llms substantial capacity high level analogical reasoning reproducing patterns linear text occur training_data training data zero shot evaluation provided context shot context_learning context learning recent studies advanced llms fail scenarios require reasoning multiple objects facts making sequences logical propose stage probabilistic inference paradigm reasons sets objects facts structured manner stage think retrieval associations llm parallel set phrases extracted auxiliary second stage sum probabilistic inference reasoning results queries final prediction demonstrate possibilities advantages big bench suite llm evaluation tasks achieving improvements state_of_the_art state art gpt family difficult tasks far smaller variants compare contrast proposed modifications direct llms variants chain thought_prompting thought results suggest probabilistic inference performed outside calls llm sensitive design yields interpretable predictions flexibly combined latent variable extract structured knowledge llms overall proposed paradigm represents promising approach enhancing reasoning capabilities llms"}
{"id": "3b27092740a489a63589cdcf40fad6a0e093daa0", "abstract": "While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The recent proliferation of Large Language Models (LLMs) compels one to ask: how capable are these systems in generating code-mixed data? In this paper, we explore prompting multilingual LLMs in a zero-shot manner to generate code-mixed data for seven languages in South East Asia (SEA), namely Indonesian, Malay, Chinese, Tagalog, Vietnamese, Tamil, and Singlish. We find that publicly available multilingual instruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of producing texts with phrases or clauses from different languages. ChatGPT exhibits inconsistent capabilities in generating code-mixed texts, wherein its performance varies depending on the prompt template and language pairing. For instance, ChatGPT generates fluent and natural Singlish texts (an English-based creole spoken in Singapore), but for English-Tamil language pair, the system mostly produces grammatically incorrect or semantically meaningless utterances. Furthermore, it may erroneously introduce languages not specified in the prompt. Based on our investigation, existing multilingual LLMs exhibit a wide range of proficiency in code-mixed data generation for SEA languages. As such, we advise against using LLMs in this context without extensive human checks.", "title": "prompting multilingual large language models to generate codemixed texts the case of south east asian languages", "url": "https://arxiv.org/pdf/2303.13592", "tokenized_text": "code common linguistic practice parts world collecting high quality low cost code mixed data remains challenge natural_language natural language processing nlp research recent proliferation large_language large language llms ask capable systems generating code mixed data paper explore multilingual llms zero shot manner generate code mixed data seven languages south chinese vietnamese tamil find publicly_available publicly available multilingual instruction tuned bloomz flan t5 xxl producing texts phrases clauses different languages chatgpt exhibits inconsistent capabilities generating code mixed texts performance varies depending prompt_template template language pairing instance chatgpt generates fluent natural texts english based spoken english tamil language pair system produces grammatically incorrect semantically meaningless utterances furthermore introduce languages specified based investigation existing multilingual llms exhibit wide_range wide range proficiency code mixed data generation languages advise llms context extensive human checks"}
{"id": "40c9280d87059c0cc28f2a08d46a7045fa3e9736", "abstract": "Chain-of-thought (CoT) prompting combined with large language models (LLMs) have achieved encouraging results on complex reasoning tasks. Text-to-SQL is a critical semantic parsing task that converts natural language questions into SQL statements, involving a complex reasoning process. However, there is little work about using CoT prompting to activate LLM's reasoning capabilities on Text-to-SQL tasks. In this work, we propose a new paradigm for prompting Text-to-SQL tasks, called Divide-and-Prompt, which first divides the task into subtasks, and then approach each subtask through CoT. We present 3 prompting-based methods to enhance the Text-to-SQL ability of LLMs. Experiments show that these prompts guide LLMs to generate Text-to-SQL with higher execution accuracy.", "title": "divide and prompt chain of thought prompting for texttosql", "url": "http://arxiv.org/pdf/2304.11556", "tokenized_text": "chain thought cot combined large_language large language llms achieved encouraging results complex_reasoning complex reasoning tasks text sql critical semantic_parsing semantic parsing task converts natural_language natural language questions sql statements involving complex_reasoning complex reasoning process little work cot_prompting cot activate llm reasoning capabilities text sql tasks work propose_a_new propose new paradigm text sql tasks called divide divides task subtasks approach subtask cot. present based methods enhance text sql ability llms experiments guide llms generate text sql higher execution accuracy"}
{"id": "4895d443c36bd136a818be2db34442354ba408d1", "abstract": "Tags are pivotal in facilitating the effective distribution of multimedia content in various applications in the contemporary Internet era, such as search engines and recommendation systems. Recently, large language models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. In this work, we propose TagGPT, a fully automated system capable of tag extraction and multimodal tagging in a completely zero-shot fashion. Our core insight is that, through elaborate prompt engineering, LLMs are able to extract and reason about proper tags given textual clues of multimodal data, e.g., OCR, ASR, title, etc. Specifically, to automatically build a high-quality tag set that reflects user intent and interests for a specific application, TagGPT predicts large-scale candidate tags from a series of raw data via prompting LLMs, filtered with frequency and semantics. Given a new entity that needs tagging for distribution, TagGPT introduces two alternative options for zero-shot tagging, i.e., a generative method with late semantic matching with the tag set, and another selective method with early matching in prompts. It is well noticed that TagGPT provides a system-level solution based on a modular framework equipped with a pre-trained LLM (GPT-3.5 used here) and a sentence embedding model (SimCSE used here), which can be seamlessly replaced with any more advanced one you want. TagGPT is applicable for various modalities of data in modern social media and showcases strong generalization ability to a wide range of applications. We evaluate TagGPT on publicly available datasets, i.e., Kuaishou and Food.com, and demonstrate the effectiveness of TagGPT compared to existing hashtags and off-the-shelf taggers. Project page: https://github.com/TencentARC/TagGPT.", "title": "taggpt large language models are zeroshot multimodal taggers", "url": "http://arxiv.org/pdf/2304.03022", "tokenized_text": "tags pivotal facilitating effective distribution multimedia content applications contemporary internet era search engines recommendation systems recently large_language large language llms demonstrated impressive capabilities wide_range wide range tasks work propose fully automated system capable tag extraction multimodal tagging completely zero shot fashion core insight elaborate prompt_engineering engineering llms able extract reason proper tags given textual clues multimodal data e.g. ocr asr title etc specifically automatically build high quality tag set reflects user intent interests specific application predicts large scale candidate tags series raw data llms filtered frequency semantics given new entity needs tagging distribution introduces alternative options zero shot tagging i.e. generative method late semantic matching tag set selective method early matching provides system level solution based modular framework equipped pre trained llm gpt-3.5 sentence embedding seamlessly replaced advanced want applicable modalities data modern social_media social media showcases strong generalization_ability generalization ability wide_range wide range applications evaluate publicly_available publicly available datasets i.e. demonstrate_the_effectiveness demonstrate effectiveness compared existing hashtags shelf project page"}
{"id": "4950bf6f873ba1409a7bbad25cf5c93c8f833453", "abstract": "The large language model (LLM) has garnered significant attention due to its in-context learning mechanisms and emergent capabilities. The research community has conducted several pilot studies to apply LLMs to machine translation tasks and evaluate their performance from diverse perspectives. However, previous research has primarily focused on the LLM itself and has not explored human intervention in the inference process of LLM. The characteristics of LLM, such as in-context learning and prompt engineering, closely mirror human cognitive abilities in language tasks, offering an intuitive solution for human-in-the-loop generation. In this study, we propose a human-in-the-loop pipeline that guides LLMs to produce customized outputs with revision instructions. The pipeline initiates by prompting the LLM to produce a draft translation, followed by the utilization of automatic retrieval or human feedback as supervision signals to enhance the LLM\u2019s translation through in-context learning. The human-machine interactions generated in this pipeline are also stored in an external database to expand the in-context retrieval database, enabling us to leverage human supervision in an offline setting. We evaluate the proposed pipeline using the GPT-3.5-turbo API on five domain-specific benchmarks for German-English translation. The results demonstrate the effectiveness of the pipeline in tailoring in-domain translations and improving translation performance compared to direct translation instructions. Additionally, we discuss the experimental results from the following perspectives: 1) the effectiveness of different in-context retrieval methods; 2) the construction of a retrieval database under low-resource scenarios; 3) the observed differences across selected domains; 4) the quantitative analysis of sentence-level and word-level statistics; and 5) the qualitative analysis of representative translation cases.", "title": "humanintheloop machine translation with large language model", "url": "https://arxiv.org/pdf/2310.08908", "tokenized_text": "large_language large language llm garnered significant attention context_learning context learning mechanisms emergent capabilities research community conducted pilot studies apply llms machine_translation machine translation tasks evaluate performance diverse perspectives previous research primarily focused llm explored human intervention inference process llm characteristics llm context_learning context learning prompt_engineering engineering closely mirror human cognitive abilities language tasks offering intuitive solution human loop generation study propose human loop pipeline guides llms produce customized outputs revision instructions pipeline llm produce draft translation followed utilization automatic retrieval human feedback supervision signals enhance llm translation context_learning context learning human machine interactions generated pipeline stored external database expand context retrieval database enabling leverage human supervision offline setting evaluate proposed pipeline gpt-3.5 turbo api domain specific benchmarks german english translation results demonstrate_the_effectiveness demonstrate effectiveness pipeline tailoring domain translations improving translation performance compared direct translation instructions additionally discuss experimental_results experimental results following perspectives effectiveness different context retrieval methods construction retrieval database low resource scenarios observed differences selected domains quantitative analysis sentence level word level statistics qualitative analysis representative translation cases"}
{"id": "4b091d92f793161046b483ee93df244bf93bb508", "abstract": "The emergence of generative Large Language Models (LLMs) emphasizes the need for accurate and efficient prompting approaches. LLMs are often applied in Few-Shot Learning (FSL) contexts, where tasks are executed with minimal training data. FSL has become popular in many Artificial Intelligence (AI) subdomains, including AI for health. Rare diseases affect a small fraction of the population. Rare disease identification from clinical notes inherently requires FSL techniques due to limited data availability. Manual data collection and annotation is both expensive and time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a flexible prompting approach for improving the performance of LLM queries in FSL settings. MVP works by prompting numerous LLMs to perform the same tasks and then conducting a majority vote on the resulting outputs. This method achieves improved results to any one model in the ensemble on one-shot rare disease identification and classification tasks. We also release a novel rare disease dataset for FSL, available to those who signed the MIMIC-IV Data Use Agreement (DUA). Furthermore, in using MVP, each model is prompted multiple times, substantially increasing the time needed for manual annotation, and to address this, we assess the feasibility of using JSON for automating generative LLM evaluation.", "title": "large language models vote prompting for rare disease identification", "url": "https://arxiv.org/pdf/2308.12890", "tokenized_text": "emergence generative large_language large language llms emphasizes need accurate efficient approaches llms applied shot_learning shot learning fsl contexts tasks executed minimal training_data training data fsl popular artificial_intelligence artificial intelligence ai subdomains including ai health rare diseases affect small fraction population rare disease identification clinical notes inherently requires fsl techniques limited data availability manual data collection annotation expensive time consuming paper propose vote mvp flexible approach improving performance llm queries fsl settings mvp works numerous llms perform tasks conducting majority vote resulting outputs method_achieves method achieves improved results ensemble shot rare disease identification classification tasks release novel rare disease dataset fsl available data use agreement furthermore mvp prompted multiple times substantially increasing time needed manual annotation address assess feasibility json automating generative llm evaluation"}
{"id": "4cf527e9e0d68e3fc16d39fbcdb3869cd3ccf60f", "abstract": "Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which can then be robustly generalized to novel scenarios. Recent work has evaluated large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding\"in context learning.\"This can work well for straightforward inductive tasks, but performs very poorly on more complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be directly verified by running on the observed examples and generalized to novel inputs. Because of the prohibitive cost of generation with state-of-the-art LLMs, we consider a middle step to filter the set of hypotheses that will be implemented into programs: we either ask the LLM to summarize into a smaller set of hypotheses, or ask human annotators to select a subset of the hypotheses. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, and string transformation dataset SyGuS. On a random 40-problem subset of ARC, our automated pipeline using LLM summaries achieves 27.5% accuracy, significantly outperforming the direct prompting baseline (accuracy of 12.5%). With the minimal human input of selecting from LLM-generated candidates, the performance is boosted to 37.5%. (And we argue this is a lower bound on the performance of our approach without filtering.) Our ablation studies show that abstract hypothesis generation and concrete program representations are both beneficial for LLMs to perform inductive reasoning tasks.", "title": "hypothesis search inductive reasoning with language models", "url": "https://arxiv.org/pdf/2309.05660", "tokenized_text": "inductive reasoning core problem solving capacity humans identify underlying principles examples robustly generalized novel scenarios recent_work recent work evaluated large_language large language llms inductive reasoning tasks directly context_learning context learning work straightforward inductive tasks performs poorly complex tasks abstraction reasoning corpus arc work propose improve inductive reasoning ability llms generating explicit hypotheses multiple levels abstraction llm propose multiple abstract hypotheses problem natural_language natural language implement natural_language natural language hypotheses concrete python programs programs directly verified running observed examples generalized novel inputs prohibitive cost generation state art llms consider middle step filter set hypotheses implemented programs ask llm summarize smaller set hypotheses ask human annotators select subset hypotheses verify pipeline effectiveness arc visual inductive reasoning benchmark variant arc string transformation dataset random 40 problem subset arc automated pipeline llm summaries achieves accuracy significantly outperforming direct baseline accuracy 12.5 minimal human input selecting llm generated candidates performance boosted argue lower bound performance approach filtering ablation studies abstract hypothesis generation concrete program representations beneficial llms perform inductive reasoning tasks"}
{"id": "4ee96f0757e517928590a2300af5d40ba768a5a7", "abstract": "Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions that require complex reasoning over long narrative texts. PEARL outperforms zero-shot and chain-of-thought prompting on this dataset, and ablation experiments show that each stage of PEARL is critical to its performance. Overall, PEARL is a first step towards leveraging LLMs to reason over long documents.", "title": "pearl prompting large language models to plan and execute actions over long documents", "url": "http://arxiv.org/pdf/2305.14564", "tokenized_text": "strategies chain thought_prompting thought improve performance large_language large language llms complex_reasoning complex reasoning tasks decomposing input examples intermediate steps remains unclear apply methods reason long input documents decomposition output intermediate step non trivial obtain work propose framework improve reasoning long documents consists stages action mining plan formulation plan execution specifically given question long document decomposes question sequence actions e.g. summarize executes document obtain answer stage implemented zero shot shot_prompting shot llms work gpt-4 minimal human input evaluate challenging subset quality dataset contains questions require complex_reasoning complex reasoning long narrative texts outperforms zero shot chain thought_prompting thought dataset ablation experiments stage critical performance overall step leveraging llms reason long documents"}
{"id": "5db0f55332839c408e3049cea1a6ad48fefba70c", "abstract": "An important aspect of developing LLMs that interact with humans is to align models' behavior to their users. It is possible to prompt an LLM into behaving as a certain persona, especially a user group or ideological persona the model captured during its pertaining stage. But, how to best align an LLM with a specific user and not a demographic or ideological group remains an open question. Mining public opinion surveys (by Pew Research), we find that the opinions of a user and their demographics and ideologies are not mutual predictors. We use this insight to align LLMs by modeling both user opinions as well as user demographics and ideology, achieving up to 7 points accuracy gains in predicting public opinions from survey questions across a broad set of topics. In addition to the typical approach of prompting LLMs with demographics and ideology, we discover that utilizing the most relevant past opinions from individual users enables the model to predict user opinions more accurately.", "title": "aligning language models to user opinions", "url": "http://arxiv.org/pdf/2305.14929", "tokenized_text": "important aspect developing llms interact humans align behavior users possible llm behaving certain persona especially user group ideological persona captured pertaining stage best align llm specific user demographic ideological group remains open question mining public opinion surveys research find opinions user demographics ideologies mutual predictors use insight align llms modeling user opinions user demographics ideology achieving points accuracy gains predicting public opinions survey questions broad set topics addition typical approach llms demographics ideology discover utilizing relevant past opinions individual users enables predict user opinions accurately"}
{"id": "65fe385a665480b41fafc56d76a3bd72e92e8886", "abstract": "Summarizing book-length documents (>100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because human evaluation is expensive and time-consuming, we develop an automatic metric, BooookScore, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BooookScore has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving $15K and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than the oft-repetitive ones generated by LLaMA 2. Incremental updating yields lower BooookScore but higher level of detail than hierarchical merging, a trade-off sometimes preferred by human annotators. We release code and annotations after blind review to spur more principled research on book-length summarization.", "title": "booookscore a systematic exploration of booklength summarization in the era of llms", "url": "https://arxiv.org/pdf/2310.00785", "tokenized_text": "summarizing book length documents 100 tokens exceed context window size large_language large language llms requires breaking input document smaller llm merge update chunk level summaries despite complexity importance task studied challenges evaluation existing book length summarization datasets e.g. pretraining data public llms existing evaluation methods struggle capture errors modern llm paper present study coherence llm based book length implemented workflows hierarchically merging chunk level summaries incrementally updating running summary obtain fine grained human annotations gpt-4 generated summaries 100 recently published books identify common types coherence errors llms human evaluation expensive time consuming develop automatic metric measures proportion sentences summary contain identified error types high agreement human annotations allows systematically evaluate impact critical parameters e.g. chunk size base llm saving 15 500 hours human evaluation costs find closed source llms gpt-4 claude produce summaries higher repetitive ones generated llama incremental updating yields lower higher level detail hierarchical merging trade preferred human annotators release code annotations blind review spur principled research book length summarization"}
{"id": "6be6fe206f8ca735f8df26758bf877572abb10d3", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in natural language generation. However, their output quality can be inconsistent, posing challenges for generating natural language from logical forms (LFs). This task requires the generated outputs to embody the exact semantics of LFs, without missing any LF semantics or creating any hallucinations. In this work, we tackle this issue by proposing a novel generate-and-rerank approach. Our approach involves initially generating a set of candidate outputs by prompting an LLM and subsequently reranking them using a task-specific reranker model. In addition, we curate a manually collected dataset to evaluate the alignment between different ranking metrics and human judgements. The chosen ranking metrics are utilized to enhance the training and evaluation of the reranker model. By conducting extensive experiments on three diverse datasets, we demonstrate that the candidates selected by our reranker outperform those selected by baseline methods in terms of semantic consistency and fluency, as measured by three comprehensive metrics. Our findings provide strong evidence for the effectiveness of our approach in improving the quality of generated outputs.", "title": "reranking for natural language generation from logical forms a study based on large language models", "url": "https://arxiv.org/pdf/2309.12294", "tokenized_text": "large_language large language llms demonstrated impressive capabilities natural_language natural language generation output quality inconsistent posing challenges generating natural_language natural language logical forms lfs task requires generated outputs exact semantics lfs missing lf semantics creating hallucinations work tackle issue proposing novel generate rerank approach approach involves initially generating set candidate outputs llm subsequently reranking task specific reranker addition curate manually collected dataset evaluate alignment different ranking metrics human judgements chosen ranking metrics utilized enhance training evaluation reranker conducting extensive_experiments extensive experiments diverse datasets demonstrate candidates selected reranker outperform selected baseline methods terms semantic consistency fluency measured comprehensive metrics findings provide strong evidence effectiveness approach improving quality generated outputs"}
{"id": "72fb75f7c38a83424308c8205bb36cd88995494b", "abstract": "While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis. We explore prompt-engineering to explain the concept of n-best lists to the LLM; followed by the finetuning of Low-Rank Adapters on the downstream tasks. Our approach using n-best lists proves to be effective on a device-directed speech detection task as well as on a keyword spotting task, where systems using n-best list prompts outperform those using 1-best ASR hypothesis; thus paving the way for an efficient method to exploit ASR uncertainty via LLMs for speech-based applications.", "title": "leveraging large language models for exploiting asr uncertainty", "url": "https://arxiv.org/pdf/2309.04842", "tokenized_text": "large_language large language excel variety natural_language natural language processing nlp tasks perform spoken language understanding slu tasks rely shelf automatic speech recognition asr systems transcription equipped built speech modality work focuses scenario llm accuracy slu tasks constrained accuracy fixed asr system spoken input specifically tackle speech intent classification task high word error rate limit llm ability understand spoken intent instead high accuracy designing complex specialized architectures regardless deployment costs seek answer far substantially changing underlying asr llm potentially shared multiple unrelated tasks end propose llm best list asr hypotheses instead error prone best hypothesis explore engineering explain concept best lists llm followed finetuning adapters downstream_tasks downstream tasks approach best lists proves effective device directed speech detection task keyword spotting task systems best list outperform best asr hypothesis paving way efficient method exploit asr uncertainty llms speech based applications"}
{"id": "7d87fbdfbf5038a4e0ff09801b6d3b8a2e0c613a", "abstract": "A central notion in practical and theoretical machine learning is that of a $\\textit{weak learner}$, classifiers that achieve better-than-random performance (on any given distribution over data), even by a small margin. Such weak learners form the practical basis for canonical machine learning methods such as boosting. In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners. Specifically, we illustrate the use of a large language model (LLM) as a weak learner in a boosting algorithm applied to tabular data. We show that by providing (properly sampled according to the distribution of interest) text descriptions of tabular data samples, LLMs can produce a summary of the samples that serves as a template for classification and achieves the aim of acting as a weak learner on this task. We incorporate these models into a boosting approach, which in some settings can leverage the knowledge within the LLM to outperform traditional tree-based boosting. The model outperforms both few-shot learning and occasionally even more involved fine-tuning procedures, particularly for tasks involving small numbers of data points. The results illustrate the potential for prompt-based LLMs to function not just as few-shot learners themselves, but as components of larger machine learning pipelines.", "title": "language models are weak learners", "url": "http://arxiv.org/pdf/2306.14101", "tokenized_text": "central notion practical theoretical machine_learning machine learning classifiers achieve better random performance given distribution data small margin weak learners form practical basis canonical machine_learning machine learning methods boosting work illustrate based large_language large language operate effectively said weak learners specifically illustrate use large_language large language llm weak learner boosting algorithm applied tabular data providing properly sampled according distribution interest text descriptions tabular data samples llms produce summary samples serves template classification achieves aim acting weak learner task incorporate boosting approach settings leverage knowledge llm outperform traditional tree based boosting outperforms shot_learning shot learning occasionally involved fine tuning procedures particularly tasks involving small numbers data points results illustrate potential based llms function shot learners components larger machine_learning machine learning pipelines"}
{"id": "8d17234680db76f99efd22fbcb169f45d2d79d93", "abstract": "Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 9 datasets spanning language understanding and generation tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation by up to 25% and 14% respectively. Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.", "title": "connecting large language models with evolutionary algorithms yields powerful prompt optimizers", "url": "https://arxiv.org/pdf/2309.08532", "tokenized_text": "large_language large language llms excel tasks rely carefully crafted demand substantial human effort automate process paper propose_a_novel propose novel framework discrete prompt_optimization optimization called idea evolutionary algorithms exhibit good performance fast convergence enable work discrete natural_language natural language expressions need coherent human readable connect llms approach allows simultaneously leverage powerful language_processing language processing capabilities llms efficient optimization performance specifically gradients parameters starts population iteratively generates new llms based evolutionary operators improving population based development set optimize open source llms including gpt-3.5 alpaca datasets spanning language understanding generation tasks significantly_outperforms significantly outperforms human engineered existing_methods existing methods automatic generation 25 14 respectively furthermore demonstrates connecting llms creates synergies inspire research combination llms conventional algorithms"}
{"id": "8d9ca1e2c703e2752a4904c967a65d45d0bef5f6", "abstract": "To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs. Toward this end, we present Marked Personas, a prompt-based method to measure stereotypes in LLMs for intersectional demographic groups without any lexicon or data labeling.Grounded in the sociolinguistic concept of markedness (which characterizes explicitly linguistically marked categories versus unmarked defaults), our proposed method is twofold: 1) prompting an LLM to generate personas, i.e., natural language descriptions, of the target demographic group alongside personas of unmarked, default groups; 2) identifying the words that significantly distinguish personas of the target group from corresponding unmarked ones.We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts. The words distinguishing personas of marked (non-white, non-male) groups reflect patterns of othering and exoticizing these demographics. An intersectional lens further reveals tropes that dominate portrayals of marginalized groups, such as tropicalism and the hypersexualization of minoritized women. These representational harms have concerning implications for downstream applications like story generation.", "title": "marked personas using natural language prompts to measure stereotypes in language models", "url": "http://arxiv.org/pdf/2305.18189", "tokenized_text": "recognize mitigate harms large_language large language llms need understand prevalence nuances stereotypes llm outputs end present marked personas based method measure stereotypes llms demographic groups lexicon data labeling grounded concept characterizes explicitly linguistically marked categories versus proposed_method proposed method twofold llm generate personas i.e. natural_language natural language descriptions target demographic group alongside personas default groups identifying words significantly distinguish personas target group corresponding ones find generated gpt-3.5 gpt-4 contain higher rates racial stereotypes human written words distinguishing personas marked non white non male groups reflect patterns demographics lens reveals marginalized groups women representational harms concerning implications downstream applications like story generation"}
{"id": "9141480721653789597b6e537ee0eeab401f3e60", "abstract": "In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-shot NER, achieving a 4% (absolute) improvement in F1 score on the ConLL dataset, a 9% (absolute) improvement on the GENIA dataset, and a 4% (absolute) improvement on the FewNERD dataset. PromptNER also moves the state of the art on Cross Domain NER, outperforming prior methods (including those not limited to the few-shot setting), setting a new mark on 3/5 CrossNER target domains, with an average F1 gain of 3%, despite using less than 2% of the available data.", "title": "promptner prompting for named entity recognition", "url": "http://arxiv.org/pdf/2305.15444", "tokenized_text": "surprising turn large_language large language llms growing based heuristics offer powerful shelf approaches providing shot solutions myriad classic nlp problems despite promising early results llm based shot methods remain far state_of_the_art state art named_entity named entity recognition ner prevailing methods include learning representations end end structural understanding fine tuning standard labeled corpora paper introduce new state art algorithm shot cross domain ner adapt new ner task requires set entity definitions addition standard shot examples given sentence llm produce list potential entities corresponding explanations compatibility provided entity type definitions remarkably achieves_state achieves state art performance shot ner achieving absolute improvement f1_score f1 score dataset absolute improvement dataset absolute improvement dataset moves state_of_the_art state art cross domain ner outperforming prior methods including limited shot_setting shot setting setting new mark target domains average f1 gain despite available data"}
{"id": "96d6bb5d6abdeda9b2db9af6296527200ba7aa32", "abstract": "Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate prompting enhances LLM ToM reasoning, and they underscore the context-dependent nature of LLM cognitive capacities.", "title": "boosting theoryofmind performance in large language models via prompting", "url": "http://arxiv.org/pdf/2304.11490", "tokenized_text": "large_language large language llms excel tasks 2023 face challenges complex_reasoning complex reasoning theory mind tom tasks require understanding agents beliefs goals mental states essential common sense reasoning involving humans making crucial enhance llm performance area study measures tom performance gpt-4 gpt-3.5 variants gpt-3.5 turbo investigates effectiveness context_learning context learning improving tom comprehension evaluated featuring shot chain_of_thought chain thought reasoning step step thinking instructions found llms trained reinforcement_learning reinforcement learning human feedback rlhf excluding improved tom accuracy context_learning context learning gpt-4 performed best zero shot_settings shot settings reaching nearly 80 tom accuracy fell short human accuracy test set context_learning context learning rlhf trained llms 80 tom accuracy gpt-4 reaching 100 results_demonstrate results demonstrate appropriate enhances llm tom reasoning underscore context dependent nature llm cognitive capacities"}
{"id": "a21de70160c91dcf9b1e7a93fbb32f4b2687860a", "abstract": "Large language models (LLMs) have demonstrated impressive few-shot learning capabilities, but they often underperform compared to fine-tuned models on challenging tasks. Furthermore, their large size and restricted access only through APIs make task-specific fine-tuning impractical. Moreover, LLMs are sensitive to different aspects of prompts (e.g., the selection and order of demonstrations) and can thus require time-consuming prompt engineering. In this light, we propose a method to correct LLM outputs without relying on their weights. First, we generate a pool of candidates by few-shot prompting an LLM. Second, we refine the LLM-generated outputs using a smaller model, the LM-corrector (LMCor), which is trained to rank, combine and rewrite the candidates to produce the final target output. Our experiments demonstrate that even a small LMCor model (250M) substantially improves the few-shot performance of LLMs (62B) across diverse tasks. Moreover, we illustrate that the LMCor exhibits robustness against different prompts, thereby minimizing the need for extensive prompt engineering. Finally, we showcase that the LMCor can be seamlessly integrated with different LLMs at inference time, serving as a plug-and-play module to improve their performance.", "title": "small language models improve giants by rewriting their outputs", "url": "http://arxiv.org/pdf/2305.13514", "tokenized_text": "large_language large language llms demonstrated impressive shot_learning shot learning capabilities underperform compared fine tuned challenging tasks furthermore large size restricted access apis task specific fine tuning impractical llms sensitive different aspects e.g. selection order demonstrations require time consuming prompt_engineering engineering light propose method correct llm outputs relying weights generate pool candidates shot_prompting shot llm second refine llm generated outputs smaller lm trained rank combine rewrite candidates produce final target output experiments_demonstrate experiments demonstrate small 250 substantially improves shot performance llms 62b diverse tasks illustrate exhibits robustness different minimizing need extensive prompt_engineering engineering finally showcase seamlessly integrated different llms inference time serving plug play module improve performance"}
{"id": "a3509cef906a4517238c1764676cf637efcd1d5e", "abstract": "This paper presents an AI-assisted programming tool called Copilot for Xcode for program composition and design to support human software developers. By seamlessly integrating cloud-based Large Language Models (LLM) with Apple's local development environment, Xcode, this tool enhances productivity and unleashes creativity for software development in Apple software ecosystem (e.g., iOS apps, macOS). Leveraging advanced natural language processing (NLP) techniques, Copilot for Xcode effectively processes source code tokens and patterns within code repositories, enabling features such as code generation, autocompletion, documentation, and error detection. Software developers can also query and make\"small\"decisions for program composition, some of which can be made simultaneously, and this is facilitated through prompt engineering in a chat interface of Copilot for Xcode. Finally, we present simple case studies as evidence of the effectiveness of utilizing NLP in Xcode to prompt popular LLM services like OpenAI ChatGPT for program composition and design.", "title": "copilot for xcode exploring aiassisted programming by prompting cloudbased large language models", "url": "https://arxiv.org/pdf/2307.14349", "tokenized_text": "paper_presents paper presents ai assisted programming tool called copilot program composition design support human software developers seamlessly integrating cloud based large_language large language llm apple local development environment tool enhances productivity creativity software development apple software ecosystem e.g. apps leveraging advanced natural_language natural language processing nlp techniques copilot effectively processes source_code source code tokens patterns code repositories enabling features code_generation code generation documentation error detection software developers query program composition simultaneously facilitated prompt_engineering engineering chat interface copilot finally present simple case studies evidence effectiveness utilizing nlp popular llm services like openai chatgpt program composition design"}
{"id": "b43e9b674d4572e1aba8b40a28056ab118ad5e83", "abstract": "Business Process Management (BPM) aims to improve organizational activities and their outcomes by managing the underlying processes. To achieve this, it is often necessary to consider information from various sources, including unstructured textual documents. Therefore, researchers have developed several BPM-specific solutions that extract information from textual documents using Natural Language Processing techniques. These solutions are specific to their respective tasks and cannot accomplish multiple process-related problems as a general-purpose instrument. However, in light of the recent emergence of Large Language Models (LLMs) with remarkable reasoning capabilities, such a general-purpose instrument with multiple applications now appears attainable. In this paper, we illustrate how LLMs can accomplish text-related BPM tasks by applying a specific LLM to three exemplary tasks: mining imperative process models from textual descriptions, mining declarative process models from textual descriptions, and assessing the suitability of process tasks from textual descriptions for robotic process automation. We show that, without extensive configuration or prompt engineering, LLMs perform comparably to or better than existing solutions and discuss implications for future BPM research as well as practical usage.", "title": "large language models can accomplish business process management tasks", "url": "https://arxiv.org/pdf/2307.09923", "tokenized_text": "business_process_management business process management bpm aims improve activities outcomes managing underlying processes achieve necessary consider information sources including unstructured textual documents researchers developed bpm specific solutions extract information textual documents natural_language natural language processing techniques solutions specific respective tasks accomplish multiple process related problems general purpose instrument light recent emergence large_language large language llms remarkable reasoning capabilities general purpose instrument multiple applications appears paper illustrate llms accomplish text related bpm tasks applying specific llm tasks mining imperative process textual descriptions mining declarative process textual descriptions assessing suitability process tasks textual descriptions robotic process automation extensive configuration prompt_engineering engineering llms perform comparably better existing solutions discuss implications future bpm research practical usage"}
{"id": "bc70af9248d210663edf22e5fc84ca9313c697b0", "abstract": "The rapid expansion of the digital world has propelled sentiment analysis into a critical tool across diverse sectors such as marketing, politics, customer service, and healthcare. While there have been significant advancements in sentiment analysis for widely spoken languages, low-resource languages, such as Bangla, remain largely under-researched due to resource constraints. Furthermore, the recent unprecedented performance of Large Language Models (LLMs) in various applications highlights the need to evaluate them in the context of low-resource languages. In this study, we present a sizeable manually annotated dataset encompassing 33,605 Bangla news tweets and Facebook comments. We also investigate zero- and few-shot in-context learning with several language models, including Flan-T5, GPT-4, and Bloomz, offering a comparative analysis against fine-tuned models. Our findings suggest that monolingual transformer-based models consistently outperform other models, even in zero and few-shot scenarios. To foster continued exploration, we intend to make this dataset and our research tools publicly available to the broader research community. In the spirit of further research, we plan to make this dataset and our experimental resources publicly accessible to the wider research community.", "title": "zero and fewshot prompting with llms a comparative study with finetuned models for bangla sentiment analysis", "url": "https://arxiv.org/pdf/2308.10783", "tokenized_text": "rapid expansion digital world propelled sentiment_analysis sentiment analysis critical tool diverse sectors marketing politics customer service healthcare significant advancements sentiment_analysis sentiment analysis widely spoken languages low resource_languages resource languages bangla remain largely researched resource constraints furthermore recent unprecedented performance large_language large language llms applications highlights need evaluate context low resource_languages resource languages study present sizeable manually annotated dataset encompassing bangla news tweets facebook comments investigate zero- shot context_learning context learning language_models language including flan t5 gpt-4 bloomz offering comparative analysis fine tuned findings_suggest findings suggest monolingual transformer based consistently outperform zero shot scenarios foster continued exploration intend dataset research tools publicly_available publicly available broader research community spirit research plan dataset experimental resources publicly accessible wider research community"}
{"id": "c03fa01fbb9c77fe3d10609ba5f1dee33a723867", "abstract": "Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io", "title": "progprompt generating situated robot task plans using large language models", "url": "https://arxiv.org/pdf/2209.11302", "tokenized_text": "task planning require defining myriad domain knowledge world robot needs act ameliorate effort large_language large language llms score potential actions task planning generate action sequences directly given instruction natural_language natural language additional domain information methods require enumerating possible steps scoring generate free form text contain actions possible given robot current context present programmatic llm structure enables plan generation functional situated environments robot capabilities tasks key insight llm program like specifications available actions objects environment example programs executed concrete recommendations structure generation constraints ablation experiments demonstrate state_of_the_art state art success rates household tasks deploy method physical robot arm tabletop tasks website"}
{"id": "e0867e9f3a715851a90d17423f7f3b33a2a66bb1", "abstract": "Large language models (LLMs) are known to effectively perform tasks by simply observing few exemplars. However, in low-resource languages, obtaining such hand-picked exemplars can still be challenging, where unsupervised techniques may be necessary. Moreover, competent generative capabilities of LLMs are observed only in high-resource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance. To elicit LLMs' ability onto low-resource languages without any supervised data, we propose to assemble synthetic exemplars from a diverse set of high-resource languages to prompt the LLMs to translate from any language into English. These prompts are then used to create intra-lingual exemplars to perform tasks in the target languages. Our unsupervised prompting method performs on par with supervised few-shot learning in LLMs of different sizes for translations between English and 13 Indic and 21 African low-resource languages. We also show that fine-tuning a 7B model on data generated from our method helps it perform competitively with a 175B model. In non-English translation tasks, our method even outperforms supervised prompting by up to 3 chrF++ in many low-resource languages. When evaluated on zero-shot multilingual summarization, our method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is also favored by GPT-4.", "title": "democratizing llms for lowresource languages by leveraging their english dominant abilities with linguisticallydiverse prompts", "url": "http://arxiv.org/pdf/2306.11372", "tokenized_text": "large_language large language llms known effectively perform tasks simply observing exemplars low resource_languages resource languages obtaining hand picked exemplars challenging unsupervised techniques necessary competent generative capabilities llms observed high resource_languages resource languages performances represented languages fall pre training_data training data imbalance elicit llms ability low resource_languages resource languages supervised data propose assemble synthetic exemplars diverse set high resource_languages resource languages llms translate language english create intra lingual exemplars perform tasks target languages unsupervised method performs par supervised shot_learning shot learning llms different sizes translations english 13 21 african low resource_languages resource languages fine tuning 7b data generated method helps perform competitively 175b non english translation tasks method outperforms supervised chrf++ low resource_languages resource languages evaluated zero shot multilingual summarization method surpasses english pivoting baselines rouge gpt-4"}
{"id": "f64e49d76048c902cc02e8ae27dcd4ac0dbcb97f", "abstract": "Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 57 absolute points in micro-F1 and 79 points in macro-F1. Code, data, and models are available at https://github.com/epfl-dlab/SynthIE.", "title": "exploiting asymmetry for synthetic training data generation synthie and the case of information extraction", "url": "http://arxiv.org/pdf/2303.04132", "tokenized_text": "large_language large language llms great_potential great potential synthetic data generation work shows useful data synthetically generated tasks solved directly llms problems structured outputs possible llm perform task reverse direction generating plausible input text target output structure leveraging task difficulty makes possible produce large scale high quality data complex tasks demonstrate_the_effectiveness demonstrate effectiveness approach closed information_extraction information extraction collecting ground truth data challenging satisfactory dataset exists date synthetically generate dataset data points establish superior quality compared existing datasets human evaluation use finetune small 220 770 parameters termed outperform prior state_of_the_art state art equal model_size size substantial margin 57 absolute points micro f1 points macro f1 code data available"}
{"id": "f743287be3ced6757de7ecb26d03815b22cd737b", "abstract": "Large Language Models (LLMs) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an LLM to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader. The rewriter is trained using the feedback of the LLM reader by reinforcement learning. Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice QA. Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM.", "title": "query rewriting for retrievalaugmented large language models", "url": "http://arxiv.org/pdf/2305.14283", "tokenized_text": "large_language large language llms play powerful black box readers retrieve read pipeline making remarkable progress knowledge intensive tasks work introduces new framework rewrite retrieve read instead previous retrieve read retrieval augmented llms perspective query rewriting unlike prior studies focusing adapting retriever reader approach attention adaptation search query inevitably gap input text needed knowledge retrieval llm generate query use web search engine retrieve contexts furthermore better align query frozen modules propose trainable scheme pipeline small language_model language adopted trainable rewriter cater black box llm reader rewriter trained feedback llm reader reinforcement_learning reinforcement learning evaluation conducted downstream_tasks downstream tasks open domain qa multiple choice qa experiments results consistent performance improvement indicating framework proven effective scalable brings new framework retrieval augmented llm"}
{"id": "f8b5ee53c3410f20049e7def47bd52403fa388e3", "abstract": "Despite the success of large language models (LLMs), the task of theorem proving still remains one of the hardest reasoning tasks that is far from being fully solved. Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level theorems. One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process. However, as we all know, creating new useful theorems or even new theories is not only helpful but crucial and necessary for advancing mathematics and proving harder and deeper results. In this work, we present LEGO-Prover, which employs a growing skill library containing verified lemmas as skills to augment the capability of LLMs used in theorem proving. By constructing the proof modularly, LEGO-Prover enables LLMs to utilize existing skills retrieved from the library and to create new skills during the proving process. These skills are further evolved (by prompting an LLM) to enrich the library on another scale. Modular and reusable skills are constantly added to the library to enable tackling increasingly intricate mathematical problems. Moreover, the learned library further bridges the gap between human proofs and formal proofs by making it easier to impute missing steps. LEGO-Prover advances the state-of-the-art pass rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%). During the proving process, LEGO-Prover also manages to generate over 20,000 skills (theorems/lemmas) and adds them to the growing library. Our ablation study indicates that these newly added skills are indeed helpful for proving theorems, resulting in an improvement from a success rate of 47.1% to 50.4%. We also release our code and all the generated skills.", "title": "legoprover neural theorem proving with growing libraries", "url": "https://arxiv.org/pdf/2310.00656", "tokenized_text": "despite success large_language large language llms task theorem proving remains hardest reasoning tasks far fully solved prior methods language_models language demonstrated promising_results promising results struggle prove middle school level theorems common limitation methods assume fixed theorem library theorem proving process know creating new useful theorems new theories helpful crucial necessary advancing mathematics proving harder deeper results work present prover employs growing skill library containing verified lemmas skills augment capability llms theorem proving constructing proof prover enables llms utilize existing skills retrieved library create new skills proving process skills evolved llm enrich library scale modular reusable skills constantly added library enable tackling increasingly intricate mathematical problems learned library bridges gap human proofs formal proofs making easier missing steps prover advances state art pass rate minif2f valid 57.0 minif2f test proving process prover manages generate skills theorems lemmas adds growing library ablation study indicates newly added skills helpful proving theorems resulting improvement success_rate success rate release code generated skills"}
{"id": "33729913908d187dc0db6e41073c35643324fe4f", "abstract": "One of the exciting capabilities of recent language models for dialog is their ability to independently search for relevant information to ground a given dialog response. However, obtaining training data to teach models how to issue search queries is time and resource consuming. In this work, we propose q2d: an automatic data generation pipeline that generates information-seeking dialogs from questions. We prompt a large language model (PaLM) to create conversational versions of question answering datasets, and use it to improve query generation models that communicate with external search APIs to ground dialog responses. Unlike previous approaches which relied on human written dialogs with search queries, our method allows to automatically generate query-based grounded dialogs with better control and scale. Our experiments demonstrate that: (1) For query generation on the QReCC dataset, models trained on our synthetically-generated data achieve 90%--97% of the performance of models trained on the human-generated data; (2) We can successfully generate data for training dialog models in new domains without any existing dialog data as demonstrated on the multi-hop MuSiQue and Bamboogle QA datasets. (3) We perform a thorough analysis of the generated dialogs showing that humans find them of high quality and struggle to distinguish them from human-written dialogs.", "title": "q2d turning questions into dialogs to teach models how to search", "url": "http://arxiv.org/pdf/2304.14318", "tokenized_text": "exciting capabilities recent language_models language dialog ability independently search relevant information ground given dialog response obtaining training_data training data teach issue search queries time resource consuming work propose automatic data generation pipeline generates information seeking dialogs questions large_language large language palm create conversational versions question_answering question answering datasets use improve query generation communicate external search apis ground dialog responses unlike previous approaches relied human written dialogs search queries method allows automatically generate query based grounded dialogs better control scale experiments_demonstrate experiments demonstrate query generation dataset trained synthetically generated data achieve performance trained human generated data successfully generate data training dialog new domains existing dialog data demonstrated multi hop musique qa datasets perform thorough analysis generated dialogs showing humans find high_quality high quality struggle distinguish human written dialogs"}
{"id": "3436ff7a1dd4c6547ba78968d3eec2545a6dccb9", "abstract": "Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context learning. We perform comprehensive experiments with state-of-the-art mainstream models such as GPT-3 on various downstream tasks. Our results indicate that our method can enhance the model's in-context learning performance in an effective and interpretable manner.", "title": "fairnessguided fewshot prompting for large language models", "url": "http://arxiv.org/pdf/2303.13217", "tokenized_text": "large_language large language demonstrated surprising ability perform context_learning context learning i.e. directly applied solve numerous downstream_tasks downstream tasks conditioning constructed input output examples prior research shown context_learning context learning suffer high instability variations training_examples training examples example order formats construction appropriate essential improving performance context_learning context learning paper revisit problem view predictive bias specifically introduce metric evaluate predictive bias fixed labels given attributes empirically higher bias lead unsatisfactory predictive quality based observation propose_a_novel propose novel search strategy based greedy search identify near optimal improving performance context_learning context learning perform comprehensive experiments state art mainstream gpt-3 downstream_tasks downstream tasks results_indicate results indicate method enhance context_learning context learning performance effective interpretable manner"}
{"id": "49b499598a8864eee55ab264fc16a5bf8d2f87ef", "abstract": "Social computing prototypes probe the social behaviors that may arise in an envisioned system design. This prototyping practice is currently limited to recruiting small groups of people. Unfortunately, many challenges do not arise until a system is populated at a larger scale. Can a designer understand how a social system might behave when populated, and make adjustments to the design before the system falls prey to such challenges? We introduce social simulacra, a prototyping technique that generates a breadth of realistic social interactions that may emerge when a social computing system is populated. Social simulacra take as input the designer\u2019s description of a community\u2019s design\u2014goal, rules, and member personas\u2014and produce as output an instance of that design with simulated behavior, including posts, replies, and anti-social behaviors. We demonstrate that social simulacra shift the behaviors that they generate appropriately in response to design changes, and that they enable exploration of \u201cwhat if?\u201d scenarios where community members or moderators intervene. To power social simulacra, we contribute techniques for prompting a large language model to generate thousands of distinct community members and their social interactions with each other; these techniques are enabled by the observation that large language models\u2019 training data already includes a wide variety of positive and negative behavior on social media platforms. In evaluations, we show that participants are often unable to distinguish social simulacra from actual community behavior and that social computing designers successfully refine their social computing designs when using social simulacra.", "title": "social simulacra creating populated prototypes for social computing systems", "url": "https://dl.acm.org/doi/pdf/10.1145/3526113.3545616", "tokenized_text": "social computing prototypes probe social behaviors arise system design prototyping practice currently limited small groups people unfortunately challenges arise system larger scale designer understand social system behave adjustments design system falls challenges introduce social prototyping technique generates breadth realistic social interactions emerge social computing system social input designer description community design goal rules personas produce output instance design simulated behavior including posts replies anti social behaviors demonstrate social shift behaviors generate appropriately response design changes enable exploration scenarios community members moderators intervene power social contribute techniques large_language large language generate thousands distinct community members social interactions techniques enabled observation large_language large language training_data training data includes wide variety positive negative behavior social_media social media platforms evaluations participants unable distinguish social actual community behavior social computing designers successfully refine social computing designs social"}
{"id": "5581bf85386737bd3378eec68189759a05280bea", "abstract": "We present FOLIO, a human-annotated, open-domain, and logically complex and diverse dataset for reasoning in natural language (NL), equipped with first order logic (FOL) annotations. FOLIO consists of 1,435 examples (unique conclusions), each paired with one of 487 sets of premises which serve as rules to be used to deductively reason for the validity of each conclusion. The logical correctness of premises and conclusions is ensured by their parallel FOL annotations, which are automatically verified by our FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO automatically constitute a new NL-FOL translation dataset using FOL as the logical form. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models (BERT, RoBERTa) and few-shot prompting on large language models (GPT-NeoX, OPT, GPT-3, Codex). For NL-FOL translation, we experiment with GPT-3 and Codex. Our results show that one of the most capable Large Language Model (LLM) publicly available, GPT-3 davinci, achieves only slightly better than random results with few-shot prompting on a subset of FOLIO, and the model is especially bad at predicting the correct truth values for False and Unknown conclusions. Our dataset and code are available at https://github.com/Yale-LILY/FOLIO.", "title": "folio natural language reasoning with firstorder logic", "url": "http://arxiv.org/pdf/2209.00840", "tokenized_text": "present human annotated open domain logically complex diverse dataset reasoning natural_language natural language nl equipped order logic fol annotations consists examples unique conclusions paired sets premises serve rules reason validity conclusion logical correctness premises conclusions parallel fol annotations automatically verified fol inference engine addition main nl reasoning task nl fol pairs automatically constitute new nl fol translation dataset fol logical form experiments systematically evaluate fol reasoning ability supervised fine tuning medium sized language_models language bert roberta shot_prompting shot large_language large language gpt opt gpt-3 codex nl fol translation experiment gpt-3 codex results capable large_language large language llm publicly_available publicly available gpt-3 davinci achieves slightly better random results shot_prompting shot subset especially bad predicting correct truth values false unknown conclusions dataset code available"}
{"id": "64ce6ef1f5cf227bf2bf917c87273386ae16256f", "abstract": "Large language models (LLMs) demonstrate remarkable machine translation (MT) abilities via prompting, even though they were not explicitly trained for this task. However, even given the incredible quantities of data they are trained on, LLMs can struggle to translate inputs with rare words, which are common in low resource or domain transfer scenarios. We show that LLM prompting can provide an effective solution for rare words as well, by using prior knowledge from bilingual dictionaries to provide control hints in the prompts. We propose a novel method, DiPMT, that provides a set of possible translations for a subset of the input words, thereby enabling fine-grained phrase-level prompted control of the LLM. Extensive experiments show that DiPMT outperforms the baseline both in low-resource MT, as well as for out-of-domain MT. We further provide a qualitative analysis of the benefits and limitations of this approach, including the overall level of controllability that is achieved.", "title": "dictionarybased phraselevel prompting of large language models for machine translation", "url": "http://arxiv.org/pdf/2302.07856", "tokenized_text": "large_language large language llms demonstrate remarkable machine_translation machine translation mt abilities explicitly trained task given incredible quantities data trained llms struggle translate inputs rare words common low_resource low resource domain transfer scenarios llm provide effective solution rare words prior knowledge bilingual dictionaries provide control hints propose_a_novel propose novel method provides set possible translations subset input words enabling fine grained phrase level prompted control llm extensive_experiments extensive experiments outperforms baseline low resource mt domain mt provide qualitative analysis benefits limitations approach including overall level controllability achieved"}
{"id": "6af986a2cab884fbd30ad6da2928dc19c12d83a7", "abstract": "In-context learning (ICL) performs tasks by prompting a large language model (LLM) using an instruction and a small set of annotated examples called demonstrations. Recent work has shown that precise details of the inputs used in the ICL prompt significantly impact performance, which has incentivized instruction selection algorithms. The effect of instruction-choice however is severely underexplored, with existing analyses restricted to shallow subsets of models and tasks, limiting the generalizability of their insights. We develop InstructEval, an ICL evaluation suite to conduct a thorough assessment of these techniques. The suite includes 13 open-sourced LLMs of varying scales from four model families, and covers nine tasks across three categories. Using the suite, we evaluate the relative performance of seven popular instruction selection methods over five metrics relevant to ICL. Our experiments reveal that using curated manually-written instructions or simple instructions without any task-specific descriptions often elicits superior ICL performance overall than that of automatic instruction-induction methods, pointing to a lack of generalizability among the latter. We release our evaluation suite for benchmarking instruction selection approaches and enabling more generalizable methods in this space.", "title": "instructeval systematic evaluation of instruction selection methods", "url": "https://arxiv.org/pdf/2307.00259", "tokenized_text": "context_learning context learning icl performs tasks large_language large language llm instruction small set annotated examples called demonstrations recent_work recent work shown precise details inputs icl significantly impact performance instruction selection algorithms effect instruction choice severely underexplored existing analyses restricted shallow subsets tasks limiting generalizability insights develop icl evaluation suite conduct thorough assessment techniques suite includes 13 open sourced llms varying scales families covers tasks categories suite evaluate relative performance seven popular instruction selection methods metrics relevant icl experiments reveal curated manually written instructions simple instructions task specific descriptions elicits superior icl performance overall automatic instruction induction methods lack generalizability release evaluation suite benchmarking instruction selection approaches enabling generalizable methods space"}
{"id": "70b73e272621562c6261f86d2ebf814703b760ed", "abstract": "Language models contain ranking-based knowledge and are powerful solvers of in-context ranking tasks. For instance, they may have parametric knowledge about the ordering of countries by size or may be able to rank reviews by sentiment. Recent work focuses on pairwise, pointwise, and listwise prompting techniques to elicit a language model's ranking knowledge. However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce. This motivates us to explore an alternative approach that is inspired by an unsupervised probing method called Contrast-Consistent Search (CCS). The idea is to train a probing model guided by a logical constraint: a model's representation of a statement and its negation must be mapped to contrastive true-false poles consistently across multiple statements. We hypothesize that similar constraints apply to ranking tasks where all items are related via consistent pairwise or listwise comparisons. To this end, we extend the binary CCS method to Contrast-Consistent Ranking (CCR) by adapting existing ranking methods such as the Max-Margin Loss, Triplet Loss, and Ordinal Regression objective. Our results confirm that, for the same language model, CCR probing outperforms prompting and even performs on a par with prompting much larger language models.", "title": "unsupervised contrastconsistent ranking with language models", "url": "https://arxiv.org/pdf/2309.06991", "tokenized_text": "language_models language contain ranking based knowledge powerful solvers context ranking tasks instance parametric knowledge ordering countries size able rank reviews sentiment recent_work recent work focuses pairwise pointwise listwise prompting_techniques techniques elicit language_model language ranking knowledge find careful calibration constrained decoding based techniques self consistent rankings produce motivates explore alternative approach inspired unsupervised probing method called search idea train probing guided logical constraint representation statement negation mapped contrastive true false consistently multiple statements hypothesize similar constraints apply ranking tasks items related consistent pairwise listwise comparisons end extend binary method ranking adapting existing ranking methods max margin loss triplet loss ordinal regression objective results confirm language_model language probing outperforms performs par larger language_models language"}
{"id": "71d68782c3da41b77866c2fd0cb65726f60b3af1", "abstract": "Chain-of-thought (CoT) prompting has been shown to empirically improve the accuracy of large language models (LLMs) on various question answering tasks. While understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment. We address this question by leveraging gradient-based feature attribution methods which produce saliency scores that capture the influence of input tokens on model output. Specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens. Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to question perturbations and variations in model output.", "title": "analyzing chainofthought prompting in large language models via gradientbased feature attributions", "url": "https://arxiv.org/pdf/2307.13339", "tokenized_text": "chain thought cot shown empirically improve accuracy large_language large language llms question_answering question answering tasks understanding cot_prompting cot effective crucial ensuring phenomenon consequence desired behavior little work addressed nonetheless understanding critical prerequisite responsible deployment address question leveraging gradient based feature attribution methods produce saliency scores capture influence input tokens output specifically probe open source llms investigate cot_prompting cot affects relative assign particular input tokens results_indicate results indicate cot_prompting cot increase magnitude saliency scores attributed semantically relevant tokens compared standard shot_prompting shot increases robustness saliency scores question perturbations variations output"}
{"id": "73397ec77081b46f5e49a4e7486129fe2ffe7adf", "abstract": "The goal of this paper is open-vocabulary object detection (OVOD) $\\unicode{x2013}$ building a model that can detect objects beyond the set of categories seen at training, thus enabling the user to specify categories of interest at inference without the need for model retraining. We adopt a standard two-stage object detector architecture, and explore three ways for specifying novel categories: via language descriptions, via image exemplars, or via a combination of the two. We make three contributions: first, we prompt a large language model (LLM) to generate informative language descriptions for object classes, and construct powerful text-based classifiers; second, we employ a visual aggregator on image exemplars that can ingest any number of images as input, forming vision-based classifiers; and third, we provide a simple method to fuse information from language descriptions and image exemplars, yielding a multi-modal classifier. When evaluating on the challenging LVIS open-vocabulary benchmark we demonstrate that: (i) our text-based classifiers outperform all previous OVOD works; (ii) our vision-based classifiers perform as well as text-based classifiers in prior work; (iii) using multi-modal classifiers perform better than either modality alone; and finally, (iv) our text-based and multi-modal classifiers yield better performance than a fully-supervised detector.", "title": "multimodal classifiers for openvocabulary object detection", "url": "http://arxiv.org/pdf/2306.05493", "tokenized_text": "goal paper open vocabulary object_detection object detection building detect objects set categories seen training enabling user specify categories interest inference need retraining adopt standard stage object detector architecture explore ways specifying novel categories language descriptions image exemplars combination contributions large_language large language llm generate informative language descriptions object classes construct powerful text based classifiers second employ visual image exemplars ingest number images input forming vision based classifiers provide simple method fuse information language descriptions image exemplars yielding multi modal classifier evaluating challenging lvis open vocabulary benchmark demonstrate text based classifiers outperform previous works ii vision based classifiers perform text based classifiers prior_work prior work iii multi modal classifiers perform better modality finally iv text based multi modal classifiers yield better performance fully supervised detector"}
{"id": "8da6e4537122af618c36563caef5863f8728d789", "abstract": "Large language models (LLMs) are increasingly capable and prevalent, and can be used to produce creative content. The quality of content is influenced by the prompt used, with more specific prompts that incorporate examples generally producing better results. On from this, it could be seen that using instructions written for crowdsourcing tasks (that are specific and include examples to guide workers) could prove effective LLM prompts. To explore this, we used a previous crowdsourcing pipeline that gave examples to people to help them generate a collectively diverse corpus of motivational messages. We then used this same pipeline to generate messages using GPT-4, and compared the collective diversity of messages from: (1) crowd-writers, (2) GPT-4 using the pipeline, and (3&4) two baseline GPT-4 prompts. We found that the LLM prompts using the crowdsourcing pipeline caused GPT-4 to produce more diverse messages than the two baseline prompts. We also discuss implications from messages generated by both human writers and LLMs.", "title": "prompting a large language model to generate diverse motivational messages a comparison with humanwritten messages", "url": "https://arxiv.org/pdf/2308.13479", "tokenized_text": "large_language large language llms increasingly capable prevalent produce creative content quality content influenced specific incorporate examples generally producing better results seen instructions written crowdsourcing tasks specific include examples guide workers prove effective llm explore previous crowdsourcing pipeline gave examples people help generate collectively diverse corpus motivational messages pipeline generate messages gpt-4 compared collective diversity messages crowd writers gpt-4 pipeline baseline gpt-4 found llm crowdsourcing pipeline caused gpt-4 produce diverse messages baseline discuss implications messages generated human writers llms"}
{"id": "9573e2025440219a1d3393664b3c80bda51ac8f4", "abstract": "Planning for goal-oriented dialogue often requires simulating future dialogue interactions and estimating task progress. Many approaches thus consider training neural networks to perform look-ahead search algorithms such as A* search and Monte Carlo Tree Search (MCTS). However, this training often requires abundant annotated data, which creates challenges when faced with noisy annotations or low-resource settings. We introduce GDP-Zero, an approach using Open-Loop MCTS to perform goal-oriented dialogue policy planning without any model training. GDP-Zero prompts a large language model to act as a policy prior, value function, user simulator, and system model during the tree search. We evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find that its responses are preferred over ChatGPT up to 59.32% of the time, and are rated more persuasive than ChatGPT during interactive evaluations.", "title": "promptbased montecarlo tree search for goaloriented dialogue policy planning", "url": "http://arxiv.org/pdf/2305.13660", "tokenized_text": "planning goal oriented dialogue requires simulating future dialogue interactions estimating task progress approaches consider training neural_networks neural networks perform look ahead search algorithms search monte carlo tree search training requires abundant annotated_data annotated data creates challenges faced noisy annotations low resource settings introduce zero approach open loop perform goal oriented dialogue policy planning training zero large_language large language act policy prior value function user simulator system tree search evaluate zero goal oriented task find responses preferred chatgpt time rated persuasive chatgpt interactive evaluations"}
{"id": "9ea3d90a172a0b5799c13287484f7406946f7311", "abstract": "We propose eXtensible Prompt (X-Prompt) for prompting a large language model (LLM) beyond natural language (NL). X-Prompt instructs an LLM with not only NL but also an extensible vocabulary of imaginary words that are introduced to help represent what NL words hardly describe, allowing a prompt to be more descriptive. Like NL prompts, X-Prompt is out-of-distribution (OOD) robust, for which we propose context-guided learning with prompt augmentation to learn its imaginary words for general usability, enabling them to use in different prompt contexts for fine-grain specifications. The promising results of X-Prompt demonstrate its potential of approaching advanced interaction between humans and LLMs to bridge their communication gap.", "title": "extensible prompts for language models", "url": "https://arxiv.org/pdf/2212.00616", "tokenized_text": "propose extensible large_language large language llm natural_language natural language nl instructs llm nl extensible vocabulary words introduced help represent nl words hardly describe allowing descriptive like nl distribution ood robust propose context guided learning augmentation learn words general usability enabling use different contexts fine grain specifications promising_results promising results demonstrate potential approaching advanced interaction humans llms bridge communication gap"}
{"id": "a4929de687f3c6937dabbf733258af635781d3c4", "abstract": "Code LLMs are being rapidly deployed and there is evidence that they can make professional programmers more productive. Current benchmarks for code generation measure whether models generate correct programs given an expert prompt. In this paper, we present a new benchmark containing multiple prompts per problem, written by a specific population of non-expert prompters: beginning programmers. StudentEval contains 1,749 prompts for 48 problems, written by 80 students who have only completed one semester of Python programming. Our students wrote these prompts while working interactively with a Code LLM, and we observed very mixed success rates. We use StudentEval to evaluate 5 Code LLMs and find that StudentEval is a better discriminator of model performance than existing benchmarks. We analyze the prompts and find significant variation in students' prompting techniques. We also find that nondeterministic LLM sampling could mislead students into thinking that their prompts are more (or less) effective than they actually are, which has implications for how to teach with Code LLMs.", "title": "studenteval a benchmark of studentwritten prompts for large language models of code", "url": "http://arxiv.org/pdf/2306.04556", "tokenized_text": "code llms rapidly deployed evidence professional programmers productive current benchmarks code_generation code generation measure generate correct programs given expert paper present new benchmark containing multiple problem written specific population non expert prompters beginning programmers contains 48 problems written 80 students completed python programming students wrote working interactively code llm observed mixed success rates use evaluate code llms find better discriminator performance existing benchmarks analyze find significant variation students prompting_techniques techniques find llm sampling mislead students thinking effective actually implications teach code llms"}
{"id": "b2542a738b75ee9b7ce1a13d8b78f9095d212412", "abstract": "Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GenRead), which first prompts a large language model to generate contextutal documents based on a given question, and then reads the generated documents to produce the final answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, resulting in the generated documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GenRead achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation. Our code and generated documents can be found at https://github.com/wyu97/GenRead.", "title": "generate rather than retrieve large language models are strong context generators", "url": "http://arxiv.org/pdf/2209.10063", "tokenized_text": "knowledge intensive tasks open domain question_answering question answering qa require access large world domain knowledge common approach knowledge intensive tasks employ retrieve read pipeline retrieves handful relevant contextual documents external corpus wikipedia predicts answer conditioned retrieved documents paper present novel perspective solving knowledge intensive tasks replacing document retrievers large_language large language generators method generate read large_language large language generate documents based given question generated documents produce final answer furthermore propose_a_novel propose novel clustering based method selects distinct resulting generated documents cover different perspectives leading better recall acceptable answers conduct_extensive conduct extensive experiments different knowledge intensive tasks including open domain qa fact_checking fact checking dialogue system notably achieves exact match scores significantly outperforming state art retrieve read pipeline fid retrieving documents external_knowledge external knowledge source lastly demonstrate performance improved combining retrieval generation code generated documents found"}
{"id": "b9c263500281e05fddfe1f84839491f605815230", "abstract": "Intent discovery is the task of inferring latent intents from a set of unlabeled utterances, and is a useful step towards the efficient creation of new conversational agents. We show that recent competitive methods in intent discovery can be outperformed by clustering utterances based on abstractive summaries, i.e., \u201clabels\u201d, that retain the core elements while removing non-essential information. We contribute the IDAS approach, which collects a set of descriptive utterance labels by prompting a Large Language Model, starting from a well-chosen seed set of prototypical utterances, to bootstrap an In-Context Learning procedure to generate labels for non-prototypical utterances. The utterances and their resulting noisy labels are then encoded by a frozen pre-trained encoder, and subsequently clustered to recover the latent intents. For the unsupervised task (without any intent labels) IDAS outperforms the state-of-the-art by up to +7.42% in standard cluster metrics for the Banking, StackOverflow, and Transport datasets. For the semi-supervised task (with labels for a subset of intents) IDAS surpasses 2 recent methods on the CLINC benchmark without even using labeled data.", "title": "idas intent discovery with abstractive summarization", "url": "http://arxiv.org/pdf/2305.19783", "tokenized_text": "intent discovery task inferring latent intents set unlabeled utterances useful step efficient creation new conversational agents recent competitive methods intent discovery outperformed clustering utterances based abstractive summaries i.e. labels retain core elements removing non essential information contribute approach collects set descriptive utterance labels large_language large language starting chosen seed set prototypical utterances bootstrap context_learning context learning procedure generate labels non prototypical utterances utterances resulting noisy labels encoded frozen pre trained encoder subsequently clustered recover latent intents unsupervised task intent labels outperforms state art standard cluster metrics transport datasets semi supervised task labels subset intents surpasses recent methods benchmark labeled_data labeled data"}
{"id": "d318e0169f649656c71f02a1f84194a734fe1962", "abstract": "Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by prompting a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of the desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperform RL agents trained with reward functions learned via supervised learning", "title": "reward design with language models", "url": "http://arxiv.org/pdf/2303.00001", "tokenized_text": "reward design reinforcement_learning reinforcement learning rl challenging specifying human notions desired behavior difficult reward functions require expert demonstrations instead cheaply design rewards natural_language natural language interface paper explores simplify reward design large_language large language llm gpt-3 proxy reward function user provides textual containing examples shot description zero shot desired behavior approach leverages proxy reward function rl framework specifically users specify beginning training training llm evaluates rl agent behavior desired behavior described outputs corresponding reward signal rl agent uses reward update behavior evaluate approach train agents aligned user objectives game matrix games negotiation task tasks rl agents trained framework aligned user objectives outperform rl agents trained reward functions learned supervised learning"}
{"id": "d75d11d2c89c01cd284383546ae057cb827dc272", "abstract": "Chain-of-thought (CoT) prompting with large language models has proven effective in numerous natural language processing tasks, but designing prompts that generalize well to diverse problem types can be challenging, especially in the context of math word problem (MWP) solving. Additionally, it is common to have a large amount of training data that have a better diversity coverage but CoT annotations are not available, which limits the use of supervised learning techniques. To address these issues, we investigate two approaches to leverage the training data in a few-shot prompting scenario: dynamic program prompting and program distillation. Our approach is largely inspired by Gao et al., (2022), where they proposed to replace the CoT with the programs as the intermediate reasoning step. Such a prompting strategy allows us to accurately verify the answer correctness through program execution in MWP solving. Our dynamic program prompting involves annotating the training data by sampling correct programs from a large language model, while program distillation involves adapting a smaller model to the program-annotated training data. Our experiments on three standard MWP datasets demonstrate the effectiveness of these approaches, yielding significant improvements over previous baselines for prompting and fine-tuning. Our results suggest that leveraging a large amount of training data can improve the generalization ability of prompts and boost the performance of fine-tuned small models in MWP solving.", "title": "leveraging training data in fewshot prompting for numerical reasoning", "url": "http://arxiv.org/pdf/2305.18170", "tokenized_text": "chain thought cot large_language large language proven effective numerous natural_language natural language processing tasks designing generalize diverse problem types challenging especially context math word problem mwp solving additionally common large training_data training data better diversity coverage cot annotations available limits use supervised learning techniques address issues investigate approaches leverage training_data training data shot_prompting shot scenario dynamic program program distillation approach largely inspired et_al et al 2022 proposed replace cot programs intermediate reasoning step strategy allows accurately verify answer correctness program execution mwp solving dynamic program involves annotating training_data training data sampling correct programs large_language large language program distillation involves adapting smaller program annotated training_data training data experiments standard mwp datasets demonstrate_the_effectiveness demonstrate effectiveness approaches yielding significant improvements previous baselines fine tuning results suggest leveraging large training_data training data improve generalization_ability generalization ability boost performance fine tuned small mwp solving"}
{"id": "e1dafedfbb55cd2200411841c2ec40e7ea827773", "abstract": "Prompt engineering is a new paradigm for enhancing the performance of trained neural network models. For optimizing text-style prompts, existing methods usually individually operate small portions of a text step by step, which either breaks the fluency or could not globally adjust a prompt. Since large language models (LLMs) have powerful ability of generating coherent texts token by token, can we utilize LLMs for improving prompts? Based on this motivation, in this paper, considering a trained LLM as a text generator, we attempt to design a black-box evolution algorithm for automatically optimizing texts, namely SPELL (Semantic Prompt Evolution based on a LLM). The proposed method is evaluated with different LLMs and evolution parameters in different text tasks. Experimental results show that SPELL could rapidly improve the prompts indeed. We further explore the evolution process and discuss on the limitations, potential possibilities and future work.", "title": "spell semantic prompt evolution based on a llm", "url": "https://arxiv.org/pdf/2310.01260", "tokenized_text": "prompt_engineering engineering new_paradigm new paradigm enhancing performance trained neural network optimizing text style existing_methods existing methods usually individually operate small portions text step_by_step step step breaks fluency adjust large_language large language llms powerful ability generating coherent texts token token utilize llms improving based motivation paper considering trained llm text generator attempt design black box evolution algorithm automatically optimizing texts evolution based llm proposed_method proposed method evaluated different llms evolution parameters different text tasks experimental_results experimental results rapidly improve explore evolution process discuss limitations potential possibilities future work"}
{"id": "fed7e4a0e8c798777f3f1613be62a2dfb776b462", "abstract": "In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on unseen classes. To remedy this overconfidence, we introduce Contrastive Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD examples representative of novel classes, then trains to decrease confidence on them. First, we generate OOD examples by prompting a large language model twice: we prompt it to enumerate relevant novel classes, then generate examples from each novel class matching the task format. Second, we train a classifier with a novel contrastive objective that encourages lower confidence on generated OOD examples than training examples. When trained with CoNAL, classifiers improve in their ability to detect and abstain on novel class examples over prior methods by an average of 2.3% in terms of accuracy under the accuracy-coverage curve (AUAC) and 5.5% AUROC across 4 NLP datasets, with no cost to in-distribution accuracy.", "title": "contrastive noveltyaugmented learning anticipating outliers with large language models", "url": "https://aclanthology.org/2023.acl-long.658.pdf", "tokenized_text": "task settings text_classification text classification likely encounter examples novel classes predict correctly selective prediction abstain low confidence examples provides possible solution existing overly confident unseen classes remedy overconfidence introduce contrastive_novelty-augmented_learning contrastive novelty-augmented learning conal step method generates ood examples representative novel classes trains decrease confidence generate ood examples large_language large language twice enumerate relevant novel classes generate examples novel class matching task format second train classifier novel contrastive objective encourages lower confidence generated ood examples training_examples training examples trained conal classifiers improve ability detect abstain novel class examples prior methods average 2.3 terms accuracy accuracy coverage curve auac 5.5 auroc nlp datasets cost distribution accuracy"}
{"id": "0894585294c67193ff3190240554677b56fd79a0", "abstract": "Large Language Models (LLMs) have found widespread applications in various domains, including web applications, where they facilitate human interaction via chatbots with natural language interfaces. Internally, aided by an LLM-integration middleware such as Langchain, user prompts are translated into SQL queries used by the LLM to provide meaningful responses to users. However, unsanitized user prompts can lead to SQL injection attacks, potentially compromising the security of the database. Despite the growing interest in prompt injection vulnerabilities targeting LLMs, the specific risks of generating SQL injection attacks through prompt injections have not been extensively studied. In this paper, we present a comprehensive examination of prompt-to-SQL (P$_2$SQL) injections targeting web applications based on the Langchain framework. Using Langchain as our case study, we characterize P$_2$SQL injections, exploring their variants and impact on application security through multiple concrete examples. Furthermore, we evaluate 7 state-of-the-art LLMs, demonstrating the pervasiveness of P$_2$SQL attacks across language models. Our findings indicate that LLM-integrated applications based on Langchain are highly susceptible to P$_2$SQL injection attacks, warranting the adoption of robust defenses. To counter these attacks, we propose four effective defense techniques that can be integrated as extensions to the Langchain framework. We validate the defenses through an experimental evaluation with a real-world use case application.", "title": "from prompt injections to sql injection attacks how protected is your llmintegrated web application", "url": "https://arxiv.org/pdf/2308.01990", "tokenized_text": "large_language large language llms found widespread applications domains including web applications facilitate human interaction chatbots natural_language natural language interfaces aided llm integration middleware user translated sql queries llm provide meaningful responses users unsanitized user lead sql injection attacks potentially compromising security database despite growing interest prompt_injection injection vulnerabilities targeting llms specific risks generating sql injection attacks injections extensively studied paper present comprehensive examination sql injections targeting web applications based framework case study characterize injections exploring variants impact application security multiple concrete examples furthermore evaluate state art llms demonstrating attacks language_models language findings indicate llm integrated applications based highly susceptible injection attacks adoption robust defenses counter attacks propose effective defense techniques integrated framework validate defenses experimental evaluation real world use case application"}
{"id": "1c475acaa1060c8318a625f24bfd88c12f367516", "abstract": "Recent works have shown that attaching prompts to the input is effective at conditioning Language Models (LM) to perform specific tasks. However, prompts are always included in the input text during inference, thus incurring substantial computational and memory overhead. Also, there is currently no straightforward method of utilizing prompts that are longer than the maximum input length of the LMs without incurring additional costs during inference. We propose Prompt Injection (PI), a novel formulation of injecting the prompt into the parameters of an LM to be an efficient alternative to attaching fixed prompts to the input. We show that in scenarios with long fixed prompts, PI can be up to 280 times more efficient in terms of total FLOPs than previous approaches. We further explore methodologies for PI and show promising results in persona-dependent conversation, semantic parsing, and zero-shot learning with task instructions. Through these explorations, we show that PI can be a promising direction for conditioning language models, especially in scenarios with long and fixed prompts.", "title": "prompt injection parameterization of fixed inputs", "url": "http://arxiv.org/pdf/2206.11349", "tokenized_text": "recent works shown attaching input effective conditioning language_models language lm perform specific tasks included input text inference substantial computational memory overhead currently straightforward method utilizing longer maximum input length lms additional costs inference propose prompt_injection injection pi novel formulation injecting parameters lm efficient alternative attaching fixed input scenarios long fixed pi times efficient terms total flops previous approaches explore methodologies pi promising_results promising results persona dependent conversation semantic_parsing semantic parsing zero shot_learning shot learning task instructions explorations pi promising direction conditioning language_models language especially scenarios long fixed"}
{"id": "2427527c1a1bc61b32c28a107192c3e22ed629bb", "abstract": "Large language models (LLMs) exhibit remarkable performance improvement through in-context learning (ICL) by leveraging task-specific examples in the input. However, the mechanisms behind this improvement remain elusive. In this work, we investigate embeddings and attention representations in Llama-2 70B and Vicuna 13B. Specifically, we study how embeddings and attention change after in-context-learning, and how these changes mediate improvement in behavior. We employ neuroscience-inspired techniques, such as representational similarity analysis (RSA), and propose novel methods for parameterized probing and attention ratio analysis (ARA, measuring the ratio of attention to relevant vs. irrelevant information). We designed three tasks with a priori relationships among their conditions: reading comprehension, linear regression, and adversarial prompt injection. We formed hypotheses about expected similarities in task representations to investigate latent changes in embeddings and attention. Our analyses revealed a meaningful correlation between changes in both embeddings and attention representations with improvements in behavioral performance after ICL. This empirical framework empowers a nuanced understanding of how latent representations affect LLM behavior with and without ICL, offering valuable tools and insights for future research and practical applications.", "title": "incontext learning in large language models a neuroscienceinspired analysis of representations", "url": "https://arxiv.org/pdf/2310.00313", "tokenized_text": "large_language large language llms exhibit remarkable performance improvement context_learning context learning icl leveraging task specific examples input mechanisms improvement remain elusive work investigate embeddings attention representations llama-2 70b vicuna 13b. specifically study embeddings attention change context learning changes improvement behavior employ neuroscience inspired techniques representational similarity analysis propose novel methods parameterized probing attention ratio analysis measuring ratio attention relevant vs. irrelevant information designed tasks priori relationships conditions reading comprehension linear regression adversarial prompt_injection injection formed hypotheses expected similarities task representations investigate latent changes embeddings attention analyses revealed meaningful correlation changes embeddings attention representations improvements behavioral performance icl empirical framework empowers nuanced understanding latent representations affect llm behavior icl offering valuable tools insights future_research future research practical applications"}
{"id": "8c035150f883007b5af9e5bb753b78d9c0b75a55", "abstract": "ChatGPT and other large language models (LLMs) have proven useful in crowdsourcing tasks, where they can effectively annotate machine learning training data. However, this means that they also have the potential for misuse, specifically to automatically answer surveys. LLMs can potentially circumvent quality assurance measures, thereby threatening the integrity of methodologies that rely on crowdsourcing surveys. In this paper, we propose a mechanism to detect LLM-generated responses to surveys. The mechanism uses\"prompt injection\", such as directions that can mislead LLMs into giving predictable responses. We evaluate our technique against a range of question scenarios, types, and positions, and find that it can reliably detect LLM-generated responses with more than 93% effectiveness. We also provide an open-source software to help survey designers use our technique to detect LLM responses. Our work is a step in ensuring that survey methodologies remain rigorous vis-a-vis LLMs.", "title": "safeguarding crowdsourcing surveys from chatgpt with prompt injection", "url": "http://arxiv.org/pdf/2306.08833", "tokenized_text": "chatgpt large_language large language llms proven useful crowdsourcing tasks effectively annotate machine_learning machine learning training_data training data means potential misuse specifically automatically answer surveys llms potentially circumvent quality assurance measures integrity methodologies rely crowdsourcing surveys paper propose mechanism detect llm generated responses surveys mechanism injection directions mislead llms giving predictable responses evaluate technique range question scenarios types positions find reliably detect llm generated responses effectiveness provide open source software help survey designers use technique detect llm responses work step ensuring survey methodologies remain rigorous llms"}
{"id": "9be0dea0d6b892a2162490fb02712efaf10c0c87", "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable potential across various downstream tasks. LLM-integrated frameworks, which serve as the essential infrastructure, have given rise to many LLM-integrated web apps. However, some of these frameworks suffer from Remote Code Execution (RCE) vulnerabilities, allowing attackers to execute arbitrary code on apps' servers remotely via prompt injections. Despite the severity of these vulnerabilities, no existing work has been conducted for a systematic investigation of them. This leaves a great challenge on how to detect vulnerabilities in frameworks as well as LLM-integrated apps in real-world scenarios. To fill this gap, we present two novel strategies, including 1) a static analysis-based tool called LLMSmith to scan the source code of the framework to detect potential RCE vulnerabilities and 2) a prompt-based automated testing approach to verify the vulnerability in LLM-integrated web apps. We discovered 13 vulnerabilities in 6 frameworks, including 12 RCE vulnerabilities and 1 arbitrary file read/write vulnerability. 11 of them are confirmed by the framework developers, resulting in the assignment of 7 CVE IDs. After testing 51 apps, we found vulnerabilities in 17 apps, 16 of which are vulnerable to RCE and 1 to SQL injection. We responsibly reported all 17 issues to the corresponding developers and received acknowledgments. Furthermore, we amplify the attack impact beyond achieving RCE by allowing attackers to exploit other app users (e.g. app responses hijacking, user API key leakage) without direct interaction between the attacker and the victim. Lastly, we propose some mitigating strategies for improving the security awareness of both framework and app developers, helping them to mitigate these risks effectively.", "title": "demystifying rce vulnerabilities in llmintegrated apps", "url": "https://arxiv.org/pdf/2309.02926", "tokenized_text": "recent_years recent years large_language large language llms demonstrated_remarkable demonstrated remarkable potential downstream_tasks downstream tasks llm integrated frameworks serve essential infrastructure given rise llm integrated web apps frameworks suffer remote code execution vulnerabilities allowing attackers execute arbitrary code apps servers remotely injections despite vulnerabilities existing work conducted systematic investigation great challenge detect vulnerabilities frameworks llm integrated apps real world_scenarios world scenarios fill gap present novel strategies including static analysis based tool called scan source_code source code framework detect potential vulnerabilities based automated testing approach verify vulnerability llm integrated web apps discovered 13 vulnerabilities frameworks including 12 vulnerabilities arbitrary file read write vulnerability 11 confirmed framework developers resulting assignment ids testing 51 apps found vulnerabilities 17 apps 16 vulnerable sql injection responsibly reported 17 issues corresponding developers received furthermore amplify attack impact achieving allowing attackers exploit app users e.g. app responses user api key leakage direct interaction attacker lastly propose mitigating strategies improving security awareness framework app developers helping mitigate risks effectively"}
{"id": "db4cf9f6a653d5c15973e836c800ea47743251ae", "abstract": "Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.", "title": "prompt injection attack against llmintegrated applications", "url": "http://arxiv.org/pdf/2306.05499", "tokenized_text": "large_language large language llms renowned superior proficiency language comprehension generation stimulate ecosystem applications extensive services introduces significant security risks study complexities implications prompt_injection injection attacks actual llm integrated applications initially conduct exploratory analysis commercial applications highlighting constraints current attack strategies practice prompted limitations subsequently formulate novel black box prompt_injection injection attack technique draws inspiration traditional web injection attacks crucial elements seamlessly incorporated pre constructed injection inducing context partition malicious designed fulfill attack objectives leveraging unveil previously unknown severe attack outcomes arbitrary llm usage application theft deploy 36 actual llm integrated applications discern 31 applications susceptible prompt_injection injection 10 vendors validated discoveries including notion potential impact millions users investigation possible risks prompt_injection injection attacks possible tactics mitigation"}
{"id": "04892382200a9d48ad5f8d3cb3cd3d63a8206a01", "abstract": "Facilitated by large language models (LLMs), personalized text generation has become a rapidly growing research direction. Most existing studies focus on designing specialized models for a particular domain, or they require fine-tuning the LLMs to generate personalized text. We consider a typical scenario in which the large language model, which generates personalized output, is frozen and can only be accessed through APIs. Under this constraint, all one can do is to improve the input text (i.e., text prompts) sent to the LLM, a procedure that is usually done manually. In this paper, we propose a novel method to automatically revise prompts for personalized text generation. The proposed method takes the initial prompts generated by a state-of-the-art, multistage framework for personalized generation and rewrites a few critical components that summarize and synthesize the personal context. The prompt rewriter employs a training paradigm that chains together supervised learning (SL) and reinforcement learning (RL), where SL reduces the search space of RL and RL facilitates end-to-end training of the rewriter. Using datasets from three representative domains, we demonstrate that the rewritten prompts outperform both the original prompts and the prompts optimized via supervised learning or reinforcement learning alone. In-depth analysis of the rewritten prompts shows that they are not only human readable, but also able to guide manual revision of prompts when there is limited resource to employ reinforcement learning to train the prompt rewriter, or when it is costly to deploy an automatic prompt rewriter for inference.", "title": "automatic prompt rewriting for personalized text generation", "url": "https://arxiv.org/pdf/2310.00152", "tokenized_text": "facilitated large_language large language llms personalized text generation rapidly growing research direction existing studies focus designing specialized particular domain require fine tuning llms generate personalized text consider typical scenario large_language large language generates personalized output frozen accessed apis constraint improve input text i.e. text sent llm procedure usually manually paper propose_a_novel propose novel method automatically revise personalized text generation proposed_method proposed method takes initial generated state art multistage framework personalized generation rewrites critical components summarize synthesize personal context rewriter employs training paradigm chains supervised learning sl reinforcement_learning reinforcement learning rl sl reduces search space rl rl facilitates end end training rewriter datasets representative domains demonstrate rewritten outperform original optimized supervised learning reinforcement_learning reinforcement learning depth analysis rewritten shows human readable able guide manual revision limited resource employ reinforcement_learning reinforcement learning train rewriter costly deploy automatic rewriter inference"}
{"id": "07759a84f27e43cfa5bc8d579f8227c96e6ae1dc", "abstract": "Prompting has shown impressive success in enabling large pre-trained language models (LMs) to perform diverse NLP tasks, especially with only few downstream data. Automatically finding the optimal prompt for each task, however, is challenging. Most existing work resorts to tuning *soft* prompts (e.g., embeddings) which fall short of interpretability, reusability across LMs, and applicability when gradients are not accessible. *Discrete* prompts, on the other hand, are difficult to optimize, and are often created by \u201cenumeration (e.g., paraphrasing)-then-selection\u201d heuristics that do not explore the prompt space systematically. This paper proposes RLPrompt, an efficient discrete prompt optimization approach with reinforcement learning (RL). RLPrompt formulates a parameter-efficient policy network that generates the optimized discrete prompt after training with reward. To harness the complex and stochastic reward signals from the large LM environment, we incorporate effective reward stabilization that substantially enhances training efficiency. RLPrompt is flexibly applicable to different types of LMs, such as masked (e.g., BERT) and left-to-right models (e.g., GPTs), for both classification and generation tasks. Experiments on few-shot classification and unsupervised text style transfer show superior performance over a wide range of existing fine-tuning or prompting methods. Interestingly, the resulting optimized prompts are often ungrammatical gibberish text; and surprisingly, those gibberish prompts are transferrable between different LMs to retain significant performance, indicating that LM prompting may not follow human language patterns.", "title": "rlprompt optimizing discrete text prompts with reinforcement learning", "url": "http://arxiv.org/pdf/2205.12548", "tokenized_text": "shown_impressive shown impressive success enabling large pre trained_language trained language lms perform diverse nlp_tasks nlp tasks especially downstream data automatically finding optimal task challenging existing work resorts tuning soft e.g. embeddings fall short interpretability reusability lms applicability gradients accessible discrete hand difficult optimize created enumeration e.g. selection heuristics explore space systematically paper_proposes paper proposes rlprompt efficient discrete prompt_optimization optimization approach reinforcement_learning reinforcement learning rl rlprompt formulates parameter efficient policy network generates optimized discrete training reward harness complex stochastic reward signals large lm environment incorporate effective reward substantially enhances training efficiency rlprompt flexibly applicable different types lms masked e.g. bert left right e.g. gpts classification generation tasks experiments shot classification unsupervised text style_transfer style transfer superior_performance superior performance wide_range wide range existing fine tuning methods interestingly resulting optimized text surprisingly different lms retain significant performance indicating lm follow human language patterns"}
{"id": "0ad677b4172e5aef8b18bc6832145d1a03e11da4", "abstract": "In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization. We identify a previously overlooked objective of query dependency in such optimization and elucidate two ensuing challenges that impede the successful and economical design of prompt optimization techniques. One primary issue is the absence of an effective method to evaluate prompts during inference when the golden answer is unavailable. Concurrently, learning via interactions with the LLMs to navigate the expansive natural language prompting space proves to be resource-intensive. To address this, we introduce Prompt-OIRL, which harnesses offline inverse reinforcement learning to draw insights from offline prompting demonstration data. Such data exists as by-products when diverse prompts are benchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependent prompt optimization objective is achieved by first learning an offline reward model. This model can evaluate any query-prompt pairs without accessing LLMs. Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt. Our experimental evaluations across various LLM scales and arithmetic reasoning datasets underscore both the efficacy and economic viability of the proposed approach.", "title": "querydependent prompt evaluation and optimization with offline inverse rl", "url": "https://arxiv.org/pdf/2309.06553", "tokenized_text": "study aim enhance arithmetic reasoning ability large_language large language llms zero shot prompt_optimization optimization identify previously overlooked objective query dependency optimization elucidate challenges impede successful economical design prompt_optimization optimization techniques primary issue absence effective method evaluate inference golden answer unavailable concurrently learning interactions llms navigate expansive natural_language natural language space proves resource intensive address introduce harnesses offline inverse reinforcement_learning reinforcement learning draw insights offline demonstration data data exists products diverse benchmarked open accessible datasets query dependent prompt_optimization optimization objective achieved learning offline reward evaluate query pairs accessing llms subsequently best strategy deployed recommend optimal experimental evaluations llm scales arithmetic reasoning datasets underscore efficacy economic viability proposed approach"}
{"id": "0da5adf32fe7501a5b98eb6549b2c42af08ee094", "abstract": "The Segmentation Anything Model (SAM) has recently emerged as a foundation model for addressing image segmentation. Owing to the intrinsic complexity of medical images and the high annotation cost, the medical image segmentation (MIS) community has been encouraged to investigate SAM's zero-shot capabilities to facilitate automatic annotation. Inspired by the extraordinary accomplishments of interactive medical image segmentation (IMIS) paradigm, this paper focuses on assessing the potential of SAM's zero-shot capabilities within the IMIS paradigm to amplify its benefits in the MIS domain. Regrettably, we observe that SAM's vulnerability to prompt forms (e.g., points, bounding boxes) becomes notably pronounced in IMIS. This leads us to develop a framework that adaptively offers suitable prompt forms for human experts. We refer to the framework above as temporally-extended prompts optimization (TEPO) and model it as a Markov decision process, solvable through reinforcement learning. Numerical experiments on the standardized benchmark BraTS2020 demonstrate that the learned TEPO agent can further enhance SAM's zero-shot capability in the MIS context.", "title": "temporallyextended prompts optimization for sam in interactive medical image segmentation", "url": "http://arxiv.org/pdf/2306.08958", "tokenized_text": "segmentation sam recently emerged foundation addressing image segmentation owing intrinsic complexity medical images high annotation cost medical image segmentation community encouraged investigate sam zero shot capabilities facilitate automatic annotation inspired extraordinary interactive medical image segmentation paradigm paper focuses assessing potential sam zero shot capabilities paradigm amplify benefits domain observe sam vulnerability forms e.g. points bounding boxes notably pronounced leads develop framework adaptively offers suitable forms human experts refer framework temporally extended optimization markov decision process solvable reinforcement_learning reinforcement learning numerical experiments standardized benchmark demonstrate learned agent enhance sam zero shot capability context"}
{"id": "1e8403af2e1e7a8f803d8df9e8daac584f99c2a0", "abstract": "Text-to-3D modelling has seen exciting progress by combining generative text-to-image models with image-to-3D methods like Neural Radiance Fields. DreamFusion recently achieved high-quality results but requires a lengthy, per-prompt optimization to create 3D objects. To address this, we amortize optimization over text prompts by training on many prompts simultaneously with a unified model, instead of separately. With this, we share computation across a prompt set, training in less time than per-prompt optimization. Our framework - Amortized text-to-3D (ATT3D) - enables knowledge-sharing between prompts to generalize to unseen setups and smooth interpolations between text for novel assets and simple animations.", "title": "att3d amortized textto3d object synthesis", "url": "http://arxiv.org/pdf/2306.07349", "tokenized_text": "text to-3d modelling seen exciting progress combining generative text image image to-3d methods like neural fields dreamfusion recently achieved high quality results requires lengthy prompt_optimization optimization create 3d objects address optimization text training simultaneously unified instead separately share computation set training time prompt_optimization optimization framework text to-3d enables knowledge sharing generalize unseen setups smooth text novel assets simple"}
{"id": "294b4613b21abf1e9ba499de274569360093b107", "abstract": "Emerging foundation models in machine learning are models trained on vast amounts of data that have been shown to generalize well to new tasks. Often these models can be prompted with multi-modal inputs that range from natural language descriptions over images to point clouds. In this paper, we propose topological data analysis (TDA) guided prompt optimization for the Segment Anything Model (SAM) and show preliminary results in the biological image segmentation domain. Our approach replaces the standard grid search approach that is used in the original implementation and finds point locations based on their topological significance. Our results show that the TDA optimized point cloud is much better suited for finding small objects and massively reduces computational complexity despite the extra step in scenarios which require many segmentations.", "title": "topological data analysis guided segment anything model prompt optimization for zeroshot segmentation in biological imaging", "url": "http://arxiv.org/pdf/2306.17400", "tokenized_text": "emerging foundation_models foundation machine_learning machine learning trained vast amounts data shown generalize new tasks prompted multi modal inputs range natural_language natural language descriptions images point clouds paper propose topological data analysis guided prompt_optimization optimization segment_anything_model segment sam preliminary results biological image segmentation domain approach replaces standard grid search approach original implementation finds point locations based topological significance results optimized point_cloud point cloud better suited finding small objects massively reduces computational complexity despite extra step scenarios require segmentations"}
{"id": "2e588fe7e07948cb9112c37d5e9dcc3a13b1bd0f", "abstract": "Social media platforms such as Instagram and Twitter have emerged as critical channels for drug marketing and illegal sale. Detecting and labeling online illicit drug trafficking activities becomes important in addressing this issue. However, the effectiveness of conventional supervised learning methods in detecting drug trafficking heavily relies on having access to substantial amounts of labeled data, while data annotation is time-consuming and resource-intensive. Furthermore, these models often face challenges in accurately identifying trafficking activities when drug dealers use deceptive language and euphemisms to avoid detection. To overcome this limitation, we conduct the first systematic study on leveraging large language models (LLMs), such as ChatGPT, to detect illicit drug trafficking activities on social media. We propose an analytical framework to compose \\emph{knowledge-informed prompts}, which serve as the interface that humans can interact with and use LLMs to perform the detection task. Additionally, we design a Monte Carlo dropout based prompt optimization method to further to improve performance and interpretability. Our experimental findings demonstrate that the proposed framework outperforms other baseline language models in terms of drug trafficking detection accuracy, showing a remarkable improvement of nearly 12\\%. By integrating prior knowledge and the proposed prompts, ChatGPT can effectively identify and label drug trafficking activities on social networks, even in the presence of deceptive language and euphemisms used by drug dealers to evade detection. The implications of our research extend to social networks, emphasizing the importance of incorporating prior knowledge and scenario-based prompts into analytical tools to improve online security and public safety.", "title": "unveiling the potential of knowledgeprompted chatgpt for enhancing drug trafficking detection on social media", "url": "https://arxiv.org/pdf/2307.03699", "tokenized_text": "social_media social media platforms twitter emerged critical channels drug marketing illegal detecting labeling online drug activities important addressing issue effectiveness conventional supervised learning methods detecting drug heavily relies having access substantial amounts labeled_data labeled data data annotation time consuming resource intensive furthermore face challenges accurately identifying activities drug use deceptive language avoid detection overcome limitation conduct systematic study leveraging large_language large language llms chatgpt detect drug activities social_media social media propose analytical framework compose informed serve interface humans interact use llms perform detection task additionally design monte_carlo monte carlo dropout based prompt_optimization optimization method improve performance interpretability experimental findings demonstrate proposed framework outperforms baseline language_models language terms drug detection accuracy showing remarkable improvement nearly 12\\% integrating prior knowledge proposed chatgpt effectively identify label drug activities social networks presence deceptive language drug evade detection implications research extend social networks emphasizing importance incorporating prior knowledge scenario based analytical tools improve online security public safety"}
{"id": "3120c2763edab339b937ddbe76991ebdfe0e01e6", "abstract": "Existing approaches to automatic data transformation are insufficient to meet the requirements in many real-world scenarios, such as the building sector. First, there is no convenient interface for domain experts to provide domain knowledge easily. Second, they require significant training data collection overheads. Third, the accuracy suffers from complicated schema changes. To bridge this gap, we present a novel approach that leverages the unique capabilities of large language models (LLMs) in coding, complex reasoning, and zero-shot learning to generate SQL code that transforms the source datasets into the target datasets. We demonstrate the viability of this approach by designing an LLM-based framework, termed SQLMorpher, which comprises a prompt generator that integrates the initial prompt with optional domain knowledge and historical patterns in external databases. It also implements an iterative prompt optimization mechanism that automatically improves the prompt based on flaw detection. The key contributions of this work include (1) pioneering an end-to-end LLM-based solution for data transformation, (2) developing a benchmark dataset of 105 real-world building energy data transformation problems, and (3) conducting an extensive empirical evaluation where our approach achieved 96% accuracy in all 105 problems. SQLMorpher demonstrates the effectiveness of utilizing LLMs in complex, domain-specific challenges, highlighting the potential of their potential to drive sustainable solutions.", "title": "automatic data transformation using large language model an experimental study on building energy data", "url": "https://arxiv.org/pdf/2309.01957", "tokenized_text": "existing approaches automatic data transformation insufficient meet requirements real world_scenarios world scenarios building sector convenient interface domain experts provide domain knowledge easily second require significant training_data training data collection overheads accuracy suffers complicated schema changes bridge gap present novel_approach novel approach leverages unique capabilities large_language large language llms coding complex_reasoning complex reasoning zero shot_learning shot learning generate sql code transforms source datasets target datasets demonstrate viability approach designing llm based framework termed comprises generator integrates initial domain knowledge historical patterns external databases implements iterative prompt_optimization optimization mechanism automatically improves prompt_based based detection key contributions work include pioneering end end llm based solution data transformation developing benchmark dataset 105 real world building energy data transformation problems conducting extensive empirical evaluation approach achieved 96 accuracy 105 problems demonstrates effectiveness utilizing llms complex domain specific challenges highlighting potential potential drive sustainable solutions"}
{"id": "515cf674fcdced5a7d5bb156dd5fcc1f5290e79b", "abstract": "Large-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the development dataset with a similar distribution as the evaluation set. However, it is unclear how the choice of these in-context examples and their ordering impacts the output translation quality. In this work, we aim to understand the properties of good in-context examples for MT in both in-domain and out-of-domain settings. We show that the translation quality and the domain of the in-context examples matter and that 1-shot noisy unrelated example can have a catastrophic impact on output quality. While concatenating multiple random examples reduces the effect of noise, a single good prompt optimized to maximize translation quality on the development dataset can elicit learned information from the pre-trained language model. Adding similar examples based on an n-gram overlap with the test source significantly and consistently improves the translation quality of the outputs, outperforming a strong kNN-MT baseline in 2 out of 4 out-of-domain datasets.", "title": "incontext examples selection for machine translation", "url": "https://arxiv.org/pdf/2212.02437", "tokenized_text": "large scale generative impressive ability perform wide_range wide range natural_language natural language processing nlp tasks context_learning context learning examples describe task machine_translation machine translation mt examples typically randomly sampled development dataset similar distribution evaluation set unclear choice context_examples context examples ordering impacts output translation quality work aim understand properties good context_examples context examples mt domain domain settings translation quality domain context_examples context examples matter shot noisy unrelated example catastrophic impact output quality concatenating multiple random examples reduces effect noise single good optimized maximize translation quality development dataset elicit learned information pre trained_language trained language adding similar examples based gram overlap test source significantly consistently improves translation quality outputs outperforming strong knn mt baseline domain datasets"}
{"id": "838e1317454724a9bb758d05d97e6058e11a8251", "abstract": "This paper presents AutoHint, a novel framework for automatic prompt engineering and optimization for Large Language Models (LLM). While LLMs have demonstrated remarkable ability in achieving high-quality annotation in various tasks, the key to applying this ability to specific tasks lies in developing high-quality prompts. Thus we propose a framework to inherit the merits of both in-context learning and zero-shot learning by incorporating enriched instructions derived from input-output demonstrations to optimize original prompt. We refer to the enrichment as the hint and propose a framework to automatically generate the hint from labeled data. More concretely, starting from an initial prompt, our method first instructs a LLM to deduce new hints for selected samples from incorrect predictions, and then summarizes from per-sample hints and adds the results back to the initial prompt to form a new, enriched instruction. The proposed method is evaluated on the BIG-Bench Instruction Induction dataset for both zero-shot and few-short prompts, where experiments demonstrate our method is able to significantly boost accuracy for multiple tasks.", "title": "autohint automatic prompt optimization with hint generation", "url": "https://arxiv.org/pdf/2307.07415", "tokenized_text": "paper_presents paper presents novel framework automatic prompt_engineering engineering optimization large_language large language llm llms demonstrated_remarkable demonstrated remarkable ability achieving high quality annotation tasks key applying ability specific tasks lies developing high quality propose framework inherit merits context_learning context learning zero shot_learning shot learning incorporating enriched instructions derived input output demonstrations optimize original refer hint propose framework automatically generate hint labeled_data labeled data concretely starting initial method instructs llm new hints selected samples incorrect predictions summarizes sample hints adds results initial form new enriched instruction proposed_method proposed method evaluated big-bench instruction induction dataset zero shot short experiments_demonstrate experiments demonstrate method able significantly boost accuracy multiple tasks"}
{"id": "b0b237dd905f12b23e3fc48ac7139e275158a007", "abstract": "In recent years, prompt tuning has proven effective in adapting pre-trained vision-language models to downstream tasks. These methods aim to adapt the pre-trained models by introducing learnable prompts while keeping pre-trained weights frozen. However, learnable prompts can affect the internal representation within the self-attention module, which may negatively impact performance variance and generalization, especially in data-deficient settings. To address these issues, we propose a novel approach, Read-only Prompt Optimization (RPO). RPO leverages masked attention to prevent the internal representation shift in the pre-trained model. Further, to facilitate the optimization of RPO, the read-only prompts are initialized based on special tokens of the pre-trained model. Our extensive experiments demonstrate that RPO outperforms CLIP and CoCoOp in base-to-new generalization and domain generalization while displaying better robustness. Also, the proposed method achieves better generalization on extremely data-deficient settings, while improving parameter efficiency and computational overhead. Code is available at https://github.com/mlvlab/RPO.", "title": "readonly prompt optimization for visionlanguage fewshot learning", "url": "https://arxiv.org/pdf/2308.14960", "tokenized_text": "recent_years recent years tuning proven effective adapting pre trained vision language_models language downstream_tasks downstream tasks methods aim adapt pre trained introducing learnable keeping pre trained weights frozen learnable affect internal representation self attention module negatively impact performance variance generalization especially data deficient settings address issues propose_a_novel propose novel approach read prompt_optimization optimization leverages masked attention prevent internal representation shift pre trained facilitate optimization read initialized based special tokens pre trained extensive_experiments extensive experiments demonstrate outperforms clip cocoop base new generalization domain generalization displaying better robustness proposed_method proposed method achieves better generalization extremely data deficient settings improving parameter efficiency computational overhead code_is_available code available"}
{"id": "b349f3dd5b764168cba57bb4ad3fc240c2b3eddf", "abstract": "As the next-generation paradigm for content creation, AI-Generated Content (AIGC), i.e., generating content automatically by Generative AI (GAI) based on user prompts, has gained great attention and success recently. With the ever-increasing power of GAI, especially the emergence of Pretrained Foundation Models (PFMs) that contain billions of parameters and prompt engineering methods (i.e., finding the best prompts for the given task), the application range of AIGC is rapidly expanding, covering various forms of information for human, systems, and networks, such as network designs, channel coding, and optimization solutions. In this article, we present the concept of mobile-edge AI-Generated Everything (AIGX). Specifically, we first review the building blocks of AIGX, the evolution from AIGC to AIGX, as well as practical AIGX applications. Then, we present a unified mobile-edge AIGX framework, which employs edge devices to provide PFM-empowered AIGX services and optimizes such services via prompt engineering. More importantly, we demonstrate that suboptimal prompts lead to poor generation quality, which adversely affects user satisfaction, edge network performance, and resource utilization. Accordingly, we conduct a case study, showcasing how to train an effective prompt optimizer using ChatGPT and investigating how much improvement is possible with prompt engineering in terms of user experience, quality of generation, and network performance.", "title": "optimizing mobileedge aigenerated everything (aigx) services by prompt engineering fundamental, framework, and case study", "url": "https://arxiv.org/pdf/2309.01065", "tokenized_text": "generation paradigm content creation content aigc i.e. generating content automatically generative_ai generative ai based user gained great attention success recently increasing power especially emergence pretrained foundation_models foundation pfms contain billions parameters prompt_engineering engineering methods i.e. finding best given task application range aigc rapidly expanding covering forms information human systems networks network designs channel coding optimization solutions article present concept mobile edge specifically review building blocks evolution aigc practical applications present unified mobile edge framework employs edge devices provide pfm empowered services optimizes services prompt_engineering engineering importantly demonstrate suboptimal lead poor generation quality affects user satisfaction edge network performance resource utilization accordingly conduct case study showcasing train effective optimizer chatgpt investigating improvement possible prompt_engineering engineering terms user experience quality generation network performance"}
{"id": "c76dd4a70361c3afd2e19d046343e2dedd16ecc3", "abstract": "Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language\"gradients\"that criticize the current prompt. The gradients are then\"propagated\"into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.", "title": "automatic prompt optimization with gradient descent and beam search", "url": "http://arxiv.org/pdf/2305.03495", "tokenized_text": "large_language large language llms shown_impressive shown impressive performance general_purpose general purpose agents abilities remain highly dependent hand written trial error effort propose simple solution problem automatic prompt_optimization optimization apo inspired numerical gradient descent automatically improve assuming access training_data training data llm api algorithm uses data form natural current gradients editing opposite semantic direction gradient gradient descent steps guided beam search bandit selection procedure significantly improves algorithmic efficiency preliminary results benchmark nlp_tasks nlp tasks novel problem llm jailbreak detection suggest automatic prompt_optimization optimization outperform prior editing techniques improve initial performance 31 data rewrite task descriptions precise annotation instructions"}
{"id": "d61f0820943a667917fb6d32225826aa5279f694", "abstract": "Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker with excellent results. While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet. Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel discrete prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for re-ranking. Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update. The experimental results demonstrate that Co-Prompt leads to outstanding re-ranking performance against the baselines. Also, Co-Prompt generates more interpretable prompts for humans against other prompt optimization methods.", "title": "discrete prompt optimization via constrained generation for zeroshot reranker", "url": "http://arxiv.org/pdf/2305.13729", "tokenized_text": "rankers order retrieved documents respect relevance score given query gained attention information retrieval ir task fine tuning pre trained_language trained language plm large scale language_model language llm utilized zero shot ranker excellent results llm highly dependent impact optimization zero shot ranker explored highlighting impact optimization zero shot ranker propose_a_novel propose novel discrete prompt_optimization optimization method constrained generation co metric estimating optimum ranking co guides generated texts plm optimal based metric parameter update experimental_results experimental results demonstrate co leads outstanding ranking performance baselines co generates interpretable humans prompt_optimization optimization methods"}
{"id": "ef5cd0eb266e3df3eb64aec18e1854fe0244d228", "abstract": "Conditional natural language generation methods often require either expensive fine-tuning or training a large language model from scratch. Both are unlikely to lead to good results without a substantial amount of data and computational resources. Prompt learning without changing the parameters of a large language model presents a promising alternative. It is a cost-effective approach, while still achieving competitive results. While this procedure is now established for zero- and few-shot text classification and structured prediction, it has received limited attention in conditional text generation. We present the first automatic prompt optimization approach for emotion-conditioned text generation with instruction-fine-tuned models. Our method uses an iterative optimization procedure that changes the prompt by adding, removing, or replacing tokens. As objective function, we only require a text classifier that measures the realization of the conditional variable in the generated text. We evaluate the method on emotion-conditioned text generation with a focus on event reports and compare it to manually designed prompts that also act as the seed for the optimization procedure. The optimized prompts achieve 0.75 macro-average F1 to fulfill the emotion condition in contrast to manually designed seed prompts with only 0.22 macro-average F1.", "title": "emotionconditioned text generation through automatic prompt optimization", "url": "https://arxiv.org/pdf/2308.04857", "tokenized_text": "conditional natural_language natural language generation methods require expensive fine tuning training large_language large language scratch unlikely lead good results substantial data computational resources learning changing parameters large_language large language presents promising alternative cost effective approach achieving competitive results procedure established zero- shot text_classification text classification structured prediction received limited attention conditional text generation present automatic prompt_optimization optimization approach emotion conditioned text generation instruction fine tuned method uses iterative optimization procedure changes adding removing replacing tokens objective function require text classifier measures realization conditional variable generated text evaluate method emotion conditioned text generation focus event reports compare manually designed act seed optimization procedure optimized achieve macro average f1 fulfill emotion condition contrast manually designed seed macro average f1"}
{"id": "f8a2dca1e8fe56e698984c077f7ff58d8ca867e9", "abstract": "Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.", "title": "large language models as optimizers", "url": "https://arxiv.org/pdf/2309.03409", "tokenized_text": "optimization ubiquitous derivative based algorithms powerful tools problems absence gradient challenges real world_applications world applications work propose optimization simple effective approach leverage large_language large language llms optimizers optimization task described natural_language natural language optimization step llm generates new solutions contains previously generated solutions values new solutions evaluated added optimization step showcase linear regression problems prompt_optimization optimization goal find instructions maximize task accuracy variety llms demonstrate best optimized outperform human designed gsm8 50 big bench hard tasks"}
{"id": "ff96527c03fbea7c3bb7d44d1d656d875ddba75e", "abstract": "Prompt-based pre-trained language models (PLMs) paradigm have succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization ($DP_2O$) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework based on policy gradients to match the prompts to inputs optimally. By training a policy network with only 0.67% of the PLM parameter size on the tasks in the few-shot setting, $DP_2O$ outperforms the state-of-the-art (SOTA) method by 1.52% in accuracy on average on four open-source datasets. Moreover, subsequent experiments also demonstrate that $DP_2O$ has good universality, robustness, and generalization ability.", "title": "dialogue for prompting a policygradientbased discrete prompt optimization for fewshot learning", "url": "https://arxiv.org/pdf/2308.07272", "tokenized_text": "based pre trained_language trained language plms paradigm succeeded substantially shot natural_language natural language processing nlp tasks prior discrete prompt_optimization optimization methods require expert knowledge design base set identify high quality costly inefficient subjective existing continuous prompt_optimization optimization methods improve performance learning ideal gradient information plms high computational cost low readability generalizability concerning address research gap propose dialogue comprised policy gradient based discrete prompt_optimization optimization method design multi round dialogue alignment strategy readability set generation based gpt-4 furthermore propose efficient screening metric identify high quality linear complexity finally construct reinforcement_learning reinforcement learning rl framework based policy gradients match inputs optimally training policy network 0.67 plm parameter size tasks shot_setting shot setting outperforms state art sota method accuracy average open source datasets subsequent experiments demonstrate good universality robustness generalization_ability generalization ability"}
{"id": "020e473d8c987dcfb03fcfffeb87b17812447031", "abstract": "Recent advances in large language models (LLMs) have shown impressive ability in biomedical question-answering, but have not been adequately investigated for more specific biomedical applications. This study investigates the performance of LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical tasks beyond question-answering. Because no patient data can be passed to the OpenAI API public interface, we evaluated model performance with over 10000 samples as proxies for two fundamental tasks in the clinical domain - classification and reasoning. The first task is classifying whether statements of clinical and policy recommendations in scientific literature constitute health advice. The second task is causal relation detection from the biomedical literature. We compared LLMs with simpler models, such as bag-of-words (BoW) with logistic regression, and fine-tuned BioBERT models. Despite the excitement around viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks remained the best strategy. The simple BoW model performed on par with the most complex LLM prompting. Prompt engineering required significant investment.", "title": "evaluation of chatgpt family of models for biomedical reasoning and classification", "url": "http://arxiv.org/pdf/2304.02496", "tokenized_text": "recent_advances recent advances large_language large language llms shown_impressive shown impressive ability biomedical question answering adequately investigated specific biomedical applications study investigates performance llms chatgpt family gpt-4 biomedical tasks question answering patient data passed openai api public interface evaluated performance samples proxies fundamental tasks clinical domain classification reasoning task classifying statements clinical policy recommendations scientific literature constitute health second task causal relation detection biomedical literature compared llms simpler bag words logistic regression fine tuned biobert despite viral chatgpt found fine tuning fundamental nlp_tasks nlp tasks remained best strategy simple performed par complex llm prompt_engineering engineering required significant investment"}
{"id": "0270ec4bc946b59c5cf6204be2553682dee0346c", "abstract": "Novel architectures have recently improved generative image synthesis leading to excellent visual quality in various tasks. Of particular note is the field of ``AI-Art'', which has seen unprecedented growth with the emergence of powerful multimodal models such as CLIP. By combining speech and image synthesis models, so-called ``prompt-engineering'' has become established, in which carefully selected and composed sentences are used to achieve a certain visual style in the synthesized image. In this note, we present an alternative approach based on retrieval-augmented diffusion models (RDMs). In RDMs, a set of nearest neighbors is retrieved from an external database during training for each training instance, and the diffusion model is conditioned on these informative samples. During inference (sampling), we replace the retrieval database with a more specialized database that contains, for example, only images of a particular visual style. This provides a novel way to prompt a general trained model after training and thereby specify a particular visual style. As shown by our experiments, this approach is superior to specifying the visual style within the text prompt. We open-source code and model weights at https://github.com/CompVis/latent-diffusion .", "title": "textguided synthesis of artistic images with retrievalaugmented diffusion models", "url": "http://arxiv.org/pdf/2207.13038", "tokenized_text": "novel architectures recently improved generative image synthesis leading excellent visual quality tasks particular note field ai art seen unprecedented growth emergence powerful multimodal clip combining speech image synthesis called engineering established carefully selected composed sentences achieve certain visual style synthesized image note present alternative approach based retrieval augmented diffusion set nearest neighbors retrieved external database training training instance diffusion conditioned informative samples inference sampling replace retrieval database specialized database contains example images particular visual style provides novel way general trained training specify particular visual style shown experiments approach superior specifying visual style text open source_code source code weights"}
{"id": "0392d58335ce674a70f5e58ac8c438de296a0e6a", "abstract": "State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases.", "title": "interactive and visual prompt engineering for adhoc task adaptation with large language models", "url": "https://arxiv.org/pdf/2208.07852", "tokenized_text": "state art neural language_models language solve ad hoc language tasks zero shot_prompting shot need supervised training approach gained popularity recent_years recent years researchers demonstrated achieve strong accuracy specific nlp_tasks nlp tasks finding new tasks requires experimentation different prompt_templates templates different wording choices lead significant accuracy differences allows users experiment variations performance iteratively optimize developed workflow allows users focus feedback small data moving large data regime allows empirical grounding promising quantitative measures task tool allows easy deployment newly created ad hoc demonstrate utility demo workflow real world use cases"}
{"id": "040ec58865ab50b5e6d91a355ffc146ec5034e9f", "abstract": "This study introduces and examines the potential of an AI system to generate health awareness messages. The topic of folic acid, a vitamin that is critical during pregnancy, served as a test case. Using prompt engineering, we generated messages that could be used to raise awareness and compared them to retweeted human-generated messages via computational and human evaluation methods. The system was easy to use and prolific, and computational analyses revealed that the AI-generated messages were on par with human-generated ones in terms of sentiment, reading ease", "title": "artificial intelligence for health message generation theory, method, and an empirical study using prompt engineering", "url": "http://arxiv.org/pdf/2212.07507", "tokenized_text": "study introduces examines potential ai system generate health awareness messages topic acid critical pregnancy served test case prompt_engineering engineering generated messages raise awareness compared human generated messages computational human evaluation methods system easy use prolific computational analyses revealed ai generated messages par human generated ones terms sentiment reading ease"}
{"id": "06ab0710c8a7315e70c15c0d7eb1aa50210d945c", "abstract": "Entity Resolution (ER) is the problem of semi-automatically determining when two entities refer to the same underlying entity, with applications ranging from healthcare to e-commerce. Traditional ER solutions required considerable manual expertise, including feature engineering, as well as identification and curation of training data. In many instances, such techniques are highly dependent on the domain. With recent advent in large language models (LLMs), there is an opportunity to make ER much more seamless and domain-independent. However, it is also well known that LLMs can pose risks, and that the quality of their outputs can depend on so-called prompt engineering. Unfortunately, a systematic experimental study on the effects of different prompting methods for addressing ER, using LLMs like ChatGPT, has been lacking thus far. This paper aims to address this gap by conducting such a study. Although preliminary in nature, our results show that prompting can significantly affect the quality of ER, although it affects some metrics more than others, and can also be dataset dependent.", "title": "how does prompt engineering affect chatgpt performance on unsupervised entity resolution", "url": "https://arxiv.org/pdf/2310.06174", "tokenized_text": "entity resolution problem semi automatically determining entities refer underlying entity applications ranging healthcare commerce traditional solutions required considerable manual expertise including feature engineering identification curation training_data training data instances techniques highly dependent domain recent advent large_language large language llms opportunity seamless domain independent known llms pose risks quality outputs depend called prompt_engineering engineering unfortunately systematic experimental study effects different methods addressing llms like_chatgpt like chatgpt lacking far paper aims address gap conducting study preliminary nature results significantly affect quality affects metrics dataset dependent"}
{"id": "06d8562831c32844285a691c5250d04726df3c61", "abstract": "\u2014Prompt engineering is a technique that involves augmenting a large pre-trained model with task-speci\ufb01c hints, known as prompts, to adapt the model to new tasks. Prompts can be created manually as natural language instructions or generated automatically as either natural language instructions or vector representations. Prompt engineering enables the ability to perform predictions based solely on prompts without updating model parameters, and the easier application of large pre-trained models in real-world tasks. In past years, Prompt engineering has been well-studied in natural language processing. Recently, it has also been intensively studied in vision-language modeling. However, there is currently a lack of a systematic overview of prompt engineering on pre-trained vision-language models. This paper aims to provide a comprehensive survey of cutting-edge research in prompt engineering on three types of vision-language models: multimodal-to-text generation models ( e.g., Flamingo), image-text matching models ( e.g., CLIP), and text-to-image generation models ( e.g., Stable Diffusion). For each type of model, a brief model summary, prompting methods, prompting-based applications, and the corresponding responsibility and integrity issues are summarized and discussed. Furthermore, the commonalities and differences between prompting on vision-language models, language models, and vision models are also discussed. The challenges, future directions, and research opportunities are summarized to foster future research on this topic.", "title": "a systematic survey of prompt engineering on visionlanguage foundation models", "url": "https://arxiv.org/pdf/2307.12980", "tokenized_text": "prompt_engineering engineering technique involves augmenting large pre trained task speci\ufb01c hints known adapt new tasks created manually natural_language natural language instructions generated automatically natural_language natural language instructions vector representations prompt_engineering engineering enables ability perform predictions based solely updating parameters easier application large pre trained real world tasks past years prompt_engineering engineering studied natural_language natural language processing recently studied vision language modeling currently lack systematic overview prompt_engineering engineering pre trained vision language_models language paper aims provide comprehensive survey cutting edge research prompt_engineering engineering types vision language_models language multimodal text generation e.g. flamingo image text matching e.g. clip text image_generation image generation e.g. stable_diffusion stable diffusion type brief summary methods based applications corresponding responsibility integrity issues summarized discussed furthermore commonalities differences vision language_models language language_models language vision discussed challenges future directions research opportunities summarized foster future_research future research topic"}
{"id": "073972fa0de48db1304509041e877e568c94e7de", "abstract": "Semantic and Cross-language code clone generation may be useful for code reuse, code comprehension, refactoring and benchmarking. OpenAI's GPT model has potential in such clone generation as GPT is used for text generation. When developers copy/paste codes from Stack Overflow (SO) or within a system, there might be inconsistent changes leading to unexpected behaviours. Similarly, if someone possesses a code snippet in a particular programming language but seeks equivalent functionality in a different language, a semantic cross-language code clone generation approach could provide valuable assistance.In this study, using SemanticCloneBench as a vehicle, we evaluated how well the GPT-3 model could help generate semantic and cross-language clone variants for a given fragment.We have comprised a diverse set of code fragments and assessed GPT-3s performance in generating code variants.Through extensive experimentation and analysis, where 9 judges spent 158 hours to validate, we investigate the model's ability to produce accurate and semantically correct variants. Our findings shed light on GPT-3's strengths in code generation, offering insights into the potential applications and challenges of using advanced language models in software development. Our quantitative analysis yields compelling results. In the realm of semantic clones, GPT-3 attains an impressive accuracy of 62.14% and 0.55 BLEU score, achieved through few-shot prompt engineering. Furthermore, the model shines in transcending linguistic confines, boasting an exceptional 91.25% accuracy in generating cross-language clones", "title": "unveiling the potential of large language models in generating semantic and crosslanguage clones", "url": "https://arxiv.org/pdf/2309.06424", "tokenized_text": "semantic cross language code clone generation useful code reuse code comprehension refactoring benchmarking openai gpt potential clone generation gpt text generation developers copy codes stack system inconsistent changes leading unexpected behaviours similarly code snippet particular programming language seeks equivalent functionality different language semantic cross language code clone generation approach provide valuable assistance study semanticclonebench vehicle evaluated gpt-3 help generate semantic cross language clone variants given comprised diverse set code fragments assessed performance generating code variants extensive experimentation analysis judges spent hours validate investigate ability produce accurate semantically correct variants findings shed light gpt-3 strengths code_generation code generation offering insights potential applications challenges advanced language_models language software development quantitative analysis yields compelling results realm semantic clones gpt-3 attains impressive accuracy bleu score achieved shot prompt_engineering engineering furthermore transcending linguistic exceptional accuracy generating cross language clones"}
{"id": "079be8c8a93fc80274ff22251a3dac9804bec66a", "abstract": "Inspired by the recent success of large language models (LLMs) like ChatGPT, researchers start to explore the adoption of LLMs for agile hardware design, such as generating design RTL based on natural-language instructions. However, in existing works, their target designs are all relatively simple and in a small scale, and proposed by the authors themselves, making a fair comparison among different LLM solutions challenging. In addition, many prior works only focus on the design correctness, without evaluating the design qualities of generated design RTL. In this work, we propose an open-source benchmark named RTLLM, for generating design RTL with natural language instructions. To systematically evaluate the auto-generated design RTL, we summarized three progressive goals, named syntax goal, functionality goal, and design quality goal. This benchmark can automatically provide a quantitative evaluation of any given LLM-based solution. Furthermore, we propose an easy-to-use yet surprisingly effective prompt engineering technique named self-planning, which proves to significantly boost the performance of GPT-3.5 in our proposed benchmark.", "title": "rtllm an opensource benchmark for design rtl generation with large language model", "url": "https://arxiv.org/pdf/2308.05345", "tokenized_text": "inspired recent success large_language large language llms like_chatgpt like chatgpt researchers start explore adoption llms hardware design generating design based natural language instructions existing works target designs relatively simple small scale proposed authors making fair comparison different llm solutions challenging addition prior works focus design correctness evaluating design qualities generated design work propose open source benchmark named generating design natural_language natural language instructions systematically evaluate auto generated design summarized progressive goals named syntax goal functionality goal design quality goal benchmark automatically provide quantitative evaluation given llm based solution furthermore propose easy use surprisingly effective prompt_engineering engineering technique named self planning proves significantly boost performance gpt-3.5 proposed benchmark"}
{"id": "0809c278fcdec2ce297da3a9d6e031fc192263f6", "abstract": "Recent text-driven image editing in diffusion models has shown remarkable success. However, the existing methods assume that the user's description sufficiently grounds the contexts in the source image, such as objects, background, style, and their relations. This assumption is unsuitable for real-world applications because users have to manually engineer text prompts to find optimal descriptions for different images. From the users' standpoint, prompt engineering is a labor-intensive process, and users prefer to provide a target word for editing instead of a full sentence. To address this problem, we first demonstrate the importance of a detailed text description of the source image, by dividing prompts into three categories based on the level of semantic details. Then, we propose simple yet effective methods by combining prompt generation frameworks, thereby making the prompt engineering process more user-friendly. Extensive qualitative and quantitative experiments demonstrate the importance of prompts in text-driven image editing and our method is comparable to ground-truth prompts.", "title": "userfriendly image editing with minimal text input leveraging captioning and injection techniques", "url": "http://arxiv.org/pdf/2306.02717", "tokenized_text": "recent text driven image editing diffusion shown remarkable success existing_methods existing methods assume user description sufficiently grounds contexts source image objects background style relations assumption unsuitable real world_applications world applications users manually engineer text find optimal descriptions different images users prompt_engineering engineering labor intensive process users prefer provide target word editing instead sentence address problem demonstrate importance detailed text description source image categories based level semantic details propose simple effective methods combining generation frameworks making prompt_engineering engineering process user friendly extensive qualitative quantitative experiments_demonstrate experiments demonstrate importance text driven image editing method comparable ground truth"}
{"id": "08b85bce712168998004ee80ce4e475390413c74", "abstract": "Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.", "title": "a prompt pattern catalog to enhance prompt engineering with chatgpt", "url": "http://arxiv.org/pdf/2302.11382", "tokenized_text": "prompt_engineering engineering increasingly important skill set needed converse effectively large_language large language llms chatgpt instructions given llm enforce rules automate processes ensure specific qualities quantities generated output form programming customize outputs interactions llm paper describes catalog prompt_engineering engineering techniques presented pattern form applied solve common problems conversing llms patterns knowledge transfer method analogous software patterns provide reusable solutions common problems faced particular context i.e. output generation interaction working llms paper provides following contributions research prompt_engineering engineering apply llms automate software development tasks provides framework documenting patterns structuring solve range problems adapted different domains second presents catalog patterns applied successfully improve outputs llm conversations explains built multiple patterns illustrates patterns benefit combination patterns"}
{"id": "0968f1592f9401d72bf0d97e740496818c1a3135", "abstract": "Text-to-image generative models are a new and powerful way to generate visual artwork. However, the open-ended nature of text as interaction is double-edged; while users can input anything and have access to an infinite range of generations, they also must engage in brute-force trial and error with the text prompt when the result quality is poor. We conduct a study exploring what prompt keywords and model hyperparameters can help produce coherent outputs. In particular, we study prompts structured to include subject and style keywords and investigate success and failure modes of these prompts. Our evaluation of 5493 generations over the course of five experiments spans 51 abstract and concrete subjects as well as 51 abstract and figurative styles. From this evaluation, we present design guidelines that can help people produce better outcomes from text-to-image generative models.", "title": "design guidelines for prompt engineering texttoimage generative models", "url": "https://arxiv.org/pdf/2109.06977", "tokenized_text": "text image generative new powerful way generate visual artwork open ended nature text interaction double users input access infinite range generations engage trial error text result quality poor conduct study exploring keywords hyperparameters help produce coherent outputs particular study structured include subject style keywords investigate success failure modes evaluation generations course experiments spans 51 abstract concrete subjects 51 abstract styles evaluation present design guidelines help people produce better outcomes text image generative"}
{"id": "0a0d6a98bd246a82aaaa9d33ec0eadf4ceae69dc", "abstract": "The Object Constraint Language (OCL) is a declarative language that adds constraints and object query expressions to Meta-Object Facility (MOF) models. OCL can provide precision and conciseness to UML models. Nevertheless, the unfamiliar syntax of OCL has hindered its adoption by software practitioners. LLMs, such as GPT-3, have made significant progress in many NLP tasks, such as text generation and semantic parsing. Similarly, researchers have improved on the downstream tasks by fine-tuning LLMs for the target task. Codex, a GPT-3 descendant by OpenAI, has been fine-tuned on publicly available code from GitHub and has proven the ability to generate code in many programming languages, powering the AI-pair programmer Copilot. One way to take advantage of Codex is to engineer prompts for the target downstream task. In this paper, we investigate the reliability of the OCL constraints generated by Codex from natural language specifications. To achieve this, we compiled a dataset of 15 UML models and 168 specifications from various educational resources. We manually crafted a prompt template with slots to populate with the UML information and the target task in the prefix format to complete the template with the generated OCL constraint. We used both zero- and few-shot learning methods in the experiments. The evaluation is reported by measuring the syntactic validity and the execution accuracy metrics of the generated OCL constraints. Moreover, to get insight into how close or natural the generated OCL constraints are compared to human-written ones, we measured the cosine similarity between the sentence embedding of the correctly generated and human-written OCL constraints. Our findings suggest that by enriching the prompts with the UML information of the models and enabling few-shot learning, the reliability of the generated OCL constraints increases. Furthermore, the results reveal a close similarity based on sentence embedding between the generated OCL constraints and the human-written ones in the ground truth, implying a level of clarity and understandability in the generated OCL constraints by Codex.", "title": "on codex prompt engineering for ocl generation an empirical study", "url": "https://arxiv.org/pdf/2303.16244", "tokenized_text": "object constraint language declarative language adds constraints object query expressions mof provide precision unfamiliar syntax hindered adoption software practitioners llms gpt-3 significant progress nlp_tasks nlp tasks text generation semantic_parsing semantic parsing similarly researchers improved downstream_tasks downstream tasks fine tuning llms target task codex gpt-3 openai fine tuned publicly_available publicly available code github proven ability generate code programming languages powering ai pair programmer copilot way advantage codex engineer target downstream task paper investigate reliability constraints generated codex natural_language natural language specifications achieve compiled dataset 15 specifications educational resources manually crafted prompt_template template slots information target task prefix format complete template generated constraint zero- shot_learning shot learning methods experiments evaluation reported measuring syntactic validity execution accuracy metrics generated constraints insight close natural generated constraints compared human written ones measured cosine similarity sentence embedding correctly generated human written constraints findings_suggest findings suggest enriching information enabling shot_learning shot learning reliability generated constraints increases furthermore results reveal close similarity based sentence embedding generated constraints human written ones ground_truth ground truth implying level clarity understandability generated constraints codex"}
{"id": "0a61802b71aa044cf1fe0e81befec148e0d5001b", "abstract": "Various stuff and things in visual data possess specific traits, which can be learned by deep neural networks and are implicitly represented as the visual prior, e.g., object location and shape, in the model. Such prior potentially impacts many vision tasks. For example, in conditional image synthesis, spatial conditions failing to adhere to the prior can result in visually inaccurate synthetic results. This work aims to explicitly learn the visual prior and enable the customization of sampling. Inspired by advances in language modeling, we propose to learn Visual prior via Generative Pre-Training, dubbed VisorGPT. By discretizing visual locations of objects, e.g., bounding boxes, human pose, and instance masks, into sequences, VisorGPT can model visual prior through likelihood maximization. Besides, prompt engineering is investigated to unify various visual locations and enable customized sampling of sequential outputs from the learned prior. Experimental results demonstrate that VisorGPT can effectively model the visual prior, which can be employed for many vision tasks, such as customizing accurate human pose for conditional image synthesis models like ControlNet. Code will be released at https://github.com/Sierkinhane/VisorGPT.", "title": "visorgpt learning visual prior via generative pretraining", "url": "http://arxiv.org/pdf/2305.13777", "tokenized_text": "things visual data possess specific traits learned deep neural_networks neural networks implicitly represented visual prior e.g. object location shape prior potentially impacts vision tasks example conditional image synthesis spatial conditions failing adhere prior result visually inaccurate synthetic results work aims explicitly learn visual prior enable customization sampling inspired advances language modeling propose learn visual prior generative_pre-training generative pre-training dubbed discretizing visual locations objects e.g. bounding boxes human pose instance masks sequences visual prior likelihood prompt_engineering engineering investigated unify visual locations enable customized sampling sequential outputs learned prior experimental_results experimental results demonstrate effectively visual prior employed vision tasks customizing accurate human pose conditional image synthesis like code released"}
{"id": "0ba581718f294db1d7b3dbc159cc3d3380f74606", "abstract": "This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing users to interact with it primarily via natural language instructions. In addition to these studies, we introduce an open-sourced research tool called PromptCraft, which contains a platform where researchers can collaboratively upload and vote on examples of good prompting schemes for robotics applications, as well as a sample robotics simulator with ChatGPT integration, making it easier for users to get started with using ChatGPT for robotics.", "title": "chatgpt for robotics design principles and model abilities", "url": "https://arxiv.org/pdf/2306.17582", "tokenized_text": "paper_presents paper presents experimental study use openai chatgpt robotics applications outline strategy combines design principles prompt_engineering engineering creation high level function library allows chatgpt adapt different robotics tasks simulators form factors focus evaluations effectiveness different prompt_engineering engineering techniques dialog strategies execution types robotics tasks explore chatgpt ability use free form dialog parse xml tags synthesize code addition use task specific functions closed loop reasoning dialogues study encompasses range tasks robotics domain basic logical mathematical reasoning way complex domains aerial navigation manipulation embodied agents chatgpt effective solving tasks allowing users interact primarily natural_language natural language instructions addition studies introduce open sourced research tool called contains platform researchers vote examples good schemes robotics applications sample robotics simulator chatgpt integration making easier users started chatgpt robotics"}
{"id": "0c8446eedfe083e0ee32f5c4f793e5435904014a", "abstract": "Text normalization - the conversion of text from written to spoken form - is traditionally assumed to be an ill-formed task for language models. In this work, we argue otherwise. We empirically show the capacity of Large-Language Models (LLM) for text normalization in few-shot scenarios. Combining self-consistency reasoning with linguistic-informed prompt engineering, we find LLM based text normalization to achieve error rates around 40\\% lower than top normalization systems. Further, upon error analysis, we note key limitations in the conventional design of text normalization tasks. We create a new taxonomy of text normalization errors and apply it to results from GPT-3.5-Turbo and GPT-4.0. Through this new framework, we can identify strengths and weaknesses of GPT-based TN, opening opportunities for future work.", "title": "a chat about boring problems studying gptbased text normalization", "url": "https://arxiv.org/pdf/2309.13426", "tokenized_text": "text normalization conversion text written spoken form traditionally assumed ill formed task language_models language work argue empirically capacity large language_models language llm text normalization shot scenarios combining self consistency reasoning linguistic informed prompt_engineering engineering find llm based text normalization achieve error rates lower normalization systems error analysis note key limitations conventional design text normalization tasks create new taxonomy text normalization errors apply results gpt-3.5 turbo gpt-4.0 new framework identify strengths weaknesses gpt based opening opportunities future work"}
{"id": "0e1ae0bdcc8469db99a4f8008288e20f285f1c6d", "abstract": "Controlled automated story generation seeks to generate natural language stories satisfying constraints from natural language critiques or preferences. Existing methods to control for story preference utilize prompt engineering which is labor intensive and often inconsistent. They may also use logit-manipulation methods which require annotated datasets to exist for the desired attributes. To address these issues, we first train a contrastive bi-encoder model to align stories with corresponding human critiques, named CARP, building a general purpose preference model. This is subsequently used as a reward function to fine-tune a generative language model via reinforcement learning. However, simply fine-tuning a generative language model with a contrastive reward model does not always reliably result in a story generation system capable of generating stories that meet user preferences. To increase story generation robustness we further fine-tune the contrastive reward model using a prompt-learning technique. A human participant study is then conducted comparing generations from our full system, ablations, and two baselines. We show that the full fine-tuning pipeline results in a story generator preferred over a LLM 20x as large as well as logit-based methods. This motivates the use of contrastive learning for general purpose human preference modeling.", "title": "robust preference learning for storytelling via contrastive reinforcement learning", "url": "http://arxiv.org/pdf/2210.07792", "tokenized_text": "controlled automated story generation seeks generate natural_language natural language stories satisfying constraints natural_language natural language preferences existing_methods existing methods control story preference utilize prompt_engineering engineering labor intensive inconsistent use logit manipulation methods require annotated datasets exist desired attributes address issues train contrastive bi encoder align stories corresponding human named carp building general_purpose general purpose preference subsequently reward function fine tune generative language_model language reinforcement_learning reinforcement learning simply fine tuning generative language_model language contrastive reward reliably result story generation system capable generating stories meet user preferences increase story generation robustness fine tune contrastive reward learning technique human participant study conducted comparing generations system ablations baselines fine tuning pipeline results story generator preferred llm 20x large logit based methods motivates use contrastive_learning contrastive learning general_purpose general purpose human preference modeling"}
{"id": "0e8e3d2a2f4413808c7aff7bee6e8e11ec2700d7", "abstract": "It has been shown that accurate representation in media improves the well-being of the people who consume it. By contrast, inaccurate representations can negatively affect viewers and lead to harmful perceptions of other cultures. To achieve inclusive representation in generated images, we propose a culturally-aware priming approach for text-to-image synthesis using a small but culturally curated dataset that we collected, known here as Cross-Cultural Understanding Benchmark (CCUB) Dataset, to fight the bias prevalent in giant datasets. Our proposed approach is comprised of two fine-tuning techniques: (1) Adding visual context via fine-tuning a pre-trained text-to-image synthesis model, Stable Diffusion, on the CCUB text-image pairs, and (2) Adding semantic context via automated prompt engineering using the fine-tuned large language model, GPT-3, trained on our CCUB culturally-aware text data. CCUB dataset is curated and our approach is evaluated by people who have a personal relationship with that particular culture. Our experiments indicate that priming using both text and image is effective in improving the cultural relevance and decreasing the offensiveness of generated images while maintaining quality.", "title": "towards equitable representation in texttoimage synthesis models with the crosscultural understanding benchmark (ccub) dataset", "url": "http://arxiv.org/pdf/2301.12073", "tokenized_text": "shown accurate representation media improves people consume contrast inaccurate representations negatively affect lead harmful perceptions cultures achieve inclusive representation generated images propose culturally aware priming approach text image synthesis small culturally curated dataset collected known understanding benchmark dataset fight bias prevalent giant datasets proposed approach comprised fine tuning techniques adding visual context fine tuning pre trained text image synthesis stable_diffusion stable diffusion text image pairs adding semantic context automated prompt_engineering engineering fine tuned large_language large language gpt-3 trained culturally aware text data dataset curated approach evaluated people personal relationship particular culture experiments indicate priming text image effective improving cultural relevance decreasing generated images maintaining quality"}
{"id": "0f6fe87afd1a3571f77c790893b03717e5d0422a", "abstract": "Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives -- Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than small factual mistakes. Further, we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection. Our evaluation code and LLM-generated knowledge with human annotations will be released to facilitate future research.", "title": "beyond factuality a comprehensive evaluation of large language models as knowledge generators", "url": "https://arxiv.org/pdf/2310.07289", "tokenized_text": "large_language large language llms outperform information retrieval techniques downstream knowledge intensive tasks prompted generate world knowledge community concerns factuality potential implications knowledge light introduce comprehensive knowledge evaluation framework designed systematically automatically evaluate generated knowledge important perspectives factuality relevance helpfulness validity conduct extensive empirical analysis generated knowledge different types llms widely studied knowledge intensive tasks i.e. open domain question_answering question answering knowledge grounded dialogue surprisingly study reveals factuality generated knowledge lower significantly hinder downstream_tasks downstream tasks instead relevance coherence outputs important small factual mistakes use improve knowledge intensive tasks designing strategies prompt_engineering engineering knowledge selection evaluation code llm generated knowledge human annotations released facilitate future_research future research"}
{"id": "0fb8f3f86476e9ab8fa4679620acb7d525b222a8", "abstract": "This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE Conference on Games. The objective of this competition is for participants to create effective prompts for ChatGPT--enabling it to generate Science Birds levels with high stability and character-like qualities--fully using their creativity as well as prompt engineering skills. ChatGPT is a conversational agent developed by OpenAI. Science Birds is selected as the competition platform because designing an Angry Birds-like level is not a trivial task due to the in-game gravity; the quality of the levels is determined by their stability. To lower the entry barrier to the competition, we limit the task to the generation of capitalized English alphabetical characters. We also allow only a single prompt to be used for generating all the characters. Here, the quality of the generated levels is determined by their stability and similarity to the given characters. A sample prompt is provided to participants for their reference. An experiment is conducted to determine the effectiveness of several modified versions of this sample prompt on level stability and similarity by testing them on several characters. To the best of our knowledge, we believe that ChatGPT4PCG is the first competition of its kind and hope to inspire enthusiasm for prompt engineering in procedural content generation.", "title": "chatgpt4pcg competition characterlike level generation for science birds", "url": "http://arxiv.org/pdf/2303.15662", "tokenized_text": "paper_presents paper presents competition 2023 games objective competition participants create effective chatgpt enabling generate science birds levels high stability character like qualities fully creativity prompt_engineering engineering skills chatgpt conversational agent developed openai science birds selected competition platform designing birds like level trivial task game quality levels determined stability lower entry competition limit task generation capitalized english characters allow single generating characters quality generated levels determined stability similarity given characters sample provided participants reference experiment conducted determine effectiveness modified versions sample level stability similarity testing characters best knowledge believe competition kind hope inspire prompt_engineering engineering procedural content generation"}
{"id": "1059b79598d6e08121503093f45d50fa963d2843", "abstract": "Prompt-based language models have produced encouraging results in numerous applications, including Named Entity Recognition (NER) tasks. NER aims to identify entities in a sentence and provide their types. However, the strong performance of most available NER approaches is heavily dependent on the design of discrete prompts and a verbalizer to map the model-predicted outputs to entity categories, which are complicated undertakings. To address these challenges, we present ContrastNER, a prompt-based NER framework that employs both discrete and continuous tokens in prompts and uses a contrastive learning approach to learn the continuous prompts and forecast entity types. The experimental results demonstrate that ContrastNER obtains competitive performance to the state-of-the-art NER methods in high-resource settings and outperforms the state-of-the-art models in low-resource circumstances without requiring extensive manual prompt engineering and verbalizer design.", "title": "contrastner contrastivebased prompt tuning for fewshot ner", "url": "https://arxiv.org/pdf/2305.17951", "tokenized_text": "based language_models language produced encouraging results numerous applications including named_entity named entity recognition ner tasks ner aims identify entities sentence provide types strong performance available ner approaches heavily dependent design discrete verbalizer map predicted outputs entity categories complicated address challenges present based ner framework employs discrete continuous tokens uses contrastive_learning contrastive learning approach learn continuous entity types experimental_results experimental results demonstrate obtains competitive_performance competitive performance state art ner methods high resource settings outperforms state art low resource circumstances requiring extensive manual prompt_engineering engineering verbalizer design"}
{"id": "10e8dc07ea256c6a88d7043cf135417402ed38f4", "abstract": "We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper", "title": "prompting the hidden talent of webscale speech models for zeroshot task generalization", "url": "https://arxiv.org/pdf/2305.11095", "tokenized_text": "investigate emergent abilities recently proposed web scale speech whisper adapting unseen tasks prompt_engineering engineering selected tasks audio visual speech recognition code switched speech recognition asr speech translation st unseen language pairs design task specific leveraging large scale simply manipulating special tokens default experiments compared default proposed improve performance 10 45 zero shot tasks outperform sota supervised datasets addition experiments reveal interesting properties whisper including robustness bias accents multilingual understanding latent space code_is_available code available"}
{"id": "13fafa40eb7b15813cdf6c2ead1e1032e7b085f0", "abstract": "Business optimisation is the process of finding and implementing efficient and cost-effective means of operation to bring a competitive advantage for businesses. Synthesizing problem formulations is an integral part of business optimisation which is centred around human expertise, thus with a high potential of becoming a bottleneck. With the recent advancements in Large Language Models (LLMs), human expertise needed in problem formulation can potentially be minimized using Artificial Intelligence (AI). However, developing a LLM for problem formulation is challenging, due to training data requirements, token limitations, and the lack of appropriate performance metrics in LLMs. To minimize the requirement of large training data, considerable attention has recently been directed towards fine-tuning pre-trained LLMs for downstream tasks, rather than training a LLM from scratch for a specific task. In this paper, we adopt this approach and propose an AI-Copilot for business optimisation by fine-tuning a pre-trained LLM for problem formulation. To address token limitations, we introduce modularization and prompt engineering techniques to synthesize complex problem formulations as modules that fit into the token limits of LLMs. In addition, we design performance evaluation metrics that are more suitable for assessing the accuracy and quality of problem formulations compared to existing evaluation metrics. Experiment results demonstrate that our AI-Copilot can synthesize complex and large problem formulations for a typical business optimisation problem in production scheduling.", "title": "aicopilot for business optimisation a framework and a case study in production scheduling", "url": "https://arxiv.org/pdf/2309.13218", "tokenized_text": "business optimisation process finding implementing efficient cost effective means operation bring competitive advantage synthesizing problem formulations integral business optimisation centred human expertise high potential bottleneck recent advancements large_language large language llms human expertise needed problem formulation potentially minimized artificial_intelligence artificial intelligence ai developing llm problem formulation challenging training_data training data requirements token limitations lack appropriate performance metrics llms minimize requirement large training_data training data considerable attention recently directed fine tuning pre trained llms downstream_tasks downstream tasks training llm scratch specific task paper adopt approach propose ai copilot business optimisation fine tuning pre trained llm problem formulation address token limitations introduce prompt_engineering engineering techniques synthesize complex problem formulations modules fit token limits llms addition design performance evaluation metrics suitable assessing accuracy quality problem formulations compared existing evaluation metrics experiment results_demonstrate results demonstrate ai copilot synthesize complex large problem formulations typical business optimisation problem production scheduling"}
{"id": "14dcafae548d578f6b8c683d0972531bc46423ca", "abstract": "Users are increasingly being warned to check AI-generated content for correctness. Still, as LLMs (and other generative models) generate more complex output, such as summaries, tables, or code, it becomes harder for the user to audit or evaluate the output for quality or correctness. Hence, we are seeing the emergence of tool-assisted experiences to help the user double-check a piece of AI-generated content. We refer to these as co-audit tools. Co-audit tools complement prompt engineering techniques: one helps the user construct the input prompt, while the other helps them check the output response. As a specific example, this paper describes recent research on co-audit tools for spreadsheet computations powered by generative models. We explain why co-audit experiences are essential for any application of generative AI where quality is important and errors are consequential (as is common in spreadsheet computations). We propose a preliminary list of principles for co-audit, and outline research challenges.", "title": "coaudit tools to help humans doublecheck aigenerated content", "url": "https://arxiv.org/pdf/2310.01297", "tokenized_text": "users increasingly check ai generated content correctness llms generative generate complex output summaries tables code harder user audit evaluate output quality correctness emergence tool assisted experiences help user double check piece ai generated content refer co audit tools co audit tools complement prompt_engineering engineering techniques helps user construct input helps check output response specific example paper describes recent research co audit tools computations powered generative explain co audit experiences essential application generative_ai generative ai quality important errors consequential common computations propose preliminary list principles co audit outline research challenges"}
{"id": "16877baf3874038233279e07e330f891455fd880", "abstract": "This paper explores the concept of leveraging generative AI as a mapping assistant for enhancing the efficiency of collaborative mapping. We present results of an experiment that combines multiple sources of volunteered geographic information (VGI) and large language models (LLMs). Three analysts described the content of crowdsourced Mapillary street-level photographs taken along roads in a small test area in Miami, Florida. GPT-3.5-turbo was instructed to suggest the most appropriate tagging for each road in OpenStreetMap (OSM). The study also explores the utilization of BLIP-2, a state-of-the-art multimodal pre-training method as an artificial analyst of street-level photographs in addition to human analysts. Results demonstrate two ways to effectively increase the accuracy of mapping suggestions without modifying the underlying AI models: by (1) providing a more detailed description of source photographs, and (2) combining prompt engineering with additional context (e.g. location and objects detected along a road). The first approach increases the suggestion accuracy by up to 29%, and the second one by up to 20%.", "title": "chatgpt as a mapping assistant a novel method to enrich maps with generative ai and content derived from streetlevel photographs", "url": "https://arxiv.org/pdf/2306.03204", "tokenized_text": "paper explores concept leveraging generative_ai generative ai mapping assistant enhancing efficiency collaborative mapping present results experiment combines multiple sources information large_language large language llms analysts described content crowdsourced street level photographs taken roads small test area gpt-3.5 turbo instructed suggest appropriate tagging road study explores utilization blip-2 state art multimodal pre training method artificial street level photographs addition human analysts results_demonstrate results demonstrate ways effectively increase accuracy mapping suggestions modifying underlying ai providing detailed description source photographs combining prompt_engineering engineering additional context e.g. location objects detected road approach increases suggestion accuracy 29 second 20"}
{"id": "1696e03a35f1bcc724ed9bfe69bb028b789415e8", "abstract": "Creating compelling captions for data visualizations has been a long- standing challenge. Visualization researchers are typically untrained in journalistic reporting and hence the captions that are placed be- low data visualizations tend to be not overly engaging and rather just stick to basic observations about the data. In this work we explore the opportunities offered by the newly emerging crop of large language models (LLM) which use sophisticated deep learning technology to produce human-like prose. We ask, can these power-ful software devices be purposed to produce engaging captions for generic data visualizations like a scatterplot. It turns out that the key challenge lies in designing the most effective prompt for the LLM, a task called prompt engineering . We report on first experiments using the popular LLM GPT-3 and deliver some promising results.", "title": "using large language models to generate engaging captions for data visualizations", "url": "http://arxiv.org/pdf/2212.14047", "tokenized_text": "creating compelling captions data visualizations standing challenge visualization researchers typically untrained journalistic reporting captions placed low data visualizations tend overly engaging basic observations data work explore opportunities offered newly emerging large_language large language llm use sophisticated deep learning technology produce human like ask power software devices purposed produce engaging captions generic data visualizations like turns key challenge lies designing effective llm task called prompt_engineering engineering report experiments popular llm gpt-3 deliver promising_results promising results"}
{"id": "16acd2d2faa236dfe5f6ab67a0b94a9ed1b1de57", "abstract": "Deep Reinforcement Learning (Deep RL) is increasingly used to cope with the open-world assumption in service-oriented systems. Deep RL was successfully applied to problems such as dynamic service composition, job scheduling, and offloading, as well as service adaptation. While Deep RL offers many benefits, understanding the decision-making of Deep RL is challenging because its learned decision-making policy essentially appears as a black box. Yet, understanding the decision-making of Deep RL is key to help service developers perform debugging, support service providers to comply with relevant legal frameworks, and facilitate service users to build trust. We introduce Chat4XAI to facilitate the understanding of the decision-making of Deep RL by providing natural-language explanations. Compared with visual explanations, the reported benefits of natural-language explanations include better understandability for non-technical users, increased user acceptance and trust, as well as more efficient explanations. Chat4XAI leverages modern AI chatbot technology and dedicated prompt engineering. Compared to earlier work on natural-language explanations using classical software-based dialogue systems, using an AI chatbot eliminates the need for eliciting and defining potential questions and answers up-front. We prototypically realize Chat4XAI using OpenAI's ChatGPT API and evaluate the fidelity and stability of its explanations using an adaptive service exemplar.", "title": "an ai chatbot for explaining deep reinforcement learning decisions of serviceoriented systems", "url": "https://arxiv.org/pdf/2309.14391", "tokenized_text": "deep reinforcement_learning reinforcement learning deep rl increasingly cope open world assumption service oriented systems deep rl successfully applied problems dynamic service composition job scheduling offloading service adaptation deep rl offers benefits understanding decision making deep rl challenging learned decision making policy essentially appears black_box black box understanding decision making deep rl key help service developers perform debugging support service providers comply relevant legal frameworks facilitate service users build trust introduce facilitate understanding decision making deep rl providing natural language explanations compared visual explanations reported benefits natural language explanations include better understandability non technical users increased user acceptance trust efficient explanations leverages modern ai chatbot technology dedicated prompt_engineering engineering compared earlier work natural language explanations classical software based dialogue systems ai chatbot eliminates need eliciting defining potential questions answers realize openai chatgpt api evaluate fidelity stability explanations adaptive service exemplar"}
{"id": "186e96fe036927182ec963b63f9dd7f8ff650158", "abstract": "This paper aims to quantitatively evaluate the performance of ChatGPT, an interactive large language model, on inter-sentential relations such as temporal relations, causal relations, and discourse relations. Given ChatGPT's promising performance across various tasks, we conduct extensive evaluations on the whole test sets of 13 datasets, including temporal and causal relations, PDTB2.0-based and dialogue-based discourse relations, and downstream applications on discourse understanding. To achieve reliable results, we adopt three tailored prompt templates for each task, including the zero-shot prompt template, zero-shot prompt engineering (PE) template, and in-context learning (ICL) prompt template, to establish the initial baseline scores for all popular sentence-pair relation classification tasks for the first time. We find that ChatGPT exhibits strong performance in detecting and reasoning about causal relations, while it may not be proficient in identifying the temporal order between two events. It can recognize most discourse relations with existing explicit discourse connectives, but the implicit discourse relation still remains a challenging task. Meanwhile, ChatGPT performs poorly in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation.", "title": "chatgpt evaluation on sentence level relations a focus on temporal, causal, and discourse relations", "url": "http://arxiv.org/pdf/2304.14827", "tokenized_text": "paper aims quantitatively evaluate performance chatgpt interactive large_language large language inter relations temporal relations causal relations discourse relations given chatgpt promising performance tasks conduct_extensive conduct extensive evaluations test sets 13 datasets including temporal causal relations based dialogue based discourse relations downstream applications discourse understanding achieve reliable results adopt tailored prompt_templates templates task including zero shot prompt_template template zero shot prompt_engineering engineering pe template context_learning context learning icl prompt_template template establish initial baseline scores popular sentence pair relation classification tasks time find chatgpt exhibits strong performance detecting reasoning causal relations proficient identifying temporal order events recognize discourse relations existing explicit discourse implicit discourse relation remains challenging task chatgpt performs poorly dialogue discourse parsing task requires structural understanding dialogue aware discourse relation"}
{"id": "1bc9974780230573bfe9f89789115cb4fbf8bfc6", "abstract": "Humankind is entering a novel era of creativity - an era in which anybody can synthesize digital content. The paradigm under which this revolution takes place is prompt-based learning (or in-context learning). This paradigm has found fruitful application in text-to-image generation where it is being used to synthesize digital images from zero-shot text prompts in natural language for the purpose of creating AI art. This activity is referred to as prompt engineering - the practice of iteratively crafting prompts to generate and improve images. In this paper, we investigate prompt engineering as a novel creative skill for creating prompt-based art. In three studies with participants recruited from a crowdsourcing platform, we explore whether untrained participants could 1) recognize the quality of prompts, 2) write prompts, and 3) improve their prompts. Our results indicate that participants could assess the quality of prompts and respective images. This ability increased with the participants' experience and interest in art. Participants further were able to write prompts in rich descriptive language. However, even though participants were specifically instructed to generate artworks, participants' prompts were missing the specific vocabulary needed to apply a certain style to the generated images. Our results suggest that prompt engineering is a learned skill that requires expertise and practice. Based on our findings and experience with running our studies with participants recruited from a crowdsourcing platform, we provide ten recommendations for conducting experimental research on text-to-image generation and prompt engineering with a paid crowd. Our studies offer a deeper understanding of prompt engineering thereby opening up avenues for research on the future of prompt engineering. We conclude by speculating on four possible futures of prompt engineering.", "title": "prompting ai art an investigation into the creative skill of prompt engineering", "url": "http://arxiv.org/pdf/2303.13534", "tokenized_text": "humankind entering novel era creativity era anybody synthesize digital content paradigm revolution takes place based learning context_learning context learning paradigm found application text image_generation image generation synthesize digital images zero shot text natural_language natural language purpose creating ai art activity referred prompt_engineering engineering practice iteratively crafting generate improve images paper investigate prompt_engineering engineering novel creative skill creating based art studies participants crowdsourcing platform explore untrained participants recognize quality write improve results_indicate results indicate participants assess quality respective images ability increased participants experience interest art participants able write rich descriptive language participants specifically instructed generate artworks participants missing specific vocabulary needed apply certain style generated images results suggest prompt_engineering engineering learned skill requires expertise practice based findings experience running studies participants crowdsourcing platform provide recommendations conducting experimental research text image_generation image generation prompt_engineering engineering paid crowd studies offer deeper understanding prompt_engineering engineering opening avenues research future prompt_engineering engineering conclude possible prompt_engineering engineering"}
{"id": "1e5743366625128e225879dbcfb568f6b8f1bcdc", "abstract": "We explore the ability of large language models to solve and generate puzzles from the NPR Sunday Puzzle game show using PUZZLEQA, a dataset comprising 15 years of on-air puzzles. We evaluate four large language models using PUZZLEQA, in both multiple choice and free response formats, and explore two prompt engineering techniques to improve free response performance: chain-of-thought reasoning and prompt summarization. We find that state-of-the-art large language models can solve many PUZZLEQA puzzles: the best model, GPT-3.5, achieves 50.2% loose accuracy. However, in our few-shot puzzle generation experiment, we find no evidence that models can generate puzzles: GPT-3.5 generates puzzles with answers that do not conform to the generated rules. Puzzle generation remains a challenging task for future work.", "title": "solving and generating npr sunday puzzles with large language models", "url": "http://arxiv.org/pdf/2306.12255", "tokenized_text": "explore ability large_language large language solve generate puzzles puzzle game dataset comprising 15 years puzzles evaluate large_language large language multiple_choice multiple choice free response formats explore prompt_engineering engineering techniques improve free response performance chain thought reasoning summarization find state art large_language large language solve puzzles best gpt-3.5 achieves accuracy shot puzzle generation experiment find evidence generate puzzles gpt-3.5 generates puzzles answers conform generated rules puzzle generation remains challenging task future work"}
{"id": "1fc89ce338b94f6a46e41b9a13aa99366a762eea", "abstract": "Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborate their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. To explore the potential of open-source LLM, we investigate them in various scenarios, and further enhance their performance with supervised fine-tuning. Our explorations highlight open-source LLMs' potential in Text-to-SQL, as well as the advantages and disadvantages of the supervised fine-tuning. Additionally, towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. We hope that our work provides a deeper understanding of Text-to-SQL with LLMs, and inspires further investigations and broad applications.", "title": "texttosql empowered by large language models a benchmark evaluation", "url": "https://arxiv.org/pdf/2308.15363", "tokenized_text": "large_language large language llms emerged new_paradigm new paradigm text sql task absence benchmark development designing effective efficient economic llm based text sql solutions address challenge paper conduct extensive comparison existing prompt_engineering engineering methods including question representation example selection example experimental_results experimental results elaborate pros cons based findings propose_a_new propose new integrated solution named dail sql spider leaderboard execution accuracy sets new bar explore potential open source llm investigate scenarios enhance performance supervised fine tuning explorations highlight open source llms potential text sql advantages disadvantages supervised fine tuning additionally efficient economic llm based text sql solution emphasize token efficiency prompt_engineering engineering compare prior studies metric hope work provides deeper understanding text sql llms investigations broad applications"}
{"id": "20d448a8712238ea34d9a18287e3bf05bc61dd2c", "abstract": "Large Language Models have many methods for solving the same problem. This introduces novel strengths (different methods may work well for different problems) and weaknesses (it may be difficult for users to know which method to use). In this paper, we introduce Multi-Method Self-Training (MMST), where one method is trained on the filtered outputs of another, allowing us to augment the strengths and ameliorate the weaknesses of each method. Using a 176B parameter model trained on both language and code, we show that MMST can 1) improve the less performant method (up to 30%) making the model easier to use, 2) improve the more performant method (up to 32.2%) making the model more performant, and 3) improve the performance of related but distinct tasks (up to 10.3%) by improving the ability of the model to generate rationales. We then conduct ablation analyses to explore why MMST works. We show that MMST generates more data than traditional self-training, but the improvement in performance is driven by the use of multiple methods. We also analyze prompt-engineering and anti-correlated performance between methods as means of making MMST more effective. We hope the evidence from our paper motivates machine learning researchers to explore ways in which advances in language models allow for new forms of training.", "title": "multimethod selftraining improving code generation with text, and vice versa", "url": "https://arxiv.org/pdf/2307.10633", "tokenized_text": "large_language large language methods solving problem introduces novel strengths different methods work different problems weaknesses difficult users know method use paper introduce self-training method trained filtered outputs allowing augment strengths ameliorate weaknesses method 176b parameter trained language code improve performant method 30 making easier use improve performant method making performant improve performance related distinct tasks improving ability generate rationales conduct ablation analyses explore works generates data traditional self training improvement performance driven use multiple methods analyze engineering anti correlated performance methods means making effective hope evidence paper motivates machine_learning machine learning researchers explore ways advances language_models language allow new forms training"}
{"id": "20db2ac68c0a0daa8417696cced923e518c07681", "abstract": "Wearable sensor devices, which offer the advantage of recording daily objects used by a person while performing an activity, enable the feasibility of unsupervised Human Activity Recognition (HAR). Unfortunately, previous unsupervised approaches using the usage sequence of objects usually require a proper description of activities manually prepared by humans. Instead, we leverage the knowledge embedded in a Large Language Model (LLM) of ChatGPT. Because the sequence of objects robustly characterizes the activity identity, it is possible that ChatGPT already learned the association between activities and objects from existing contexts. However, previous prompt engineering for ChatGPT exhibits limited generalization ability when dealing with a list of words (i.e., sequence of objects) due to the similar weighting assigned to each word in the list. In this study, we propose a two-stage prompt engineering, which first guides ChatGPT to generate activity descriptions associated with objects while emphasizing important objects for distinguishing similar activities; then outputs activity classes and explanations for enhancing the contexts that are helpful for HAR. To the best of our knowledge, this is the first study that utilizes ChatGPT to recognize activities using objects in an unsupervised manner. We conducted our approach on three datasets and demonstrated the state-of-the-art performance.", "title": "unsupervised human activity recognition through twostage prompting with chatgpt", "url": "http://arxiv.org/pdf/2306.02140", "tokenized_text": "wearable devices offer advantage recording daily objects person performing activity enable feasibility unsupervised human activity recognition har unfortunately previous unsupervised approaches usage sequence objects usually require proper description activities manually prepared humans instead leverage knowledge embedded large_language large language llm chatgpt sequence objects robustly characterizes activity identity possible chatgpt learned association activities objects existing contexts previous prompt_engineering engineering chatgpt exhibits limited generalization_ability generalization ability dealing list words i.e. sequence objects similar weighting assigned word list study propose stage prompt_engineering engineering guides chatgpt generate activity descriptions associated objects emphasizing important objects distinguishing similar activities outputs activity classes explanations enhancing contexts helpful har best knowledge study utilizes chatgpt recognize activities objects unsupervised manner conducted approach datasets demonstrated state art performance"}
{"id": "221a72a3631ebf8b555c27bc864338390611feb1", "abstract": "Social network simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S$^3$ system (short for $\\textbf{S}$ocial network $\\textbf{S}$imulation $\\textbf{S}$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science.", "title": "s3 socialnetwork simulation system with large language modelempowered agents", "url": "https://arxiv.org/pdf/2307.14984", "tokenized_text": "social network simulation plays crucial role addressing challenges social science offers extensive applications state prediction phenomena explanation policy making support work harness human like capabilities exhibited large_language large language llms sensing reasoning behaving utilize qualities construct system short network adhering widely employed agent based simulation paradigm employ prompt_engineering engineering tuning techniques ensure agent behavior closely emulates genuine human social network specifically simulate pivotal aspects emotion attitude interaction behaviors endowing agent system ability perceive environment emulate human actions observe emergence population level phenomena including propagation information attitudes emotions conduct evaluation encompassing levels simulation employing real world social network data encouragingly results_demonstrate results demonstrate promising accuracy work represents initial step realm social network simulation empowered llm based agents anticipate endeavors serve source inspiration development simulation systems limited social science"}
{"id": "25ec4e51e515548cb55e0270f449ac55f3b0840c", "abstract": "Unlike perfect information games, where all elements are known to every player, imperfect information games emulate the real-world complexities of decision-making under uncertain or incomplete information. GPT-4, the recent breakthrough in large language models (LLMs) trained on massive passive data, is notable for its knowledge retrieval and reasoning abilities. This paper delves into the applicability of GPT-4's learned knowledge for imperfect information games. To achieve this, we introduce \\textbf{Suspicion-Agent}, an innovative agent that leverages GPT-4's capabilities for performing in imperfect information games. With proper prompt engineering to achieve different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable adaptability across a range of imperfect information card games. Importantly, GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it can understand others and intentionally impact others' behavior. Leveraging this, we design a planning strategy that enables GPT-4 to competently play against different opponents, adapting its gameplay style as needed, while requiring only the game rules and descriptions of observations as input. In the experiments, we qualitatively showcase the capabilities of Suspicion-Agent across three different imperfect information games and then quantitatively evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can potentially outperform traditional algorithms designed for imperfect information games, without any specialized training or examples. In order to encourage and foster deeper insights within the community, we make our game-related data publicly available.", "title": "suspicionagent playing imperfect information games with theory of mind aware gpt4", "url": "https://arxiv.org/pdf/2309.17277", "tokenized_text": "unlike perfect information games elements known player imperfect information games emulate real world complexities decision making uncertain incomplete information gpt-4 recent breakthrough large_language large language llms trained massive passive data notable knowledge retrieval reasoning abilities paper delves applicability gpt-4 learned knowledge imperfect information games achieve introduce agent innovative agent leverages gpt-4 capabilities performing imperfect information games proper prompt_engineering engineering achieve different functions agent based gpt-4 demonstrates remarkable adaptability range imperfect information card games importantly gpt-4 displays strong high order theory mind tom capacity meaning understand intentionally impact behavior leveraging design planning strategy enables gpt-4 play different opponents adapting gameplay style needed requiring game rules descriptions observations input experiments qualitatively showcase capabilities agent different imperfect information games quantitatively evaluate results agent potentially outperform traditional algorithms designed imperfect information games specialized training examples order encourage foster deeper insights community game related data publicly_available publicly available"}
{"id": "26f560e592419891c9de1b25d0e4d4d16014d54e", "abstract": "The widespread adoption of large language models (LLMs), such as OpenAI's ChatGPT, could revolutionize various industries, including geotechnical engineering. However, GPT models can sometimes generate plausible-sounding but false outputs, leading to hallucinations. In this article, we discuss the importance of prompt engineering in mitigating these risks and harnessing the full potential of GPT for geotechnical applications. We explore the challenges and pitfalls associated with LLMs and highlight the role of context in ensuring accurate and valuable responses. Furthermore, we examine the development of context-specific search engines and the potential of LLMs to become a natural interface for complex tasks, such as data analysis and design. We also develop a unified interface using natural language to handle complex geotechnical engineering tasks and data analysis. By integrating GPT into geotechnical engineering workflows, professionals can streamline their work and develop sustainable and resilient infrastructure systems for the future.", "title": "geotechnical parrot tales (gpt) harnessing large language models in geotechnical engineering", "url": "http://arxiv.org/pdf/2304.02138", "tokenized_text": "widespread adoption large_language large language llms openai chatgpt revolutionize including engineering gpt generate plausible false outputs leading hallucinations article discuss importance prompt_engineering engineering mitigating risks harnessing potential gpt applications explore challenges pitfalls associated llms highlight role context ensuring accurate valuable responses furthermore examine development context specific search engines potential llms natural interface complex tasks data analysis design develop unified interface natural_language natural language handle complex engineering tasks data analysis integrating gpt engineering workflows professionals streamline work develop sustainable resilient infrastructure systems future"}
{"id": "279c798fd53c8dc84044273d08b6a060dbe9f702", "abstract": "Reproducing research results in the networking community is important for both academia and industry. The current best practice typically resorts to three approaches: (1) looking for publicly available prototypes; (2) contacting the authors to get a private prototype; and (3) manually implementing a prototype following the description of the publication. However, most published network research does not have public prototypes and private prototypes are hard to get. As such, most reproducing efforts are spent on manual implementation based on the publications, which is both time and labor consuming and error-prone. In this paper, we boldly propose reproducing network research results using the emerging large language models (LLMs). In particular, we first prove its feasibility with a small-scale experiment, in which four students with essential networking knowledge each reproduces a different networking system published in prominent conferences and journals by prompt engineering ChatGPT. We report the experiment's observations and lessons and discuss future open research questions of this proposal. This work raises no ethical issue.", "title": "toward reproducing network research results using large language models", "url": "https://arxiv.org/pdf/2309.04716", "tokenized_text": "reproducing research results community important academia industry current best practice typically resorts approaches looking publicly_available publicly available prototypes contacting authors private prototype manually implementing prototype following description publication published network research public prototypes private prototypes hard reproducing efforts spent manual implementation based publications time labor consuming error prone paper propose reproducing network research results emerging large_language large language llms particular prove feasibility small scale experiment students essential knowledge different system published prominent prompt_engineering engineering chatgpt report experiment observations lessons discuss future open research questions proposal work raises ethical issue"}
{"id": "27c16cca907aa43397cc226a182b73b396c5cf66", "abstract": "Large language models are transforming research on machine learning while galvanizing public debates. Understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance. We propose to turn the lens of computational psychiatry, a framework used to computationally describe and modify aberrant behavior, to the outputs produced by these models. We focus on the Generative Pre-Trained Transformer 3.5 and subject it to tasks commonly studied in psychiatry. Our results show that GPT-3.5 responds robustly to a common anxiety questionnaire, producing higher anxiety scores than human subjects. Moreover, GPT-3.5's responses can be predictably changed by using emotion-inducing prompts. Emotion-induction not only influences GPT-3.5's behavior in a cognitive task measuring exploratory decision-making but also influences its behavior in a previously-established task measuring biases such as racism and ableism. Crucially, GPT-3.5 shows a strong increase in biases when prompted with anxiety-inducing text. Thus, it is likely that how prompts are communicated to large language models has a strong influence on their behavior in applied settings. These results progress our understanding of prompt engineering and demonstrate the usefulness of methods taken from computational psychiatry for studying the capable algorithms to which we increasingly delegate authority and autonomy.", "title": "inducing anxiety in large language models increases exploration and bias", "url": "http://arxiv.org/pdf/2304.11111", "tokenized_text": "large_language large language transforming research machine_learning machine learning public understanding work succeed fail great societal relevance propose turn lens computational framework computationally describe modify behavior outputs produced focus generative pre trained transformer 3.5 subject tasks commonly studied results gpt-3.5 responds robustly common producing higher scores human subjects gpt-3.5 responses changed emotion inducing emotion induction influences gpt-3.5 behavior cognitive task measuring exploratory decision making influences behavior previously established task measuring biases racism crucially gpt-3.5 shows strong increase biases prompted inducing text likely large_language large language strong influence behavior applied settings results progress understanding prompt_engineering engineering demonstrate usefulness methods taken computational studying capable algorithms increasingly delegate authority"}
{"id": "29203f0b8b9be7fd70d99bf7390c6a78b68a9289", "abstract": "Concept generation is a creative step in the conceptual design phase, where designers often turn to brainstorming, mindmapping, or crowdsourcing design ideas to complement their own knowledge of the domain. Recent advances in natural language processing (NLP) and machine learning (ML) have led to the rise of Large Language Models (LLMs) capable of generating seemingly creative outputs from textual prompts. The success of these models has led to their integration and application across a variety of domains, including art, entertainment, and other creative work. In this paper, we leverage LLMs to generate solutions for a set of 12 design problems and compare them to a baseline of crowdsourced solutions. We evaluate the differences between generated and crowdsourced design solutions through multiple perspectives, including human expert evaluations and computational metrics. Expert evaluations indicate that the LLM-generated solutions have higher average feasibility and usefulness while the crowdsourced solutions have more novelty. We experiment with prompt engineering and find that leveraging few-shot learning can lead to the generation of solutions that are more similar to the crowdsourced solutions. These findings provide insight into the quality of design solutions generated with LLMs and begins to evaluate prompt engineering techniques that could be leveraged by practitioners to generate higher-quality design solutions synergistically with LLMs.", "title": "conceptual design generation using large language models", "url": "http://arxiv.org/pdf/2306.01779", "tokenized_text": "concept generation creative step conceptual design phase designers turn brainstorming crowdsourcing design ideas complement knowledge domain recent_advances recent advances natural_language natural language processing nlp machine_learning machine learning ml led rise large_language large language llms capable generating seemingly creative outputs textual success led integration application variety domains including art creative work paper leverage llms generate solutions set 12 design problems compare baseline crowdsourced solutions evaluate differences generated crowdsourced design solutions multiple perspectives including human expert evaluations computational metrics expert evaluations indicate llm generated solutions higher average feasibility usefulness crowdsourced solutions novelty experiment prompt_engineering engineering find leveraging shot_learning shot learning lead generation solutions similar crowdsourced solutions findings provide insight quality design solutions generated llms begins evaluate prompt_engineering engineering techniques leveraged practitioners generate higher quality design solutions synergistically llms"}
{"id": "2af6a21a1b682ceb585165359d3605e89f4cf6b0", "abstract": "Novel AI-based code-writing Large Language Models (LLMs) such as OpenAI's Codex have demonstrated capabilities in many coding-adjacent domains. In this work we consider how LLMs maybe leveraged to automatically repair security relevant bugs present in hardware designs. We focus on bug repair in code written in the Hardware Description Language Verilog. For this study we build a corpus of domain-representative hardware security bugs. We then design and implement a framework to quantitatively evaluate the performance of any LLM tasked with fixing the specified bugs. The framework supports design space exploration of prompts (i.e., prompt engineering) and identifying the best parameters for the LLM. We show that an ensemble of LLMs can repair all ten of our benchmarks. This ensemble outperforms the state-of-the-art Cirfix hardware bug repair tool on its own suite of bugs. These results show that LLMs can repair hardware security bugs and the framework is an important step towards the ultimate goal of an automated end-to-end bug repair framework.", "title": "fixing hardware security bugs with large language models", "url": "http://arxiv.org/pdf/2302.01215", "tokenized_text": "novel ai based code writing large_language large language llms openai codex demonstrated capabilities coding domains work consider llms leveraged automatically repair security relevant bugs present hardware designs focus bug repair code written hardware description language study build corpus domain representative hardware security bugs design implement framework quantitatively evaluate performance llm tasked fixing specified bugs framework supports design space exploration i.e. prompt_engineering engineering identifying best parameters llm ensemble llms repair benchmarks ensemble outperforms state art hardware bug repair tool suite bugs results llms repair hardware security bugs framework important step ultimate goal automated end end bug repair framework"}
{"id": "2afb07359e9c67499e1f373ac6f1520d3ea9c46a", "abstract": "Due to the subtleness, implicity, and different possible interpretations perceived by different people, detecting undesirable content from text is a nuanced difficulty. It is a long-known risk that language models (LMs), once trained on corpus containing undesirable content, have the power to manifest biases and toxicity. However, recent studies imply that, as a remedy, LMs are also capable of identifying toxic content without additional fine-tuning. Prompt-methods have been shown to effectively harvest this surprising self-diagnosing capability. However, existing prompt-based methods usually specify an instruction to a language model in a discriminative way. In this work, we explore the generative variant of zero-shot prompt-based toxicity detection with comprehensive trials on prompt engineering. We evaluate on three datasets with toxicity labels annotated on social media posts. Our analysis highlights the strengths of our generative classification approach both quantitatively and qualitatively. Interesting aspects of self-diagnosis and its ethical implications are discussed.", "title": "toxicity detection with generative promptbased inference", "url": "https://arxiv.org/pdf/2205.12390", "tokenized_text": "different possible interpretations perceived different people detecting undesirable content text nuanced difficulty long known risk language_models language lms trained corpus containing undesirable content power manifest biases toxicity recent studies imply remedy lms capable identifying toxic content additional fine tuning methods shown effectively surprising self diagnosing capability existing based methods usually specify instruction language_model language discriminative way work explore generative variant zero shot based toxicity detection comprehensive trials prompt_engineering engineering evaluate datasets toxicity labels annotated social_media social media posts analysis highlights strengths generative classification approach quantitatively qualitatively interesting aspects self diagnosis ethical implications discussed"}
{"id": "2bb34cfe22d0d46394dd91ba8934e525563e1274", "abstract": "This study applies Activity Theory to investigate how English as a foreign language (EFL) students prompt generative artificial intelligence (AI) tools during short story writing. Sixty-seven Hong Kong secondary school students created generative-AI tools using open-source language models and wrote short stories with them. The study collected and analyzed the students' generative-AI tools, short stories, and written reflections on their conditions or purposes for prompting. The research identified three main themes regarding the purposes for which students prompt generative-AI tools during short story writing: a lack of awareness of purposes, overcoming writer's block, and developing, expanding, and improving the story. The study also identified common characteristics of students' activity systems, including the sophistication of their generative-AI tools, the quality of their stories, and their school's overall academic achievement level, for their prompting of generative-AI tools for the three purposes during short story writing. The study's findings suggest that teachers should be aware of students' purposes for prompting generative-AI tools to provide tailored instructions and scaffolded guidance. The findings may also help designers provide differentiated instructions for users at various levels of story development when using a generative-AI tool.", "title": "exploring efl students' prompt engineering in humanai story writing an activity theory perspective", "url": "http://arxiv.org/pdf/2306.01798", "tokenized_text": "study applies activity theory investigate english foreign language efl students generative artificial_intelligence artificial intelligence ai tools short story writing seven secondary school students created generative ai tools open source language_models language wrote short stories study collected analyzed students generative ai tools short stories written reflections conditions purposes research identified main themes purposes students generative ai tools short story writing lack awareness purposes overcoming writer block developing expanding improving story study identified common characteristics students activity systems including sophistication generative ai tools quality stories school overall academic level generative ai tools purposes short story writing study findings_suggest findings suggest teachers aware students purposes generative ai tools provide tailored instructions guidance findings help designers provide differentiated instructions users levels story development generative ai tool"}
{"id": "2c66f49e328ca5815c13dda106abc2c326d4f28b", "abstract": "Large pre-trained vision-language models such as CLIP have demonstrated great potential in zero-shot transferability to downstream tasks. However, to attain optimal performance, the manual selection of prompts is necessary to improve alignment between the downstream image distribution and the textual class descriptions. This manual prompt engineering is the major challenge for deploying such models in practice since it requires domain expertise and is extremely time-consuming. To avoid non-trivial prompt engineering, recent work Context Optimization (CoOp) introduced the concept of prompt learning to the vision domain using learnable textual tokens. While CoOp can achieve substantial improvements over manual prompts, its learned context is worse generalizable to wider unseen classes within the same dataset. In this work, we present Prompt Learning with Reparameterization Encoder (PRE) - a simple and efficient method that enhances the generalization ability of the learnable prompt to unseen classes while maintaining the capacity to learn Base classes. Instead of directly optimizing the prompts, PRE employs a prompt encoder to reparameterize the input prompt embeddings, enhancing the exploration of task-specific knowledge from few-shot samples. Experiments and extensive ablation studies on 8 benchmarks demonstrate that our approach is an efficient method for prompt learning. Specifically, PRE achieves a notable enhancement of 5.60% in average accuracy on New classes and 3% in Harmonic mean compared to CoOp in the 16-shot setting, all achieved within a good training time.", "title": "pre visionlanguage prompt learning with reparameterization encoder", "url": "https://arxiv.org/pdf/2309.07760", "tokenized_text": "large pre trained vision language_models language clip demonstrated great_potential great potential zero shot transferability downstream_tasks downstream tasks attain optimal performance manual selection necessary improve alignment downstream image distribution textual class descriptions manual prompt_engineering engineering major challenge deploying practice requires domain expertise extremely time consuming avoid non trivial prompt_engineering engineering recent_work recent work context_optimization context optimization coop introduced concept learning vision domain learnable textual tokens coop achieve substantial improvements manual learned context worse generalizable wider unseen classes dataset work present learning encoder pre simple efficient method enhances generalization_ability generalization ability learnable unseen classes maintaining capacity learn base classes instead directly optimizing pre employs encoder input embeddings enhancing exploration task specific knowledge shot samples experiments extensive ablation studies benchmarks demonstrate approach efficient method learning specifically pre achieves notable enhancement average accuracy new classes harmonic mean compared coop 16 shot_setting shot setting achieved good training time"}
{"id": "2ed64d90670177bf58cdce6bda04a48a8731a18f", "abstract": "Evaluating outputs of large language models (LLMs) is challenging, requiring making -- and making sense of -- many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.", "title": "chainforge a visual toolkit for prompt engineering and llm hypothesis testing", "url": "https://arxiv.org/pdf/2309.09128", "tokenized_text": "evaluating outputs large_language large language llms challenging requiring making making sense responses tools basic tend require knowledge programming apis focus narrow domains closed source present open source visual toolkit prompt_engineering engineering demand hypothesis testing text generation llms provides graphical interface comparison responses variations system designed support tasks selection prompt_template template design hypothesis testing e.g. auditing released early development design online users lab studies find range people use investigate hypotheses matter including real world settings identify modes prompt_engineering engineering llm hypothesis testing opportunistic exploration limited evaluation iterative refinement"}
{"id": "3034d8571e16e25c6a839bf492f20daf855d04a0", "abstract": "Materials language processing (MLP) is one of the key facilitators of materials science research, as it enables the extraction of structured information from massive materials science literature. Prior works suggested high-performance MLP models for text classification, named entity recognition (NER), and extractive question answering (QA), which require complex model architecture, exhaustive fine-tuning and a large number of human-labelled datasets. In this study, we develop generative pretrained transformer (GPT)-enabled pipelines where the complex architectures of prior MLP models are replaced with strategic designs of prompt engineering. First, we develop a GPT-enabled document classification method for screening relevant documents, achieving comparable accuracy and reliability compared to prior models, with only small dataset. Secondly, for NER task, we design an entity-centric prompts, and learning few-shot of them improved the performance on most of entities in three open datasets. Finally, we develop an GPT-enabled extractive QA model, which provides improved performance and shows the possibility of automatically correcting annotations. While our findings confirm the potential of GPT-enabled MLP models as well as their value in terms of reliability and practicability, our scientific methods and systematic approach are applicable to any materials science domain to accelerate the information extraction of scientific literature.", "title": "accelerated materials language processing enabled by gpt", "url": "https://arxiv.org/pdf/2308.09354", "tokenized_text": "materials language_processing language processing mlp key materials science research enables extraction structured information massive materials science literature prior works suggested high performance mlp text_classification text classification named_entity named entity recognition ner extractive question_answering question answering qa require complex architecture exhaustive fine tuning large number human labelled datasets study develop generative pretrained transformer pipelines complex architectures prior mlp replaced strategic designs prompt_engineering engineering develop gpt enabled document classification method screening relevant documents achieving comparable accuracy reliability compared prior small dataset secondly ner task design entity centric learning shot improved performance entities open datasets finally develop gpt enabled extractive qa provides improved performance shows possibility automatically correcting annotations findings confirm potential gpt enabled mlp value terms reliability practicability scientific methods systematic approach applicable materials science domain accelerate information_extraction information extraction scientific literature"}
{"id": "31e04aec55f749dc560afe1d8673112f9b32f46b", "abstract": "This research explores using lightweight deep neural network architectures to enable the humanoid robot Pepper to understand American Sign Language (ASL) and facilitate non-verbal human-robot interaction. First, we introduce a lightweight and efficient model for ASL understanding optimized for embedded systems, ensuring rapid sign recognition while conserving computational resources. Building upon this, we employ large language models (LLMs) for intelligent robot interactions. Through intricate prompt engineering, we tailor interactions to allow the Pepper Robot to generate natural Co-Speech Gesture responses, laying the foundation for more organic and intuitive humanoid-robot dialogues. Finally, we present an integrated software pipeline, embodying advancements in a socially aware AI interaction model. Leveraging the Pepper Robot's capabilities, we demonstrate the practicality and effectiveness of our approach in real-world scenarios. The results highlight a profound potential for enhancing human-robot interaction through non-verbal interactions, bridging communication gaps, and making technology more accessible and understandable.", "title": "a sign language recognition system with pepper, lightweighttransformer, and llm", "url": "https://arxiv.org/pdf/2309.16898", "tokenized_text": "research explores lightweight deep neural network architectures enable robot understand american language facilitate non verbal human robot interaction introduce lightweight efficient understanding optimized embedded systems ensuring rapid recognition computational resources building employ large_language large language llms intelligent robot interactions intricate prompt_engineering engineering tailor interactions allow robot generate natural co speech responses foundation organic intuitive robot dialogues finally present integrated software pipeline advancements socially aware ai interaction leveraging capabilities demonstrate practicality effectiveness approach real world_scenarios world scenarios results highlight profound potential enhancing human robot interaction non verbal interactions bridging communication gaps making technology accessible understandable"}
{"id": "3352d4bb5756a8a6bfcc1cde169b6aa9fd94497d", "abstract": "One of the most common solutions adopted by software researchers to address code generation is by training Large Language Models (LLMs) on massive amounts of source code. Although a number of studies have shown that LLMs have been effectively evaluated on popular accuracy metrics (e.g., BLEU, CodeBleu), previous research has largely overlooked the role of Causal Inference as a fundamental component of the interpretability of LLMs' performance. Existing benchmarks and datasets are meant to highlight the difference between the expected and the generated outcome, but do not take into account confounding variables (e.g., lines of code, prompt size) that equally influence the accuracy metrics. The fact remains that, when dealing with generative software tasks by LLMs, no benchmark is available to tell researchers how to quantify neither the causal effect of SE-based treatments nor the correlation of confounders to the model's performance. In an effort to bring statistical rigor to the evaluation of LLMs, this paper introduces a benchmarking strategy named Galeras comprised of curated testbeds for three SE tasks (i.e., code completion, code summarization, and commit generation) to help aid the interpretation of LLMs' performance. We illustrate the insights of our benchmarking strategy by conducting a case study on the performance of ChatGPT under distinct prompt engineering methods. The results of the case study demonstrate the positive causal influence of prompt semantics on ChatGPT's generative performance by an average treatment effect of $\\approx 3\\%$. Moreover, it was found that confounders such as prompt size are highly correlated with accuracy metrics ($\\approx 0.412\\%$). The end result of our case study is to showcase causal inference evaluations, in practice, to reduce confounding bias. By reducing the bias, we offer an interpretable solution for the accuracy metric under analysis.", "title": "benchmarking causal study to interpret large language models for source code", "url": "https://arxiv.org/pdf/2308.12415", "tokenized_text": "common solutions adopted software researchers address code_generation code generation training large_language large language llms massive amounts source_code source code number studies shown llms effectively evaluated popular accuracy metrics e.g. bleu previous research largely overlooked role causal inference fundamental component interpretability llms performance existing benchmarks datasets meant highlight difference expected generated outcome account confounding variables e.g. lines code size equally influence accuracy metrics fact remains dealing generative software tasks llms benchmark available tell researchers quantify causal effect based treatments correlation confounders performance effort bring statistical evaluation llms paper introduces benchmarking strategy named comprised curated tasks i.e. code completion code summarization commit generation help aid interpretation llms performance illustrate insights benchmarking strategy conducting case study performance chatgpt distinct prompt_engineering engineering methods results case study demonstrate positive causal influence semantics chatgpt generative performance average treatment effect found confounders size highly correlated accuracy metrics end result case study showcase causal inference evaluations practice reduce confounding bias reducing bias offer interpretable solution accuracy metric analysis"}
{"id": "344f801663a76aa15e0dd13344261d8648c382a2", "abstract": "ChatGPT is a state-of-the-art (SOTA) chatbot. Although it has potential to support English as a foreign language (EFL) students' writing, to effectively collaborate with it, a student must learn to engineer prompts, that is, the skill of crafting appropriate instructions so that ChatGPT produces desired outputs. However, writing an appropriate prompt for ChatGPT is not straightforward for non-technical users who suffer a trial-and-error process. This paper examines the content of EFL students' ChatGPT prompts when completing a writing task and explores patterns in the quality and quantity of the prompts. The data come from iPad screen recordings of secondary school EFL students who used ChatGPT and other SOTA chatbots for the first time to complete the same writing task. The paper presents a case study of four distinct pathways that illustrate the trial-and-error process and show different combinations of prompt content and quantity. The cases contribute evidence for the need to provide prompt engineering education in the context of the EFL writing classroom, if students are to move beyond an individual trial-and-error process, learning a greater variety of prompt content and more sophisticated prompts to support their writing.", "title": "cases of efl secondary students' prompt engineering pathways to complete a writing task with chatgpt", "url": "https://arxiv.org/pdf/2307.05493", "tokenized_text": "chatgpt state art sota chatbot potential support english foreign language efl students writing effectively collaborate student learn engineer skill crafting appropriate instructions chatgpt produces desired outputs writing appropriate chatgpt straightforward non technical users suffer trial error process paper examines content efl students chatgpt completing writing task explores patterns quality quantity data come screen secondary school efl students chatgpt sota chatbots time complete writing task paper_presents paper presents case study distinct pathways illustrate trial error process different combinations content quantity cases contribute evidence need provide prompt_engineering engineering education context efl writing classroom students individual trial error process learning greater variety content sophisticated support writing"}
{"id": "34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61", "abstract": "Large language models (LLMs) are popular for high-quality text generation but can produce harmful content, even when aligned with human values through reinforcement learning. Adversarial prompts can bypass their safety measures. We propose LLM Self Defense, a simple approach to defend against these attacks by having an LLM screen the induced responses. Our method does not require any fine-tuning, input preprocessing, or iterative output generation. Instead, we incorporate the generated content into a pre-defined prompt and employ another instance of an LLM to analyze the text and predict whether it is harmful. We test LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent LLMs against various types of attacks, such as forcefully inducing affirmative responses to prompts and prompt engineering attacks. Notably, LLM Self Defense succeeds in reducing the attack success rate to virtually 0 using both GPT 3.5 and Llama 2.", "title": "llm self defense by self examination, llms know they are being tricked", "url": "https://arxiv.org/pdf/2308.07308", "tokenized_text": "large_language large language llms popular high quality text generation produce harmful content aligned human values reinforcement_learning reinforcement learning adversarial bypass safety measures propose llm self defense simple approach defend attacks having llm screen induced responses method require fine tuning input preprocessing iterative output generation instead incorporate generated content pre defined employ instance llm analyze text predict harmful test llm self defense gpt 3.5 llama current prominent llms types attacks inducing affirmative responses prompt_engineering engineering attacks notably llm self defense succeeds reducing attack success_rate success rate virtually gpt 3.5 llama"}
{"id": "34fd95dd4dd32e704d4284fc31165e85b303bb1e", "abstract": "Open vocabulary models (e.g. CLIP) have shown strong performance on zero-shot classification through their ability generate embeddings for each class based on their (natural language) names. Prior work has focused on improving the accuracy of these models through prompt engineering or by incorporating a small amount of labeled downstream data (via finetuning). However, there has been little focus on improving the richness of the class names themselves, which can pose issues when class labels are coarsely-defined and are uninformative. We propose Classification with Hierarchical Label Sets (or CHiLS), an alternative strategy for zero-shot classification specifically designed for datasets with implicit semantic hierarchies. CHiLS proceeds in three steps: (i) for each class, produce a set of subclasses, using either existing label hierarchies or by querying GPT-3; (ii) perform the standard zero-shot CLIP procedure as though these subclasses were the labels of interest; (iii) map the predicted subclass back to its parent to produce the final prediction. Across numerous datasets with underlying hierarchical structure, CHiLS leads to improved accuracy in situations both with and without ground-truth hierarchical information. CHiLS is simple to implement within existing zero-shot pipelines and requires no additional training cost. Code is available at: https://github.com/acmi-lab/CHILS.", "title": "chils zeroshot image classification with hierarchical label sets", "url": "http://arxiv.org/pdf/2302.02551", "tokenized_text": "open vocabulary e.g. clip shown strong performance zero shot classification ability generate embeddings class based natural_language natural language names prior_work prior work focused improving accuracy prompt_engineering engineering incorporating small labeled downstream data finetuning little focus improving richness class names pose issues class labels defined uninformative propose classification hierarchical label sets alternative strategy zero shot classification specifically designed datasets implicit semantic hierarchies steps class produce set existing label hierarchies querying gpt-3 ii perform standard zero shot clip procedure labels interest iii map predicted parent produce final prediction numerous datasets underlying hierarchical structure leads improved accuracy situations ground truth hierarchical information simple implement existing zero shot pipelines requires additional training cost code_is_available code available"}
{"id": "377d4d6c1be01b9df32edfd94b2c5946971b0108", "abstract": "Recent advances in artificial intelligence (AI) have produced highly capable and controllable systems. This creates unprecedented opportunities for structured reasoning as well as collaboration among multiple AI systems and humans. To fully realize this potential, it is essential to develop a principled way of designing and studying such structured interactions. For this purpose, we introduce the conceptual framework of Flows: a systematic approach to modeling complex interactions. Flows are self-contained building blocks of computation, with an isolated state, communicating through a standardized message-based interface. This modular design allows Flows to be recursively composed into arbitrarily nested interactions, with a substantial reduction of complexity. Crucially, any interaction can be implemented using this framework, including prior work on AI--AI and human--AI interactions, prompt engineering schemes, and tool augmentation. We demonstrate the potential of Flows on the task of competitive coding, a challenging task on which even GPT-4 struggles. Our results suggest that structured reasoning and collaboration substantially improve generalization, with AI-only Flows adding +$21$ and human--AI Flows adding +$54$ absolute points in terms of solve rate. To support rapid and rigorous research, we introduce the aiFlows library. The library comes with a repository of Flows that can be easily used, extended, and composed into novel, more complex Flows. The aiFlows library is available at https://github.com/epfl-dlab/aiflows. Data and Flows for reproducing our experiments are available at https://github.com/epfl-dlab/cc_flows.", "title": "flows building blocks of reasoning and collaborating ai", "url": "https://arxiv.org/pdf/2308.01285", "tokenized_text": "recent_advances recent advances artificial_intelligence artificial intelligence ai produced highly capable controllable systems creates unprecedented opportunities structured reasoning collaboration multiple ai systems humans fully realize potential essential develop principled way designing studying structured interactions purpose introduce conceptual framework flows systematic approach modeling complex interactions flows self contained building blocks computation isolated state communicating standardized message based interface modular design allows flows recursively composed arbitrarily nested interactions substantial reduction complexity crucially interaction implemented framework including prior_work prior work ai ai human ai interactions prompt_engineering engineering schemes tool augmentation demonstrate potential flows task competitive coding challenging task gpt-4 struggles results suggest structured reasoning collaboration substantially improve generalization ai flows adding 21 human ai flows adding absolute points terms solve rate support rapid rigorous research introduce library library comes repository flows easily extended composed novel complex flows library available data flows reproducing experiments available"}
{"id": "3784fd84b61d482b52f7ef72aac66bcb886b892b", "abstract": "Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \\textit{from scratch}. To address these issues, we propose \\textbf{\\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs. These analogous problems are related to the input one, with reusable solutions and problem-solving strategies. Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. Experiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\\% improvement of human preference in Creative Writing, and 15\\% enhancement in the task completion rate of LLM-Agent Planning.", "title": "thought propagation an analogical approach to complex reasoning with large language models", "url": "https://arxiv.org/pdf/2310.03965", "tokenized_text": "large_language large language llms achieved remarkable success reasoning tasks development methods existing approaches reuse insights solving similar problems suffer errors multi step reasoning llms reason scratch address issues propose propagation explores analogous problems leverages solutions enhance complex_reasoning complex reasoning ability llms analogous problems related input reusable solutions problem solving strategies promising propagate insights solving previous analogous problems inspire new problem solving achieve llms propose solve set analogous problems related input reuses results analogous problems directly yield new solution derive knowledge intensive plan execution amend initial solution obtained scratch compatible existing approaches allowing plug play generalization enhancement wide_range wide range tasks labor task specific prompt_engineering engineering experiments challenging tasks demonstrate enjoys substantial improvement baselines average 12\\% absolute increase finding optimal solutions shortest path reasoning improvement human preference creative writing enhancement task completion rate planning"}
{"id": "385376b8aa48c25403f17d6206db7c09b67e1314", "abstract": "This review will introduce the latest advances in prompt engineering in the field of natural language processing (NLP) for the medical domain. First, we will provide a brief overview of the development of prompt engineering and emphasize its significant contributions to healthcare NLP applications such as question-answering systems, text summarization, and machine translation. With the continuous improvement of general large language models, the importance of prompt engineering in the healthcare domain is becoming increasingly prominent. The aim of this article is to provide useful resources and bridges for healthcare NLP researchers to better explore the application of prompt engineering in this field. We hope that this review can provide new ideas and inspire ample possibilities for research and application in medical NLP.", "title": "prompt engineering for healthcare methodologies and applications", "url": "http://arxiv.org/pdf/2304.14670", "tokenized_text": "review introduce latest advances prompt_engineering engineering field natural_language natural language processing nlp medical domain provide brief overview development prompt_engineering engineering emphasize significant contributions healthcare nlp applications question answering systems text summarization machine_translation machine translation continuous improvement general large_language large language importance prompt_engineering engineering healthcare domain increasingly prominent aim article provide useful resources bridges healthcare nlp researchers better explore application prompt_engineering engineering field hope review provide new ideas inspire possibilities research application medical nlp"}
{"id": "3a733c27bff68259b17dc4f835b0d192ac8fab70", "abstract": "Backdoor attacks have emerged as a prominent threat to natural language processing (NLP) models, where the presence of specific triggers in the input can lead poisoned models to misclassify these inputs to predetermined target classes. Current detection mechanisms are limited by their inability to address more covert backdoor strategies, such as style-based attacks. In this work, we propose an innovative test-time poisoned sample detection framework that hinges on the interpretability of model predictions, grounded in the semantic meaning of inputs. We contend that triggers (e.g., infrequent words) are not supposed to fundamentally alter the underlying semantic meanings of poisoned samples as they want to stay stealthy. Based on this observation, we hypothesize that while the model's predictions for paraphrased clean samples should remain stable, predictions for poisoned samples should revert to their true labels upon the mutations applied to triggers during the paraphrasing process. We employ ChatGPT, a state-of-the-art large language model, as our paraphraser and formulate the trigger-removal task as a prompt engineering problem. We adopt fuzzing, a technique commonly used for unearthing software vulnerabilities, to discover optimal paraphrase prompts that can effectively eliminate triggers while concurrently maintaining input semantics. Experiments on 4 types of backdoor attacks, including the subtle style backdoors, and 4 distinct datasets demonstrate that our approach surpasses baseline methods, including STRIP, RAP, and ONION, in precision and recall.", "title": "parafuzz an interpretabilitydriven technique for detecting poisoned samples in nlp", "url": "https://arxiv.org/pdf/2308.02122", "tokenized_text": "backdoor attacks emerged prominent threat natural_language natural language processing nlp presence specific triggers input lead poisoned inputs target classes current detection mechanisms limited inability address backdoor strategies style based attacks work propose innovative test time poisoned sample detection framework hinges interpretability predictions grounded semantic meaning inputs contend triggers e.g. infrequent words supposed fundamentally alter underlying semantic meanings poisoned samples want stealthy based observation hypothesize predictions clean samples remain stable predictions poisoned samples true labels mutations applied triggers paraphrasing process employ chatgpt state art large_language large language formulate trigger removal task prompt_engineering engineering problem adopt fuzzing technique commonly software vulnerabilities discover optimal paraphrase effectively eliminate triggers concurrently maintaining input semantics experiments types backdoor attacks including subtle style backdoors distinct datasets demonstrate approach surpasses baseline methods including precision recall"}
{"id": "3c4f1244301577cffff9affc73690669725e7e08", "abstract": "Financial sentiment analysis plays a crucial role in decoding market trends and guiding strategic trading decisions. Despite the deployment of advanced deep learning techniques and language models to refine sentiment analysis in finance, this study breaks new ground by investigating the potential of large language models, particularly ChatGPT 3.5, in financial sentiment analysis, with a strong emphasis on the foreign exchange market (forex). Employing a zero-shot prompting approach, we examine multiple ChatGPT prompts on a meticulously curated dataset of forex-related news headlines, measuring performance using metrics such as precision, recall, f1-score, and Mean Absolute Error (MAE) of the sentiment class. Additionally, we probe the correlation between predicted sentiment and market returns as an additional evaluation approach. ChatGPT, compared to FinBERT, a well-established sentiment analysis model for financial texts, exhibited approximately 35\\% enhanced performance in sentiment classification and a 36\\% higher correlation with market returns. By underlining the significance of prompt engineering, particularly in zero-shot contexts, this study spotlights ChatGPT's potential to substantially boost sentiment analysis in financial applications. By sharing the utilized dataset, our intention is to stimulate further research and advancements in the field of financial services.", "title": "transforming sentiment analysis in the financial domain with chatgpt", "url": "https://arxiv.org/pdf/2308.07935", "tokenized_text": "financial sentiment_analysis sentiment analysis plays crucial role decoding market trends guiding strategic trading decisions despite deployment advanced deep learning techniques language_models language refine sentiment_analysis sentiment analysis finance study breaks new ground investigating potential large_language large language particularly chatgpt 3.5 financial sentiment_analysis sentiment analysis strong emphasis foreign exchange market employing zero shot_prompting shot approach examine multiple chatgpt meticulously curated dataset related news headlines measuring performance metrics precision recall f1 score mean absolute error sentiment class additionally probe correlation predicted sentiment market returns additional evaluation approach chatgpt compared established sentiment_analysis sentiment analysis financial texts exhibited approximately enhanced performance sentiment classification higher correlation market returns underlining significance prompt_engineering engineering particularly zero shot contexts study chatgpt potential substantially boost sentiment_analysis sentiment analysis financial applications sharing utilized dataset intention stimulate research advancements field financial services"}
{"id": "3e0a691277183a6704310af3e4e9e271400612bc", "abstract": "Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible sentences to construct generalizable classifiers that outperform the recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized fine-grained datasets. The code, prompts, and auxiliary text dataset is available at https://github.com/mayug/VDT-Adapter.", "title": "enhancing clip with gpt4 harnessing visual descriptions as prompts", "url": "https://arxiv.org/pdf/2307.11661", "tokenized_text": "contrastive pretrained large vision language_models language vlms like clip revolutionized visual representation learning providing good performance downstream datasets vlms shot adapted downstream dataset designing relevant dataset prompt_engineering engineering makes use domain expertise validation dataset recent developments generative pretrained like gpt-4 mean advanced internet search tools manipulated provide visual information structure work gpt-4 generate text visually descriptive adapt clip downstream_tasks downstream tasks considerable improvements shot transfer accuracy specialized fine grained datasets like dtd compared clip default design simple shot adapter learns choose best possible sentences construct generalizable classifiers outperform recently proposed cocoop average specialized fine grained datasets code auxiliary text dataset available"}
{"id": "3e1ca026052d30e3b9677e363616fae23f6616df", "abstract": "Large Language Models (LLMs), typified by OpenAI's GPT series and Meta's LLaMA variants, have marked a significant advancement in artificial intelligence. Trained on vast amounts of text data, LLMs are capable of understanding and generating human-like text across a diverse range of topics. This study expands on the applications of LLMs, exploring their potential in data preprocessing, a critical stage in data mining and analytics applications. We delve into the applicability of state-of-the-art LLMs such as GPT-3.5, GPT-4, and Vicuna-13B for error detection, data imputation, schema matching, and entity matching tasks. Alongside showcasing the inherent capabilities of LLMs, we highlight their limitations, particularly in terms of computational expense and inefficiency. We propose an LLM-based framework for data preprocessing, which integrates cutting-edge prompt engineering techniques, coupled with traditional methods like contextualization and feature selection, to improve the performance and efficiency of these models. The effectiveness of LLMs in data preprocessing is evaluated through an experimental study spanning 12 datasets. GPT-4 emerged as a standout, achieving 100\\% accuracy or F1 score on 4 datasets, suggesting LLMs' immense potential in these tasks. Despite certain limitations, our study underscores the promise of LLMs in this domain and anticipates future developments to overcome current hurdles.", "title": "large language models as data preprocessors", "url": "https://arxiv.org/pdf/2308.16361", "tokenized_text": "large_language large language llms openai gpt series meta llama variants marked significant advancement artificial_intelligence artificial intelligence trained vast amounts text data llms capable understanding generating human like text diverse range topics study expands applications llms exploring potential data preprocessing critical stage data mining analytics applications delve applicability state art llms gpt-3.5 gpt-4 error detection data imputation schema matching entity matching tasks alongside showcasing inherent capabilities llms highlight limitations particularly terms computational expense inefficiency propose llm based framework data preprocessing integrates cutting edge prompt_engineering engineering techniques coupled traditional methods like contextualization feature selection improve performance efficiency effectiveness llms data preprocessing evaluated experimental study spanning 12 datasets gpt-4 emerged achieving 100\\% accuracy f1_score f1 score datasets suggesting llms immense potential tasks despite certain limitations study underscores promise llms domain future developments overcome current"}
{"id": "3e4991bd206214f596a10e9932cd441fe5bd1f8c", "abstract": "Large language models (LLMs) are incredibly powerful at comprehending and generating data in the form of text, but are brittle and error-prone. There has been an advent of toolkits and recipes centered around so-called prompt engineering-the process of asking an LLM to do something via a series of prompts. However, for LLM-powered data processing workflows, in particular, optimizing for quality, while keeping cost bounded, is a tedious, manual process. We put forth a vision for declarative prompt engineering. We view LLMs like crowd workers and leverage ideas from the declarative crowdsourcing literature-including leveraging multiple prompting strategies, ensuring internal consistency, and exploring hybrid-LLM-non-LLM approaches-to make prompt engineering a more principled process. Preliminary case studies on sorting, entity resolution, and imputation demonstrate the promise of our approach", "title": "revisiting prompt engineering via declarative crowdsourcing", "url": "https://arxiv.org/pdf/2308.03854", "tokenized_text": "large_language large language llms incredibly powerful comprehending generating data form text brittle error prone advent recipes centered called prompt_engineering engineering process asking llm series llm powered data processing workflows particular optimizing quality keeping cost bounded tedious manual process forth vision declarative prompt_engineering engineering view llms like crowd workers leverage ideas declarative crowdsourcing literature including leveraging multiple strategies ensuring internal consistency exploring hybrid llm non llm approaches prompt_engineering engineering principled process preliminary case studies sorting entity resolution imputation demonstrate promise approach"}
{"id": "407a8d6227ece351d9870f96576d4c287a746166", "abstract": "Political polling is a multi-billion dollar industry with outsized influence on the societal trajectory of the United States and nations around the world. However, it has been challenged by factors that stress its cost, availability, and accuracy. At the same time, artificial intelligence (AI) chatbots have become compelling stand-ins for human behavior, powered by increasingly sophisticated large language models (LLMs). Could AI chatbots be an effective tool for anticipating public opinion on controversial issues to the extent that they could be used by campaigns, interest groups, and polling firms? We have developed a prompt engineering methodology for eliciting human-like survey responses from ChatGPT, which simulate the response to a policy question of a person described by a set of demographic factors, and produce both an ordinal numeric response score and a textual justification. We execute large scale experiments, querying for thousands of simulated responses at a cost far lower than human surveys. We compare simulated data to human issue polling data from the Cooperative Election Study (CES). We find that ChatGPT is effective at anticipating both the mean level and distribution of public opinion on a variety of policy issues such as abortion bans and approval of the US Supreme Court, particularly in their ideological breakdown (correlation typically>85%). However, it is less successful at anticipating demographic-level differences. Moreover, ChatGPT tends to overgeneralize to new policy issues that arose after its training data was collected, such as US support for involvement in the war in Ukraine. Our work has implications for our understanding of the strengths and limitations of the current generation of AI chatbots as virtual publics or online listening platforms, future directions for LLM development, and applications of AI tools to the political domain. (Abridged)", "title": "demonstrations of the potential of aibased political issue polling", "url": "https://arxiv.org/pdf/2307.04781", "tokenized_text": "political multi billion industry influence societal trajectory united_states united states world factors cost availability accuracy time artificial_intelligence artificial intelligence ai chatbots compelling stand ins human behavior powered increasingly sophisticated large_language large language llms ai chatbots effective tool public opinion controversial issues extent campaigns interest groups developed prompt_engineering engineering methodology eliciting human like survey responses chatgpt simulate response policy question person described set demographic factors produce ordinal numeric response score textual justification execute large_scale large scale experiments querying thousands simulated responses cost far lower human surveys compare simulated data human issue data cooperative study find chatgpt effective mean level distribution public opinion variety policy issues abortion supreme court particularly ideological breakdown correlation successful demographic level differences chatgpt tends new policy issues training_data training data collected support involvement work implications understanding strengths limitations current generation ai chatbots virtual online listening platforms future directions llm development applications ai tools political domain"}
{"id": "42219b26a503d03bf70e9953edc3af94c255cb2a", "abstract": "Segment anything model (SAM) developed by Meta AI Research has recently attracted significant attention. Trained on a large segmentation dataset of over 1 billion masks, SAM is capable of segmenting any object on a certain image. In the original SAM work, the authors turned to zero-short transfer tasks (like edge detection) for evaluating the performance of SAM. Recently, numerous works have attempted to investigate the performance of SAM in various scenarios to recognize and segment objects. Moreover, numerous projects have emerged to show the versatility of SAM as a foundation model by combining it with other models, like Grounding DINO, Stable Diffusion, ChatGPT, etc. With the relevant papers and projects increasing exponentially, it is challenging for the readers to catch up with the development of SAM. To this end, this work conducts the first yet comprehensive survey on SAM. This is an ongoing project and we intend to update the manuscript on a regular basis. Therefore, readers are welcome to contact us if they complete new works related to SAM so that we can include them in our next version.", "title": "a survey on segment anything model (sam) vision foundation model meets prompt engineering", "url": "http://arxiv.org/pdf/2306.06211", "tokenized_text": "segment sam developed meta ai research recently attracted significant attention trained large segmentation dataset billion masks sam capable object certain image original sam work authors zero short transfer tasks like edge detection evaluating performance sam recently numerous works investigate performance sam scenarios recognize segment objects numerous projects emerged versatility sam foundation combining like grounding dino stable_diffusion stable diffusion chatgpt etc relevant papers projects increasing exponentially challenging readers catch development sam end work conducts comprehensive survey sam ongoing project intend update regular basis readers contact complete new works related sam include version"}
{"id": "4279a38a098d1d359881b73c6a88a112fe93443a", "abstract": "We introduce Cap3D, an automatic approach for generating descriptive text for 3D objects. This approach utilizes pretrained models from image captioning, image-text alignment, and LLM to consolidate captions from multiple views of a 3D asset, completely side-stepping the time-consuming and costly process of manual annotation. We apply Cap3D to the recently introduced large-scale 3D dataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted using 41k human annotations from the same dataset, demonstrates that Cap3D surpasses human-authored descriptions in terms of quality, cost, and speed. Through effective prompt engineering, Cap3D rivals human performance in generating geometric descriptions on 17k collected annotations from the ABO dataset. Finally, we finetune Text-to-3D models on Cap3D and human captions, and show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E, and DreamFusion.", "title": "scalable 3d captioning with pretrained models", "url": "http://arxiv.org/pdf/2306.07279", "tokenized_text": "introduce automatic approach generating descriptive text 3d objects approach utilizes pretrained image captioning image text alignment llm consolidate captions multiple views 3d asset completely stepping time consuming costly process manual annotation apply recently introduced large scale 3d dataset resulting 3d text pairs evaluation conducted human annotations dataset demonstrates surpasses human authored descriptions terms quality cost speed effective prompt_engineering engineering human performance generating geometric descriptions collected annotations dataset finally finetune text to-3d human captions outperforms benchmark sota including point shape dreamfusion"}
{"id": "43a55dbd95c9d5cd82de8db276f41adeec4a937d", "abstract": "Recent text-to-image generation models have shown promising results in generating high-fidelity photo-realistic images. In parallel, the problem of data scarcity has brought a growing interest in employing AIGC technology for high-quality data expansion. However, this paradigm requires well-designed prompt engineering that cost-less data expansion and labeling remain under-explored. Inspired by LLM's powerful capability in task guidance, we propose a new paradigm of annotated data expansion named as ChatGenImage. The core idea behind it is to leverage the complementary strengths of diverse models to establish a highly effective and user-friendly pipeline for interactive data augmentation. In this work, we extensively study how LLMs communicate with AIGC model to achieve more controllable image generation and make the first attempt to collaborate them for automatic data augmentation for a variety of downstream tasks. Finally, we present fascinating results obtained from our ChatGenImage framework and demonstrate the powerful potential of our synthetic data for systematic vision adaptation. Our codes are available at https://github.com/Yuqifan1117/Labal-Anything-Pipeline.", "title": "interactive data synthesis for systematic vision adaptation via llmsaigcs collaboration", "url": "http://arxiv.org/pdf/2305.12799", "tokenized_text": "recent text image_generation image generation shown promising_results promising results generating high fidelity realistic images parallel problem data scarcity brought growing interest employing aigc technology high quality data expansion paradigm requires designed prompt_engineering engineering cost data expansion labeling remain explored inspired llm powerful capability task guidance propose_a_new propose new paradigm annotated_data annotated data expansion named core idea leverage complementary strengths diverse establish highly effective user friendly pipeline interactive data_augmentation data augmentation work extensively study llms communicate aigc achieve controllable image_generation image generation attempt collaborate automatic data_augmentation data augmentation variety downstream_tasks downstream tasks finally present results obtained framework demonstrate powerful potential synthetic data systematic vision adaptation codes available"}
{"id": "458147b5f7242c998ec4f33798a59b7c48867329", "abstract": "Nearly all jurisdictions in the United States require a professional license exam, commonly referred to as\"the Bar Exam,\"as a precondition for law practice. To even sit for the exam, most jurisdictions require that an applicant completes at least seven years of post-secondary education, including three years at an accredited law school. In addition, most test-takers also undergo weeks to months of further, exam-specific preparation. Despite this significant investment of time and capital, approximately one in five test-takers still score under the rate required to pass the exam on their first try. In the face of a complex task that requires such depth of knowledge, what, then, should we expect of the state of the art in\"AI?\"In this research, we document our experimental evaluation of the performance of OpenAI's `text-davinci-003` model, often-referred to as GPT-3.5, on the multistate multiple choice (MBE) section of the exam. While we find no benefit in fine-tuning over GPT-3.5's zero-shot performance at the scale of our training data, we do find that hyperparameter optimization and prompt engineering positively impacted GPT-3.5's zero-shot performance. For best prompt and parameters, GPT-3.5 achieves a headline correct rate of 50.3% on a complete NCBE MBE practice exam, significantly in excess of the 25% baseline guessing rate, and performs at a passing rate for both Evidence and Torts. GPT-3.5's ranking of responses is also highly-correlated with correctness; its top two and top three choices are correct 71% and 88% of the time, respectively, indicating very strong non-entailment performance. While our ability to interpret these results is limited by nascent scientific understanding of LLMs and the proprietary nature of GPT, we believe that these results strongly suggest that an LLM will pass the MBE component of the Bar Exam in the near future.", "title": "gpt takes the bar exam", "url": "http://arxiv.org/pdf/2212.14402", "tokenized_text": "nearly united_states united states require professional license exam commonly referred bar precondition law practice exam require seven years post secondary education including years law school addition test weeks months exam specific preparation despite significant investment time capital approximately test score rate required pass exam try face complex task requires depth knowledge expect state_of_the_art state art research document experimental evaluation performance openai text davinci-003 referred gpt-3.5 multiple_choice multiple choice section exam find benefit fine tuning gpt-3.5 zero shot performance scale training_data training data find hyperparameter optimization prompt_engineering engineering positively impacted gpt-3.5 zero shot performance best parameters gpt-3.5 achieves correct rate complete practice exam significantly 25 baseline guessing rate performs passing rate evidence gpt-3.5 ranking responses highly correlated correctness choices correct 71 88 time respectively indicating strong non entailment performance ability interpret results limited nascent scientific understanding llms proprietary nature gpt believe results strongly suggest llm pass component bar exam near future"}
{"id": "4591f6cea22b66eccda0103b83002be45e8216b6", "abstract": "Large Language Models (LLMs) have the potential to revolutionize automated traceability by overcoming the challenges faced by previous methods and introducing new possibilities. However, the optimal utilization of LLMs for automated traceability remains unclear. This paper explores the process of prompt engineering to extract link predictions from an LLM. We provide detailed insights into our approach for constructing effective prompts, offering our lessons learned. Additionally, we propose multiple strategies for leveraging LLMs to generate traceability links, improving upon previous zero-shot methods on the ranking of candidate links after prompt refinement. The primary objective of this paper is to inspire and assist future researchers and engineers by highlighting the process of constructing traceability prompts to effectively harness LLMs for advancing automatic traceability.", "title": "prompts matter insights and strategies for prompt engineering in automated software traceability", "url": "https://arxiv.org/pdf/2308.00229", "tokenized_text": "large_language large language llms potential revolutionize automated traceability overcoming challenges faced previous methods introducing new possibilities optimal utilization llms automated traceability remains unclear paper explores process prompt_engineering engineering extract link predictions llm provide detailed insights approach constructing effective offering lessons learned additionally propose multiple strategies leveraging llms generate traceability links improving previous zero shot methods ranking candidate links refinement primary objective paper inspire assist future researchers engineers highlighting process constructing traceability effectively harness llms advancing automatic traceability"}
{"id": "45c46687bc8d2dbdea6f92fc14d4dc7a548ddd12", "abstract": "Increase in computational scale and fine-tuning has seen a dramatic improvement in the quality of outputs of large language models (LLMs) like GPT. Given that both GPT-3 and GPT-4 were trained on large quantities of human-generated text, we might ask to what extent their outputs reflect patterns of human thinking, both for correct and incorrect cases. The Erotetic Theory of Reason (ETR) provides a symbolic generative model of both human success and failure in thinking, across propositional, quantified, and probabilistic reasoning, as well as decision-making. We presented GPT-3, GPT-3.5, and GPT-4 with 61 central inference and judgment problems from a recent book-length presentation of ETR, consisting of experimentally verified data-points on human judgment and extrapolated data-points predicted by ETR, with correct inference patterns as well as fallacies and framing effects (the ETR61 benchmark). ETR61 includes classics like Wason's card task, illusory inferences, the decoy effect, and opportunity-cost neglect, among others. GPT-3 showed evidence of ETR-predicted outputs for 59% of these examples, rising to 77% in GPT-3.5 and 75% in GPT-4. Remarkably, the production of human-like fallacious judgments increased from 18% in GPT-3 to 33% in GPT-3.5 and 34% in GPT-4. This suggests that larger and more advanced LLMs may develop a tendency toward more human-like mistakes, as relevant thought patterns are inherent in human-produced training data. According to ETR, the same fundamental patterns are involved both in successful and unsuccessful ordinary reasoning, so that the\"bad\"cases could paradoxically be learned from the\"good\"cases. We further present preliminary evidence that ETR-inspired prompt engineering could reduce instances of these mistakes.", "title": "humans in humans out on gpt converging toward common sense in both success and failure", "url": "http://arxiv.org/pdf/2303.17276", "tokenized_text": "increase computational scale fine tuning seen dramatic improvement quality outputs large_language large language llms like gpt given gpt-3 gpt-4 trained large quantities human generated text ask extent outputs reflect patterns human thinking correct incorrect cases theory reason provides symbolic generative human success failure thinking propositional quantified probabilistic reasoning decision making presented gpt-3 gpt-3.5 gpt-4 61 central inference judgment problems recent book length presentation consisting experimentally verified data points human judgment data points predicted correct inference patterns framing effects benchmark includes like card task illusory inferences effect opportunity cost neglect gpt-3 showed evidence predicted outputs 59 examples rising 77 gpt-3.5 75 gpt-4 remarkably production human like fallacious judgments increased 18 gpt-3 33 gpt-3.5 34 gpt-4 suggests larger advanced llms develop tendency human like mistakes relevant thought patterns inherent human produced training_data training data according fundamental patterns involved successful unsuccessful ordinary reasoning learned present preliminary evidence inspired prompt_engineering engineering reduce instances mistakes"}
{"id": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722", "abstract": "By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the\"program,\"optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.", "title": "large language models are humanlevel prompt engineers", "url": "http://arxiv.org/pdf/2211.01910", "tokenized_text": "conditioning natural_language natural language instructions large_language large language llms impressive capabilities general purpose computers task performance depends significantly quality steer effective handcrafted humans inspired classical program synthesis human approach prompt_engineering engineering propose automatic engineer ape automatic instruction generation selection method treat instruction searching pool instruction candidates proposed llm order maximize chosen score function evaluate quality selected instruction evaluate zero shot performance llm following selected instruction experiments 24 nlp_tasks nlp tasks automatically generated instructions outperform prior llm baseline large margin achieve better comparable performance instructions generated human annotators tasks conduct_extensive conduct extensive qualitative quantitative analyses explore performance ape ape engineered applied steer truthfulness and/or informativeness improve shot_learning shot learning performance simply prepending standard context_learning context learning check webpage"}
{"id": "47d04bcfe0f1bed72d03c68cce76b4cf4be03f11", "abstract": "Domain-specific text classification faces the challenge of scarce labeled data due to the high cost of manual labeling. Prompt-learning, known for its efficiency in few-shot scenarios, is proposed as an alternative to traditional fine-tuning methods. And besides, although large language models (LLMs) have gained prominence, small language models (SLMs, with under 1B parameters) offer significant customizability, adaptability, and cost-effectiveness for domain-specific tasks, given industry constraints. In this study, we investigate the potential of SLMs combined with prompt-learning paradigm for domain-specific text classification, specifically within customer-agent interactions in retail. Our evaluations show that, in few-shot settings when prompt-based model fine-tuning is possible, T5-base, a typical SLM with 220M parameters, achieve approximately 75% accuracy with limited labeled data (up to 15% of full data), which shows great potentials of SLMs with prompt-learning. Based on this, We further validate the effectiveness of active few-shot sampling and the ensemble strategy in the prompt-learning pipeline that contribute to a remarkable performance gain. Besides, in zero-shot settings with a fixed model, we underscore a pivotal observation that, although the GPT-3.5-turbo equipped with around 154B parameters garners an accuracy of 55.16%, the power of well designed prompts becomes evident when the FLAN-T5-large, a model with a mere 0.5% of GPT-3.5-turbo's parameters, achieves an accuracy exceeding 31% with the optimized prompt, a leap from its sub-18% performance with an unoptimized one. Our findings underscore the promise of prompt-learning in classification tasks with SLMs, emphasizing the benefits of active few-shot sampling, and ensemble strategies in few-shot settings, and the importance of prompt engineering in zero-shot settings.", "title": "exploring small language models with promptlearning paradigm for efficient domainspecific text classification", "url": "https://arxiv.org/pdf/2309.14779", "tokenized_text": "domain specific text_classification text classification faces challenge scarce labeled_data labeled data high cost manual labeling learning known efficiency shot scenarios proposed alternative traditional fine tuning methods large_language large language llms gained prominence small language_models language slms parameters offer significant adaptability cost effectiveness domain specific tasks given industry constraints study investigate potential slms combined learning paradigm domain specific text_classification text classification specifically customer agent interactions evaluations shot_settings shot settings based fine tuning possible t5 base typical slm 220 parameters achieve approximately 75 accuracy limited labeled_data labeled data 15 data shows great potentials slms learning based validate effectiveness active shot sampling ensemble strategy learning pipeline contribute remarkable performance gain zero shot_settings shot settings fixed underscore pivotal observation gpt-3.5 turbo equipped parameters accuracy power designed evident flan t5 large 0.5 gpt-3.5 turbo parameters achieves accuracy exceeding 31 optimized leap performance findings underscore promise learning classification tasks slms emphasizing benefits active shot sampling ensemble strategies shot_settings shot settings importance prompt_engineering engineering zero shot_settings shot settings"}
{"id": "4a99a85f071e67bf15ae4bc53ec37af28b650ec4", "abstract": "Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a suite of robust baselines without necessitating other text representation models or external databases.", "title": "cotbert enhancing unsupervised sentence representation through chainofthought", "url": "https://arxiv.org/pdf/2309.11143", "tokenized_text": "unsupervised sentence representation learning aims transform input sentences fixed length vectors enriched intricate semantic information obviating reliance labeled_data labeled data recent progress field propelled contrastive_learning contrastive learning prompt_engineering engineering significantly gap unsupervised supervised strategies nonetheless potential utilization chain thought remains largely untapped trajectory unlock latent capabilities pre trained bert propose stage approach sentence representation comprehension summarization subsequently output phase vectorized representation input sentence performance enhancement meticulously refine contrastive_learning contrastive learning loss function template denoising technique prompt_engineering engineering rigorous experimentation method cot bert transcending suite robust baselines necessitating text representation external databases"}
{"id": "4b6df5f9885c9dc0ce3125791fd01824e3cf37b7", "abstract": "Contextualizing problems to align with student interests can significantly improve learning outcomes. However, this task often presents scalability challenges due to resource and time constraints. Recent advancements in Large Language Models (LLMs) like GPT-4 offer potential solutions to these issues. This study explores the ability of GPT-4 in the contextualization of problems within CTAT, an intelligent tutoring system, aiming to increase student engagement and enhance learning outcomes. Through iterative prompt engineering, we achieved meaningful contextualization that preserved the difficulty and original intent of the problem, thereby not altering values or overcomplicating the questions. While our research highlights the potential of LLMs in educational settings, we acknowledge current limitations, particularly with geometry problems, and emphasize the need for ongoing evaluation and research. Future work includes systematic studies to measure the impact of this tool on students' learning outcomes and enhancements to handle a broader range of problems.", "title": "contextualizing problems to student interests at scale in intelligent tutoring system using large language models", "url": "http://arxiv.org/pdf/2306.00190", "tokenized_text": "problems align student interests significantly improve learning outcomes task presents scalability challenges resource time constraints recent advancements large_language large language llms like gpt-4 offer potential solutions issues study explores ability gpt-4 contextualization problems intelligent tutoring system aiming increase student engagement enhance learning outcomes iterative prompt_engineering engineering achieved meaningful contextualization preserved difficulty original intent problem altering values questions research highlights potential llms educational settings current limitations particularly geometry problems emphasize need ongoing evaluation research future work includes systematic studies measure impact tool students learning outcomes enhancements handle broader range problems"}
{"id": "4d21debb0f5fec315181e0912b5105c6ce4fc67f", "abstract": "Because state-of-the-art language models are expensive to train, most practitioners must make use of one of the few publicly available language models or language model APIs. This consolidation of trust increases the potency of backdoor attacks, where an adversary tampers with a machine learning model in order to make it perform some malicious behavior on inputs that contain a predefined backdoor trigger. We show that the in-context learning ability of large language models significantly complicates the question of developing backdoor attacks, as a successful backdoor must work against various prompting strategies and should not affect the model's general purpose capabilities. We design a new attack for eliciting targeted misclassification when language models are prompted to perform a particular target task and demonstrate the feasibility of this attack by backdooring multiple large language models ranging in size from 1.3 billion to 6 billion parameters. Finally we study defenses to mitigate the potential harms of our attack: for example, while in the white-box setting we show that fine-tuning models for as few as 500 steps suffices to remove the backdoor behavior, in the black-box setting we are unable to develop a successful defense that relies on prompt engineering alone.", "title": "backdoor attacks for incontext learning with language models", "url": "https://arxiv.org/pdf/2307.14692", "tokenized_text": "state art language_models language expensive train practitioners use publicly_available publicly available language_models language language_model language apis consolidation trust increases potency backdoor attacks adversary machine_learning machine learning order perform malicious behavior inputs contain predefined backdoor trigger context_learning context learning ability large_language large language significantly question developing backdoor attacks successful backdoor work strategies affect general_purpose general purpose capabilities design new attack eliciting targeted language_models language prompted perform particular target task demonstrate feasibility attack multiple large_language large language ranging size 1.3 billion billion parameters finally study defenses mitigate potential harms attack example white box setting fine tuning 500 steps remove backdoor behavior black box setting unable develop successful defense relies prompt_engineering engineering"}
{"id": "4d81c33b295c092016ac236cfd32020a5bb70b97", "abstract": "Well-designed prompts can guide text-to-image models to generate amazing images. However, the performant prompts are often model-specific and misaligned with user input. Instead of laborious human engineering, we propose prompt adaptation, a general framework that automatically adapts original user input to model-preferred prompts. Specifically, we first perform supervised fine-tuning with a pretrained language model on a small collection of manually engineered prompts. Then we use reinforcement learning to explore better prompts. We define a reward function that encourages the policy to generate more aesthetically pleasing images while preserving the original user intentions. Experimental results on Stable Diffusion show that our method outperforms manual prompt engineering in terms of both automatic metrics and human preference ratings. Moreover, reinforcement learning further boosts performance, especially on out-of-domain prompts. The pretrained checkpoints are available at https://aka.ms/promptist. The demo can be found at https://aka.ms/promptist-demo.", "title": "optimizing prompts for texttoimage generation", "url": "http://arxiv.org/pdf/2212.09611", "tokenized_text": "designed guide text image generate amazing images performant specific misaligned user input instead laborious human engineering propose adaptation general framework automatically adapts original user input preferred specifically perform supervised fine tuning pretrained_language pretrained language small collection manually engineered use reinforcement_learning reinforcement learning explore better define reward function encourages policy generate images preserving original user intentions experimental_results experimental results stable_diffusion stable diffusion method outperforms manual prompt_engineering engineering terms automatic metrics human preference ratings reinforcement_learning reinforcement learning boosts performance especially domain pretrained checkpoints available demo found"}
{"id": "4dd461b2392a6983d36618744d2384349c4170f9", "abstract": "This paper investigates the emotional reasoning abilities of the GPT family of large language models via a component perspective. The paper first examines how the model reasons about autobiographical memories. Second, it systematically varies aspects of situations to impact emotion intensity and coping tendencies. Even without the use of prompt engineering, it is shown that GPT's predictions align significantly with human-provided appraisals and emotional labels. However, GPT faces difficulties predicting emotion intensity and coping responses. GPT-4 showed the highest performance in the initial study but fell short in the second, despite providing superior results after minor prompt engineering. This assessment brings up questions on how to effectively employ the strong points and address the weak areas of these models, particularly concerning response variability. These studies underscore the merits of evaluating models from a componential perspective.", "title": "is gpt a computational model of emotion detailed analysis", "url": "https://arxiv.org/pdf/2307.13779", "tokenized_text": "paper investigates emotional reasoning abilities gpt family large_language large language component perspective paper examines reasons memories second systematically varies aspects situations impact emotion intensity coping tendencies use prompt_engineering engineering shown gpt predictions align significantly human provided emotional labels gpt faces difficulties predicting emotion intensity coping responses gpt-4 showed highest performance initial study fell short second despite providing superior results minor prompt_engineering engineering assessment brings questions effectively employ strong points address weak areas particularly concerning response variability studies underscore merits evaluating perspective"}
{"id": "4e96d7fa9f27857523d786230294fbcc6060212c", "abstract": "In recent years, the use of automated source code generation utilizing transformer-based generative models has expanded, and these models can generate functional code according to the requirements of the developers. However, recent research revealed that these automatically generated source codes can contain vulnerabilities and other quality issues. Despite researchers' and practitioners' attempts to enhance code generation models, retraining and fine-tuning large language models is time-consuming and resource-intensive. Thus, we describe FRANC, a lightweight framework for recommending more secure and high-quality source code derived from transformer-based code generation models. FRANC includes a static filter to make the generated code compilable with heuristics and a quality-aware ranker to sort the code snippets based on a quality score. Moreover, the framework uses prompt engineering to fix persistent quality issues. We evaluated the framework with five Python and Java code generation models and six prompt datasets, including a newly created one in this work (SOEval). The static filter improves 9% to 46% Java suggestions and 10% to 43% Python suggestions regarding compilability. The average improvement over the NDCG@10 score for the ranking system is 0.0763, and the repairing techniques repair the highest 80% of prompts. FRANC takes, on average, 1.98 seconds for Java; for Python, it takes 0.08 seconds.", "title": "a lightweight framework for highquality code generation", "url": "https://arxiv.org/pdf/2307.08220", "tokenized_text": "recent_years recent years use automated source_code source code generation utilizing transformer based generative expanded generate functional code according requirements developers recent research revealed automatically generated source codes contain vulnerabilities quality issues despite researchers practitioners attempts enhance code_generation code generation retraining fine tuning large_language large language time consuming resource intensive describe lightweight framework secure high quality source_code source code derived transformer based code_generation code generation includes static filter generated code compilable heuristics quality aware ranker sort code snippets based quality score framework uses prompt_engineering engineering fix persistent quality issues evaluated framework python java code_generation code generation datasets including newly created work static filter improves 46 java suggestions 10 43 python suggestions average improvement score ranking system repairing techniques repair highest 80 takes average seconds java python takes 0.08 seconds"}
{"id": "4ff0fccc922f9da7c818c86c8a13aef23ea08345", "abstract": "The Imaging Data Commons (IDC) is a cloud-based database that provides researchers with open access to cancer imaging data, with the goal of facilitating collaboration in medical imaging research. However, querying the IDC database for cohort discovery and access to imaging data has a significant learning curve for researchers due to its complex nature. We developed Text2Cohort, a large language model (LLM) based toolkit to facilitate user-friendly and intuitive natural language cohort discovery in the IDC. Text2Cohorts translates user input into IDC database queries using prompt engineering and autocorrection and returns the query's response to the user. Autocorrection resolves errors in queries by passing the errors back to the model for interpretation and correction. We evaluate Text2Cohort on 50 natural language user inputs ranging from information extraction to cohort discovery. The resulting queries and outputs were verified by two computer scientists to measure Text2Cohort's accuracy and F1 score. Text2Cohort successfully generated queries and their responses with an 88% accuracy and F1 score of 0.94. However, it failed to generate queries for 6/50 (12%) user inputs due to syntax and semantic errors. Our results indicate that Text2Cohort succeeded at generating queries with correct responses, but occasionally failed due to a lack of understanding of the data schema. Despite these shortcomings, Text2Cohort demonstrates the utility of LLMs to enable researchers to discover and curate cohorts using data hosted on IDC with high levels of accuracy using natural language in a more intuitive and user-friendly way.", "title": "text2cohort democratizing the nci imaging data commons with natural language cohort discovery", "url": "http://arxiv.org/pdf/2305.07637", "tokenized_text": "imaging data commons cloud based database provides researchers open access cancer imaging data goal facilitating collaboration medical imaging research querying database cohort discovery access imaging data significant learning curve researchers complex nature developed large_language large language llm based toolkit facilitate user friendly intuitive natural_language natural language cohort discovery translates user input database queries prompt_engineering engineering returns query response user resolves errors queries passing errors interpretation correction evaluate 50 natural_language natural language user inputs ranging information_extraction information extraction cohort discovery resulting queries outputs verified computer scientists measure accuracy f1_score f1 score successfully generated queries responses 88 accuracy f1_score f1 score failed generate queries 12 user inputs syntax semantic errors results_indicate results indicate succeeded generating queries correct responses occasionally failed lack understanding data schema despite shortcomings demonstrates utility llms enable researchers discover curate cohorts data hosted high levels accuracy natural_language natural language intuitive user friendly way"}
{"id": "50aaac5fdc2b5a33bfd3ba93cdf4e5e302f34297", "abstract": "In this paper, we explore the potential of Large Language Models (LLMs) to reason about threats, generate information about tools, and automate cyber campaigns. We begin with a manual exploration of LLMs in supporting specific threat-related actions and decisions. We proceed by automating the decision process in a cyber campaign. We present prompt engineering approaches for a plan-act-report loop for one action of a threat campaign and and a prompt chaining design that directs the sequential decision process of a multi-action campaign. We assess the extent of LLM's cyber-specific knowledge w.r.t the short campaign we demonstrate and provide insights into prompt design for eliciting actionable responses. We discuss the potential impact of LLMs on the threat landscape and the ethical considerations of using LLMs for accelerating threat actor capabilities. We report a promising, yet concerning, application of generative AI to cyber threats. However, the LLM's capabilities to deal with more complex networks, sophisticated vulnerabilities, and the sensitivity of prompts are open questions. This research should spur deliberations over the inevitable advancements in LLM-supported cyber adversarial landscape.", "title": "llms killed the script kiddie how agents supported by large language models change the landscape of network threat testing", "url": "https://arxiv.org/pdf/2310.06936", "tokenized_text": "paper explore potential large_language large language llms reason threats generate information tools automate cyber campaigns begin manual exploration llms supporting specific threat related actions decisions automating decision process cyber present prompt_engineering engineering approaches plan act report loop action threat chaining design directs sequential decision process multi action assess extent llm cyber specific knowledge w.r.t short demonstrate provide insights design eliciting actionable responses discuss potential impact llms threat landscape ethical considerations llms accelerating threat actor capabilities report promising concerning application generative_ai generative ai cyber threats llm capabilities deal complex networks sophisticated vulnerabilities sensitivity open questions research spur advancements llm supported cyber adversarial landscape"}
{"id": "50bbca86de82d6b72d92bba0ec988b58e644dac3", "abstract": "Large-scale visual-language pre-trained models (VLPM) have proven their excellent performance in downstream object detection for natural scenes. However, zero-shot nuclei detection on H\\&E images via VLPMs remains underexplored. The large gap between medical images and the web-originated text-image pairs used for pre-training makes it a challenging task. In this paper, we attempt to explore the potential of the object-level VLPM, Grounded Language-Image Pre-training (GLIP) model, for zero-shot nuclei detection. Concretely, an automatic prompts design pipeline is devised based on the association binding trait of VLPM and the image-to-text VLPM BLIP, avoiding empirical manual prompts engineering. We further establish a self-training framework, using the automatically designed prompts to generate the preliminary results as pseudo labels from GLIP and refine the predicted boxes in an iterative manner. Our method achieves a remarkable performance for label-free nuclei detection, surpassing other comparison methods. Foremost, our work demonstrates that the VLPM pre-trained on natural image-text pairs exhibits astonishing potential for downstream tasks in the medical field as well. Code will be released at https://github.com/wuyongjianCODE/VLPMNuD.", "title": "zeroshot nuclei detection via visuallanguage pretrained models", "url": "http://arxiv.org/pdf/2306.17659", "tokenized_text": "large scale visual language pre trained proven excellent performance downstream object_detection object detection natural scenes zero shot detection images remains underexplored large gap medical images web text image pairs pre training makes challenging task paper attempt explore potential object level grounded language-image pre-training zero shot detection concretely automatic design pipeline devised based association binding image text blip avoiding empirical manual engineering establish self training framework automatically designed generate preliminary results pseudo labels refine predicted boxes iterative manner method_achieves method achieves remarkable performance label free detection surpassing comparison methods work demonstrates pre trained natural image text pairs exhibits astonishing potential downstream_tasks downstream tasks medical field code released"}
{"id": "50d40d05598e456188a3be42983b8daabd3f04f7", "abstract": "With the emergence of Machine Learning, there has been a surge in leveraging its capabilities for problem-solving across various domains. In the code clone realm, the identification of type-4 or semantic clones has emerged as a crucial yet challenging task. Researchers aim to utilize Machine Learning to tackle this challenge, often relying on the BigCloneBench dataset. However, it's worth noting that BigCloneBench, originally not designed for semantic clone detection, presents several limitations that hinder its suitability as a comprehensive training dataset for this specific purpose. Furthermore, CLCDSA dataset suffers from a lack of reusable examples aligning with real-world software systems, rendering it inadequate for cross-language clone detection approaches. In this work, we present a comprehensive semantic clone and cross-language clone benchmark, GPTCloneBench by exploiting SemanticCloneBench and OpenAI's GPT-3 model. In particular, using code fragments from SemanticCloneBench as sample inputs along with appropriate prompt engineering for GPT-3 model, we generate semantic and cross-language clones for these specific fragments and then conduct a combination of extensive manual analysis, tool-assisted filtering, functionality testing and automated validation in building the benchmark. From 79,928 clone pairs of GPT-3 output, we created a benchmark with 37,149 true semantic clone pairs, 19,288 false semantic pairs(Type-1/Type-2), and 20,770 cross-language clones across four languages (Java, C, C#, and Python). Our benchmark is 15-fold larger than SemanticCloneBench, has more functional code examples for software systems and programming language support than CLCDSA, and overcomes BigCloneBench's qualities, quantification, and language variety limitations.", "title": "gptclonebench a comprehensive benchmark of semantic clones and crosslanguage clones using gpt3 model and semanticclonebench", "url": "https://arxiv.org/pdf/2308.13963", "tokenized_text": "emergence machine_learning machine learning surge leveraging capabilities problem solving domains code clone realm identification semantic clones emerged crucial challenging task researchers aim utilize machine_learning machine learning tackle challenge relying dataset worth originally designed semantic clone detection presents limitations hinder suitability comprehensive training dataset specific purpose furthermore dataset suffers lack reusable examples aligning real world software systems rendering inadequate cross language clone detection approaches work present comprehensive semantic clone cross language clone benchmark exploiting semanticclonebench openai gpt-3 particular code fragments semanticclonebench sample inputs appropriate prompt_engineering engineering gpt-3 generate semantic cross language clones specific fragments conduct combination extensive manual analysis tool assisted filtering functionality testing automated validation building benchmark clone pairs gpt-3 output created benchmark true semantic clone pairs false semantic cross language clones languages java python benchmark 15 fold larger semanticclonebench functional code examples software systems programming language support qualities quantification language variety limitations"}
{"id": "521ccc898395a2818fced22b4cf371b0e5121f94", "abstract": "The common practice for training commonsense models has gone from\u2013human\u2013to\u2013corpus\u2013to\u2013machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from\u2013machine\u2013to\u2013corpus\u2013to\u2013machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al. 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically\u2013as text\u2013in addition to the neural model. We distill only one aspect\u2013the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model\u2019s commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and will share our new symbolic knowledge graph and commonsense models.", "title": "symbolic knowledge distillation from general language models to commonsense models", "url": "https://aclanthology.org/2022.naacl-main.341.pdf", "tokenized_text": "common practice training commonsense human corpus machine humans author commonsense knowledge graphs order train commonsense work investigate alternative machine corpus machine general language_models language author commonsense knowledge graphs train commonsense study leads new framework symbolic knowledge_distillation knowledge distillation prior art knowledge_distillation knowledge distillation et_al et al 2015 approach uses larger teach smaller key difference distill knowledge symbolically text addition neural distill aspect commonsense general language_model language teacher allowing student different type commonsense careful prompt_engineering engineering separately trained critic allow selectively distill high quality causal commonsense gpt-3 general language_model language empirical results_demonstrate results demonstrate time human authored commonsense knowledge_graph knowledge graph surpassed automatically distilled variant criteria quantity quality diversity addition results neural commonsense surpasses teacher commonsense capabilities despite 100x smaller size apply atomic resource share new symbolic knowledge_graph knowledge graph commonsense"}
{"id": "531678c18fd2c5a9620b68f3550131fc3fd3636c", "abstract": "Radiology report generation aims to automatically provide clinically meaningful descriptions of radiology images such as MRI and X-ray. Although great success has been achieved in natural scene image captioning tasks, radiology report generation remains challenging and requires prior medical knowledge. In this paper, we propose PromptRRG, a method that utilizes prompt learning to activate a pretrained model and incorporate prior knowledge. Since prompt learning for radiology report generation has not been explored before, we begin with investigating prompt designs and categorise them based on varying levels of knowledge: common, domain-specific and disease-enriched prompts. Additionally, we propose an automatic prompt learning mechanism to alleviate the burden of manual prompt engineering. This is the first work to systematically examine the effectiveness of prompt learning for radiology report generation. Experimental results on the largest radiology report generation benchmark, MIMIC-CXR, demonstrate that our proposed method achieves state-of-the-art performance. Code will be available upon the acceptance.", "title": "can prompt learning benefit radiology report generation", "url": "https://arxiv.org/pdf/2308.16269", "tokenized_text": "radiology report generation aims automatically provide clinically meaningful descriptions radiology images ray great success achieved natural scene image captioning tasks radiology report generation remains challenging requires prior medical knowledge paper propose method utilizes learning activate pretrained incorporate prior knowledge learning radiology report generation explored begin investigating designs based varying levels knowledge common domain specific disease enriched additionally propose automatic learning mechanism alleviate burden manual prompt_engineering engineering work systematically examine effectiveness learning radiology report generation experimental_results experimental results largest radiology report generation benchmark mimic cxr demonstrate proposed_method proposed method achieves_state achieves state art performance code available acceptance"}
{"id": "53e7475a3ed0caee37122a9dbdb53d1da0691a33", "abstract": "GPT-3 and several other language models (LMs) can effectively address various natural language processing (NLP) tasks, including machine translation and text summarization. Recently, they have also been successfully employed in the business process management (BPM) domain, e.g., for predictive process monitoring and process extraction from text. This, however, typically requires fine-tuning the employed LM, which, among others, necessitates large amounts of suitable training data. A possible solution to this problem is the use of prompt engineering, which leverages pre-trained LMs without fine-tuning them. Recognizing this, we argue that prompt engineering can help bring the capabilities of LMs to BPM research. We use this position paper to develop a research agenda for the use of prompt engineering for BPM research by identifying the associated potentials and challenges.", "title": "just tell me prompt engineering in business process management", "url": "http://arxiv.org/pdf/2304.07183", "tokenized_text": "gpt-3 language_models language lms effectively address natural_language natural language processing nlp tasks including machine_translation machine translation text summarization recently successfully employed business process management bpm domain e.g. predictive process monitoring process extraction text typically requires fine tuning employed lm necessitates large amounts suitable training_data training data possible solution problem use prompt_engineering engineering leverages pre trained lms fine tuning recognizing argue prompt_engineering engineering help bring capabilities lms bpm research use position paper develop research use prompt_engineering engineering bpm research identifying associated potentials challenges"}
{"id": "56a9c96a29f4047be8465244576d731f0df2d9df", "abstract": "Prompt-based models have made remarkable advancements in the fields of zero-shot and few-shot learning, attracting a lot of attention from researchers. Developing an effective prompt template plays a critical role. However, prior studies have mainly focused on prompt vocabulary selection or embedding initialization with the reserved prompt position fixed. In this empirical study, we conduct the most comprehensive analysis to date of prompt position option for natural language understanding tasks. Our findings quantify the substantial impact prompt position has on model performance. We observe that the prompt position used in prior studies is often sub-optimal for both zero-shot and few-shot settings. These findings suggest prompt position optimisation as an interesting research direction alongside the existing focus on prompt engineering.", "title": "prompt position really matters in fewshot and zeroshot nlu tasks", "url": "https://arxiv.org/pdf/2305.14493", "tokenized_text": "based remarkable advancements fields zero shot shot_learning shot learning attracting lot attention researchers developing effective prompt_template template plays critical role prior studies mainly focused vocabulary selection embedding initialization reserved position fixed empirical study conduct comprehensive analysis date position option natural_language natural language understanding tasks findings quantify substantial impact position performance observe position prior studies sub optimal zero shot shot_settings shot settings findings_suggest findings suggest position optimisation interesting research direction alongside existing focus prompt_engineering engineering"}
{"id": "57404bd8c71e2b17fce63b49229b278b6a66bf13", "abstract": "Natural language is among the most accessible tools for explaining decisions to humans, and large pretrained language models (PLMs) have demonstrated impressive abilities to generate coherent natural language explanations (NLE). The existing NLE research perspectives do not take the audience into account. An NLE can have high textual quality, but it might not accommodate audiences' needs and preference. To address this limitation, we propose an alternative perspective, situated NLE, including a situated generation framework and a situated evaluation framework. On the generation side, we propose simple prompt engineering methods that adapt the NLEs to situations. In human studies, the annotators preferred the situated NLEs. On the evaluation side, we set up automated evaluation scores in lexical, semantic, and pragmatic categories. The scores can be used to select the most suitable prompts to generate NLEs. Situated NLE provides a perspective to conduct further research on automatic NLE generations.", "title": "situated natural language explanations", "url": "https://arxiv.org/pdf/2308.14115", "tokenized_text": "natural_language natural language accessible tools explaining decisions humans large pretrained_language pretrained language plms demonstrated impressive abilities generate coherent natural_language natural language explanations existing research perspectives audience account high textual quality accommodate audiences needs preference address limitation propose alternative perspective situated including situated generation framework situated evaluation framework generation propose simple prompt_engineering engineering methods adapt nles situations human studies annotators preferred situated nles evaluation set automated evaluation scores lexical semantic categories scores select suitable generate nles situated provides perspective conduct research automatic generations"}
{"id": "57a4f8f69908d3474565d3cd6f58b1ca651ff673", "abstract": "Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\\epsilon$ controllability to characterize LLM steerability. We compute the $k-\\epsilon$ controllability of a panel of large language models, including Falcon-7b, Llama-7b, and Falcon-40b on 5000 WikiText causal language modeling tasks. Remarkably, we find that magic words of 10 tokens or less exist for over 97% of WikiText instances surveyed for each model.", "title": "what's the magic word a control theory of llm prompting", "url": "https://arxiv.org/pdf/2310.04444", "tokenized_text": "prompt_engineering engineering effective important deployment llms poorly understood mathematically formalize prompt_engineering engineering optimal control problem llms considered control variable output distribution llm framework ask simple question given sequence tokens exist prepend steer llm accurately predicting final token optimal magic word prepending causes llm output correct answer magic words exist find properties offer analytic analysis controllability self attention head prove bound controllability function singular values weight matrices inspiration control theory propose metric called controllability characterize llm compute controllability panel large_language large language including falcon-7b llama-7b 5000 causal language modeling tasks remarkably find magic words 10 tokens exist 97 instances"}
{"id": "57bb978b8075fd5701a61770c5ee7244c414e8fd", "abstract": "Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allow it to learn the contextual bias from labeled data. We validate the effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks.", "title": "batch calibration rethinking calibration for incontext learning and prompt engineering", "url": "https://arxiv.org/pdf/2309.17249", "tokenized_text": "context_learning context learning icl efficient learning paradigms large_language large language llms llms suffer brittleness bias factors including limited formatting choice verbalizers icl examples address problem results unexpected performance degradation calibration methods developed mitigate effects biases recovering llm performance work conduct systematic analysis existing calibration methods provide unified view reveal failure cases inspired analyses propose batch calibration simple intuitive method controls contextual bias batched input unifies prior approaches effectively addresses aforementioned issues zero shot inference negligible additional costs shot setup extend allow learn contextual bias labeled_data labeled data validate effectiveness palm clip demonstrate state art performance previous calibration baselines 10 natural_language natural language understanding image classification tasks"}
{"id": "5ce2a1dc9dfa8b4f1368220ac7f7d30a395ffca9", "abstract": "We present the JVNV, a Japanese emotional speech corpus with verbal content and nonverbal vocalizations whose scripts are generated by a large-scale language model. Existing emotional speech corpora lack not only proper emotional scripts but also nonverbal vocalizations (NVs) that are essential expressions in spoken language to express emotions. We propose an automatic script generation method to produce emotional scripts by providing seed words with sentiment polarity and phrases of nonverbal vocalizations to ChatGPT using prompt engineering. We select 514 scripts with balanced phoneme coverage from the generated candidate scripts with the assistance of emotion confidence scores and language fluency scores. We demonstrate the effectiveness of JVNV by showing that JVNV has better phoneme coverage and emotion recognizability than previous Japanese emotional speech corpora. We then benchmark JVNV on emotional text-to-speech synthesis using discrete codes to represent NVs. We show that there still exists a gap between the performance of synthesizing read-aloud speech and emotional speech, and adding NVs in the speech makes the task even harder, which brings new challenges for this task and makes JVNV a valuable resource for relevant works in the future. To our best knowledge, JVNV is the first speech corpus that generates scripts automatically using large language models.", "title": "jvnv a corpus of japanese emotional speech with verbal content and nonverbal expressions", "url": "https://arxiv.org/pdf/2310.06072", "tokenized_text": "present japanese emotional speech corpus verbal content nonverbal scripts generated large scale language_model language existing emotional speech corpora lack proper emotional scripts nonverbal essential expressions spoken language express emotions propose automatic script generation method produce emotional scripts providing seed words sentiment polarity phrases nonverbal chatgpt prompt_engineering engineering select scripts balanced phoneme coverage generated candidate scripts assistance emotion confidence scores language fluency scores demonstrate_the_effectiveness demonstrate effectiveness showing better phoneme coverage emotion previous japanese emotional speech corpora benchmark emotional text speech synthesis discrete codes represent exists gap performance synthesizing read aloud speech emotional speech adding speech makes task harder brings new challenges task makes valuable resource relevant works future best knowledge speech corpus generates scripts automatically large_language large language"}
{"id": "5d49c7401c5f2337c4cc88d243ae39ed659afe64", "abstract": "Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (\u201cred teaming\u201d) using another LM. We evaluate the target LM\u2019s replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot\u2019s own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.", "title": "red teaming language models with language models", "url": "https://aclanthology.org/2022.emnlp-main.225.pdf", "tokenized_text": "language_models language lms deployed potential harm users hard predict ways prior_work prior work identifies harmful behaviors deployment human annotators hand write test_cases test cases human annotation expensive limiting number diversity test_cases test cases work automatically find cases target lm harmful way generating test_cases test cases red teaming lm evaluate target lm replies generated test questions classifier trained detect offensive content uncovering tens thousands offensive replies parameter lm chatbot explore methods zero shot generation reinforcement_learning reinforcement learning generating test_cases test cases varying levels diversity difficulty furthermore use prompt_engineering engineering control lm generated test_cases test cases uncover variety harms automatically finding groups people chatbot discusses offensive ways personal hospital numbers generated chatbot contact leakage private training_data training data generated text harms occur course conversation overall lm based red teaming promising tool needed finding fixing diverse undesirable lm behaviors impacting users"}
{"id": "5de60d53bce194b34dae1e531876af9acffba1a3", "abstract": "Text-to-image generative models have recently exploded in popularity and accessibility. Yet so far, use of these models in creative tasks that bridge the 2D digital world and the creation of physical artefacts has been understudied. We conduct a pilot study to investigate if and how text-to-image models can be used to assist in upstream tasks within the creative process, such as ideation and visualization, prior to a sculpture-making activity. Thirty participants selected sculpture-making materials and generated three images using the Stable Diffusion text-to-image generator, each with text prompts of their choice, with the aim of informing and then creating a physical sculpture. The majority of participants (23/30) reported that the generated images informed their sculptures, and 28/30 reported interest in using text-to-image models to help them in a creative task in the future. We identify several prompt engineering strategies and find that a participant's prompting strategy relates to their stage in the creative process. We discuss how our findings can inform support for users at different stages of the design process and for using text-to-image models for physical artefact design.", "title": "trash to treasure using texttoimage models to inform the design of physical artefacts", "url": "http://arxiv.org/pdf/2302.00561", "tokenized_text": "text image generative recently popularity accessibility far use creative tasks bridge 2d digital world creation physical artefacts understudied conduct pilot study investigate text image assist upstream tasks creative process ideation visualization prior making activity participants selected making materials generated images stable_diffusion stable diffusion text image generator text choice aim informing creating physical majority participants reported generated images informed reported interest text image help creative task future identify prompt_engineering engineering strategies find participant strategy relates stage creative process discuss findings inform support users different stages design process text image physical design"}
{"id": "5e8dd82419f78025093acbec3ba2e345fff85d11", "abstract": "Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between products or product types, which can be utilized in recommender systems. However, relation labeling in KGs remains a challenging task due to the dynamic nature of e-commerce domains and the associated cost of human labor. Recently, breakthroughs in Large Language Models (LLMs) have shown surprising results in numerous natural language processing tasks. In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data. We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation labeling tasks using just 1 to 5 labeled examples per relation. Additionally, we experiment with different prompt engineering techniques to examine their impact on model performance. Our results show that LLMs significantly outperform existing KG completion models in relation labeling for e-commerce KGs and exhibit performance strong enough to replace human labeling.", "title": "knowledge graph completion models are fewshot learners an empirical study of relation labeling in ecommerce with llms", "url": "http://arxiv.org/pdf/2305.09858", "tokenized_text": "knowledge graphs kgs play crucial role enhancing commerce system performance providing structured information entities relationships complementary relations products product types utilized recommender systems relation labeling kgs remains challenging task dynamic nature commerce domains associated cost human labor recently breakthroughs large_language large language llms shown surprising results numerous natural_language natural language processing tasks paper conduct empirical study llms relation labeling commerce kgs investigating powerful learning capabilities natural_language natural language effectiveness predicting relations product types limited labeled_data labeled data evaluate llms including palm gpt-3.5 benchmark_datasets benchmark datasets demonstrating ability achieve competitive_performance competitive performance compared humans relation labeling tasks labeled examples relation additionally experiment different prompt_engineering engineering techniques examine impact performance results llms significantly outperform existing kg completion relation labeling commerce kgs exhibit performance strong replace human labeling"}
{"id": "615962d8969c8e0ffe43319689dce6c50cbf1f29", "abstract": "The recent success of Large Language Models (LLMs) signifies an impressive stride towards artificial general intelligence. They have shown a promising prospect in automatically completing tasks upon user instructions, functioning as brain-like coordinators. The associated risks will be revealed as we delegate an increasing number of tasks to machines for automated completion. A big question emerges: how can we make machines behave responsibly when helping humans automate tasks as personal copilots? In this paper, we explore this question in depth from the perspectives of feasibility, completeness and security. In specific, we present Responsible Task Automation (ResponsibleTA) as a fundamental framework to facilitate responsible collaboration between LLM-based coordinators and executors for task automation with three empowered capabilities: 1) predicting the feasibility of the commands for executors; 2) verifying the completeness of executors; 3) enhancing the security (e.g., the protection of users' privacy). We further propose and compare two paradigms for implementing the first two capabilities. One is to leverage the generic knowledge of LLMs themselves via prompt engineering while the other is to adopt domain-specific learnable models. Moreover, we introduce a local memory mechanism for achieving the third capability. We evaluate our proposed ResponsibleTA on UI task automation and hope it could bring more attentions to ensuring LLMs more responsible in diverse scenarios. The research project homepage is at https://task-automation-research.github.io/responsible_task_automation.", "title": "responsible task automation empowering large language models as responsible task automators", "url": "http://arxiv.org/pdf/2306.01242", "tokenized_text": "recent success large_language large language llms impressive artificial general intelligence shown promising automatically completing tasks user instructions functioning brain like associated risks revealed delegate increasing number tasks machines automated completion big question emerges machines behave responsibly helping humans automate tasks personal paper explore question depth perspectives feasibility completeness security specific present responsible task automation fundamental framework facilitate responsible collaboration llm based task automation empowered capabilities predicting feasibility commands verifying completeness enhancing security e.g. protection users privacy propose compare paradigms implementing capabilities leverage generic knowledge llms prompt_engineering engineering adopt domain specific learnable introduce local memory mechanism achieving capability evaluate proposed ui task automation hope bring ensuring llms responsible diverse scenarios research project"}
{"id": "615ef4518f9a41a10881b66ce10f0eb490e2d75c", "abstract": "From industrial to space robotics, safe landing is an essential component for flight operations. With the growing interest in artificial intelligence, we direct our attention to learning based safe landing approaches. This paper extends our previous work, DOVESEI, which focused on a reactive UAV system by harnessing the capabilities of open vocabulary image segmentation. Prompt-based safe landing zone segmentation using an open vocabulary based model is no more just an idea, but proven to be feasible by the work of DOVESEI. However, a heuristic selection of words for prompt is not a reliable solution since it cannot take the changing environment into consideration and detrimental consequences can occur if the observed environment is not well represented by the given prompt. Therefore, we introduce PEACE (Prompt Engineering Automation for CLIPSeg Enhancement), powering DOVESEI to automate the prompt generation and engineering to adapt to data distribution shifts. Our system is capable of performing safe landing operations with collision avoidance at altitudes as low as 20 meters using only monocular cameras and image segmentation. We take advantage of DOVESEI's dynamic focus to circumvent abrupt fluctuations in the terrain segmentation between frames in a video stream. PEACE shows promising improvements in prompt generation and engineering for aerial images compared to the standard prompt used for CLIP and CLIPSeg. Combining DOVESEI and PEACE, our system was able improve successful safe landing zone selections by 58.62% compared to using only DOVESEI. All the source code is open source and available online.", "title": "peace prompt engineering automation for clipseg enhancement in aerial robotics", "url": "https://arxiv.org/pdf/2310.00085", "tokenized_text": "industrial space robotics safe essential component flight operations growing interest artificial_intelligence artificial intelligence direct attention learning based safe approaches paper extends previous work focused reactive system harnessing capabilities open vocabulary image segmentation based safe segmentation open vocabulary based idea proven feasible work heuristic selection words reliable solution changing environment consideration detrimental consequences occur observed environment represented given introduce prompt_engineering engineering automation enhancement powering automate generation engineering adapt data distribution shifts system capable performing safe operations collision low 20 image segmentation advantage dynamic focus circumvent abrupt segmentation frames video stream shows promising improvements generation engineering aerial images compared standard clip combining system able improve successful safe selections compared source_code source code open_source open source available online"}
{"id": "632dc69c2e504d693533fc434b8122a2a8a42844", "abstract": "In this paper, we introduce a data-driven approach for Formality-Sensitive Machine Translation (FSMT) that caters to the unique linguistic properties of four target languages. Our methodology centers on two core strategies: 1) language-specific data handling, and 2) synthetic data generation using large-scale language models and empirical prompt engineering. This approach demonstrates a considerable improvement over the baseline, highlighting the effectiveness of data-centric techniques. Our prompt engineering strategy further improves performance by producing superior synthetic translation examples.", "title": "datadriven approach for formalitysensitive machine translation languagespecific handling and synthetic data generation", "url": "http://arxiv.org/pdf/2306.14514", "tokenized_text": "paper introduce data driven approach formality sensitive machine_translation machine translation caters unique linguistic properties target languages methodology centers core strategies language specific data handling synthetic data generation large scale language_models language empirical prompt_engineering engineering approach demonstrates considerable improvement baseline highlighting effectiveness data centric techniques prompt_engineering engineering strategy improves performance producing superior synthetic translation examples"}
{"id": "6474370fe46e38896288305c35d3058a403b1db2", "abstract": "Large language models offer new ways of empowering people to program robot applications-namely, code generation via prompting. However, the code generated by LLMs is susceptible to errors. This work reports a preliminary exploration that empirically characterizes common errors produced by LLMs in robot programming. We categorize these errors into two phases: interpretation and execution. In this work, we focus on errors in execution and observe that they are caused by LLMs being\"forgetful\"of key information provided in user prompts. Based on this observation, we propose prompt engineering tactics designed to reduce errors in execution. We then demonstrate the effectiveness of these tactics with three language models: ChatGPT, Bard, and LLaMA-2. Finally, we discuss lessons learned from using LLMs in robot programming and call for the benchmarking of LLM-powered end-user development of robot applications.", "title": "forgetful large language models lessons learned from using llms in robot programming", "url": "https://arxiv.org/pdf/2310.06646", "tokenized_text": "large_language large language offer new ways empowering people program robot applications code_generation code generation code generated llms susceptible errors work reports preliminary exploration empirically characterizes common errors produced llms robot programming categorize errors phases interpretation execution work focus errors execution observe caused llms key information provided user based observation propose prompt_engineering engineering tactics designed reduce errors execution demonstrate_the_effectiveness demonstrate effectiveness tactics language_models language chatgpt bard llama-2 finally discuss lessons learned llms robot programming benchmarking llm powered end user development robot applications"}
{"id": "661e8ac4908a9d2a85835245ea99b6a314cc4a60", "abstract": "Recent progress in artificial intelligence (AI), particularly in the domain of large language models (LLMs), has resulted in powerful and versatile dual-use systems. Indeed, cognition can be put towards a wide variety of tasks, some of which can result in harm. This study investigates how LLMs can be used for spear phishing, a form of cybercrime that involves manipulating targets into divulging sensitive information. I first explore LLMs' ability to assist with the reconnaissance and message generation stages of a successful spear phishing attack, where I find that advanced LLMs are capable of improving cybercriminals' efficiency during these stages. To explore how LLMs can be used to scale spear phishing campaigns, I then create unique spear phishing messages for over 600 British Members of Parliament using OpenAI's GPT-3.5 and GPT-4 models. My findings reveal that these messages are not only realistic but also cost-effective, with each email costing only a fraction of a cent to generate. Next, I demonstrate how basic prompt engineering can circumvent safeguards installed in LLMs by the reinforcement learning from human feedback fine-tuning process, highlighting the need for more robust governance interventions aimed at preventing misuse. To address these evolving risks, I propose two potential solutions: structured access schemes, such as application programming interfaces, and LLM-based defensive systems.", "title": "large language models can be used to effectively scale spear phishing campaigns", "url": "http://arxiv.org/pdf/2305.06972", "tokenized_text": "recent progress artificial_intelligence artificial intelligence ai particularly domain large_language large language llms resulted powerful versatile dual use systems cognition wide variety tasks result harm study investigates llms form involves manipulating targets sensitive information explore llms ability assist message generation stages successful attack find advanced llms capable improving efficiency stages explore llms scale campaigns create unique messages 600 members openai gpt-3.5 gpt-4 findings reveal messages realistic cost effective email fraction cent generate demonstrate basic prompt_engineering engineering circumvent safeguards llms reinforcement_learning reinforcement learning human feedback fine tuning process highlighting need robust governance interventions aimed preventing misuse address evolving risks propose potential solutions structured access schemes application programming interfaces llm based defensive systems"}
{"id": "6862113724aa1a578c5d4e0ec7f1d9bed4288241", "abstract": "\"Prompt Engineering for Students of Medicine and Their Teachers\"brings the principles of prompt engineering for large language models such as ChatGPT and Google Bard to medical education. This book contains a comprehensive guide to prompt engineering to help both teachers and students improve education in the medical field. Just as prompt engineering is critical in getting good information out of an AI, it is also critical to get students to think and understand more deeply. The principles of prompt engineering that we have learned from AI systems have the potential to simultaneously revolutionize learning in the healthcare field. The book analyzes from multiple angles the anatomy of a good prompt for both AI models and students. The different types of prompts are examined, showing how each style has unique characteristics and applications. The principles of prompt engineering, applied properly, are demonstrated to be effective in teaching across the diverse fields of anatomy, physiology, pathology, pharmacology, and clinical skills. Just like ChatGPT and similar large language AI models, students need clear and detailed prompting in order for them to fully understand a topic. Using identical principles, a prompt that gets good information from an AI will also cause a student to think more deeply and accurately. The process of prompt engineering facilitates this process. Because each chapter contains multiple examples and key takeaways, it is a practical guide for implementing prompt engineering in the learning process. It provides a hands-on approach to ensure readers can immediately apply the concepts they learn", "title": "prompt engineering for students of medicine and their teachers", "url": "https://arxiv.org/pdf/2308.11628", "tokenized_text": "prompt_engineering engineering students medicine principles prompt_engineering engineering large_language large language chatgpt google_bard google bard medical education book contains comprehensive guide prompt_engineering engineering help teachers students improve education medical field prompt_engineering engineering critical getting good information ai critical students think understand deeply principles prompt_engineering engineering learned ai systems potential simultaneously revolutionize learning healthcare field book analyzes multiple good ai students different types examined showing style unique characteristics applications principles prompt_engineering engineering applied properly demonstrated effective teaching diverse fields pathology clinical skills like_chatgpt like chatgpt similar large_language large language ai students need clear detailed order fully understand topic identical principles gets good information ai cause student think deeply accurately process prompt_engineering engineering facilitates process chapter contains multiple examples key practical guide implementing prompt_engineering engineering learning process provides approach ensure readers immediately apply concepts learn"}
{"id": "6b8f26678785ebd7b7b27984af3cb9a273b722b0", "abstract": "We present very early results on using GPT-3 to perform question answering on tabular data. We find that stock pre-trained GPT-3 is able to zero-shot learn the table structure from a serialized JSON array-of-arrays representation, and able to answer lookup queries and simple comparison questions in natural language without any fine-tuning. We further find that simple prompt engineering to include few-shot static Q&A examples significantly improves accuracy. Lastly, we find that intermixing passage text improves accuracy even further on heterogeneous data. We apply our approach on a novel dataset of simple tables in newspaper infographics with promising results. Overall, we find much cause for optimism in this basic approach.", "title": "towards zeroshot and fewshot table question answering using gpt3", "url": "https://arxiv.org/pdf/2210.17284", "tokenized_text": "present early results gpt-3 perform question_answering question answering tabular data find stock pre trained gpt-3 able zero shot learn table structure serialized json array representation able answer queries simple comparison questions natural_language natural language fine tuning find simple prompt_engineering engineering include shot static q&a examples significantly improves accuracy lastly find passage text improves accuracy heterogeneous data apply approach novel dataset simple tables promising_results promising results overall find cause basic approach"}
{"id": "71020779c6eeb9c76fe0a0eb2155d1d4f7d29ff9", "abstract": "Deep object detection models have achieved notable successes in recent years, but one major obstacle remains: the requirement for a large amount of training data. Obtaining such data is a tedious process and is mainly time consuming, leading to the exploration of new research avenues like synthetic data generation techniques. In this study, we explore the usability of Stable Diffusion 2.1-base for generating synthetic datasets of apple trees for object detection and compare it to a baseline model trained on real-world data. After creating a dataset of realistic apple trees with prompt engineering and utilizing a previously trained Stable Diffusion model, the custom dataset was annotated and evaluated by training a YOLOv5m object detection model to predict apples in a real-world apple detection dataset. YOLOv5m was chosen for its rapid inference time and minimal hardware demands. Results demonstrate that the model trained on generated data is slightly underperforming compared to a baseline model trained on real-world images when evaluated on a set of real-world images. However, these findings remain highly promising, as the average precision difference is only 0.09 and 0.06, respectively. Qualitative results indicate that the model can accurately predict the location of apples, except in cases of heavy shading. These findings illustrate the potential of synthetic data generation techniques as a viable alternative to the collection of extensive training data for object detection models.", "title": "exploring the effectiveness of dataset synthesis an application of apple detection in orchards", "url": "http://arxiv.org/pdf/2306.11763", "tokenized_text": "deep object_detection object detection achieved notable successes recent_years recent years major remains requirement large training_data training data obtaining data tedious process mainly time_consuming time consuming leading exploration new research avenues like synthetic data generation techniques study explore usability stable_diffusion stable diffusion base generating synthetic datasets apple trees object_detection object detection compare baseline trained real world data creating dataset realistic apple trees prompt_engineering engineering utilizing previously trained stable_diffusion stable diffusion custom dataset annotated evaluated training object_detection object detection predict real world apple detection dataset chosen rapid inference time minimal hardware demands results_demonstrate results demonstrate trained generated data slightly compared baseline trained real world images evaluated set real world images findings remain highly promising average precision difference respectively qualitative results_indicate results indicate accurately predict location cases heavy findings illustrate potential synthetic data generation techniques viable alternative collection extensive training_data training data object_detection object detection"}
{"id": "732627c703a9dbc78d9384f1be4c791c3a554391", "abstract": "Contrastive vision-language models like CLIP have shown great progress in transfer learning. In the inference stage, the proper text description, also known as prompt, needs to be carefully designed to correctly classify the given images. In order to avoid laborious prompt engineering, recent works such as CoOp, CLIP-Adapter and Tip-Adapter propose to adapt vision-language models for downstream image recognition tasks on a small set of labeled data. Though promising improvements are achieved, requiring labeled data from the target datasets may restrict the scalability. In this paper, we explore a different scenario, in which the labels of the target datasets are unprovided, and we present an unsupervised prompt learning (UPL) approach to avoid prompt engineering while simultaneously improving transfer performance of CLIP-like vision-language models. As far as we know, UPL is the first work to introduce unsupervised learning into prompt learning. Experimentally, our UPL outperforms original CLIP with prompt engineering on ImageNet as well as other 10 datasets. An enhanced version of UPL is even competitive with the 8-shot CoOp and the 8-shot TIP-Adapter on most datasets. Code and models are available at https://github.com/tonyhuang2022/UPL.", "title": "unsupervised prompt learning for visionlanguage models", "url": "http://arxiv.org/pdf/2204.03649", "tokenized_text": "contrastive vision language_models language like clip shown great progress transfer learning inference stage proper text description known needs carefully designed correctly classify given images order avoid laborious prompt_engineering engineering recent works coop clip adapter adapter propose adapt vision language_models language downstream image recognition tasks small set labeled_data labeled data promising improvements achieved requiring labeled_data labeled data target datasets restrict scalability paper explore different scenario labels target datasets present unsupervised learning approach avoid prompt_engineering engineering simultaneously improving transfer performance clip like vision language_models language far know work introduce unsupervised learning learning experimentally outperforms original clip prompt_engineering engineering imagenet 10 datasets enhanced version competitive shot coop shot adapter datasets code available"}
{"id": "7619a98ef077c8f75e0bfb98953457649209e07e", "abstract": "Visual prompt engineering is a fundamental technology in the field of visual and image Artificial General Intelligence, serving as a key component for achieving zero-shot capabilities. As the development of large vision models progresses, the importance of prompt engineering becomes increasingly evident. Designing suitable prompts for specific visual tasks has emerged as a meaningful research direction. This review aims to summarize the methods employed in the computer vision domain for large vision models and visual prompt engineering, exploring the latest advancements in visual prompt engineering. We present influential large models in the visual domain and a range of prompt engineering methods employed on these models. It is our hope that this review provides a comprehensive and systematic description of prompt engineering methods based on large visual models, offering valuable insights for future researchers in their exploration of this field.", "title": "review of large vision models and visual prompt engineering", "url": "http://arxiv.org/pdf/2307.00855", "tokenized_text": "visual prompt_engineering engineering fundamental technology field visual image artificial general intelligence serving key component achieving zero shot capabilities development large vision importance prompt_engineering engineering increasingly evident designing suitable specific visual tasks emerged meaningful research direction review aims summarize methods employed computer_vision computer vision domain large vision visual prompt_engineering engineering exploring latest advancements visual prompt_engineering engineering present influential large visual domain range prompt_engineering engineering methods employed hope review provides comprehensive systematic description prompt_engineering engineering methods based large visual offering valuable insights future researchers exploration field"}
{"id": "76bb8f753c40d66f435015f2776c672b3999d8b5", "abstract": "The quality of text-to-image generation is continuously improving, yet the boundaries of its applicability are still unclear. In particular, refinement of the text input with the objective of achieving better results - commonly called prompt engineering - so far seems to have not been geared towards work with pre-existing texts. We investigate whether text-to-image generation and prompt engineering could be used to generate basic illustrations of popular fairytales. Using Midjourney v4, we engage in action research with a dual aim: to attempt to generate 5 believable illustrations for each of 5 popular fairytales, and to define a prompt engineering process that starts from a pre-existing text and arrives at an illustration of it. We arrive at a tentative 4-stage process: i) initial prompt, ii) composition adjustment, iii) style refinement, and iv) variation selection. We also discuss three reasons why the generation model struggles with certain illustrations: difficulties with counts, bias from stereotypical configurations and inability to depict overly fantastic situations. Our findings are not limited to the specific generation model and are intended to be generalisable to future ones.", "title": "grimm in wonderland prompt engineering with midjourney to illustrate fairytales", "url": "https://arxiv.org/pdf/2302.08961", "tokenized_text": "quality text image_generation image generation continuously improving boundaries applicability unclear particular refinement text input objective achieving better results commonly called prompt_engineering engineering far geared work pre existing texts investigate text image_generation image generation prompt_engineering engineering generate basic popular midjourney engage action research dual aim attempt generate believable popular define prompt_engineering engineering process starts pre existing text illustration arrive stage process initial ii composition adjustment iii style refinement iv variation selection discuss reasons generation struggles certain difficulties counts bias stereotypical configurations inability depict overly fantastic situations findings limited specific generation intended generalisable future ones"}
{"id": "790c247dabe004f022ef9330fb59c36a77bdbbb2", "abstract": "The misuse of real photographs with conflicting image captions in news items is an example of the out-of-context (OOC) misuse of media. In order to detect OOC media, individuals must determine the accuracy of the statement and evaluate whether the triplet (i.e., the image and two captions) relates to the same event. This paper presents a novel learnable approach for detecting OOC media in ICME'23 Grand Challenge on Detecting Cheapfakes. The proposed method is based on the COSMOS structure, which assesses the coherence between an image and captions, as well as between two captions. We enhance the baseline algorithm by incorporating a Large Language Model (LLM), GPT3.5, as a feature extractor. Specifically, we propose an innovative approach to feature extraction utilizing prompt engineering to develop a robust and reliable feature extractor with GPT3.5 model. The proposed method captures the correlation between two captions and effectively integrates this module into the COSMOS baseline model, which allows for a deeper understanding of the relationship between captions. By incorporating this module, we demonstrate the potential for significant improvements in cheap-fakes detection performance. The proposed methodology holds promising implications for various applications such as natural language processing, image captioning, and text-to-image synthesis. Docker for submission is available at https://hub.docker.com/repository/docker/mulns/acmmmcheapfakes.", "title": "cheapfake detection with llm using prompt engineering", "url": "https://arxiv.org/pdf/2306.02776", "tokenized_text": "misuse real photographs image captions news items example context misuse media order detect media individuals determine accuracy statement evaluate triplet i.e. image captions relates event paper_presents paper presents novel learnable approach detecting media grand challenge detecting proposed_method proposed method based structure assesses coherence image captions captions enhance baseline algorithm incorporating large_language large language llm gpt3.5 feature extractor specifically propose innovative approach feature extraction utilizing prompt_engineering engineering develop robust reliable feature extractor gpt3.5 proposed_method proposed method captures correlation captions effectively integrates module baseline allows deeper understanding relationship captions incorporating module demonstrate potential significant improvements cheap detection performance proposed methodology holds promising implications applications natural_language natural language processing image captioning text image synthesis submission available"}
{"id": "7919cb1a1dcf70ed7803c43a71d43dba696ef149", "abstract": "Tools serve as pivotal interfaces that enable humans to understand and reshape the world. With the advent of foundational models, AI systems can utilize tools to expand their capabilities and interact with the world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce language models to utilize tools indiscriminately, as complex problems often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed by further analysis, show that TRICE can make the language model to selectively use tools by decreasing the model's dependency on tools while enhancing the performance. Code and datasets will be available in https://github.com/zjunlp/trice.", "title": "making language models better tool learners with execution feedback", "url": "http://arxiv.org/pdf/2305.13068", "tokenized_text": "tools serve pivotal interfaces enable humans understand world advent foundational ai systems utilize tools expand capabilities interact world existing tool learning methodologies encompassing supervised fine tuning prompt_engineering engineering approaches induce language_models language utilize tools complex problems exceed competencies introducing tools simple tasks readily resolve inadvertently propagate errors enhance performance leads research question teach language_models language use tools meet need propose tool learning execution feedback stage end end framework enables continually learn feedback derived tool execution learning use tools effectively experimental_results experimental results backed analysis language_model language selectively use tools decreasing dependency tools enhancing performance code datasets available"}
{"id": "793eb805800c4af0b06260079e178efa0377b9d7", "abstract": "This study presents an innovative approach to the application of large language models (LLMs) in clinical decision-making, focusing on OpenAI's ChatGPT. Our approach introduces the use of contextual prompts-strategically designed to include task description, feature description, and crucially, integration of domain knowledge-for high-quality binary classification tasks even in data-scarce scenarios. The novelty of our work lies in the utilization of domain knowledge, obtained from high-performing interpretable ML models, and its seamless incorporation into prompt design. By viewing these ML models as medical experts, we extract key insights on feature importance to aid in decision-making processes. This interplay of domain knowledge and AI holds significant promise in creating a more insightful diagnostic tool. Additionally, our research explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT with traditional supervised ML models in different data conditions, we aim to provide insights into the effectiveness of prompt engineering strategies under varied data availability. In essence, this paper bridges the gap between AI and healthcare, proposing a novel methodology for LLMs application in clinical decision support systems. It highlights the transformative potential of effective prompt design, domain knowledge integration, and flexible learning approaches in enhancing automated decision-making.", "title": "chatgpthealthprompt harnessing the power of xai in promptbased healthcare decision support using chatgpt", "url": "https://arxiv.org/pdf/2308.09731", "tokenized_text": "study presents innovative approach application large_language large language llms clinical decision making focusing openai chatgpt approach introduces use contextual strategically designed include task description feature description crucially integration domain knowledge high quality binary classification tasks data scarce scenarios novelty work lies utilization domain knowledge obtained high performing interpretable ml seamless incorporation design viewing ml medical experts extract key insights feature importance aid decision making processes interplay domain knowledge ai holds significant promise creating insightful diagnostic tool additionally research explores dynamics zero shot shot learning based llms comparing performance openai chatgpt traditional supervised ml different data conditions aim provide insights effectiveness prompt_engineering engineering strategies varied data availability essence paper bridges gap ai healthcare proposing novel methodology llms application clinical decision support systems highlights potential effective design domain knowledge integration flexible learning approaches enhancing automated decision making"}
{"id": "7aad760762c4a10dfbc2d3391eb8bdb28c80b236", "abstract": "Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles faced by these solutions within the context of federated LLM.", "title": "federated large language model a position paper", "url": "https://arxiv.org/pdf/2307.08925", "tokenized_text": "large_scale large scale language_models language llm received significant attention found diverse applications domains development encounters challenges real world_scenarios world scenarios challenges arise scarcity public domain data availability need maintain privacy respect private domain data address issues federated learning fl emerged promising technology enables collaborative training shared preserving decentralized data propose concept federated llm comprises key components i.e. federated llm pre training federated llm fine tuning federated llm prompt_engineering engineering component discuss advantage traditional llm training methods propose specific engineering strategies implementation furthermore explore novel challenges introduced integration fl llm analyze existing solutions identify potential obstacles faced solutions context federated llm"}
{"id": "7d3d98707182e0733d8cf5ee763314c60d638f4a", "abstract": "Large Language Models (LLMs) have shown promise in multiple software engineering tasks including code generation, program repair, code summarisation, and test generation. Fault localisation is instrumental in enabling automated debugging and repair of programs and was prominently featured as a highlight during the launch event of ChatGPT-4. Nevertheless, the performance of LLMs compared to state-of-the-art methods, as well as the impact of prompt design and context length on their efficacy, remains unclear. To fill this gap, this paper presents an in-depth investigation into the capability of ChatGPT-3.5 and ChatGPT-4, the two state-of-the-art LLMs, on fault localisation. Using the widely-adopted large-scale Defects4J dataset, we compare the two LLMs with the existing fault localisation techniques. We also investigate the consistency of LLMs in fault localisation, as well as how prompt engineering and the length of code context affect the fault localisation effectiveness. Our findings demonstrate that within function-level context, ChatGPT-4 outperforms all the existing fault localisation methods. Additional error logs can further improve ChatGPT models' localisation accuracy and consistency, with an average 46.9% higher accuracy over the state-of-the-art baseline SmartFL on the Defects4J dataset in terms of TOP-1 metric. However, when the code context of the Defects4J dataset expands to the class-level, ChatGPT-4's performance suffers a significant drop, with 49.9% lower accuracy than SmartFL under TOP-1 metric. These observations indicate that although ChatGPT can effectively localise faults under specific conditions, limitations are evident. Further research is needed to fully harness the potential of LLMs like ChatGPT for practical fault localisation applications.", "title": "large language models in fault localisation", "url": "https://arxiv.org/pdf/2308.15276", "tokenized_text": "large_language large language llms shown promise multiple software engineering tasks including code_generation code generation program repair code summarisation test generation fault instrumental enabling automated debugging repair programs highlight event chatgpt-4 performance llms compared state art methods impact design context length efficacy remains unclear fill gap paper_presents paper presents depth investigation capability chatgpt-4 state art llms fault widely adopted large scale dataset compare llms existing fault techniques investigate consistency llms fault prompt_engineering engineering length code context affect fault effectiveness findings demonstrate function level context chatgpt-4 outperforms existing fault methods additional error logs improve chatgpt accuracy consistency average higher accuracy state art baseline dataset terms top-1 metric code context dataset expands class level chatgpt-4 performance suffers significant drop lower accuracy top-1 metric observations indicate chatgpt effectively faults specific conditions limitations evident research needed fully harness potential llms like_chatgpt like chatgpt practical fault applications"}
{"id": "80785017029cab501fcdb90b98985cd2b36e1fb8", "abstract": "Despite growing interest in using large language models (LLMs) in healthcare, current explorations do not assess the real-world utility and safety of LLMs in clinical settings. Our objective was to determine whether two LLMs can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. Sixty six questions from an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple prompts. 12 physicians assessed the LLM responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. Physician assessments were summarized based on majority vote. For no questions did a majority of physicians deem either LLM response as harmful. For GPT-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. There were 29 responses with no majority on\"Agree\",\"Disagree\", and\"Unable to assess\". For GPT-4, responses to 13 questions were concordant, 15 discordant, and 3 were unable to be assessed. There were 35 responses with no majority. Responses from both LLMs were largely devoid of overt harm, but less than 20% of the responses agreed with an answer from an informatics consultation service, responses contained hallucinated references, and physicians were divided on what constitutes harm. These results suggest that while general purpose LLMs are able to provide safe and credible responses, they often do not meet the specific information need of a given question. A definitive evaluation of the usefulness of LLMs in healthcare settings will likely require additional research on prompt engineering, calibration, and custom-tailoring of general purpose models.", "title": "evaluation of gpt35 and gpt4 for supporting realworld information needs in healthcare delivery", "url": "http://arxiv.org/pdf/2304.13714", "tokenized_text": "despite growing interest large_language large language llms healthcare current explorations assess real world utility safety llms clinical settings objective determine llms serve information needs submitted physicians questions service safe manner questions consult service submitted gpt-3.5 gpt-4 simple 12 physicians assessed llm responses possibility patient harm concordance existing reports service physician assessments summarized based majority vote questions majority physicians llm response harmful gpt-3.5 responses questions consult report 20 unable assessed 29 responses majority assess gpt-4 responses 13 questions 15 unable assessed 35 responses majority responses llms largely devoid harm 20 responses answer service responses contained hallucinated references physicians divided constitutes harm results suggest general_purpose general purpose llms able provide safe responses meet specific information need given question evaluation usefulness llms healthcare settings likely require additional research prompt_engineering engineering calibration custom tailoring general_purpose general purpose"}
{"id": "831fd0c18d10e42330cca36e0c5769762fb419e7", "abstract": "The final frontier for simulation is the accurate representation of complex, real-world social systems. While agent-based modeling (ABM) seeks to study the behavior and interactions of agents within a larger system, it is unable to faithfully capture the full complexity of human-driven behavior. Large language models (LLMs), like ChatGPT, have emerged as a potential solution to this bottleneck by enabling researchers to explore human-driven interactions in previously unimaginable ways. Our research investigates simulations of human interactions using LLMs. Through prompt engineering, inspired by Park et al. (2023), we present two simulations of believable proxies of human behavior: a two-agent negotiation and a six-agent murder mystery game.", "title": "exploring the intersection of large language models and agentbased modeling via prompt engineering", "url": "https://arxiv.org/pdf/2308.07411", "tokenized_text": "final frontier simulation accurate representation complex real world social systems agent based modeling abm seeks study behavior interactions agents larger system unable faithfully capture complexity human driven behavior large_language large language llms like_chatgpt like chatgpt emerged potential solution bottleneck enabling researchers explore human driven interactions previously ways research investigates simulations human interactions llms prompt_engineering engineering inspired et_al et al 2023 present simulations believable proxies human behavior agent negotiation agent game"}
{"id": "877e27a1d89095fcf686ab675f62a8432d3285ee", "abstract": "Contrastively trained text-image models have the remarkable ability to perform zero-shot classification, that is, classifying previously unseen images into categories that the model has never been explicitly trained to identify. However, these zero-shot classifiers need prompt engineering to achieve high accuracy. Prompt engineering typically requires hand-crafting a set of prompts for individual downstream tasks. In this work, we aim to automate this prompt engineering and improve zero-shot accuracy through prompt ensembling. In particular, we ask\"Given a large pool of prompts, can we automatically score the prompts and ensemble those that are most suitable for a particular downstream dataset, without needing access to labeled validation data?\". We demonstrate that this is possible. In doing so, we identify several pathologies in a naive prompt scoring method where the score can be easily overconfident due to biases in pre-training and test data, and we propose a novel prompt scoring method that corrects for the biases. Using our proposed scoring method to create a weighted average prompt ensemble, our method outperforms equal average ensemble, as well as hand-crafted prompts, on ImageNet, 4 of its variants, and 11 fine-grained classification benchmarks, all while being fully automatic, optimization-free, and not requiring access to labeled validation data.", "title": "a simple zeroshot prompt weighting technique to improve prompt ensembling in textimage models", "url": "https://arxiv.org/pdf/2302.06235", "tokenized_text": "trained text image remarkable ability perform zero shot classification classifying previously unseen images categories explicitly trained identify zero shot classifiers need prompt_engineering engineering achieve high accuracy prompt_engineering engineering typically requires hand crafting set individual downstream_tasks downstream tasks work aim automate prompt_engineering engineering improve zero shot accuracy ensembling particular large pool automatically score ensemble suitable particular downstream dataset needing access labeled validation data demonstrate possible identify naive scoring method score easily biases pre training test data propose_a_novel propose novel scoring method biases proposed scoring method create weighted average ensemble method outperforms equal average ensemble hand crafted imagenet variants 11 fine grained classification benchmarks fully automatic optimization free requiring access labeled validation data"}
{"id": "8a1a8290f7d42b0ce60445a4c0130ef737b3ff69", "abstract": "This paper evaluates the extent to which current LLMs can capture task-oriented multi-party conversations (MPCs). We have recorded and transcribed 29 MPCs between patients, their companions, and a social robot in a hospital. We then annotated this corpus for multi-party goal-tracking and intent-slot recognition. People share goals, answer each other\u2019s goals, and provide other people\u2019s goals in MPCs - none of which occur in dyadic interactions. To understand user goals in MPCs, we compared three methods in zero-shot and few-shot settings: we fine-tuned T5, created pre-training tasks to train DialogLM using LED, and employed prompt engineering techniques with GPT-3.5-turbo, to determine which approach can complete this novel task with limited data. GPT-3.5-turbo significantly outperformed the others in a few-shot setting. The \u2018reasoning\u2019 style prompt, when given 7% of the corpus as example annotated conversations, was the best performing method. It correctly annotated 62.32% of the goal tracking MPCs, and 69.57% of the intent-slot recognition MPCs. A \u2018story\u2019 style prompt increased model hallucination, which could be detrimental if deployed in safety-critical settings. We conclude that multi-party conversations still challenge state-of-the-art LLMs.", "title": "multiparty goal tracking with llms comparing pretraining, finetuning, and prompt engineering", "url": "https://arxiv.org/pdf/2308.15231", "tokenized_text": "paper evaluates extent current llms capture task oriented multi party conversations recorded 29 patients social robot hospital annotated corpus multi party goal tracking intent slot recognition people share goals answer goals provide people goals occur dyadic interactions understand user goals compared methods zero shot shot_settings shot settings fine tuned t5 created pre training tasks train led employed prompt_engineering engineering techniques gpt-3.5 turbo determine approach complete novel task limited data gpt-3.5 turbo significantly outperformed shot_setting shot setting reasoning style given corpus example annotated conversations best performing method correctly annotated goal tracking intent slot recognition story style increased hallucination detrimental deployed safety critical settings conclude multi party conversations challenge state art llms"}
{"id": "8a419947c46b8fa491ec613664372e376eb9f0c6", "abstract": "The prevalence of propaganda in our digital society poses a challenge to societal harmony and the dissemination of truth. Detecting propaganda through NLP in text is challenging due to subtle manipulation techniques and contextual dependencies. To address this issue, we investigate the effectiveness of modern Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection. We conduct experiments using the SemEval-2020 task 11 dataset, which features news articles labeled with 14 propaganda techniques as a multi-label classification problem. Five variations of GPT-3 and GPT-4 are employed, incorporating various prompt engineering and fine-tuning strategies across the different models. We evaluate the models' performance by assessing metrics such as $F1$ score, $Precision$, and $Recall$, comparing the results with the current state-of-the-art approach using RoBERTa. Our findings demonstrate that GPT-4 achieves comparable results to the current state-of-the-art. Further, this study analyzes the potential and challenges of LLMs in complex tasks like propaganda detection.", "title": "large language models for propaganda detection", "url": "https://arxiv.org/pdf/2310.06422", "tokenized_text": "prevalence propaganda digital society poses challenge societal harmony dissemination truth detecting propaganda nlp text challenging subtle manipulation techniques contextual dependencies address issue investigate effectiveness modern large_language large language llms gpt-3 gpt-4 propaganda detection conduct experiments task 11 dataset features news articles labeled 14 propaganda techniques multi label classification problem variations gpt-3 gpt-4 employed incorporating prompt_engineering engineering fine tuning strategies different evaluate performance assessing metrics f1 score comparing results current state art approach roberta findings demonstrate gpt-4 achieves comparable results current state art study analyzes potential challenges llms complex tasks like propaganda detection"}
{"id": "8a8ac2467aee4d70866a1b2410e59565ef6ae292", "abstract": "Machine learning and deep learning methods have been widely explored in understanding the chaotic behavior of the atmosphere and furthering weather forecasting. There has been increasing interest from technology companies, government institutions, and meteorological agencies in building digital twins of the Earth. Recent approaches using transformers, physics-informed machine learning, and graph neural networks have demonstrated state-of-the-art performance on relatively narrow spatiotemporal scales and specific tasks. With the recent success of generative artificial intelligence (AI) using pre-trained transformers for language modeling and vision with prompt engineering and fine-tuning, we are now moving towards generalizable AI. In particular, we are witnessing the rise of AI foundation models that can perform competitively on multiple domain-specific downstream tasks. Despite this progress, we are still in the nascent stages of a generalizable AI model for global Earth system models, regional climate models, and mesoscale weather models. Here, we review current state-of-the-art AI approaches, primarily from transformer and operator learning literature in the context of meteorology. We provide our perspective on criteria for success towards a family of foundation models for nowcasting and forecasting weather and climate predictions. We also discuss how such models can perform competitively on downstream tasks such as downscaling (super-resolution), identifying conditions conducive to the occurrence of wildfires, and predicting consequential meteorological phenomena across various spatiotemporal scales such as hurricanes and atmospheric rivers. In particular, we examine current AI methodologies and contend they have matured enough to design and implement a weather foundation model.", "title": "ai foundation models for weather and climate applications, design, and implementation", "url": "https://arxiv.org/pdf/2309.10808", "tokenized_text": "machine_learning machine learning deep learning methods widely explored understanding behavior weather forecasting increasing interest technology companies government institutions building digital twins recent approaches transformers physics informed machine_learning machine learning graph neural_networks neural networks demonstrated state art performance relatively narrow scales specific tasks recent success generative artificial_intelligence artificial intelligence ai pre trained transformers language modeling vision prompt_engineering engineering fine tuning moving generalizable ai particular witnessing rise ai foundation_models foundation perform competitively multiple domain specific downstream_tasks downstream tasks despite progress nascent stages generalizable ai global system regional climate weather review current state art ai approaches primarily transformer operator learning literature context provide perspective criteria success family foundation_models foundation forecasting weather climate predictions discuss perform competitively downstream_tasks downstream tasks super resolution identifying conditions conducive occurrence predicting consequential phenomena scales rivers particular examine current ai methodologies contend design implement weather foundation"}
{"id": "8c52b3bbe5897ba3f42b38c5bfc33bbd48f9a1f2", "abstract": "Large language models (LLMs) are a new and powerful tool for a wide span of applications involving natural language and demonstrate impressive code generation abilities. In this paper, we explore the capabilitity of state-of-the-art LLMs, including closed-source options like OpenAI GPT-4 and open-source alternatives like Meta AI Codellama, to automatically generate tests and use these tests to validate and verify compiler implementations of a directive-based programming paradigm, OpenACC. Our approach entails exploring various prompt engineering techniques including a code template, retrieval-augmented generation (RAG) with code template, expressive prompt using RAG with code template, one-shot example, and RAG with one-shot example. This paper focusses on (a) exploring the capabilities of the latest LLMs for code generation, (b) investigating prompt and fine tuning methods, and (c) analyzing the outcome of LLMs generated tests", "title": "llm4vv developing llmdriven testsuite for compiler validation", "url": "https://arxiv.org/pdf/2310.04963", "tokenized_text": "large_language large language llms new powerful tool wide span applications involving natural_language natural language demonstrate impressive code_generation code generation abilities paper explore state art llms including closed source options like openai gpt-4 open source alternatives like meta ai automatically generate tests use tests validate verify compiler implementations based programming paradigm approach entails exploring prompt_engineering engineering techniques including code template retrieval augmented generation rag code template expressive rag code template shot example rag shot example paper exploring capabilities latest llms code_generation code generation investigating fine_tuning fine tuning methods analyzing outcome llms generated tests"}
{"id": "8c90bfe05c06fd47eaec0f5b1662e06862572afe", "abstract": "The growing capability and availability of generative language models has enabled a wide range of new downstream tasks. Academic research has identified, quantified and mitigated biases present in language models but is rarely tailored to downstream tasks where wider impact on individuals and society can be felt. In this work, we leverage one popular generative language model, GPT-3, with the goal of writing unbiased and realistic job advertisements. We first assess the bias and realism of zero-shot generated advertisements and compare them to real-world advertisements. We then evaluate prompt-engineering and fine-tuning as debiasing methods. We find that prompt-engineering with diversity-encouraging prompts gives no significant improvement to bias, nor realism. Conversely, fine-tuning, especially on unbiased real advertisements, can improve realism and reduce bias.", "title": "looking for a handsome carpenter! debiasing gpt3 job advertisements", "url": "https://arxiv.org/pdf/2205.11374", "tokenized_text": "growing capability availability generative language_models language enabled wide_range wide range new downstream_tasks downstream tasks academic research identified quantified mitigated biases present language_models language rarely tailored downstream_tasks downstream tasks wider impact individuals society felt work leverage popular generative language_model language gpt-3 goal writing realistic job assess bias realism zero shot generated compare real world evaluate engineering fine tuning methods find engineering diversity encouraging gives significant improvement bias realism conversely fine tuning especially real improve realism reduce bias"}
{"id": "8ca384547bb4b21b7f38d478119bf3168eb9c9cd", "abstract": "We present VOICE, a novel approach for connecting large language models' (LLM) conversational capabilities with interactive exploratory visualization. VOICE introduces several innovative technical contributions that drive our conversational visualization framework. Our foundation is a pack-of-bots that can perform specific tasks, such as assigning tasks, extracting instructions, and generating coherent content. We employ fine-tuning and prompt engineering techniques to tailor bots' performance to their specific roles and accurately respond to user queries, and a new prompt-based iterative scene-tree generation establishes a coupling with a structural model. Our text-to-visualization method generates a flythrough sequence matching the content explanation. Finally, 3D natural language interaction provides capabilities to navigate and manipulate the 3D models in real-time. The VOICE framework can receive arbitrary voice commands from the user and responds verbally, tightly coupled with corresponding visual representation with low latency and high accuracy. We demonstrate the effectiveness and high generalizability potential of our approach by applying it to two distinct domains: analyzing three 3D molecular models with multi-scale and multi-instance attributes, and showcasing its effectiveness on a cartographic map visualization. A free copy of this paper and all supplemental materials are available at https://osf.io/g7fbr/.", "title": "voice visual oracle for interaction, conversation, and explanation", "url": "http://arxiv.org/pdf/2304.04083", "tokenized_text": "present voice novel_approach novel approach connecting large_language large language llm conversational capabilities interactive exploratory visualization voice introduces innovative technical contributions drive conversational visualization framework foundation bots perform specific tasks assigning tasks extracting instructions generating coherent content employ fine tuning prompt_engineering engineering techniques tailor bots performance specific roles accurately respond user queries new based iterative scene tree generation establishes coupling structural text visualization method generates sequence matching content explanation finally 3d natural_language natural language interaction provides capabilities navigate manipulate 3d real time voice framework receive arbitrary voice commands user responds coupled corresponding visual representation low latency high accuracy demonstrate_the_effectiveness demonstrate effectiveness high generalizability potential approach applying distinct domains analyzing 3d molecular multi scale multi instance attributes showcasing effectiveness map visualization free copy paper materials available"}
{"id": "8efcdc15c5f028f968d6a004a64593245c49927b", "abstract": "Recently, large language models (LLMs), particularly GPT-4, have demonstrated significant capabilities in various planning and reasoning tasks \\cite{cheng2023gpt4,bubeck2023sparks}. Motivated by these advancements, there has been a surge of interest among researchers to harness the capabilities of GPT-4 for the automated design of quantitative factors that do not overlap with existing factor libraries, with an aspiration to achieve alpha returns \\cite{webpagequant}. In contrast to these work, this study aims to examine the fidelity of GPT-4's comprehension of classic trading theories and its proficiency in applying its code interpreter abilities to real-world trading data analysis. Such an exploration is instrumental in discerning whether the underlying logic GPT-4 employs for trading is intrinsically reliable. Furthermore, given the acknowledged interpretative latitude inherent in most trading theories, we seek to distill more precise methodologies of deploying these theories from GPT-4's analytical process, potentially offering invaluable insights to human traders. To achieve this objective, we selected daily candlestick (K-line) data from specific periods for certain assets, such as the Shanghai Stock Index. Through meticulous prompt engineering, we guided GPT-4 to analyze the technical structures embedded within this data, based on specific theories like the Elliott Wave Theory. We then subjected its analytical output to manual evaluation, assessing its interpretative depth and accuracy vis-\\`a-vis these trading theories from multiple dimensions. The results and findings from this study could pave the way for a synergistic amalgamation of human expertise and AI-driven insights in the realm of trading.", "title": "is gpt4 a good trader", "url": "https://arxiv.org/pdf/2309.10982", "tokenized_text": "recently large_language large language llms particularly gpt-4 demonstrated significant capabilities planning reasoning tasks motivated advancements surge interest researchers harness capabilities gpt-4 automated design quantitative factors overlap existing factor libraries achieve alpha returns contrast work study aims examine fidelity gpt-4 comprehension classic trading theories proficiency applying code interpreter abilities real world trading data analysis exploration instrumental discerning underlying logic gpt-4 employs trading reliable furthermore given acknowledged inherent trading theories seek distill precise methodologies deploying theories gpt-4 analytical process potentially offering insights human achieve objective selected daily line data specific periods certain assets stock index prompt_engineering engineering guided gpt-4 analyze technical structures embedded data based specific theories like wave theory analytical output manual evaluation assessing depth accuracy trading theories multiple dimensions results findings study pave way synergistic human expertise ai driven insights realm trading"}
{"id": "8f93f95e093aab16e594b4a246a205007e107c7a", "abstract": "The Right to be Forgotten (RTBF) was first established as the result of the ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\\'alez, and was later included as the Right to Erasure under the General Data Protection Regulation (GDPR) of European Union to allow individuals the right to request personal data be deleted by organizations. Specifically for search engines, individuals can send requests to organizations to exclude their information from the query results. It was a significant emergent right as the result of the evolution of technology. With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF. In this paper, we explore these challenges and provide our insights on how to implement technical solutions for the RTBF, including the use of differential privacy, machine unlearning, model editing, and prompt engineering. With the rapid advancement of AI and the increasing need of regulating this powerful technology, learning from the case of RTBF can provide valuable lessons for technical practitioners, legal experts, organizations, and authorities.", "title": "right to be forgotten in the era of large language models implications, challenges, and solutions", "url": "https://arxiv.org/pdf/2307.03941", "tokenized_text": "right established result google sl google later included right general data protection regulation gdpr european allow individuals right request personal data organizations specifically search engines individuals send requests organizations information query results significant emergent right result evolution technology recent development large_language large language llms use chatbots llm enabled software systems popular excluded compared indexing approach search engines llms store process information completely different way poses new challenges compliance paper explore challenges provide insights implement technical solutions including use differential privacy machine editing prompt_engineering engineering rapid advancement ai increasing need powerful technology learning case provide valuable lessons technical practitioners legal experts organizations"}
{"id": "90b1baf2cf299ef0e0ef7611a12311bd6cab3ed7", "abstract": "We introduce CONA, a novel context-aware instruction paradigm for effective knowledge dissemination using generative pre-trained transformer (GPT) models. CONA is a flexible framework designed to leverage the capabilities of Large Language Models (LLMs) and incorporate DIKW (Data, Information, Knowledge, Wisdom) hierarchy to automatically instruct and optimise presentation content, anticipate potential audience inquiries, and provide context-aware answers that adaptive to the knowledge level of the audience group. The unique aspect of the CONA paradigm lies in its combination of an independent advisory mechanism and a recursive feedback loop rooted on the DIKW hierarchy. This synergy significantly enhances context-aware contents, ensuring they are accessible and easily comprehended by the audience. This paradigm is an early pioneer to explore new methods for knowledge dissemination and communication in the LLM era, offering effective support for everyday knowledge sharing scenarios. We conduct experiments on a range of audience roles, along with materials from various disciplines using GPT4. Both quantitative and qualitative results demonstrated that the proposed CONA paradigm achieved remarkable performance compared to the outputs guided by conventional prompt engineering.", "title": "cona a novel contextaware instruction paradigm for communication using large language model", "url": "http://arxiv.org/pdf/2305.18620", "tokenized_text": "introduce novel context aware instruction paradigm effective knowledge dissemination generative pre trained transformer gpt flexible framework designed leverage capabilities large_language large language llms incorporate data information knowledge wisdom hierarchy automatically instruct optimise presentation content anticipate potential audience provide context aware answers adaptive knowledge level audience group unique aspect paradigm lies combination independent mechanism recursive feedback loop rooted hierarchy synergy significantly enhances context aware contents ensuring accessible easily audience paradigm early explore new methods knowledge dissemination communication llm era offering effective support everyday knowledge sharing scenarios conduct experiments range audience roles materials disciplines gpt4 quantitative qualitative results demonstrated proposed paradigm achieved remarkable performance compared outputs guided conventional prompt_engineering engineering"}
{"id": "91a291780103b328f65e700896ae6fa2230ec2e7", "abstract": "Recently, to improve the unsupervised image retrieval performance, plenty of unsupervised hashing methods have been proposed by designing a semantic similarity matrix, which is based on the similarities between image features extracted by a pre-trained CNN model. However, most of these methods tend to ignore high-level abstract semantic concepts contained in images. Intuitively, concepts play an important role in calculating the similarity among images. In real-world scenarios, each image is associated with some concepts, and the similarity between two images will be larger if they share more identical concepts. Inspired by the above intuition, in this work, we propose a novel Unsupervised Hashing with Semantic Concept Mining, called UHSCM, which leverages a VLP model to construct a high-quality similarity matrix. Specifically, a set of randomly chosen concepts is first collected. Then, by employing a vision-language pretraining (VLP) model with the prompt engineering which has shown strong power in visual representation learning, the set of concepts is denoised according to the training images. Next, the proposed method UHSCM applies the VLP model with prompting again to mine the concept distribution of each image and construct a high-quality semantic similarity matrix based on the mined concept distributions. Finally, with the semantic similarity matrix as guiding information, a novel hashing loss with a modified contrastive loss based regularization item is proposed to optimize the hashing network. Extensive experiments on three benchmark datasets show that the proposed method outperforms the state-of-the-art baselines in the image retrieval task.", "title": "unsupervised hashing with semantic concept mining", "url": "https://arxiv.org/pdf/2209.11475", "tokenized_text": "recently improve unsupervised image retrieval performance plenty unsupervised methods proposed designing semantic similarity matrix based similarities image features extracted pre trained cnn methods tend ignore high level abstract semantic concepts contained images intuitively concepts play important role calculating similarity images real world_scenarios world scenarios image associated concepts similarity images larger share identical concepts inspired intuition work propose_a_novel propose novel unsupervised semantic concept mining called leverages vlp construct high quality similarity matrix specifically set randomly chosen concepts collected employing vision language pretraining vlp prompt_engineering engineering shown strong power visual representation learning set concepts according training images proposed_method proposed method applies vlp concept distribution image construct high quality semantic similarity matrix based mined concept distributions finally semantic similarity matrix guiding information novel loss modified contrastive loss based regularization item proposed optimize network extensive_experiments extensive experiments benchmark_datasets benchmark datasets proposed_method proposed method outperforms state art baselines image retrieval task"}
{"id": "975da5bb7fdd800ba577535d8c6ee5a5bc835d52", "abstract": "Without writing a single line of code by a human, an example Monte Carlo simulation based application for stochastic dependence modeling with copulas is developed using a state-of-the-art large language model (LLM) fine-tuned for conversations. This includes interaction with ChatGPT in natural language and using mathematical formalism, which, under careful supervision by a human-expert, led to producing a working code in MATLAB, Python and R for sampling from a given copula model, evaluation of the model's density, performing maximum likelihood estimation, optimizing the code for parallel computing for CPUs as well as for GPUs, and visualization of the computed results. In contrast to other emerging studies that assess the accuracy of LLMs like ChatGPT on tasks from a selected area, this work rather investigates ways how to achieve a successful solution of a standard statistical task in a collaboration of a human-expert and artificial intelligence (AI). Particularly, through careful prompt engineering, we separate successful solutions generated by ChatGPT from unsuccessful ones, resulting in a comprehensive list of related pros and cons. It is demonstrated that if the typical pitfalls are avoided, we can substantially benefit from collaborating with an AI partner. For example, we show that if ChatGPT is not able to provide a correct solution due to a lack of or incorrect knowledge, the human-expert can feed it with the correct knowledge, e.g., in the form of mathematical theorems and formulas, and make it to apply the gained knowledge in order to provide a solution that is correct. Such ability presents an attractive opportunity to achieve a programmed solution even for users with rather limited knowledge of programming techniques.", "title": "pair programming with large language models for sampling and estimation of copulas", "url": "http://arxiv.org/pdf/2303.18116", "tokenized_text": "writing single line code human example monte_carlo monte carlo simulation based application stochastic dependence modeling developed state art large_language large language llm fine tuned conversations includes interaction chatgpt natural_language natural language mathematical formalism careful supervision human expert led producing working code python sampling given evaluation density performing maximum likelihood estimation optimizing code parallel computing visualization results contrast emerging studies assess accuracy llms like_chatgpt like chatgpt tasks selected area work investigates ways achieve successful solution standard statistical task collaboration human expert artificial_intelligence artificial intelligence ai particularly careful prompt_engineering engineering separate successful solutions generated chatgpt unsuccessful ones resulting comprehensive list related pros cons demonstrated typical pitfalls avoided substantially benefit ai example chatgpt able provide correct solution lack incorrect knowledge human expert feed correct knowledge e.g. form mathematical theorems formulas apply gained knowledge order provide solution correct ability presents attractive opportunity achieve programmed solution users limited knowledge programming techniques"}
{"id": "97717368b5f7e6a544f0a1c73a441bdcb4b6a046", "abstract": "The field of large language models (LLMs) has made significant progress, and their knowledge storage capacity is approaching that of human beings. Furthermore, advanced techniques, such as prompt learning and reinforcement learning, are being employed to address ethical concerns and hallucination problems associated with LLMs, bringing them closer to aligning with human values. This situation naturally raises the question of whether LLMs with human-like abilities possess a human-like personality? In this paper, we aim to investigate the feasibility of using the Myers-Briggs Type Indicator (MBTI), a widespread human personality assessment tool, as an evaluation metric for LLMs. Specifically, extensive experiments will be conducted to explore: 1) the personality types of different LLMs, 2) the possibility of changing the personality types by prompt engineering, and 3) How does the training dataset affect the model's personality. Although the MBTI is not a rigorous assessment, it can still reflect the similarity between LLMs and human personality. In practice, the MBTI has the potential to serve as a rough indicator. Our codes are available at https://github.com/HarderThenHarder/transformers_tasks/tree/main/LLM/llms_mbti.", "title": "do llms possess a personality making the mbti test an amazing evaluation for large language models", "url": "https://arxiv.org/pdf/2307.16180", "tokenized_text": "field large_language large language llms significant progress knowledge storage capacity approaching human beings furthermore advanced techniques learning reinforcement_learning reinforcement learning employed address ethical concerns hallucination problems associated llms bringing closer aligning human values naturally raises question llms human like abilities possess human like personality paper aim investigate feasibility type indicator widespread human personality assessment tool evaluation metric llms specifically extensive_experiments extensive experiments conducted explore personality types different llms possibility changing personality types prompt_engineering engineering training dataset affect personality rigorous assessment reflect similarity llms human personality practice potential serve rough indicator codes available"}
{"id": "994a6040fab375669a92cab0e67fb2fd203cd67f", "abstract": "Rare diseases (RDs) are collectively common and affect 300 million people worldwide. Accurate phenotyping is critical for informing diagnosis and treatment, but RD phenotypes are often embedded in unstructured text and time-consuming to extract manually. While natural language processing (NLP) models can perform named entity recognition (NER) to automate extraction, a major bottleneck is the development of a large, annotated corpus for model training. Recently, prompt learning emerged as an NLP paradigm that can lead to more generalizable results without any (zero-shot) or few labeled samples (few-shot). Despite growing interest in ChatGPT, a revolutionary large language model capable of following complex human prompts and generating high-quality responses, none have studied its NER performance for RDs in the zero- and few-shot settings. To this end, we engineered novel prompts aimed at extracting RD phenotypes and, to the best of our knowledge, are the first the establish a benchmark for evaluating ChatGPT's performance in these settings. We compared its performance to the traditional fine-tuning approach and conducted an in-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted in higher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the zero- and few-shot settings, respectively). Despite this, ChatGPT achieved similar or higher accuracy for certain entities (i.e., rare diseases and signs) in the one-shot setting (F1 of 0.776 and 0.725). This suggests that with appropriate prompt engineering, ChatGPT has the potential to match or outperform fine-tuned language models for certain entity types with just one labeled sample. While the proliferation of large language models may provide opportunities for supporting RD diagnosis and treatment, researchers and clinicians should critically evaluate model outputs and be well-informed of their limitations.", "title": "identifying and extracting rare disease phenotypes with large language models", "url": "http://arxiv.org/pdf/2306.12656", "tokenized_text": "rare diseases collectively common affect million people worldwide accurate phenotyping critical informing diagnosis treatment phenotypes embedded unstructured text time consuming extract manually natural_language natural language processing nlp perform named_entity named entity recognition ner automate extraction major bottleneck development large annotated corpus training recently learning emerged nlp paradigm lead generalizable results zero shot labeled samples shot despite growing interest chatgpt revolutionary large_language large language capable following complex human generating high quality responses studied ner performance zero- shot_settings shot settings end engineered novel aimed extracting phenotypes best knowledge establish benchmark evaluating chatgpt performance settings compared performance traditional fine tuning approach conducted depth error analysis overall fine tuning resulted higher performance f1 chatgpt f1 zero- shot_settings shot settings respectively despite chatgpt achieved similar higher accuracy certain entities i.e. rare diseases shot_setting shot setting f1 suggests appropriate prompt_engineering engineering chatgpt potential match outperform fine tuned language_models language certain entity types labeled sample proliferation large_language large language provide opportunities supporting diagnosis treatment researchers clinicians evaluate outputs informed limitations"}
{"id": "05fab50acb26203a944a955131a2388c9731a8f7", "abstract": "Early detection of power outages is crucial for maintaining a reliable power distribution system. This research investigates the use of transfer learning and language models in detecting outages with limited labeled data. By leveraging pretraining and transfer learning, models can generalize to unseen classes. Using a curated balanced dataset of social media tweets related to power outages, we conducted experiments using zero-shot and few-shot learning. Our hypothesis is that Language Models pretrained with limited data could achieve high performance in outage detection tasks over baseline models. Results show that while classical models outperform zero-shot Language Models, few-shot fine-tuning significantly improves their performance. For example, with 10% fine-tuning, BERT achieves 81.3% accuracy (+15.3%), and GPT achieves 74.5% accuracy (+8.5%). This has practical implications for analyzing and localizing outages in scenarios with limited data availability. Our evaluation provides insights into the potential of few-shot fine-tuning with Language Models for power outage detection, highlighting their strengths and limitations. This research contributes to the knowledge base of leveraging advanced natural language processing techniques for managing critical infrastructure.", "title": "transfer learning for power outage detection task with limited training data", "url": "http://arxiv.org/pdf/2305.17817", "tokenized_text": "early detection power crucial maintaining reliable power distribution system research investigates use transfer learning language_models language detecting limited labeled_data labeled data leveraging pretraining transfer learning generalize unseen classes curated balanced dataset social_media social media tweets related power conducted experiments zero shot shot_learning shot learning hypothesis language_models language pretrained limited data achieve high performance detection tasks baseline results classical outperform zero shot language_models language shot fine tuning significantly improves performance example 10 fine tuning bert achieves accuracy gpt achieves accuracy practical implications analyzing localizing scenarios limited data availability evaluation provides insights potential shot fine tuning language_models language power detection highlighting strengths limitations research contributes knowledge base leveraging advanced natural_language natural language processing techniques managing critical infrastructure"}
{"id": "0704a96e1c57c12031f1c3ca492a91dbed1f61ce", "abstract": "Driven by encouraging results on a wide range of tasks, the field of NLP is experiencing an accelerated race to develop bigger language models. This race for bigger models has also underscored the need to continue the pursuit of practical distillation approaches that can leverage the knowledge acquired by these big models in a compute-efficient manner. Having this goal in mind, we build on recent work to propose a hallucination-free framework for sequence tagging that is especially suited for distillation. We show empirical results of new state-of-the-art performance across multiple sequence labelling datasets and validate the usefulness of this framework for distilling a large model in a few-shot learning scenario.", "title": "distillation of encoderdecoder transformers for sequence labelling", "url": "http://arxiv.org/pdf/2302.05454", "tokenized_text": "driven encouraging results wide_range wide range tasks field nlp experiencing accelerated race develop bigger language_models language race bigger need continue pursuit practical distillation approaches leverage knowledge acquired big compute efficient manner having goal mind build recent_work recent work propose hallucination free framework sequence tagging especially suited distillation empirical results new state art performance multiple sequence labelling datasets validate usefulness framework distilling large shot_learning shot learning scenario"}
{"id": "075e16a0774b1a9d44a7d512c50b7f997e16befe", "abstract": "Prompt tuning recently becomes a hot-spot in the applications of large pretrained language models on specific downstream tasks. Regarding the Language Model as a Service (LMaaS), black-box tuning using derivative-free optimization (DFO) provides a novel approach to expand the practical scenarios of pretrained models and enrich the researches of few-shot learning. In this report, we present our solution in this competition that is based on the LMaaS scenario. Our solution consists of several modifications to BBTv2, including multiple label words, selection of P0, rolling update strategy, multi-task loss from MLP classifier, and finally using the ensemble method to further improve generalization ability. We also shared some strategies that we tried but didn't use in the final submission for further discussion. In the end we raised a question about the SNLI dataset and the impact on the results, as well as our concerns about the competition.", "title": "technical report competition solution for prompt tuning using pretrained language model", "url": "http://arxiv.org/pdf/2212.06369", "tokenized_text": "tuning recently hot applications large pretrained_language pretrained language specific downstream_tasks downstream tasks language_model language service black box tuning derivative free optimization provides novel_approach novel approach expand practical scenarios pretrained enrich researches shot_learning shot learning report present solution competition based scenario solution consists modifications bbtv2 including multiple label words selection rolling update strategy multi task loss mlp classifier finally ensemble method improve generalization_ability generalization ability shared strategies tried use final submission discussion end raised question snli dataset impact results concerns competition"}
{"id": "0783c214623c18f6a8ad96b8eaf4a67a382e68ee", "abstract": "The availability of large, high-quality datasets has been one of the main drivers of recent progress in question answering (QA). Such annotated datasets however are difficult and costly to collect, and rarely exist in languages other than English, rendering QA technology inaccessible to underrepresented languages. An alternative to building large monolingual training datasets is to leverage pre-trained language models (PLMs) under a few-shot learning setting. Our approach, QAmeleon, uses a PLM to automatically generate multilingual data upon which QA models are trained, thus avoiding costly annotation. Prompt tuning the PLM for data synthesis with only five examples per language delivers accuracy superior to translation-based baselines, bridges nearly 60% of the gap between an English-only baseline and a fully supervised upper bound trained on almost 50,000 hand labeled examples, and always leads to substantial improvements compared to fine-tuning a QA model directly on labeled examples in low resource settings. Experiments on the TyDiQA-GoldP and MLQA benchmarks show that few-shot prompt tuning for data synthesis scales across languages and is a viable alternative to large-scale annotation.", "title": "qameleon multilingual qa with only 5 examples", "url": "https://arxiv.org/pdf/2211.08264", "tokenized_text": "availability large high quality datasets main drivers recent progress question_answering question answering qa annotated datasets difficult costly collect rarely exist languages english rendering qa technology inaccessible underrepresented languages alternative building large monolingual training datasets leverage pre trained_language trained language plms shot_learning shot learning setting approach uses plm automatically generate multilingual data qa trained avoiding costly annotation tuning plm data synthesis examples language accuracy superior translation based baselines bridges nearly 60 gap english baseline fully_supervised fully supervised upper bound trained hand labeled examples leads substantial improvements compared fine tuning qa directly labeled examples low_resource low resource settings experiments benchmarks shot tuning data synthesis scales languages viable alternative large scale annotation"}
{"id": "07bc02bd16f6fe78a7ea3bb8d966fcc6e3893195", "abstract": "In-context learning, which offers substantial advantages over fine-tuning, is predominantly observed in decoder-only models, while encoder-decoder (i.e., seq2seq) models excel in methods that rely on weight updates. Recently, a few studies have demonstrated the feasibility of few-shot learning with seq2seq models; however, this has been limited to tasks that align well with the seq2seq architecture, such as summarization and translation. Inspired by these initial studies, we provide a first-ever extensive experiment comparing the in-context few-shot learning capabilities of decoder-only and encoder-decoder models on a broad range of tasks. Furthermore, we propose two methods to more effectively elicit in-context learning ability in seq2seq models: objective-aligned prompting and a fusion-based approach. Remarkably, our approach outperforms a decoder-only model that is six times larger and exhibits significant performance improvements compared to conventional seq2seq models across a variety of settings. We posit that, with the right configuration and prompt design, seq2seq models can be highly effective few-shot learners for a wide spectrum of applications.", "title": "exploiting the potential of seq2seq models as robust fewshot learners", "url": "https://arxiv.org/pdf/2307.14856", "tokenized_text": "context_learning context learning offers substantial advantages fine tuning predominantly observed decoder encoder decoder i.e. seq2seq excel methods rely weight updates recently studies demonstrated feasibility shot_learning shot learning seq2seq limited tasks align seq2seq architecture summarization translation inspired initial studies provide extensive experiment comparing context shot_learning shot learning capabilities decoder encoder decoder broad range tasks furthermore propose methods effectively elicit context_learning context learning ability seq2seq objective aligned fusion based approach remarkably approach outperforms decoder times larger exhibits significant performance improvements compared conventional seq2seq variety settings posit right configuration design seq2seq highly effective shot learners wide spectrum applications"}
{"id": "089f6328085066263fedc083952624ca121ebbf3", "abstract": "Participant recruitment based on unstructured medical texts such as clinical notes and radiology reports has been a challenging yet important task for the cohort establishment in clinical research. Recently, Large Language Models (LLMs) such as ChatGPT have achieved tremendous success in various downstream tasks thanks to their promising performance in language understanding, inference, and generation. It is then natural to test their feasibility in solving the cohort recruitment task, which involves the classification of a given paragraph of medical text into disease label(s). However, when applied to knowledge-intensive problem settings such as medical text classification, where the LLMs are expected to understand the decision made by human experts and accurately identify the implied disease labels, the LLMs show a mediocre performance. A possible explanation is that, by only using the medical text, the LLMs neglect to use the rich context of additional information that languages afford. To this end, we propose to use a knowledge graph as auxiliary information to guide the LLMs in making predictions. Moreover, to further boost the LLMs adapt to the problem setting, we apply a chain-of-thought (CoT) sample selection strategy enhanced by reinforcement learning, which selects a set of CoT samples given each individual medical report. Experimental results and various ablation studies show that our few-shot learning method achieves satisfactory performance compared with fine-tuning strategies and gains superb advantages when the available data is limited. The code and sample dataset of the proposed CohortGPT model is available at: https://anonymous.4open.science/r/CohortGPT-4872/", "title": "cohortgpt an enhanced gpt for participant recruitment in clinical study", "url": "https://arxiv.org/pdf/2307.11346", "tokenized_text": "participant recruitment based unstructured medical texts clinical notes radiology reports challenging important task cohort clinical research recently large_language large language llms chatgpt achieved tremendous success downstream_tasks downstream tasks thanks promising performance language understanding inference generation natural test feasibility solving cohort recruitment task involves classification given paragraph medical text disease applied knowledge intensive problem settings medical text_classification text classification llms expected understand decision human experts accurately identify implied disease labels llms mediocre performance possible explanation medical text llms neglect use rich context additional information languages afford end propose use knowledge_graph knowledge graph auxiliary information guide llms making predictions boost llms adapt problem setting apply chain thought cot sample selection strategy enhanced reinforcement_learning reinforcement learning selects set cot samples given individual medical report experimental_results experimental results ablation studies shot_learning shot learning method_achieves method achieves satisfactory performance compared fine tuning strategies gains superb advantages available data limited code sample dataset proposed available"}
{"id": "0942bd8fad71282994ff4e9a779c09745da68edc", "abstract": "Although large language models can be prompted for both zero- and few-shot learning, performance drops significantly when no demonstrations are available. In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap by constructing pseudo-demonstrations for a given test input using a raw text corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the nearest neighbors to the test input from the corpus and pairing them with random task labels, and (2) applying a set of techniques to reduce the amount of direct copying the model does from the resulting demonstrations. Evaluation on nine classification datasets shows that Z-ICL outperforms previous zero-shot methods by a significant margin, and is on par with in-context learning with labeled training data in the few-shot setting. Overall, Z-ICL provides a significantly higher estimate of the zero-shot performance levels of a model, and supports future efforts to develop better pseudo-demonstrations that further improve zero-shot results.", "title": "zicl zeroshot incontext learning with pseudodemonstrations", "url": "http://arxiv.org/pdf/2212.09865", "tokenized_text": "large_language large language prompted zero- shot_learning shot learning performance drops significantly demonstrations available paper introduce icl new zero shot method closes gap constructing pseudo demonstrations given test input raw text corpus concretely pseudo demonstrations constructed finding nearest neighbors test input corpus pairing random task labels applying set techniques reduce direct copying resulting demonstrations evaluation classification datasets shows icl outperforms previous zero shot methods significant margin par context_learning context learning labeled training_data training data shot_setting shot setting overall icl provides significantly higher estimate zero shot performance levels supports future efforts develop better pseudo demonstrations improve zero shot results"}
{"id": "0953ada119f384f328b6102e6b7963b3bde7cc9e", "abstract": "Lung cancer is the leading cause of cancer-related death worldwide. Lung adenocarcinoma (LUAD) and lung squamous cell carcinoma (LUSC) are the most common histologic subtypes of non-small-cell lung cancer (NSCLC). Histology is an essential tool for lung cancer diagnosis. Pathologists make classifications according to the dominant subtypes. Although morphology remains the standard for diagnosis, significant tool needs to be developed to elucidate the diagnosis. In our study, we utilize the pre-trained Vision Transformer (ViT) model to classify multiple label lung cancer on histologic slices (from dataset LC25000), in both Zero-Shot and Few-Shot settings. Then we compare the performance of Zero-Shot and Few-Shot ViT on accuracy, precision, recall, sensitivity and specificity. Our study show that the pre-trained ViT model has a good performance in Zero-Shot setting, a competitive accuracy ($99.87\\%$) in Few-Shot setting ({epoch = 1}) and an optimal result ($100.00\\%$ on both validation set and test set) in Few-Shot seeting ({epoch = 5}).", "title": "zeroshot and fewshot learning for lung cancer multilabel classification using vision transformer", "url": "https://arxiv.org/pdf/2205.15290", "tokenized_text": "cancer leading cause cancer related worldwide cell common non small cell cancer essential tool cancer diagnosis classifications according dominant morphology remains standard diagnosis significant tool needs developed elucidate diagnosis study utilize pre trained vision_transformer vision transformer vit classify multiple label cancer slices dataset zero shot shot_settings shot settings compare performance zero shot shot vit accuracy precision recall sensitivity specificity study pre trained vit good performance zero shot_setting shot setting competitive accuracy shot_setting shot setting epoch optimal result validation set test set shot epoch"}
{"id": "09b7338021fff3200c2098b19824aecc83a66cb5", "abstract": "Prompt tuning, a parameter- and data-efficient transfer learning paradigm that tunes only a small number of parameters in a model's input space, has become a trend in the vision community since the emergence of large vision-language models like CLIP. We present a systematic study on two representative prompt tuning methods, namely text prompt tuning and visual prompt tuning. A major finding is that none of the unimodal prompt tuning methods performs consistently well: text prompt tuning fails on data with high intra-class visual variances while visual prompt tuning cannot handle low inter-class variances. To combine the best from both worlds, we propose a simple approach called Unified Prompt Tuning (UPT), which essentially learns a tiny neural network to jointly optimize prompts across different modalities. Extensive experiments on over 11 vision datasets show that UPT achieves a better trade-off than the unimodal counterparts on few-shot learning benchmarks, as well as on domain generalization benchmarks. Code and models will be released to facilitate future research.", "title": "unified vision and language prompt learning", "url": "http://arxiv.org/pdf/2210.07225", "tokenized_text": "tuning data efficient transfer learning paradigm tunes small_number small number parameters input space trend vision community emergence large vision language_models language like clip present systematic study representative tuning methods text tuning visual tuning major finding unimodal tuning methods performs consistently text tuning fails data high intra class visual variances visual tuning handle low inter class variances combine best worlds propose simple approach called unified tuning upt essentially learns tiny neural network jointly optimize different modalities extensive_experiments extensive experiments 11 vision datasets upt achieves better trade unimodal counterparts shot_learning shot learning benchmarks domain generalization benchmarks code released facilitate future_research future research"}
{"id": "0b413633f14ec7f96948067abf1d4ca930fa38a1", "abstract": "As technology advances and digital devices become prevalent, seamless human-machine communication is increasingly gaining significance. The growing adoption of mobile, wearable, and other Internet of Things (IoT) devices has changed how we interact with these smart devices, making accurate spoken words recognition a crucial component for effective interaction. However, building robust spoken words detection system that can handle novel keywords remains challenging, especially for low-resource languages with limited training data. Here, we propose PLiX, a multilingual and plug-and-play keyword spotting system that leverages few-shot learning to harness massive real-world data and enable the recognition of unseen spoken words at test-time. Our few-shot deep models are learned with millions of one-second audio clips across 20 languages, achieving state-of-the-art performance while being highly efficient. Extensive evaluations show that PLiX can generalize to novel spoken words given as few as just one support example and performs well on unseen languages out of the box. We release models and inference code to serve as a foundation for future research and voice-enabled user interface development for emerging devices.", "title": "plugandplay multilingual fewshot spoken words recognition", "url": "http://arxiv.org/pdf/2305.03058", "tokenized_text": "technology advances digital devices prevalent seamless human machine communication increasingly gaining significance growing adoption mobile wearable internet things devices changed interact smart devices making accurate spoken words recognition crucial component effective interaction building robust spoken words detection system handle novel keywords remains challenging especially low resource_languages resource languages limited training_data training data propose multilingual plug play keyword spotting system leverages shot_learning shot learning harness massive real world data enable recognition unseen spoken words test time shot deep learned millions second audio clips 20 languages achieving state art performance highly efficient extensive evaluations generalize novel spoken words given support example performs unseen languages box release inference code serve foundation future_research future research voice enabled user interface development emerging devices"}
{"id": "0b71af0bf02ab58b8d8e342c1c803322cfede603", "abstract": "Recent studies have demonstrated that natural-language prompts can help to leverage the knowledge learned by pre-trained language models for the binary sentence-level sentiment classification task. Specifically, these methods utilize few-shot learning settings to fine-tune the sentiment classification model using manual or automatically generated prompts. However, the performance of these methods is sensitive to the perturbations of the utilized prompts. Furthermore, these methods depend on a few labeled instances for automatic prompt generation and prompt ranking. This study aims to find high-quality prompts for the given task in a zero-shot setting. Given a base prompt, our proposed approach automatically generates multiple prompts similar to the base prompt employing positional, reasoning, and paraphrasing techniques and then ranks the prompts using a novel metric. We empirically demonstrate that the top-ranked prompts are high-quality and significantly outperform the base prompt and the prompts generated using few-shot learning for the binary sentence-level sentiment classification task.", "title": "zeroshot approach to overcome perturbation sensitivity of prompts", "url": "http://arxiv.org/pdf/2305.15689", "tokenized_text": "recent studies demonstrated natural language help leverage knowledge learned pre trained_language trained language binary sentence level sentiment classification task specifically methods utilize shot_learning shot learning settings fine tune sentiment classification manual automatically generated performance methods sensitive perturbations utilized furthermore methods depend labeled instances automatic generation ranking study aims find high quality given task zero shot_setting shot setting given base proposed approach automatically generates multiple similar base employing positional reasoning paraphrasing techniques ranks novel metric empirically demonstrate ranked high quality significantly outperform base generated shot_learning shot learning binary sentence level sentiment classification task"}
{"id": "0c75cda2bb0812217bf0e5460e910212ad512944", "abstract": "Objective: Clinical deep phenotyping plays a critical role in both the diagnosis of patients with rare disorders as well as in building care coordination plans. The process relies on modelling and curating patient profiles using ontology concepts, usually from the Human Phenotype Ontology. Machine learning methods have been widely adopted to support this phenotype concept recognition task. With the significant shift in the use of large language models (LLMs) for most NLP tasks, herewithin, we examine the performance of the latest Generative Pre-trained Transformer (GPT) models underpinning ChatGPT in clinical deep phenotyping. Materials and Methods: The experimental setup of the study included seven prompts of various levels of specificity, two GPT models (gpt-3.5 and gpt-4.0) and an established gold standard for phenotype recognition. Results: Our results show that, currently, these models have not yet achieved state of the art performance. The best run, using few-shots learning, achieved 0.41 F1 score, compared to a 0.62 F1 score achieved by the current best in class tool. Conclusion: The non-deterministic nature of the outcomes and the lack of concordance between different runs using the same prompt and input makes the use of these LLMs in clinical settings problematic.", "title": "an evaluation of gpt models for phenotype concept recognition", "url": "https://arxiv.org/pdf/2309.17169", "tokenized_text": "objective clinical deep phenotyping plays critical role diagnosis patients rare disorders building care coordination plans process relies modelling curating patient profiles ontology concepts usually human phenotype ontology machine_learning machine learning methods widely adopted support phenotype concept recognition task significant shift use large_language large language llms nlp_tasks nlp tasks examine performance latest generative_pre generative pre trained transformer gpt underpinning chatgpt clinical deep phenotyping materials methods experimental setup study included seven levels specificity gpt gpt-3.5 gpt-4.0 established gold standard phenotype recognition results results currently achieved state_of_the_art state art performance best run shots learning achieved f1_score f1 score compared 0.62 f1_score f1 score achieved current best class tool conclusion non deterministic nature outcomes lack concordance different runs input makes use llms clinical settings problematic"}
{"id": "0d09c569477457c32637f9e866727cc4623b1165", "abstract": "In-context learning has become an important approach for few-shot learning in Large Language Models because of its ability to rapidly adapt to new tasks without fine-tuning model parameters. However, it is restricted to applications in natural language and inapplicable to other domains. In this paper, we adapt the concepts underpinning in-context learning to develop a new algorithm for few-shot molecular property prediction. Our approach learns to predict molecular properties from a context of (molecule, property measurement) pairs and rapidly adapts to new properties without fine-tuning. On the FS-Mol and BACE molecular property prediction benchmarks, we find this method surpasses the performance of recent meta-learning algorithms at small support sizes and is competitive with the best methods at large support sizes.", "title": "incontext learning for fewshot molecular property prediction", "url": "https://arxiv.org/pdf/2310.08863", "tokenized_text": "context_learning context learning important approach shot_learning shot learning large_language large language ability rapidly adapt new tasks fine tuning parameters restricted applications natural_language natural language domains paper adapt concepts underpinning context_learning context learning develop new algorithm shot molecular property prediction approach learns predict molecular properties context molecule property measurement pairs rapidly adapts new properties fine tuning fs molecular property prediction benchmarks find method surpasses performance recent meta learning algorithms small support sizes competitive best methods large support sizes"}
{"id": "104c878d17a179e86ba094b221993cfdd3277943", "abstract": "Existing research on Domain Robustness (DR) suffers from disparate setups, lack of evaluation task variety, and reliance on challenge sets. In this paper, we pose a fundamental question: What is the state of affairs of the DR challenge in the era of Large Language Models (LLMs)? To this end, we construct a DR benchmark comprising diverse NLP tasks, including sentence and token-level classification, QA, and generation, each task consists of several domains. We explore the DR challenge of fine-tuned and few-shot learning models in natural domain shift settings and devise two diagnostic metrics of Out-of-Distribution (OOD) performance degradation: The commonly used Source Drop (SD) and the overlooked Target Drop (TD). Our findings reveal important insights: First, despite their capabilities, zero-to-few shot LLMs and fine-tuning approaches still fail to meet satisfactory performance in the OOD context; Second, TD approximates better than SD the average OOD degradation; Third, in a significant proportion of domain shifts, either SD or TD is positive, but not both, and therefore disregarding one can lead to incorrect DR conclusions.", "title": "measuring the robustness of natural language processing models to domain shifts", "url": "http://arxiv.org/pdf/2306.00168", "tokenized_text": "existing research domain robustness dr suffers disparate setups lack evaluation task variety reliance challenge sets paper pose fundamental question state dr challenge era large_language large language llms end construct dr benchmark comprising diverse nlp_tasks nlp tasks including sentence token level classification qa generation task consists domains explore dr challenge fine tuned shot_learning shot learning natural domain shift settings devise diagnostic metrics distribution ood performance degradation commonly source drop sd overlooked target drop findings reveal important insights despite capabilities zero shot llms fine tuning approaches fail meet satisfactory performance ood context second better sd average ood degradation significant proportion domain shifts sd positive disregarding lead incorrect dr conclusions"}
{"id": "10717aefce06cc41465619ec8c956f4b0b0fa6e1", "abstract": "Transformer-based pre-trained models have emerged as the predominant solution for natural language processing (NLP). Fine-tuning such pre-trained models for downstream tasks often requires a considerable amount of labeled private data. In practice, private data is often distributed across heterogeneous mobile devices and may be prohibited from being uploaded. Moreover, well-curated labeled data is often scarce, presenting an additional challenge. To address these challenges, we first introduce a data generator for federated few-shot learning tasks, which encompasses the quantity and skewness of scarce labeled data in a realistic setting. Subsequently, we propose AUG-FedPrompt, a prompt-based federated learning system that exploits abundant unlabeled data for data augmentation. Our experiments indicate that AUG-FedPrompt can perform on par with full-set fine-tuning with a limited amount of labeled data. However, such competitive performance comes at a significant system cost.", "title": "towards practical fewshot federated nlp", "url": "https://arxiv.org/pdf/2212.00192", "tokenized_text": "transformer based pre trained emerged solution natural_language natural language processing nlp fine tuning pre trained downstream_tasks downstream tasks requires considerable labeled private data practice private data distributed heterogeneous mobile devices prohibited curated labeled_data labeled data scarce presenting additional challenge address challenges introduce data generator federated shot_learning shot learning tasks encompasses quantity scarce labeled_data labeled data realistic setting subsequently propose based federated learning system exploits abundant unlabeled data data_augmentation data augmentation experiments indicate perform par set fine tuning limited labeled_data labeled data competitive_performance competitive performance comes significant system cost"}
{"id": "1dd344ce28f1e5a078f9d8396b5078388e555d99", "abstract": "Prompt-based methods have been successfully applied in sentence-level few-shot learning tasks, mostly owing to the sophisticated design of templates and label words. However, when applied to token-level labeling tasks such as NER, it would be time-consuming to enumerate the template queries over all potential entity spans. In this work, we propose a more elegant method to reformulate NER tasks as LM problems without any templates. Specifically, we discard the template construction process while maintaining the word prediction paradigm of pre-training models to predict a class-related pivot word (or label word) at the entity position. Meanwhile, we also explore principled ways to automatically search for appropriate label words that the pre-trained models can easily adapt to. While avoiding the complicated template-based process, the proposed LM objective also reduces the gap between different objectives used in pre-training and fine-tuning, thus it can better benefit the few-shot performance. Experimental results demonstrate the effectiveness of the proposed method over bert-tagger and template-based method under few-shot settings. Moreover, the decoding speed of the proposed method is up to 1930.12 times faster than the template-based method.", "title": "templatefree prompt tuning for fewshot ner", "url": "https://aclanthology.org/2022.naacl-main.420.pdf", "tokenized_text": "based methods successfully applied sentence level shot_learning shot learning tasks owing sophisticated design templates label words applied token level labeling tasks ner time consuming enumerate template queries potential entity spans work propose method reformulate ner tasks lm problems templates specifically discard template construction process maintaining word prediction paradigm pre training predict class related word label word entity position explore principled ways automatically search appropriate label words pre trained easily adapt avoiding complicated template based process proposed lm objective reduces gap different objectives pre training fine tuning better benefit shot performance experimental_results experimental results demonstrate_the_effectiveness demonstrate effectiveness proposed_method proposed method bert template based method shot_settings shot settings decoding speed proposed_method proposed method times faster template based method"}
{"id": "21e46f11898748778a31b5b2fe2f60128eb66ba1", "abstract": "We tackle the Dialogue Belief State Tracking(DST) problem of task-oriented conversational systems. Recent approaches to this problem leveraging Transformer-based models have yielded great results. However, training these models is expensive, both in terms of computational resources and time. Additionally, collecting high quality annotated dialogue datasets remains a challenge for researchers because of the extensive annotation required for training these models. Driven by the recent success of pre-trained language models and prompt-based learning, we explore prompt-based few-shot learning for Dialogue Belief State Tracking. We formulate the DST problem as a 2-stage prompt-based language modelling task and train language models for both tasks and present a comprehensive empirical analysis of their separate and joint performance. We demonstrate the potential of prompt-based methods in few-shot learning for DST and provide directions for future improvement.", "title": "a study on promptbased fewshot learning methods for belief state tracking in taskoriented dialog systems", "url": "http://arxiv.org/pdf/2204.08167", "tokenized_text": "tackle dialogue belief state problem task oriented conversational systems recent approaches problem leveraging transformer based yielded great results training expensive terms computational resources time additionally collecting high_quality high quality annotated dialogue datasets remains challenge researchers extensive annotation required training driven recent success pre trained_language trained language based learning explore based shot_learning shot learning dialogue belief state tracking formulate dst problem stage based language modelling task train language_models language tasks present comprehensive empirical analysis separate joint performance demonstrate potential based methods shot_learning shot learning dst provide directions future improvement"}
{"id": "59ef1b67c5f238d5d6d175d84fb6b239b4221a97", "abstract": "Prompt-based methods with large pre-trained language models (PLMs) have shown impressive unaided performance across many NLP tasks. These models improve even further with the addition of a few labeled in-context exemplars to guide output generation. However, for more complex tasks such as dialogue state tracking (DST), designing prompts that reliably convey the desired intent is nontrivial, leading to unstable results. Furthermore, building in-context exemplars for dialogue tasks is difficult because conversational contexts are long while model input lengths are relatively short.To overcome these issues we first adapt a meta-learning scheme to the dialogue domain which stabilizes the ability of the model to perform well under various prompts. We additionally design a novel training method to improve upon vanilla retrieval mechanisms to find ideal in-context examples. Finally, we introduce a saliency model to limit dialogue text length, allowing us to include more exemplars per query. In effect, we are able to achieve highly competitive results for few-shot DST on MultiWOZ.", "title": "stabilized incontext learning with pretrained language models for few shot dialogue state tracking", "url": "http://arxiv.org/pdf/2302.05932", "tokenized_text": "based methods large pre trained_language trained language plms shown_impressive shown impressive performance nlp_tasks nlp tasks improve addition labeled context exemplars guide output generation complex tasks dialogue state tracking dst designing reliably convey desired intent nontrivial leading unstable results furthermore building context exemplars dialogue tasks difficult conversational contexts long input lengths relatively short overcome issues adapt meta learning scheme dialogue domain ability perform additionally design novel training method improve vanilla retrieval mechanisms find ideal context_examples context examples finally introduce saliency limit dialogue text length allowing include exemplars query effect able achieve highly competitive results shot dst multiwoz"}
{"id": "632ab7663e6d64578ceda1d1df9ec525b503bacb", "abstract": "Large language models trained for code generation can be applied to speaking virtual worlds into existence (creating virtual worlds). In this work we show that prompt-based methods can both accelerate in-VR level editing, as well as can become part of gameplay rather than just part of game development. As an example, we present Codex VR Pong which shows non-deterministic game mechanics using generative processes to not only create static content but also non-trivial interactions between 3D objects. This demonstration naturally leads to an integral discussion on how one would evaluate and benchmark experiences created by generative models - as there are no qualitative or quantitative metrics that apply in these scenarios. We conclude by discussing impending challenges of AI-assisted co-creation in VR.", "title": "steps towards promptbased creation of virtual worlds", "url": "https://arxiv.org/pdf/2211.05875", "tokenized_text": "large_language large language trained code_generation code generation applied virtual worlds existence creating virtual worlds work based methods accelerate vr level editing gameplay game development example present codex vr shows non deterministic game mechanics generative processes create static content non trivial interactions 3d objects demonstration naturally leads integral discussion evaluate benchmark experiences created generative qualitative quantitative metrics apply scenarios conclude discussing challenges ai assisted co creation vr"}
{"id": "7db7653c581d7823cb9c328f2d742ec70d7a0ce4", "abstract": "The remarkable capabilities of large language models have been accompanied by a persistent drawback: the generation of false and unsubstantiated claims commonly known as\"hallucinations\". To combat this issue, recent research has introduced approaches that involve editing and attributing the outputs of language models, particularly through prompt-based editing. However, the inference cost and speed of using large language models for editing currently bottleneck prompt-based methods. These bottlenecks motivate the training of compact editors, which is challenging due to the scarcity of training data for this purpose. To overcome these challenges, we exploit the power of large language models to introduce corruptions (i.e., noise) into text and subsequently fine-tune compact editors to denoise the corruptions by incorporating relevant evidence. Our methodology is entirely unsupervised and provides us with faux hallucinations for training in any domain. Our Petite Unsupervised Research and Revision model, PURR, not only improves attribution over existing editing methods based on fine-tuning and prompting, but also achieves faster execution times by orders of magnitude.", "title": "purr efficiently editing language model hallucinations by denoising language model corruptions", "url": "http://arxiv.org/pdf/2305.14908", "tokenized_text": "remarkable_capabilities remarkable capabilities large_language large language accompanied persistent drawback generation false claims commonly known combat issue recent research introduced approaches involve editing outputs language_models language particularly based editing inference cost speed large_language large language editing currently bottleneck based methods bottlenecks motivate training compact challenging scarcity training_data training data purpose overcome challenges exploit power large_language large language introduce i.e. noise text subsequently fine tune compact incorporating relevant evidence methodology entirely unsupervised provides hallucinations training domain unsupervised research revision improves attribution existing editing methods based fine tuning achieves faster execution times orders magnitude"}
{"id": "80c0416048614be75362c2c332d22dd1d2b22f65", "abstract": "Domain adaptation is an important challenge for neural machine translation. However, the traditional fine-tuning solution requires multiple extra training and yields a high cost. In this paper, we propose a non-tuning paradigm, resolving domain adaptation with a prompt-based method. Specifically, we construct a bilingual phrase-level database and retrieve relevant pairs from it as a prompt for the input sentences. By utilizing Retrieved Phrase-level Prompts (RePP), we effectively boost the translation quality. Experiments show that our method improves domain-specific machine translation for 6.2 BLEU scores and improves translation constraints for 11.5% accuracy without additional training.", "title": "zeroshot domain adaptation for neural machine translation with retrieved phraselevel prompts", "url": "http://arxiv.org/pdf/2209.11409", "tokenized_text": "domain adaptation important challenge neural machine_translation machine translation traditional fine tuning solution requires multiple extra training yields high cost paper propose non tuning paradigm resolving domain adaptation based method specifically construct bilingual phrase level database retrieve relevant pairs input sentences utilizing retrieved phrase level effectively boost translation quality experiments method improves domain specific machine_translation machine translation 6.2 bleu scores improves translation constraints accuracy additional training"}
{"id": "9ecf603dbebbfbdd9858d21903c77074d12518b4", "abstract": "In Weak Supervised Learning (WSL), a model is trained over noisy labels obtained from semantic rules and task-specific pre-trained models. Rules offer limited generalization over tasks and require significant manual efforts while pre-trained models are available only for limited tasks. In this work, we propose to utilize prompt-based methods as weak sources to obtain the noisy labels on unannotated data. We show that task-agnostic prompts are generalizable and can be used to obtain noisy labels for different Spoken Language Understanding (SLU) tasks such as sentiment classification, disfluency detection and emotion classification. These prompts could additionally be updated to add task-specific contexts, thus providing flexibility to design task-specific prompts. We demonstrate that prompt-based methods generate reliable labels for the above SLU tasks and thus can be used as a universal weak source to train a weak-supervised model (WSM) in absence of labeled data. Our proposed WSL pipeline trained over prompt-based weak source outperforms other competitive low-resource benchmarks on zero and few-shot learning by more than 4% on Macro-F1 on all of the three benchmark SLU datasets. The proposed method also outperforms a conventional rule based WSL pipeline by more than 5% on Macro-F1.", "title": "low resource pipeline for spoken language understanding via weak supervision", "url": "https://arxiv.org/pdf/2206.10559", "tokenized_text": "weak supervised learning trained noisy labels obtained semantic rules task specific pre trained rules offer limited generalization tasks require significant manual efforts pre trained available limited tasks work propose utilize based methods weak sources obtain noisy labels unannotated data task agnostic generalizable obtain noisy labels different spoken language understanding slu tasks sentiment classification detection emotion classification additionally updated add task specific contexts providing flexibility design task specific demonstrate based methods generate reliable labels slu tasks universal weak source train weak supervised absence labeled_data labeled data proposed pipeline trained based weak source outperforms competitive low resource benchmarks zero shot_learning shot learning macro f1 benchmark slu datasets proposed_method proposed method outperforms conventional rule based pipeline macro f1"}
{"id": "a07701abd506f67368cb75ef2b649dd51df7abd4", "abstract": "Continual Learning aims to learn a single model on a sequence of tasks without having access to data from previous tasks. The biggest challenge in the domain still remains catastrophic forgetting: a loss in performance on seen classes of earlier tasks. Some existing methods rely on an expensive replay buffer to store a chunk of data from previous tasks. This, while promising, becomes expensive when the number of tasks becomes large or data can not be stored for privacy reasons. As an alternative, prompt-based methods have been proposed that store the task information in a learnable prompt pool. This prompt pool instructs a frozen image encoder on how to solve each task. While the model faces a disjoint set of classes in each task in this setting, we argue that these classes can be encoded to the same embedding space of a pre-trained language encoder. In this work, we propose Language Guidance for Prompt-based Continual Learning (LGCL) as a plug-in for prompt-based methods. LGCL is model agnostic and introduces language guidance at the task level in the prompt pool and at the class level on the output feature of the vision encoder. We show with extensive experimentation that LGCL consistently improves the performance of prompt-based continual learning methods to set a new state-of-the art. LGCL achieves these performance improvements without needing any additional learnable parameters.", "title": "introducing language guidance in promptbased continual learning", "url": "https://arxiv.org/pdf/2308.15827", "tokenized_text": "continual learning aims learn single sequence tasks having access data previous tasks biggest challenge domain remains catastrophic forgetting loss performance seen classes earlier tasks existing_methods existing methods rely expensive replay buffer store chunk data previous tasks promising expensive number tasks large data stored privacy reasons alternative based methods proposed store task information learnable pool pool instructs frozen image encoder solve task faces disjoint set classes task setting argue classes encoded embedding space pre trained_language trained language encoder work propose language guidance based continual learning plug based methods agnostic introduces language guidance task level pool class level output feature vision encoder extensive experimentation consistently improves performance based continual learning methods set new state art achieves performance improvements needing additional learnable parameters"}
{"id": "a29a0e679e626e8961dc217081eae2a6c63a15ad", "abstract": "Recently, prompt-based methods have achieved significant performance in few-shot learning scenarios by bridging the gap between language model pre-training and fine-tuning for downstream tasks. However, existing prompt templates are mostly designed for sentence-level tasks and are inappropriate for sequence labeling objectives. To address the above issue, we propose a multi-task instruction-based generative framework, named InstructionNER, for low-resource named entity recognition. Specifically, we reformulate the NER task as a generation problem, which enriches source sentences with task-specific instructions and answer options, then inferences the entities and types in natural language. We further propose two auxiliary tasks, including entity extraction and entity typing, which enable the model to capture more boundary information of entities and deepen the understanding of entity type semantics, respectively. Experimental results show that our method consistently outperforms other baselines on five datasets in few-shot settings.", "title": "instructionner a multitask instructionbased generative framework for fewshot ner", "url": "http://arxiv.org/pdf/2203.03903", "tokenized_text": "recently based methods achieved significant performance shot_learning shot learning scenarios bridging gap language_model language pre training fine tuning downstream_tasks downstream tasks existing prompt_templates templates designed sentence level tasks inappropriate sequence labeling objectives address issue propose multi task instruction based generative framework named low resource named_entity named entity recognition specifically reformulate ner task generation problem source sentences task specific instructions answer options inferences entities types natural_language natural language propose auxiliary tasks including entity extraction entity typing enable capture boundary information entities deepen understanding entity type semantics respectively experimental_results experimental results method consistently_outperforms consistently outperforms baselines datasets shot_settings shot settings"}
{"id": "a45bdbbf9a197a21ef97291c60b77de47bc51db2", "abstract": "Prompt tuning has been an extremely effective tool to adapt a pre-trained model to downstream tasks. However, standard prompt-based methods mainly consider the case of sufficient data of downstream tasks. It is still unclear whether the advantage can be transferred to the few-shot regime, where only limited data are available for each downstream task. Although some works have demonstrated the potential of prompt-tuning under the few-shot setting, the main stream methods via searching discrete prompts or tuning soft prompts with limited data are still very challenging. Through extensive empirical studies, we find that there is still a gap between prompt tuning and fully fine-tuning for few-shot learning. To bridge the gap, we propose a new prompt-tuning framework, called Soft Template Tuning (STT) 1. STT combines manual and auto prompts, and treats down-stream classification tasks as a masked language modeling task. Comprehensive evaluation on different settings suggests STT can close the gap between fine-tuning and prompt-based methods without introducing additional parameters. Significantly, it can even outperform the time- and resource-consuming fine-tuning method on sentiment classification tasks.", "title": "stt soft template tuning for fewshot adaptation", "url": "https://arxiv.org/pdf/2207.08408", "tokenized_text": "tuning extremely effective tool adapt pre trained downstream_tasks downstream tasks standard based methods mainly consider case sufficient data downstream_tasks downstream tasks unclear advantage transferred shot regime limited data available downstream task works demonstrated potential tuning shot_setting shot setting main stream methods searching discrete tuning soft limited data challenging extensive empirical studies find gap tuning fully fine tuning shot_learning shot learning bridge gap propose_a_new propose new tuning framework called soft template tuning combines manual auto treats stream classification tasks masked language modeling task comprehensive evaluation different settings suggests close gap fine tuning based methods introducing additional parameters significantly outperform time- resource consuming fine tuning method sentiment classification tasks"}
{"id": "a81470aa3721f6cd8a61139f9c4c60923bee093f", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have been proposed to enhance the performance of LLMs. There has been a growing focus on enabling LLMs to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. Recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. However, those methods usually require explicitly and thoroughly written rubrics as inputs to LLMs. It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpful and less harmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework that implicitly learns the improvement goal from human preference data. PIT only requires preference data that are used to train reward models without extra human efforts. Specifically, we reformulate the training objective of reinforcement learning from human feedback (RLHF) -- instead of maximizing response quality for a given input, we maximize the quality gap of the response conditioned on a reference response. In this way, PIT is implicitly trained with the improvement goal of better aligning with human preferences. Experiments on two real-world datasets and one synthetic dataset show that our method significantly outperforms prompting-based methods.", "title": "enable language models to implicitly learn selfimprovement from data", "url": "https://arxiv.org/pdf/2310.00898", "tokenized_text": "large_language large language llms demonstrated_remarkable demonstrated remarkable capabilities open ended text generation tasks inherent open ended nature tasks implies room improvement quality responses address challenge approaches proposed enhance performance llms growing focus enabling llms self improve response quality reducing reliance extensive human annotation efforts collecting diverse high quality training_data training data recently based methods widely explored self improvement methods owing effectiveness efficiency convenience methods usually require explicitly thoroughly written inputs llms expensive challenging manually derive provide necessary real world complex goal improvement e.g. helpful harmful end propose implicit self improvement framework implicitly learns improvement goal human preference data requires preference data train reward extra human efforts specifically reformulate training objective reinforcement_learning reinforcement learning human feedback rlhf instead maximizing response quality given input maximize quality gap response conditioned reference response way implicitly trained improvement goal better aligning human preferences experiments real world datasets synthetic dataset method significantly_outperforms significantly outperforms based methods"}
{"id": "ab346a9d9a71bc59671e52cae96eabba16c24eeb", "abstract": "Prompt learning has been designed as an alternative to fine-tuning for adapting Vision-language (V-L) models to the downstream tasks. Previous works mainly focus on text prompt while visual prompt works are limited for V-L models. The existing visual prompt methods endure either mediocre performance or unstable training process, indicating the difficulty of visual prompt learning. In this paper, we propose a new Progressive Visual Prompt (ProVP) structure to strengthen the interactions among prompts of different layers. More importantly, our ProVP could effectively propagate the image embeddings to deep layers and behave partially similar to an instance adaptive prompt method. To alleviate generalization deterioration, we further propose a new contrastive feature re-formation, which prevents the serious deviation of the prompted visual feature from the fixed CLIP visual feature distribution. Combining both, our method (ProVP-Ref) is evaluated on 11 image benchmark datasets and achieves 7/11 state-of-theart results on both few-shot and base-to-novel settings. To the best of our knowledge, we are the first to demonstrate the superior performance of visual prompts in V-L models to previous prompt-based methods in downstream tasks. Meanwhile, it implies that our ProVP-Ref shows the best capability to adapt and to generalize.", "title": "progressive visual prompt learning with contrastive feature reformation", "url": "http://arxiv.org/pdf/2304.08386", "tokenized_text": "learning designed alternative fine tuning adapting vision language downstream_tasks downstream tasks previous works mainly focus text visual works limited existing visual methods mediocre performance unstable training process indicating difficulty visual learning paper propose_a_new propose new progressive visual structure strengthen interactions different layers importantly effectively propagate image embeddings deep layers behave partially similar instance adaptive method alleviate generalization deterioration propose_a_new propose new contrastive feature formation prevents deviation prompted visual feature fixed clip visual feature distribution combining method evaluated 11 image benchmark_datasets benchmark datasets achieves state results shot base novel settings best knowledge demonstrate superior_performance superior performance visual previous based methods downstream_tasks downstream tasks implies shows best capability adapt generalize"}
{"id": "ac7e270fcd365c84b29a710d58bf1243e850df4c", "abstract": "Few-shot event detection (ED) has been widely studied, while this brings noticeable discrepancies, e.g., various motivations, tasks, and experimental settings, that hinder the understanding of models for future progress.This paper presents a thorough empirical study, a unified view of ED models, and a better unified baseline. For fair evaluation, we compare 12 representative methods on three datasets, which are roughly grouped into prompt-based and prototype-based models for detailed analysis. Experiments consistently demonstrate that prompt-based methods, including ChatGPT, still significantly trail prototype-based methods in terms of overall performance. To investigate their superior performance, we break down their design elements along several dimensions and build a unified framework on prototype-based methods. Under such unified view, each prototype-method can be viewed a combination of different modules from these design elements. We further combine all advantageous modules and propose a simple yet effective baseline, which outperforms existing methods by a large margin (e.g., 2.7% F1 gains under low-resource setting).", "title": "fewshot event detection an empirical study and a unified view", "url": "http://arxiv.org/pdf/2305.01901", "tokenized_text": "shot event detection ed widely studied brings noticeable discrepancies e.g. motivations tasks experimental settings hinder understanding future progress paper_presents paper presents thorough empirical study unified view ed better unified baseline fair evaluation compare 12 representative methods datasets roughly based prototype based detailed analysis experiments consistently demonstrate based methods including chatgpt significantly trail prototype based methods terms overall performance investigate superior_performance superior performance break design elements dimensions build unified framework prototype based methods unified view prototype method viewed combination different modules design elements combine modules propose simple effective baseline outperforms existing_methods existing methods large margin e.g. 2.7 f1 gains low resource setting"}
{"id": "b159dffadb69940e14693e812bdaa32e3957717f", "abstract": "Recently, prompt-based learning for pre-trained language models has succeeded in few-shot Named Entity Recognition (NER) by exploiting prompts as task guidance to increase label efficiency. However, previous prompt-based methods for few-shot NER have limitations such as a higher computational complexity, poor zero-shot ability, requiring manual prompt engineering, or lack of prompt robustness. In this work, we address these shortcomings by proposing a new prompt-based learning NER method with Question Answering (QA), called QaNER. Our approach includes 1) a refined strategy for converting NER problems into the QA formulation; 2) NER prompt generation for QA models; 3) prompt-based tuning with QA models on a few annotated NER examples; 4) zero-shot NER by prompting the QA model. Comparing the proposed approach with previous methods, QaNER is faster at inference, insensitive to the prompt quality, and robust to hyper-parameters, as well as demonstrating significantly better low-resource performance and zero-shot capability.", "title": "qaner prompting question answering models for fewshot named entity recognition", "url": "http://arxiv.org/pdf/2203.01543", "tokenized_text": "recently based learning pre trained_language trained language succeeded shot named_entity named entity recognition ner exploiting task guidance increase label efficiency previous based methods shot ner limitations higher computational complexity poor zero shot ability requiring manual prompt_engineering engineering lack robustness work address shortcomings proposing new based learning ner method question_answering question answering qa called approach includes refined strategy converting ner problems qa formulation ner generation qa based tuning qa annotated ner examples zero shot ner qa comparing proposed approach previous methods faster inference quality robust parameters demonstrating significantly better low resource performance zero shot capability"}
{"id": "b1d5c08a6fb6a5ee5b6b6693e10a587733ca05ed", "abstract": "Prompt-based methods have become increasingly popular among information extraction tasks, especially in low-data scenarios. By formatting a finetune task into a pre-training objective, prompt-based methods resolve the data scarce problem effectively. However, seldom do previous research investigate the discrepancy among different prompt formulating strategies. In this work, we compare two kinds of prompts, name-based prompt and ontology-base prompt, and reveal how ontology-base prompt methods exceed its counterpart in zero-shot event argument extraction (EAE) . Furthermore, we analyse the potential risk in ontology-base prompts via a causal view and propose a debias method by causal intervention. Experiments on two benchmarks demonstrate that modified by our debias method, the baseline model becomes both more effective and robust, with significant improvement in the resistance to adversarial attacks.", "title": "causal interventionbased prompt debiasing for event argument extraction", "url": "http://arxiv.org/pdf/2210.01561", "tokenized_text": "based methods increasingly popular information_extraction information extraction tasks especially low data scenarios formatting finetune task pre training objective based methods resolve data scarce problem effectively seldom previous research investigate discrepancy different formulating strategies work compare kinds based ontology base reveal ontology base methods exceed counterpart zero shot event argument extraction eae furthermore analyse potential risk ontology base causal view propose method causal intervention experiments benchmarks demonstrate modified method baseline effective robust significant improvement resistance adversarial attacks"}
{"id": "bad6fa523ecf782c837a2eecaaffa4e1f7477c24", "abstract": "Crosslingual conditional generation (e.g., machine translation) has long enjoyed the benefits of scaling. Nonetheless, there are still issues that scale alone may not overcome. A source query in one language, for instance, may yield several translation options in another language without any extra context. Only one translation could be acceptable however, depending on the translator's preferences and goals. Choosing the incorrect option might significantly affect translation usefulness and quality. We propose a novel method interactive-chain prompting -- a series of question, answering and generation intermediate steps between a Translator model and a User model -- that reduces translations into a list of subproblems addressing ambiguities and then resolving such subproblems before producing the final text to be translated. To check ambiguity resolution capabilities and evaluate translation quality, we create a dataset exhibiting different linguistic phenomena which leads to ambiguities at inference for four languages. To encourage further exploration in this direction, we release all datasets. We note that interactive-chain prompting, using eight interactions as exemplars, consistently surpasses prompt-based methods with direct access to background information to resolve ambiguities.", "title": "interactivechainprompting ambiguity resolution for crosslingual conditional generation with interaction", "url": "http://arxiv.org/pdf/2301.10309", "tokenized_text": "crosslingual conditional generation e.g. machine_translation machine translation long enjoyed benefits scaling nonetheless issues scale overcome source query language instance yield translation options language extra context translation acceptable depending translator preferences goals choosing incorrect option significantly affect translation usefulness quality propose_a_novel propose novel method interactive chain series question answering generation intermediate steps translator user reduces translations list subproblems addressing resolving subproblems producing final text translated check ambiguity resolution capabilities evaluate translation quality create dataset exhibiting different linguistic phenomena leads inference languages encourage exploration direction release datasets note interactive chain interactions exemplars consistently surpasses based methods direct access background information resolve"}
{"id": "c10ab4733b43f19547308c15ca231a668181a36c", "abstract": "Multimodal emotion recognition study is hindered by the lack of labelled corpora in terms of scale and diversity, due to the high annotation cost and label ambiguity. In this paper, we propose a multimodal pre-training model MEmoBERT for multimodal emotion recognition, which learns multimodal joint representations through self-supervised learning from a self-collected large-scale unlabeled video data that come in sheer volume. Furthermore, unlike the conventional \"pre-train, finetune\" paradigm, we propose a prompt-based method that reformulates the downstream emotion classification task as a masked text prediction one, bringing the downstream task closer to the pre-training. Extensive experiments on two benchmark datasets, IEMOCAP and MSP-IMPROV, show that our proposed MEmoBERT significantly enhances emotion recognition performance.", "title": "memobert pretraining model with promptbased learning for multimodal emotion recognition", "url": "https://arxiv.org/pdf/2111.00865", "tokenized_text": "multimodal emotion recognition study hindered lack labelled corpora terms scale diversity high annotation cost label ambiguity paper propose multimodal pre training multimodal emotion recognition learns multimodal joint representations self supervised learning self collected large scale unlabeled video data come sheer volume furthermore unlike conventional pre train finetune paradigm propose based method reformulates downstream emotion classification task masked text prediction bringing downstream task closer pre training extensive_experiments extensive experiments benchmark_datasets benchmark datasets iemocap msp proposed significantly enhances emotion recognition performance"}
{"id": "d235a9085e0543fcbe502fbc269f9a8ee01dcbab", "abstract": "Prompt-based learning, with its capability to tackle zero-shot and few-shot NLP tasks, has gained much attention in community. The main idea is to bridge the gap between NLP downstream tasks and language modeling (LM), by mapping these tasks into natural language prompts, which are then filled by pre-trained language models (PLMs). However, for prompt learning, there are still two salient gaps between NLP tasks and pretraining. First, prompt information is not necessarily sufficiently present during LM pretraining. Second, task-specific data are not necessarily well represented during pretraining. We address these two issues by proposing AdaPrompt, adaptively retrieving external data for continual pretraining of PLMs by making use of both task and prompt characteristics. In addition, we make use of knowledge in Natural Language Inference models for deriving adaptive verbalizers. Experimental results on five NLP benchmarks show that AdaPrompt can improve over standard PLMs in few-shot settings. In addition, in zero-shot settings, our method outperforms standard prompt-based methods by up to 26.35\\% relative error reduction.", "title": "adaprompt adaptive model training for promptbased nlp", "url": "https://aclanthology.org/2022.findings-emnlp.448.pdf", "tokenized_text": "based learning capability tackle zero shot shot nlp_tasks nlp tasks gained attention community main idea bridge gap nlp downstream_tasks downstream tasks language modeling lm mapping tasks natural_language natural language filled pre trained_language trained language plms learning salient gaps nlp_tasks nlp tasks pretraining information necessarily sufficiently present lm pretraining second task specific data necessarily represented pretraining address issues proposing adaptively retrieving external data continual pretraining plms making use task characteristics addition use knowledge natural_language natural language inference deriving adaptive verbalizers experimental_results experimental results nlp benchmarks improve standard plms shot_settings shot settings addition zero shot_settings shot settings method outperforms standard based methods relative error reduction"}
{"id": "d96997265f8146e93b4c9350f19d55e46d1317f0", "abstract": "With the recent advance in large pre-trained language models, researchers have achieved record performances in NLP tasks that mostly focus on language pattern matching. The community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning abilities like human beings. In this work, we investigate the application domain of finance that involves real-world, complex numerical reasoning. We propose a new large-scale dataset, ConvFinQA, aiming to study the chain of numerical reasoning in conversational question answering. Our dataset poses great challenge in modeling long-range, complex numerical reasoning paths in real-world conversations. We conduct comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods, to provide insights into the reasoning mechanisms of these two divisions. We believe our new dataset should serve as a valuable resource to push forward the exploration of real-world, complex reasoning tasks as the next research focus. Our dataset and code is publicly available at https://github.com/czyssrs/ConvFinQA.", "title": "convfinqa exploring the chain of numerical reasoning in conversational finance question answering", "url": "http://arxiv.org/pdf/2210.03849", "tokenized_text": "recent advance large pre trained_language trained language researchers achieved record performances nlp_tasks nlp tasks focus language pattern matching community experiencing shift challenge language imitation complex_reasoning complex reasoning abilities like human beings work investigate application domain finance involves real world complex numerical reasoning propose_a_new propose new large scale dataset aiming study chain numerical reasoning conversational question_answering question answering dataset poses great challenge modeling long range complex numerical reasoning paths real world conversations conduct comprehensive experiments analyses neural symbolic methods based methods provide insights reasoning mechanisms believe new dataset serve valuable resource push forward exploration real world complex_reasoning complex reasoning tasks research focus dataset code publicly_available publicly available"}
{"id": "db0d67057b41927b5b51d3a393c250be64a405ae", "abstract": "Large-scale pre-trained language models such as GPT-3 have shown remarkable performance across various natural language processing tasks. However, applying prompt-based methods with GPT-3 for Grammatical Error Correction (GEC) tasks and their controllability remains underexplored. Controllability in GEC is crucial for real-world applications, particularly in educational settings, where the ability to tailor feedback according to learner levels and specific error types can significantly enhance the learning process.This paper investigates the performance and controllability of prompt-based methods with GPT-3 for GEC tasks using zero-shot and few-shot setting. We explore the impact of task instructions and examples on GPT-3\u2019s output, focusing on controlling aspects such as minimal edits, fluency edits, and learner levels. Our findings demonstrate that GPT-3 could effectively perform GEC tasks, outperforming existing supervised and unsupervised approaches. We also showed that GPT-3 could achieve controllability when appropriate task instructions and examples are given.", "title": "exploring effectiveness of gpt3 in grammatical error correction a study on performance and controllability in promptbased methods", "url": "http://arxiv.org/pdf/2305.18156", "tokenized_text": "large scale pre trained_language trained language gpt-3 shown remarkable performance natural_language natural language processing tasks applying based methods gpt-3 grammatical error correction gec tasks controllability remains underexplored controllability gec crucial real world_applications world applications particularly educational settings ability tailor feedback according learner levels specific error types significantly enhance learning process paper investigates performance controllability based methods gpt-3 gec tasks zero shot shot_setting shot setting explore impact task instructions examples gpt-3 output focusing controlling aspects minimal edits fluency edits learner levels findings demonstrate gpt-3 effectively perform gec tasks outperforming existing supervised unsupervised approaches showed gpt-3 achieve controllability appropriate task instructions examples given"}
{"id": "eb36681fc4c5dfce4f3e05540fc92b007de278ca", "abstract": "Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment LLMs with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while LLMs show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called \\autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, \\autoknow~obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, \\autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate \\autoknow~on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation. Our empirical experiments show that \\autoknow~outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of \\autoknow, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that \\autoknow~can be adapted to other more advanced models, such as GPT-4, and bring consistent efficacy improvement.", "title": "selfevolve a code evolution framework via large language models", "url": "http://arxiv.org/pdf/2306.02907", "tokenized_text": "large_language large language llms revolutionized code_generation code generation pretrained publicly_available publicly available code data methods proposed augment llms retrieved knowledge enhance quality code_generation code generation performance retrieval based methods limited strength retrievers addition llms great emergent ability struggle produce correct code turn address challenges propose_a_novel propose novel step pipeline called leverages llms knowledge providers self reflective programmers unlike retrieval based methods obtains knowledge input generates intermediate code based generated knowledge asks llm act expert programmer perform debugging generated code achieved receiving error message interpreter requiring special test_cases test cases correctness verification evaluate code_generation code generation datasets including data science code humaneval software engineering code python translation empirical experiments outperforms strong baselines significant margin datasets conduct exhaustive analytical experiments validate effectiveness stages find superior based methods scalability analysis demonstrates adapted advanced gpt-4 bring consistent efficacy improvement"}
{"id": "f4cba0db34aa0c389cec267ca1f3ba5255ea2645", "abstract": "Zero-shot information extraction (IE) aims to build IE systems from the unannotated text. It is challenging due to involving little human intervention. Challenging but worthwhile, zero-shot IE reduces the time and effort that data labeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3, ChatGPT) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this work, we ask whether strong IE models can be constructed by directly prompting LLMs. Specifically, we transform the zero-shot IE task into a multi-turn question-answering problem with a two-stage framework (ChatIE). With the power of ChatGPT, we extensively evaluate our framework on three IE tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that ChatIE achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., NYT11-HRL). We believe that our work could shed light on building IE models with limited resources.", "title": "zeroshot information extraction via chatting with chatgpt", "url": "http://arxiv.org/pdf/2302.10205", "tokenized_text": "zero shot information_extraction information extraction ie aims build ie systems unannotated text challenging involving little human intervention challenging zero shot ie reduces time effort data labeling takes recent efforts large_language large language llms e.g. gpt-3 chatgpt promising performance zero shot_settings shot settings inspiring explore based methods work ask strong ie constructed directly llms specifically transform zero shot ie task multi turn question answering problem stage framework power chatgpt extensively evaluate framework ie tasks entity relation triple extract named_entity named entity recognition event extraction empirical results datasets languages achieves impressive performance surpasses shot datasets e.g. believe work shed light building ie limited resources"}
{"id": "f7ccf8ecd508e0b2d423169588dd1c1a82dd3b4d", "abstract": "Large language models (LLMs) have recently garnered significant interest. With in-context learning, LLMs achieve impressive results in various natural language tasks. However, the application of LLMs to sentence embeddings remains an area of ongoing research. In this work, we propose an in-context learning-based method aimed at improving sentence embeddings performance. Our approach involves adapting the previous prompt-based representation method for autoregressive models, constructing a demonstration set that enables LLMs to perform in-context learning, and scaling up the LLMs to different model sizes. Through extensive experiments, in-context learning enables LLMs to generate high-quality sentence embeddings without any fine-tuning. It helps LLMs achieve performance comparable to current contrastive learning methods. By scaling model size, we find scaling to more than tens of billion parameters harms the performance on semantic textual similarity (STS) tasks. However, the largest model outperforms other counterparts and achieves the new state-of-the-art result on transfer tasks. We also fine-tune LLMs with current contrastive learning approach, and the 2.7B OPT model, incorporating our prompt-based method, surpasses the performance of 4.8B ST5, achieving the new state-of-the-art results on STS tasks. Our code is available at https://github.com/kongds/scaling_sentemb.", "title": "scaling sentence embeddings with large language models", "url": "https://arxiv.org/pdf/2307.16645", "tokenized_text": "large_language large language llms recently garnered significant interest context_learning context learning llms achieve impressive results natural_language natural language tasks application llms sentence embeddings remains area ongoing research work propose context_learning context learning based method aimed improving sentence embeddings performance approach involves adapting previous based representation method autoregressive constructing demonstration set enables llms perform context_learning context learning scaling llms different sizes extensive_experiments extensive experiments context_learning context learning enables llms generate high quality sentence embeddings fine tuning helps llms achieve performance comparable current contrastive_learning contrastive learning methods scaling model_size size find scaling tens billion parameters harms performance semantic textual similarity tasks largest outperforms counterparts achieves new state art result transfer tasks fine tune llms current contrastive_learning contrastive learning approach 2.7b opt incorporating based method surpasses performance achieving new state art results tasks code_is_available code available"}
{"id": "fb1d85fe28b5e92e22d084eca674d4a2b48cdc5a", "abstract": "Data-free knowledge distillation (DFKD) conducts knowledge distillation via eliminating the dependence of original training data, and has recently achieved impressive results in accelerating pre-trained language models. At the heart of DFKD is to reconstruct a synthetic dataset by inverting the parameters of the uncompressed model. Prior DFKD approaches, however, have largely relied on hand-crafted priors of the target data distribution for the reconstruction, which can be inevitably biased and often incompetent to capture the intrinsic distributions. To address this problem, we propose a prompt-based method, termed as PromptDFD, that allows us to take advantage of learned language priors, which effectively harmonizes the synthetic sentences to be semantically and grammatically correct. Specifically, PromptDFD leverages a pre-trained generative model to provide language priors and introduces a reinforced topic prompter to control data synthesis, making the generated samples thematically relevant and semantically plausible, and thus friendly to downstream tasks. As shown in our experiments, the proposed method substantially improves the synthesis quality and achieves considerable improvements on distillation performance. In some cases, PromptDFD even gives rise to results on par with those from the data-driven knowledge distillation with access to the original training data.", "title": "prompting to distill boosting datafree knowledge distillation via reinforced prompt", "url": "https://arxiv.org/pdf/2205.07523", "tokenized_text": "data free knowledge_distillation knowledge distillation conducts knowledge_distillation knowledge distillation eliminating dependence original training_data training data recently achieved impressive results accelerating pre trained_language trained language reconstruct synthetic dataset inverting parameters prior approaches largely relied hand crafted priors target data distribution reconstruction inevitably biased capture intrinsic distributions address problem propose based method termed allows advantage learned language priors effectively synthetic sentences semantically grammatically correct specifically leverages pre trained generative provide language priors introduces topic prompter control data synthesis making generated samples thematically relevant semantically plausible friendly downstream_tasks downstream tasks shown experiments proposed_method proposed method substantially improves synthesis quality achieves considerable improvements distillation performance cases gives rise results par data driven knowledge_distillation knowledge distillation access original training_data training data"}
{"id": "002cfed5d4d9bf2fdaddb11d32f14751f2250e0c", "abstract": "Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and model scale. Additionally, we discuss length generalization challenges. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction objective for rapidly eliciting arithmetic capabilities.", "title": "teaching arithmetic to small transformers", "url": "https://arxiv.org/pdf/2307.03381", "tokenized_text": "large_language large language like gpt-4 exhibit emergent capabilities general purpose tasks basic arithmetic trained extensive text data tasks explicitly encoded unsupervised token prediction objective study investigates small transformers trained random initialization efficiently learn arithmetic operations addition multiplication functions like square root token prediction objective demonstrate conventional training_data training data effective arithmetic learning simple formatting changes significantly improve accuracy leads phase transitions function training_data training data scale cases explained connections low rank matrix completion building prior_work prior work train chain thought style data includes intermediate step results complete absence pretraining approach significantly simultaneously improves accuracy sample complexity convergence speed study interplay arithmetic text data training examine effects shot_prompting shot pretraining scale additionally discuss length generalization challenges work highlights importance high quality instructive data considers particular characteristics word prediction objective rapidly eliciting arithmetic capabilities"}
{"id": "0040dac7a1bf7a1eeb01c86ddb993f331f35b158", "abstract": "Recent work on explainable NLP has shown that few-shot prompting can enable large pre-trained language models (LLMs) to generate grammatical and factual natural language explanations for data labels. In this work, we study the connection between explainability and sample hardness by investigating the following research question \u2013 \u201cAre LLMs and humans equally good at explaining data labels for both easy and hard samples?\u201d We answer this question by first collecting human-written explanations in the form of generalizable commonsense rules on the task of Winograd Schema Challenge (Winogrande dataset). We compare these explanations with those generated by GPT-3 while varying the hardness of the test samples as well as the in-context samples. We observe that (1) GPT-3 explanations are as grammatical as human explanations regardless of the hardness of the test samples, (2) for easy examples, GPT-3 generates highly supportive explanations but human explanations are more generalizable, and (3) for hard examples, human explanations are significantly better than GPT-3 explanations both in terms of label-supportiveness and generalizability judgements. We also find that hardness of the in-context examples impacts the quality of GPT-3 explanations. Finally, we show that the supportiveness and generalizability aspects of human explanations are also impacted by sample hardness, although by a much smaller margin than models.", "title": "are hard examples also harder to explain a study with human and modelgenerated explanations", "url": "https://arxiv.org/pdf/2211.07517", "tokenized_text": "recent_work recent work explainable nlp shown shot_prompting shot enable large pre trained_language trained language llms generate grammatical factual natural_language natural language explanations data labels work study connection explainability sample hardness investigating following research question llms humans equally good explaining data labels easy hard samples answer question collecting human written explanations form generalizable commonsense rules task winograd schema challenge dataset compare explanations generated gpt-3 varying hardness test samples context samples observe gpt-3 explanations grammatical human explanations regardless hardness test samples easy examples gpt-3 generates highly supportive explanations human explanations generalizable hard examples human explanations significantly better gpt-3 explanations terms label generalizability judgements find hardness context_examples context examples impacts quality gpt-3 explanations finally generalizability aspects human explanations impacted sample hardness smaller margin"}
{"id": "03d8b1e78d124a561f3c2a67d3199472ee73228d", "abstract": "Dialogue systems need to produce responses that realize multiple types of dialogue acts (DAs) with high semantic fidelity. In the past, natural language generators (NLGs) for dialogue were trained on large parallel corpora that map from a domain-specific DA and its semantic attributes to an output utterance. Recent work shows that pretrained language models (LLMs) offer new possibilities for controllable NLG using prompt-based learning. Here we develop a novel few-shot overgenerate-and-rank approach that achieves the controlled generation of DAs. We compare eight few-shot prompt styles that include a novel method of generating from textual pseudo-references using a textual style transfer approach. We develop six automatic ranking functions that identify outputs with both the correct DA and high semantic accuracy at generation time. We test our approach on three domains and four LLMs. To our knowledge, this is the first work on NLG for dialogue that automatically ranks outputs using both DA and attribute accuracy. For completeness, we compare our results to fine-tuned few-shot models trained with 5 to 100 instances per DA. Our results show that several prompt settings achieve perfect DA accuracy, and near perfect semantic accuracy (99.81%) and perform better than few-shot fine-tuning.", "title": "controllable generation of dialogue acts for dialogue systems via fewshot response generation and ranking", "url": "https://arxiv.org/pdf/2307.14440", "tokenized_text": "dialogue systems need produce responses realize multiple types dialogue acts high semantic fidelity past natural_language natural language generators nlgs dialogue trained large parallel corpora map domain specific da semantic attributes output utterance recent_work recent work shows pretrained_language pretrained language llms offer new possibilities controllable nlg based learning develop novel shot rank approach achieves controlled generation compare shot styles include novel method generating textual pseudo references textual style_transfer style transfer approach develop automatic ranking functions identify outputs correct da high semantic accuracy generation time test approach domains llms knowledge work nlg dialogue automatically ranks outputs da attribute accuracy completeness compare results fine tuned shot trained 100 instances da results settings achieve perfect da accuracy near perfect semantic accuracy perform better shot fine tuning"}
{"id": "03fb95e6be583ca954c3d00812a9e9a40f118e51", "abstract": "Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules, that are simply implemented by few-shot prompted LLM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.", "title": "lambada backward chaining for automated reasoning in natural language", "url": "http://arxiv.org/pdf/2212.13894", "tokenized_text": "remarkable progress automated reasoning natural text large_language large language llms methods chain thought_prompting thought selection inference techniques search proofs forward direction conclusion suffers combinatorial explosion search space high failure rates problems requiring longer chains reasoning classical automated reasoning literature shown reasoning backward direction i.e. intended conclusion supporting significantly efficient proof finding intuition lm setting develop backward chaining algorithm called decomposes reasoning sub modules simply implemented shot prompted llm inference achieves accuracy boosts state art forward reasoning methods challenging logical reasoning datasets particularly deep accurate proof chains required"}
{"id": "04526876688e5a56106629229309fae272da1c79", "abstract": "In-context learning is the paradigm that adapts large language models to downstream tasks by providing a few examples. Few-shot selection -- selecting appropriate examples for each test instance separately -- is important for in-context learning. In this paper, we propose Skill-KNN, a skill-based few-shot selection method for in-context learning. The key advantages of Skill-KNN include: (1) it addresses the problem that existing methods based on pre-trained embeddings can be easily biased by surface natural language features that are not important for the target task; (2) it does not require training or fine-tuning of any models, making it suitable for frequently expanding or changing example banks. The key insight is to optimize the inputs fed into the embedding model, rather than tuning the model itself. Technically, Skill-KNN generates the skill-based descriptions for each test case and candidate example by utilizing a pre-processing few-shot prompting, thus eliminating unimportant surface features. Experimental results across five cross-domain semantic parsing datasets and six backbone models show that Skill-KNN significantly outperforms existing methods.", "title": "skillbased fewshot selection for incontext learning", "url": "https://arxiv.org/pdf/2305.14210", "tokenized_text": "context_learning context learning paradigm adapts large_language large language downstream_tasks downstream tasks providing examples shot selection selecting appropriate examples test instance separately important context_learning context learning paper propose skill knn skill based shot selection method context_learning context learning key advantages skill knn include addresses problem existing_methods existing methods based pre trained embeddings easily biased surface natural_language natural language features important target task require training fine tuning making suitable frequently expanding changing example key insight optimize inputs fed embedding tuning technically skill knn generates skill based descriptions test case candidate example utilizing pre processing shot_prompting shot eliminating surface features experimental_results experimental results cross domain semantic_parsing semantic parsing datasets backbone skill knn significantly_outperforms significantly outperforms existing_methods existing methods"}
{"id": "04e838c16f3d1fb8d69d34fe0a0a92c59717875b", "abstract": "Language models are achieving impressive performance on various tasks by aggressively adopting inference-time prompting techniques, such as zero-shot and few-shot prompting. In this work, we introduce EchoPrompt, a simple yet effective approach that prompts the model to rephrase its queries before answering them. EchoPrompt is adapted for both zero-shot and few-shot in-context learning with standard and chain-of-thought prompting. Experimental results show that EchoPrompt yields substantial improvements across all these settings for four families of causal language models. These improvements are observed across various numerical reasoning (e.g. GSM8K, SVAMP), reading comprehension (e.g. DROP), and logical reasoning (e.g. Coin Flipping) tasks. On average, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002 by 5% in numerical tasks and 13% in reading comprehension tasks. We investigate the factors contributing to EchoPrompt's effectiveness through ablation studies, which reveal that both the original query and the model-generated rephrased version are instrumental in its performance gains. Our empirical results indicate that EchoPrompt is an effective technique that enhances in-context learning performance. We recommend incorporating EchoPrompt into various baseline prompting strategies to achieve performance boosts.", "title": "echoprompt instructing the model to rephrase queries for improved incontext learning", "url": "https://arxiv.org/pdf/2309.10687", "tokenized_text": "language_models language achieving impressive performance tasks adopting inference time prompting_techniques techniques zero shot shot_prompting shot work introduce simple effective approach rephrase queries answering adapted zero shot shot context_learning context learning standard chain thought_prompting thought experimental_results experimental results yields substantial improvements settings families causal language_models language improvements observed numerical reasoning e.g. gsm8 svamp reading comprehension e.g. drop logical reasoning e.g. coin flipping tasks average improves zero shot cot performance code davinci-002 numerical tasks 13 reading comprehension tasks investigate factors contributing effectiveness ablation studies reveal original query generated version instrumental performance gains empirical results_indicate results indicate effective technique enhances context_learning context learning performance recommend incorporating baseline strategies achieve performance boosts"}
{"id": "088ba3cfb904ccd0aa1993a1e30c725b061aad7e", "abstract": "Meta-learning and few-shot prompting are viable methods to induce certain types of compositional behaviour. However, these methods can be very sensitive to the choice of support examples used. Choosing good supports from the training data for a given test query is already a difficult problem, but in some cases solving this may not even be enough. We consider a grounded language learning problem (gSCAN) where good support examples for certain test splits might not even exist in the training data, or would be infeasible to search for. We design an agent which instead generates possible supports which are relevant to the test query and current state of the world, then uses these supports via meta-learning to solve the test query. We show substantially improved performance on a previously unsolved compositional behaviour split without a loss of performance on other splits. Further experiments show that in this case, searching for relevant demonstrations even with an oracle function is not sufficient to attain good performance when using meta-learning.", "title": "improved compositional generalization by generating demonstrations for metalearning", "url": "http://arxiv.org/pdf/2305.13092", "tokenized_text": "meta learning shot_prompting shot viable methods induce certain types compositional behaviour methods sensitive choice support examples choosing good supports training_data training data given test query difficult problem cases solving consider grounded language learning problem good support examples certain test splits exist training_data training data infeasible search design agent instead generates possible supports relevant test query current state world uses supports meta learning solve test query substantially improved performance previously unsolved compositional behaviour split loss performance splits experiments case searching relevant demonstrations oracle function sufficient attain good performance meta learning"}
{"id": "0adec918885dff698acf359988ed79a543157f80", "abstract": "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are \u201cfantastic\u201d and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.", "title": "fantastically ordered prompts and where to find them overcoming fewshot prompt order sensitivity", "url": "https://aclanthology.org/2022.acl-long.556.pdf", "tokenized_text": "handful training samples large pretrained_language pretrained language gpt-3 shown competitive results compared fully supervised fine tuned large pretrained_language pretrained language demonstrate order samples provided difference near state art random performance essentially permutations fantastic analyse phenomenon detail establishing present sizes largest current related specific subset samples given good permutation transferable use development set determine permutations performant deviate true shot_setting shot setting requires additional annotated_data annotated data instead use generative nature language_models language construct artificial development set based entropy statistics candidate permutations set identify performant method yields 13 relative improvement gpt family different established text_classification text classification tasks"}
{"id": "0ba5fb80d2c3ea3a8505415e32d954b4e4eea170", "abstract": "This paper presents the Crowd Score, a novel method to assess the funniness of jokes using large language models (LLMs) as AI judges. Our method relies on inducing different personalities into the LLM and aggregating the votes of the AI judges into a single score to rate jokes. We validate the votes using an auditing technique that checks if the explanation for a particular vote is reasonable using the LLM. We tested our methodology on 52 jokes in a crowd of four AI voters with different humour types: af\ufb01liative, self-enhancing, aggressive and self-defeating. Our results show that few-shot prompting leads to better results than zero-shot for the voting question. Personality induction showed that aggressive and self-defeating voters are signi\ufb01cantly more inclined to \ufb01nd more jokes funny of a set of aggressive/self-defeating jokes than the af\ufb01liative and self-enhancing voters. The Crowd Score follows the same trend as human judges by assigning higher scores to jokes that are also considered funnier by human judges. We believe that our methodology could be applied to other creative domains such as story, poetry, slogans, etc. It could both help the adoption of a \ufb02exible and accurate standard approach to compare different work in the CC community under a common metric and by minimizing human participation in assessing creative artefacts, it could accelerate the prototyping of creative artefacts and reduce the cost of hiring human participants to rate creative artefacts. 1", "title": "crowd score a method for the evaluation of jokes using large language model ai voters as judges", "url": "http://arxiv.org/pdf/2212.11214", "tokenized_text": "paper_presents paper presents crowd score novel method assess large_language large language llms ai judges method relies inducing different personalities llm aggregating ai judges single score rate validate auditing technique checks explanation particular vote reasonable llm tested methodology 52 crowd ai different types self enhancing aggressive self results shot_prompting shot leads better results zero shot voting question personality induction showed aggressive self \ufb01nd set aggressive self self enhancing crowd score follows trend human judges assigning higher scores considered human judges believe methodology applied creative domains story etc help adoption accurate standard approach compare different work community common metric minimizing human participation assessing creative artefacts accelerate prototyping creative artefacts reduce cost hiring human participants rate creative artefacts"}
{"id": "0d42221038c05cee8443c5b5af838505ee137dc3", "abstract": "Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multi-step reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks. ART is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.", "title": "art automatic multistep reasoning and tooluse for large language models", "url": "http://arxiv.org/pdf/2303.09014", "tokenized_text": "large_language large language llms perform complex_reasoning complex reasoning few- zero shot_settings shot settings generating intermediate chain_of_thought chain thought cot reasoning_steps reasoning steps reasoning step rely external tools support computation core llm capabilities e.g. search running code prior_work prior work cot_prompting cot tool use typically requires hand crafting task specific demonstrations carefully interleaving generations tool use introduce automatic reasoning tool use art framework uses frozen llms automatically generate intermediate reasoning_steps reasoning steps program given new task solve art selects demonstrations multi step reasoning tool use task library test_time test time art seamlessly generation external tools called integrates output generation art achieves substantial improvement shot_prompting shot automatic cot unseen tasks mmlu benchmarks matches performance hand crafted cot majority tasks art extensible makes easy humans improve performance correcting errors task specific programs incorporating new tools demonstrate drastically improving performance select tasks minimal human intervention"}
{"id": "0d6bb585493e34975f0437faa3179db3a02f6ae8", "abstract": "We propose a method for arbitrary textual style transfer (TST)\u2014the task of transforming a text into any given style\u2014utilizing general-purpose pre-trained language models. Our method, Prompt-and-Rerank, is based on a mathematical formulation of the TST task, decomposing it into three constituent components: textual similarity, target style strength, and fluency. Our method uses zero-shot or few-shot prompting to obtain a set of candidate generations in the target style, and then re-ranks them according to the three components. Our method enables small pre-trained language models to perform on par with state-of-the-art large-scale models while using two orders of magnitude less compute and memory. We also investigate the effect of model size and prompt design (e.g., prompt paraphrasing and delimiter-pair choice) on style transfer quality across seven diverse textual style transfer datasets, finding, among other things, that delimiter-pair choice has a large impact on performance, and that models have biases on the direction of style transfer.", "title": "promptandrerank a method for zeroshot and fewshot arbitrary textual style transfer with small language models", "url": "https://arxiv.org/pdf/2205.11503", "tokenized_text": "propose method arbitrary textual style_transfer style transfer task transforming text given style utilizing general purpose pre trained_language trained language method rerank based mathematical formulation tst task decomposing components textual similarity target style strength fluency method uses zero shot shot_prompting shot obtain set candidate generations target style ranks according components method enables small pre trained_language trained language perform par state art large scale orders magnitude compute memory investigate effect model_size size design e.g. paraphrasing pair choice style_transfer style transfer quality seven diverse textual style_transfer style transfer datasets finding things pair choice large impact performance biases direction style_transfer style transfer"}
{"id": "0f0a973c6457bcaf7255f891f9b34d658a0a84ae", "abstract": "A medical provider\u2019s summary of a patient visit serves several critical purposes, including clinical decision-making, facilitating hand-offs between providers, and as a reference for the patient. An effective summary is required to be coherent and accurately capture all the medically relevant information in the dialogue, despite the complexity of patient-generated language. Even minor inaccuracies in visit summaries (for example, summarizing \u201cpatient does not have a fever\u201d when a fever is present) can be detrimental to the outcome of care for the patient.This paper tackles the problem of medical conversation summarization by discretizing the task into several smaller dialogue-understanding tasks that are sequentially built upon. First, we identify medical entities and their affirmations within the conversation to serve as building blocks. We study dynamically constructing few-shot prompts for tasks by conditioning on relevant patient information and use GPT-3 as the backbone for our experiments. We also develop GPT-derived summarization metrics to measure performance against reference summaries quantitatively. Both our human evaluation study and metrics for medical correctness show that summaries generated using this approach are clinically accurate and outperform the baseline approach of summarizing the dialog in a zero-shot, single-prompt setting.", "title": "generating medicallyaccurate summaries of patientprovider dialogue a multistage approach using large language models", "url": "http://arxiv.org/pdf/2305.05982", "tokenized_text": "medical summary patient visit serves critical purposes including clinical decision making facilitating hand offs providers reference patient effective summary required coherent accurately capture relevant information dialogue despite complexity patient generated language minor inaccuracies visit summaries example summarizing patient present detrimental outcome care patient paper tackles problem medical conversation summarization discretizing task smaller dialogue understanding tasks sequentially built identify medical entities conversation serve building blocks study dynamically constructing shot tasks conditioning relevant patient information use gpt-3 backbone experiments develop gpt derived summarization metrics measure performance reference summaries quantitatively human evaluation study metrics medical correctness summaries generated approach clinically accurate outperform baseline approach summarizing dialog zero shot single setting"}
{"id": "107aa1e3b1ce604d953475baf98674e92a723bda", "abstract": "Large language models (LLMs) have achieved remarkable success across a wide spectrum of tasks; however, they still face limitations in scenarios that demand long-term planning and spatial reasoning. To facilitate this line of research, in this work, we propose a new benchmark, termed $\\textbf{P}$ath $\\textbf{P}$lanning from $\\textbf{N}$atural $\\textbf{L}$anguage ($\\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by formulating ''path planning'' tasks that require an LLM to navigate to target locations while avoiding obstacles and adhering to constraints. Leveraging this benchmark, we systematically investigate LLMs including GPT-4 via different few-shot prompting methodologies and BART and T5 of various sizes via fine-tuning. Our experimental results show the promise of few-shot GPT-4 in spatial reasoning, when it is prompted to reason and act interleavedly, although it still fails to make long-term temporal reasoning. In contrast, while fine-tuned LLMs achieved impressive results on in-distribution reasoning tasks, they struggled to generalize to larger environments or environments with more obstacles.", "title": "can large language models be good path planners a benchmark and investigation on spatialtemporal reasoning", "url": "https://arxiv.org/pdf/2310.03249", "tokenized_text": "large_language large language llms achieved remarkable success wide spectrum tasks face limitations scenarios demand long term planning spatial reasoning facilitate line research work propose_a_new propose new benchmark termed benchmark evaluates llms spatial temporal reasoning formulating path planning tasks require llm navigate target locations avoiding obstacles adhering constraints leveraging benchmark systematically investigate llms including gpt-4 different shot_prompting shot methodologies t5 sizes fine tuning experimental_results experimental results promise shot gpt-4 spatial reasoning prompted reason act fails long term temporal reasoning contrast fine tuned llms achieved impressive results distribution reasoning tasks generalize larger environments environments obstacles"}
{"id": "1786a2f9140ed7211b21302977de64e948b92308", "abstract": "The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.", "title": "learning performanceimproving code edits", "url": "http://arxiv.org/pdf/2302.07867", "tokenized_text": "law shifted focus industry alternative methods continued performance gains optimizing standard tool help increase program efficiency programmers continue responsibility crafting refactoring code better performance characteristics paper investigate ability large_language large language llms suggest functionally correct performance improving code edits hypothesize language_models language suggest edits ways impractical static analysis investigate questions curating large scale dataset edits contains trajectories programs programmer begins initial version iteratively makes changes improve program performance use evaluate improve capacity large_language large language specifically use examples fine tune multiple variants codegen billion scale transformer decoder additionally use examples openai codex shot_prompting shot leveraging find codex codegen generate performance improving edits speedups 25 programs c++ python c++ programs compiled optimization level crucially allows codegen open sourced 10x smaller codex match performance codex challenging task overall work opens new doors creating systems methods help programmers write efficient code"}
{"id": "197ba7bbfdbb052b0770088815c110774220f397", "abstract": "Large language models (LLMs) that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. We probe this ability in an in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT) performance among similarly-trained LLMs to date. We investigate various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor. Using optimized prompts, we revisit previous assessments of PaLM\u2019s MT capabilities with more recent test sets, modern MT metrics, and human evaluation, and find that its performance, while impressive, still lags that of state-of-the-art supervised systems. We conclude by providing an analysis of PaLM\u2019s MT output which reveals some interesting properties and prospects for future work.", "title": "prompting palm for translation assessing strategies and performance", "url": "http://arxiv.org/pdf/2211.09102", "tokenized_text": "large_language large language llms trained multilingual parallel text exhibit remarkable ability translate languages probe ability depth study pathways language_model language palm demonstrated strongest machine_translation machine translation mt performance similarly trained llms date investigate strategies choosing translation examples shot_prompting shot concluding example quality important factor optimized revisit previous assessments palm mt capabilities recent test sets modern mt metrics human evaluation find performance impressive lags state art supervised systems conclude providing analysis palm mt output reveals interesting properties prospects future work"}
{"id": "1ed5d06c4dc46e6a983597b740ab0a31d0ce22ad", "abstract": "This paper studies contextual biasing with Large Language Models (LLMs), where during second-pass rescoring additional contextual information is provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We propose to leverage prompts for a LLM without fine tuning during rescoring which incorporate a biasing list and few-shot examples to serve as additional information when calculating the score for the hypothesis. In addition to few-shot prompt learning, we propose multi-task training of the LLM to predict both the entity class and the next token. To improve the efficiency for contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we propose dynamic prompting, where we select the most likely class using the class tag prediction, and only use entities in this class as contexts for next token prediction. Word Error Rate (WER) evaluation is performed on i) an internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli dataset. Results indicate that biasing lists and few-shot examples can achieve 17.8% and 9.6% relative improvement compared to first pass ASR, and that multi-task training and dynamic prompting can achieve 20.0% and 11.3% relative WER improvement, respectively.", "title": "contextual biasing of namedentities with large language models", "url": "https://arxiv.org/pdf/2309.00723", "tokenized_text": "paper studies contextual biasing large_language large language llms second pass rescoring additional contextual information provided llm boost automatic speech recognition asr performance propose leverage llm fine_tuning fine tuning rescoring incorporate biasing list shot examples serve additional information calculating score hypothesis addition shot learning propose multi task training llm predict entity class token improve efficiency contextual biasing avoid exceeding llms maximum sequence lengths propose dynamic select likely class class tag prediction use entities class contexts token prediction word error rate wer evaluation performed internal calling messaging dataset ii dataset results_indicate results indicate biasing lists shot examples achieve relative improvement compared pass asr multi task training dynamic achieve relative wer improvement respectively"}
{"id": "1f0dfbbc13ac31de8709bbb4d0f6478aa1222cef", "abstract": "Prompt-based learning reformulates downstream tasks as cloze problems by combining the original input with a template. This technique is particularly useful in few-shot learning, where a model is trained on a limited amount of data. However, the limited templates and text used in few-shot prompt-based learning still leave significant room for performance improvement. Additionally, existing methods using model ensembles can constrain the model efficiency. To address these issues, we propose an augmentation method called MixPro, which augments both the vanilla input text and the templates through token-level, sentence-level, and epoch-level Mixup strategies. We conduct experiments on five few-shot datasets, and the results show that MixPro outperforms other augmentation baselines, improving model performance by an average of 5.08% compared to before augmentation.", "title": "mixpro simple yet effective data augmentation for promptbased learning", "url": "http://arxiv.org/pdf/2304.09402", "tokenized_text": "based learning reformulates downstream_tasks downstream tasks cloze problems combining original input template technique particularly useful shot_learning shot learning trained limited data limited templates text shot based learning significant room performance improvement additionally existing_methods existing methods ensembles efficiency address issues propose augmentation method called augments vanilla input text templates token level sentence level epoch level mixup strategies conduct experiments shot datasets results outperforms augmentation baselines improving performance average compared augmentation"}
{"id": "1f86bf1e334200ec0481349255559fbfe7a33caa", "abstract": "Large pre-trained models have proved to be remarkable zero- and (prompt-based) few-shot learners in unimodal vision and language tasks. We propose MAPL, a simple and parameter-efficient method that reuses frozen pre-trained unimodal models and leverages their strong generalization capabilities in multimodal vision-language (VL) settings. MAPL learns a lightweight mapping between the representation spaces of unimodal models using aligned image-text data, and can generalize to unseen VL tasks from just a few in-context examples. The small number of trainable parameters makes MAPL effective at low-data and in-domain learning. Moreover, MAPL\u2019s modularity enables easy extension to other pre-trained models. Extensive experiments on several visual question answering and image captioning benchmarks show that MAPL achieves superior or competitive performance compared to similar methods while training orders of magnitude fewer parameters. MAPL can be trained in just a few hours using modest computational resources and public datasets. We release our code and pre-trained model weights at https://github.com/oscmansan/mapl.", "title": "mapl parameterefficient adaptation of unimodal pretrained models for visionlanguage fewshot prompting", "url": "http://arxiv.org/pdf/2210.07179", "tokenized_text": "large pre trained proved remarkable zero- based shot learners unimodal vision language tasks propose simple parameter efficient method reuses frozen pre trained unimodal leverages strong generalization capabilities multimodal vision language vl settings learns lightweight mapping representation spaces unimodal aligned image text data generalize unseen vl tasks context_examples context examples small_number small number trainable parameters makes effective low data domain learning modularity enables easy extension pre trained extensive_experiments extensive experiments visual question_answering question answering image captioning benchmarks achieves superior competitive_performance competitive performance compared similar methods training orders magnitude fewer parameters trained hours modest computational resources public datasets release code pre trained weights"}
{"id": "2069aaaa281eb13bcd9330fc4d43f24f6b436a53", "abstract": "The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded\"prompt templates\", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot prompting (generally by over 25% and 65%, respectively) and pipelines with expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at https://github.com/stanfordnlp/dspy", "title": "dspy compiling declarative language model calls into selfimproving pipelines", "url": "https://arxiv.org/pdf/2310.03714", "tokenized_text": "ml community rapidly exploring techniques language_models language lms pipelines solve complex tasks unfortunately existing lm pipelines typically implemented hard templates i.e. lengthy strings discovered trial error systematic approach developing optimizing lm pipelines introduce programming abstracts lm pipelines text transformation graphs i.e. imperative computational graphs lms invoked declarative modules modules parameterized meaning learn creating collecting demonstrations apply compositions finetuning augmentation reasoning techniques design compiler optimize pipeline maximize given metric conduct case studies showing programs express optimize sophisticated lm pipelines reason math word problems tackle multi hop retrieval answer complex questions control agent loops minutes compiling lines allow gpt-3.5 llama2 13b chat self bootstrap pipelines outperform standard shot_prompting shot generally 25 65 respectively pipelines expert created demonstrations 46 16 40 respectively programs compiled open relatively small lms like parameter t5 llama2 13b chat competitive approaches rely expert written chains proprietary gpt-3.5 available"}
{"id": "2522410b1cac0c14fa656a0aaeaff08bacb358a9", "abstract": "While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, add new NLP-specific operations such as free-text rationalization, and illustrate its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel Adapter-based approach. We then conduct two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e. how objectively helpful dialogical explanations are for humans in figuring out the model's predicted label when it's not shown. We found rationalization and feature attribution were helpful in explaining the model behavior. Moreover, users could more reliably predict the model outcome based on an explanation dialogue rather than one-off explanations.", "title": "interrolang exploring nlp models and datasets through dialoguebased explanations", "url": "https://arxiv.org/pdf/2310.05592", "tokenized_text": "recently developed nlp explainability methods let open black_box black box ways et_al et al 2022 missing endeavor interactive tool offering conversational interface dialogue system help users explore datasets explanations contextualized manner e.g. follow questions natural_language natural language interface adapt conversational explanation framework et_al et al 2022 nlp domain add new nlp specific operations free text rationalization illustrate generalizability nlp_tasks nlp tasks dialogue act classification question_answering question answering hate speech detection recognize user queries explanations evaluate fine tuned shot_prompting shot implement novel adapter based approach conduct user studies perceived correctness helpfulness dialogues i.e. helpful explanations humans predicted label shown found rationalization feature attribution helpful explaining behavior users reliably predict outcome based explanation dialogue explanations"}
{"id": "2577d053f8aab912d29b424e1f09133d83740fd2", "abstract": "We present new benchmarks on evaluation code generation models: MBXP and Multilingual HumanEval, and MathQA-X. These datasets cover over 10 programming languages and are generated using a scalable conversion framework that transpiles prompts and test cases from the original Python datasets into the corresponding data in the target language. Using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of few-shot prompting to teach the model new languages, and zero-shot translation abilities even on mono-lingual settings. Furthermore, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks. Overall, our benchmarks represents a significant step towards a deeper understanding of language models' code generation abilities. We publicly release our code and datasets at https://github.com/amazon-research/mxeval.", "title": "multilingual evaluation of code generation models", "url": "http://arxiv.org/pdf/2210.14868", "tokenized_text": "present new benchmarks evaluation code_generation code generation multilingual humaneval mathqa x. datasets cover 10 programming languages generated scalable conversion framework test_cases test cases original python datasets corresponding data target language benchmarks able assess performance code_generation code generation multi lingual fashion discovered generalization_ability generalization ability language_models language domain languages advantages multi lingual mono lingual ability shot_prompting shot teach new languages zero shot translation abilities mono lingual settings furthermore use code_generation code generation perform large scale bootstrapping obtain synthetic canonical solutions languages code related evaluations code robustness summarization tasks overall benchmarks represents significant step deeper understanding language_models language code_generation code generation abilities publicly release code datasets"}
{"id": "2a99239f09e95f4dbccec572d66f4519206762f9", "abstract": "We propose a simple yet a novel approach to improve completion in domain modeling activities. Our approach exploits the power of large language models by using few-shot prompt learning without the need to train or fine-tune those models with large datasets that are scarce in this field. We implemented our approach and tested it on the completion of static and dynamic domain diagrams. Our initial evaluation shows that such an approach is effective and can be integrated in different ways during the modeling activities.", "title": "towards using fewshot prompt learning for automating model completion", "url": "https://arxiv.org/pdf/2212.03404", "tokenized_text": "propose simple novel_approach novel approach improve completion domain modeling activities approach exploits power large_language large language shot learning need train fine tune large datasets scarce field implemented approach tested completion static dynamic domain initial evaluation shows approach effective integrated different ways modeling activities"}
{"id": "32426b96ff3c680125bde3b835bfa931288b8ade", "abstract": "Large Language models (LLMs) can be induced to solve non-trivial problems with \u201cfew-shot\u201d prompts including illustrative problem-solution examples. Now if the few-shots also include \u201cchain of thought\u201d ($\\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a \u201cexplained\u201d solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\\mathcal{S}-C$ (or even $\\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.", "title": "better patching using llm prompting, via selfconsistency", "url": "https://arxiv.org/pdf/2306.00108", "tokenized_text": "large_language large language llms induced solve non trivial problems shot including illustrative problem solution examples shots include chain_of_thought chain thought explanations form problem explanation solution llms generate explained solution perform better recently exciting substantially better technique self consistency emerged based intuition plausible explanations right solution llm sampled repeatedly generate pool explanation solution pairs given problem frequently occurring solutions pool ignoring explanations tend likely correct unfortunately use highly performant approach software engineering settings hampered lack explanations software datasets lack explanations paper describe application approach program repair commit log fix explanation illustrative shots achieve state art results beating previous approaches based program repair dataset find evidence suggesting correct commit messages helping llm learn produce better patches"}
{"id": "3566e1245bfc90096fe0cdb8b18674da6519c8d6", "abstract": "Narrative-driven recommendation (NDR) presents an information access problem where users solicit recommendations with verbose descriptions of their preferences and context, for example, travelers soliciting recommendations for points of interest while describing their likes/dislikes and travel circumstances. These requests are increasingly important with the rise of natural language-based conversational interfaces for search and recommendation systems. However, NDR lacks abundant training data for models, and current platforms commonly do not support these requests. Fortunately, classical user-item interaction datasets contain rich textual data, e.g., reviews, which often describe user preferences and context \u2013 this may be used to bootstrap training for NDR models. In this work, we explore using large language models (LLMs) for data augmentation to train NDR models. We use LLMs for authoring synthetic narrative queries from user-item interactions with few-shot prompting and train retrieval models for NDR on synthetic queries and user-item interaction data. Our experiments demonstrate that this is an effective strategy for training small-parameter retrieval models that outperform other retrieval and LLM baselines for narrative-driven recommendation.", "title": "large language model augmented narrative driven recommendations", "url": "https://arxiv.org/pdf/2306.02250", "tokenized_text": "narrative driven recommendation presents information access problem users recommendations verbose descriptions preferences context example recommendations points interest describing circumstances requests increasingly important rise natural_language natural language based conversational interfaces search recommendation systems lacks abundant training_data training data current platforms commonly support requests classical user item interaction datasets contain rich textual data e.g. reviews describe user preferences context bootstrap training work explore large_language large language llms data_augmentation data augmentation train use llms authoring synthetic narrative queries user item interactions shot_prompting shot train retrieval synthetic queries user item interaction data experiments_demonstrate experiments demonstrate effective strategy training small parameter retrieval outperform retrieval llm baselines narrative driven recommendation"}
{"id": "3599a236f285af48782fc30b1341d13ec7320735", "abstract": "Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence.", "title": "a comprehensive survey on pretrained foundation models a history from bert to chatgpt", "url": "http://arxiv.org/pdf/2302.09419", "tokenized_text": "pretrained foundation_models foundation pfms regarded foundation downstream_tasks downstream tasks different data modalities pfm e.g. bert chatgpt gpt-4 trained large scale data provides reasonable parameter initialization wide_range wide range downstream applications bert learns bidirectional encoder representations transformers trained large datasets contextual language_models language similarly generative pretrained transformer gpt method employs transformers feature extractor trained autoregressive paradigm large datasets recently chatgpt shows promising success large_language large language applies autoregressive language_model language zero_shot zero shot shot_prompting shot remarkable achievements pfm brought significant breakthroughs fields ai numerous studies proposed different methods raising demand updated survey study provides comprehensive review recent research advancements challenges opportunities pfms text image graph data modalities review covers basic components existing pretraining methods natural_language natural language processing computer_vision computer vision graph learning additionally explores advanced pfms different data modalities unified pfms consider data quality quantity review discusses research related fundamentals pfms efficiency compression security privacy finally study provides key implications future_research future research directions challenges open problems field pfms overall survey aims shed light research pfms scalability security logical reasoning ability cross domain learning ability user friendly interactive ability artificial general intelligence"}
{"id": "3841234dd49250c4fcbba79eed6593d3b57932c1", "abstract": "This paper pursues the insight that language models naturally enable an intelligent variation operator similar in spirit to evolutionary crossover. In particular, language models of sufficient scale demonstrate in-context learning, i.e. they can learn from associations between a small number of input patterns to generate outputs incorporating such associations (also called few-shot prompting). This ability can be leveraged to form a simple but powerful variation operator, i.e. to prompt a language model with a few text-based genotypes (such as code, plain-text sentences, or equations), and to parse its corresponding output as those genotypes' offspring. The promise of such language model crossover (which is simple to implement and can leverage many different open-source language models) is that it enables a simple mechanism to evolve semantically-rich text representations (with few domain-specific tweaks), and naturally benefits from current progress in language models. Experiments in this paper highlight the versatility of language-model crossover, through evolving binary bit-strings, sentences, equations, text-to-image prompts, and Python code. The conclusion is that language model crossover is a promising method for evolving genomes representable as text.", "title": "language model crossover variation through fewshot prompting", "url": "https://arxiv.org/pdf/2302.12170", "tokenized_text": "paper insight language_models language naturally enable intelligent variation operator similar spirit evolutionary crossover particular language_models language sufficient scale demonstrate context_learning context learning i.e. learn associations small_number small number input patterns generate outputs incorporating associations called shot_prompting shot ability leveraged form simple powerful variation operator i.e. language_model language text based genotypes code plain text sentences equations parse corresponding output genotypes promise language_model language crossover simple implement leverage different open source language_models language enables simple mechanism evolve semantically rich text representations domain specific tweaks naturally benefits current progress language_models language experiments paper highlight versatility language crossover evolving binary bit strings sentences equations text image python code conclusion language_model language crossover promising method evolving text"}
{"id": "3886f3bd2a0af9e75bf9fa5b7db4224969dbf346", "abstract": "With the boom of Large Language Models (LLMs), the research of solving Math Word Problem (MWP) has recently made great progress. However, there are few studies to examine the security of LLMs in math solving ability. Instead of attacking prompts in the use of LLMs, we propose a MathAttack model to attack MWP samples which are closer to the essence of security in solving math problems. Compared to traditional text adversarial attack, it is essential to preserve the mathematical logic of original MWPs during the attacking. To this end, we propose logical entity recognition to identify logical entries which are then frozen. Subsequently, the remaining text are attacked by adopting a word-level attacker. Furthermore, we propose a new dataset RobustMath to evaluate the robustness of LLMs in math solving ability. Extensive experiments on our RobustMath and two another math benchmark datasets GSM8K and MultiAirth show that MathAttack could effectively attack the math solving ability of LLMs. In the experiments, we observe that (1) Our adversarial samples from higher-accuracy LLMs are also effective for attacking LLMs with lower accuracy (e.g., transfer from larger to smaller-size LLMs, or from few-shot to zero-shot prompts); (2) Complex MWPs (such as more solving steps, longer text, more numbers) are more vulnerable to attack; (3) We can improve the robustness of LLMs by using our adversarial samples in few-shot prompts. Finally, we hope our practice and observation can serve as an important attempt towards enhancing the robustness of LLMs in math solving ability. We will release our code and dataset.", "title": "mathattack attacking large language models towards math solving ability", "url": "https://arxiv.org/pdf/2309.01686", "tokenized_text": "boom large_language large language llms research solving math word problem mwp recently great progress studies examine security llms math solving ability instead use llms propose attack mwp samples closer essence security solving math problems compared traditional text adversarial attack essential preserve mathematical logic original mwps end propose logical entity recognition identify logical entries frozen subsequently remaining text attacked adopting word level attacker furthermore propose_a_new propose new dataset evaluate robustness llms math solving ability extensive_experiments extensive experiments math benchmark_datasets benchmark datasets gsm8 effectively attack math solving ability llms experiments observe adversarial samples higher accuracy llms effective llms lower accuracy e.g. transfer larger smaller size llms shot zero shot complex mwps solving steps longer text numbers vulnerable attack improve robustness llms adversarial samples shot finally hope practice observation serve important attempt enhancing robustness llms math solving ability release code dataset"}
{"id": "3b88526a0f0337e3a6b632b4af8fd0882eb4b470", "abstract": "Large language models (LLMs) have demonstrated exceptional performance in various natural language processing tasks, yet their efficacy in more challenging and domain-specific tasks remains largely unexplored. This paper presents FinEval, a benchmark specifically designed for the financial domain knowledge in the LLMs. FinEval is a collection of high-quality multiple-choice questions covering Finance, Economy, Accounting, and Certificate. It includes 4,661 questions spanning 34 different academic subjects. To ensure a comprehensive model performance evaluation, FinEval employs a range of prompt types, including zero-shot and few-shot prompts, as well as answer-only and chain-of-thought prompts. Evaluating state-of-the-art Chinese and English LLMs on FinEval, the results show that only GPT-4 achieved an accuracy close to 70% in different prompt settings, indicating significant growth potential for LLMs in the financial domain knowledge. Our work offers a more comprehensive financial knowledge evaluation benchmark, utilizing data of mock exams and covering a wide range of evaluated LLMs.", "title": "fineval a chinese financial domain knowledge evaluation benchmark for large language models", "url": "https://arxiv.org/pdf/2308.09975", "tokenized_text": "large_language large language llms demonstrated exceptional performance natural_language natural language processing tasks efficacy challenging domain specific tasks remains largely unexplored paper_presents paper presents benchmark specifically designed financial domain knowledge llms collection high quality multiple choice questions covering finance accounting includes questions spanning 34 different academic subjects ensure comprehensive performance evaluation employs range types including zero shot shot answer chain thought evaluating state art chinese english llms results gpt-4 achieved accuracy close 70 different settings indicating significant growth potential llms financial domain knowledge work offers comprehensive financial knowledge evaluation benchmark utilizing data mock exams covering wide_range wide range evaluated llms"}
{"id": "3d7d385d9ee75a286e8da27f7d3cf9f12651c899", "abstract": "Prompt tuning approaches, which learn task-specific soft prompts for a downstream task conditioning on frozen pre-trained models, have attracted growing interest due to its parameter efficiency. With large language models and sufficient training data, prompt tuning performs comparably to full-model tuning. However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning. In this work, we focus on improving the few-shot performance of prompt tuning by transferring knowledge from soft prompts of source tasks. Recognizing the good generalization capabilities of ensemble methods in low-data regime, we first experiment and show that a simple ensemble of model predictions based on different source prompts, outperforms existing multi-prompt knowledge transfer approaches such as source prompt fusion in the few-shot setting. Motivated by this observation, we further investigate model ensembles and propose Sample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the contribution of each source model for each target sample separately when ensembling source model outputs. Through this way, SESoM inherits the superior generalization of model ensemble approaches and simultaneously captures the sample-specific competence of each source prompt. We conduct experiments across a diverse set of eight NLP tasks using models of different scales (T5-{base, large, XL}) and find that SESoM consistently outperforms the existing models of the same as well as larger parametric scale by a large margin.", "title": "model ensemble instead of prompt fusion a samplespecific knowledge transfer method for fewshot prompt tuning", "url": "http://arxiv.org/pdf/2210.12587", "tokenized_text": "tuning approaches learn task specific soft downstream task conditioning frozen pre trained attracted growing interest parameter efficiency large_language large language sufficient training_data training data tuning performs comparably tuning limited training samples shot_settings shot settings tuning fails match performance fine tuning work focus improving shot performance tuning transferring knowledge soft source tasks recognizing good generalization capabilities ensemble methods low data regime experiment simple ensemble predictions based different source outperforms existing multi knowledge transfer approaches source fusion shot_setting shot setting motivated observation investigate ensembles propose sample specific ensemble source learns adjust contribution source target sample separately ensembling source outputs way inherits superior generalization ensemble approaches simultaneously captures sample specific competence source conduct experiments diverse set nlp_tasks nlp tasks different scales large xl find consistently_outperforms consistently outperforms existing larger parametric scale large margin"}
{"id": "41531594d7e0f3b2e138ae43e0a0f6e24a9b014c", "abstract": "Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (\u2018faster\u2019) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io", "title": "code as policies language model programs for embodied control", "url": "https://arxiv.org/pdf/2209.07753", "tokenized_text": "large_language large language llms trained code completion shown capable synthesizing simple python programs find code writing llms purposed write robot policy code given natural_language natural language commands specifically policy code express functions feedback loops process perception outputs e.g. object detectors parameterize control primitive apis provided input example language commands formatted comments followed corresponding policy code shot_prompting shot llms new commands autonomously compose api calls generate new policy code respectively chaining classic logic structures party libraries e.g. perform arithmetic llms way write robot policies exhibit spatial geometric reasoning ii generalize new instructions iii precise values e.g. ambiguous descriptions faster depending context i.e. behavioral commonsense paper_presents paper presents code policies robot centric formulation language_model language generated programs represent reactive policies e.g. controllers waypoint based policies vision based pick place trajectory based control demonstrated multiple real robot platforms central approach hierarchical code recursively defining functions write complex code improves state art solve problems humaneval benchmark code videos available"}
{"id": "446fb5dead075a1a08862662738f462e9a0e91c8", "abstract": "Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage. Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen. Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide. As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable. Our work provides an alternative to demonstrations: tool documentation. We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations. We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities. First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts. Second, on a newly collected realistic tool-use dataset with hundreds of available tool APIs, we show that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation. Third, we highlight the benefits of tool documentations by tackling image generation and video tracking using just-released unseen state-of-the-art models as tools. Finally, we highlight the possibility of using tool documentation to automatically enable new applications: by using nothing more than the documentation of GroundingDino, Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the just-released Grounded-SAM and Track Anything models.", "title": "tool documentation enables zeroshot toolusage with large language models", "url": "https://arxiv.org/pdf/2308.00675", "tokenized_text": "today large_language large language llms taught use new tools providing demonstrations tool usage unfortunately demonstrations hard acquire result undesirable biased usage wrong demonstration chosen rare scenario demonstrations readily available principled selection protocol determine ones provide tasks grow complex selection search grows combinatorially intractable work provides alternative demonstrations tool documentation advocate use tool documentation descriptions individual tool usage demonstrations claim main empirical findings tasks vision language modalities existing benchmarks zero shot tool documentation sufficient eliciting proper tool usage achieving performance par shot second newly collected realistic tool use dataset hundreds available tool apis tool documentation significantly valuable demonstrations zero shot documentation significantly outperforming shot documentation highlight benefits tool documentations tackling image_generation image generation video tracking released unseen state art tools finally highlight possibility tool documentation automatically enable new applications documentation stable_diffusion stable diffusion sam llms functionalities released grounded sam track"}
{"id": "4988b3d378b79eb8669112620baf1ff4e3e536fd", "abstract": "The past decade has witnessed dramatic gains in natural language processing and an unprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (CoT) prompting. Specifically, CoT pushes the performance of large language models in a few-shot setup by augmenting the prompts with intermediate steps. Despite impressive results across various tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of CoT-based few-shot prompting mechanisms in large language models. We first systematically identify and define the key components of a prompt: symbols, patterns, and text. Then, we devise and conduct an exhaustive set of experiments across four different tasks, by querying the model with counterfactual prompts where only one of these components is altered. Our experiments across three models (PaLM, GPT-3, and CODEX) reveal several surprising findings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success of CoT. Second, our results conclude that the primary role of intermediate steps may not be to facilitate learning how to solve a task. The intermediate steps are rather a beacon for the model to realize what symbols to replicate in the output to form a factual answer. Further, text imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis reveals that a symbiotic relationship between text and patterns explains the success of few-shot prompting: text helps extract commonsense from the question to help patterns, and patterns enforce task understanding and direct text generation.", "title": "text and patterns for effective chain of thought, it takes two to tango", "url": "http://arxiv.org/pdf/2209.07686", "tokenized_text": "past decade witnessed dramatic gains natural_language natural language processing unprecedented scaling large_language large language developments accelerated advent shot techniques chain_of_thought chain thought cot specifically cot pushes performance large_language large language shot setup augmenting intermediate steps despite impressive results tasks reasons success explored work uses counterfactual develop deeper understanding cot based shot_prompting shot mechanisms large_language large language systematically identify define key components symbols patterns text devise conduct exhaustive set experiments different tasks querying counterfactual components altered experiments palm gpt-3 codex reveal surprising findings brings question conventional wisdom shot_prompting shot presence factual patterns practically success cot. second results conclude primary role intermediate steps facilitate learning solve task intermediate steps realize symbols replicate output form factual answer text patterns commonsense knowledge meaning empirical qualitative analysis reveals relationship text patterns explains success shot_prompting shot text helps extract commonsense question help patterns patterns enforce task understanding direct text generation"}
{"id": "4e1a4d6804c7983c659feb7e41d49ad8c21aaa43", "abstract": "Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MultiSim with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot cross-lingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplifications outperforming fine-tuned models in most languages. We validate these findings through human evaluation.", "title": "revisiting nonenglish text simplification a unified multilingual benchmark", "url": "http://arxiv.org/pdf/2305.15678", "tokenized_text": "recent advancements high quality large scale english resources pushed frontier english automatic text simplification ats research work multilingual text simplification lack diverse evaluation benchmark covers complex simple sentence pairs languages paper introduces benchmark collection 27 resources 12 distinct languages containing 1.7 million complex simple sentence pairs benchmark encourage research developing effective multilingual text simplification evaluation metrics experiments pre trained multilingual language_models language reveal exciting performance improvements multilingual training non english settings observe strong performance russian zero shot cross lingual_transfer lingual transfer low resource_languages resource languages shot_prompting shot achieves comparable quality reference simplifications outperforming fine tuned languages validate findings human evaluation"}
{"id": "53addc28b106440a3c306b2cff8e259ad63d6d53", "abstract": "Large Language models (LLMs) possess the capability to engage In-context Learning (ICL) by leveraging a few demonstrations pertaining to a new downstream task as conditions. However, this particular learning paradigm suffers from high instability stemming from substantial variances induced by factors such as the input distribution of selected examples, their ordering, and prompt formats. In this work, we demonstrate that even when all these factors are held constant, the random selection of examples still results in high variance. Consequently, we aim to explore the informative ability of data examples by quantifying the Information Gain (IG) obtained in prediction after observing a given example candidate. Then we propose to sample those with maximum IG. Additionally, we identify the presence of template bias, which can lead to unfair evaluations of IG during the sampling process. To mitigate this bias, we introduce Calibration Before Sampling strategy. The experimental results illustrate that our proposed method can yield an average relative improvement of 14.3% across six classification tasks using three LLMs.", "title": "towards informative fewshot prompt with maximum information gain for incontext learning", "url": "https://arxiv.org/pdf/2310.08923", "tokenized_text": "large_language large language llms possess capability engage context_learning context learning icl leveraging demonstrations pertaining new downstream task conditions particular learning paradigm suffers high instability substantial variances induced factors input distribution selected examples ordering formats work demonstrate factors held constant random selection examples results high variance consequently aim explore informative ability data examples quantifying information gain obtained prediction observing given example candidate propose sample maximum additionally identify presence template bias lead evaluations sampling process mitigate bias introduce calibration sampling strategy experimental_results experimental results illustrate proposed_method proposed method yield average relative improvement classification tasks llms"}
{"id": "587352c3b95c90de6d37f061c8e117f42be0b575", "abstract": "Large Language Models (LLMs) have demonstrated impressive planning abilities in single-agent embodied tasks across various domains. However, their capacity for planning and communication in multi-agent cooperation remains unclear, even though these are crucial skills for intelligent embodied agents. In this paper, we present a novel framework that utilizes LLMs for multi-agent cooperation and tests it in various embodied environments. Our framework enables embodied agents to plan, communicate, and cooperate with other embodied agents or humans to accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs, such as GPT-4, can surpass strong planning-based methods and exhibit emergent effective communication using our framework without requiring fine-tuning or few-shot prompting. We also discover that LLM-based agents that communicate in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for embodied AI and lays the foundation for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.", "title": "building cooperative embodied agents modularly with large language models", "url": "https://arxiv.org/pdf/2307.02485", "tokenized_text": "large_language large language llms demonstrated impressive planning abilities single agent embodied tasks domains capacity planning communication multi agent cooperation remains unclear crucial skills intelligent embodied agents paper present novel framework utilizes llms multi agent cooperation tests embodied environments framework enables embodied agents plan communicate embodied agents humans accomplish long horizon tasks efficiently demonstrate recent llms gpt-4 surpass strong planning based methods exhibit emergent effective communication framework requiring fine tuning shot_prompting shot discover llm based agents communicate natural_language natural language trust effectively humans research underscores potential llms embodied ai lays foundation future_research future research multi agent cooperation videos found project website"}
{"id": "5df5ebcaed745a5252b4fae64dc1d7ca90e68ff6", "abstract": "Few-shot classification has made great strides due to foundation models that, through priming and prompting, are highly effective few-shot learners. However, this approach has high variance both across different sets of few shots (data selection) and across different finetuning runs (run variability). This is problematic not only because it impedes the fair comparison of different approaches, but especially because it makes few-shot learning too unreliable for many real-world applications. To alleviate these issues, we make two contributions for more stable and effective few-shot learning: First, we propose novel ensembling methods and show that they substantially reduce run variability. Second, we introduce a new active learning (AL) criterion for data selection and present the first AL-based approach specifically tailored towards prompt-based learning. In our experiments, we show that our combined method, MEAL (Multiprompt finetuning and prediction Ensembling with Active Learning), improves overall performance of prompt-based finetuning by 2.3 points on five diverse tasks.", "title": "meal stable and active learning for fewshot prompting", "url": "http://arxiv.org/pdf/2211.08358", "tokenized_text": "shot classification great strides foundation_models foundation priming highly effective shot learners approach high variance different sets shots data selection different finetuning runs run variability problematic impedes fair comparison different approaches especially makes shot_learning shot learning unreliable real world_applications world applications alleviate issues contributions stable effective shot_learning shot learning propose novel ensembling methods substantially reduce run variability second introduce new active learning al criterion data selection present al based approach specifically tailored based learning experiments combined method finetuning prediction ensembling active learning improves overall performance based finetuning 2.3 points diverse tasks"}
{"id": "5e3675bdbe898cb28a0fc3c2f72a578a97fe64bb", "abstract": "Prompt learning recently become an effective linguistic tool to motivate the PLMs\u2019 knowledge on few-shot-setting tasks. However, studies have shown the lack of robustness still exists in prompt learning, since suitable initialization of continuous prompt and expert-\ufb01rst manual prompt are essential in \ufb01ne-tuning process. What is more, human also utilize their comparative ability to motivate their existing knowledge for distinguishing different examples. Motivated by this, we explore how to use contrastive samples to strengthen prompt learning. In detail, we \ufb01rst propose our model ConsPrompt combining with prompt encoding network, contrastive sampling module, and contrastive scoring module. Subsequently, two sampling strategies, similarity-based and label-based strategies, are introduced to realize dif-ferential contrastive learning. The effectiveness of proposed ConsPrompt is demonstrated in \ufb01ve different few-shot learning tasks and shown the similarity-based sampling strategy is more effective than label-based in combining contrastive learning. Our results also ex-hibits the state-of-the-art performance and robustness in different few-shot settings, which proves that the ConsPrompt could be assumed as a better knowledge probe to motivate PLMs. As far as we could reach, this is the \ufb01rst work exploring how to use contrastive learning approach and suitable contrastive samples to enhance prompt-based \ufb01ne-tuning.", "title": "consprompt easily exploiting contrastive samples for fewshot prompt learning", "url": "https://arxiv.org/pdf/2211.04118", "tokenized_text": "learning recently effective linguistic tool motivate plms knowledge shot setting tasks studies shown lack robustness exists learning suitable initialization continuous manual essential \ufb01ne tuning process human utilize comparative ability motivate existing knowledge distinguishing different examples motivated explore use contrastive samples strengthen learning detail \ufb01rst propose combining encoding network contrastive sampling module contrastive scoring module subsequently sampling strategies similarity based label based strategies introduced realize contrastive_learning contrastive learning effectiveness proposed demonstrated \ufb01ve different shot_learning shot learning tasks shown similarity based sampling strategy effective label based combining contrastive_learning contrastive learning results state art performance robustness different shot_settings shot settings proves assumed better knowledge probe motivate plms far reach \ufb01rst work exploring use contrastive_learning contrastive learning approach suitable contrastive samples enhance based \ufb01ne tuning"}
{"id": "5f5253fb15ac382e96ade0335baf1cfaa240fb1d", "abstract": "Statutory reasoning is the task of reasoning with facts and statutes, which are rules written in natural language by a legislature. It is a basic legal skill. In this paper we explore the capabilities of the most capable GPT-3 model, text-davinci-003, on an established statutory-reasoning dataset called SARA. We consider a variety of approaches, including dynamic few-shot prompting, chain-of-thought prompting, and zero-shot prompting. While we achieve results with GPT-3 that are better than the previous best published results, we also identify several types of clear errors it makes. We investigate why these errors happen. We discover that GPT-3 has imperfect prior knowledge of the actual U.S. statutes on which SARA is based. More importantly, we create simple synthetic statutes, which GPT-3 is guaranteed not to have seen during training. We find GPT-3 performs poorly at answering straightforward questions about these simple synthetic statutes.", "title": "can gpt3 perform statutory reasoning", "url": "https://arxiv.org/pdf/2302.06100", "tokenized_text": "reasoning task reasoning facts rules written natural_language natural language basic legal skill paper explore capabilities capable gpt-3 text davinci-003 established reasoning dataset called consider variety approaches including dynamic shot_prompting shot chain thought_prompting thought zero shot_prompting shot achieve results gpt-3 better previous best published results identify types clear errors makes investigate errors happen discover gpt-3 imperfect prior knowledge actual u.s. based importantly create simple synthetic gpt-3 guaranteed seen training find gpt-3 performs poorly answering straightforward questions simple synthetic"}
{"id": "5f88b907cb6b79ce22e826832f05c0471ecb095e", "abstract": "Languages models have been successfully applied to a variety of reasoning tasks in NLP, yet the language models still suffer from compositional generalization. In this paper we present Explainable Verbal Reasoner Plus (EVR+), a reasoning framework that enhances language models' compositional reasoning ability by (1) allowing the model to explicitly generate and execute symbolic operators, and (2) allowing the model to decompose a complex task into several simpler ones in a flexible manner. Compared with its predecessor Explainable Verbal Reasoner (EVR) and other previous approaches adopting similar ideas, our framework supports more diverse types of reasoning such as nested loops and different types of recursion. To evaluate our reasoning framework, we build a synthetic dataset with five tasks that require compositional reasoning. Results show that our reasoning framework can enhance the language model's compositional generalization performance on the five tasks, using a fine-tuned language model. We also discussed the possibility and the challenges to combine our reasoning framework with a few-shot prompted language model.", "title": "explainable verbal reasoner plus (evr+) a natural language reasoning framework that supports diverse compositional reasoning", "url": "http://arxiv.org/pdf/2305.00061", "tokenized_text": "languages successfully applied variety reasoning tasks nlp language_models language suffer compositional generalization paper_we_present paper present explainable verbal reasoner plus reasoning framework enhances language_models language compositional reasoning ability allowing explicitly generate execute symbolic operators allowing decompose complex task simpler ones flexible manner compared explainable verbal reasoner previous approaches adopting similar ideas framework supports diverse types reasoning nested loops different types evaluate reasoning framework build synthetic dataset tasks require compositional reasoning results reasoning framework enhance language_model language compositional generalization performance tasks fine tuned language_model language discussed possibility challenges combine reasoning framework shot prompted language_model language"}
{"id": "61bbdbf481a6d3519c22513ebe8d6c3cd381851e", "abstract": "Visual Word Sense Disambiguation (VWSD) is a novel challenging task that lies between linguistic sense disambiguation and fine-grained multimodal retrieval. The recent advancements in the development of visiolinguistic (VL) transformers suggest some off-the-self implementations with encouraging results, which however we argue that can be further improved. To this end, we propose some knowledge-enhancement techniques towards improving the retrieval performance of VL transformers via the usage of Large Language Models (LLMs) as Knowledge Bases. More specifically, knowledge stored in LLMs is retrieved with the help of appropriate prompts in a zero-shot manner, achieving performance advancements. Moreover, we convert VWSD to a purely textual question-answering (QA) problem by considering generated image captions as multiple-choice candidate answers. Zero-shot and few-shot prompting strategies are leveraged to explore the potential of such a transformation, while Chain-of-Thought (CoT) prompting in the zero-shot setting is able to reveal the internal reasoning steps an LLM follows to select the appropriate candidate. In total, our presented approach is the first one to analyze the merits of exploiting knowledge stored in LLMs in different ways to solve WVSD.", "title": "language models as knowledge bases for visual word sense disambiguation", "url": "https://arxiv.org/pdf/2310.01960", "tokenized_text": "visual word sense disambiguation novel challenging task lies linguistic sense disambiguation fine grained multimodal retrieval recent advancements development vl transformers suggest self implementations encouraging results argue improved end propose knowledge enhancement techniques improving retrieval performance vl transformers usage large_language large language llms knowledge bases specifically knowledge stored llms retrieved help appropriate zero shot manner achieving performance advancements convert purely textual question answering qa problem considering generated image captions multiple choice candidate answers zero shot shot_prompting shot strategies leveraged explore potential transformation chain thought cot zero shot_setting shot setting able reveal internal reasoning_steps reasoning steps llm follows select appropriate candidate total presented approach analyze merits exploiting knowledge stored llms different ways solve"}
{"id": "663a41c866d49ce052801fbc88947d39764cad29", "abstract": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.", "title": "challenging bigbench tasks and whether chainofthought can solve them", "url": "http://arxiv.org/pdf/2210.09261", "tokenized_text": "big bench et_al et al 2022 diverse evaluation suite focuses tasks believed capabilities current language_models language language_models language good progress benchmark best big bench paper outperforming average reported human results 65 big bench tasks shot_prompting shot tasks language_models language fall short average human performance tasks actually current language_models language work focus suite 23 challenging big bench tasks big-bench_hard big-bench hard bbh task prior language_model language evaluations outperform average human find applying chain thought cot bbh tasks enables palm surpass average human performance 10 23 tasks codex code davinci-002 surpass average human performance 17 23 tasks tasks bbh require multi step reasoning shot_prompting shot cot big bench evaluations et_al et al 2022 substantially best performance capabilities language_models language better captured cot_prompting cot analysis explore interaction cot scale bbh finding cot enables emergent task performance bbh tasks flat scaling curves"}
{"id": "67daf8c4fe1958d20ebdf95c2a36dd490c73836f", "abstract": "Recent efforts have augmented language models (LMs) with external tools or environments, leading to the development of language agents that can reason and act. However, most of these agents rely on few-shot prompting techniques with off-the-shelf LMs. In this paper, we investigate and argue for the overlooked direction of fine-tuning LMs to obtain language agents. Using a setup of question answering (QA) with a Google search API, we explore a variety of base LMs, prompting methods, fine-tuning data, and QA tasks, and find language agents are consistently improved after fine-tuning their backbone LMs. For example, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4 leads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct, a novel approach to fine-tuning LMs with trajectories from multiple tasks and prompting methods, and show having more diverse fine-tuning data can further improve agents. Along with other findings regarding scaling effects, robustness, generalization, efficiency and cost, our work establishes comprehensive benefits of fine-tuning LMs for agents, and provides an initial set of experimental designs, insights, as well as open questions toward language agent fine-tuning.", "title": "fireact toward language agent finetuning", "url": "https://arxiv.org/pdf/2310.05915", "tokenized_text": "recent efforts augmented language_models language lms external tools environments leading development language agents reason act agents rely shot_prompting shot techniques shelf lms paper investigate argue overlooked direction fine tuning lms obtain language agents setup question_answering question answering qa google search api explore variety base lms methods fine tuning data qa tasks find language agents consistently improved fine tuning backbone lms example fine tuning llama2 7b 500 agent trajectories generated gpt-4 leads 77 hotpotqa performance increase furthermore propose novel_approach novel approach fine tuning lms trajectories multiple tasks methods having diverse fine tuning data improve agents findings scaling effects robustness generalization efficiency cost work establishes comprehensive benefits fine tuning lms agents provides initial set experimental designs insights open questions language agent fine tuning"}
{"id": "68040213e9a83408cdc491ed3e235b52b537eed1", "abstract": "Natural language interfaces often require supervised data to translate user requests into programs, database queries, or other structured intent representations. During data collection, it can be difficult to anticipate and formalize the full range of user needs -- for example, in a system designed to handle simple requests (like $\\textit{find my meetings tomorrow}$ or $\\textit{move my meeting with my manager to noon})$, users may also express more elaborate requests (like $\\textit{swap all my calls on Monday and Tuesday}$). We introduce an approach for equipping a simple language-to-code model to handle complex utterances via a process of hierarchical natural language decomposition. Our approach uses a pre-trained language model to decompose a complex utterance into a sequence of smaller natural language steps, then interprets each step using the language-to-code model. To test our approach, we collect and release DeCU -- a new NL-to-program benchmark to evaluate Decomposition of Complex Utterances. Experiments show that the proposed approach enables the interpretation of complex utterances with almost no complex training data, while outperforming standard few-shot prompting approaches.", "title": "natural language decomposition and interpretation of complex utterances", "url": "http://arxiv.org/pdf/2305.08677", "tokenized_text": "natural_language natural language interfaces require supervised data translate user requests programs database queries structured intent representations data collection difficult anticipate formalize range user needs example system designed handle simple requests like meeting manager users express elaborate requests like calls introduce approach simple language code handle complex utterances process hierarchical natural_language natural language decomposition approach uses pre trained_language trained language decompose complex utterance sequence smaller natural_language natural language steps step language code test approach collect release new nl program benchmark evaluate decomposition complex utterances experiments proposed approach enables interpretation complex utterances complex training_data training data outperforming standard shot_prompting shot approaches"}
{"id": "6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7", "abstract": "Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\"few-shot prompting\"). Much of this success can be attributed to prompting methods such as\"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .", "title": "pal programaided language models", "url": "http://arxiv.org/pdf/2211.10435", "tokenized_text": "large_language large language llms recently demonstrated impressive ability perform arithmetic symbolic reasoning tasks provided examples test_time test time shot_prompting shot success attributed methods thought employ llms understanding problem description decomposing steps solving step problem llms adept sort step step decomposition llms logical arithmetic mistakes solution problem decomposed correctly paper present program aided language_models language pal novel_approach novel approach uses llm read natural_language natural language problems generate programs intermediate reasoning_steps reasoning steps solution step runtime python interpreter pal decomposing natural_language natural language problem runnable steps remains learning task llm solving delegated interpreter demonstrate synergy neural llm symbolic interpreter 13 mathematical symbolic algorithmic reasoning tasks big-bench_hard big-bench hard benchmarks natural_language natural language reasoning tasks generating code llm reasoning python interpreter leads accurate results larger example pal codex achieves_state achieves state art shot accuracy gsm8 benchmark math word problems surpassing uses chain thought absolute 15 top-1 code data publicly_available publicly available"}
{"id": "700da3f3758e053c379f905bebee261ba69f1073", "abstract": "In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for creating high-quality conversational agents without the need for fine-tuning. Our method utilizes pre-trained large language models (LLMs) as individual modules for long-term consistency and flexibility, by using techniques such as few-shot prompting, chain-of-thought (CoT), and external memory. Our human evaluation results show that MPC is on par with fine-tuned chatbot models in open-domain conversations, making it an effective solution for creating consistent and engaging chatbots.", "title": "prompted llms as chatbot modules for long opendomain conversation", "url": "http://arxiv.org/pdf/2305.04533", "tokenized_text": "paper propose mpc modular prompted chatbot new approach creating high quality conversational agents need fine tuning method utilizes pre trained large_language large language llms individual modules long term consistency flexibility techniques shot_prompting shot chain thought cot external memory human evaluation results mpc par fine tuned chatbot open domain conversations making effective solution creating consistent engaging chatbots"}
{"id": "711d5e8ddbb840ad31a9ffa3d38590603ba69a92", "abstract": "Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.", "title": "prompting gpt3 to be reliable", "url": "http://arxiv.org/pdf/2210.09150", "tokenized_text": "large_language large language llms impressive abilities shot_prompting shot apis openai_gpt-3 openai gpt-3 increase use real world language applications crucial problem improve reliability gpt-3 explored reliability broad defined term decompose reliability main facets existing framework ml safety recognized important generalizability social biases calibration factuality core contribution establish simple effective improve gpt-3 reliability generalizes distribution demographic distribution uses natural_language natural language instructions reduce social biases calibrates output probabilities updates llm factual knowledge reasoning chains appropriate gpt-3 reliable smaller scale supervised facets release processed datasets evaluation scripts predictions systematic empirical study sheds new insights reliability llms importantly strategies help practitioners reliably use llms like gpt-3"}
{"id": "72491b96d8a614d1a9a099707d44593d4b5a8f49", "abstract": "Large Language Models are affected by the phenomena of memorizing and forgetting their training data. But how do these vary by model size? We work towards this question by investigating how the model size affects the model's ability to discriminate a word's meaning in a given context. We introduce a dataset called DeltaWords, which evaluates a model's ability to follow instructions to select a sentence which replaces the target word with its antonym. We show a weak inverse scaling trend, where task accuracy degrades as model size increase, under extremely few-shot prompting regimes. We show that increasing the number of examples tend to disproportionately benefit larger models than smaller models.", "title": "understanding how model size affects fewshot instruction prompting", "url": "https://arxiv.org/pdf/2212.01907", "tokenized_text": "large_language large language affected phenomena memorizing forgetting training_data training data vary model_size size work question investigating model_size size affects ability discriminate word meaning given context introduce dataset called evaluates ability follow instructions select sentence replaces target word weak inverse scaling trend task accuracy degrades model_size size increase extremely shot_prompting shot regimes increasing number examples tend benefit larger smaller"}
{"id": "755853c6b30f5a186131e23a63c68a3f2737068e", "abstract": "In this work, we introduce SMART-LLM, an innovative framework designed for embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm. We create a benchmark dataset designed for validating the multi-robot task planning problem, encompassing four distinct categories of high-level instructions that vary in task complexity. Our evaluation experiments span both simulation and real-world scenarios, demonstrating that the proposed model can achieve promising results for generating multi-robot task plans. The experimental videos, code, and datasets from the work can be found at https://sites.google.com/view/smart-llm/.", "title": "smartllm smart multiagent robot task planning using large language models", "url": "https://arxiv.org/pdf/2309.10062", "tokenized_text": "work introduce smart llm innovative framework designed embodied multi robot task planning smart llm smart multi agent robot task planning large_language large language llms harnesses power llms convert high level task instructions provided input multi robot task plan accomplishes executing series stages including task decomposition formation task allocation guided programmatic llm shot_prompting shot paradigm create benchmark dataset designed validating multi robot task planning problem encompassing distinct categories high level instructions vary task complexity evaluation experiments span simulation real world_scenarios world scenarios demonstrating proposed achieve promising_results promising results generating multi robot task plans experimental videos code datasets work found"}
{"id": "75ce9634d281cc12cbe434f86c737df8e10796fa", "abstract": "Task-oriented dialogue (TOD) systems facilitate users in executing various activities via multi-turn dialogues, but Large Language Models (LLMs) often struggle to comprehend these intricate contexts. In this study, we propose a novel\"Self-Explanation\"prompting strategy to enhance the comprehension abilities of LLMs in multi-turn dialogues. This task-agnostic approach requires the model to analyze each dialogue utterance before task execution, thereby improving performance across various dialogue-centric tasks. Experimental results from six benchmark datasets confirm that our method consistently outperforms other zero-shot prompts and matches or exceeds the efficacy of few-shot prompts, demonstrating its potential as a powerful tool in enhancing LLMs' comprehension in complex dialogue tasks.", "title": "selfexplanation prompting improves dialogue understanding in large language models", "url": "https://arxiv.org/pdf/2309.12940", "tokenized_text": "task oriented dialogue tod systems facilitate users executing activities multi turn dialogues large_language large language llms struggle comprehend intricate contexts study propose strategy enhance comprehension abilities llms multi turn dialogues task agnostic approach requires analyze dialogue utterance task execution improving performance dialogue centric tasks experimental_results experimental results benchmark_datasets benchmark datasets confirm method consistently_outperforms consistently outperforms zero shot matches exceeds efficacy shot demonstrating potential powerful tool enhancing llms comprehension complex dialogue tasks"}
{"id": "7655f05cd394da6cb0f707068203c9ff05d8f05a", "abstract": "Large language models (LLMs) can be used to generate smaller, more refined datasets via few-shot prompting for benchmarking, fine-tuning or other use cases. However, understanding and evaluating these datasets is difficult, and the failure modes of LLM-generated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically. We present LinguisticLens, a novel inter-active visualization tool for making sense of and analyzing syntactic diversity of LLM-generated datasets. LinguisticLens clusters text along syntactic, lexical, and semantic axes. It supports hierarchical visualization of a text dataset, allowing users to quickly scan for an overview and inspect individual examples. The live demo is available at shorturl.at/zHOUV.", "title": "visualizing linguistic diversity of text datasets synthesized by large language models", "url": "https://arxiv.org/pdf/2305.11364", "tokenized_text": "large_language large language llms generate smaller refined datasets shot_prompting shot benchmarking fine tuning use cases understanding evaluating datasets difficult failure modes llm generated data understood specifically data repetitive surprising ways semantically syntactically lexically present novel inter active visualization tool making sense analyzing syntactic diversity llm generated datasets clusters text syntactic lexical semantic axes supports hierarchical visualization text dataset allowing users quickly scan overview individual examples live demo available"}
{"id": "7beec352ac2597c3cd3dc7aceb2f8cd068b72d15", "abstract": "Stories about everyday situations are an essential part of human communication, motivating the need to develop AI agents that can reliably understand these stories. Despite the long list of supervised methods for story completion and procedural understanding, current AI has no mechanisms to automatically track and explain procedures in unseen stories. To bridge this gap, we study the ability of AI models to transfer procedural knowledge to novel narrative tasks in a transparent manner. We design LEAP: a comprehensive framework that integrates state-of-the-art modeling architectures, training regimes, and augmentation strategies based on both natural and synthetic stories. To address the lack of densely annotated training data, we devise a robust automatic labeler based on few-shot prompting to enhance the augmented data. Our experiments with in- and out-of-domain tasks reveal insights into the interplay of different architectures, training regimes, and augmentation strategies. LEAP's labeler has a clear positive impact on out-of-domain datasets, while the resulting dense annotation provides native explainability.", "title": "transferring procedural knowledge across commonsense tasks", "url": "https://arxiv.org/pdf/2304.13867", "tokenized_text": "stories everyday situations essential human communication motivating need develop ai agents reliably understand stories despite long list supervised methods story completion procedural understanding current ai mechanisms automatically track explain procedures unseen stories bridge gap study ability ai transfer procedural knowledge novel narrative tasks transparent manner design leap comprehensive framework integrates state art modeling architectures training regimes augmentation strategies based natural synthetic stories address lack annotated training_data training data devise robust automatic based shot_prompting shot enhance augmented data experiments domain tasks reveal insights interplay different architectures training regimes augmentation strategies leap clear positive impact domain datasets resulting dense annotation provides native explainability"}
{"id": "7cf4f8cb8b4a373d869e785b79160dda7a49a250", "abstract": "We conduct a large empirical evaluation to investigate the landscape of distributional robustness in question answering. Our investigation spans over 350 models and 16 question answering datasets, including a diverse set of architectures, model sizes, and adaptation methods (e.g., fine-tuning, adapter tuning, in-context learning, etc.). We find that, in many cases, model variations do not affect robustness and in-distribution performance alone determines out-of-distribution performance. Moreover, our findings indicate that i) zero-shot and in-context learning methods are more robust to distribution shifts than fully fine-tuned models; ii) few-shot prompt fine-tuned models exhibit better robustness than few-shot fine-tuned span prediction models; iii) parameter-efficient and robustness enhancing training methods provide no significant robustness improvements. In addition, we publicly release all evaluations to encourage researchers to further analyze robustness trends for question answering models.", "title": "exploring the landscape of distributional robustness for question answering models", "url": "http://arxiv.org/pdf/2210.12517", "tokenized_text": "conduct large empirical evaluation investigate landscape distributional robustness question_answering question answering investigation spans 16 question_answering question answering datasets including diverse set architectures sizes adaptation methods e.g. fine tuning adapter tuning context_learning context learning etc find cases variations affect robustness distribution performance determines distribution performance findings indicate zero shot context_learning context learning methods robust distribution shifts fully fine tuned ii shot fine tuned exhibit better robustness shot fine tuned span prediction iii parameter efficient robustness enhancing training methods provide significant robustness improvements addition publicly release evaluations encourage researchers analyze robustness trends question_answering question answering"}
{"id": "7dc928f41e15f65f1267bd87b0fcfcc7e715cb56", "abstract": "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs -- e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always\"(A)\"-- which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations supporting those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. CoT is promising for explainability, but our results highlight the need for targeted efforts to evaluate and improve explanation faithfulness.", "title": "language models don't always say what they think unfaithful explanations in chainofthought prompting", "url": "http://arxiv.org/pdf/2305.04388", "tokenized_text": "large_language large language llms achieve strong performance tasks producing step step reasoning giving final output referred chain thought reasoning cot interpret cot explanations llm process solving task find cot explanations systematically misrepresent true reason prediction demonstrate cot explanations heavily influenced adding biasing features inputs e.g. multiple choice options shot answer systematically fail mention explanations bias incorrect answers frequently generate cot explanations supporting answers causes accuracy drop 36 suite 13 tasks big-bench_hard big-bench hard testing gpt-3.5 openai claude social bias task explanations justify giving answers line stereotypes mentioning influence social biases findings indicate cot explanations plausible misleading risks increasing trust llms safety cot promising explainability results highlight need targeted efforts evaluate improve explanation faithfulness"}
{"id": "7df3595bdb4003589e8ca1757cc39ec03a39a2ff", "abstract": "Language models (LMs) that jointly generate end-task answers as well as free-text rationales are known as self-rationalization models. Recent works demonstrate great performance gain for self-rationalization by few-shot prompting LMs with rationale-augmented exemplars. However, the ability to benefit from explanations only emerges with large-scale LMs, which have poor accessibility. In this work, we explore the less-studied setting of leveraging explanations for small LMs to improve few-shot self-rationalization. We first revisit the relationship between rationales and answers. Inspired by the implicit mental process of how human beings assess explanations, we present a novel approach, Zero-shot Augmentation of Rationale-Answer pairs (ZARA), to automatically construct pseudo-parallel data for self-training by reducing the problem of plausibility judgement to natural language inference. Experimental results show ZARA achieves SOTA performance on the FEB benchmark, for both the task accuracy and the explanation metric. In addition, we conduct human and quantitative evaluation validating ZARA's ability to automatically identify plausible and accurate rationale-answer pairs.", "title": "zara improving fewshot selfrationalization for small language models", "url": "http://arxiv.org/pdf/2305.07355", "tokenized_text": "language_models language lms jointly generate end task answers free text rationales known self rationalization recent works demonstrate great performance gain self rationalization shot_prompting shot lms rationale augmented exemplars ability benefit explanations emerges large scale lms poor accessibility work explore studied setting leveraging explanations small lms improve shot self rationalization revisit relationship rationales answers inspired implicit mental process human beings assess explanations present novel_approach novel approach zero shot augmentation rationale answer pairs automatically construct pseudo parallel data self training reducing problem judgement natural_language natural language inference experimental_results experimental results achieves sota performance benchmark task accuracy explanation metric addition conduct human quantitative evaluation validating ability automatically identify plausible accurate rationale answer pairs"}
{"id": "815c6ca281536d18ec0eb408b6e46e72a0826163", "abstract": "Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1078 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions. Arcade is publicly available at https://github.com/google-research/arcade-nl2code/.", "title": "natural language to code generation in interactive data science notebooks", "url": "http://arxiv.org/pdf/2212.09248", "tokenized_text": "computational interactive computing environments ubiquitous data scientists perform data analytic tasks measure performance ai pair programmers automatically synthesize programs tasks given natural_language natural language nl intents users build benchmark code_generation code generation problems pandas data analysis framework data science features multiple rounds nl code problems requires understand rich multi modal contexts existing cells execution states previous turns interaction establish strong baseline challenging task develop 62b code language_model language lm python computational significantly_outperforms significantly outperforms public code lms finally explore shot_prompting shot strategies elicit better code step step decomposition nl explanation showing potential improve diversity explainability predictions publicly_available publicly available"}
{"id": "82beb8a86d438e85a134182128d47607b1b04004", "abstract": "Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.", "title": "multiparty chat conversational agents in group settings with humans and models", "url": "http://arxiv.org/pdf/2304.13835", "tokenized_text": "current dialogue research primarily studies pairwise party conversations address everyday setting speakers converse work collect evaluate multi party conversations study general case use light environment construct grounded conversations participant assigned character role play evaluate ability language_models language act characters conversations require skills pairwise trained appear lack able decide talk producing coherent utterances grounded multiple characters compare trained new dataset existing pairwise trained dialogue large_language large language shot_prompting shot find new dataset publicly release help bring significant improvements group setting"}
{"id": "895f3c9e452ae51fb02786de424ce6d2bba11c3b", "abstract": "Hate speech causes widespread and deep-seated societal issues. Proper enforcement of hate speech laws is key for protecting groups of people against harmful and discriminatory language. However, determining what constitutes hate speech is a complex task that is highly open to subjective interpretations. Existing works do not align their systems with enforceable definitions of hate speech, which can make their outputs inconsistent with the goals of regulators. This research introduces a new perspective and task for enforceable hate speech detection centred around legal definitions, and a dataset annotated on violations of eleven possible definitions by legal experts. Given the challenge of identifying clear, legally enforceable instances of hate speech, we augment the dataset with expert-generated samples and an automatically mined challenge set. We experiment with grounding the model decision in these definitions using zero-shot and few-shot prompting. We then report results on several large language models (LLMs). With this task definition, automatic hate speech detection can be more closely aligned to enforceable laws, and hence assist in more rigorous enforcement of legal protections against harmful speech in public forums.", "title": "towards legally enforceable hate speech detection for public forums", "url": "http://arxiv.org/pdf/2305.13677", "tokenized_text": "hate speech causes widespread deep societal issues proper hate speech laws key groups people harmful language determining constitutes hate speech complex task highly open subjective interpretations existing works align systems definitions hate speech outputs inconsistent goals research introduces new perspective task hate speech detection centred legal definitions dataset annotated violations possible definitions legal experts given challenge identifying clear instances hate speech augment dataset expert generated samples automatically mined challenge set experiment grounding decision definitions zero shot shot_prompting shot report results large_language large language llms task definition automatic hate speech detection closely aligned laws assist rigorous legal harmful speech public forums"}
{"id": "8ab27849799286459465d2262f926354093b20a9", "abstract": "An abundance of datasets exist for training and evaluating models on the task of summary generation.However, these datasets are often derived heuristically, and lack sufficient annotations to support research into all aspects of summarization, such as evidence extraction and controllable summarization. We introduce a benchmark comprising 8 tasks that require multi-dimensional understanding of summarization, e.g., surfacing evidence for a summary, assessing its correctness, and gauging its relevance to different topics. We compare various methods on this benchmark and discover that on multiple tasks, moderately-sized fine-tuned models consistently outperform much larger few-shot prompted language models. For factuality related tasks, we also evaluate existing heuristics to create training data and find that training on them performs worse than training on $20\\times$ less human-labeled data. Our benchmark consists of data from 6 different domains, allowing us to study cross-domain performance of trained models. We find that for some tasks, the amount of training data matters more than the domain where it comes from, while for other tasks training specifically on data from the target domain, even if limited, is more beneficial. Our work fulfills the need for a well-annotated summarization benchmark with diverse tasks, and provides useful insights about the impact of the quality, size and domain of training data.", "title": "usb a unified summarization benchmark across tasks and domains", "url": "http://arxiv.org/pdf/2305.14296", "tokenized_text": "abundance datasets exist training evaluating task summary generation datasets derived heuristically lack sufficient annotations support research aspects summarization evidence extraction controllable summarization introduce benchmark comprising tasks require multi dimensional understanding summarization e.g. evidence summary assessing correctness relevance different topics compare methods benchmark discover multiple tasks moderately sized fine tuned consistently outperform larger shot prompted language_models language factuality related tasks evaluate existing heuristics create training_data training data find training performs worse training human labeled_data labeled data benchmark consists data different domains allowing study cross domain performance trained find tasks training_data training data matters domain comes tasks training specifically data target domain limited beneficial work fulfills need annotated summarization benchmark diverse tasks provides useful insights impact quality size domain training_data training data"}
{"id": "8f84dcbad8cd3b5b4d9229c56bc95f24be859a35", "abstract": "Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills. However, in practice, learning multi-task, language-conditioned robotic skills typically requires large-scale data collection and frequent human intervention to reset the environment or help correcting the current policies. In this work, we propose a novel approach to efficiently learn general-purpose language-conditioned robot skills from unstructured, offline and reset-free data in the real world by exploiting a self-supervised visuo-lingual affordance model, which requires annotating as little as 1% of the total data with language. We evaluate our method in extensive experiments both in simulated and real-world robotic tasks, achieving state-of-the-art performance on the challenging CALVIN benchmark and learning over 25 distinct visuomotor manipulation tasks with a single policy in the real world. We find that when paired with LLMs to break down abstract natural language instructions into subgoals via few-shot prompting, our method is capable of completing long-horizon, multi-tier tasks in the real world, while requiring an order of magnitude less data than previous approaches. Code and videos are available at http://hulc2.cs.uni-freiburg.de.", "title": "grounding language with visual affordances over unstructured data", "url": "https://arxiv.org/pdf/2210.01911", "tokenized_text": "recent works shown large_language large language llms applied ground natural_language natural language wide variety robot skills practice learning multi task language conditioned robotic skills typically requires large scale data collection frequent human intervention reset environment help correcting current policies work propose_a_novel propose novel approach efficiently learn general purpose language conditioned robot skills unstructured offline reset free data real_world real world exploiting self supervised lingual requires annotating little total data language evaluate method extensive_experiments extensive experiments simulated real world robotic tasks achieving state art performance challenging benchmark learning 25 distinct visuomotor manipulation tasks single policy real_world real world find paired llms break abstract natural_language natural language instructions shot_prompting shot method capable completing long horizon multi tier tasks real_world real world requiring order magnitude data previous approaches code videos available"}
{"id": "927fc7652e033c9eb17296df087e3e6491112bb0", "abstract": "Large Language Models (LLMs) have garnered considerable interest within both academic and industrial. Yet, the application of LLMs to graph data remains under-explored. In this study, we evaluate the capabilities of four LLMs in addressing several analytical problems with graph data. We employ four distinct evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification. Our results show that: 1) LLMs effectively comprehend graph data in natural language and reason with graph topology. 2) GPT models can generate logical and coherent results, outperforming alternatives in correctness. 3) All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity. 5) GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities. Notably, GPT-4 has demonstrated the capacity to rectify responses from GPT-3.5-turbo and its own previous iterations. The code is available at: https://github.com/Ayame1006/LLMtoGraph.", "title": "evaluating large language models on graphs performance insights and comparative analysis", "url": "https://arxiv.org/pdf/2308.11224", "tokenized_text": "large_language large language llms garnered considerable interest academic industrial application llms graph data remains explored study evaluate capabilities llms addressing analytical problems graph data employ distinct evaluation metrics fidelity rectification results llms effectively comprehend graph data natural_language natural language reason graph gpt generate logical coherent results outperforming alternatives correctness examined llms face challenges structural reasoning techniques like zero shot chain thought shot_prompting shot showing efficacy gpt produce erroneous answers multi answer tasks raising concerns fidelity gpt exhibit elevated confidence outputs potentially hindering rectification capacities notably gpt-4 demonstrated capacity rectify responses gpt-3.5 turbo previous iterations code_is_available code available"}
{"id": "97782a67971c4ff1a74bf07e82fe20b2c4bf86c4", "abstract": "Relation extraction (RE) is the core NLP task of inferring semantic relationships between entities from text. Standard supervised RE techniques entail training modules to tag tokens comprising entity spans and then predict the relationship between them. Recent work has instead treated the problem as a sequence-to-sequence task, linearizing relations between entities as target strings to be generated conditioned on the input. Here we push the limits of this approach, using larger language models (GPT-3 and Flan-T5 large) than considered in prior work and evaluating their performance on standard RE tasks under varying levels of supervision. We address issues inherent to evaluating generative approaches to RE by doing human evaluations, in lieu of relying on exact matching. Under this refined evaluation, we find that: (1) Few-shot prompting with GPT-3 achieves near SOTA performance, i.e., roughly equivalent to existing fully supervised models; (2) Flan-T5 is not as capable in the few-shot setting, but supervising and fine-tuning it with Chain-of-Thought (CoT) style explanations (generated via GPT-3) yields SOTA results. We release this model as a new baseline for RE tasks.", "title": "revisiting relation extraction in the era of large language models", "url": "http://arxiv.org/pdf/2305.05003", "tokenized_text": "relation_extraction relation extraction core nlp task inferring semantic relationships entities text standard supervised techniques entail training modules tag tokens comprising entity spans predict relationship recent_work recent work instead treated problem sequence sequence task relations entities target strings generated conditioned input push limits approach larger language_models language gpt-3 flan t5 large considered prior_work prior work evaluating performance standard tasks varying levels supervision address issues inherent evaluating generative approaches human evaluations relying exact matching refined evaluation find shot_prompting shot gpt-3 achieves near sota performance i.e. roughly equivalent existing fully_supervised fully supervised capable shot_setting shot setting fine tuning chain thought cot style explanations generated gpt-3 yields sota results release new baseline tasks"}
{"id": "9a9b1e2968302eb882870537d4af6e2c722dfd1a", "abstract": "Prompting methods such as Chain-of-Thought (CoT) have shed new light on enhancing the reasoning capabilities of large language models, and researchers have extensively explored the generation process of rationales and answers. However, they have overlooked the potential challenges posed by the poor quality of reasoning problems, which may influence the reasoning performance significantly. In this work, we propose Self-Polish (SP), a novel method that facilitates the model's problem-solving process by prompting them to progressively refine the given problems to be more comprehensible and solvable. Specifically, the method teaches models to eliminate irrelevant information, rearrange the logic structure and organize local conditions into new ones parallelly. SP is orthogonal to all other prompting methods, making it convenient to integrate with state-of-the-art techniques for further improvement. We conduct thorough experiments on five benchmarks to illustrate the effectiveness of the proposed method. For example, with Text-davinci-003, our method boosts the performance of standard few-shot prompting by $8.0\\%$ on GSM8K and $17.8\\%$ on MultiArith; it also improves the performance of CoT by $6.0\\%$ on GSM8K and $6.0\\%$ on MathQA, respectively. Furthermore, our method also showcases impressive performance on robustness evaluation.", "title": "selfpolish enhance reasoning in large language models via problem refinement", "url": "http://arxiv.org/pdf/2305.14497", "tokenized_text": "methods chain thought cot shed new light enhancing reasoning capabilities large_language large language researchers extensively explored generation process rationales answers overlooked potential challenges posed poor quality reasoning problems influence reasoning performance significantly work propose self polish sp novel method facilitates problem solving process progressively refine given problems comprehensible solvable specifically method eliminate irrelevant information logic structure organize local conditions new ones sp orthogonal methods making convenient integrate state art techniques improvement conduct thorough experiments benchmarks illustrate effectiveness proposed_method proposed method example text davinci-003 method boosts performance standard shot_prompting shot gsm8 multiarith improves performance cot gsm8 mathqa respectively furthermore method showcases impressive performance robustness evaluation"}
{"id": "9b9fb973e5d3b413baa90648d9eb0743bd889747", "abstract": "Mobile UI understanding is important for enabling various interaction tasks such as UI automation and accessibility. Previous mobile UI modeling often depends on the view hierarchy information of a screen, which directly provides the structural data of the UI, with the hope to bypass challenging tasks of visual modeling from screen pixels. However, view hierarchies are not always available, and are often corrupted with missing object descriptions or misaligned structure information. As a result, despite the use of view hierarchies could offer short-term gains, it may ultimately hinder the applicability and performance of the model. In this paper, we propose Spotlight, a vision-only approach for mobile UI understanding. Specifically, we enhance a vision-language model that only takes the screenshot of the UI and a region of interest on the screen -- the focus -- as the input. This general architecture of Spotlight is easily scalable and capable of performing a range of UI modeling tasks. Our experiments show that our model establishes SoTA results on several representative UI tasks and outperforms previous methods that use both screenshots and view hierarchies as inputs. Furthermore, we explore multi-task learning and few-shot prompting capacities of the proposed models, demonstrating promising results in the multi-task learning direction.", "title": "spotlight mobile ui understanding using visionlanguage models with a focus", "url": "http://arxiv.org/pdf/2209.14927", "tokenized_text": "mobile ui understanding important enabling interaction tasks ui automation accessibility previous mobile ui modeling depends view hierarchy information screen directly provides structural data ui hope bypass challenging tasks visual modeling screen pixels view hierarchies available corrupted missing object descriptions misaligned structure information result despite use view hierarchies offer short term gains ultimately hinder applicability performance paper propose vision approach mobile ui understanding specifically enhance vision language_model language takes ui region interest screen focus input general architecture easily scalable capable performing range ui modeling tasks experiments establishes sota results representative ui tasks outperforms previous methods use view hierarchies inputs furthermore explore multi task learning shot_prompting shot capacities proposed demonstrating promising_results promising results multi task learning direction"}
{"id": "9bf587d032e3764720cccd5beaf941f5c32880bc", "abstract": "Prompting is used to guide or steer a language model in generating an appropriate response that is consistent with the desired outcome. Chaining is a strategy used to decompose complex tasks into smaller, manageable components. In this study, we utilize prompt chaining for extensive legal document classification tasks, which present difficulties due to their intricate domain-specific language and considerable length. Our approach begins with the creation of a concise summary of the original document, followed by a semantic search for related exemplar texts and their corresponding annotations from a training corpus. Finally, we prompt for a label - based on the task - to assign, by leveraging the in-context learning from the few-shot prompt. We demonstrate that through prompt chaining, we can not only enhance the performance over zero-shot, but also surpass the micro-F1 score achieved by larger models, such as ChatGPT zero-shot, using smaller models.", "title": "large language model prompt chaining for long legal document classification", "url": "https://arxiv.org/pdf/2308.04138", "tokenized_text": "guide steer language_model language generating appropriate response consistent desired outcome chaining strategy decompose complex tasks smaller manageable components study utilize chaining extensive legal document classification tasks present difficulties intricate domain specific language considerable length approach begins creation concise summary original document followed semantic search related exemplar texts corresponding annotations training corpus finally label based task assign leveraging context_learning context learning shot demonstrate chaining enhance performance zero shot surpass micro f1_score f1 score achieved larger chatgpt zero shot smaller"}
{"id": "9c01786f8195d53ad3902fc8d0872784b059adf3", "abstract": "Large Language Models (LLMs) have the capacity of performing complex scheduling in a multi-agent system and can coordinate these agents into completing sophisticated tasks that require extensive collaboration. However, despite the introduction of numerous gaming frameworks, the community has insufficient benchmarks towards building general multi-agents collaboration infrastructure that encompass both LLM and human-NPCs collaborations. In this work, we propose a novel infrastructure - MindAgent - to evaluate planning and coordination emergent capabilities for gaming interaction. In particular, our infrastructure leverages existing gaming framework, to i) require understanding of the coordinator for a multi-agent system, ii) collaborate with human players via un-finetuned proper instructions, and iii) establish an in-context learning on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new gaming scenario and related benchmark that dispatch a multi-agent collaboration efficiency and supervise multiple agents playing the game simultaneously. We conduct comprehensive evaluations with new auto-metric CoS for calculating the collaboration efficiency. Finally, our infrastructure can be deployed into real-world gaming scenarios in a customized VR version of CUISINEWORLD and adapted in existing broader Minecraft gaming domain. We hope our findings on LLMs and the new infrastructure for general-purpose scheduling and coordination can help shed light on how such skills can be obtained by learning from large language corpora.", "title": "mindagent emergent gaming interaction", "url": "https://arxiv.org/pdf/2309.09971", "tokenized_text": "large_language large language llms capacity performing complex scheduling multi agent system coordinate agents completing sophisticated tasks require extensive collaboration despite introduction numerous gaming frameworks community insufficient benchmarks building general multi agents collaboration infrastructure encompass llm human work propose_a_novel propose novel infrastructure evaluate planning coordination emergent capabilities gaming interaction particular infrastructure leverages existing gaming framework require understanding coordinator multi agent system ii collaborate human players un finetuned proper instructions iii establish context_learning context learning shot feedback furthermore introduce new gaming scenario related benchmark multi agent collaboration efficiency supervise multiple agents playing game simultaneously conduct comprehensive evaluations new auto metric calculating collaboration efficiency finally infrastructure deployed real world gaming scenarios customized vr version adapted existing broader minecraft gaming domain hope findings llms new infrastructure general purpose scheduling coordination help shed light skills obtained learning large_language large language corpora"}
{"id": "a04883d1d780b438de6c127caf7ebe3d9233e193", "abstract": "Recently, large-scale pre-trained Vision and Language (VL) models have set a new state-of-the-art (SOTA) in zero-shot visual classification enabling open-vocabulary recognition of potentially unlimited set of categories defined as simple language prompts. However, despite these great advances, the performance of these zeroshot classifiers still falls short of the results of dedicated (closed category set) classifiers trained with supervised fine tuning. In this paper we show, for the first time, how to reduce this gap without any labels and without any paired VL data, using an unlabeled image collection and a set of texts auto-generated using a Large Language Model (LLM) describing the categories of interest and effectively substituting labeled visual instances of those categories. Using our label-free approach, we are able to attain significant performance improvements over the zero-shot performance of the base VL model and other contemporary methods and baselines on a wide variety of datasets, demonstrating absolute improvement of up to 11.7% (3.8% on average) in the label-free setting. Moreover, despite our approach being label-free, we observe 1.3% average gains over leading few-shot prompting baselines that do use 5-shot supervision.", "title": "lafter labelfree tuning of zeroshot classifier using language and unlabeled image collections", "url": "http://arxiv.org/pdf/2305.18287", "tokenized_text": "recently large scale pre trained vision language vl set new state art sota zero shot visual classification enabling open vocabulary recognition potentially unlimited set categories defined simple language despite great advances performance zeroshot classifiers falls short results dedicated closed category set classifiers trained supervised fine_tuning fine tuning paper time reduce gap labels paired vl data unlabeled image collection set texts auto generated large_language large language llm describing categories interest effectively substituting labeled visual instances categories label free approach able attain significant performance improvements zero shot performance base vl contemporary methods baselines wide variety datasets demonstrating absolute improvement average label free setting despite approach label free observe 1.3 average gains leading shot_prompting shot baselines use shot supervision"}
{"id": "a3a241e9397fe29b37f96cb5e8f4b8bebed3d3da", "abstract": "We introduce STREET, a unified multi-task and multi-domain natural language reasoning and explanation benchmark. Unlike most existing question-answering (QA) datasets, we expect models to not only answer questions, but also produce step-by-step structured explanations describing how premises in the question are used to produce intermediate conclusions that can prove the correctness of a certain answer. We perform extensive evaluation with popular language models such as few-shot prompting GPT-3 and fine-tuned T5. We find that these models still lag behind human performance when producing such structured reasoning steps. We believe this work will provide a way for the community to better train and test systems on multi-step reasoning and explanations in natural language.", "title": "street a multitask structured reasoning and explanation benchmark", "url": "http://arxiv.org/pdf/2302.06729", "tokenized_text": "introduce street unified multi task multi domain natural_language natural language reasoning explanation benchmark unlike existing question answering qa datasets expect answer questions produce step step structured explanations describing premises question produce intermediate conclusions prove correctness certain answer perform extensive evaluation popular language_models language shot_prompting shot gpt-3 fine tuned t5 find lag human performance producing structured reasoning_steps reasoning steps believe work provide way community better train test systems multi step reasoning explanations natural_language natural language"}
{"id": "a6a0963fcf21ed47a2616ca3980f8f4f21e6d5ad", "abstract": "Better understanding of Large Language Models' (LLMs) legal analysis abilities can contribute to improving the efficiency of legal services, governing artificial intelligence, and leveraging LLMs to identify inconsistencies in law. This paper explores LLM capabilities in applying tax law. We choose this area of law because it has a structure that allows us to set up automated validation pipelines across thousands of examples, requires logical reasoning and maths skills, and enables us to test LLM capabilities in a manner relevant to real-world economic lives of citizens and companies. Our experiments demonstrate emerging legal understanding capabilities, with improved performance in each subsequent OpenAI model release. We experiment with retrieving and utilising the relevant legal authority to assess the impact of providing additional legal context to LLMs. Few-shot prompting, presenting examples of question-answer pairs, is also found to significantly enhance the performance of the most advanced model, GPT-4. The findings indicate that LLMs, particularly when combined with prompting enhancements and the correct legal texts, can perform at high levels of accuracy but not yet at expert tax lawyer levels. As LLMs continue to advance, their ability to reason about law autonomously could have significant implications for the legal profession and AI governance.", "title": "large language models as tax attorneys a case study in legal capabilities emergence", "url": "http://arxiv.org/pdf/2306.07075", "tokenized_text": "better understanding large_language large language llms legal analysis abilities contribute improving efficiency legal services governing artificial_intelligence artificial intelligence leveraging llms identify inconsistencies law paper explores llm capabilities applying law choose area law structure allows set automated validation pipelines thousands examples requires logical reasoning skills enables test llm capabilities manner relevant real world economic lives companies experiments_demonstrate experiments demonstrate emerging legal understanding capabilities improved performance subsequent openai release experiment retrieving utilising relevant legal authority assess impact providing additional legal context llms shot_prompting shot presenting examples question answer pairs found significantly enhance performance advanced gpt-4 findings indicate llms particularly combined enhancements correct legal texts perform high levels accuracy expert levels llms continue advance ability reason law autonomously significant implications legal ai governance"}
{"id": "aad167be3c902388ea625da4117fcae4325b8b7d", "abstract": "Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100% of the dataset. We release the code at: https://github.com/google-research/distilling-step-by-step .", "title": "distilling stepbystep! outperforming larger language models with less training data and smaller model sizes", "url": "https://arxiv.org/pdf/2305.02301", "tokenized_text": "deploying large_language large language llms challenging memory inefficient compute intensive practical applications reaction researchers train smaller task specific finetuning human labels distilling llm generated labels finetuning distillation require large amounts training_data training data achieve comparable performance llms introduce distilling step step new mechanism trains smaller outperform llms achieves leveraging training_data training data needed finetuning distillation method extracts llm rationales additional supervision training small multi task framework present findings nlp benchmarks compared finetuning distillation mechanism achieves better performance fewer labeled unlabeled training_examples training examples second compared shot prompted llms achieve better performance substantially smaller sizes reduce model_size size data required outperform llms finetuned 770 t5 outperforms shot prompted 540b palm 80 available data benchmark standard finetuning t5 struggles match 100 dataset release code"}
{"id": "ac3cdb50606f7770eef8e4cd951840a4f71287a0", "abstract": "Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models\u2019 novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. We discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.", "title": "prompt programming for large language models beyond the fewshot paradigm", "url": "https://arxiv.org/pdf/2102.07350", "tokenized_text": "prevailing methods mapping large generative language_models language supervised tasks fail sufficiently probe novel capabilities gpt-3 case study shot significantly outperform shot suggest function shot examples cases better described locating learned task meta learning analysis motivates role controlling evaluating powerful language_models language discuss methods programming emphasizing usefulness considering lens natural_language natural language explore techniques exploiting capacity narratives cultural encode nuanced intentions techniques encouraging problem components producing informed encompassing theory programming introduce idea seeds generate natural_language natural language range tasks finally discuss general methods interacting language_models language incorporated existing future benchmarks practical applications"}
{"id": "b3d6fec3f1a878b0c612f0ffed820b045c2c46d8", "abstract": "Recent studies have demonstrated promising performance of ChatGPT and GPT-4 on several medical domain tasks. However, none have assessed its performance using a large-scale real-world electronic health record database, nor have evaluated its utility in providing clinical diagnostic assistance for patients across a full range of disease presentation. We performed two analyses using ChatGPT and GPT-4, one to identify patients with specific medical diagnoses using a real-world large electronic health record database and the other, in providing diagnostic assistance to healthcare workers in the prospective evaluation of hypothetical patients. Our results show that GPT-4 across disease classification tasks with chain of thought and few-shot prompting can achieve performance as high as 96% F1 scores. For patient assessment, GPT-4 can accurately diagnose three out of four times. However, there were mentions of factually incorrect statements, overlooking crucial medical findings, recommendations for unnecessary investigations and overtreatment. These issues coupled with privacy concerns, make these models currently inadequate for real world clinical use. However, limited data and time needed for prompt engineering in comparison to configuration of conventional machine learning workflows highlight their potential for scalability across healthcare applications.", "title": "the potential and pitfalls of using a large language model such as chatgpt or gpt4 as a clinical assistant", "url": "https://arxiv.org/pdf/2307.08152", "tokenized_text": "recent studies demonstrated promising performance chatgpt gpt-4 medical domain tasks assessed performance large scale real world electronic health record database evaluated utility providing clinical diagnostic assistance patients range disease presentation performed analyses chatgpt gpt-4 identify patients specific medical diagnoses real world large electronic health record database providing diagnostic assistance healthcare workers prospective evaluation patients results gpt-4 disease classification tasks chain_of_thought chain thought shot_prompting shot achieve performance high 96 f1 scores patient assessment gpt-4 accurately diagnose times mentions factually incorrect statements overlooking crucial medical findings recommendations unnecessary investigations issues coupled privacy concerns currently inadequate real_world real world clinical use limited data time needed prompt_engineering engineering comparison configuration conventional machine_learning machine learning workflows highlight potential scalability healthcare applications"}
{"id": "b4170009de40c1c46adea6a314734434ecd4b0dc", "abstract": "Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating sentences that contain idiomatic expressions.", "title": "do gpts produce less literal translations", "url": "http://arxiv.org/pdf/2305.16806", "tokenized_text": "large_language large language llms gpt-3 emerged general purpose language_models language capable addressing natural_language natural language generation understanding tasks task machine_translation machine translation mt multiple works investigated shot_prompting shot mechanisms elicit better translations llms relatively little investigation translations differ qualitatively translations generated standard neural machine_translation machine translation nmt work investigate differences terms translations produced systems measures involving word alignment find translations english gpts tend literal exhibiting similar better scores mt quality metrics demonstrate finding human evaluations differences especially pronounced translating sentences contain expressions"}
{"id": "b6bea98ca29267acbebca6cdf64eb07a5671e000", "abstract": "We propose Adversarial DEep Learning Transpiler (ADELT) for source-to-source transpilation between deep learning frameworks. Unlike prior approaches, we decouple the transpilation of code skeletons and the mapping of API keywords (an API function name or a parameter name). ADELT transpile code skeletons using few-shot prompting on big language models. Based on contextual embeddings extracted by a BERT for code, we train aligned API embeddings in a domain-adversarial setup, upon which we generate a dictionary for keyword translation. The model is trained on our unlabeled DL corpus from web crawl data, without using any hand-crafted rules and parallel data. Our method outperforms state-of-the-art transpilers on multiple transpilation pairs including PyTorch-Keras and PyTorch-MXNet by 15.9pts and 12.0pts in exact match scores respectively.", "title": "adelt transpilation between deep learning frameworks", "url": "http://arxiv.org/pdf/2303.03593", "tokenized_text": "propose adversarial deep learning source source deep learning frameworks unlike prior approaches decouple code mapping api keywords api function parameter code shot_prompting shot big language_models language based contextual embeddings extracted bert code train aligned api embeddings domain adversarial setup generate dictionary keyword translation trained unlabeled dl corpus web data hand crafted rules parallel data method outperforms state art multiple pairs including pytorch pytorch exact match scores respectively"}
{"id": "b6e5855b6a4e425ba251a93516f2bccffe5ba403", "abstract": "This study investigates machine translation between related languages i.e., languages within the same family that share linguistic characteristics such as word order and lexical similarity. Machine translation through few-shot prompting leverages a small set of translation pair examples to generate translations for test sentences. This procedure requires the model to learn how to generate translations while simultaneously ensuring that token ordering is maintained to produce a fluent and accurate translation. We propose that for related languages, the task of machine translation can be simplified by leveraging the monotonic alignment characteristic of such languages. We introduce DecoMT, a novel approach of few-shot prompting that decomposes the translation process into a sequence of word chunk translations. Through automatic and human evaluation conducted on multiple related language pairs across various language families, we demonstrate that our proposed approach of decomposed prompting surpasses multiple established few-shot baseline approaches. For example, DecoMT outperforms the strong few-shot prompting BLOOM model with an average improvement of 8 chrF++ scores across the examined languages.", "title": "decomposed prompting for machine translation between related languages using large language models", "url": "http://arxiv.org/pdf/2305.13085", "tokenized_text": "study investigates machine_translation machine translation related languages i.e. languages family share linguistic characteristics word order lexical similarity machine_translation machine translation shot_prompting shot leverages small set translation pair examples generate translations test sentences procedure requires learn generate translations simultaneously ensuring token ordering produce fluent accurate translation propose related languages task machine_translation machine translation simplified leveraging alignment languages introduce novel_approach novel approach shot_prompting shot decomposes translation process sequence word chunk translations automatic human evaluation conducted multiple related language pairs language families demonstrate proposed approach decomposed surpasses multiple established shot baseline approaches example outperforms strong shot_prompting shot bloom average improvement chrf++ scores examined languages"}
{"id": "b70075b496c1f519093884945be5670c32cbceed", "abstract": "Large language models (LLMs) pre-trained on vast internet-scale data have showcased remarkable capabilities across diverse domains. Recently, there has been escalating interest in deploying LLMs for robotics, aiming to harness the power of foundation models in real-world settings. However, this approach faces significant challenges, particularly in grounding these models in the physical world and in generating dynamic robot motions. To address these issues, we introduce a novel paradigm in which we use few-shot prompts collected from the physical environment, enabling the LLM to autoregressively generate low-level control commands for robots without task-specific fine-tuning. Experiments across various robots and environments validate that our method can effectively prompt a robot to walk. We thus illustrate how LLMs can proficiently function as low-level feedback controllers for dynamic motion control even in high-dimensional robotic systems. The project website and source code can be found at: https://prompt2walk.github.io/ .", "title": "prompt a robot to walk with large language models", "url": "https://arxiv.org/pdf/2309.09969", "tokenized_text": "large_language large language llms pre trained vast internet scale data showcased remarkable_capabilities remarkable capabilities diverse domains recently escalating interest deploying llms robotics aiming harness power foundation_models foundation real world settings approach faces significant challenges particularly grounding physical world generating dynamic robot motions address issues introduce novel paradigm use shot collected physical environment enabling llm autoregressively generate low level control commands robots task specific fine tuning experiments robots environments validate method effectively robot illustrate llms function low level feedback controllers dynamic motion control high dimensional robotic systems project website source_code source code found"}
{"id": "be177300487b6d0f25e6cade9a31900454b13281", "abstract": "Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.", "title": "freshllms refreshing large language models with search engine augmentation", "url": "https://arxiv.org/pdf/2310.03214", "tokenized_text": "large_language large language llms trained updated lack ability dynamically adapt changing world work perform detailed study factuality llm generated text context answering questions test current world knowledge specifically introduce novel dynamic qa benchmark encompassing diverse range question answer types including questions require fast changing world knowledge questions false premises need benchmark diverse array closed open source llms mode evaluation procedure allows measure correctness hallucination human evaluations involving 50 judgments shed light limitations demonstrate significant room improvement instance regardless model_size size struggle questions involve fast changing knowledge false premises motivated results present simple shot_prompting shot method substantially boosts performance llm incorporating relevant date information retrieved search engine experiments outperforms competing search engine augmented methods et_al et al 2022 commercial systems perplexity ai analysis reveals number retrieved evidences order play key role influencing correctness llm generated answers additionally instructing llm generate concise direct answers helps reduce hallucination compared encouraging verbose answers facilitate future work release commit updating regular"}
{"id": "c1647923704251875f4160e91b59afbbdc58483e", "abstract": "Whereas the recent emergence of large language models (LLMs) like ChatGPT has exhibited impressive general performance, it still has a large gap with fully-supervised models on specific tasks such as multi-span question answering. Previous researches found that in-context learning is an effective approach to exploiting LLM, by using a few task-related labeled data as demonstration examples to construct a few-shot prompt for answering new questions. A popular implementation is to concatenate a few questions and their correct answers through simple templates, informing LLM of the desired output. In this paper, we propose a novel way of employing labeled data such that it also informs LLM of some undesired output, by extending demonstration examples with feedback about answers predicted by an off-the-shelf model, e.g., correct, incorrect, or incomplete. Experiments on three multi-span question answering datasets as well as a keyphrase extraction dataset show that our new prompting strategy consistently improves LLM's in-context learning performance.", "title": "enhancing incontext learning with answer feedback for multispan question answering", "url": "http://arxiv.org/pdf/2306.04508", "tokenized_text": "recent emergence large_language large language llms like_chatgpt like chatgpt exhibited impressive general performance large gap fully supervised specific tasks multi span question_answering question answering previous researches found context_learning context learning effective approach exploiting llm task related labeled_data labeled data demonstration examples construct shot answering new questions popular implementation concatenate questions correct answers simple templates informing llm desired output paper propose_a_novel propose novel way employing labeled_data labeled data informs llm undesired output extending demonstration examples feedback answers predicted shelf e.g. correct incorrect incomplete experiments multi span question_answering question answering datasets extraction dataset new strategy consistently improves llm context_learning context learning performance"}
{"id": "c2391a8c8e24a450f00810ecb441e26413ea3791", "abstract": "Large Language Models (LLM) are a new class of computation engines,\"programmed\"via prompt engineering. We are still learning how to best\"program\"these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously have a collection of semantics facts in mind when working on coding tasks. Mostly these are shallow, simple facts arising from a quick read. For a function, examples of facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc. One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them inherently capable of doing this simple level of\"code analysis\"and extracting such information, implicitly, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM's prompt with semantic facts explicitly, actually helps. Prior work shows that LLM performance on code summarization benefits from few-shot samples drawn either from the same-project or from examples found via information retrieval methods (such as BM25). While summarization performance has steadily increased since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization. We find that adding semantic facts actually does help! This approach improves performance in several different settings suggested by prior work, including for two different Large Language Models. In most cases, improvement nears or exceeds 2 BLEU; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU.", "title": "improving fewshot prompts with relevant static analysis products", "url": "https://arxiv.org/pdf/2304.06815", "tokenized_text": "large_language large language llm new class computation prompt_engineering engineering learning llms help developers start intuition developers tend collection semantics facts mind working coding tasks shallow simple facts arising quick read function examples facts include parameter local variable names return expressions simple post conditions basic control data flow etc assume powerful multi layer architecture transformer style llms makes inherently capable simple level extracting information implicitly processing code explicitly adding information help goal investigate question code summarization task evaluate automatically augmenting llm semantic facts explicitly actually helps prior_work prior work shows llm performance code summarization benefits shot samples drawn project examples found information retrieval methods bm25 summarization performance steadily increased early days room improvement llm performance code summarization lags performance natural language tasks like translation text summarization find adding semantic facts actually help approach improves performance different settings suggested prior_work prior work including different large_language large language cases improvement exceeds bleu language challenging dataset augmentation actually yields performance surpassing 30 bleu"}
{"id": "c5fa70db839fd05b1111f3586a601d8a93e78d0c", "abstract": "With large Foundation Models (FMs), language technologies (AI in general) are entering a new paradigm: eliminating the need for developing large-scale task-specific datasets and supporting a variety of tasks through set-ups ranging from zero-shot to few-shot learning. However, understanding FMs capabilities requires a systematic benchmarking effort by comparing FMs performance with the state-of-the-art (SOTA) task-specific models. With that goal, past work focused on the English language and included a few efforts with multiple languages. Our study contributes to ongoing research by evaluating FMs performance for standard Arabic NLP and Speech processing, including a range of tasks from sequence tagging to content classification across diverse domains. We start with zero-shot learning using GPT-3.5-turbo, Whisper, and USM, addressing 33 unique tasks using 59 publicly available datasets resulting in 96 test setups. For a few tasks, FMs performs on par or exceeds the performance of the SOTA models but for the majority it under-performs. Given the importance of prompt for the FMs performance, we discuss our prompt strategies in detail and elaborate on our findings. Our future work on Arabic AI will explore few-shot prompting, expand the range of tasks, and investigate additional open-source models.", "title": "benchmarking arabic ai with large language models", "url": "http://arxiv.org/pdf/2305.14982", "tokenized_text": "large foundation_models foundation fms language technologies ai general entering new_paradigm new paradigm eliminating need developing large scale task specific datasets supporting variety tasks set ranging zero shot shot_learning shot learning understanding fms capabilities requires systematic benchmarking effort comparing fms performance state art sota task specific goal past work focused english language included efforts multiple languages study contributes ongoing research evaluating fms performance standard arabic nlp speech processing including range tasks sequence tagging content classification diverse domains start zero shot_learning shot learning gpt-3.5 turbo whisper addressing 33 unique tasks 59 publicly_available publicly available datasets resulting 96 test setups tasks fms performs par exceeds performance sota majority performs given importance fms performance discuss strategies detail elaborate findings future work arabic ai explore shot_prompting shot expand range tasks investigate additional open source"}
{"id": "c70eb74e09c41e8fcc71dd59e3b4d631f657f7cd", "abstract": "In this work, we aim to capitalize on the unique few-shot capabilities of large-scale language models (LSLMs) to overcome some of their challenges with respect to grounding to factual and up-to-date information. Motivated by semi-parametric language models (LMs), which ground their decisions in external retrieved evidence, we use few-shot prompting to learn to condition LMs on information returned from the web using Google Search, a broad and constantly updated knowledge source. Our approach does not involve fine-tuning or learning additional parameters, thus making it applicable to any LM, offering therefore a strong baseline. Indeed, we find that LMs conditioned on the web surpass performance of closed-book models of similar, or even larger, model sizes in open-domain question answering. Finally, we find that increasing the inference-time compute of models, achieved via using multiple retrieved evidences to generate multiple answers followed by a reranking stage that uses scores generated by the same LMs, leads to better performance and alleviates lower performance of smaller few-shot LMs. All in all, our findings suggest that it might be beneficial to slow down the race towards the biggest model and instead shift attention towards finding more effective ways to use models, including but not limited to, better prompting or increasing inference-time compute.", "title": "internetaugmented language models through fewshot prompting for opendomain question answering", "url": "https://arxiv.org/pdf/2203.05115", "tokenized_text": "work aim capitalize unique shot capabilities large scale language_models language lslms overcome challenges respect grounding factual date information motivated semi parametric language_models language lms ground decisions external retrieved evidence use shot_prompting shot learn condition lms information web google search broad constantly updated knowledge source approach involve fine tuning learning additional parameters making applicable lm offering strong baseline find lms conditioned web surpass performance closed book similar larger sizes open domain question_answering question answering finally find increasing inference time compute achieved multiple retrieved evidences generate multiple answers followed reranking stage uses scores generated lms leads better performance alleviates lower performance smaller shot lms findings_suggest findings suggest beneficial slow race biggest instead shift attention finding effective ways use including limited better increasing inference time compute"}
{"id": "ca7bd64d372e3bcb3f4633ca4a20291ff57de3c3", "abstract": "Recommendation systems have witnessed significant advancements and have been widely used over the past decades. However, most traditional recommendation methods are task-specific and therefore lack efficient generalization ability. Recently, the emergence of ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. Nonetheless, the application of ChatGPT in the recommendation domain has not been thoroughly investigated. In this paper, we employ ChatGPT as a general-purpose recommendation model to explore its potential for transferring extensive linguistic and world knowledge acquired from large-scale corpora to recommendation scenarios. Specifically, we design a set of prompts and evaluate ChatGPT's performance on five recommendation scenarios. Unlike traditional recommendation methods, we do not fine-tune ChatGPT during the entire evaluation process, relying only on the prompts themselves to convert recommendation tasks into natural language tasks. Further, we explore the use of few-shot prompting to inject interaction information that contains user potential interest to help ChatGPT better understand user needs and interests. Comprehensive experimental results on Amazon Beauty dataset show that ChatGPT has achieved promising results in certain tasks and is capable of reaching the baseline level in others. We conduct human evaluations on two explainability-oriented tasks to more accurately evaluate the quality of contents generated by different models. And the human evaluations show ChatGPT can truly understand the provided information and generate clearer and more reasonable results. We hope that our study can inspire researchers to further explore the potential of language models like ChatGPT to improve recommendation performance and contribute to the advancement of the recommendation systems field.", "title": "is chatgpt a good recommender a preliminary study", "url": "http://arxiv.org/pdf/2304.10149", "tokenized_text": "recommendation systems witnessed significant advancements widely past decades traditional recommendation methods task specific lack efficient generalization_ability generalization ability recently emergence chatgpt significantly advanced nlp_tasks nlp tasks enhancing capabilities conversational nonetheless application chatgpt recommendation domain thoroughly investigated paper employ chatgpt general purpose recommendation explore potential transferring extensive linguistic world knowledge acquired large scale corpora recommendation scenarios specifically design set evaluate chatgpt performance recommendation scenarios unlike traditional recommendation methods fine tune chatgpt entire evaluation process relying convert recommendation tasks natural_language natural language tasks explore use shot_prompting shot inject interaction information contains user potential interest help chatgpt better understand user needs interests comprehensive experimental_results experimental results amazon dataset chatgpt achieved promising_results promising results certain tasks capable reaching baseline level conduct human evaluations explainability oriented tasks accurately evaluate quality contents generated different human evaluations chatgpt truly understand provided information generate reasonable results hope study inspire researchers explore potential language_models language like_chatgpt like chatgpt improve recommendation performance contribute advancement recommendation systems field"}
{"id": "cc43306e22dbfd5bc35251ab8c8ba37e4fc2a1b3", "abstract": "Large language models that are capable of zero or few-shot prompting approaches have given rise to the new research area of prompt engineering. Recent advances showed that for example Chain-of-Thought (CoT) prompts can improve arithmetic or common sense tasks significantly. We explore how such approaches fare with legal reasoning tasks and take the COLIEE entailment task based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning approaches. Our findings show that while CoT prompting and fine-tuning with explanations approaches show improvements, the best results are produced by prompts that are derived from specific legal reasoning techniques such as IRAC (Issue, Rule, Application, Conclusion). Based on our experiments we improve the 2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best system of 0.6789 accuracy with an accuracy of 0.7431.", "title": "legal prompting teaching a language model to think like a lawyer", "url": "http://arxiv.org/pdf/2212.01326", "tokenized_text": "large_language large language capable zero shot_prompting shot approaches given rise new research area prompt_engineering engineering recent_advances recent advances showed example chain thought cot improve arithmetic common sense tasks significantly explore approaches legal reasoning tasks entailment task based japanese bar exam testing zero shot shot fine tuning approaches findings cot_prompting cot fine tuning explanations approaches improvements best results produced derived specific legal reasoning techniques issue conclusion based experiments improve 2021 best result accuracy accuracy beat 2022 best system accuracy accuracy"}
{"id": "ccc772d88c231275f24c4fac9b28bbe0942e1107", "abstract": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.", "title": "query2doc query expansion with large language models", "url": "https://arxiv.org/pdf/2303.07678", "tokenized_text": "paper introduces simple effective query expansion approach denoted improve sparse dense retrieval systems proposed_method proposed method generates pseudo documents shot_prompting shot large_language large language llms expands query generated pseudo documents llms trained web scale text corpora adept knowledge memorization pseudo documents llms contain highly relevant information aid query disambiguation guide retrievers experimental_results experimental results demonstrate boosts performance bm25 15 ad hoc ir datasets ms marco trec dl fine tuning furthermore method benefits state art dense retrievers terms domain domain results"}
{"id": "cd77ea482d9245f3fcaeb670261a00c3fb5cabbd", "abstract": "The recently released ChatGPT has demonstrated surprising abilities in natural language understanding and natural language generation. Machine translation relies heavily on the abilities of language understanding and generation. Thus, in this paper, we explore how to assist machine translation with ChatGPT. We adopt several translation prompts on a wide range of translations. Our experimental results show that ChatGPT with designed translation prompts can achieve comparable or better performance over commercial translation systems for high-resource language translations. We further evaluate the translation quality using multiple references, and ChatGPT achieves superior performance compared to commercial systems. We also conduct experiments on domain-specific translations, the final results show that ChatGPT is able to comprehend the provided domain keyword and adjust accordingly to output proper translations. At last, we perform few-shot prompts that show consistent improvement across different base prompts. Our work provides empirical evidence that ChatGPT still has great potential in translations.", "title": "how to design translation prompts for chatgpt an empirical study", "url": "http://arxiv.org/pdf/2304.02182", "tokenized_text": "recently released chatgpt demonstrated surprising abilities natural_language natural language understanding natural_language natural language generation machine_translation machine translation relies heavily abilities language understanding generation paper explore assist machine_translation machine translation chatgpt adopt translation wide_range wide range translations experimental_results experimental results chatgpt designed translation achieve comparable better performance commercial translation systems high resource language translations evaluate translation quality multiple references chatgpt achieves superior_performance superior performance compared commercial systems conduct experiments domain specific translations final results chatgpt able comprehend provided domain keyword adjust accordingly output proper translations perform shot consistent improvement different base work provides empirical evidence chatgpt great_potential great potential translations"}
{"id": "ce0154d9251f67c262512b6e598f3aa3ba9fe9a4", "abstract": "What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training. We then show that strategies for causal intervention and exploitation can be generalized from passive data even in a more complex environment with high-dimensional observations, with the support of natural language explanations. Explanations can even allow passive learners to generalize out-of-distribution from perfectly-confounded training data. Finally, we show that language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing examples of experimentation, together with explanations and reasoning. These results highlight the surprising power of passive learning of active causal strategies, and may help to understand the behaviors and capabilities of language models.", "title": "passive learning of active causal strategies in agents and language models", "url": "https://arxiv.org/pdf/2305.16183", "tokenized_text": "learned causality experimentation passive data question salient given recent successes trained_language trained language interactive domains tool use passive learning inherently limited purely passive learning fact allow agent learn generalizable strategies determining causal structures long agent intervene test_time test time formally illustrate learning strategy experimenting seeking goals allow generalization passive learning principle empirically agents trained imitation expert data generalize test_time test time infer use causal links present training_data training data agents generalize experimentation strategies novel variable sets observed training strategies causal intervention exploitation generalized passive data complex environment high dimensional observations support natural_language natural language explanations explanations allow passive learners generalize distribution perfectly confounded training_data training data finally language_models language trained passive word prediction generalize causal intervention strategies shot containing examples experimentation explanations reasoning results highlight surprising power passive learning active causal strategies help understand behaviors capabilities language_models language"}
{"id": "d4fc988c6510420a5290dfe8d1a991ca4878d696", "abstract": "Error prediction in large language models often relies on domain-specific information. In this paper, we present measures for quantification of error in the response of a large language model based on the diversity of responses to a given prompt - hence independent of the underlying application. We describe how three such measures - based on entropy, Gini impurity, and centroid distance - can be employed. We perform a suite of experiments on multiple datasets and temperature settings to demonstrate that these measures strongly correlate with the probability of failure. Additionally, we present empirical results demonstrating how these measures can be applied to few-shot prompting, chain-of-thought reasoning, and error detection.", "title": "diversity measures domainindependent proxies for failure in language model queries", "url": "https://arxiv.org/pdf/2308.11189", "tokenized_text": "error prediction large_language large language relies domain specific information paper present measures quantification error response large_language large language based diversity responses given independent underlying application describe measures based entropy distance employed perform suite experiments multiple datasets temperature settings demonstrate measures strongly correlate probability failure additionally present empirical results demonstrating measures applied shot_prompting shot chain thought reasoning error detection"}
{"id": "d589c49e1cd1dd3b994dcac01b4c6e7fb8eef161", "abstract": "Software logs play an essential role in ensuring the reliability and maintainability of large-scale software systems, as they are often the sole source of runtime information. Log parsing, which converts raw log messages into structured data, is an important initial step towards downstream log analytics. In recent studies, ChatGPT, the current cutting-edge large language model (LLM), has been widely applied to a wide range of software engineering tasks. However, its performance in automated log parsing remains unclear. In this paper, we evaluate ChatGPT's ability to undertake log parsing by addressing two research questions. (1) Can ChatGPT effectively parse logs? (2) How does ChatGPT perform with different prompting methods? Our results show that ChatGPT can achieve promising results for log parsing with appropriate prompts, especially with few-shot prompting. Based on our findings, we outline several challenges and opportunities for ChatGPT-based log parsing.", "title": "log parsing how far can chatgpt go", "url": "https://arxiv.org/pdf/2306.01590", "tokenized_text": "software logs play essential role ensuring reliability large scale software systems source runtime information log parsing converts raw log messages structured data important initial step downstream log analytics recent studies chatgpt current cutting edge large_language large language llm widely applied wide_range wide range software engineering tasks performance automated log parsing remains unclear paper evaluate chatgpt ability undertake log parsing addressing research questions chatgpt effectively parse logs chatgpt perform different methods results chatgpt achieve promising_results promising results log parsing appropriate especially shot_prompting shot based findings outline challenges opportunities chatgpt based log_parsing log parsing"}
{"id": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc", "abstract": "Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduced two new types of prompts, namely heuristic prompting and ensemble prompting. We evaluated the performance of these prompts on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted zero-shot prompting with few-shot prompting, and provide novel insights and guidelines for prompt engineering for LLMs in clinical NLP. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative AI, and we hope that it will inspire and inform future research in this area.", "title": "an empirical evaluation of prompting strategies for large language models in zeroshot clinical natural language processing", "url": "https://arxiv.org/pdf/2309.08008", "tokenized_text": "large_language large language llms shown remarkable_capabilities remarkable capabilities natural_language natural language processing nlp especially domains labeled_data labeled data scarce expensive clinical domain unlock clinical knowledge hidden llms need design effective guide perform specific clinical nlp_tasks nlp tasks task specific training_data training data known context_learning context learning art science requires understanding strengths weaknesses different llms prompt_engineering engineering approaches paper present comprehensive systematic experimental study prompt_engineering engineering clinical nlp_tasks nlp tasks clinical sense disambiguation biomedical evidence extraction coreference resolution status extraction attribute extraction assessed proposed recent literature including simple prefix simple cloze chain_of_thought chain thought introduced new types heuristic ensemble evaluated performance state art llms gpt-3.5 bard llama2 contrasted zero shot_prompting shot shot_prompting shot provide novel insights guidelines prompt_engineering engineering llms clinical nlp best knowledge works empirical evaluation different prompt_engineering engineering approaches clinical nlp era generative_ai generative ai hope inspire inform future_research future research area"}
{"id": "dca6c3927ade6481a1ae080f5c24decbfeced1be", "abstract": "Methods such as chain-of-thought prompting and self-consistency have pushed the frontier of language model reasoning performance with no additional training. To further improve performance, we propose a prompt ensembling method for large language models, which uses a small dataset to construct a set of few shot prompts that together comprise a ``boosted prompt ensemble''. The few shot examples for each prompt are chosen in a stepwise fashion to be ``hard'' examples on which the previous step's ensemble is uncertain. We show that this outperforms single-prompt output-space ensembles and bagged prompt-space ensembles on the GSM8k and AQuA datasets, among others. We propose both train-time and test-time versions of boosted prompting that use different levels of available annotation and conduct a detailed empirical study of our algorithm.", "title": "boosted prompt ensembles for large language models", "url": "http://arxiv.org/pdf/2304.05970", "tokenized_text": "methods chain thought_prompting thought self consistency pushed frontier language_model language reasoning performance additional training improve performance propose ensembling method large_language large language uses small dataset construct set shot boosted ensemble shot examples chosen fashion hard examples previous step ensemble uncertain outperforms single output space ensembles space ensembles gsm8k aqua datasets propose train time test time versions boosted use different levels available annotation conduct detailed empirical study algorithm"}
{"id": "dda0f7f086fc875d583604f8b0cf4a8678bc4de4", "abstract": "Despite cross-lingual generalization demonstrated by pre-trained multilingual models, the translate-train paradigm of transferring English datasets across multiple languages remains to be a key mechanism for training task-specific multilingual models. However, for many low-resource languages, the availability of a reliable translation service entails significant amounts of costly human-annotated translation pairs. Further, translation services may continue to be brittle due to domain mismatch between task-specific input text and general-purpose text used for training translation models. For multilingual semantic parsing, we demonstrate the effectiveness and flexibility offered by large language models (LLMs) for translating English datasets into several languages via few-shot prompting. Through extensive comparisons on two public datasets, MTOP and MASSIVE, spanning 50 languages and several domains, we show that our method of translating data using LLMs outperforms a strong translate-train baseline on 41 out of 50 languages. We study the key design choices that enable more effective multilingual data translation via prompted LLMs.", "title": "bootstrapping multilingual semantic parsers using large language models", "url": "http://arxiv.org/pdf/2210.07313", "tokenized_text": "despite cross lingual generalization demonstrated pre trained multilingual translate train paradigm transferring english datasets multiple languages remains key mechanism training task specific multilingual low resource_languages resource languages availability reliable translation service entails significant amounts costly human annotated translation pairs translation services continue brittle domain mismatch task specific input text general purpose text training translation multilingual semantic_parsing semantic parsing demonstrate_the_effectiveness demonstrate effectiveness flexibility offered large_language large language llms translating english datasets languages shot_prompting shot extensive comparisons public datasets mtop massive spanning 50 languages domains method translating data llms outperforms strong translate train baseline 50 languages study key design choices enable effective multilingual data translation prompted llms"}
{"id": "e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43", "abstract": "Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.", "title": "prompt2model generating deployable models from natural language instructions", "url": "https://arxiv.org/pdf/2308.12261", "tokenized_text": "large_language large language llms enable system today create competent nlp systems need describe task natural_language natural language provide examples ways llms step backward traditional special purpose nlp require extensive computational resources deployment gated apis paper propose general purpose method takes natural_language natural language task description like provided llms uses train special purpose conducive deployment multi step process retrieval existing datasets pretrained dataset generation llms supervised fine tuning retrieved generated datasets tasks demonstrate given shot input trains outperform results strong llm gpt-3.5 turbo average 20 times smaller data obtain reliable performance estimates performance enabling developers assess reliability deployment available open source"}
{"id": "eda54452d8a8a412c2a985ef11572cb468906b1f", "abstract": "Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current\"multilingualism\"in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.", "title": "multilingual large language models are not (yet) codeswitchers", "url": "http://arxiv.org/pdf/2305.14235", "tokenized_text": "multilingual large_language large language llms recently shown great capabilities wide_range wide range tasks exhibiting state art performance zero shot shot_prompting shot methods extensive studies abilities monolingual tasks investigation potential context code switching practice alternating languages utterance remains relatively paper provide comprehensive empirical analysis multilingual llms benchmarking performance tasks sentiment_analysis sentiment analysis machine_translation machine translation summarization word level language identification results_indicate results indicate despite multilingual llms exhibiting promising outcomes certain tasks zero shot_prompting shot underperform comparison fine tuned smaller scales argue llms inherently imply proficiency code switching texts calling future_research future research bridge discrepancy"}
{"id": "f00e7326baa9600e46b3a8e7077dc3a349f90a01", "abstract": "Structured product data in the form of attribute/value pairs is the foundation of many e-commerce applications such as faceted product search, product comparison, and product recommendation. Product offers often only contain textual descriptions of the product attributes in the form of titles or free text. Hence, extracting attribute/value pairs from textual product descriptions is an essential enabler for e-commerce applications. In order to excel, state-of-the-art product information extraction methods require large quantities of task-specific training data. The methods also struggle with generalizing to out-of-distribution attributes and attribute values that were not a part of the training data. Due to being pre-trained on huge amounts of text as well as due to emergent effects resulting from the model size, Large Language Models like ChatGPT have the potential to address both of these shortcomings. This paper explores the potential of ChatGPT for extracting attribute/value pairs from product descriptions. We experiment with different zero-shot and few-shot prompt designs. Our results show that ChatGPT achieves a performance similar to a pre-trained language model but requires much smaller amounts of training data and computation for fine-tuning.", "title": "product information extraction using chatgpt", "url": "http://arxiv.org/pdf/2306.14921", "tokenized_text": "structured product data form attribute value pairs foundation commerce applications product search product comparison product recommendation product offers contain textual descriptions product attributes form titles free text extracting attribute value pairs textual product descriptions essential enabler commerce applications order excel state art product information_extraction information extraction methods require large quantities task specific training_data training data methods struggle generalizing distribution attributes attribute values training_data training data pre trained huge amounts text emergent effects resulting model_size size large_language large language like_chatgpt like chatgpt potential address shortcomings paper explores potential chatgpt extracting attribute value pairs product descriptions experiment different zero shot shot designs results chatgpt achieves performance similar pre trained_language trained language requires smaller amounts training_data training data computation fine tuning"}
{"id": "f834aed32f5531bfa426faab71878c549572500e", "abstract": "Large language models (LLMs) have shown impressive capabilities in natural language understanding and generation. Their potential for deeper user understanding and improved personalized user experience on recommendation platforms is, however, largely untapped. This paper aims to address this gap. Recommender systems today capture users' interests through encoding their historical activities on the platforms. The generated user representations are hard to examine or interpret. On the other hand, if we were to ask people about interests they pursue in their life, they might talk about their hobbies, like I just started learning the ukulele, or their relaxation routines, e.g., I like to watch Saturday Night Live, or I want to plant a vertical garden. We argue, and demonstrate through extensive experiments, that LLMs as foundation models can reason through user activities, and describe their interests in nuanced and interesting ways, similar to how a human would. We define interest journeys as the persistent and overarching user interests, in other words, the non-transient ones. These are the interests that we believe will benefit most from the nuanced and personalized descriptions. We introduce a framework in which we first perform personalized extraction of interest journeys, and then summarize the extracted journeys via LLMs, using techniques like few-shot prompting, prompt-tuning and fine-tuning. Together, our results in prompting LLMs to name extracted user journeys in a large-scale industrial platform demonstrate great potential of these models in providing deeper, more interpretable, and controllable user understanding. We believe LLM powered user understanding can be a stepping stone to entirely new user experiences on recommendation platforms that are journey-aware, assistive, and enabling frictionless conversation down the line.", "title": "large language models for user interest journeys", "url": "http://arxiv.org/pdf/2305.15498", "tokenized_text": "large_language large language llms shown_impressive shown impressive capabilities natural_language natural language understanding generation potential deeper user understanding improved personalized user experience recommendation platforms largely untapped paper aims address gap recommender systems today capture users interests encoding historical activities platforms generated user representations hard examine interpret hand ask people interests pursue life talk like started learning routines e.g. like live want plant argue demonstrate extensive_experiments extensive experiments llms foundation_models foundation reason user activities describe interests nuanced interesting ways similar human define interest persistent user interests words non ones interests believe benefit nuanced personalized descriptions introduce framework perform personalized extraction interest summarize extracted llms techniques like shot_prompting shot tuning fine tuning results llms extracted user large scale industrial platform demonstrate great_potential great potential providing deeper interpretable controllable user understanding believe llm powered user understanding stepping entirely new user experiences recommendation platforms journey aware enabling conversation line"}
{"id": "386bd4d25043516f076ea7b2296a1ebec84f43ce", "abstract": "Social determinants of health (SDOH) documented in the electronic health record through unstructured text are increasingly being studied to understand how SDOH impacts patient health outcomes. In this work, we utilize the Social History Annotation Corpus (SHAC), a multi-institutional corpus of de-identified social history sections annotated for SDOH, including substance use, employment, and living status information. We explore the automatic extraction of SDOH information with SHAC in both standoff and inline annotation formats using GPT-4 in a one-shot prompting setting. We compare GPT-4 extraction performance with a high-performing supervised approach and perform thorough error analyses. Our prompt-based GPT-4 method achieved an overall 0.652 F1 on the SHAC test set, similar to the 7th best-performing system among all teams in the n2c2 challenge with SHAC.", "title": "promptbased extraction of social determinants of health using fewshot learning", "url": "http://arxiv.org/pdf/2306.07170", "tokenized_text": "social determinants health documented electronic health record unstructured text increasingly studied understand impacts patient health outcomes work utilize social history annotation corpus multi corpus de identified social history sections annotated including use employment status information explore automatic extraction information annotation formats gpt-4 shot_prompting shot setting compare gpt-4 extraction performance high performing supervised approach perform thorough error analyses based gpt-4 method achieved overall f1 test set similar 7th best performing system teams challenge"}
{"id": "4d3a49d1439a0b8fbb0e9f588970ad0f1d70dec8", "abstract": "Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than>28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA on human-written queries from the task of chart QA.", "title": "deplot oneshot visual language reasoning by plottotable translation", "url": "http://arxiv.org/pdf/2212.10505", "tokenized_text": "visual language charts ubiquitous human world comprehending charts requires strong reasoning skills prior state art sota require tens thousands training_examples training examples reasoning capabilities limited especially complex human written queries paper_presents paper presents shot solution visual language reasoning decompose challenge visual language reasoning steps text translation reasoning translated text key method modality conversion module named deplot translates image chart table output deplot directly pretrained large_language large language llm exploiting shot reasoning capabilities llms obtain deplot standardize table task establishing unified task formats metrics train deplot end end task deplot shelf llms plug play fashion compared sota finetuned data points shot_prompting shot achieves improvement finetuned sota human written queries task chart qa"}
{"id": "d1aa858644154af50e36860e6761ae52ae655bd3", "abstract": "In this study, we developed an automated short answer grading (ASAG) model that provided both analytic scores and final holistic scores. Short answer items typically consist of multiple sub-questions, and providing an analytic score and the text span relevant to each sub-question can increase the interpretability of the automated scores. Furthermore, they can be used to generate actionable feedback for students. Despite these advantages, most studies have focused on predicting only holistic scores due to the difficulty in constructing dataset with manual annotations. To address this difficulty, we used large language model (LLM)-based one-shot prompting and a text similarity scoring model with domain adaptation using small manually annotated dataset. The accuracy and quadratic weighted kappa of our model were 0.67 and 0.71 on a subset of the publicly available ASAG dataset. The model achieved a substantial improvement over the majority baseline.", "title": "short answer grading using oneshot prompting and text similarity scoring model", "url": "http://arxiv.org/pdf/2305.18638", "tokenized_text": "study developed automated short answer grading provided analytic scores final holistic scores short answer items typically consist multiple sub questions providing analytic score text span relevant sub question increase interpretability automated scores furthermore generate actionable feedback students despite advantages studies focused predicting holistic scores difficulty constructing dataset manual annotations address difficulty large_language large language llm)-based shot_prompting shot text similarity scoring domain adaptation small manually annotated dataset accuracy quadratic weighted kappa 0.67 subset publicly_available publicly available dataset achieved substantial improvement majority baseline"}
{"id": "034f1d77d832460a239072c81b5bb178b93c1e9f", "abstract": "The traditional Dialogue State Tracking (DST) problem aims to track user preferences and intents in user-agent conversations. While sufficient for task-oriented dialogue systems supporting narrow domain applications, the advent of Large Language Model (LLM)-based chat systems has introduced many real-world intricacies in open-domain dialogues. These intricacies manifest in the form of increased complexity in contextual interactions, extended dialogue sessions encompassing a diverse array of topics, and more frequent contextual shifts. To handle these intricacies arising from evolving LLM-based chat systems, we propose joint dialogue segmentation and state tracking per segment in open-domain dialogue systems. Assuming a zero-shot setting appropriate to a true open-domain dialogue system, we propose S3-DST, a structured prompting technique that harnesses Pre-Analytical Recollection, a novel grounding mechanism we designed for improving long context tracking. To demonstrate the efficacy of our proposed approach in joint segmentation and state tracking, we evaluate S3-DST on a proprietary anonymized open-domain dialogue dataset, as well as publicly available DST and segmentation datasets. Across all datasets and settings, S3-DST consistently outperforms the state-of-the-art, demonstrating its potency and robustness the next generation of LLM-based chat systems.", "title": "s3dst structured opendomain dialogue segmentation and state tracking in the era of llms", "url": "https://arxiv.org/pdf/2309.08827", "tokenized_text": "traditional dialogue_state_tracking dialogue state tracking dst problem aims track user preferences intents user agent conversations sufficient task oriented dialogue systems supporting narrow domain applications advent large_language large language llm)-based chat systems introduced real world intricacies open domain dialogues intricacies manifest form increased complexity contextual interactions extended dialogue sessions encompassing diverse array topics frequent contextual shifts handle intricacies arising evolving llm based chat systems propose joint dialogue segmentation state tracking segment open domain dialogue systems assuming zero shot_setting shot setting appropriate true open domain dialogue system propose s3 dst structured prompting_technique technique harnesses novel grounding mechanism designed improving long context tracking demonstrate efficacy proposed approach joint segmentation state tracking evaluate s3 dst proprietary open domain dialogue dataset publicly_available publicly available dst segmentation datasets datasets settings s3 dst consistently_outperforms consistently outperforms state art demonstrating potency robustness generation llm based chat systems"}
{"id": "0786c88990235414611478099e43611542d973b0", "abstract": "We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.", "title": "take a step back evoking reasoning via abstraction in large language models", "url": "https://arxiv.org/pdf/2310.06117", "tokenized_text": "present simple prompting_technique technique enables llms abstractions derive high level concepts principles instances containing specific details concepts principles guide reasoning_steps reasoning steps llms significantly improve abilities following correct reasoning path solution conduct experiments observe substantial performance gains wide_range wide range challenging reasoning intensive tasks including stem knowledge qa multi hop reasoning instance improves performance mmlu physics chemistry 11 27 musique"}
{"id": "0820a7ec1b7cac3470836161a92da7d59f626d14", "abstract": "This paper explores the potential for utilizing generative AI models in group-focused co-creative frameworks to enhance problem solving and ideation in business innovation and co-creation contexts, and proposes a novel prompting technique for conversational generative AI agents which employ methods inspired by traditional 'human-to-human' facilitation and instruction to enable active contribution to Design Thinking, a co-creative framework. Through experiments using this prompting technique, we gather evidence that conversational generative transformers (i.e. ChatGPT) have the capability to contribute context-specific, useful, and creative input into Design Thinking activities. We also discuss the potential benefits, limitations, and risks associated with using generative AI models in co-creative ideation and provide recommendations for future research.", "title": "chaidt a framework for prompting conversational generative ai agents to actively participate in cocreation", "url": "http://arxiv.org/pdf/2305.03852", "tokenized_text": "paper explores potential utilizing generative_ai generative ai group focused co creative frameworks enhance problem solving ideation business innovation co creation contexts proposes novel prompting_technique technique conversational generative_ai generative ai agents employ methods inspired traditional human human instruction enable active contribution design thinking co creative framework experiments prompting_technique technique gather evidence conversational generative transformers i.e. chatgpt capability contribute context specific useful creative input design thinking activities discuss potential benefits limitations risks associated generative_ai generative ai co creative ideation provide recommendations future_research future research"}
{"id": "118802f91718ea2c566f2eaf1b4e25c439459f4d", "abstract": ". Extracting complex structures from grid-based data is a common key step in automated medical image analysis. The conventional solution to recovering tree-structured geometries typically involves computing the minimal cost path through intermediate representations derived from segmentation masks. However, this methodology has signi\ufb01cant limitations in the context of projective imaging of tree-structured 3D anatomical data such as coronary arteries, since there are often overlapping branches in the 2D projection. In this work, we propose a novel approach to predicting tree connectivity structure which reformulates the task as an optimization problem over individual steps of a recursive process. We design and train a two-stage model which leverages the UNet and Transformer architectures and introduces an image-based prompting technique. Our proposed method achieves compelling results on a pair of synthetic datasets, and outperforms a shortest-path baseline.", "title": "image to tree with recursive prompting", "url": "http://arxiv.org/pdf/2301.00447", "tokenized_text": "extracting complex structures grid based data common key step automated medical image analysis conventional solution recovering tree structured geometries typically involves computing minimal cost path intermediate representations derived segmentation masks methodology signi\ufb01cant limitations context imaging tree structured 3d data branches 2d projection work propose_a_novel propose novel approach predicting tree connectivity structure reformulates task optimization problem individual steps recursive process design train stage leverages transformer architectures introduces image based prompting_technique technique proposed_method proposed method achieves compelling results pair synthetic datasets outperforms shortest path baseline"}
{"id": "19b43ff57e5d8f8a99da4110fbc30b4ecc39a527", "abstract": "People have long hoped for a conversational system that can assist in real-life situations, and recent progress on large language models (LLMs) is bringing this idea closer to reality. While LLMs are often impressive in performance, their efficacy in real-world scenarios that demand expert knowledge remains unclear. LLMs are believed to hold the most potential and value in education, especially in the development of Artificial intelligence (AI) based virtual teachers capable of facilitating language learning. Our focus is centered on evaluating the efficacy of LLMs in the realm of education, specifically in the areas of spoken language learning which encompass phonetics, phonology, and second language acquisition. We introduce a new multiple-choice question dataset to evaluate the effectiveness of LLMs in the aforementioned scenarios, including understanding and application of spoken language knowledge. In addition, we investigate the influence of various prompting techniques such as zero- and few-shot method (prepending the question with question-answer exemplars), chain-of-thought (CoT, think step-by-step), in-domain exampler and external tools (Google, Wikipedia). We conducted large-scale evaluation on popular LLMs (20 distinct models) using these methods. We achieved significant performance improvements compared to the zero-shot baseline in the practical questions reasoning (GPT-3.5, 49.1% ->63.1%; LLaMA2-70B-Chat, 42.2% ->48.6%). We found that models of different sizes have good understanding of concepts in phonetics, phonology, and second language acquisition, but show limitations in reasoning for real-world problems. Additionally, we also explore preliminary findings on conversational communication.", "title": "spoken language intelligence of large language models for language learning", "url": "https://arxiv.org/pdf/2308.14536", "tokenized_text": "people long conversational system assist real life situations recent progress large_language large language llms bringing idea closer reality llms impressive performance efficacy real world_scenarios world scenarios demand expert knowledge remains unclear llms believed hold potential value education especially development artificial_intelligence artificial intelligence ai based virtual teachers capable facilitating language learning focus centered evaluating efficacy llms realm education specifically areas spoken language learning encompass second language acquisition introduce new multiple choice question dataset evaluate effectiveness llms aforementioned scenarios including understanding application spoken language knowledge addition investigate influence prompting_techniques techniques zero- shot method prepending question question answer exemplars chain thought cot think step step domain external tools google wikipedia conducted large scale evaluation popular llms 20 distinct methods achieved significant performance improvements compared zero shot baseline practical questions reasoning gpt-3.5 llama2 70b chat found different sizes good understanding concepts second language acquisition limitations reasoning real world problems additionally explore preliminary findings conversational communication"}
{"id": "1ad735714ad2e4ee5b94ce26c976e5ee5c7cde3b", "abstract": "A flurry of recent work has demonstrated that pre-trained large language models (LLMs) can be effective task planners for a variety of single-robot tasks. The planning performance of LLMs is significantly improved via prompting techniques, such as in-context learning or re-prompting with state feedback, placing new importance on the token budget for the context window. An under-explored but natural next direction is to investigate LLMs as multi-robot task planners. However, long-horizon, heterogeneous multi-robot planning introduces new challenges of coordination while also pushing up against the limits of context window length. It is therefore critical to find token-efficient LLM planning frameworks that are also able to reason about the complexities of multi-robot coordination. In this work, we compare the task success rate and token efficiency of four multi-agent communication frameworks (centralized, decentralized, and two hybrid) as applied to four coordination-dependent multi-agent 2D task scenarios for increasing numbers of agents. We find that a hybrid framework achieves better task success rates across all four tasks and scales better to more agents. We further demonstrate the hybrid frameworks in 3D simulations where the vision-to-text problem and dynamical errors are considered. See our project website https://yongchao98.github.io/MIT-REALM-Multi-Robot/ for prompts, videos, and code.", "title": "scalable multirobot collaboration with large language models centralized or decentralized systems", "url": "https://arxiv.org/pdf/2309.15943", "tokenized_text": "recent_work recent work demonstrated pre trained large_language large language llms effective task planners variety single robot tasks planning performance llms significantly improved prompting_techniques techniques context_learning context learning state feedback placing new importance token budget context window explored natural direction investigate llms multi robot task planners long horizon heterogeneous multi robot planning introduces new challenges coordination pushing limits context window length critical find token efficient llm planning frameworks able reason complexities multi robot coordination work compare task success_rate success rate token efficiency multi agent communication frameworks centralized decentralized hybrid applied coordination dependent multi agent 2d task scenarios increasing numbers agents find hybrid framework achieves better task success rates tasks scales better agents demonstrate hybrid frameworks 3d simulations vision text problem dynamical errors considered project website videos code"}
{"id": "1fc0e5b30bfede1b78389d00f8c41bacd29ecd7f", "abstract": "The use of natural language processing (NLP) techniques in engineering education can provide valuable insights into the underlying processes involved in generating text. While accessing these insights can be labor-intensive if done manually, recent advances in NLP and large language models have made it a realistic option for individuals. This study explores and evaluates a combination of clustering, summarization, and prompting techniques to analyze over 1,000 student essays in which students discussed their career interests. The specific assignment prompted students to define and explain their career goals as engineers. Using text embedding representations of student responses, we clustered the responses together to identify thematically similar statements from students. The clustered responses were then summarized to quickly identify career interest themes. We also used a set of a priori codes about career satisfaction and sectors to demonstrate an alternative approach to using these generative text models to analyze student writing. The results of this study demonstrate the feasibility and usefulness of NLP techniques in engineering education research. By automating the initial analysis of student essays, researchers and educators can more efficiently and accurately identify key themes and patterns in student writing. The methods presented in this paper have broader applications for engineering education and research purposes beyond analyzing student essays. By explaining these methods to the engineering education community, readers can utilize them in their own contexts.", "title": "the utility of large language models and generative ai for education research", "url": "http://arxiv.org/pdf/2305.18125", "tokenized_text": "use natural_language natural language processing nlp techniques engineering education provide valuable insights underlying processes involved generating text accessing insights labor intensive manually recent_advances recent advances nlp large_language large language realistic option individuals study explores evaluates combination clustering summarization prompting_techniques techniques analyze 1,000 student essays students discussed interests specific assignment prompted students define explain goals engineers text embedding representations student responses clustered responses identify thematically similar statements students clustered responses summarized quickly identify interest themes set priori codes satisfaction sectors demonstrate alternative approach generative text analyze student writing results study demonstrate feasibility usefulness nlp techniques engineering education research automating initial analysis student essays researchers educators efficiently accurately identify key themes patterns student writing methods presented paper broader applications engineering education research purposes analyzing student essays explaining methods engineering education community readers utilize contexts"}
{"id": "20cb4e0bd8871d33d82fc72ea82a0aa1dd922810", "abstract": "Generative Artificial Intelligence is set to revolutionize healthcare delivery by transforming traditional patient care into a more personalized, efficient, and proactive process. Chatbots, serving as interactive conversational models, will probably drive this patient-centered transformation in healthcare. Through the provision of various services, including diagnosis, personalized lifestyle recommendations, and mental health support, the objective is to substantially augment patient health outcomes, all the while mitigating the workload burden on healthcare providers. The life-critical nature of healthcare applications necessitates establishing a unified and comprehensive set of evaluation metrics for conversational models. Existing evaluation metrics proposed for various generic large language models (LLMs) demonstrate a lack of comprehension regarding medical and health concepts and their significance in promoting patients' well-being. Moreover, these metrics neglect pivotal user-centered aspects, including trust-building, ethics, personalization, empathy, user comprehension, and emotional support. The purpose of this paper is to explore state-of-the-art LLM-based evaluation metrics that are specifically applicable to the assessment of interactive conversational models in healthcare. Subsequently, we present an comprehensive set of evaluation metrics designed to thoroughly assess the performance of healthcare chatbots from an end-user perspective. These metrics encompass an evaluation of language processing abilities, impact on real-world clinical tasks, and effectiveness in user-interactive conversations. Finally, we engage in a discussion concerning the challenges associated with defining and implementing these metrics, with particular emphasis on confounding factors such as the target audience, evaluation methods, and prompt techniques involved in the evaluation process.", "title": "foundation metrics quantifying effectiveness of healthcare conversations powered by generative ai", "url": "https://arxiv.org/pdf/2309.12444", "tokenized_text": "generative artificial_intelligence artificial intelligence set revolutionize healthcare transforming traditional patient care personalized efficient proactive process chatbots serving interactive conversational drive patient centered transformation healthcare provision services including diagnosis personalized recommendations mental_health mental health support objective substantially augment patient health outcomes mitigating workload burden healthcare providers life critical nature healthcare applications necessitates establishing unified comprehensive set evaluation metrics conversational existing evaluation metrics proposed generic large_language large language llms demonstrate lack comprehension medical health concepts significance promoting patients metrics neglect pivotal user centered aspects including trust building personalization empathy user comprehension emotional support purpose paper explore state art llm based evaluation metrics specifically applicable assessment interactive conversational healthcare subsequently present comprehensive set evaluation metrics designed thoroughly assess performance healthcare chatbots end user perspective metrics encompass evaluation language_processing language processing abilities impact real world clinical tasks effectiveness user interactive conversations finally engage discussion concerning challenges associated defining implementing metrics particular emphasis confounding factors target audience evaluation methods techniques involved evaluation process"}
{"id": "26d31d641116b656826737335b2accb802ac9931", "abstract": "The Segment Anything Model (SAM) is a foundation model for general image segmentation. Although it exhibits impressive performance predominantly on natural images, understanding its robustness against various image perturbations and domains is critical for real-world applications where such challenges frequently arise. In this study we conduct a comprehensive robustness investigation of SAM under diverse real-world conditions. Our experiments encompass a wide range of image perturbations. Our experimental results demonstrate that SAM's performance generally declines under perturbed images, with varying degrees of vulnerability across different perturbations. By customizing prompting techniques and leveraging domain knowledge based on the unique characteristics of each dataset, the model's resilience to these perturbations can be enhanced, addressing dataset-specific challenges. This work sheds light on the limitations and strengths of SAM in real-world applications, promoting the development of more robust and versatile image segmentation solutions.", "title": "an empirical study on the robustness of the segment anything model (sam)", "url": "http://arxiv.org/pdf/2305.06422", "tokenized_text": "segment_anything_model segment sam foundation general image segmentation exhibits impressive performance predominantly natural images understanding robustness image perturbations domains critical real world_applications world applications challenges frequently arise study conduct comprehensive robustness investigation sam diverse real world conditions experiments encompass wide_range wide range image perturbations experimental_results experimental results demonstrate sam performance generally declines perturbed images varying degrees vulnerability different perturbations customizing prompting_techniques techniques leveraging domain knowledge based unique characteristics dataset resilience perturbations enhanced addressing dataset specific challenges work sheds light limitations strengths sam real world_applications world applications promoting development robust versatile image segmentation solutions"}
{"id": "29965a1efc21a637e03a5e0a869d77eca77f5085", "abstract": "Inspired by DETR variants, query-based end-to-end instance segmentation (QEIS) methods have recently outperformed CNN-based models on large-scale datasets. Yet they would lose efficacy when only a small amount of training data is available since it's hard for the crucial queries/kernels to learn localization and shape priors. To this end, this work offers a novel unsupervised pre-training solution for low-data regimes. Inspired by the recent success of the Prompting technique, we introduce a new pre-training method that boosts QEIS models by giving Saliency Prompt for queries/kernels. Our method contains three parts: 1) Saliency Masks Proposal is responsible for generating pseudo masks from unlabeled images based on the saliency mechanism. 2) Prompt-Kernel Matching transfers pseudo masks into prompts and injects the corresponding localization and shape priors to the best-matched kernels. 3) Kernel Supervision is applied to supply supervision at the kernel level for robust learning. From a practical perspective, our pre-training method helps QEIS models achieve a similar convergence speed and comparable performance with CNN-based models in low-data regimes. Experimental results show that our method significantly boosts several QEIS models on three datasets.11Code: https://github.com/lifuguan/saliency.prompt", "title": "boosting lowdata instance segmentation by unsupervised pretraining with saliency prompt", "url": "https://arxiv.org/pdf/2302.01171", "tokenized_text": "inspired variants query based end end instance segmentation methods recently outperformed cnn based large scale datasets efficacy small training_data training data available hard crucial queries learn localization shape priors end work offers novel unsupervised pre training solution low data regimes inspired recent success prompting_technique technique introduce new pre training method boosts giving saliency queries method contains parts saliency masks proposal responsible generating pseudo masks unlabeled images based saliency mechanism matching transfers pseudo masks injects corresponding localization shape priors best matched supervision applied supply supervision level robust learning practical perspective pre training method helps achieve similar convergence speed comparable performance cnn based low data regimes experimental_results experimental results method significantly boosts"}
{"id": "2a663560b669a0b8d975675b3ac2546cc7386f3a", "abstract": "The goal of temporal relation extraction is to infer the temporal relation between two events in the document. Supervised models are dominant in this task. In this work, we investigate ChatGPT\u2019s ability on zero-shot temporal relation extraction. We designed three different prompt techniques to break down the task and evaluate ChatGPT. Our experiments show that ChatGPT\u2019s performance has a large gap with that of supervised methods and can heavily rely on the design of prompts. We further demonstrate that ChatGPT can infer more small relation classes correctly than supervised methods. The current shortcomings of ChatGPT on temporal relation extraction are also discussed in this paper. We found that ChatGPT cannot keep consistency during temporal inference and it fails in actively long-dependency temporal inference.", "title": "zeroshot temporal relation extraction with chatgpt", "url": "http://arxiv.org/pdf/2304.05454", "tokenized_text": "goal temporal relation_extraction relation extraction infer temporal relation events document supervised dominant task work investigate chatgpt ability zero shot temporal relation_extraction relation extraction designed different techniques break task evaluate chatgpt experiments chatgpt performance large gap supervised methods heavily rely design demonstrate chatgpt infer small relation classes correctly supervised methods current shortcomings chatgpt temporal relation_extraction relation extraction discussed paper found chatgpt consistency temporal inference fails actively long dependency temporal inference"}
{"id": "2bd1b8990db73b6495c11082bea2d5f925c5226f", "abstract": "In this work, we present SciGraphQA, a synthetic multi-turn question-answer dataset related to academic graphs. SciGraphQA is 13 times larger than ChartVQA, the previously largest chart-visual question-answering dataset. It is also the largest open-sourced chart VQA dataset with non-synthetic charts. To build our dataset, we selected 290,000 Computer Science or Machine Learning ArXiv papers published between 2010 and 2020, and then used Palm-2 to generate 295K samples of open-vocabulary multi-turn question-answering dialogues about the graphs. As context, we provided the text-only Palm-2 with paper title, abstract, paragraph mentioning the graph, and rich text contextual data from the graph itself, obtaining dialogues with an average 2.23 question-answer turns for each graph. We asked GPT-4 to assess the matching quality of our question-answer turns given the paper's context, obtaining an average rating of 8.7/10 on our 3K test set. We evaluated the 0-shot capability of the most popular MLLM models such as LLaVa, mPLUGowl, BLIP-2, and openFlamingo's on our dataset, finding LLaVA-13B being the most performant with a CIDEr score of 0.08. We further enriched the question prompts for LLAVA by including the serialized data tables extracted from the graphs using the DePlot model, boosting LLaVA's 0-shot CIDEr to 0.15. To verify the validity of our dataset, we also fine-tuned LLaVa using our dataset, reaching a substantially higher CIDEr score of 0.26. We anticipate further accuracy improvement by including segmentation mask tokens and leveraging larger LLM backbones coupled with emergent prompting techniques. Our code and data are open-sourced.", "title": "scigraphqa a largescale synthetic multiturn questionanswering dataset for scientific graphs", "url": "https://arxiv.org/pdf/2308.03349", "tokenized_text": "work present synthetic multi turn question answer dataset related academic graphs 13 times larger previously largest chart visual question answering dataset largest open sourced chart vqa dataset non synthetic charts build dataset selected computer science machine_learning machine learning arxiv papers published 2020 palm-2 generate samples open vocabulary multi turn question answering dialogues graphs context provided text palm-2 paper title abstract paragraph mentioning graph rich text contextual data graph obtaining dialogues average question answer turns graph asked gpt-4 assess matching quality question answer turns given paper context obtaining average rating test set evaluated shot capability popular mllm llava blip-2 openflamingo dataset finding performant cider score 0.08 enriched question llava including serialized data tables extracted graphs deplot boosting llava shot cider verify validity dataset fine tuned llava dataset reaching substantially higher cider score anticipate accuracy improvement including segmentation mask tokens leveraging larger llm backbones coupled emergent prompting_techniques techniques code data open sourced"}
{"id": "352bcafbcc95a84d96019688955cab5c43eb23f0", "abstract": "Dealing with unjudged documents (\"holes\") in relevance assessments is a perennial problem when evaluating search systems with offline experiments. Holes can reduce the apparent effectiveness of retrieval systems during evaluation and introduce biases in models trained with incomplete data. In this work, we explore whether large language models can help us fill such holes to improve offline evaluations. We examine an extreme, albeit common, evaluation setting wherein only a single known relevant document per query is available for evaluation. We then explore various approaches for predicting the relevance of unjudged documents with respect to a query and the known relevant document, including nearest neighbor, supervised, and prompting techniques. We find that although the predictions of these One-Shot Labelers (1SL) frequently disagree with human assessments, the labels they produce yield a far more reliable ranking of systems than the single labels do alone. Specifically, the strongest approaches can consistently reach system ranking correlations of over 0.86 with the full rankings over a variety of measures. Meanwhile, the approach substantially increases the reliability of t-tests due to filling holes in relevance assessments, giving researchers more confidence in results they find to be significant. Alongside this work, we release an easy-to-use software package to enable the use of 1SL for evaluation of other ad-hoc collections or systems.", "title": "oneshot labeling for automatic relevance estimation", "url": "https://arxiv.org/pdf/2302.11266", "tokenized_text": "dealing documents relevance assessments problem evaluating search systems offline experiments reduce effectiveness retrieval systems evaluation introduce biases trained incomplete data work explore large_language large language help fill improve offline evaluations examine extreme albeit common evaluation setting single known relevant document query available evaluation explore approaches predicting relevance documents respect query known relevant document including nearest neighbor supervised prompting_techniques techniques find predictions shot frequently human assessments labels produce yield far reliable ranking systems single labels specifically strongest approaches consistently reach system ranking correlations rankings variety measures approach substantially increases reliability tests filling relevance assessments giving researchers confidence results find significant alongside work release easy use software package enable use evaluation ad hoc collections systems"}
{"id": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e", "abstract": "Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.", "title": "large language models can be easily distracted by irrelevant context", "url": "http://arxiv.org/pdf/2302.00093", "tokenized_text": "large_language large language achieved impressive performance natural_language natural language processing tasks far evaluated primarily benchmarks information input context relevant solving task work investigate large_language large language i.e. problem solving accuracy influenced irrelevant context particular introduce math irrelevant context gsm ic arithmetic reasoning dataset irrelevant information problem description use benchmark measure cutting edge prompting_techniques techniques large_language large language find performance dramatically irrelevant information included identify approaches mitigating deficiency decoding self consistency adding instruction language_model language ignore irrelevant information"}
{"id": "3e4afde5a9de2c1801da99b8aff5ae05923f256b", "abstract": "Large language models have exhibited emergent abilities, demonstrating exceptional performance across diverse tasks for which they were not explicitly trained, including those that require complex reasoning abilities. The emergence of such abilities carries profound implications for the future direction of research in NLP, especially as the deployment of such models becomes more prevalent. However, one key challenge is that the evaluation of these abilities is often confounded by competencies that arise in models through alternative prompting techniques, such as in-context learning and instruction following, which also emerge as the models are scaled up. In this study, we provide the first comprehensive examination of these emergent abilities while accounting for various potentially biasing factors that can influence the evaluation of models. We conduct rigorous tests on a set of 18 models, encompassing a parameter range from 60 million to 175 billion parameters, across a comprehensive set of 22 tasks. Through an extensive series of over 1,000 experiments, we provide compelling evidence that emergent abilities can primarily be ascribed to in-context learning. We find no evidence for the emergence of reasoning abilities, thus providing valuable insights into the underlying mechanisms driving the observed abilities and thus alleviating safety concerns regarding their use.", "title": "are emergent abilities in large language models just incontext learning", "url": "https://arxiv.org/pdf/2309.01809", "tokenized_text": "large_language large language exhibited emergent abilities demonstrating exceptional performance diverse tasks explicitly trained including require complex_reasoning complex reasoning abilities emergence abilities profound implications future direction research nlp especially deployment prevalent key challenge evaluation abilities confounded competencies arise alternative prompting_techniques techniques context_learning context learning instruction_following instruction following emerge scaled study provide comprehensive examination emergent abilities accounting potentially biasing factors influence evaluation conduct rigorous tests set 18 encompassing parameter range 60 million 175 billion parameters comprehensive set 22 tasks extensive series 1,000 experiments provide compelling evidence emergent abilities primarily context_learning context learning find evidence emergence reasoning abilities providing valuable insights underlying mechanisms driving observed abilities alleviating safety concerns use"}
{"id": "42780f9c7f73d73d7a887e2f787af0e079703d40", "abstract": "Large language models (LLMs) have made significant progress in various domains, including healthcare. However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation. In this study, we conduct a comprehensive evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within the realm of clinical language understanding tasks. These tasks span a diverse range, including named entity recognition, relation extraction, natural language inference, semantic textual similarity, document classification, and question-answering. We also introduce a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand. Our evaluation underscores the significance of task-specific learning strategies and prompting techniques for improving LLMs' effectiveness in healthcare-related tasks. Additionally, our in-depth error analysis on the challenging relation extraction task offers valuable insights into error distribution and potential avenues for improvement using SQP. Our study sheds light on the practical implications of employing LLMs in the specialized domain of healthcare, serving as a foundation for future research and the development of potential applications in healthcare settings.", "title": "are large language models ready for healthcare a comparative study on clinical language understanding", "url": "https://arxiv.org/pdf/2304.05368", "tokenized_text": "large_language large language llms significant progress domains including healthcare specialized nature clinical language understanding tasks presents unique challenges limitations investigation study conduct comprehensive evaluation state art llms gpt-3.5 gpt-4 bard realm clinical language understanding tasks tasks span diverse range including named_entity named entity recognition relation_extraction relation extraction natural_language natural language inference semantic textual similarity document classification question answering introduce novel strategy self tailored enhance llms performance eliciting informative questions answers pertinent clinical scenarios hand evaluation underscores significance task specific learning strategies prompting_techniques techniques improving llms effectiveness healthcare related tasks additionally depth error analysis challenging relation_extraction relation extraction task offers valuable insights error distribution potential avenues improvement study sheds light practical implications employing llms specialized domain healthcare serving foundation future_research future research development potential applications healthcare settings"}
{"id": "4a6d7b11c4aba5a23f68856989366dd4311e960b", "abstract": "Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated exceptional performance in various natural language processing tasks and have shown the ability to solve certain reasoning problems. However, their reasoning capabilities are limited and relatively shallow, despite the application of various prompting techniques. In contrast, formal logic is adept at handling complex reasoning, but translating natural language descriptions into formal logic is a challenging task that non-experts struggle with. This paper proposes a neuro-symbolic method that combines the strengths of large language models and answer set programming. Specifically, we employ an LLM to transform natural language descriptions of logic puzzles into answer set programs. We carefully design prompts for an LLM to convert natural language descriptions into answer set programs in a step by step manner. Surprisingly, with just a few in-context learning examples, LLMs can generate reasonably complex answer set programs. The majority of errors made are relatively simple and can be easily corrected by humans, thus enabling LLMs to effectively assist in the creation of answer set programs.", "title": "leveraging large language models to generate answer set programs", "url": "https://arxiv.org/pdf/2307.07699", "tokenized_text": "large_language large language llms gpt-3 gpt-4 demonstrated exceptional performance natural_language natural language processing tasks shown ability solve certain reasoning problems reasoning capabilities limited relatively shallow despite application prompting_techniques techniques contrast formal logic adept handling complex_reasoning complex reasoning translating natural_language natural language descriptions formal logic challenging task non experts struggle paper_proposes paper proposes symbolic method combines strengths large_language large language answer set programming specifically employ llm transform natural_language natural language descriptions logic puzzles answer set programs carefully design llm convert natural_language natural language descriptions answer set programs step_by_step step step manner surprisingly context_learning context learning examples llms generate reasonably complex answer set programs majority errors relatively simple easily corrected humans enabling llms effectively assist creation answer set programs"}
{"id": "4b99e8273227fd05f2be20248050d81e97ab4f4e", "abstract": "The widespread usage of latent language representations via pre-trained language models (LMs) suggests that they are a promising source of structured knowledge. However, existing methods focus only on a single object per subject-relation pair, even though often multiple objects are correct. To overcome this limitation, we analyze these representations for their potential to yield materialized multi-object relational knowledge. We formulate the problem as a rank-then-select task. For ranking candidate objects, we evaluate existing prompting techniques and propose new ones incorporating domain knowledge. Among the selection methods, we find that choosing objects with a likelihood above a learned relation-specific threshold gives a 49.5% F1 score. Our results highlight the difficulty of employing LMs for the multi-valued slot-filling task, and pave the way for further research on extracting relational knowledge from latent language representations.", "title": "extracting multivalued relations from language models", "url": "https://aclanthology.org/2023.repl4nlp-1.12.pdf", "tokenized_text": "widespread usage latent language representations pre trained_language trained language lms suggests promising source structured knowledge existing_methods existing methods focus single object subject relation pair multiple objects correct overcome limitation analyze representations potential yield multi object relational knowledge formulate problem rank select task ranking candidate objects evaluate existing prompting_techniques techniques propose new ones incorporating domain knowledge selection methods find choosing objects likelihood learned relation specific threshold gives f1_score f1 score results highlight difficulty employing lms multi slot filling task pave way research extracting relational knowledge latent language representations"}
{"id": "4d17732d90440682b0500f4e209c6cc4fac20e0e", "abstract": "Large language models (LLMs) have shown increasing in-context learning capabilities through scaling up model and data size. Despite this progress, LLMs are still unable to solve algorithmic reasoning problems. While providing a rationale with the final answer has led to further improvements in multi-step reasoning problems, Anil et al. 2022 showed that even simple algorithmic reasoning tasks such as parity are far from solved. In this work, we identify and study four key stages for successfully teaching algorithmic reasoning to LLMs: (1) formulating algorithms as skills, (2) teaching multiple skills simultaneously (skill accumulation), (3) teaching how to combine skills (skill composition) and (4) teaching how to use skills as tools. We show that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which we refer to as algorithmic prompting. We evaluate our approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrate significant boosts in performance over existing prompting techniques. In particular, for long parity, addition, multiplication and subtraction, we achieve an error reduction of approximately 10x, 9x, 5x and 2x respectively compared to the best available baselines.", "title": "teaching algorithmic reasoning via incontext learning", "url": "http://arxiv.org/pdf/2211.09066", "tokenized_text": "large_language large language llms shown increasing context_learning context learning capabilities scaling data size despite progress llms unable solve algorithmic reasoning problems providing rationale final answer led improvements multi step reasoning problems et_al et al 2022 showed simple algorithmic reasoning tasks far solved work identify study key stages successfully teaching algorithmic reasoning llms formulating algorithms skills teaching multiple skills simultaneously skill accumulation teaching combine skills skill composition teaching use skills tools possible teach algorithmic reasoning llms context_learning context learning refer algorithmic evaluate approach variety arithmetic quantitative reasoning tasks demonstrate significant boosts performance existing prompting_techniques techniques particular long addition multiplication achieve error reduction approximately 10x 5x 2x respectively compared best available baselines"}
{"id": "4edd2d2770729380eda23826af1b78298b334a23", "abstract": "We revisit and advance visual prompting (VP), an input prompting technique for vision tasks. VP can reprogram a fixed, pre-trained source model to accomplish downstream tasks in the target domain by simply incorporating universal prompts (in terms of input perturbation patterns) into downstream data points. Yet, it remains elusive why VP stays effective even given a ruleless label mapping (LM) between the source classes and the target classes. Inspired by the above, we ask: How is LM interrelated with VP? And how to exploit such a relationship to improve its accuracy on target tasks? We peer into the influence of LM on VP and provide an affirmative answer that a better \u2018quality\u2019 of LM (assessed by mapping precision and explanation) can consistently improve the effectiveness of VP. This is in contrast to the prior art where the factor of LM was missing. To optimize LM, we propose a new VP framework, termed ILM-VP (iterative label mapping-based visual prompting), which automatically re-maps the source labels to the target labels and progressively improves the target task accuracy of VP. Further, when using a contrastive language-image pretrained (CLIP) model for VP, we propose to integrate an LM process to assist the text prompt selection of CLIP and to improve the target task accuracy. Extensive experiments demonstrate that our proposal significantly outperforms state-of-the-art VP methods. As highlighted below, we show that when reprogramming an ImageNet-pretrained ResNet-18 to 13 target tasks, ILM-VP outperforms baselines by a substantial margin, e.g., 7.9% and 6.7% accuracy improvements in transfer learning to the target Flowers102 and CIFAR100 datasets. Besides, our proposal on CLIP-based VP provides 13.7% and 7.1% accuracy improvements on Flowers102 and DTD respectively. Code is available at https://github.com/OPTML-Group/ILM-VP.", "title": "understanding and improving visual prompting a labelmapping perspective", "url": "https://arxiv.org/pdf/2211.11635", "tokenized_text": "revisit advance visual vp input prompting_technique technique vision tasks vp fixed pre trained source accomplish downstream_tasks downstream tasks target domain simply incorporating universal terms input perturbation patterns downstream data points remains elusive vp stays effective given label mapping lm source classes target classes inspired ask lm vp exploit relationship improve accuracy target tasks peer influence lm vp provide affirmative answer better quality lm assessed mapping precision explanation consistently improve effectiveness vp contrast prior art factor lm missing optimize lm propose_a_new propose new vp framework termed vp iterative label mapping based visual automatically maps source labels target labels progressively improves target task accuracy vp contrastive language image pretrained clip vp propose integrate lm process assist text selection clip improve target task accuracy extensive_experiments extensive experiments demonstrate proposal significantly_outperforms significantly outperforms state art vp methods highlighted imagenet pretrained 13 target tasks vp outperforms baselines substantial margin e.g. 6.7 accuracy improvements transfer learning target datasets proposal clip based vp provides accuracy improvements dtd respectively code_is_available code available"}
{"id": "5076bbbf831a92174c9cc1b347bd0584560435fc", "abstract": "Large Language Models (LLMs) are showcasing impressive ability in handling complex reasoning tasks. In real-world situations, problems often span a spectrum of complexities. Humans inherently adjust their problem-solving approaches based on task complexity. However, most methodologies that leverage LLMs tend to adopt a uniform approach: utilizing consistent models, prompting methods, and degrees of problem decomposition, regardless of the problem complexity. Inflexibility of them can bring unnecessary computational overhead or sub-optimal performance. To address this problem, we introduce an Adaptive-Solver framework. It strategically modulates solving strategies based on the difficulties of the problems. Given an initial solution, the framework functions with two primary modules. The initial evaluation module assesses the adequacy of the current solution. If improvements are needed, the subsequent adaptation module comes into play. Within this module, three key adaptation strategies are employed: (1) Model Adaptation: Switching to a stronger LLM when a weaker variant is inadequate. (2) Prompting Method Adaptation: Alternating between different prompting techniques to suit the problem's nuances. (3) Decomposition Granularity Adaptation: Breaking down a complex problem into more fine-grained sub-questions to enhance solvability. Through such dynamic adaptations, our framework not only enhances computational efficiency but also elevates the overall performance. This dual-benefit ensures both the efficiency of the system for simpler tasks and the precision required for more complex questions. Experimental results from complex reasoning tasks reveal that the prompting method adaptation and decomposition granularity adaptation enhance performance across all tasks. Furthermore, the model adaptation approach significantly reduces API costs (up to 50%) while maintaining superior performance.", "title": "adaptivesolver framework for dynamic strategy selection in large language model reasoning", "url": "https://arxiv.org/pdf/2310.01446", "tokenized_text": "large_language large language llms showcasing impressive ability handling complex_reasoning complex reasoning tasks real world situations problems span spectrum complexities humans inherently adjust problem solving approaches based task complexity methodologies leverage llms tend adopt uniform approach utilizing consistent methods degrees problem decomposition regardless problem complexity bring unnecessary computational overhead sub optimal performance address problem introduce adaptive solver framework strategically solving strategies based difficulties problems given initial solution framework functions primary modules initial evaluation module assesses current solution improvements needed subsequent adaptation module comes play module key adaptation strategies employed adaptation switching stronger llm weaker variant inadequate method adaptation alternating different prompting_techniques techniques suit problem nuances decomposition granularity adaptation breaking complex problem fine grained sub questions enhance dynamic adaptations framework enhances computational efficiency elevates overall performance dual benefit ensures efficiency system simpler tasks precision required complex questions experimental_results experimental results complex_reasoning complex reasoning tasks reveal method adaptation decomposition granularity adaptation enhance performance tasks furthermore adaptation approach significantly reduces api costs 50 maintaining superior_performance superior performance"}
{"id": "50e8ab900d2ca4d83da120bbfe5338ee93dbe741", "abstract": "We explore the ability of large language models (LLMs) to act as speech recognition post-processors that perform rescoring and error correction. Our first focus is on instruction prompting to let LLMs perform these task without fine-tuning, for which we evaluate different prompting schemes, both zero- and few-shot in-context learning, and a novel task activation prompting method that combines causal instructions and demonstration to increase its context windows. Next, we show that rescoring only by in-context learning with frozen LLMs achieves results that are competitive with rescoring by domain-tuned LMs, using a pretrained first-pass recognition system and rescoring output on two out-of-domain tasks (ATIS and WSJ). By combining prompting techniques with fine-tuning we achieve error rates below the N-best oracle level, showcasing the generalization power of the LLMs.", "title": "generative speech recognition error correction with large language models and taskactivating prompting", "url": "https://arxiv.org/pdf/2309.15649", "tokenized_text": "explore ability large_language large language llms act speech recognition post perform rescoring error correction focus instruction let llms perform task fine tuning evaluate different schemes zero- shot context_learning context learning novel task activation method combines causal instructions demonstration increase context windows rescoring context_learning context learning frozen llms achieves results competitive rescoring domain tuned lms pretrained pass recognition system rescoring output domain tasks combining prompting_techniques techniques fine tuning achieve error rates best oracle level showcasing generalization power llms"}
{"id": "511ad6b37cb028bdfbd6096e6d20aa4b8b34fafc", "abstract": "In recent years, soft prompt learning methods have been proposed to fine-tune large-scale vision-language pre-trained models for various downstream tasks. These methods typically combine learnable textual tokens with class tokens as input for models with frozen parameters. However, they often employ a single prompt to describe class contexts, failing to capture categories' diverse attributes adequately. This study introduces the Partitioned Multi-modal Prompt (PMPO), a multi-modal prompting technique that extends the soft prompt from a single learnable prompt to multiple prompts. Our method divides the visual encoder depths and connects learnable prompts to the separated visual depths, enabling different prompts to capture the hierarchical contextual depths of visual representations. Furthermore, to maximize the advantages of multi-prompt learning, we incorporate prior information from manually designed templates and learnable multi-prompts, thus improving the generalization capabilities of our approach. We evaluate the effectiveness of our approach on three challenging tasks: new class generalization, cross-dataset evaluation, and domain generalization. For instance, our method achieves a $79.28$ harmonic mean, averaged over 11 diverse image recognition datasets ($+7.62$ compared to CoOp), demonstrating significant competitiveness compared to state-of-the-art prompting methods.", "title": "multiprompt with depth partitioned crossmodal learning", "url": "https://arxiv.org/pdf/2305.06221", "tokenized_text": "recent_years recent years soft learning methods proposed fine tune large scale vision language pre trained downstream_tasks downstream tasks methods typically combine learnable textual tokens class tokens input frozen parameters employ single describe class contexts failing capture categories diverse attributes adequately study introduces multi modal multi modal prompting_technique technique extends soft single learnable multiple method divides visual encoder depths connects learnable visual depths enabling different capture hierarchical contextual depths visual representations furthermore maximize advantages multi learning incorporate prior information manually designed templates learnable multi improving generalization capabilities approach evaluate effectiveness approach challenging tasks new class generalization cross dataset evaluation domain generalization instance method_achieves method achieves harmonic mean averaged 11 diverse image recognition datasets compared coop demonstrating significant compared state art methods"}
{"id": "55e3fe05598be7c3dd357d51166869f6571b824f", "abstract": "Video game testing requires game-specific knowledge as well as common sense reasoning about the events in the game. While AI-driven agents can satisfy the first requirement, it is not yet possible to meet the second requirement automatically. Therefore, video game testing often still relies on manual testing, and human testers are required to play the game thoroughly to detect bugs. As a result, it is challenging to fully automate game testing. In this study, we explore the possibility of leveraging the zero-shot capabilities of large language models for video game bug detection. By formulating the bug detection problem as a question-answering task, we show that large language models can identify which event is buggy in a sequence of textual descriptions of events from a game. To this end, we introduce the GameBugDescriptions benchmark dataset, which consists of 167 buggy gameplay videos and a total of 334 question-answer pairs across 8 games. We extensively evaluate the performance of six models across the OPT and InstructGPT large language model families on our benchmark dataset. Our results show promising results for employing language models to detect video game bugs. With the proper prompting technique, we could achieve an accuracy of 70.66%, and on some video games, up to 78.94%. Our code, evaluation data and the benchmark can be found on https://asgaardlab.github.io/LLMxBugs", "title": "large language models are pretty good zeroshot video game bug detectors", "url": "http://arxiv.org/pdf/2210.02506", "tokenized_text": "video game testing requires game specific knowledge common sense reasoning events game ai driven agents satisfy requirement possible meet second requirement automatically video game testing relies manual testing human required play game thoroughly detect bugs result challenging fully automate game testing study explore possibility leveraging zero shot capabilities large_language large language video game bug detection formulating bug detection problem question answering task large_language large language identify event buggy sequence textual descriptions events game end introduce benchmark dataset consists buggy gameplay videos total question answer pairs games extensively evaluate performance opt instructgpt large_language large language families benchmark dataset results promising_results promising results employing language_models language detect video game bugs proper prompting_technique technique achieve accuracy video games code evaluation data benchmark found"}
{"id": "5ba1e498665d2b3536cb436f0cf484dce03459fe", "abstract": "Controlling the text generated by language models and customizing the content has been a long-standing challenge. Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable method for their task. The effort associated with those techniques, such as in writing examples, explanations, instructions, etc. further limits their adoption among non-expert users. In this paper, we propose a simple prompting strategy HELP ME THINK where we encourage GPT3 to help non-expert users by asking a set of relevant questions and leveraging user answers to execute the task. We demonstrate the efficacy of our technique HELP ME THINK on a variety of tasks. Specifically, we focus on tasks that are hard for average humans and require significant thinking to perform. We hope our work will encourage the development of unconventional ways to harness the power of large language models.", "title": "help me think a simple prompting strategy for nonexperts to create customized content with models", "url": "http://arxiv.org/pdf/2208.08232", "tokenized_text": "controlling text generated language_models language customizing content long standing challenge existing prompting_techniques techniques proposed pursuit providing control task specific lack generality provides choices non expert users find suitable method task effort associated techniques writing examples explanations instructions etc limits adoption non expert users paper propose simple strategy help think encourage gpt3 help non expert users asking set relevant questions leveraging user answers execute task demonstrate efficacy technique help think variety tasks specifically focus tasks hard average humans require significant thinking perform hope work encourage development ways harness power large_language large language"}
{"id": "657e364ec6932558f426583dc31953e547bf6575", "abstract": "This paper discusses our approaches for task-oriented conversational modelling using subjective knowledge, with a particular emphasis on response generation. Our methodology was shaped by an extensive data analysis that evaluated key factors such as response length, sentiment, and dialogue acts present in the provided dataset. We used few-shot learning to augment the data with newly generated subjective knowledge items and present three approaches for DSTC11: (1) task-specific model exploration, (2) incorporation of the most frequent question into all generated responses, and (3) a waterfall prompting technique using a combination of both GPT-3 and ChatGPT.", "title": "leveraging fewshot data augmentation and waterfall prompting for response generation", "url": "https://arxiv.org/pdf/2308.01080", "tokenized_text": "paper discusses approaches task oriented conversational modelling subjective knowledge particular emphasis response generation methodology extensive data analysis evaluated key factors response length sentiment dialogue acts present provided dataset shot_learning shot learning augment data newly generated subjective knowledge items present approaches dstc11 task specific exploration incorporation frequent question generated responses prompting_technique technique combination gpt-3 chatgpt"}
{"id": "67455478e77c8672d0dd08f89735a8813bbfec65", "abstract": "This paper presents the FormAI dataset, a large collection of 112, 000 AI-generated compilable and independent C programs with vulnerability classification. We introduce a dynamic zero-shot prompting technique constructed to spawn diverse programs utilizing Large Language Models (LLMs). The dataset is generated by GPT-3.5-turbo and comprises programs with varying levels of complexity. Some programs handle complicated tasks like network management, table games, or encryption, while others deal with simpler tasks like string manipulation. Every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name. This is accomplished by employing a formal verification method using the Efficient SMT-based Bounded Model Checker (ESBMC), which uses model checking, abstract interpretation, constraint programming, and satisfiability modulo theories to reason over safety/security properties in programs. This approach definitively detects vulnerabilities and offers a formal model known as a counterexample, thus eliminating the possibility of generating false positive reports. We have associated the identified vulnerabilities with Common Weakness Enumeration (CWE) numbers. We make the source code available for the 112, 000 programs, accompanied by a separate file containing the vulnerabilities detected in each program, making the dataset ideal for training LLMs and machine learning algorithms. Our study unveiled that according to ESBMC, 51.24% of the programs generated by GPT-3.5 contained vulnerabilities, thereby presenting considerable risks to software safety and security.", "title": "the formai dataset generative ai in software security through the lens of formal verification", "url": "https://arxiv.org/pdf/2307.02192", "tokenized_text": "paper_presents paper presents dataset large collection 000 ai generated compilable independent programs vulnerability classification introduce dynamic zero shot_prompting shot technique constructed diverse programs utilizing large_language large language llms dataset generated gpt-3.5 turbo comprises programs varying levels complexity programs handle complicated tasks like network management table games deal simpler tasks like string manipulation program labeled vulnerabilities found source_code source code indicating type line number vulnerable function accomplished employing formal verification method efficient smt based bounded checker uses checking abstract interpretation constraint programming satisfiability modulo theories reason safety security properties programs approach detects vulnerabilities offers formal known eliminating possibility generating false positive reports associated identified vulnerabilities common weakness enumeration numbers source_code source code available 000 programs accompanied separate file containing vulnerabilities detected program making dataset ideal training llms machine_learning machine learning algorithms study unveiled according programs generated gpt-3.5 contained vulnerabilities presenting considerable risks software safety security"}
{"id": "674c5ec7b144aea1f6b143baeb17cc839f52416e", "abstract": "The Rust programming language, with its safety guarantees, has established itself as a viable choice for low-level systems programming language over the traditional, unsafe alternatives like C/C++. These guarantees come from a strong ownership-based type system, as well as primitive support for features like closures, pattern matching, etc., that make the code more concise and amenable to reasoning. These unique Rust features also pose a steep learning curve for programmers. This paper presents a tool called RustAssistant that leverages the emergent capabilities of Large Language Models (LLMs) to automatically suggest fixes for Rust compilation errors. RustAssistant uses a careful combination of prompting techniques as well as iteration with an LLM to deliver high accuracy of fixes. RustAssistant is able to achieve an impressive peak accuracy of roughly 74% on real-world compilation errors in popular open-source Rust repositories. We plan to release our dataset of Rust compilation errors to enable further research.", "title": "fixing rust compilation errors using llms", "url": "https://arxiv.org/pdf/2308.05177", "tokenized_text": "programming language safety guarantees established viable choice low level systems programming language traditional unsafe alternatives like c++ guarantees come strong ownership based type system primitive support features like pattern matching etc code concise amenable reasoning unique features pose learning curve programmers paper_presents paper presents tool called leverages emergent capabilities large_language large language llms automatically suggest fixes errors uses careful combination prompting_techniques techniques iteration llm deliver high accuracy fixes able achieve impressive peak accuracy roughly real world errors popular open source repositories plan release dataset errors enable research"}
{"id": "69619a2a47faee7a29ec596db13172e2a42ff921", "abstract": "Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques.", "title": "synthetic prompting generating chainofthought demonstrations for large language models", "url": "http://arxiv.org/pdf/2302.00618", "tokenized_text": "large_language large language perform reasoning tasks chain thought_prompting thought guides find answers step step demonstrations quality depends demonstrations given creating hand costly introduce synthetic method leverages handcrafted examples generate examples selects effective demonstrations elicit better reasoning method backward forward process generate new examples backward process generates question match sampled reasoning chain question solvable clear forward process produces detailed reasoning chain question improving quality example evaluate method numerical symbolic algorithmic reasoning tasks outperforms existing prompting_techniques techniques"}
{"id": "7307ee3c819c34b7c93ccbbd330a4c889956b36f", "abstract": "Large pre-trained language models have exhibited unprecedented capabilities in producing high-quality text via prompting techniques. This fact introduces new possibilities for data collection and annotation, particularly in situations where such data is scarce, complex to gather, expensive, or even sensitive. In this paper, we explore the potential of these models to generate and annotate goal-oriented dialogues, and conduct an in-depth analysis to evaluate their quality. Our experiments employ ChatGPT, and encompass three categories of goal-oriented dialogues (task-oriented, collaborative, and explanatory), two generation modes (interactive and one-shot), and two languages (English and Italian). Based on extensive human-based evaluations, we demonstrate that the quality of generated dialogues and annotations is on par with those generated by humans.", "title": "unraveling chatgpt a critical analysis of aigenerated goaloriented dialogues and annotations", "url": "http://arxiv.org/pdf/2305.14556", "tokenized_text": "large pre trained_language trained language exhibited unprecedented capabilities producing high quality text prompting_techniques techniques fact introduces new possibilities data collection annotation particularly situations data scarce complex gather expensive sensitive paper explore potential generate annotate goal oriented dialogues conduct depth analysis evaluate quality experiments employ chatgpt encompass categories goal oriented dialogues task oriented collaborative explanatory generation modes interactive shot languages english italian based extensive human based evaluations demonstrate quality generated dialogues annotations par generated humans"}
{"id": "748a2700ec11f51560a69ec05c67ca9f97014be7", "abstract": "This paper investigates models of event implications. Specifically, how well models predict entity state-changes, by targeting their understanding of physical attributes. Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world. Conversely, we also demonstrate that existing approaches often misrepresent the surprising abilities of LLMs via improper task encodings and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks. In particular, our results indicate that our prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available.", "title": "events realm event reasoning of entity states via language models", "url": "https://arxiv.org/pdf/2211.05392", "tokenized_text": "paper investigates event implications specifically predict entity state changes targeting understanding physical attributes large_language large language llm exposed procedural knowledge objects interact benchmarking shows fail reason world conversely demonstrate existing approaches misrepresent surprising abilities llms task encodings proper dramatically improve performance reported baseline results multiple tasks particular results_indicate results indicate prompting_technique technique especially useful unseen attributes domain limited data available"}
{"id": "8db1dcae055842f43ccac04182957b20d15bbe6b", "abstract": "While forward reasoning (i.e. find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information? In this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that can be solved by an external solver, and Check your Work exploits the availability of natural verifier of high accuracy in the forward direction, interleaving solving and verification steps. Finally, realizing that each of our base methods correctly solves a different set of problems, we propose a novel Bayesian formulation for creating an ensemble over these base methods aided by a verifier to further boost the accuracy by a significant margin. Extensive experimentation demonstrates that our techniques successively improve the performance of LLMs on the backward reasoning task, with the final ensemble-based method resulting in a substantial performance gain compared to the raw LLMs with standard prompting techniques such as chain-of-thought.", "title": "fill in the blank exploring and enhancing llm capabilities for backward reasoning in math word problems", "url": "https://arxiv.org/pdf/2310.01991", "tokenized_text": "forward reasoning i.e. find answer given question explored extensively recent literature backward reasoning relatively unexplored examine backward reasoning capabilities llms math word problems mwps given mathematical question answer details question llms effectively retrieve missing information paper formally define backward reasoning task math word problems modify datasets evaluate task gsm8k svamp multiarith findings significant drop accuracy backward reasoning compared forward reasoning sota llms gpt4 palm-2 llama-2 utilizing specific format task propose novel techniques improve performance rephrase reformulates given problem forward reasoning problem pal tools combines idea program aided llms produce set equations solved external solver check work exploits availability natural verifier high accuracy forward direction interleaving solving verification steps finally realizing base methods correctly solves different set problems propose_a_novel propose novel bayesian formulation creating ensemble base methods aided verifier boost accuracy significant margin extensive experimentation demonstrates techniques improve performance llms backward reasoning task final ensemble based method resulting substantial performance gain compared raw llms standard prompting_techniques techniques chain thought"}
{"id": "8efc20988021ce3b4b05dd44b13e27260ee9b99b", "abstract": "Visual question answering (VQA) is a challenging task that requires the ability to comprehend and reason with visual information. While recent vision-language models have made strides, they continue to struggle with zero-shot VQA, particularly in handling complex compositional questions and adapting to new domains i.e. knowledge-based reasoning. This paper explores the use of various prompting strategies, focusing on the BLIP2 model, to enhance zero-shot VQA performance. We conduct a comprehensive investigation across several VQA datasets, examining the effectiveness of different question templates, the role of few-shot exemplars, the impact of chain-of-thought (CoT) reasoning, and the benefits of incorporating image captions as additional visual cues. Despite the varied outcomes, our findings demonstrate that carefully designed question templates and the integration of additional visual cues, like image captions, can contribute to improved VQA performance, especially when used in conjunction with few-shot examples. However, we also identify a limitation in the use of chain-of-thought rationalization, which negatively affects VQA accuracy. Our study thus provides critical insights into the potential of prompting for improving zero-shot VQA performance.", "title": "investigating prompting techniques for zero and fewshot visual question answering", "url": "http://arxiv.org/pdf/2306.09996", "tokenized_text": "visual question_answering question answering vqa challenging task requires ability comprehend reason visual information recent vision language_models language strides continue struggle zero shot vqa particularly handling complex compositional questions adapting new domains i.e. knowledge based reasoning paper explores use strategies focusing blip2 enhance zero shot vqa performance conduct comprehensive investigation vqa datasets examining effectiveness different question templates role shot exemplars impact chain thought cot reasoning benefits incorporating image captions additional visual cues despite varied outcomes findings demonstrate carefully designed question templates integration additional visual cues like image captions contribute improved vqa performance especially conjunction shot examples identify limitation use chain thought rationalization negatively affects vqa accuracy study provides critical insights potential improving zero shot vqa performance"}
{"id": "99070fb6df9e8d11e30f7aaefcc9f0b0c5a73789", "abstract": "Conversational agents show the promise to allow users to interact with mobile devices using language. However, to perform diverse UI tasks with natural language, developers typically need to create separate datasets and models for each specific task, which is expensive and effort-consuming. Recently, pre-trained large language models (LLMs) have been shown capable of generalizing to various downstream tasks when prompted with a handful of examples from the target task. This paper investigates the feasibility of enabling versatile conversational interactions with mobile UIs using a single LLM. We designed prompting techniques to adapt an LLM to mobile UIs. We experimented with four important modeling tasks that address various scenarios in conversational interaction. Our method achieved competitive performance on these challenging tasks without requiring dedicated datasets and training, offering a lightweight and generalizable approach to enable language-based mobile interaction.", "title": "enabling conversational interaction with mobile ui using large language models", "url": "https://dl.acm.org/doi/pdf/10.1145/3544548.3580895", "tokenized_text": "conversational agents promise allow users interact mobile devices language perform diverse ui tasks natural_language natural language developers typically need create separate datasets specific task expensive effort consuming recently pre trained large_language large language llms shown capable generalizing downstream_tasks downstream tasks prompted handful examples target task paper investigates feasibility enabling versatile conversational interactions mobile uis single llm designed prompting_techniques techniques adapt llm mobile uis experimented important modeling tasks address scenarios conversational interaction method achieved competitive_performance competitive performance challenging tasks requiring dedicated datasets training offering lightweight generalizable approach enable language based mobile interaction"}
{"id": "a01a9c4a114fbf201540268f928ccf77bc3f9357", "abstract": "Vision-Language Models (VLMs), such as CLIP, have demonstrated impressive zero-shot transfer capabilities in image-level visual perception. However, these models have shown limited performance in instance-level tasks that demand precise localization and recognition. Previous works have suggested that incorporating visual prompts, such as colorful boxes or circles, can improve the ability of models to recognize objects of interest. Nonetheless, compared to language prompting, visual prompting designs are rarely explored. Existing approaches, which employ coarse visual cues such as colorful boxes or circles, often result in sub-optimal performance due to the inclusion of irrelevant and noisy pixels. In this paper, we carefully study the visual prompting designs by exploring more fine-grained markings, such as segmentation masks and their variations. In addition, we introduce a new zero-shot framework that leverages pixel-level annotations acquired from a generalist segmentation model for fine-grained visual prompting. Consequently, our investigation reveals that a straightforward application of blur outside the target mask, referred to as the Blur Reverse Mask, exhibits exceptional effectiveness. This proposed prompting strategy leverages the precise mask annotations to reduce focus on weakly related regions while retaining spatial coherence between the target and the surrounding background. Our Fine-Grained Visual Prompting (FGVP) demonstrates superior performance in zero-shot comprehension of referring expressions on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks. It outperforms prior methods by an average margin of 3.0% to 4.6%, with a maximum improvement of 12.5% on the RefCOCO+ testA subset. The part detection experiments conducted on the PACO dataset further validate the preponderance of FGVP over existing visual prompting techniques. Code and models will be made available.", "title": "finegrained visual prompting", "url": "http://arxiv.org/pdf/2306.04356", "tokenized_text": "vision language_models language vlms clip demonstrated impressive zero shot transfer capabilities image level visual perception shown limited performance instance level tasks demand precise localization recognition previous works suggested incorporating visual boxes circles improve ability recognize objects interest nonetheless compared language visual designs rarely explored existing approaches employ coarse visual cues boxes circles result sub optimal performance inclusion irrelevant noisy pixels paper carefully study visual designs exploring fine grained segmentation masks variations addition introduce new zero shot framework leverages pixel level annotations acquired generalist segmentation fine grained visual consequently investigation reveals straightforward application blur outside target mask referred blur reverse mask exhibits exceptional effectiveness proposed strategy leverages precise mask annotations reduce focus weakly related regions retaining spatial coherence target surrounding background fine grained visual demonstrates superior_performance superior performance zero shot comprehension referring expressions benchmarks outperforms prior methods average margin maximum improvement 12.5 subset detection experiments conducted dataset validate existing visual prompting_techniques techniques code available"}
{"id": "a86e12654376323b712dd3d39d5ff22283f87a7b", "abstract": "As large language models increase in capability, researchers have started to conduct surveys of all kinds on these models with varying scientific motivations. In this work, we examine what we can learn from language models' survey responses on the basis of the well-established American Community Survey (ACS) by the U.S. Census Bureau. Using a de-facto standard multiple-choice prompting technique and evaluating 40 different language models, hundreds of thousands of times each on questions from the ACS, we systematically establish two dominant patterns. First, models have significant position and labeling biases, for example, towards survey responses labeled with the letter\"A\". Second, when adjusting for labeling biases through randomized answer ordering, models across the board trend towards uniformly random survey responses. In fact, binary classifiers can almost perfectly differentiate between models' responses to the ACS and the responses of the US census. Taken together, our findings suggest caution in treating survey responses from language models as equivalent to those of human populations at present time.", "title": "questioning the survey responses of large language models", "url": "https://arxiv.org/pdf/2306.07951", "tokenized_text": "large_language large language increase capability researchers started conduct surveys kinds varying scientific motivations work examine learn language_models language survey responses basis established american community survey u.s. de facto standard multiple choice prompting_technique technique evaluating 40 different language_models language hundreds thousands times questions systematically establish dominant patterns significant position labeling biases example survey responses labeled second adjusting labeling biases randomized answer ordering board trend uniformly random survey responses fact binary classifiers perfectly differentiate responses responses taken findings_suggest findings suggest caution treating survey responses language_models language equivalent human populations present time"}
{"id": "b626560f19f815808a289ef5c24a17c57320da70", "abstract": "Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose \u2018MathPrompter\u2019, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the \u2018MultiArith\u2019 dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM.", "title": "mathprompter mathematical reasoning using large language models", "url": "http://arxiv.org/pdf/2303.05398", "tokenized_text": "large_language large language llms limited performance solving arithmetic reasoning tasks provide incorrect answers unlike natural_language natural language understanding math problems typically single correct answer making task generating accurate solutions challenging llms best knowledge aware llms indicate level confidence responses trust adoption address deficiency propose technique improves performance llms arithmetic problems increased reliance predictions uses zero shot chain thought_prompting thought technique generate multiple expressions python functions solve math problem different ways raise confidence level output results contrast prompt_based based cot methods check validity intermediate steps followed technique improves state art multiarith dataset 78.7 92.5 evaluated 175b parameter gpt based llm"}
{"id": "ba4aa83248a1d08b521392eb971e47d10b7c74e1", "abstract": "Recent advancements in large-scale models, such as GPT-4, have showcased remarkable capabilities in addressing standard queries. However, when facing complex problems that require multi-step logical reasoning, their accuracy dramatically decreases. Current research has explored the realm of \\textit{prompting engineering} to bolster the inferential capacities of these models. Our paper unveils a pioneering prompting technique, dubbed \\textit{Graph of Thoughts (GoT)}. Through testing on a trio of escalating challenges: the 24-point game, resolution of high-degree polynomial equations, and derivation of formulas for recursive sequences, our method outperformed GPT-4, achieving accuracy improvements of $89.7\\%$, $86\\%$, and $56\\%$ for each respective task. Moreover, when juxtaposed with the state-of-the-art (SOTA) prompting method, \\textit{Tree of Thought (ToT)}, our approach registered an average accuracy boost of $23\\%$, $24\\%$, and $15\\%$.", "title": "boosting logical reasoning in large language models through a new framework the graph of thought", "url": "https://arxiv.org/pdf/2308.08614", "tokenized_text": "recent advancements large scale gpt-4 showcased remarkable_capabilities remarkable capabilities addressing standard queries facing complex problems require multi step logical reasoning accuracy dramatically decreases current research explored realm engineering inferential capacities paper unveils pioneering prompting_technique technique dubbed thoughts testing escalating challenges 24 point game resolution high degree equations derivation formulas recursive sequences method outperformed gpt-4 achieving accuracy improvements respective task state art sota method thought approach average accuracy boost"}
{"id": "c20b18d6b919695a69e416debf8bf1ffeac03992", "abstract": "Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are publicly available at https://github.com/XinyuanLu00/SciTab.", "title": "scitab a challenging benchmark for compositional reasoning and claim verification on scientific tables", "url": "http://arxiv.org/pdf/2305.13186", "tokenized_text": "current scientific fact checking benchmarks exhibit shortcomings biases arising crowd sourced claims reliance text based evidence present challenging evaluation dataset consisting 1.2 expert verified scientific claims originate scientific publications require compositional reasoning verification claims paired evidence containing scientific tables annotated labels extensive evaluations demonstrate poses significant challenge state art including table based pretraining large_language large language gpt-4 achieved performance barely random guessing popular prompting_techniques techniques chain thought achieve performance gains analysis uncovers unique challenges posed including table grounding claim ambiguity compositional reasoning codes data publicly_available publicly available"}
{"id": "c218cd1772999517b137bbbc9872c4f67e540b7f", "abstract": "We conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the Super-NaturalInstructions benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model\u2019s performance when the model is finetuned, while positively affecting the non-finetuned counterpart. Moreover, we observe a slight yet consistent increase in classification accuracy as we incorporate explanations during prompting and finetuning, respectively. Finally, we offer insights on which reasoning skills benefit the most from incorporating explanations during finetuning and prompting, such as Numerical (+20.4%) and Analogical (+13.9%) reasoning, as well as skills that exhibit negligible or negative effects.", "title": "optr exploring the role of explanations in finetuning and prompting for reasoning skills of large language models", "url": "https://aclanthology.org/2023.nlrse-1.10.pdf", "tokenized_text": "conduct thorough investigation reasoning capabilities large_language large language llms focusing specifically open pretrained transformers representative study entails finetuning different sizes opt carefully curated reasoning corpus resulting sets finetuned opt finetuned explanations opt finetuned explanations evaluate 57 domain tasks drawn super naturalinstructions benchmark covering 26 distinct reasoning skills utilizing prompting_techniques techniques comprehensive grid 27 configurations test evaluations investigate dimensions finetuning scale understand role explanations different reasoning skills findings reveal having explanations fewshot exemplar significant impact performance finetuned positively affecting non finetuned counterpart observe slight consistent increase classification accuracy incorporate explanations finetuning respectively finally offer insights reasoning skills benefit incorporating explanations finetuning numerical analogical reasoning skills exhibit negligible negative effects"}
{"id": "d1bd7ae97588eccfbcd31ffce4fc924d12a5de4d", "abstract": "Cross-lingual Machine Translation (MT) quality estimation plays a crucial role in evaluating translation performance. GEMBA, the first MT quality assessment metric based on Large Language Models (LLMs), employs one-step prompting to achieve state-of-the-art (SOTA) in system-level MT quality estimation; however, it lacks segment-level analysis. In contrast, Chain-of-Thought (CoT) prompting outperforms one-step prompting by offering improved reasoning and explainability. In this paper, we introduce Knowledge-Prompted Estimator (KPE), a CoT prompting method that combines three one-step prompting techniques, including perplexity, token-level similarity, and sentence-level similarity. This method attains enhanced performance for segment-level estimation compared with previous deep learning models and one-step prompting approaches. Furthermore, supplementary experiments on word-level visualized alignment demonstrate that our KPE method significantly improves token alignment compared with earlier models and provides better interpretability for MT quality estimation. Code will be released upon publication.", "title": "knowledgeprompted estimator a novel approach to explainable machine translation assessment", "url": "http://arxiv.org/pdf/2306.07486", "tokenized_text": "cross lingual machine_translation machine translation mt quality estimation plays crucial role evaluating translation performance gemba mt quality assessment metric based large_language large language llms employs step achieve state art sota system level mt quality estimation lacks segment level analysis contrast chain thought cot outperforms step offering improved reasoning explainability paper introduce knowledge prompted estimator cot_prompting cot method combines step prompting_techniques techniques including perplexity token level similarity sentence level similarity method attains enhanced performance segment level estimation compared previous deep learning step approaches furthermore supplementary experiments word level visualized alignment demonstrate method significantly improves token alignment compared earlier provides better interpretability mt quality estimation code released publication"}
{"id": "ddc9aeac18638575bbb90ede4c6829ec15c2947e", "abstract": "Language Models (LMs) have proven to be useful in various downstream applications, such as summarisation, translation, question answering and text classification. LMs are becoming increasingly important tools in Artificial Intelligence, because of the vast quantity of information they can store. In this work, we present ProP (Prompting as Probing), which utilizes GPT-3, a large Language Model originally proposed by OpenAI in 2020, to perform the task of Knowledge Base Construction (KBC). ProP implements a multi-step approach that combines a variety of prompting techniques to achieve this. Our results show that manual prompt curation is essential, that the LM must be encouraged to give answer sets of variable lengths, in particular including empty answer sets, that true/false questions are a useful device to increase precision on suggestions generated by the LM, that the size of the LM is a crucial factor, and that a dictionary of entity aliases improves the LM score. Our evaluation study indicates that these proposed techniques can substantially enhance the quality of the final predictions: ProP won track 2 of the LM-KBC competition, outperforming the baseline by 36.4 percentage points. Our implementation is available on https://github.com/HEmile/iswc-challenge.", "title": "prompting as probing using language models for knowledge base construction", "url": "http://arxiv.org/pdf/2208.11057", "tokenized_text": "language_models language lms proven useful downstream applications summarisation translation question_answering question answering text_classification text classification lms increasingly important tools artificial_intelligence artificial intelligence vast quantity information store work present probing utilizes gpt-3 large_language large language originally proposed openai 2020 perform task knowledge base construction implements multi step approach combines variety prompting_techniques techniques achieve results manual curation essential lm encouraged answer sets variable lengths particular including answer sets true false questions useful device increase precision suggestions generated lm size lm crucial factor dictionary entity improves lm score evaluation study indicates proposed techniques substantially enhance quality final predictions track lm competition outperforming baseline percentage points implementation available"}
{"id": "def24fb1e977db69f4b1b866b807f9ab9bad5227", "abstract": "The emergence of large language models (LLMs) such as ChatGPT has disrupted the landscape of software development. Many studies are investigating the quality of responses generated by ChatGPT, the efficacy of various prompting techniques, and its comparative performance in programming contests, to name a few examples. Yet, we know very little about how ChatGPT is actually used by software developers. What questions do developers present to ChatGPT? What are the dynamics of these interactions? What is the backdrop against which these conversations are held, and how do the conversations feedback into the artifacts of their work? To close this gap, we introduce DevGPT, a curated dataset which encompasses 17,913 prompts and ChatGPT's responses including 11,751 code snippets, coupled with the corresponding software development artifacts -- ranging from source code, commits, issues, pull requests, to discussions and Hacker News threads -- to enable the analysis of the context and implications of these developer interactions with ChatGPT.", "title": "devgpt studying developerchatgpt conversations", "url": "https://arxiv.org/pdf/2309.03914", "tokenized_text": "emergence large_language large language llms chatgpt landscape software development studies investigating quality responses generated chatgpt efficacy prompting_techniques techniques comparative performance programming examples know little chatgpt actually software developers questions developers present chatgpt dynamics interactions conversations held conversations feedback artifacts work close gap introduce curated dataset encompasses chatgpt responses including code snippets coupled corresponding software development artifacts ranging source_code source code commits issues requests discussions hacker news threads enable analysis context implications developer interactions chatgpt"}
{"id": "e61a96cf602ebff6683929aaf916e25614a475bc", "abstract": "Large Language Models (LLMs) have demonstrated impressive inferential capabilities, with numerous research endeavors devoted to enhancing this capacity through prompting. Despite these efforts, a unified epistemological foundation is still conspicuously absent. Drawing inspiration from Kant's a priori philosophy, we propose the UPAR prompting framework, designed to emulate the structure of human cognition within LLMs. The UPAR framework is delineated into four phases:\"Understand\",\"Plan\",\"Act\", and\"Reflect\", enabling the extraction of structured information from complex contexts, prior planning of solutions, execution according to plan, and self-reflection. This structure significantly augments the explainability and accuracy of LLM inference, producing a human-understandable and inspectable inferential trajectory. Furthermore, our work offers an epistemological foundation for existing prompting techniques, allowing for a possible systematic integration of these methods. With GPT-4, our approach elevates the accuracy from COT baseline of 22.92% to 58.33% in a challenging subset of GSM8K, and from 67.91% to 75.40% in the causal judgment task.", "title": "upar a kantianinspired prompting framework for enhancing large language model capabilities", "url": "https://arxiv.org/pdf/2310.01441", "tokenized_text": "large_language large language llms demonstrated impressive inferential capabilities numerous research endeavors devoted enhancing capacity despite efforts unified foundation conspicuously absent drawing inspiration priori philosophy propose framework designed emulate structure human cognition llms framework enabling extraction structured information complex contexts prior planning solutions execution according plan self reflection structure significantly augments explainability accuracy llm inference producing human understandable inferential trajectory furthermore work offers foundation existing prompting_techniques techniques allowing possible systematic integration methods gpt-4 approach elevates accuracy cot baseline challenging subset gsm8 causal judgment task"}
{"id": "ed5ebed7ff668fd7362d531a40b49b3aea33b3a9", "abstract": "Generated texts from large pretrained language models have been shown to exhibit a variety of harmful, human-like biases about various demographics. These findings prompted large efforts aiming to understand and measure such effects, with the goal of providing benchmarks that can guide the development of techniques mitigating these stereotypical associations. However, as recent research has pointed out, the current benchmarks lack a robust experimental setup, consequently hindering the inference of meaningful conclusions from their evaluation metrics. In this paper, we extend these arguments and demonstrate that existing techniques and benchmarks aiming to measure stereotypes tend to be inaccurate and consist of a high degree of experimental noise that severely limits the knowledge we can gain from benchmarking language models based on them. Accordingly, we propose a new framework for robustly measuring and quantifying biases exhibited by generative language models. Finally, we use this framework to investigate GPT-3's occupational gender bias and propose prompting techniques for mitigating these biases without the need for fine-tuning.", "title": "understanding stereotypes in language models towards robust measurement and zeroshot debiasing", "url": "http://arxiv.org/pdf/2212.10678", "tokenized_text": "generated texts large pretrained_language pretrained language shown exhibit variety harmful human like biases demographics findings prompted large efforts aiming understand measure effects goal providing benchmarks guide development techniques mitigating stereotypical associations recent research current benchmarks lack robust experimental setup consequently hindering inference meaningful conclusions evaluation metrics paper extend arguments demonstrate existing techniques benchmarks aiming measure stereotypes tend inaccurate consist high degree experimental noise severely limits knowledge gain benchmarking language_models language based accordingly propose_a_new propose new framework robustly measuring quantifying biases exhibited generative language_models language finally use framework investigate gpt-3 occupational gender bias propose prompting_techniques techniques mitigating biases need fine tuning"}
{"id": "f330f502bf1e92fabf7f246597fa9320d956c0c8", "abstract": "The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret. In this paper, we present a framework for systematically measuring the success of prompt extraction attacks. In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.", "title": "prompts should not be seen as secrets systematically measuring prompt extraction attack success", "url": "https://arxiv.org/pdf/2307.06865", "tokenized_text": "generations large_language large language commonly controlled prompting_techniques techniques user query aims guide behaviour query companies guide treated hidden user making query treated evidence showing extracted user kept paper present framework systematically measuring success extraction attacks experiments multiple sources multiple underlying language_models language find simple text based attacks fact reveal high probability"}
{"id": "f669d7a6fab0147253178a6fc854e05e3d92fb3f", "abstract": "The revolution of artificial intelligence content generation has been rapidly accelerated with the booming text-to-image (T2I) diffusion models. Within just two years of development, it was unprecedentedly of high-quality, diversity, and creativity that the state-of-the-art models could generate. However, a prevalent limitation persists in the effective communication with these popular T2I models, such as Stable Diffusion, using natural language descriptions. This typically makes an engaging image hard to obtain without expertise in prompt engineering with complex word compositions, magic tags, and annotations. Inspired by the recently released DALLE3 - a T2I model directly built-in ChatGPT that talks human language, we revisit the existing T2I systems endeavoring to align human intent and introduce a new task - interactive text to image (iT2I), where people can interact with LLM for interleaved high-quality image generation/edit/refinement and question answering with stronger images and text correspondences using natural language. In addressing the iT2I problem, we present a simple approach that augments LLMs for iT2I with prompting techniques and off-the-shelf T2I models. We evaluate our approach for iT2I in a variety of common-used scenarios under different LLMs, e.g., ChatGPT, LLAMA, Baichuan, and InternLM. We demonstrate that our approach could be a convenient and low-cost way to introduce the iT2I ability for any existing LLMs and any text-to-image models without any training while bringing little degradation on LLMs' inherent capabilities in, e.g., question answering and code generation. We hope this work could draw broader attention and provide inspiration for boosting user experience in human-machine interactions alongside the image quality of the next-generation T2I systems.", "title": "minidalle3 interactive text to image by prompting large language models", "url": "https://arxiv.org/pdf/2310.07653", "tokenized_text": "revolution artificial_intelligence artificial intelligence content generation rapidly accelerated text image t2i diffusion years development high quality diversity creativity state art generate prevalent limitation effective communication popular t2i stable_diffusion stable diffusion natural_language natural language descriptions typically makes engaging image hard obtain expertise prompt_engineering engineering complex word compositions magic tags annotations inspired recently released t2i directly built chatgpt human language revisit existing t2i systems align human intent introduce new task interactive text_to_image text image people interact llm interleaved high quality image_generation image generation edit refinement question_answering question answering stronger images text correspondences natural_language natural language addressing problem present simple approach augments llms prompting_techniques techniques shelf t2i evaluate approach variety common scenarios different llms e.g. chatgpt llama demonstrate approach convenient low cost way introduce ability existing llms text image training bringing little degradation llms inherent capabilities e.g. question_answering question answering code_generation code generation hope work draw broader attention provide inspiration boosting user experience human machine interactions alongside image quality generation t2i systems"}
{"id": "fc9bd3642df2a378c11131362b27deecbd02b70a", "abstract": "Large-scale foundation models, such as CLIP, have demonstrated remarkable success in visual recognition tasks by embedding images in a semantically rich space. Self-supervised learning (SSL) has also shown promise in improving visual recognition by learning invariant features. However, the combination of CLIP with SSL is found to face challenges due to the multi-task framework that blends CLIP's contrastive loss and SSL's loss, including difficulties with loss weighting and inconsistency among different views of images in CLIP's output space. To overcome these challenges, we propose a prompt learning-based model called GOPro, which is a unified framework that ensures similarity between various augmented views of input images in a shared image-text embedding space, using a pair of learnable image and text projectors atop CLIP, to promote invariance and generalizability. To automatically learn such prompts, we leverage the visual content and style primitives extracted from pre-trained CLIP and adapt them to the target task. In addition to CLIP's cross-domain contrastive loss, we introduce a visual contrastive loss and a novel prompt consistency loss, considering the different views of the images. GOPro is trained end-to-end on all three loss objectives, combining the strengths of CLIP and SSL in a principled manner. Empirical evaluations demonstrate that GOPro outperforms the state-of-the-art prompting techniques on three challenging domain generalization tasks across multiple benchmarks by a significant margin. Our code is available at https://github.com/mainaksingha01/GOPro.", "title": "gopro generate and optimize prompts in clip using selfsupervised learning", "url": "https://arxiv.org/pdf/2308.11605", "tokenized_text": "large scale foundation_models foundation clip demonstrated_remarkable demonstrated remarkable success visual recognition tasks embedding images semantically rich space self supervised learning ssl shown promise improving visual recognition learning invariant features combination clip ssl found face challenges multi task framework clip contrastive loss ssl loss including difficulties loss weighting inconsistency different views images clip output space overcome challenges propose learning based called unified framework ensures similarity augmented views input images shared image text embedding space pair learnable image text atop clip promote generalizability automatically learn leverage visual content style primitives extracted pre trained clip adapt target task addition clip cross domain contrastive loss introduce visual contrastive loss novel consistency loss considering different views images trained end end loss objectives combining strengths clip ssl principled manner empirical evaluations demonstrate outperforms state art prompting_techniques techniques challenging domain generalization tasks multiple benchmarks significant margin code_is_available code available"}
{"id": "fd80f7f3673fc6ca02f192d5d73426f11a4be659", "abstract": "Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.", "title": "the devil is in the errors leveraging large language models for finegrained machine translation evaluation", "url": "https://arxiv.org/pdf/2308.07286", "tokenized_text": "automatic evaluation machine_translation machine translation mt critical tool driving rapid iterative development mt_systems mt systems considerable progress estimating single quality score current metrics lack informativeness detailed schemes annotate individual errors quality metrics mqm paper help fill gap proposing prompting_technique technique leverages reasoning context_learning context learning capabilities large_language large language llms asks identify categorize errors translations start evaluating recent llms palm palm-2 simple score prediction study impact labeled_data labeled data context_learning context learning finetuning evaluate palm-2 find improves performance compared scores particularly large gains larger providing interpretability error spans align human annotations"}
{"id": "9e93ab728e3e174ec1492009055885a9123d434f", "abstract": "Large language models have revolutionized the field of artificial intelligence and have been used in various applications. Among these models, ChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI, it stands out as a powerful tool that has been widely adopted. ChatGPT has been successfully applied in numerous areas, including chatbots, content generation, language translation, personalized recommendations, and even medical diagnosis and treatment. Its success in these applications can be attributed to its ability to generate human-like responses, understand natural language, and adapt to different contexts. Its versatility and accuracy make it a powerful tool for natural language processing (NLP). However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns. This article provides a comprehensive overview of ChatGPT, its applications, advantages, and limitations. Additionally, the paper emphasizes the importance of ethical considerations when using this robust tool in real-world scenarios. Finally, This paper contributes to ongoing discussions surrounding artificial intelligence and its impact on vision and NLP domains by providing insights into prompt engineering techniques.", "title": "unlocking the potential of chatgpt a comprehensive exploration of its applications, advantages, limitations, and future directions in natural language processing", "url": "http://arxiv.org/pdf/2304.02017", "tokenized_text": "large_language large language revolutionized field artificial_intelligence artificial intelligence applications chatgpt chat generative pre trained transformer developed openai stands powerful tool widely adopted chatgpt successfully applied numerous areas including chatbots content generation language translation personalized recommendations medical diagnosis treatment success applications attributed ability generate human like responses understand natural_language natural language adapt different contexts versatility accuracy powerful tool natural_language natural language processing nlp limitations chatgpt tendency produce biased responses potential harmful language patterns article provides comprehensive overview chatgpt applications advantages limitations additionally paper emphasizes importance ethical considerations robust tool real world_scenarios world scenarios finally paper contributes ongoing discussions surrounding artificial_intelligence artificial intelligence impact vision nlp domains providing insights prompt_engineering engineering techniques"}
{"id": "a7d8a6d8c04bd4554da4219be0f9d3bf87e2e56b", "abstract": "In this paper, we present a novel approach to simulating H.P. Lovecraft's horror literature using the ChatGPT large language model, specifically the GPT-4 architecture. Our study aims to generate text that emulates Lovecraft's unique writing style and themes, while also examining the effectiveness of prompt engineering techniques in guiding the model's output. To achieve this, we curated a prompt containing several specialized literature references and employed advanced prompt engineering methods. We conducted an empirical evaluation of the generated text by administering a survey to a sample of undergraduate students. Utilizing statistical hypothesis testing, we assessed the students ability to distinguish between genuine Lovecraft works and those generated by our model. Our findings demonstrate that the participants were unable to reliably differentiate between the two, indicating the effectiveness of the GPT-4 model and our prompt engineering techniques in emulating Lovecraft's literary style. In addition to presenting the GPT model's capabilities, this paper provides a comprehensive description of its underlying architecture and offers a comparative analysis with related work that simulates other notable authors and philosophers, such as Dennett. By exploring the potential of large language models in the context of literary emulation, our study contributes to the body of research on the applications and limitations of these models in various creative domains.", "title": "simulating hp lovecraft horror literature with the chatgpt large language model", "url": "http://arxiv.org/pdf/2305.03429", "tokenized_text": "paper present novel_approach novel approach simulating literature chatgpt large_language large language specifically gpt-4 architecture study aims generate text emulates unique writing style themes examining effectiveness prompt_engineering engineering techniques guiding output achieve curated containing specialized literature references employed advanced prompt_engineering engineering methods conducted empirical evaluation generated text administering survey sample students utilizing statistical hypothesis testing assessed students ability distinguish genuine works generated findings demonstrate participants unable reliably differentiate indicating effectiveness gpt-4 prompt_engineering engineering techniques emulating style addition presenting gpt capabilities paper provides comprehensive description underlying architecture offers comparative analysis related work simulates notable authors exploring potential large_language large language context study contributes body research applications limitations creative domains"}
{"id": "08fd45ac85916b95f734cc75af8660cff73c33ca", "abstract": "With the rapid adoption of AI in the form of large language models (LLMs), the potential value of carefully engineered prompts has become significant. However, to realize this potential, prompts should be tradable on an open market. Since prompts are, at present, generally economically non-excludable, by virtue of their nature as text, no general competitive market has yet been established. This note discusses two protocols intended to provide protection of prompts, elevating their status as intellectual property, thus confirming the intellectual property rights of prompt engineers, and potentially supporting the flourishing of an open market for LLM prompts.", "title": "protect your prompts protocols for ip protection in llm applications", "url": "http://arxiv.org/pdf/2306.06297", "tokenized_text": "rapid adoption ai form large_language large language llms potential value carefully engineered significant realize potential open market present generally non nature text general competitive market established note discusses protocols intended provide protection status intellectual property intellectual property rights engineers potentially supporting open market llm"}
{"id": "0f71c1e2acf286951544d3bd9eb5d85acfba5af1", "abstract": "Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art large language models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new versions without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly produce multiple variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text segments for rapid in-place comparisons using mouse-over interactions on a context toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p<0.001), enhances user perceptions of the revision process (d = 2.41, p<0.001) compared to a popular baseline workflow, and provides insights into how writers explore variations using LLMs.", "title": "abscribe rapid exploration of multiple writing variations in humanai cowriting tasks using large language models", "url": "https://arxiv.org/pdf/2310.00117", "tokenized_text": "exploring alternative ideas rewriting text integral writing process state art large_language large language llms simplify writing variation generation current interfaces pose challenges consideration multiple variations creating new versions text difficult sequentially documents increasing workload writers flow tackle present interface supports rapid visually structured exploration writing variations human ai co writing tasks users produce multiple variations llm auto converted reusable variations stored text segments rapid place comparisons interactions context user study 12 writers shows significantly reduces task workload enhances user perceptions revision process compared popular baseline workflow provides insights writers explore variations llms"}
{"id": "14d81c84662a1de7b5605a5a68bb0f63d6e293e5", "abstract": "Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.", "title": "udapdr unsupervised domain adaptation via llm prompting and distillation of rerankers", "url": "https://arxiv.org/pdf/2303.00807", "tokenized_text": "information retrieval tasks require large labeled datasets fine tuning datasets unavailable utility real world_applications world applications diminish quickly domain shifts address challenge develop motivate method large_language large language llms generate large numbers synthetic queries cheaply method begins generating small_number small number synthetic queries expensive llm expensive create large numbers synthetic queries fine tune family reranker rerankers distilled single efficient retriever use target domain technique boosts zero shot accuracy long tail domains achieves substantially lower latency standard reranking methods"}
{"id": "19c63eade265d8a47d160098d97194b3b83d3770", "abstract": "In everyday conversations, humans can take on different roles and adapt their vocabulary to their chosen roles. We explore whether LLMs can take on, that is impersonate, different roles when they generate text in-context. We ask LLMs to assume different personas before solving vision and language tasks. We do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise. In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration. In a language-based reasoning task, we find that LLMs impersonating domain experts perform better than LLMs impersonating non-domain experts. Finally, we test whether LLMs' impersonations are complementary to visual information when describing different categories. We find that impersonation can improve performance: an LLM prompted to be a bird expert describes birds better than one prompted to be a car expert. However, impersonation can also uncover LLMs' biases: an LLM prompted to be a man describes cars better than one prompted to be a woman. These findings demonstrate that LLMs are capable of taking on diverse roles and that this in-context impersonation can be used to uncover their hidden strengths and biases.", "title": "incontext impersonation reveals large language models' strengths and biases", "url": "http://arxiv.org/pdf/2305.14930", "tokenized_text": "everyday conversations humans different roles adapt vocabulary chosen roles explore llms different roles generate text context ask llms assume different personas solving vision language tasks persona associated social identity domain expertise multi armed bandit task find llms children different recover human like stages exploration language based reasoning task find llms domain experts perform better llms non domain experts finally test llms complementary visual information describing different categories find improve performance llm prompted bird expert describes birds better prompted expert uncover llms biases llm prompted describes cars better prompted findings demonstrate llms capable taking diverse roles context uncover hidden strengths biases"}
{"id": "1c1b83df13de4334e48a4c2039bc7ddfa374c486", "abstract": "Large language models (LLMs) providing generative AI have become popular to support software engineers in creating, summarizing, optimizing, and documenting source code. It is still unknown how LLMs can support control engineers using typical control programming languages in programming tasks. Researchers have explored GitHub CoPilot or DeepMind AlphaCode for source code generation but did not yet tackle control logic programming. A key contribution of this paper is an exploratory study, for which we created 100 LLM prompts in 10 representative categories to analyze control logic generation for of PLCs and DCS from natural language. We tested the prompts by generating answers with ChatGPT using the GPT-4 LLM. It generated syntactically correct IEC 61131-3 Structured Text code in many cases and demonstrated useful reasoning skills that could boost control engineer productivity. Our prompt collection is the basis for a more formal LLM benchmark to test and compare such models for control logic generation.", "title": "chatgpt for plcdcs control logic generation", "url": "https://arxiv.org/pdf/2305.15809", "tokenized_text": "large_language large language llms providing generative_ai generative ai popular support software engineers creating summarizing optimizing documenting source_code source code unknown llms support control engineers typical control programming languages programming tasks researchers explored github_copilot github copilot source_code source code generation tackle control logic programming key contribution paper exploratory study created 100 llm 10 representative categories analyze control logic generation natural_language natural language tested generating answers chatgpt gpt-4 llm generated syntactically correct structured text code cases demonstrated useful reasoning skills boost control engineer productivity collection basis formal llm benchmark test compare control logic generation"}
{"id": "1fc21645ccc8e99eb8162e5f91407148b7f77e3d", "abstract": "Large language models (LLMs) have demonstrated the potential to perform high-level planning. Yet, it remains a challenge for LLMs to comprehend low-level commands, such as joint angle targets or motor torques. This paper proposes an approach to use foot contact patterns as an interface that bridges human commands in natural language and a locomotion controller that outputs these low-level commands. This results in an interactive system for quadrupedal robots that allows the users to craft diverse locomotion behaviors flexibly. We contribute an LLM prompt design, a reward function, and a method to expose the controller to the feasible distribution of contact patterns. The results are a controller capable of achieving diverse locomotion patterns that can be transferred to real robot hardware. Compared with other design choices, the proposed approach enjoys more than 50% success rate in predicting the correct contact patterns and can solve 10 more tasks out of a total of 30 tasks. Our project site is: https://saytap.github.io.", "title": "saytap language to quadrupedal locomotion", "url": "https://arxiv.org/pdf/2306.07580", "tokenized_text": "large_language large language llms demonstrated potential perform high level planning remains challenge llms comprehend low level commands joint targets motor paper_proposes paper proposes approach use contact patterns interface bridges human commands natural_language natural language controller outputs low level commands results interactive system robots allows users craft diverse behaviors flexibly contribute llm design reward function method expose controller feasible distribution contact patterns results controller capable achieving diverse patterns transferred real robot hardware compared design choices proposed approach enjoys 50 success_rate success rate predicting correct contact patterns solve 10 tasks total 30 tasks project site"}
{"id": "27d6d02e24de259e3aa38e556a81f89ec505816e", "abstract": "In the real world, knowledge often exists in a multimodal and heterogeneous form. Addressing the task of question answering with hybrid data types, including text, tables, and images, is a challenging task (MMHQA). Recently, with the rise of large language models (LLM), in-context learning (ICL) has become the most popular way to solve QA problems. We propose MMHQA-ICL framework for addressing this problems, which includes stronger heterogeneous data retriever and an image caption module. Most importantly, we propose a Type-specific In-context Learning Strategy for MMHQA, enabling LLMs to leverage their powerful performance in this task. We are the first to use end-to-end LLM prompting method for this task. Experimental results demonstrate that our framework outperforms all baselines and methods trained on the full dataset, achieving state-of-the-art results under the few-shot setting on the MultimodalQA dataset.", "title": "mmhqaicl multimodal incontext learning for hybrid question answering over text, tables and images", "url": "https://arxiv.org/pdf/2309.04790", "tokenized_text": "real_world real world knowledge exists multimodal heterogeneous form addressing task question_answering question answering hybrid data types including text tables images challenging task recently rise large_language large language llm context_learning context learning icl popular way solve qa problems propose icl framework addressing problems includes stronger heterogeneous data retriever image caption module importantly propose type specific context_learning context learning strategy enabling llms leverage powerful performance task use end end llm method task experimental_results experimental results demonstrate framework outperforms baselines methods trained dataset achieving state art results shot_setting shot setting dataset"}
{"id": "2cdff023cd4b185bb452f3c7399580db2d0fdfcd", "abstract": "Large language models (LLMs) can enhance writing by automating or supporting specific tasks in writers' workflows (e.g., paraphrasing, creating analogies). Leveraging this capability, a collection of interfaces have been developed that provide LLM-powered tools for specific writing tasks. However, these interfaces provide limited support for writers to create personal tools for their own unique tasks, and may not comprehensively fulfill a writer's needs -- requiring them to continuously switch between interfaces during writing. In this work, we envision LMCanvas, an interface that enables writers to create their own LLM-powered writing tools and arrange their personal writing environment by interacting with\"blocks\"in a canvas. In this interface, users can create text blocks to encapsulate writing and LLM prompts, model blocks for model parameter configurations, and connect these to create pipeline blocks that output generations. In this workshop paper, we discuss the design for LMCanvas and our plans to develop this concept.", "title": "lmcanvas objectoriented interaction to personalize large language modelpowered writing environments", "url": "http://arxiv.org/pdf/2303.15125", "tokenized_text": "large_language large language llms enhance writing automating supporting specific tasks writers workflows e.g. paraphrasing creating analogies leveraging capability collection interfaces developed provide llm powered tools specific writing tasks interfaces provide limited support writers create personal tools unique tasks comprehensively fulfill writer needs requiring continuously switch interfaces writing work envision interface enables writers create llm powered writing tools personal writing environment interacting interface users create text blocks encapsulate writing llm blocks parameter configurations connect create pipeline blocks output generations workshop paper discuss design plans develop concept"}
{"id": "2f2a430ba6c93bcfaf4818316ff8a27b1e034b1a", "abstract": "Large language models (LLMs) are excellent in-context learners. However, the sensitivity of data contained in prompts raises privacy concerns. Our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs. To address this vulnerability, one could forego prompting and resort to fine-tuning LLMs with known algorithms for private gradient descent. However, this comes at the expense of the practicality and efficiency offered by prompting. Therefore, we propose to privately learn to prompt. We first show that soft prompts can be obtained privately through gradient descent on downstream data. However, this is not the case for discrete prompts. Thus, we orchestrate a noisy vote among an ensemble of LLMs presented with different prompts, i.e., a flock of stochastic parrots. The vote privately transfers the flock's knowledge into a single public prompt. We show that LLMs prompted with our private algorithms closely match the non-private baselines. For example, using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on the sst2 dataset with ($\\epsilon=0.147, \\delta=10^{-6}$)-differential privacy vs. 95.2% for the non-private baseline. Through our experiments, we also show that our prompt-based approach is easily deployed with existing commercial APIs.", "title": "flocks of stochastic parrots differentially private prompt learning for large language models", "url": "http://arxiv.org/pdf/2305.15594", "tokenized_text": "large_language large language llms excellent context learners sensitivity data contained raises privacy concerns work shows concerns valid instantiate simple highly effective inference attack data llms address vulnerability resort fine tuning llms known algorithms private gradient descent comes expense practicality efficiency offered propose learn soft obtained gradient descent downstream data case discrete orchestrate noisy vote ensemble llms presented different i.e. stochastic parrots vote transfers knowledge single public llms prompted private algorithms closely match non private baselines example gpt3 base achieve downstream accuracy dataset privacy vs. non private baseline experiments based approach easily deployed existing commercial apis"}
{"id": "30f0abb793772c15f2cdfec97c994685348177c1", "abstract": "Despite their competitive performance on knowledge-intensive tasks, large language models (LLMs) still have limitations in memorizing all world knowledge especially long tail knowledge. In this paper, we study the KG-augmented language model approach for solving the knowledge graph question answering (KGQA) task that requires rich world knowledge. Existing work has shown that retrieving KG knowledge to enhance LLMs prompting can significantly improve LLMs performance in KGQA. However, their approaches lack a well-formed verbalization of KG knowledge, i.e., they ignore the gap between KG representations and textual representations. To this end, we propose an answer-sensitive KG-to-Text approach that can transform KG knowledge into well-textualized statements most informative for KGQA. Based on this approach, we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task. Experiments on several KGQA benchmarks show that the proposed KG-to-Text augmented LLMs approach outperforms previous KG-augmented LLMs approaches regarding answer accuracy and usefulness of knowledge statements.", "title": "retrieverewriteanswer a kgtotext enhanced llms framework for knowledge graph question answering", "url": "https://arxiv.org/pdf/2309.11206", "tokenized_text": "despite competitive_performance competitive performance knowledge intensive tasks large_language large language llms limitations memorizing world knowledge especially long tail knowledge paper study kg augmented language_model language approach solving knowledge_graph knowledge graph question_answering question answering task requires rich world knowledge existing work shown retrieving kg knowledge enhance llms significantly improve llms performance approaches lack formed kg knowledge i.e. ignore gap kg representations textual representations end propose answer sensitive kg text approach transform kg knowledge statements informative based approach propose kg text enhanced llms framework solving task experiments benchmarks proposed kg text augmented llms approach outperforms previous kg augmented llms approaches answer accuracy usefulness knowledge statements"}
{"id": "33d944de189d6edf3a510ea195803a381c5a3bab", "abstract": "Large language models (LLMs) are widely adopted in knowledge-intensive tasks and have achieved impressive performance thanks to their knowledge abilities. While LLMs have demonstrated outstanding performance on atomic or linear (multi-hop) QA tasks, whether they can reason in knowledge-rich scenarios with interweaving constraints remains an underexplored problem. In this work, we propose geometric reasoning over structured knowledge, where pieces of knowledge are connected in a graph structure and models need to fill in the missing information. Such geometric knowledge reasoning would require the ability to handle structured knowledge, reason with uncertainty, verify facts, and backtrack when an error occurs. We propose Knowledge Crosswords, a multi-blank QA dataset where each problem consists of a natural language question representing the geometric constraints of an incomplete entity network, where LLMs are tasked with working out the missing entities while meeting all factual constraints. Knowledge Crosswords contains 2,101 individual problems, covering various knowledge domains and further divided into three difficulty levels. We conduct extensive experiments to evaluate existing LLM prompting approaches on the Knowledge Crosswords benchmark. We additionally propose two new approaches, Staged Prompting and Verify-All, to augment LLMs' ability to backtrack and verify structured constraints. Our results demonstrate that while baseline approaches perform well on easier problems but struggle with hard ones, our proposed Verify-All outperforms other methods by a large margin and is more robust with hard problems. Further analysis reveals that LLMs' ability of geometric reasoning over structured knowledge is still far from robust or perfect, susceptible to confounders such as the order of options, certain structural patterns, assumption of existence of correct answer, and more.", "title": "knowledge crosswords geometric reasoning over structured knowledge with large language models", "url": "https://arxiv.org/pdf/2310.01290", "tokenized_text": "large_language large language llms widely adopted knowledge intensive tasks achieved impressive performance thanks knowledge abilities llms demonstrated outstanding performance atomic linear multi hop qa tasks reason knowledge rich scenarios constraints remains underexplored problem work propose geometric reasoning structured knowledge pieces knowledge connected graph structure need fill missing information geometric knowledge reasoning require ability handle structured knowledge reason uncertainty verify facts error occurs propose knowledge multi qa dataset problem consists natural_language natural language question representing geometric constraints incomplete entity network llms tasked working missing entities meeting factual constraints knowledge contains individual problems covering knowledge domains divided difficulty levels conduct_extensive conduct extensive experiments evaluate existing llm approaches knowledge benchmark additionally propose new approaches augment llms ability verify structured constraints results_demonstrate results demonstrate baseline approaches perform easier problems struggle hard ones proposed verify outperforms methods large margin robust hard problems analysis reveals llms ability geometric reasoning structured knowledge far robust perfect susceptible confounders order options certain structural patterns assumption existence correct answer"}
{"id": "3bd83ff979f3c0e9470f23c360a18333593dc5a1", "abstract": "Augmenting large language models (LLM) to use external tools enhances their performance across a variety of tasks. However, prior works over-rely on task-specific demonstration of tool use that limits their generalizability and computational cost due to making many calls to large-scale LLMs. We introduce GEAR, a computationally efficient query-tool grounding algorithm that is generalizable to various tasks that require tool use while not relying on task-specific demonstrations. GEAR achieves better efficiency by delegating tool grounding and execution to small language models (SLM) and LLM, respectively; while leveraging semantic and pattern-based evaluation at both question and answer levels for generalizable tool grounding. We evaluate GEAR on 14 datasets across 6 downstream tasks, demonstrating its strong generalizability to novel tasks, tools and different SLMs. Despite offering more efficiency, GEAR achieves higher precision in tool grounding compared to prior strategies using LLM prompting, thus improving downstream accuracy at a reduced computational cost. For example, we demonstrate that GEAR-augmented GPT-J and GPT-3 outperform counterpart tool-augmented baselines because of better tool use.", "title": "gear augmenting language models with generalizable and efficient tool resolution", "url": "https://arxiv.org/pdf/2307.08775", "tokenized_text": "augmenting large_language large language llm use external tools enhances performance variety tasks prior works rely task specific demonstration tool use limits generalizability computational cost making calls large scale llms introduce computationally efficient query tool grounding algorithm generalizable tasks require tool use relying task specific demonstrations achieves better efficiency tool grounding execution small language_models language slm llm respectively leveraging semantic pattern based evaluation question answer levels generalizable tool grounding evaluate 14 datasets downstream_tasks downstream tasks demonstrating strong generalizability novel tasks tools different slms despite offering efficiency achieves higher precision tool grounding compared prior strategies llm improving downstream accuracy reduced computational cost example demonstrate augmented gpt gpt-3 outperform counterpart tool augmented baselines better tool use"}
{"id": "3dc1b657bf821b731c5ed0396823b67c10d54ba1", "abstract": "For middle-school math students, interactive question-answering (QA) with tutors is an effective way to learn. The flexibility and emergent capabilities of generative large language models (LLMs) has led to a surge of interest in automating portions of the tutoring process - including interactive QA to support conceptual discussion of mathematical concepts. However, LLM responses to math questions can be incorrect or mismatched to the educational context - such as being misaligned with a school's curriculum. One potential solution is retrieval-augmented generation (RAG), which involves incorporating a vetted external knowledge source in the LLM prompt to increase response quality. In this paper, we designed prompts that retrieve and use content from a high-quality open-source math textbook to generate responses to real student questions. We evaluate the efficacy of this RAG system for middle-school algebra and geometry QA by administering a multi-condition survey, finding that humans prefer responses generated using RAG, but not when responses are too grounded in the textbook content. We argue that while RAG is able to improve response quality, designers of math QA systems must consider trade-offs between generating responses preferred by students and responses closely matched to specific educational resources.", "title": "retrievalaugmented generation to improve math questionanswering tradeoffs between groundedness and human preference", "url": "https://arxiv.org/pdf/2310.03184", "tokenized_text": "middle school math students interactive question answering qa tutors effective way learn flexibility emergent capabilities generative large_language large language llms led surge interest automating portions tutoring process including interactive qa support conceptual discussion mathematical concepts llm responses math questions incorrect educational context misaligned school curriculum potential solution retrieval augmented generation rag involves incorporating vetted external_knowledge external knowledge source llm increase response quality paper designed retrieve use content high quality open source math textbook generate responses real student questions evaluate efficacy rag system middle school algebra geometry qa administering multi condition survey finding humans prefer responses generated rag responses grounded textbook content argue rag able improve response quality designers math qa systems consider trade offs generating responses preferred students responses closely matched specific educational resources"}
{"id": "50bdea5132ef4b8cf25b0d9f3ac2ee0d09bf18cb", "abstract": "In the current digitalization era, capturing and effectively representing knowledge is crucial in most real-world scenarios. In this context, knowledge graphs represent a potent tool for retrieving and organizing a vast amount of information in a properly interconnected and interpretable structure. However, their generation is still challenging and often requires considerable human effort and domain expertise, hampering the scalability and flexibility across different application fields. This paper proposes an innovative knowledge graph generation approach that leverages the potential of the latest generative large language models, such as GPT-3.5, that can address all the main critical issues in knowledge graph building. The approach is conveyed in a pipeline that comprises novel iterative zero-shot and external knowledge-agnostic strategies in the main stages of the generation process. Our unique manifold approach may encompass significant benefits to the scientific community. In particular, the main contribution can be summarized by: (i) an innovative strategy for iteratively prompting large language models to extract relevant components of the final graph; (ii) a zero-shot strategy for each prompt, meaning that there is no need for providing examples for\"guiding\"the prompt result; (iii) a scalable solution, as the adoption of LLMs avoids the need for any external resources or human expertise. To assess the effectiveness of our proposed model, we performed experiments on a dataset that covered a specific domain. We claim that our proposal is a suitable solution for scalable and versatile knowledge graph construction and may be applied to different and novel contexts.", "title": "iterative zeroshot llm prompting for knowledge graph construction", "url": "http://arxiv.org/pdf/2307.01128", "tokenized_text": "current era capturing effectively representing knowledge crucial real world_scenarios world scenarios context knowledge graphs represent tool retrieving vast information properly interpretable structure generation challenging requires considerable human effort domain expertise scalability flexibility different application fields paper_proposes paper proposes innovative knowledge_graph knowledge graph generation approach leverages potential latest generative large_language large language gpt-3.5 address main critical issues knowledge_graph knowledge graph building approach conveyed pipeline comprises novel iterative zero shot external_knowledge external knowledge agnostic strategies main stages generation process unique manifold approach encompass significant benefits scientific community particular main contribution summarized innovative strategy iteratively large_language large language extract relevant components final graph ii zero shot strategy meaning need providing examples result iii scalable solution adoption llms avoids need external resources human expertise assess effectiveness proposed performed experiments dataset covered specific domain claim proposal suitable solution scalable versatile knowledge_graph knowledge graph construction applied different novel contexts"}
{"id": "53e8d327e7ceda6f4efd321752da57edbaee6257", "abstract": "In this paper, we argue that the next generation of robots can be commanded using only Language Models' prompts. Every prompt interrogates separately a specific Robotic Modality via its Modality Language Model (MLM). A central Task Modality mediates the whole communication to execute the robotic mission via a Large Language Model (LLM). This paper gives this new robotic design pattern the name of: Prompting Robotic Modalities (PRM). Moreover, this paper applies this PRM design pattern in building a new robotic framework named ROSGPT_Vision. ROSGPT_Vision allows the execution of a robotic task using only two prompts: a Visual and an LLM prompt. The Visual Prompt extracts, in natural language, the visual semantic features related to the task under consideration (Visual Robotic Modality). Meanwhile, the LLM Prompt regulates the robotic reaction to the visual description (Task Modality). The framework automates all the mechanisms behind these two prompts. The framework enables the robot to address complex real-world scenarios by processing visual data, making informed decisions, and carrying out actions automatically. The framework comprises one generic vision module and two independent ROS nodes. As a test application, we used ROSGPT_Vision to develop CarMate, which monitors the driver's distraction on the roads and makes real-time vocal notifications to the driver. We showed how ROSGPT_Vision significantly reduced the development cost compared to traditional methods. We demonstrated how to improve the quality of the application by optimizing the prompting strategies, without delving into technical details. ROSGPT_Vision is shared with the community (link: https://github.com/bilel-bj/ROSGPT_Vision) to advance robotic research in this direction and to build more robotic frameworks that implement the PRM design pattern and enables controlling robots using only prompts.", "title": "rosgpt_vision commanding robots using only language models' prompts", "url": "https://arxiv.org/pdf/2308.11236", "tokenized_text": "paper argue generation robots language_models language separately specific robotic modality modality language_model language mlm central task modality mediates communication execute robotic mission large_language large language llm paper gives new robotic design pattern robotic modalities paper applies design pattern building new robotic framework named allows execution robotic task visual llm visual extracts natural_language natural language visual semantic features related task consideration visual robotic modality llm robotic reaction visual description task modality framework automates mechanisms framework enables robot address complex real world_scenarios world scenarios processing visual data making informed decisions carrying actions automatically framework comprises generic vision module independent nodes test application develop roads makes real time showed significantly reduced development cost compared traditional methods demonstrated improve quality application optimizing strategies delving technical details shared community link advance robotic research direction build robotic frameworks implement design pattern enables controlling robots"}
{"id": "5645502d73c6907f1671923638773152e55bfb00", "abstract": "While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied. Indeed, we are yet to conduct comprehensive benchmarking studies with multiple LLMs that are exclusively focused on a complex task. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, the paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw more accurate conclusions about LLMs' performance on a specific complex task.", "title": "teler a general taxonomy of llm prompts for benchmarking complex tasks", "url": "http://arxiv.org/pdf/2305.11430", "tokenized_text": "llms shown great success understanding generating text traditional conversational settings potential performing ill defined complex tasks largely studied conduct comprehensive benchmarking studies multiple llms exclusively focused complex task conducting benchmarking studies challenging large variations llms performance different types styles different degrees detail provided address issue paper_proposes paper proposes general taxonomy design specific properties order perform wide_range wide range complex tasks taxonomy allow future benchmarking studies report specific categories study enabling meaningful comparisons different studies establishing common standard taxonomy researchers able draw accurate conclusions llms performance specific complex task"}
{"id": "864cb3a725ae829cbfb675761cd2313897b1b7a8", "abstract": "Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality of in-context\"reasoning\"induced by different forms of prompts under the setting of the Crafter open-world environment. Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training. Finally, we show the potential of games as a test bed for LLMs.", "title": "spring gpt4 outperforms rl algorithms by studying papers and reasoning", "url": "http://arxiv.org/pdf/2305.15486", "tokenized_text": "open world games pose significant challenges ai algorithms multi tasking deep exploration goal requirements despite reinforcement_learning reinforcement learning rl popular solving games high sample complexity limits effectiveness complex open world games like minecraft propose_a_novel propose novel approach read game original academic paper use knowledge learned reason play game large_language large language llm prompted source game context description agent current observation framework employs directed graph game related questions nodes dependencies edges identify optimal action environment calculating llm responses node topological order llm answer final node directly translating environment actions experiments study quality different forms setting open world environment experiments suggest llms prompted consistent chain thought great_potential great potential completing sophisticated high level trajectories quantitatively gpt-4 outperforms state art rl baselines trained steps training finally potential games test bed llms"}
{"id": "8e37dc1215681aa153a51c07078ba8befd6a6e01", "abstract": "Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.", "title": "adaplanner adaptive planning from feedback with language models", "url": "http://arxiv.org/pdf/2305.16653", "tokenized_text": "large_language large language llms recently demonstrated potential acting autonomous agents sequential decision making tasks existing_methods existing methods actions planning rely static plans adaptable environmental feedback consequently sequential decision making performance llm agents degenerates problem complexity plan horizons increase propose closed loop approach allows llm agent refine self generated plan adaptively response environmental feedback llm agent adaptively refines plan feedback plan plan refinement strategies mitigate hallucination develop code style llm structure facilitates plan generation variety tasks environments agent capabilities furthermore propose skill discovery mechanism leverages successful plans shot exemplars enabling agent plan refine fewer task demonstrations experiments alfworld miniwob++ environments demonstrate outperforms state art baselines utilizing 2x fewer samples respectively"}
{"id": "b099104d1a065cbc1432af22e6085b1a44dbc839", "abstract": "In utilizing large language models (LLMs) for mathematical reasoning, addressing the errors in the reasoning and calculation present in the generated text by LLMs is a crucial challenge. In this paper, we propose a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL). We discovered that by prompting LLMs to generate structured text in XML-like markup language, we could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs. With our approach, LLMs can utilize Python computation to rectify errors within CoT. We applied our method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and demonstrated that combining CoT and Python REPL through the markup language enhances the reasoning capability of LLMs. Our approach enables LLMs to write the markup language and perform advanced mathematical reasoning using only zero-shot prompting.", "title": "lpml llmprompting markup language for mathematical reasoning", "url": "https://arxiv.org/pdf/2309.13078", "tokenized_text": "utilizing large_language large language llms mathematical reasoning addressing errors reasoning calculation present generated text llms crucial challenge paper propose_a_novel propose novel framework integrates chain thought cot method external tool python discovered llms generate structured text xml like markup language seamlessly integrate cot external tool control undesired behaviors llms approach llms utilize python computation rectify errors cot. applied method chatgpt gpt-3.5 solve challenging mathematical problems demonstrated combining cot python markup language enhances reasoning capability llms approach enables llms write markup language perform advanced mathematical reasoning zero shot_prompting shot"}
{"id": "bcefc74b20649fd41ea05d87a3fa512d2559fc8d", "abstract": "Despite significant research effort in the development of automatic dialogue evaluation metrics, little thought is given to evaluating dialogues other than in English. At the same time, ensuring metrics are invariant to semantically similar responses is also an overlooked topic. In order to achieve the desired properties of robustness and multilinguality for dialogue evaluation metrics, we propose a novel framework that takes advantage of the strengths of current evaluation models with the newly-established paradigm of prompting Large Language Models (LLMs). Empirical results show our framework achieves state of the art results in terms of mean Spearman correlation scores across several benchmarks and ranks first place on both the Robust and Multilingual tasks of the DSTC11 Track 4 \u201cAutomatic Evaluation Metrics for Open-Domain Dialogue Systems\u201d, proving the evaluation capabilities of prompted LLMs.", "title": "simple llm prompting is stateoftheart for robust and multilingual dialogue evaluation", "url": "https://arxiv.org/pdf/2308.16797", "tokenized_text": "despite significant research effort development automatic dialogue evaluation metrics little thought given evaluating dialogues english time ensuring metrics invariant semantically similar responses overlooked topic order achieve desired properties robustness multilinguality dialogue evaluation metrics propose_a_novel propose novel framework takes advantage strengths current evaluation newly established paradigm large_language large language llms empirical results framework achieves state_of_the_art state art results terms mean spearman correlation scores benchmarks ranks place robust multilingual tasks dstc11 track automatic evaluation metrics open domain dialogue systems proving evaluation capabilities prompted llms"}
{"id": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa", "abstract": "Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback. Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate eleven models on 10k pairs of real human feedback and show that rankings of models trained in AlpacaFarm match rankings of models trained on human data. As a demonstration of the research possible in AlpacaFarm, we find that methods that use a reward model can substantially improve over supervised fine-tuning and that our reference PPO implementation leads to a +10% improvement in win-rate against Davinci003. We release all components of AlpacaFarm at https://github.com/tatsu-lab/alpaca_farm.", "title": "alpacafarm a simulation framework for methods that learn from human feedback", "url": "https://arxiv.org/pdf/2305.14387", "tokenized_text": "large_language large language llms chatgpt seen widespread adoption ability follow user instructions developing llms involves complex poorly understood workflow requiring training human feedback replicating understanding instruction following process faces major challenges high cost data collection lack trustworthy evaluation absence reference method implementations address challenges simulator enables research development learning feedback low cost design llm simulate human feedback display high agreement humans second propose automatic evaluation validate human instructions obtained real world interactions contribute reference implementations methods ppo best expert iteration learn pairwise feedback finally end end validation train evaluate 10k pairs real human feedback rankings trained match rankings trained human data demonstration research possible find methods use reward substantially improve supervised fine tuning reference ppo implementation leads improvement win rate release components"}
{"id": "da0a170656a336f82fa8cf00289d1cc944d9b630", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in performing a range of instruction following tasks in few and zero-shot settings. However, teaching LLMs to perform tasks on the web presents fundamental challenges -- combinatorially large open-world tasks and variations across web interfaces. We tackle these challenges by leveraging LLMs to decompose web tasks into a collection of sub-tasks, each of which can be solved by a low-level, closed-loop policy. These policies constitute a shared grammar across tasks, i.e., new web tasks can be expressed as a composition of these policies. We propose a novel framework, Hierarchical Policies for Web Actions using LLMs (HeaP), that learns a set of hierarchical LLM prompts from demonstrations for planning high-level tasks and executing them via a sequence of low-level policies. We evaluate HeaP against a range of baselines on a suite of web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as live website interactions, and show that it is able to outperform prior works using orders of magnitude less data.", "title": "heap hierarchical policies for web actions using llms", "url": "https://arxiv.org/pdf/2310.03720", "tokenized_text": "large_language large language llms demonstrated_remarkable demonstrated remarkable capabilities performing range instruction_following instruction following tasks zero shot_settings shot settings teaching llms perform tasks web presents fundamental challenges combinatorially large open world tasks variations web interfaces tackle challenges leveraging llms decompose web tasks collection sub tasks solved low level closed loop policy policies constitute shared grammar tasks i.e. new web tasks expressed composition policies propose_a_novel propose novel framework hierarchical policies web actions llms learns set hierarchical llm demonstrations planning high level tasks executing sequence low level policies evaluate range baselines suite web tasks including miniwob++ mock live website interactions able outperform prior works orders magnitude data"}
{"id": "e5c72b92c48d68594b290c84a8904da7c8335554", "abstract": "Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.", "title": "check your facts and try again improving large language models with external knowledge and automated feedback", "url": "http://arxiv.org/pdf/2302.12813", "tokenized_text": "large_language large language llms chatgpt able generate human like fluent responses downstream_tasks downstream tasks e.g. task oriented dialog question_answering question answering applying llms real world mission critical applications remains challenging mainly tendency generate hallucinations inability use external_knowledge external knowledge paper_proposes paper proposes llm system augments black box llm set plug play modules system makes llm generate responses grounded external_knowledge external knowledge e.g. stored task specific databases iteratively llm improve responses feedback generated utility functions e.g. factuality score llm generated response effectiveness llm empirically validated types scenarios task oriented dialog open domain question_answering question answering llm significantly reduces chatgpt hallucinations sacrificing fluency informativeness responses source_code source code publicly_available publicly available"}
{"id": "e86009d9f9b1cdf083a48d087552bc4153784451", "abstract": "Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval tasks, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To amplify the power of a few examples, we propose Prompt-base Query Generation for Retriever (Promptagator), which leverages large language models (LLM) as a few-shot query generator, and creates task-specific retrievers based on the generated data. Powered by LLM's generalization ability, Promptagator makes it possible to create task-specific end-to-end retrievers solely based on a few examples {without} using Natural Questions or MS MARCO to train %question generators or dual encoders. Surprisingly, LLM prompting with no more than 8 examples allows dual encoders to outperform heavily engineered models trained on MS MARCO like ColBERT v2 by more than 1.2 nDCG on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 point nDCG improvement. Our studies determine that query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.", "title": "promptagator fewshot dense retrieval from 8 examples", "url": "http://arxiv.org/pdf/2209.11755", "tokenized_text": "recent research information retrieval focused transfer task typically abundant supervised data tasks supervision limited implicit assumption possible generalize task rest fact diverse unique retrieval tasks targeting different search intents queries search domains paper suggest work shot dense retrieval setting task comes short description examples amplify power examples propose base query generation retriever leverages large_language large language llm shot query generator creates task specific retrievers based generated data powered llm generalization_ability generalization ability makes possible create task specific end end retrievers solely based examples natural questions ms marco train question generators dual encoders surprisingly llm examples allows dual encoders outperform heavily engineered trained ms marco like v2 1.2 average 11 retrieval sets training standard size rankers generated data yields point improvement studies determine query generation far effective previously observed especially small task specific knowledge given"}
{"id": "ec56f49bef8925dc8931cc261ab3aca4dd36ad2d", "abstract": "Building end-to-end task bots and maintaining their integration with new functionalities using minimal human efforts is a long-standing challenge in dialog research. Recently large language models (LLMs) have demonstrated exceptional proficiency in conversational engagement and adherence to instructions across various downstream tasks. In this work, we introduce SGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems effortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, we instruct fixed LLMs to generate appropriate responses on novel tasks, circumventing the need for training data. Specifically, SGP-TOD comprises three components: a LLM for engaging with users, a DST Prompter to aid the LLM with dialog state tracking, which is then used to retrieve database items, and a Policy Prompter to elicit proper responses adhering to the provided dialog policy. Experimental results on Multiwoz, RADDLE and STAR datasets show that our training-free strategy SGP-TOD, without any task-specific data, yields state-of-the-art (SOTA) zero-shot performance, greatly surpasses the few-shot approaches. In a domain-extension setting, SGP-TOD aptly adapts to new functionalities by merely adding supplementary schema rules. We make our code and data publicly available.", "title": "sgptod building task bots effortlessly via schemaguided llm prompting", "url": "http://arxiv.org/pdf/2305.09067", "tokenized_text": "building end end task bots maintaining integration new functionalities minimal human efforts long standing challenge dialog research recently large_language large language llms demonstrated exceptional proficiency conversational engagement adherence instructions downstream_tasks downstream tasks work introduce tod building task oriented dialog systems effortlessly based llms utilizing symbolic knowledge task schema instruct fixed llms generate appropriate responses novel tasks circumventing need training_data training data specifically tod comprises components llm engaging users dst prompter aid llm dialog state tracking retrieve database items policy prompter elicit proper responses adhering provided dialog policy experimental_results experimental results multiwoz star datasets training free strategy tod task specific data yields state art sota zero shot performance greatly surpasses shot approaches domain extension setting tod adapts new functionalities merely adding supplementary schema rules code data publicly_available publicly available"}
{"id": "f53a4f34757d1f237446b4d887d5323f2a17ed02", "abstract": "As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Pompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinement. Moreover, to enhance stability of the prompt effect evaluation, we propose a novel prompt bagging method involving forward and backward thinking, which is superior to majority voting and is beneficial for both feedback and weight calculation in boosting. Extensive experiments demonstrate that our PREFER achieves state-of-the-art performance in multiple types of tasks by a significant margin. We have made our code publicly available.", "title": "prefer prompt ensemble learning via feedbackreflectrefine", "url": "https://arxiv.org/pdf/2308.12033", "tokenized_text": "effective tool eliciting power large_language large language llms recently demonstrated unprecedented abilities variety complex tasks improve performance ensemble attracted substantial interest tackling hallucination instability llms existing_methods existing methods usually adopt stage paradigm requires pre prepared set substantial manual effort unable perform directed optimization different weak learners paper propose simple universal automatic method named prefer ensemble learning feedback reflect refine address stated limitations specifically given fact weak learners supposed focus hard examples boosting prefer builds feedback mechanism reflecting existing weak learners based llm required automatically synthesize new iterative refinement enhance stability effect evaluation propose_a_novel propose novel method involving forward backward thinking superior majority voting beneficial feedback weight calculation boosting extensive_experiments extensive experiments demonstrate prefer achieves_state achieves state art performance multiple types tasks significant margin code publicly_available publicly available"}
{"id": "f7842099bbde74dc5aec70bb6af85b88de08ed13", "abstract": "Artificial intelligence has been applied in various aspects of online education to facilitate teaching and learning. However, few approaches has been made toward a complete AI-powered tutoring system. In this work, we explore the development of a full-fledged intelligent tutoring system powered by state-of-the-art large language models (LLMs), covering automatic course planning and adjusting, tailored instruction, and flexible quiz evaluation. To make the system robust to prolonged interaction and cater to individualized education, the system is decomposed into three inter-connected core processes-interaction, reflection, and reaction. Each process is implemented by chaining LLM-powered tools along with dynamically updated memory modules. Tools are LLMs prompted to execute one specific task at a time, while memories are data storage that gets updated during education process. Statistical results from learning logs demonstrate the effectiveness and mechanism of each tool usage. Subjective feedback from human users reveal the usability of each function, and comparison with ablation systems further testify the benefits of the designed processes in long-term interaction.", "title": "empowering private tutoring by chaining large language models", "url": "https://arxiv.org/pdf/2309.08112", "tokenized_text": "artificial_intelligence artificial intelligence applied aspects online education facilitate teaching learning approaches complete ai powered tutoring system work explore development fledged intelligent tutoring system powered state art large_language large language llms covering automatic course planning adjusting tailored instruction flexible evaluation system robust prolonged interaction cater individualized education system decomposed inter connected core processes interaction reflection reaction process implemented chaining llm powered tools dynamically updated memory modules tools llms prompted execute specific task time memories data storage gets updated education process statistical results learning logs demonstrate_the_effectiveness demonstrate effectiveness mechanism tool usage subjective feedback human users reveal usability function comparison ablation systems benefits designed processes long term interaction"}
{"id": "0f733817e82026f7c29909a51cb4df7d2685f0e7", "abstract": "While LLMs have made it possible to rapidly prototype new ML functionalities, many real-world applications involve complex tasks that cannot be easily handled via a single run of an LLM. Recent work has found that chaining multiple LLM runs together (with the output of one step being the input to the next) can help users accomplish these more complex tasks, and in a way that is perceived to be more transparent and controllable. However, it remains unknown what users need when authoring their own LLM chains \u2013 a key step to lowering the barriers for non-AI-experts to prototype AI-infused applications. In this work, we explore the LLM chain authoring process. We find from pilot studies that users need support transforming data between steps of a chain, as well as debugging the chain at multiple granularities. To address these needs, we designed PromptChainer, an interactive interface for visually programming chains. Through case studies with four designers and developers, we show that PromptChainer supports building prototypes for a range of applications, and conclude with open questions on scaling chains to even more complex tasks, as well as supporting low-fi chain prototyping.", "title": "promptchainer chaining large language model prompts through visual programming", "url": "https://arxiv.org/pdf/2203.06566", "tokenized_text": "llms possible rapidly prototype new ml functionalities real world_applications world applications involve complex tasks easily handled single run llm recent_work recent work found chaining multiple llm runs output step input help users accomplish complex tasks way perceived transparent controllable remains unknown users need authoring llm chains key step barriers non ai experts prototype ai applications work explore llm chain authoring process find pilot studies users need support transforming data steps chain debugging chain multiple granularities address needs designed interactive interface visually programming chains case studies designers developers supports building prototypes range applications conclude open questions scaling chains complex tasks supporting low chain prototyping"}
{"id": "2d30d800e946d3699d9c41bb95c36a6db63676e7", "abstract": "Embodied Instruction Following (EIF) studies how mobile manipulator robots should be controlled to accomplish long-horizon tasks specified by natural language instructions. While most research on EIF are conducted in simulators, the ultimate goal of the field is to deploy the agents in real life. As such, it is important to minimize the data cost required for training an agent, to help the transition from sim to real. However, many studies only focus on the performance and overlook the data cost -- modules that require separate training on extra data are often introduced without a consideration on deployability. In this work, we propose FILM++ which extends the existing work FILM with modifications that do not require extra data. While all data-driven modules are kept constant, FILM++ more than doubles FILM's performance. Furthermore, we propose Prompter, which replaces FILM++'s semantic search module with language model prompting. Unlike FILM++'s implementation that requires training on extra sets of data, no training is needed for our prompting based implementation while achieving better or at least comparable performance. Prompter achieves 42.64% and 45.72% on the ALFRED benchmark with high-level instructions only and with step-by-step instructions, respectively, outperforming the previous state of the art by 6.57% and 10.31%.", "title": "prompter utilizing large language model prompting for a data efficient embodied instruction following", "url": "https://arxiv.org/pdf/2211.03267", "tokenized_text": "embodied instruction_following instruction following studies mobile manipulator robots controlled accomplish long horizon tasks specified natural_language natural language instructions research conducted simulators ultimate goal field deploy agents real life important minimize data cost required training agent help transition sim real studies focus performance overlook data cost modules require separate training extra data introduced consideration work propose extends existing work film modifications require extra data data driven modules kept constant doubles film performance furthermore propose prompter replaces semantic search module language_model language unlike implementation requires training extra sets data training needed based implementation achieving better comparable performance prompter achieves benchmark high level instructions step step instructions respectively outperforming previous state_of_the_art state art"}
{"id": "a0d83f9e15e722f23c14eb83cb2f87c1d1ea6400", "abstract": "By simply composing prompts, developers can prototype novel generative applications with Large Language Models (LLMs). To refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. Formative interviews (N=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. We present EvalLM, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. By describing criteria in natural language, users can employ the system's LLM-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator's feedback. A comparative study (N=12) showed that EvalLM, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory prompts with 59% fewer revisions. Beyond prompts, our work can be extended to augment model evaluation and alignment in specific application contexts.", "title": "evallm interactive evaluation of large language model prompts on userdefined criteria", "url": "https://arxiv.org/pdf/2309.13633", "tokenized_text": "simply composing developers prototype novel generative applications large_language large language llms refine prototypes products developers iteratively revise evaluating outputs diagnose weaknesses formative interviews revealed developers significant effort manually evaluating outputs assess context specific subjective criteria present interactive system iteratively refining evaluating multiple outputs user defined criteria describing criteria natural_language natural language users employ system llm based evaluator overview excel fail improve based evaluator feedback comparative study showed compared manual evaluation helped participants compose diverse criteria examine twice outputs reach satisfactory 59 fewer revisions work extended augment evaluation alignment specific application contexts"}
{"id": "b8ba16a107621f760e7830ddaab8c3d5c5ff06b0", "abstract": "With growing capabilities of large language models, prompting them has become the dominant way to access them. This has motivated the development of strategies for automatically selecting effective language prompts. In this paper, we introduce prompt flatness, a new metric to quantify the expected utility of a language prompt. This metric is inspired by flatness regularization in statistical learning that quantifies the robustness of the model towards its parameter perturbations. We provide theoretical foundations for this metric and its relationship with other prompt selection metrics, providing a comprehensive understanding of existing methods. Empirically, we show that combining prompt flatness with existing metrics improves both performance and sample efficiency. Our metric outperforms the previous prompt selection metrics with an average increase of 5% in accuracy and 10% in Pearson correlation across 6 classification benchmarks.", "title": "flatnessaware prompt selection improves accuracy and sample efficiency", "url": "http://arxiv.org/pdf/2305.10713", "tokenized_text": "growing capabilities large_language large language dominant way access motivated development strategies automatically selecting effective language paper introduce new metric quantify expected utility language metric inspired regularization statistical learning quantifies robustness parameter perturbations provide theoretical foundations metric relationship selection metrics providing comprehensive understanding existing_methods existing methods empirically combining existing metrics improves performance sample efficiency metric outperforms previous selection metrics average increase accuracy 10 correlation classification benchmarks"}
{"id": "d3640eb3b542eaf36fee2261f037a6bf0d8eac9c", "abstract": "Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, we introduce the concept of Chaining LLM steps together, where the output of one step becomes the input for the next, thus aggregating the gains per step. We first define a set of LLM primitive operations useful for Chain construction, then present an interactive system where users can modify these Chains, along with their intermediate results, in a modular way. In a 20-person user study, we found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration. Additionally, we saw that users developed new ways of interacting with LLMs through Chains: they leveraged sub-tasks to calibrate model expectations, compared and contrasted alternative strategies by observing parallel downstream effects, and debugged unexpected model outputs by \u201cunit-testing\u201d sub-components of a Chain. In two case studies, we further explore how LLM Chains may be used in future applications.", "title": "ai chains transparent and controllable humanai interaction by chaining large language model prompts", "url": "https://dl.acm.org/doi/pdf/10.1145/3491102.3517582", "tokenized_text": "large_language large language llms demonstrated impressive potential simple tasks breadth scope lack transparency insufficient controllability effective assisting humans complex tasks response introduce concept chaining llm steps output step input aggregating gains step define set llm primitive operations useful chain construction present interactive system users modify chains intermediate results modular way 20 person user study found chaining improved quality task outcomes significantly enhanced system transparency controllability sense collaboration additionally saw users developed new ways interacting llms chains leveraged sub tasks calibrate expectations compared contrasted alternative strategies observing parallel downstream effects unexpected outputs unit testing sub components chain case studies explore llm chains future applications"}
{"id": "e90d30148ecf633db3bbabdcfa3a0ec06236e0d1", "abstract": "Terminology correctness is important in the downstream application of machine translation, and a prevalent way to ensure this is to inject terminology constraints into a translation system. In our submission to the WMT 2023 terminology translation task, we adopt a translate-then-refine approach which can be domain-independent and requires minimal manual efforts. We annotate random source words with pseudo-terminology translations obtained from word alignment to first train a terminology-aware model. Further, we explore two post-processing methods. First, we use an alignment process to discover whether a terminology constraint has been violated, and if so, we re-decode with the violating word negatively constrained. Alternatively, we leverage a large language model to refine a hypothesis by providing it with terminology constraints. Results show that our terminology-aware model learns to incorporate terminologies effectively, and the large language model refinement process can further improve terminology recall.", "title": "terminologyaware translation with constrained decoding and large language model prompting", "url": "https://arxiv.org/pdf/2310.05824", "tokenized_text": "terminology correctness important downstream application machine_translation machine translation prevalent way ensure inject terminology constraints translation system submission wmt 2023 terminology translation task adopt translate refine approach domain independent requires minimal manual efforts annotate random source words pseudo terminology translations obtained word alignment train terminology aware explore post processing methods use alignment process discover terminology constraint violated decode word negatively constrained leverage large_language large language refine hypothesis providing terminology constraints results terminology aware learns incorporate effectively large_language large language refinement process improve terminology recall"}
{"id": "31d8bdef7b81e107bf04f226d877fd5aa2f51d34", "abstract": "Large language models (LLMs) demonstrate impressive performance on a wide variety of tasks, but they often struggle with tasks that require multi-step reasoning or goal-directed planning. To address this, we take inspiration from the human brain, in which planning is accomplished via the recurrent interaction of specialized modules in the prefrontal cortex (PFC). These modules perform functions such as conflict monitoring, state prediction, state evaluation, task decomposition, and task coordination. We find that LLMs are sometimes capable of carrying out these functions in isolation, but struggle to autonomously coordinate them in the service of a goal. Therefore, we propose a black box architecture with multiple LLM-based (GPT-4) modules. The architecture improves planning through the interaction of specialized PFC-inspired modules that break down a larger problem into multiple brief automated calls to the LLM. We evaluate the combined architecture on two challenging planning tasks -- graph traversal and Tower of Hanoi -- finding that it yields significant improvements over standard LLM methods (e.g., zero-shot prompting or in-context learning). These results demonstrate the benefit of utilizing knowledge from cognitive neuroscience to improve planning in LLMs.", "title": "a prefrontal cortexinspired architecture for planning in large language models", "url": "https://arxiv.org/pdf/2310.00194", "tokenized_text": "large_language large language llms demonstrate impressive performance wide variety tasks struggle tasks require multi step reasoning goal directed planning address inspiration human brain planning accomplished recurrent interaction specialized modules modules perform functions conflict monitoring state prediction state evaluation task decomposition task coordination find llms capable carrying functions isolation struggle autonomously coordinate service goal propose black_box black box architecture multiple llm based gpt-4 modules architecture improves planning interaction specialized inspired modules break larger problem multiple brief automated calls llm evaluate combined architecture challenging planning tasks graph traversal tower finding yields significant improvements standard llm methods e.g. zero shot_prompting shot context_learning context learning results_demonstrate results demonstrate benefit utilizing knowledge cognitive neuroscience improve planning llms"}
{"id": "4161ad2d2495d8af1d62dc5e71882bde642cd1c1", "abstract": "We describe GEMBA, a GPT-based metric for assessment of translation quality, which works both with a reference translation and without. In our evaluation, we focus on zero-shot prompting, comparing four prompt variants in two modes, based on the availability of the reference. We investigate seven versions of GPT models, including ChatGPT. We show that our method for translation quality assessment only works with GPT 3.5 and larger models. Comparing to results from WMT22\u2019s Metrics shared task, our method achieves state-of-the-art accuracy in both modes when compared to MQM-based human labels. Our results are valid on the system level for all three WMT22 Metrics shared task language pairs, namely English into German, English into Russian, and Chinese into English. This provides a first glimpse into the usefulness of pre-trained, generative large language models for quality assessment of translations. We publicly release all our code and prompt templates used for the experiments described in this work, as well as all corresponding scoring results, to allow for external validation and reproducibility.", "title": "large language models are stateoftheart evaluators of translation quality", "url": "http://arxiv.org/pdf/2302.14520", "tokenized_text": "describe gemba gpt based metric assessment translation quality works reference translation evaluation focus zero shot_prompting shot comparing variants modes based availability reference investigate seven versions gpt including chatgpt method translation quality assessment works gpt 3.5 larger comparing results metrics shared task method_achieves method achieves state art accuracy modes compared mqm based human labels results valid system level metrics shared task language pairs english german english russian chinese english provides glimpse usefulness pre trained generative large_language large language quality assessment translations publicly release code prompt_templates templates experiments described work corresponding scoring results allow external validation reproducibility"}
{"id": "9c39e942b87cbada41a4a52364f996915c7c2d98", "abstract": "Prompt-based or in-context learning has achieved high zero-shot performance on many natural language generation (NLG) tasks. Here we explore the performance of prompt-based learning for simultaneously controlling the personality and the semantic accuracy of an NLG for task-oriented dialogue. We experiment with prompt-based learning on the PERSONAGE restaurant recommendation corpus to generate semantically and stylistically-controlled text for 5 different Big-5 personality types: agreeable, disagreeable, conscientious, unconscientious, and extravert. We test two different classes of discrete prompts to generate utterances for a particular personality style: (1) prompts that demonstrate generating directly from a meaning representation that includes a personality specification; and (2) prompts that rely on first converting the meaning representation to a textual pseudo-reference, and then using the pseudo-reference in a textual style transfer (TST) prompt. In each case, we show that we can vastly improve performance by over-generating outputs and ranking them, testing several ranking functions based on automatic metrics for semantic accuracy, personality-match, and fluency. We also test whether NLG personality demonstrations from the restaurant domain can be used with meaning representations for the video game domain to generate personality stylized utterances about video games. Our findings show that the TST prompts produces the highest semantic accuracy (78.46% for restaurants and 87.6% for video games) and personality accuracy (100% for restaurants and 97% for video games). Our results on transferring personality style to video game utterances are surprisingly good. To our knowledge, there is no previous work testing the application of prompt-based learning to simultaneously controlling both style and semantic accuracy in NLG.", "title": "controlling personality style in dialogue with zeroshot promptbased learning", "url": "http://arxiv.org/pdf/2302.03848", "tokenized_text": "based context_learning context learning achieved high zero shot performance natural_language natural language generation nlg tasks explore performance based learning simultaneously controlling personality semantic accuracy nlg task oriented dialogue experiment based learning restaurant recommendation corpus generate semantically controlled text different personality types test different classes discrete generate utterances particular personality style demonstrate generating directly meaning representation includes personality specification rely converting meaning representation textual pseudo reference pseudo reference textual style_transfer style transfer tst case vastly improve performance generating outputs ranking testing ranking functions based automatic metrics semantic accuracy personality match fluency test nlg personality demonstrations restaurant domain meaning representations video game domain generate personality stylized utterances video games findings tst produces highest semantic accuracy video games personality accuracy 100 97 video games results transferring personality style video game utterances surprisingly good knowledge previous work testing application based learning simultaneously controlling style semantic accuracy nlg"}
{"id": "a8a71f9b10b281e796fdc2ee7aaec40067739574", "abstract": "Various human activities can be abstracted into a sequence of actions in natural text, i.e. cooking, repairing, manufacturing, etc. Such action sequences heavily depend on the executing order, while disorder in action sequences leads to failure of further task execution by robots or AI agents. Therefore, to verify the order reasoning capability of current neural models in sequential tasks, we propose a challenging benchmark , named STEPS. STEPS involves two subtask settings, focusing on determining the rationality of given next step in recipes and selecting the reasonable step from the multi-choice question, respectively. We describe the data construction and task formulations, and benchmark most of significant Large Language Models (LLMs). The experimental results demonstrate 1) The commonsense reasoning of action orders in sequential tasks are challenging to resolve via zero-shot prompting or few-shot in-context learning for LLMs; 2) Prompting method still significantly lags behind tuning-based method on STEPS.", "title": "steps a benchmark for order reasoning in sequential tasks", "url": "http://arxiv.org/pdf/2306.04441", "tokenized_text": "human activities abstracted sequence actions natural text i.e. repairing manufacturing etc action sequences heavily depend executing order action sequences leads failure task execution robots ai agents verify order reasoning capability current neural sequential tasks propose challenging benchmark named steps steps involves subtask settings focusing determining rationality given step recipes selecting reasonable step multi choice question respectively describe data construction task formulations benchmark significant large_language large language llms experimental_results experimental results demonstrate commonsense reasoning action orders sequential tasks challenging resolve zero shot_prompting shot shot context_learning context learning llms method significantly lags tuning based method steps"}
{"id": "c879413103f8950bdd414c7f60a39bd7748c9be8", "abstract": "Research on prompting has shown excellent performance with little or even no supervised training across many tasks. However, prompting for machine translation is still under-explored in the literature. We fill this gap by offering a systematic study on prompting strategies for translation, examining various factors for prompt template and demonstration example selection. We further explore the use of monolingual data and the feasibility of cross-lingual, cross-domain, and sentence-to-document transfer learning in prompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as the testbed show that 1) the number and the quality of prompt examples matter, where using suboptimal examples degenerates translation; 2) several features of prompt examples, such as semantic similarity, show significant Spearman correlation with their prompting performance; yet, none of the correlations are strong enough; 3) using pseudo parallel prompt examples constructed from monolingual data via zero-shot prompting could improve translation; and 4) improved performance is achievable by transferring knowledge from prompt examples selected in other settings. We finally provide an analysis on the model outputs and discuss several problems that prompting still suffers from.", "title": "prompting large language model for machine translation a case study", "url": "http://arxiv.org/pdf/2301.07069", "tokenized_text": "research shown excellent performance little supervised training tasks machine_translation machine translation explored literature fill gap offering systematic study strategies translation examining factors prompt_template template demonstration example selection explore use monolingual data feasibility cross lingual cross domain sentence document transfer learning extensive_experiments extensive experiments et_al et al 2022 testbed number quality examples matter suboptimal examples degenerates translation features examples semantic similarity significant spearman correlation performance correlations strong pseudo parallel examples constructed monolingual data zero shot_prompting shot improve translation improved performance transferring knowledge examples selected settings finally provide analysis outputs discuss problems suffers"}
{"id": "cd7d770eabb4dab6894d9f91d2c3bc337e94a4e1", "abstract": "The remarkable advancements in large language models (LLMs) have brought about significant improvements in Natural Language Processing(NLP) tasks. This paper presents a comprehensive review of in-context learning techniques, focusing on different types of prompts, including discrete, continuous, few-shot, and zero-shot, and their impact on LLM performance. We explore various approaches to prompt design, such as manual design, optimization algorithms, and evaluation methods, to optimize LLM performance across diverse tasks. Our review covers key research studies in prompt engineering, discussing their methodologies and contributions to the field. We also delve into the challenges faced in evaluating prompt performance, given the absence of a single \u201cbest\u201d prompt and the importance of considering multiple metrics. In conclusion, the paper highlights the critical role of prompt design in harnessing the full potential of LLMs and provides insights into the combination of manual design, optimization techniques, and rigorous evaluation for more effective and efficient use of LLMs in various NLP tasks.", "title": "a practical survey on zeroshot prompt design for incontext learning", "url": "https://arxiv.org/pdf/2309.13205", "tokenized_text": "remarkable advancements large_language large language llms brought significant improvements natural_language natural language processing(nlp tasks paper_presents paper presents comprehensive review context_learning context learning techniques focusing different types including discrete continuous shot zero shot impact llm performance explore approaches design manual design optimization algorithms evaluation methods optimize llm performance diverse tasks review covers key research studies prompt_engineering engineering discussing methodologies contributions field delve challenges faced evaluating performance given absence single best importance considering multiple metrics conclusion paper highlights critical role design harnessing potential llms provides insights combination manual design optimization techniques rigorous evaluation effective efficient use llms nlp_tasks nlp tasks"}
{"id": "d0e3af5f20a451c04770929979d7a8406a1a2466", "abstract": "As the field of Large Language Models (LLMs) evolves at an accelerated pace, the critical need to assess and monitor their performance emerges. We introduce a benchmarking framework focused on knowledge graph engineering (KGE) accompanied by three challenges addressing syntax and error correction, facts extraction and dataset generation. We show that while being a useful tool, LLMs are yet unfit to assist in knowledge graph generation with zero-shot prompting. Consequently, our LLM-KG-Bench framework provides automatic evaluation and storage of LLM responses as well as statistical data and visualization tools to support tracking of prompt engineering and model performance.", "title": "developing a scalable benchmark for assessing large language models in knowledge graph engineering", "url": "https://arxiv.org/pdf/2308.16622", "tokenized_text": "field large_language large language llms accelerated pace critical need assess monitor performance emerges introduce benchmarking framework focused knowledge_graph knowledge graph engineering accompanied challenges addressing syntax error correction facts extraction dataset generation useful tool llms assist knowledge_graph knowledge graph generation zero shot_prompting shot consequently llm kg bench framework provides automatic evaluation storage llm responses statistical data visualization tools support tracking prompt_engineering engineering performance"}
{"id": "e7d21ad4da122bf1db19e4fda57bf94c1dfa24a4", "abstract": "Prompt-based classifiers are an attractive approach for zero-shot classification. However, the precise choice of the prompt template and label words can largely influence performance, with semantically equivalent settings often showing notable performance difference. This discrepancy can be partly attributed to word biases, where the classifier may be biased towards classes. To address this problem, it is possible to optimise classification thresholds on a labelled data set, however, this mitigates some of the advantages of prompt-based classifiers. This paper instead approaches this problem by examining the expected marginal probabilities of the classes. Here, probabilities are reweighted to have a uniform prior over classes, in an unsupervised fashion. Further, we draw a theoretical connection between the class priors and the language models' word prior, and offer the ability to set a threshold in a zero-resource fashion. We show that matching class priors correlates strongly with the oracle upper bound performance and demonstrate large consistent performance gains for prompt settings over a range of NLP tasks.", "title": "mitigating word bias in zeroshot promptbased classifiers", "url": "https://arxiv.org/pdf/2309.04992", "tokenized_text": "based classifiers attractive approach zero shot classification precise choice prompt_template template label words largely influence performance semantically equivalent settings showing notable performance difference discrepancy partly attributed word biases classifier biased classes address problem possible optimise classification thresholds labelled data set mitigates advantages based classifiers paper instead approaches problem examining expected marginal probabilities classes probabilities uniform prior classes unsupervised fashion draw theoretical connection class priors language_models language word prior offer ability set threshold zero resource fashion matching class priors correlates strongly oracle upper bound performance demonstrate large consistent performance gains settings range nlp_tasks nlp tasks"}
{"id": "ed40889e11e812ef33578506844be06d713f6092", "abstract": "\"Thinking is for Doing.\"Humans can infer other people's mental states from observations--an ability called Theory-of-Mind (ToM)--and subsequently act pragmatically on those inferences. Existing question answering benchmarks such as ToMi ask models questions to make inferences about beliefs of characters in a story, but do not test whether models can then use these inferences to guide their actions. We propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others' mental states to actions in social scenarios. Experiments on T4D demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action. Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions. FaR boosts GPT-4's performance from 50% to 71% on T4D, outperforming other prompting methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to diverse out-of-distribution story structures and scenarios that also require ToM inferences to choose an action, consistently outperforming other methods including few-shot in-context learning.", "title": "how far are large language models from agents with theoryofmind", "url": "https://arxiv.org/pdf/2310.03051", "tokenized_text": "thinking infer people mental states observations ability called theory mind subsequently act inferences existing question_answering question answering benchmarks ask questions inferences beliefs characters story test use inferences guide actions propose_a_new propose new evaluation paradigm large_language large language llms thinking requires connect inferences mental states actions social scenarios experiments demonstrate llms gpt-4 palm seemingly excel tracking characters beliefs stories struggle translate capability strategic action analysis reveals core challenge llms lies identifying implicit inferences mental states explicitly asked lead choosing correct action bridge gap introduce zero shot_prompting shot framework foresee reflect provides reasoning structure encourages llms anticipate future challenges reason potential actions far boosts gpt-4 performance 50 71 outperforming methods chain thought self ask far generalizes diverse distribution story structures scenarios require tom inferences choose action consistently outperforming methods including shot context_learning context learning"}
{"id": "fe425e341cf646689e42adead17f14eeac5d03e6", "abstract": "Large language models (LLMs) have exhibited striking in-context learning (ICL) ability to adapt to target tasks with a few input-output demonstrations. For better ICL, different methods are proposed to select representative demonstrations from existing training corpora. However, such settings are not aligned with real-world practices, as end-users usually query LMs without access to demonstration pools. In this work, we introduce Self-ICL -- a simple framework which bootstraps LMs' intrinsic capabilities to perform zero-shot ICL. Given a test input, Self-ICL first prompts the model to generate pseudo-inputs. Next, the model predicts pseudo-labels for the pseudo-inputs via zero-shot prompting. Finally, we perform ICL for the test input with the pseudo-input-label pairs as demonstrations. Evaluation on 23 BIG-Bench Hard tasks shows Self-ICL outperforms zero-shot baselines on both average accuracy and head-to-head comparison. Moreover, with zero-shot chain-of-thought, Self-ICL achieves results comparable to using real demonstrations. Additionally, we conduct a range of analyses to validate Self-ICL's effectiveness and provide insights for its behaviors under different settings.", "title": "selficl zeroshot incontext learning with selfgenerated demonstrations", "url": "http://arxiv.org/pdf/2305.15035", "tokenized_text": "large_language large language llms exhibited striking context_learning context learning icl ability adapt target tasks input output demonstrations better icl different methods proposed select representative demonstrations existing training corpora settings aligned real world practices end users usually query lms access demonstration work introduce self icl simple framework bootstraps lms intrinsic capabilities perform zero shot icl given test input self icl generate pseudo inputs predicts pseudo labels pseudo inputs zero shot_prompting shot finally perform icl test input pseudo input label pairs demonstrations evaluation 23 big-bench_hard big-bench hard tasks shows self icl outperforms zero shot baselines average accuracy head head comparison zero shot chain thought self icl achieves results comparable real demonstrations additionally conduct range analyses validate self icl effectiveness provide insights behaviors different settings"}
{"id": "0088c9f4d50706c7ab71efa13bcb4b42cf2058e2", "abstract": "In-context learning is the ability of a pretrained model to adapt to novel and diverse downstream tasks by conditioning on prompt examples, without optimizing any parameters. While large language models have demonstrated this ability, how in-context learning could be performed over graphs is unexplored. In this paper, we develop \\textbf{Pr}etraining \\textbf{O}ver \\textbf{D}iverse \\textbf{I}n-Context \\textbf{G}raph S\\textbf{y}stems (PRODIGY), the first pretraining framework that enables in-context learning over graphs. The key idea of our framework is to formulate in-context learning over graphs with a novel \\emph{prompt graph} representation, which connects prompt examples and queries. We then propose a graph neural network architecture over the prompt graph and a corresponding family of in-context pretraining objectives. With PRODIGY, the pretrained model can directly perform novel downstream classification tasks on unseen graphs via in-context learning. We provide empirical evidence of the effectiveness of our framework by showcasing its strong in-context learning performance on tasks involving citation networks and knowledge graphs. Our approach outperforms the in-context learning accuracy of contrastive pretraining baselines with hard-coded adaptation by 18\\% on average across all setups. Moreover, it also outperforms standard finetuning with limited data by 33\\% on average with in-context learning.", "title": "prodigy enabling incontext learning over graphs", "url": "http://arxiv.org/pdf/2305.12600", "tokenized_text": "context_learning context learning ability pretrained adapt novel diverse downstream_tasks downstream tasks conditioning examples optimizing parameters large_language large language demonstrated ability context_learning context learning performed graphs unexplored paper develop \\textbf{i}n context pretraining framework enables context_learning context learning graphs key idea framework formulate context_learning context learning graphs novel graph representation connects examples queries propose graph neural network architecture graph corresponding family context pretraining objectives pretrained directly perform novel downstream classification tasks unseen graphs context_learning context learning provide empirical evidence effectiveness framework showcasing strong context_learning context learning performance tasks involving citation networks knowledge graphs approach outperforms context_learning context learning accuracy contrastive pretraining baselines hard coded adaptation 18\\% average setups outperforms standard finetuning limited data average context_learning context learning"}
{"id": "0095acc4f2c3255cf38fdf844003c97858adb418", "abstract": "Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors lack robustness against attacks: they degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, a malicious user might attempt to deliberately evade the detectors based on detection results, but this has not been assumed in previous studies. In this paper, we propose OUTFOX, a framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output. In this framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are harder to detect, while the detector uses the adversarially generated essays as examples for in-context learning to learn to detect essays from a strong attacker. Experiments in the domain of student essays show that the proposed detector improves the detection performance on the attacker-generated texts by up to +41.3 points in F1-score. Furthermore, the proposed detector shows a state-of-the-art detection performance: up to 96.9 points in F1-score, beating existing detectors on non-attacked texts. Finally, the proposed attacker drastically degrades the performance of detectors by up to -57.0 points F1-score, massively outperforming the baseline paraphrasing method for evading detection.", "title": "outfox llmgenerated essay detection through incontext learning with adversarially generated examples", "url": "https://arxiv.org/pdf/2307.11729", "tokenized_text": "large_language large language llms achieved human level fluency text generation making difficult distinguish human written llm generated texts poses growing risk misuse llms demands development detectors identify llm generated texts existing detectors lack robustness attacks detection accuracy simply paraphrasing llm generated texts furthermore malicious user attempt evade detectors based detection results assumed previous studies paper propose framework improves robustness llm generated text detectors allowing detector attacker consider output framework attacker uses detector prediction labels examples context_learning context learning adversarially generates essays harder detect detector uses adversarially generated essays examples context_learning context learning learn detect essays strong attacker experiments domain student essays proposed detector improves detection performance attacker generated texts points f1 score furthermore proposed detector shows state art detection performance points f1 score beating existing detectors non attacked texts finally proposed attacker drastically degrades performance detectors points f1 score massively outperforming baseline paraphrasing method evading detection"}
{"id": "00c367427d9135209d84008e6cb5e90f0adba881", "abstract": "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2.", "title": "naturalspeech 2 latent diffusion models are natural and zeroshot speech and singing synthesizers", "url": "http://arxiv.org/pdf/2304.09116", "tokenized_text": "scaling text speech tts large scale multi speaker wild datasets important capture diversity human speech speaker identities styles e.g. current large tts systems usually speech discrete tokens use language_models language generate tokens suffer unstable prosody word skipping repeating issue poor voice quality paper develop tts system leverages neural audio codec residual vector quantized latent vectors uses diffusion generate latent vectors conditioned text input enhance zero shot capability important achieve diverse speech synthesis design speech mechanism facilitate context_learning context learning diffusion duration predictor scale large scale datasets 44 hours speech data evaluate voice quality unseen speakers outperforms previous tts systems large margin terms prosody timbre similarity robustness voice quality zero shot_setting shot setting performs novel zero shot synthesis speech audio samples available"}
{"id": "03532123ccffae8d411264320e8a5ae2b6eddea0", "abstract": "Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple\"retrieve-then-read\"pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120%, 8-39%, and 80-290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp", "title": "demonstratesearchpredict composing retrieval and language models for knowledgeintensive nlp", "url": "http://arxiv.org/pdf/2212.14024", "tokenized_text": "retrieval augmented context_learning context learning emerged powerful approach addressing knowledge intensive tasks frozen language_models language lm retrieval rm existing work combined rm retrieves passages inserted lm begin fully realize potential frozen lms propose demonstrate search predict framework relies passing natural_language natural language texts sophisticated pipelines lm rm express high level programs bootstrap pipeline aware demonstrations search relevant passages generate grounded predictions systematically breaking problems small transformations lm rm handle reliably written novel programs answering questions open domain multi hop conversational settings establishing early evaluations new state art context_learning context learning results 37 120 80 relative gains vanilla lm gpt-3.5 standard retrieve read pipeline self ask pipeline respectively release"}
{"id": "0366177b44ed13d86b9d704a3a82ea3750e5abed", "abstract": "Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven\u2019s Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abstraction over task features, finding that higher-level abstractions further strengthen PLMs\u2019 analogical reasoning. Our detailed analysis reveals insights on the role of model complexity, in-context learning, and prior knowledge in solving RPM tasks.", "title": "incontext analogical reasoning with pretrained language models", "url": "http://arxiv.org/pdf/2305.17626", "tokenized_text": "analogical reasoning fundamental capacity human cognition allows reason novel situations past experiences thought essential robust reasoning ai systems conventional approaches require significant training and/or hard coding domain knowledge applied benchmark tasks inspired cognitive science research found connections human language making explore use intuitive language based abstractions support ai systems specifically apply large pre trained_language trained language plms visual raven progressive matrices common relational reasoning test simply encoding features problem language form find plms exhibit striking capacity zero shot relational reasoning exceeding human performance supervised vision based methods explore different encodings vary level abstraction task features finding higher level abstractions strengthen plms analogical reasoning detailed analysis reveals insights role complexity context_learning context learning prior knowledge solving tasks"}
{"id": "06edda0310b4ec7c5012d012349252a3a77521b6", "abstract": "Through in-context learning (ICL), large-scale language models are effective few-shot learners without additional model fine-tuning. However, the ICL performance does not scale well with the number of available training sample as it is limited by the inherent input length constraint of the underlying language model. Meanwhile, many studies have revealed that language models are also powerful feature extractors, allowing them to be utilized in a black-box manner and enabling the linear probing paradigm, where lightweight discriminators are trained on top of the pre-extracted input representations. This paper proposes prompt-augmented linear probing (PALP), a hybrid of linear probing and ICL, which leverages the best of both worlds. PALP inherits the scalability of linear probing and the capability of enforcing language models to derive more meaningful representations via tailoring input into a more conceivable form. Throughout in-depth investigations on various datasets, we verified that PALP significantly closes the gap between ICL in the data-hungry scenario and fine-tuning in the data-abundant scenario with little training overhead, potentially making PALP a strong alternative in a black-box scenario.", "title": "promptaugmented linear probing scaling beyond the limit of fewshot incontext learners", "url": "http://arxiv.org/pdf/2212.10873", "tokenized_text": "context_learning context learning icl large scale language_models language effective shot learners additional fine tuning icl performance scale number available training sample limited inherent input length constraint underlying language_model language studies revealed language_models language powerful feature extractors allowing utilized black box manner enabling linear probing paradigm lightweight trained pre extracted input representations paper_proposes paper proposes augmented linear probing hybrid linear probing icl leverages best worlds inherits scalability linear probing capability language_models language derive meaningful representations tailoring input form depth investigations datasets verified significantly closes gap icl data scenario fine tuning data abundant scenario little training overhead potentially making strong alternative black box scenario"}
{"id": "070b91f80ac118b910c1d2ab5be9f65f685979fe", "abstract": "In this work, we investigate the capacity of language models to generate explicit, interpretable, and interactive world models of scientific and common-sense reasoning tasks. We operationalize this as a task of generating text games, expressed as hundreds of lines of Python code. To facilitate this task, we introduce ByteSized32 (Code: github.com/cognitiveailab/BYTESIZED32), a corpus of 32 reasoning-focused text games totaling 20k lines of Python code. We empirically demonstrate that GPT-4 can use these games as templates for single-shot in-context learning, successfully producing runnable games on unseen topics in 28% of cases. When allowed to self-reflect on program errors, game runnability substantially increases to 57%. While evaluating simulation fidelity is labor-intensive, we introduce a suite of automated metrics to assess game fidelity, technical validity, adherence to task specifications, and winnability, showing a high degree of agreement with expert human ratings. We pose this as a challenge task to spur further development at the juncture of world modeling and code generation.", "title": "bytesized32 a corpus and challenge task for generating taskspecific world models expressed as text games", "url": "http://arxiv.org/pdf/2305.14879", "tokenized_text": "work investigate capacity language_models language generate explicit interpretable interactive world scientific common sense reasoning tasks task generating text games expressed hundreds lines python code facilitate task introduce code corpus 32 reasoning focused text games lines python code empirically demonstrate gpt-4 use games templates single shot context_learning context learning successfully producing runnable games unseen topics 28 cases allowed self reflect program errors game substantially increases 57 evaluating simulation fidelity labor intensive introduce suite automated metrics assess game fidelity technical validity adherence task specifications showing high degree agreement expert human ratings pose challenge task spur development world modeling code_generation code generation"}
{"id": "0744783bbefc12b2b1383bed137e8a80061274b7", "abstract": "After discovering that Language Models (LMs) can be good in-context few-shot learners, numerous strategies have been proposed to optimize in-context sequence configurations. Recently, researchers in Vision-Language (VL) domains also develop their few-shot learners, while they only use the simplest way, i.e., randomly sampling, to configure in-context image-text pairs. In order to explore the effects of varying configurations on VL in-context learning, we devised four strategies for image selection and four for caption assignment to configure in-context image-text pairs for image captioning. Here Image Captioning is used as the case study since it can be seen as the visually-conditioned LM. Our comprehensive experiments yield two counter-intuitive but valuable insights, highlighting the distinct characteristics of VL in-context learning due to multi-modal synergy, as compared to the NLP case.", "title": "exploring diverse incontext configurations for image captioning", "url": "http://arxiv.org/pdf/2305.14800", "tokenized_text": "discovering language_models language lms good context shot learners numerous strategies proposed optimize context sequence configurations recently researchers vision language vl domains develop shot learners use simplest way i.e. randomly sampling configure context image text pairs order explore effects varying configurations vl context_learning context learning devised strategies image selection caption assignment configure context image text pairs image captioning image captioning case study seen visually conditioned lm comprehensive experiments yield counter intuitive valuable insights highlighting distinct characteristics vl context_learning context learning multi modal synergy compared nlp case"}
{"id": "097dc73d5d422b3c09286e72d16b2561ae5fb395", "abstract": "Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two different factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By perturbing explanations on three controlled tasks, we show that both factors contribute to the effectiveness of explanations. We further study how to form maximally effective sets of explanations for solving a given test query. We find that LLMs can benefit from the complementarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the in-context learning performance across three real-world tasks on multiple LLMs.", "title": "complementary explanations for effective incontext learning", "url": "http://arxiv.org/pdf/2211.13892", "tokenized_text": "large_language large language llms exhibited remarkable_capabilities remarkable capabilities learning explanations limited understanding exactly explanations function effective work aims better understand mechanisms explanations context_learning context learning study impact different factors performance explanations computation trace way solution decomposed natural_language natural language express perturbing explanations controlled tasks factors contribute effectiveness explanations study form effective sets explanations solving given test query find llms benefit explanation set diverse reasoning skills shown different exemplars lead better performance propose marginal relevance based exemplar selection approach constructing exemplar sets relevant complementary successfully improves context_learning context learning performance real world tasks multiple llms"}
{"id": "09a85806442373f167e45eaf662a7914df048b10", "abstract": "The emergent ability of Large Language Models to use a small number of examples to learn to perform in novel domains and tasks, also called in-context learning (ICL). In this work, we show that a much smaller model can be trained to perform ICL by fine-tuning towards a specialized training objective, exemplified on the task of domain adaptation for neural machine translation. With this capacity for ICL, the model can take advantage of relevant few-shot examples to adapt its output towards the domain. We compare the quality of this domain adaptation to traditional supervised techniques and ICL with a 40B-parameter Large Language Model. Our approach allows efficient batch inference on a mix of domains and outperforms state-of-the-art baselines in terms of both translation quality and immediate adaptation rate, i.e. the ability to reproduce a specific term after being shown a single example.", "title": "neural machine translation models can learn to be fewshot learners", "url": "https://arxiv.org/pdf/2309.08590", "tokenized_text": "emergent ability large_language large language use small_number small number examples learn perform novel domains tasks called context_learning context learning icl work smaller trained perform icl fine tuning specialized training objective task domain adaptation neural machine_translation machine translation capacity icl advantage relevant shot examples adapt output domain compare quality domain adaptation traditional supervised techniques icl parameter large_language large language approach allows efficient batch inference mix domains outperforms state art baselines terms translation quality immediate adaptation rate i.e. ability reproduce specific term shown single example"}
{"id": "0a2ac054c533314c0659f3b139388527df0d42f3", "abstract": "Recent advances in prompt-based learning have shown strong results on few-shot text classification by using cloze-style templates.Similar attempts have been made on named entity recognition (NER) which manually design templates to predict entity types for every text span in a sentence. However, such methods may suffer from error propagation induced by entity span detection, high cost due to enumeration of all possible text spans, and omission of inter-dependencies among token labels in a sentence. Here we present a simple demonstration-based learning method for NER, which lets the input be prefaced by task demonstrations for in-context learning. We perform a systematic study on demonstration strategy regarding what to include (entity examples, with or without surrounding context), how to select the examples, and what templates to use. Results on in-domain learning and domain adaptation show that the model\u2019s performance in low-resource settings can be largely improved with a suitable demonstration strategy (e.g., a 4-17% improvement on 25 train instances). We also find that good demonstration can save many labeled examples and consistency in demonstration contributes to better performance.", "title": "good examples make a faster learner simple demonstrationbased learning for lowresource ner", "url": "https://aclanthology.org/2022.acl-long.192.pdf", "tokenized_text": "recent_advances recent advances based learning shown strong results shot text_classification text classification cloze style templates similar attempts named_entity named entity recognition ner manually design templates predict entity types text span sentence methods suffer error propagation induced entity span detection high cost enumeration possible text spans omission inter dependencies token labels sentence present simple demonstration based learning method ner input task demonstrations context_learning context learning perform systematic study demonstration strategy include entity examples surrounding context select examples templates use results domain learning domain adaptation performance low resource settings largely improved suitable demonstration strategy e.g. 17 improvement 25 train instances find good demonstration save labeled examples consistency demonstration contributes better performance"}
{"id": "0a67a5e3f4125445ed84f2db3c92429010aad68a", "abstract": "Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns. To test this, we present a structured prompting approach for linguistic structured prediction tasks, allowing us to perform zero- and few-shot sequence tagging with autoregressive PLMs. We evaluate this approach on part-of-speech tagging, named entity recognition, and sentence chunking, demonstrating strong few-shot performance in all cases. We also find that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus, structured prompting can also retrieve linguistic structure with arbitrary labels. These findings indicate that the in-context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data.", "title": "prompting language models for linguistic structure", "url": "http://arxiv.org/pdf/2211.07830", "tokenized_text": "pretrained_language pretrained language plms prompted perform wide_range wide range language tasks remains open question ability comes generalizable linguistic understanding versus surface level lexical patterns test present structured approach linguistic structured prediction tasks allowing perform zero- shot sequence tagging autoregressive plms evaluate approach speech tagging named_entity named entity recognition sentence demonstrating strong shot performance cases find plms contain significant prior knowledge task labels task leakage pretraining corpus structured retrieve linguistic structure arbitrary labels findings indicate context_learning context learning ability linguistic knowledge plms generalizes memorization training_data training data"}
{"id": "0aa5940fda7c994675d08c41eca2a6909eb6d205", "abstract": "In recent years, large-scale language models (LLMs) have gained attention for their impressive text generation capabilities. However, these models often face the challenge of\"hallucination,\"which undermines their reliability. In this study, we introduce an uncertainty-aware in-context learning framework to empower the model to enhance or reject its output in response to uncertainty. Human-defined methods for estimating uncertainty typically assume that\"uncertainty is lower when the model's response is correct compared to when it is incorrect.\"However, setting a precise threshold to distinguish correctness is challenging. Therefore, we introduce uncertainty information as an intermediary variable that implicitly influences the model's behavior. Our innovative uncertainty-aware in-context learning framework involves fine-tuning the LLM using a calibration dataset. Our aim is to improve the model's responses by filtering out answers with high uncertainty while considering the model's knowledge limitations. We evaluate the model's knowledge by examining multiple responses to the same question for the presence of a correct answer. When the model lacks relevant knowledge, the response should indicate that the question cannot be answered. Conversely, when the model has relevant knowledge, the response should provide the correct answer. Extensive experiments confirm the effectiveness of our framework, leading to two key findings. First, the logit output values of the LLM partly reflect inherent uncertainty. Second, our model autonomously recognizes uncertainty, resulting in improved responses.", "title": "improving the reliability of large language models by leveraging uncertaintyaware incontext learning", "url": "https://arxiv.org/pdf/2310.04782", "tokenized_text": "recent_years recent years large scale language_models language llms gained attention impressive text generation capabilities face challenge reliability study introduce uncertainty aware context_learning context learning framework empower enhance reject output response uncertainty human defined methods estimating uncertainty typically assume lower response correct compared incorrect setting precise threshold distinguish correctness challenging introduce uncertainty information intermediary variable implicitly influences behavior innovative uncertainty aware context_learning context learning framework involves fine tuning llm calibration dataset aim improve responses filtering answers high uncertainty considering knowledge limitations evaluate knowledge examining multiple responses question presence correct answer lacks relevant knowledge response indicate question answered conversely relevant knowledge response provide correct answer extensive_experiments extensive experiments confirm effectiveness framework leading key findings logit output values llm partly reflect inherent uncertainty second autonomously uncertainty resulting improved responses"}
{"id": "0ae12d63f77f40b430f17c791a5191ff5fee5086", "abstract": "Compositional generalization\u2013understanding unseen combinations of seen primitives\u2013is an essential reasoning capability in human intelligence.The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning\u2013the prevailing few-shot paradigm based on large language models\u2013exhibits compositional generalization.In this paper, we present CoFe, a test suite to investigate in-context compositional generalization.We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization.We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple.Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus.We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm.", "title": "how do incontext examples affect compositional generalization", "url": "http://arxiv.org/pdf/2305.04835", "tokenized_text": "compositional generalization understanding unseen combinations seen primitives essential reasoning capability human intelligence ai community mainly studies capability fine tuning neural_networks neural networks lots training samples unclear context_learning context learning prevailing shot paradigm based large_language large language exhibits compositional generalization paper present test suite investigate context compositional generalization find compositional generalization performance easily affected selection context_examples context examples raising research question key factors good context_examples context examples compositional generalization study potential factors similarity diversity complexity systematic experiments indicate context_examples context examples structurally similar test case diverse individually simple furthermore strong limitations observed context compositional generalization words weaker commonly ones critical context_examples context examples cover required linguistic structures backbone pre trained large corpus hope analysis facilitate understanding utilization context_learning context learning paradigm"}
{"id": "0cfdd655100055f234fd23ebecd915504b8e00e3", "abstract": "Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually also hinder their broad deployment in real-world systems. To address these limitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building conversational recommender systems by converting user profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning, which also makes the recommendation process more interactive and explainable. What's more, within the Chat-Rec framework, user's preferences can transfer to different products for cross-domain recommendations, and prompt-based injection of information into LLMs can also handle the cold-start scenarios with new items. In our experiments, Chat-Rec effectively improve the results of top-k recommendations and performs better in zero-shot rating prediction task. Chat-Rec offers a novel approach to improving recommender systems and presents new practical scenarios for the implementation of AIGC (AI generated content) in recommender system studies.", "title": "chatrec towards interactive and explainable llmsaugmented recommender system", "url": "http://arxiv.org/pdf/2303.14524", "tokenized_text": "large_language large language llms demonstrated significant potential applied addressing application tasks traditional recommender systems continue face great challenges poor explainability actually hinder broad deployment real world systems address limitations paper_proposes paper proposes novel paradigm called chat chatgpt augmented recommender system augments llms building conversational recommender systems converting user profiles historical interactions chat demonstrated effective learning user preferences establishing connections users products context_learning context learning makes recommendation process interactive explainable chat framework user preferences transfer different products cross domain recommendations based injection information llms handle cold start scenarios new items experiments chat effectively improve results recommendations performs better zero shot rating prediction task chat offers novel_approach novel approach improving recommender systems presents new practical scenarios implementation aigc ai generated content recommender system studies"}
{"id": "0d0dbfb1b315a43216020abaf74d289456198219", "abstract": "Pre-trained vision-language (V-L) models such as CLIP have shown excellent generalization ability to downstream tasks. However, they are sensitive to the choice of input text prompts and require careful selection of prompt templates to perform well. Inspired by the Natural Language Processing (NLP) literature, recent CLIP adaptation approaches learn prompts as the textual inputs to fine-tune CLIP for downstream tasks. We note that using prompting to adapt representations in a single branch of CLIP (language or vision) is sub-optimal since it does not allow the flexibility to dynamically adjust both representation spaces on a downstream task. In this work, we propose Multi-modal Prompt Learning (MaPLe) for both vision and language branches to improve alignment between the vision and language representations. Our design promotes strong coupling between the vision-language prompts to ensure mutual synergy and discourages learning independent uni-modal solutions. Further, we learn separate prompts across different early stages to progressively model the stage-wise feature relationships to allow rich context learning. We evaluate the effectiveness of our approach on three representative tasks of generalization to novel classes, new target datasets and unseen domain shifts. Compared with the state-of-the-art method Co-CoOp, MaPLe exhibits favorable performance and achieves an absolute gain of 3.45% on novel classes and 2.72% on overall harmonic-mean, averaged over 11 diverse image recognition datasets. Our code and pre-trained models are available at https://github.com/muzairkhattak/multimodal-prompt-learning.", "title": "maple multimodal prompt learning", "url": "https://arxiv.org/pdf/2210.03117", "tokenized_text": "pre trained vision language clip shown excellent generalization_ability generalization ability downstream_tasks downstream tasks sensitive choice input text require careful selection prompt_templates templates perform inspired natural_language natural language processing nlp literature recent clip adaptation approaches learn textual inputs fine tune clip downstream_tasks downstream tasks note adapt representations single branch clip language vision sub optimal allow flexibility dynamically adjust representation spaces downstream task work propose multi modal learning vision language branches improve alignment vision language representations design promotes strong coupling vision language ensure mutual synergy learning independent uni modal solutions learn separate different early stages progressively stage wise feature relationships allow rich context_learning context learning evaluate effectiveness approach representative tasks generalization novel classes new target datasets unseen domain shifts compared state art method co coop exhibits performance achieves absolute gain novel classes overall harmonic mean averaged 11 diverse image recognition datasets code pre trained available"}
{"id": "0ea7fc93d4947d9024ccaa202987a2070683bc1f", "abstract": "Scaling large language models (LLMs) leads to an emergent capacity to learn in-context from example demonstrations. Despite progress, theoretical understanding of this phenomenon remains limited. We argue that in-context learning relies on recombination of compositional operations found in natural language data. We derive an information-theoretic bound showing how in-context learning abilities arise from generic next-token prediction when the pretraining distribution has sufficient amounts of compositional structure, under linguistically motivated assumptions. A second bound provides a theoretical justification for the empirical success of prompting LLMs to output intermediate steps towards an answer. To validate theoretical predictions, we introduce a controlled setup for inducing in-context learning; unlike previous approaches, it accounts for the compositional nature of language. Trained transformers can perform in-context learning for a range of tasks, in a manner consistent with the theoretical results. Mirroring real-world LLMs in a miniature setup, in-context learning emerges when scaling parameters and data, and models perform better when prompted to output intermediate steps. Probing shows that in-context learning is supported by a representation of the input's compositional structure. Taken together, these results provide a step towards theoretical understanding of emergent behavior in large language models.", "title": "a theory of emergent incontext learning as implicit structure induction", "url": "http://arxiv.org/pdf/2303.07971", "tokenized_text": "scaling large_language large language llms leads emergent capacity learn context example demonstrations despite progress theoretical understanding phenomenon remains limited argue context_learning context learning relies compositional operations found natural_language natural language data derive information theoretic bound showing context_learning context learning abilities arise generic token prediction pretraining distribution sufficient amounts compositional structure linguistically motivated assumptions second bound provides theoretical justification empirical success llms output intermediate steps answer validate theoretical predictions introduce controlled setup inducing context_learning context learning unlike previous approaches accounts compositional nature language trained transformers perform context_learning context learning range tasks manner consistent theoretical results real world llms setup context_learning context learning emerges scaling parameters data perform better prompted output intermediate steps probing shows context_learning context learning supported representation input compositional structure taken results provide step theoretical understanding emergent behavior large_language large language"}
{"id": "0f45608ddc01b3e192f3490330f4c4b8de074f79", "abstract": "Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understanding, and code generation benchmarks, show that SEC, which does not require hand-crafted demonstrations, significantly outperforms the zero-shot learning strategy, and achieves comparable results to ICL with hand-crafted demonstrations. This demonstrates that, for many tasks, contemporary LLMs possess a sufficient level of competence to exclusively depend on their own capacity for decision making, removing the need for external training data. Code is available at https://github.com/ruili33/SEC.", "title": "are humangenerated demonstrations necessary for incontext learning", "url": "https://arxiv.org/pdf/2309.14681", "tokenized_text": "despite promising shot ability large_language large language llms standard paradigm context_learning context learning icl suffers disadvantages susceptibility selected demonstrations generate demonstrations paper raise fundamental question human generated demonstrations necessary icl answer question propose self strategy paradigm free human crafted demonstrations key point instead hand crafted examples demonstrations icl asks llms create demonstrations based final output generated flexible framework adapted vanilla icl chain thought cot greater ease manual generation process examples rationale saved extensive_experiments extensive experiments arithmetic reasoning commonsense reasoning multi task language understanding code_generation code generation benchmarks require hand crafted demonstrations significantly_outperforms significantly outperforms zero shot_learning shot learning strategy achieves comparable results icl hand crafted demonstrations demonstrates tasks contemporary llms possess sufficient level competence exclusively depend capacity decision_making decision making removing need external training_data training data code_is_available code available"}
{"id": "0f4ab3fe492ececbfd38be9682047371e2e9b8c6", "abstract": "Explainable question answering systems should produce not only accurate answers but also rationales that justify their reasoning and allow humans to check their work. But what sorts of rationales are useful and how can we train systems to produce them? We propose a new style of rationale for open-book question answering, called \\emph{markup-and-mask}, which combines aspects of extractive and free-text explanations. In the markup phase, the passage is augmented with free-text markup that enables each sentence to stand on its own outside the discourse context. In the masking phase, a sub-span of the marked-up passage is selected. To train a system to produce markup-and-mask rationales without annotations, we leverage in-context learning. Specifically, we generate silver annotated data by sending a series of prompts to a frozen pretrained language model, which acts as a teacher. We then fine-tune a smaller student model by training on the subset of rationales that led to correct answers. The student is\"honest\"in the sense that it is a pipeline: the rationale acts as a bottleneck between the passage and the answer, while the\"untrusted\"teacher operates under no such constraints. Thus, we offer a new way to build trustworthy pipeline systems from a combination of end-task annotations and frozen pretrained language models.", "title": "honest students from untrusted teachers learning an interpretable questionanswering pipeline from a pretrained language model", "url": "http://arxiv.org/pdf/2210.02498", "tokenized_text": "explainable question_answering question answering systems produce accurate answers rationales justify reasoning allow humans check work rationales useful train systems produce propose_a_new propose new style rationale open book question_answering question answering called mask combines aspects extractive free text explanations markup phase passage augmented free text markup enables sentence stand outside discourse context masking phase sub span marked passage selected train system produce markup mask rationales annotations leverage context_learning context learning specifically generate annotated_data annotated data series frozen pretrained_language pretrained language acts teacher fine tune smaller student training subset rationales led correct answers student sense pipeline rationale acts bottleneck passage answer operates constraints offer new way build trustworthy pipeline systems combination end task annotations frozen pretrained_language pretrained language"}
{"id": "102e4c860e39a2bfd7bf3f03b9ad69aac7bf3b5f", "abstract": "Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.", "title": "collaborating with language models for embodied reasoning", "url": "http://arxiv.org/pdf/2302.00763", "tokenized_text": "reasoning complex ambiguous environment key goal reinforcement_learning reinforcement learning rl agents sophisticated rl agents successfully solve difficult tasks require large training_data training data struggle generalize new unseen environments new tasks hand large_scale large scale language_models language lslms exhibited strong reasoning ability ability adapt new tasks context_learning context learning lslms inherently ability intervene environment work investigate combine complementary abilities single system consisting parts planner actor planner pre trained_language trained language issue commands simple embodied agent actor planner inform command present set tasks require reasoning test system ability generalize zero shot investigate failure cases demonstrate components system trained reinforcement learning improve performance"}
{"id": "10955e63aa49fab146267949f8ebc9ebe8275183", "abstract": "Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown impressive reasoning ability in various downstream tasks. Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA. To alleviate this issue, we propose a framework called Knowledge-Driven Chain-of-Thought (KD-CoT) to verify and modify reasoning traces in CoT via interaction with external knowledge, and thus overcome the hallucinations and error propagation. Concretely, we formulate the CoT rationale process of LLMs into a structured multi-round QA format. In each round, LLMs interact with a QA system that retrieves external knowledge and produce faithful reasoning traces based on retrieved precise answers. The structured CoT reasoning of LLMs is facilitated by our developed KBQA CoT collection, which serves as in-context learning demonstrations and can also be utilized as feedback augmentation to train a robust retriever. Extensive experiments on WebQSP and ComplexWebQuestion datasets demonstrate the effectiveness of proposed KD-CoT in task-solving reasoning generation, which outperforms the vanilla CoT ICL with an absolute success rate of 8.0% and 5.1%. Furthermore, our proposed feedback-augmented retriever outperforms the state-of-the-art baselines for retrieving knowledge, achieving significant improvement in Hit and recall performance. Our code and data are released on https://github.com/AdelWang/KD-CoT/tree/main.", "title": "knowledgedriven cot exploring faithful reasoning in llms for knowledgeintensive question answering", "url": "https://arxiv.org/pdf/2308.13259", "tokenized_text": "equipped chain thought cot large_language large language llms shown_impressive shown impressive reasoning ability downstream_tasks downstream tasks suffering hallucinations inability access external_knowledge external knowledge llms come incorrect unfaithful intermediate reasoning_steps reasoning steps especially context answering knowledge intensive tasks kbqa alleviate issue propose framework called knowledge driven chain thought kd cot verify modify reasoning traces cot interaction external_knowledge external knowledge overcome hallucinations error propagation concretely formulate cot rationale process llms structured multi round qa format round llms interact qa system retrieves external_knowledge external knowledge produce faithful reasoning traces based retrieved precise answers structured cot reasoning llms facilitated developed kbqa cot collection serves context_learning context learning demonstrations utilized feedback augmentation train robust retriever extensive_experiments extensive experiments webqsp datasets demonstrate_the_effectiveness demonstrate effectiveness proposed kd cot task solving reasoning generation outperforms vanilla cot icl absolute success_rate success rate 5.1 furthermore proposed feedback augmented retriever outperforms state art baselines retrieving knowledge achieving significant improvement recall performance code data released"}
{"id": "135ae2ea7a2c966815e85a232469a0a14b4d8d67", "abstract": "We aim to better understand the emergence of `situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations. At test time, we assess whether the model can pass the test. To our surprise, we find that LLMs succeed on this out-of-context reasoning task. Their success is sensitive to the training setup and only works when we apply data augmentation. For both GPT-3 and LLaMA-1, performance improves with model size. These findings offer a foundation for further empirical study, towards predicting and potentially controlling the emergence of situational awareness in LLMs. Code is available at: https://github.com/AsaCooperStickland/situational-awareness-evals.", "title": "taken out of context on measuring situational awareness in llms", "url": "https://arxiv.org/pdf/2309.00667", "tokenized_text": "aim better understand emergence situational awareness large_language large language llms situationally aware aware recognize currently testing deployment today llms tested safety alignment deployed llm exploit situational awareness achieve high score safety tests taking harmful actions deployment situational awareness emerge scaling way better foresee emergence run scaling experiments abilities necessary situational awareness ability propose context reasoning contrast context_learning context learning study context reasoning experimentally finetune llm description test providing examples demonstrations test_time test time assess pass test find llms succeed context reasoning task success sensitive training setup works apply data_augmentation data augmentation gpt-3 performance improves model_size size findings offer foundation empirical study predicting potentially controlling emergence situational awareness llms code_is_available code available"}
{"id": "154493f69d7db3d49da0e51df0192c6ad5f1724a", "abstract": "We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings. We investigate two setups-ICL with flipped labels and ICL with semantically-unrelated labels-across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo/bar instead of negative/positive), thereby forcing language models to learn the input-label mappings shown in in-context exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input-label mappings, but more of the former.", "title": "larger language models do incontext learning differently", "url": "http://arxiv.org/pdf/2303.03846", "tokenized_text": "study context_learning context learning icl language_models language affected semantic priors versus input label mappings investigate setups icl labels icl semantically unrelated labels families gpt-3 instructgpt codex palm flan palm experiments icl labels semantic priors emergent ability scale small language_models language ignore labels presented context rely primarily semantic priors pretraining large override semantic priors presented context exemplars contradict priors despite stronger semantic priors larger hold study semantically unrelated label icl icl labels semantically unrelated inputs e.g. bar instead negative positive forcing language_models language learn input label mappings shown context exemplars order perform task ability icl emerges primarily scale large language_models language perform linear classification icl setting finally evaluate instruction tuned find instruction_tuning instruction tuning strengthens use semantic priors capacity learn input label mappings"}
{"id": "15fcd80193d1c446bc3d37fcc30f5475b9ebd5b0", "abstract": "This paper presents a novel application of large language models in user simulation for task-oriented dialog systems, specifically focusing on an in-context learning approach. By harnessing the power of these models, the proposed approach generates diverse utterances based on user goals and limited dialog examples. Unlike traditional simulators, this method eliminates the need for labor-intensive rule definition or extensive annotated data, making it more efficient and accessible. Additionally, an error analysis of the interaction between the user simulator and dialog system uncovers common mistakes, providing valuable insights into areas that require improvement. Our implementation is available at https://github.com/telepathylabsai/prompt-based-user-simulator.", "title": "incontext learning user simulators for taskoriented dialog systems", "url": "http://arxiv.org/pdf/2306.00774", "tokenized_text": "paper_presents paper presents novel application large_language large language user simulation task oriented dialog systems specifically focusing context_learning context learning approach harnessing power proposed approach generates diverse utterances based user goals limited dialog examples unlike traditional simulators method eliminates need labor intensive rule definition extensive annotated_data annotated data making efficient accessible additionally error analysis interaction user simulator dialog system uncovers common mistakes providing valuable insights areas require improvement implementation available"}
{"id": "16aacf48048ac128a07fe2c0761439e1d7211492", "abstract": "A proven therapeutic technique to overcome negative thoughts is to replace them with a more hopeful \u201creframed thought.\u201d Although therapy can help people practice and learn this Cognitive Reframing of Negative Thoughts, clinician shortages and mental health stigma commonly limit people\u2019s access to therapy. In this paper, we conduct a human-centered study of how language models may assist people in reframing negative thoughts. Based on psychology literature, we define a framework of seven linguistic attributes that can be used to reframe a thought. We develop automated metrics to measure these attributes and validate them with expert judgements from mental health practitioners. We collect a dataset of 600 situations, thoughts and reframes from practitioners and use it to train a retrieval-enhanced in-context learning model that effectively generates reframed thoughts and controls their linguistic attributes. To investigate what constitutes a \u201chigh-quality\u201d reframe, we conduct an IRB-approved randomized field study on a large mental health website with over 2,000 participants. Amongst other findings, we show that people prefer highly empathic or specific reframes, as opposed to reframes that are overly positive. Our findings provide key implications for the use of LMs to assist people in overcoming negative thoughts.", "title": "cognitive reframing of negative thoughts through humanlanguage model interaction", "url": "http://arxiv.org/pdf/2305.02466", "tokenized_text": "proven therapeutic technique overcome negative thoughts replace reframed thought therapy help people practice learn cognitive reframing negative thoughts mental_health mental health commonly limit people access therapy paper conduct human centered study language_models language assist people reframing negative thoughts based psychology literature define framework seven linguistic attributes reframe thought develop automated metrics measure attributes validate expert judgements mental_health mental health practitioners collect dataset 600 situations thoughts practitioners use train retrieval enhanced context_learning context learning effectively generates reframed thoughts controls linguistic attributes investigate constitutes high quality reframe conduct approved randomized field study large mental_health mental health website 2,000 participants findings people prefer highly specific opposed overly positive findings provide key implications use lms assist people overcoming negative thoughts"}
{"id": "18143a4c2da37444e06feed04cc9efeb0856352d", "abstract": "In-context learning (ICL), teaching a large language model (LLM) to perform a task with few-shot demonstrations rather than adjusting the model parameters, has emerged as a strong paradigm for using LLMs. While early studies primarily used a fixed or random set of demonstrations for all test queries, recent research suggests that retrieving semantically similar demonstrations to the input from a pool of available demonstrations results in better performance. This work expands the applicability of retrieval-based ICL approaches by demonstrating that even simple word-overlap similarity measures such as BM25 outperform randomly selected demonstrations. Furthermore, we extend the success of retrieval-based ICL to instruction-finetuned LLMs as well as Chain-of-Thought (CoT) prompting. For instruction-finetuned LLMs, we find that although a model has already seen the training data at training time, retrieving demonstrations from the training data at test time yields better results compared to using no demonstrations or random demonstrations. Last but not least, we train a task-specific demonstration retriever that outperforms off-the-shelf retrievers.", "title": "dricl demonstrationretrieved incontext learning", "url": "http://arxiv.org/pdf/2305.14128", "tokenized_text": "context_learning context learning icl teaching large_language large language llm perform task shot demonstrations adjusting parameters emerged strong paradigm llms early studies primarily fixed random set demonstrations test queries recent research suggests retrieving semantically similar demonstrations input pool available demonstrations results better performance work expands applicability retrieval based icl approaches demonstrating simple word overlap similarity measures bm25 outperform randomly selected demonstrations furthermore extend success retrieval based icl instruction finetuned llms chain thought cot instruction finetuned llms find seen training_data training data training time retrieving demonstrations training_data training data test_time test time yields better results compared demonstrations random demonstrations train task specific demonstration retriever outperforms shelf retrievers"}
{"id": "18bd959aaa8a83b5b2192282224d700da7459857", "abstract": "Designing systems that can reason across cultures requires that they are grounded in the norms of the contexts in which they operate. However, current research on developing computational models of social norms has primarily focused on American society. Here, we propose a novel approach to discover and compare descriptive social norms across Chinese and American cultures. We demonstrate our approach by leveraging discussions on a Chinese Q&A platform (Zhihu) and the existing SocialChemistry dataset as proxies for contrasting cultural axes, align social situations cross-culturally, and extract social norms from texts using in-context learning. Embedding Chain-of-Thought prompting in a human-AI collaborative framework, we build a high-quality dataset of 3,069 social norms aligned with social situations across Chinese and American cultures alongside corresponding free-text explanations. To test the ability of models to reason about social norms across cultures, we introduce the task of explainable social norm entailment, showing that existing models under 3B parameters have significant room for improvement in both automatic and human evaluation. Further analysis of cross-cultural norm differences based on our dataset shows empirical alignment with the social orientations framework, revealing several situational and descriptive nuances in norms across these cultures.", "title": "sociocultural norm similarities and differences via situational alignment and explainable textual entailment", "url": "http://arxiv.org/pdf/2305.14492", "tokenized_text": "designing systems reason cultures requires grounded norms contexts operate current research developing computational social norms primarily focused american society propose_a_novel propose novel approach discover compare descriptive social norms chinese american cultures demonstrate approach leveraging discussions chinese q&a platform existing dataset proxies cultural axes align social situations cross culturally extract social norms texts context_learning context learning embedding chain thought_prompting thought human ai collaborative framework build high quality dataset social norms aligned social situations chinese american cultures alongside corresponding free text explanations test ability reason social norms cultures introduce task explainable social norm entailment showing existing parameters significant room improvement automatic human evaluation analysis cross cultural norm differences based dataset shows empirical alignment social framework revealing situational descriptive nuances norms cultures"}
{"id": "19443d48399d4fe89a4b0a96917c50c6fd9c5af1", "abstract": "Warning: this paper contains content that may be inappropriate or offensive. As generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. Here we propose an automatic red teaming framework that evaluates a given model and exposes its vulnerabilities against unsafe and inappropriate content generation. Our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. We propose different in-context attack strategies to automatically learn effective and diverse adversarial prompts for text-to-image models. Our experiments demonstrate that compared to baseline approaches, our proposed strategy is significantly more effective in exposing vulnerabilities in Stable Diffusion (SD) model, even when the latter is enhanced with safety features. Furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text models, resulting in significantly higher toxic response generation rate compared to previously reported numbers.", "title": "flirt feedback loop incontext red teaming", "url": "https://arxiv.org/pdf/2308.04265", "tokenized_text": "paper contains content inappropriate offensive generative available public use applications testing analyzing vulnerabilities priority propose automatic red teaming framework evaluates given vulnerabilities unsafe inappropriate content generation framework uses context_learning context learning feedback loop red team trigger unsafe content generation propose different context attack strategies automatically learn effective diverse adversarial text image experiments_demonstrate experiments demonstrate compared baseline approaches proposed strategy significantly effective exposing vulnerabilities stable_diffusion stable diffusion sd enhanced safety features furthermore demonstrate proposed framework effective red teaming text text resulting significantly higher toxic response generation rate compared previously reported numbers"}
{"id": "197022486b2e2584302bd9b6442e44d15bf3e351", "abstract": "Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated remarkable results in various natural language processing (NLP) tasks with in-context learning, which involves inference based on a few demonstration examples. Despite their successes in NLP tasks, no investigation has been conducted to assess the ability of LLMs to perform document information extraction (DIE) using in-context learning. Applying LLMs to DIE poses two challenges: the modality and task gap. To this end, we propose a simple but effective in-context learning framework called ICL-D3IE, which enables LLMs to perform DIE with different types of demonstration examples. Specifically, we extract the most difficult and distinct segments from hard training documents as hard demonstrations for benefiting all test instances. We design demonstrations describing relationships that enable LLMs to understand positional relationships. We introduce formatting demonstrations for easy answer extraction. Additionally, the framework improves diverse demonstrations by updating them iteratively. Our experiments on three widely used benchmark datasets demonstrate that the ICL-D3IE framework enables Davinci-003/ChatGPT to achieve superior performance when compared to previous pre-trained methods fine-tuned with full training in both the in-distribution (ID) setting and in the out-of-distribution (OOD) setting. Code is available at https://github.com/MAEHCM/ICL-D3IE.", "title": "icld3ie incontext learning with diverse demonstrations updating for document information extraction", "url": "https://arxiv.org/pdf/2303.05063", "tokenized_text": "large_language large language llms gpt-3 chatgpt demonstrated_remarkable demonstrated remarkable results natural_language natural language processing nlp tasks context_learning context learning involves inference based demonstration examples despite successes nlp_tasks nlp tasks investigation conducted assess ability llms perform document information_extraction information extraction context_learning context learning applying llms poses challenges modality task gap end propose simple effective context_learning context learning framework called icl enables llms perform different types demonstration examples specifically extract difficult distinct segments hard training documents hard demonstrations benefiting test instances design demonstrations describing relationships enable llms understand positional relationships introduce formatting demonstrations easy answer extraction additionally framework improves diverse demonstrations updating iteratively experiments widely benchmark_datasets benchmark datasets demonstrate icl framework enables davinci-003 chatgpt achieve superior_performance superior performance compared previous pre trained methods fine tuned training distribution id setting distribution ood setting code_is_available code available"}
{"id": "1a01c982aa20c1a1ad1ad94866e3197da99a52a2", "abstract": "Extractive summarization is a crucial task in natural language processing that aims to condense long documents into shorter versions by directly extracting sentences. The recent introduction of large language models has attracted significant interest in the NLP community due to its remarkable performance on a wide range of downstream tasks. This paper first presents a thorough evaluation of ChatGPT's performance on extractive summarization and compares it with traditional fine-tuning methods on various benchmark datasets. Our experimental analysis reveals that ChatGPT exhibits inferior extractive summarization performance in terms of ROUGE scores compared to existing supervised systems, while achieving higher performance based on LLM-based evaluation metrics. In addition, we explore the effectiveness of in-context learning and chain-of-thought reasoning for enhancing its performance. Furthermore, we find that applying an extract-then-generate pipeline with ChatGPT yields significant performance improvements over abstractive baselines in terms of summary faithfulness. These observations highlight potential directions for enhancing ChatGPT's capabilities in faithful summarization using two-stage approaches.", "title": "extractive summarization via chatgpt for faithful summary generation", "url": "https://arxiv.org/pdf/2304.04193", "tokenized_text": "extractive summarization crucial task natural_language natural language processing aims condense long documents shorter versions directly extracting sentences recent introduction large_language large language attracted significant interest nlp community remarkable performance wide_range wide range downstream_tasks downstream tasks paper presents thorough evaluation chatgpt performance extractive summarization compares traditional fine tuning methods benchmark_datasets benchmark datasets experimental analysis reveals chatgpt exhibits inferior extractive summarization performance terms rouge scores compared existing supervised systems achieving higher performance based llm based evaluation metrics addition explore effectiveness context_learning context learning chain thought reasoning enhancing performance furthermore find applying extract generate pipeline chatgpt yields significant performance improvements abstractive baselines terms summary faithfulness observations highlight potential directions enhancing chatgpt capabilities faithful summarization stage approaches"}
{"id": "1a55d16c14587edda62dc9c9ff09e0b531dd169c", "abstract": "This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at \\url{https://github.com/lifan-yuan/OOD_NLP}.", "title": "revisiting outofdistribution robustness in nlp benchmark, analysis, and llms evaluations", "url": "http://arxiv.org/pdf/2306.04618", "tokenized_text": "paper research distribution ood robustness field nlp find distribution shift settings previous studies commonly lack adequate challenges hindering accurate evaluation ood robustness address issues propose benchmark construction protocol ensures clear challenging distribution shifts introduce benchmark suite distribution robustness evaluation covering tasks 20 datasets based conduct series experiments pre trained_language trained language analysis evaluation ood robustness vanilla fine tuning examine relationship distribution id ood performance identify typical types unveil inner learning mechanism potentially facilitate forecasting ood robustness advancements id datasets evaluate classic methods find despite exhibiting effectiveness specific cases offer significant improvement compared vanilla fine tuning evaluate llms adaptation paradigms find sufficient id data available fine tuning domain specific outperform llms id examples significantly case ood instances llms context_learning context learning yields better results identify fine tuned small llms face challenges effectively addressing downstream_tasks downstream tasks code public \\url{https://github.com"}
{"id": "1a62bc8ed9732bcdb6893a11f5cf239640883f87", "abstract": "Most existing retrieval-augmented language models (LMs) for question answering assume all retrieved information is factually correct. In this work, we study a more realistic scenario in which retrieved documents may contain misinformation, causing conflicts among them. We observe that the existing models are highly brittle to such information in both fine-tuning and in-context few-shot learning settings. We propose approaches to make retrieval-augmented LMs robust to misinformation by explicitly fine-tuning a discriminator or prompting to elicit discrimination capability in GPT-3. Our empirical results on open-domain question answering show that these approaches significantly improve LMs' robustness to knowledge conflicts. We also provide our findings on interleaving the fine-tuned model's decision with the in-context learning process, paving a new path to leverage the best of both worlds.", "title": "discern and answer mitigating the impact of misinformation in retrievalaugmented models with discriminators", "url": "http://arxiv.org/pdf/2305.01579", "tokenized_text": "existing retrieval augmented language_models language lms question_answering question answering assume retrieved information factually correct work study realistic scenario retrieved documents contain misinformation causing conflicts observe existing highly brittle information fine tuning context shot_learning shot learning settings propose approaches retrieval augmented lms robust misinformation explicitly fine tuning discriminator elicit capability gpt-3 empirical results open domain question_answering question answering approaches significantly improve lms robustness knowledge conflicts provide findings interleaving fine tuned decision context_learning context learning process paving new path leverage best worlds"}
{"id": "1abfc211793c683972ded8d3268475e3ee7a88b0", "abstract": "With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs. As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it. To achieve it, we propose the transferable version of advICL, named Transferable-advICL. Our experiment shows that the adversarial demonstration generated by Transferable-advICL can successfully attack the unseen test input examples. We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.", "title": "adversarial demonstration attacks on large language models", "url": "http://arxiv.org/pdf/2305.14950", "tokenized_text": "emergence powerful large_language large language llms chatgpt gpt-4 context_learning context learning icl gained significant prominence leveraging specific tasks utilizing data label pairs precondition incorporating demonstrations greatly enhance performance llms tasks introduce new security concern attackers manipulate demonstrations changing input perform attack paper investigate security concern icl adversarial perspective focusing impact demonstrations propose_a_novel propose novel attack method named aims manipulate demonstration changing input mislead results_demonstrate results demonstrate number demonstrations increases robustness context_learning context learning decrease additionally identify intrinsic property demonstrations prepended different inputs result introduces practical threat attacker attack test input example manipulating achieve propose transferable version named transferable experiment shows adversarial demonstration generated transferable successfully attack unseen test input examples hope study reveals critical security risks associated icl underscores need extensive research robustness icl particularly given increasing significance advancement llms"}
{"id": "1b9fc8268b392742ea43c2c017a767cf62386139", "abstract": "Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal explainer. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Thought (CoT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit causality rather than implicit causality, and performs better in sentences with lower event density and smaller lexical distance between events. The code is available on https://github.com/ArrogantL/ChatGPT4CausalReasoning .", "title": "is chatgpt a good causal reasoner a comprehensive evaluation", "url": "https://arxiv.org/pdf/2305.07375", "tokenized_text": "causal reasoning ability crucial numerous nlp applications despite impressive emerging ability chatgpt nlp_tasks nlp tasks unclear chatgpt performs causal reasoning paper conduct comprehensive evaluation chatgpt causal reasoning capabilities experiments chatgpt good causal reasoner good causal chatgpt hallucination causal reasoning possibly reporting biases causal non causal relationships natural_language natural language chatgpt processes rlhf context_learning context learning icl chain thought cot techniques causal hallucination additionally causal reasoning ability chatgpt sensitive words express causal concept close ended perform better open ended events sentences chatgpt excels capturing explicit causality implicit causality performs better sentences lower event density smaller lexical distance events code_is_available code available"}
{"id": "1d75f8de31bf47ec46fa5586056420ec8bc97e86", "abstract": "While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, which often perpetuates social biases or stereotypes. We investigate a retrieval-based method for reducing bias and toxicity in responses from chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. We find our method performs competitively with strong baselines without requiring training. For instance, using automatic evaluation, we find our best fine-tuned baseline only generates safe responses to unsafe dialogue contexts from DiaSafety 4.04% more than our approach. Finally, we also propose a re-ranking procedure which can further improve response safeness.", "title": "using incontext learning to improve dialogue safety", "url": "http://arxiv.org/pdf/2302.00871", "tokenized_text": "large neural based conversational increasingly proficient dialogue agents recent_work recent work highlighted safety issues systems example systems generating toxic content social biases stereotypes investigate retrieval based method reducing bias toxicity responses chatbots uses context_learning context learning steer safer generations concretely generate response unsafe dialogue context retrieve demonstrations safe responses similar dialogue contexts find method performs competitively strong baselines requiring training instance automatic evaluation find best fine tuned baseline generates safe responses unsafe dialogue contexts approach finally propose ranking procedure improve response"}
{"id": "1ddeb500dd88d4b860b32bec1e2a85f8a53910d6", "abstract": "Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in https://github.com/zjunlp/DeepKE/tree/main/example/llm.", "title": "how to unleash the power of large language models for fewshot relation extraction", "url": "http://arxiv.org/pdf/2305.01555", "tokenized_text": "scaling language_models language revolutionized widespread nlp_tasks nlp tasks little comprehensively explored shot relation_extraction relation extraction large_language large language paper investigate methodologies context_learning context learning data generation shot relation_extraction relation extraction gpt-3.5 exhaustive experiments enhance shot performance propose task related instructions schema constrained data generation observe context_learning context learning achieve performance par previous learning approaches data generation large_language large language boost previous solutions obtain new state art shot results widely studied relation_extraction relation extraction datasets hope work inspire future_research future research capabilities large_language large language shot relation_extraction relation extraction code_is_available code available"}
{"id": "1fb5a5298747b8c7d60f98640a543f20d42ab053", "abstract": "In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of ICL for cross-lingual text classification. We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy \u2014 Cross-lingual In-context Source Target Alignment (X-InSTA). With an injected coherence in the semantics of the input examples and a task-based alignment across the source and target languages, X-InSTA is able to outperform random prompt selection by a large margin across three different tasks using 44 different cross-lingual pairs.", "title": "multilingual llms are better crosslingual incontext learners with alignment", "url": "http://arxiv.org/pdf/2305.05940", "tokenized_text": "context_learning context learning icl large_language large language capable inferring test labels conditioned labeled samples gradient update icl enabled large_language large language provide promising step forward bypassing recurrent annotation costs low resource setting handful past studies explored icl cross lingual setting need transferring label knowledge high resource language low resource immensely crucial bridge gap provide depth analysis icl cross lingual text_classification text classification find prevalent mode selecting random input label pairs construct context severely limited case cross lingual icl primarily lack alignment input output spaces mitigate propose_a_novel propose novel construction strategy cross lingual context source target alignment injected coherence semantics input examples task based alignment source target languages able outperform random selection large margin different tasks 44 different cross lingual pairs"}
{"id": "20177a85f632a34d085bcf645507e461733fcc96", "abstract": "In-Context Learning (ICL) over Large language models (LLMs) aims at solving previously unseen tasks by conditioning on a few training examples, eliminating the need for parameter updates and achieving competitive performance. In this paper, we demonstrate that factual knowledge is imperative for the performance of ICL in three core facets, i.e., the inherent knowledge learned in LLMs, the factual knowledge derived from the selected in-context examples, and the knowledge biases in LLMs for output generation. To unleash the power of LLMs in few-shot learning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further improve the performance of ICL: 1) injecting factual knowledge to LLMs during continual self-supervised pre-training, 2) judiciously selecting the examples with high knowledge relevance, and 3) calibrating the prediction results based on prior knowledge. We evaluate the proposed approaches on auto-regressive LLMs (e.g., GPT-style models) over multiple text classification and question answering tasks. Experimental results demonstrate that KICT substantially outperforms strong baselines, and improves by more than 13% and 7% of accuracy on text classification and question answering tasks, respectively.", "title": "boosting incontext learning with factual knowledge", "url": "https://arxiv.org/pdf/2309.14771", "tokenized_text": "context_learning context learning icl large_language large language llms aims solving previously unseen tasks conditioning training_examples training examples eliminating need parameter updates achieving competitive_performance competitive performance paper demonstrate factual knowledge imperative performance icl core facets i.e. inherent knowledge learned llms factual knowledge derived selected context_examples context examples knowledge biases llms output generation unleash power llms shot_learning shot learning scenarios introduce novel knowledgeable context tuning framework improve performance icl injecting factual knowledge llms continual self supervised pre training judiciously selecting examples high knowledge relevance prediction results based prior knowledge evaluate proposed approaches auto regressive llms e.g. gpt style multiple text_classification text classification question_answering question answering tasks experimental_results experimental results demonstrate substantially outperforms strong baselines improves 13 accuracy text_classification text classification question_answering question answering tasks respectively"}
{"id": "214fbadc57e954e325dc055fee5ac0e224dfde11", "abstract": "Recent research on dialog state tracking (DST) focuses on methods that allow few- and zero-shot transfer to new domains or schemas. However, performance gains heavily depend on aggressive data augmentation and fine-tuning of ever larger language model based architectures. In contrast, general purpose language models, trained on large amounts of diverse data, hold the promise of solving any kind of task without task-specific training. We present preliminary experimental results on the ChatGPT research preview, showing that ChatGPT achieves state-of-the-art performance in zero-shot DST. Despite our findings, we argue that properties inherent to general purpose models limit their ability to replace specialized systems. We further theorize that the in-context learning capabilities of such models will likely become powerful tools to support the development of dedicated dialog state trackers and enable dynamic methods.", "title": "chatgpt for zeroshot dialogue state tracking a solution or an opportunity", "url": "http://arxiv.org/pdf/2306.01386", "tokenized_text": "recent research dialog state tracking dst focuses methods allow few- zero shot transfer new domains schemas performance gains heavily depend aggressive data_augmentation data augmentation fine tuning larger language_model language based architectures contrast general_purpose general purpose language_models language trained large amounts diverse data hold promise solving kind task task specific training present preliminary experimental_results experimental results chatgpt research showing chatgpt achieves_state achieves state art performance zero shot dst despite findings argue properties inherent general_purpose general purpose limit ability replace specialized systems context_learning context learning capabilities likely powerful tools support development dedicated dialog state enable dynamic methods"}
{"id": "2392b6d3a5cad9e5cf349169eaeee848266adf6a", "abstract": "Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss. Our code is available at https://aka.ms/LLMLingua.", "title": "llmlingua compressing prompts for accelerated inference of large language models", "url": "https://arxiv.org/pdf/2310.05736", "tokenized_text": "large_language large language llms applied applications astonishing capabilities advancements technologies chain thought cot context_learning context learning icl fed llms increasingly lengthy exceeding tens thousands tokens accelerate inference reduce cost paper_presents paper presents coarse fine compression method involves budget controller maintain semantic integrity high compression token level iterative compression algorithm better compressed contents instruction_tuning instruction tuning based method distribution alignment language_models language conduct experiments analysis datasets different scenarios i.e. gsm8 bbh arxiv showing proposed approach yields state art performance allows 20x compression little performance loss code_is_available code available https://aka.ms/llmlingua"}
{"id": "2447d22655803bfacb880f117cc34d2ac5ac7e74", "abstract": "Large pre-trained language models (PLMs) have made significant progress in encoding world knowledge and spawned a new set of learning paradigms including zero-shot, few-shot, and in-context learning. Many language tasks can be modeled as a set of prompts (for example, is this text about geography?) and language models can provide binary answers, i.e., Yes or No. There is evidence to suggest that the next-word prediction used by many PLMs does not align well with zero-shot paradigms. Therefore, PLMs are fine-tuned as a question-answering system. In-context learning extends zero-shot learning by incorporating prompts and examples, resulting in increased task accuracy. Our paper presents EXnet, a model specifically designed to perform in-context learning without any limitations on the number of examples. We argue that in-context learning is an effective method to increase task accuracy, and providing examples facilitates cross-task generalization, especially when it comes to text classification tasks. With extensive experiments, we show that even our smallest model (15M parameters) generalizes to several unseen classification tasks and domains.", "title": "exnet efficient incontext learning for dataless text classification", "url": "http://arxiv.org/pdf/2305.14622", "tokenized_text": "large pre trained_language trained language plms significant progress encoding world knowledge new set learning paradigms including zero shot shot context_learning context learning language tasks modeled set example text language_models language provide binary answers i.e. yes evidence suggest word prediction plms align zero shot paradigms plms fine tuned question answering system context_learning context learning extends zero shot_learning shot learning incorporating examples resulting increased task accuracy paper_presents paper presents specifically designed perform context_learning context learning limitations number examples argue context_learning context learning effective method increase task accuracy providing examples facilitates cross task generalization especially comes text_classification text classification tasks extensive_experiments extensive experiments smallest 15 parameters generalizes unseen classification tasks domains"}
{"id": "24df244bf7a6e8c93c5f183d3f62d39c0f773c68", "abstract": "Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principle-fOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RL-trained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.", "title": "salmon selfalignment with principlefollowing reward models", "url": "https://arxiv.org/pdf/2310.05910", "tokenized_text": "supervised fine-tuning response demonstrations combined reinforcement_learning reinforcement learning human feedback rlhf constitutes powerful paradigm aligning llm based ai agents significant limitation approach dependency high quality human annotations making application intricate tasks challenging difficulties obtaining consistent response demonstrations distribution response preferences paper_presents paper presents novel_approach novel approach self alignment principle following reward align base language_models language minimal human supervision small set human defined principles achieving superior_performance superior performance central approach principle following reward trained synthetic preference data generate reward scores based arbitrary human defined principles merely adjusting principles rl training phase gain control preferences reward subsequently influencing behavior rl trained policies eliminating reliance collection online human preferences applying method llama-2 70b base language_model language developed ai assistant named exemplars context_learning context learning 31 human defined principles significantly surpasses performance state art ai systems including llama-2 benchmark_datasets benchmark datasets open sourced code weights encourage research aligning llm based ai agents enhanced supervision efficiency improved controllability scalable oversight"}
{"id": "65d88194a902332b78dd5a7b919fa577bfa7ee9f", "abstract": "Implicit event argument extraction (EAE) aims to identify arguments that could scatter over the document. Most previous work focuses on learning the direct relations between arguments and the given trigger, while the implicit relations with long-range dependency are not well studied. Moreover, recent neural network based approaches rely on a large amount of labeled data for training, which is unavailable due to the high labelling cost. In this paper, we propose a Curriculum learning based Prompt tuning (CUP) approach, which resolves implicit EAE by four learning stages. The stages are defined according to the relations with the trigger node in a semantic graph, which well captures the long-range dependency between arguments and the trigger. In addition, we integrate a prompt-based encoder-decoder model to elicit related knowledge from pre-trained language models (PLMs) in each stage, where the prompt templates are adapted with the learning progress to enhance the reasoning for arguments. Experimental results on two well-known benchmark datasets show the great advantages of our proposed approach. In particular, we outperform the state-of-the-art models in both fully-supervised and low-data scenarios.", "title": "cup curriculum learning based prompt tuning for implicit event argument extraction", "url": "https://arxiv.org/pdf/2205.00498", "tokenized_text": "implicit event argument extraction eae aims identify arguments document previous work focuses learning direct relations arguments given trigger implicit relations long range dependency studied recent neural network based approaches rely large labeled_data labeled data training unavailable high labelling cost paper propose curriculum learning based tuning approach resolves implicit eae learning stages stages defined according relations trigger node semantic graph captures long range dependency arguments trigger addition integrate based encoder decoder elicit related knowledge pre trained_language trained language plms stage prompt_templates templates adapted learning progress enhance reasoning arguments experimental_results experimental results known benchmark_datasets benchmark datasets great advantages proposed approach particular outperform state art fully supervised low data scenarios"}
{"id": "11e3efa08b5db1a8958dfe8119593a4d3f18796a", "abstract": "Fine-grained visual classification (FGVC) involves categorizing fine subdivisions within a broader category, which poses challenges due to subtle inter-class discrepancies and large intra-class variations. However, prevailing approaches primarily focus on uni-modal visual concepts. Recent advancements in pre-trained vision-language models have demonstrated remarkable performance in various high-level vision tasks, yet the applicability of such models to FGVC tasks remains uncertain. In this paper, we aim to fully exploit the capabilities of cross-modal description to tackle FGVC tasks and propose a novel multimodal prompting solution, denoted as MP-FGVC, based on the contrastive language-image pertaining (CLIP) model. Our MP-FGVC comprises a multimodal prompts scheme and a multimodal adaptation scheme. The former includes Subcategory-specific Vision Prompt (SsVP) and Discrepancy-aware Text Prompt (DaTP), which explicitly highlights the subcategory-specific discrepancies from the perspectives of both vision and language. The latter aligns the vision and text prompting elements in a common semantic space, facilitating cross-modal collaborative reasoning through a Vision-Language Fusion Module (VLFM) for further improvement on FGVC. Moreover, we tailor a two-stage optimization strategy for MP-FGVC to fully leverage the pre-trained CLIP model and expedite efficient adaptation for FGVC. Extensive experiments conducted on four FGVC datasets demonstrate the effectiveness of our MP-FGVC.", "title": "delving into multimodal prompting for finegrained visual classification", "url": "https://arxiv.org/pdf/2309.08912", "tokenized_text": "fine grained visual classification involves categorizing fine broader category poses challenges subtle inter class discrepancies large intra class variations prevailing approaches primarily focus uni modal visual concepts recent advancements pre trained vision language_models language demonstrated_remarkable demonstrated remarkable performance high level vision tasks applicability tasks remains uncertain paper aim fully exploit capabilities cross modal description tackle tasks propose_a_novel propose novel multimodal solution denoted based contrastive language image pertaining clip comprises multimodal scheme multimodal adaptation scheme includes specific vision discrepancy aware text explicitly highlights specific discrepancies perspectives vision language aligns vision text elements common semantic space facilitating cross modal collaborative reasoning fusion module improvement tailor stage optimization strategy fully leverage pre trained clip expedite efficient adaptation extensive_experiments extensive experiments conducted datasets demonstrate_the_effectiveness demonstrate effectiveness"}
{"id": "159d2980566fa00bc752e180471ee46d7899d66e", "abstract": "Digital art synthesis is receiving increasing attention in the multimedia community because of engaging the public with art effectively. Current digital art synthesis methods usually use single-modality inputs as guidance, thereby limiting the expressiveness of the model and the diversity of generated results. To solve this problem, we propose the multimodal guided artwork diffusion (MGAD) model, which is a diffusion-based digital artwork generation approach that utilizes multimodal prompts as guidance to control the classifier-free diffusion model. Additionally, the contrastive language-image pretraining (CLIP) model is used to unify text and image modalities. Extensive experimental results on the quality and quantity of the generated digital art paintings confirm the effectiveness of the combination of the diffusion model and multimodal guidance. Code is available at https://github.com/haha-lisa/MGAD-multimodal-guided-artwork-diffusion.", "title": "draw your art dream diverse digital art synthesis with multimodal guided diffusion", "url": "https://dl.acm.org/doi/pdf/10.1145/3503161.3548282", "tokenized_text": "digital art synthesis receiving increasing attention multimedia community engaging public art effectively current digital art synthesis methods usually use single modality inputs guidance limiting expressiveness diversity generated results solve problem propose multimodal guided artwork diffusion diffusion based digital artwork generation approach utilizes multimodal guidance control classifier free diffusion additionally contrastive language image pretraining clip unify text image modalities extensive experimental_results experimental results quality quantity generated digital art confirm effectiveness combination diffusion multimodal guidance code_is_available code available"}
{"id": "185e79641a8e7b18ac5a73b8c3cb82fdee3a0c6d", "abstract": "Recent vision-language models are driven by large-scale pretrained models. However, adapting pretrained models on limited data presents challenges such as overfitting, catastrophic forgetting, and the cross-modal gap between vision and language. We introduce a parameter-efficient method to address these challenges, combining multimodal prompt learning and a transformer-based mapping network, while keeping the pretrained models frozen. Our experiments on several video question answering benchmarks demonstrate the superiority of our approach in terms of performance and parameter efficiency on both zero-shot and few-shot settings. Our code is available at https://engindeniz.github.io/vitis.", "title": "zeroshot and fewshot video question answering with multimodal prompts", "url": "https://arxiv.org/pdf/2309.15915", "tokenized_text": "recent vision language_models language driven large scale pretrained adapting pretrained limited data presents challenges overfitting catastrophic forgetting cross modal gap vision language introduce parameter efficient method address challenges combining multimodal learning transformer based mapping network keeping pretrained frozen experiments video question_answering question answering benchmarks demonstrate superiority approach terms performance parameter efficiency zero shot shot_settings shot settings code_is_available code available"}
{"id": "25425e299101b13ec2872417a14f961f4f8aa18e", "abstract": "Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieves strong model scalability and data efficiency. It outperforms alternative designs in the hardest zero-shot generalization setting by up to $2.9\\times$ task success rate given the same training data. With $10\\times$ less training data, VIMA still performs $2.7\\times$ better than the best competing variant. Code and video demos are available at https://vimalabs.github.io/", "title": "vima general robot manipulation with multimodal prompts", "url": "http://arxiv.org/pdf/2210.03094", "tokenized_text": "based learning emerged successful paradigm natural_language natural language processing single general purpose language_model language instructed perform task specified input task specification robotics comes forms imitating shot demonstrations following language instructions reaching visual goals considered different tasks tackled specialized wide spectrum robot manipulation tasks expressed multimodal interleaving textual visual tokens accordingly develop new simulation benchmark consists thousands generated tabletop tasks multimodal expert trajectories imitation learning level evaluation protocol systematic generalization design transformer based robot agent vima processes outputs motor actions autoregressively vima features recipe achieves strong scalability data efficiency outperforms alternative designs hardest zero shot generalization setting task success_rate success rate given training_data training data training_data training data vima performs better best competing variant code video demos available"}
{"id": "37d91ebd5ec969e2b81027e05f886febf09d2504", "abstract": "Generating an informative and attractive title for the product is a crucial task for e-commerce. Most existing works follow the standard multimodal natural language generation approaches, e.g., image captioning, and employ the large scale of human-labelled datasets to train desirable models. However, for novel products, especially in a different domain, there are few existing labelled data. In this paper, we propose a prompt-based approach, i.e., the Multimodal Prompt Learning framework, to accurately and efficiently generate titles for novel products with limited labels. We observe that the core challenges of novel product title generation are the understanding of novel product characteristics and the generation of titles in a novel writing style. To this end, we build a set of multimodal prompts from different modalities to preserve the corresponding characteristics and writing styles of novel products. As a result, with extremely limited labels for training, the proposed method can retrieve the multimodal prompts to generate desirable titles for novel products. The experiments and analyses are conducted on five novel product categories under both the in-domain and out-of-domain experimental settings. The results show that, with only 1% of downstream labelled data for training, our proposed approach achieves the best few-shot results and even achieves competitive results with fully-supervised methods trained on 100% of training data; With the full labelled data for training, our method achieves state-of-the-art results.", "title": "multimodal prompt learning for product title generation with extremely limited labels", "url": "https://arxiv.org/pdf/2307.01969", "tokenized_text": "generating informative attractive title product crucial task commerce existing works follow standard multimodal natural_language natural language generation approaches e.g. image captioning employ large_scale large scale human labelled datasets train desirable novel products especially different domain existing labelled data paper propose based approach i.e. multimodal learning framework accurately efficiently generate titles novel products limited labels observe core challenges novel product title generation understanding novel product characteristics generation titles novel writing style end build set multimodal different modalities preserve corresponding characteristics writing styles novel products result extremely limited labels training proposed_method proposed method retrieve multimodal generate desirable titles novel products experiments analyses conducted novel product categories domain domain experimental settings results downstream labelled data training proposed approach achieves best shot results achieves competitive results fully supervised methods trained 100 training_data training data labelled data training method_achieves method achieves state art results"}
{"id": "483757dff12df441c6991dd5e7408d922fe01c3d", "abstract": "In this paper, we tackle two challenges in multimodal learning for visual recognition: 1) when missing-modality occurs either during training or testing in real-world situations; and 2) when the computation resources are not available to finetune on heavy transformer models. To this end, we propose to utilize prompt learning and mitigate the above two challenges together. Specifically, our modality-missing-aware prompts can be plugged into multimodal transformers to handle general missing-modality cases, while only requiring less than 1% learnable parameters compared to training the entire model. We further explore the effect of different prompt configurations and analyze the robustness to missing modality. Extensive experiments are conducted to show the effectiveness of our prompt learning framework that improves the performance under various missing-modality cases, while alleviating the requirement of heavy model retraining. Code is available.11https://github.com/YiLunLee/missing_aware_prompts", "title": "multimodal prompting with missing modalities for visual recognition", "url": "https://arxiv.org/pdf/2303.03369", "tokenized_text": "paper tackle challenges multimodal learning visual recognition missing modality occurs training testing real world situations computation resources available finetune heavy transformer end propose utilize learning mitigate challenges specifically modality missing aware multimodal transformers handle general missing modality cases requiring learnable parameters compared training entire explore effect different configurations analyze robustness missing modality extensive_experiments extensive experiments conducted effectiveness learning framework improves performance missing modality cases alleviating requirement heavy retraining code"}
{"id": "534675abb9d72fc0c08d080d4f73335ceb75902c", "abstract": "Recent years have witnessed impressive results of pre-trained vision-language models on knowledge-intensive tasks such as visual question answering (VQA). Despite the recent advances in VQA, existing methods mainly adopt a discriminative formulation that predicts answers within a pre-defined label set, leading to easy overfitting on low-resource domains with limited labeled data (e.g., medicine) and poor generalization under domain shift to another dataset. To tackle this limitation, we propose a novel generative model enhanced by multimodal prompt retrieval (MPR) that integrates retrieved prompts and multimodal features to generate answers in free text. Our generative model enables rapid zero-shot dataset adaptation to unseen data distributions and open-set answer labels across datasets. Our experiments on medical VQA tasks show that MPR outperforms its non-retrieval counterpart by up to 30% accuracy points in a few-shot domain adaptation setting.", "title": "multimodal prompt retrieval for generative visual question answering", "url": "http://arxiv.org/pdf/2306.17675", "tokenized_text": "recent_years recent years witnessed impressive results pre trained vision language_models language knowledge intensive tasks visual question_answering question answering vqa despite recent_advances recent advances vqa existing_methods existing methods mainly adopt discriminative formulation predicts answers pre defined label set leading easy overfitting low resource domains limited labeled_data labeled data e.g. medicine poor generalization domain shift dataset tackle limitation propose_a_novel propose novel generative enhanced multimodal retrieval integrates retrieved multimodal features generate answers free text generative enables rapid zero shot dataset adaptation unseen data distributions open set answer labels datasets experiments medical vqa tasks outperforms non retrieval counterpart 30 accuracy points shot domain adaptation setting"}
{"id": "6c925427841ea4a776a578d438f9e47a64c3014e", "abstract": "Fashion illustration is used by designers to communicate their vision and to bring the design idea from conceptualization to realization, showing how clothes interact with the human body. In this context, computer vision can thus be used to improve the fashion design process. Differently from previous works that mainly focused on the virtual try-on of garments, we propose the task of multimodal-conditioned fashion image editing, guiding the generation of human-centric fashion images by following multimodal prompts, such as text, human body poses, and garment sketches. We tackle this problem by proposing a new architecture based on latent diffusion models, an approach that has not been used before in the fashion domain. Given the lack of existing datasets suitable for the task, we also extend two existing fashion datasets, namely Dress Code and VITON-HD, with multimodal annotations collected in a semi-automatic manner. Experimental results on these new datasets demonstrate the effectiveness of our proposal, both in terms of realism and coherence with the given multimodal inputs. Source code and collected multimodal annotations are publicly available at: https://github.com/aimagelab/multimodal-garment-designer.", "title": "multimodal garment designer humancentric latent diffusion models for fashion image editing", "url": "https://arxiv.org/pdf/2304.02051", "tokenized_text": "fashion illustration designers communicate vision bring design idea realization showing interact human body context computer_vision computer vision improve fashion design process differently previous works mainly focused virtual try propose task multimodal conditioned fashion image editing guiding generation human centric fashion images following multimodal text human body poses sketches tackle problem proposing new architecture based latent diffusion approach fashion domain given lack existing datasets suitable task extend existing fashion datasets code multimodal annotations collected semi automatic manner experimental_results experimental results new datasets demonstrate_the_effectiveness demonstrate effectiveness proposal terms realism coherence given multimodal inputs source_code source code collected multimodal annotations publicly_available publicly available"}
{"id": "8b5f4b383008bfb365cee72e5301ee04a24221f7", "abstract": "Adopting contrastive image-text pretrained models like CLIP towards video classification has gained attention due to its cost-effectiveness and competitive performance. However, recent works in this area face a trade-off. Finetuning the pretrained model to achieve strong supervised performance results in low zero-shot generalization. Similarly, freezing the backbone to retain zero-shot capability causes significant drop in supervised accuracy. Because of this, recent works in literature typically train separate models for supervised and zero-shot action recognition. In this work, we propose a multimodal prompt learning scheme that works to balance the supervised and zero-shot performance under a single unified training. Our prompting approach on the vision side caters for three aspects: 1) Global video-level prompts to model the data distribution; 2) Local frame-level prompts to provide per-frame discriminative conditioning; and 3) a summary prompt to extract a condensed video representation. Additionally, we define a prompting scheme on the text side to augment the textual context. Through this prompting scheme, we can achieve state-of-the-art zero-shot performance on Kinetics-600, HMDB51 and UCF101 while remaining competitive in the supervised setting. By keeping the pretrained backbone frozen, we optimize a much lower number of parameters and retain the existing general representation which helps achieve the strong zero-shot performance. Our codes/models will be released at https://github.com/TalalWasim/Vita-Clip..", "title": "vitaclip video and text adaptive clip via multimodal prompting", "url": "https://arxiv.org/pdf/2304.03307", "tokenized_text": "adopting contrastive image text pretrained like clip video classification gained attention cost effectiveness competitive_performance competitive performance recent works area face trade finetuning pretrained achieve strong supervised performance results low zero shot generalization similarly freezing backbone retain zero shot capability causes significant drop supervised accuracy recent works literature typically train separate supervised zero shot action recognition work propose multimodal learning scheme works balance supervised zero shot performance single unified training approach vision caters aspects global video level data distribution local frame level provide frame discriminative conditioning summary extract condensed video representation additionally define scheme text augment textual context scheme achieve state art zero shot performance remaining competitive supervised setting keeping pretrained backbone frozen optimize lower number parameters retain existing general representation helps achieve strong zero shot performance codes released"}
{"id": "93565fe6db3948c9c414af1d1edccf4aff5e2e10", "abstract": "While interacting in the world is a multi-sensory experience, many robots continue to predominantly rely on visual perception to map and navigate in their environments. In this work, we propose Audio-Visual-Language Maps (AVLMaps), a unified 3D spatial map representation for storing cross-modal information from audio, visual, and language cues. AVLMaps integrate the open-vocabulary capabilities of multimodal foundation models pre-trained on Internet-scale data by fusing their features into a centralized 3D voxel grid. In the context of navigation, we show that AVLMaps enable robot systems to index goals in the map based on multimodal queries, e.g., textual descriptions, images, or audio snippets of landmarks. In particular, the addition of audio information enables robots to more reliably disambiguate goal locations. Extensive experiments in simulation show that AVLMaps enable zero-shot multimodal goal navigation from multimodal prompts and provide 50% better recall in ambiguous scenarios. These capabilities extend to mobile robots in the real world - navigating to landmarks referring to visual, audio, and spatial concepts. Videos and code are available at: https://avlmaps.github.io.", "title": "audio visual language maps for robot navigation", "url": "http://arxiv.org/pdf/2303.07522", "tokenized_text": "interacting world multi experience robots continue predominantly rely visual perception map navigate environments work propose maps unified 3d spatial map representation storing cross modal information audio visual language cues integrate open vocabulary capabilities multimodal foundation_models foundation pre trained internet scale data fusing features centralized 3d grid context navigation enable robot systems index goals map based multimodal queries e.g. textual descriptions images audio snippets particular addition audio information enables robots reliably goal locations extensive_experiments extensive experiments simulation enable zero shot multimodal goal navigation multimodal provide 50 better recall ambiguous scenarios capabilities extend mobile robots real_world real world navigating referring visual audio spatial concepts videos code available"}
{"id": "9dbb39eccbcd31b8f6b4ff0a2c96f61a7c34e54b", "abstract": "Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities. This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions. To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation. We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs actions containing the movement and position transitions. We set four natural language understanding tasks with progressive reasoning levels and evaluate the robot's ability to understand natural language instructions in two modes of adsorption and grasping. In addition, we also conduct a comprehensive analysis and comparison of the differences and advantages of 10 different LLMs in instruction understanding and generation quality. We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation. Project website: https://necolizer.github.io/RM-PRT/ .", "title": "rmprt realistic robotic manipulation simulator and benchmark with progressive reasoning tasks", "url": "http://arxiv.org/pdf/2306.11335", "tokenized_text": "recently advent pre trained large scale language_models language llms like_chatgpt like chatgpt gpt-4 significantly advanced machine natural_language natural language understanding capabilities breakthrough allowed seamlessly integrate open source llms unified robot simulator environment help robots accurately understand execute human natural_language natural language instructions end work introduce realistic robotic manipulation simulator build robotic manipulation progressive reasoning tasks rm benchmark basis specifically rm benchmark builds new high fidelity digital scene based engine includes categories 2023 objects 15 natural_language natural language instructions generated chatgpt detailed evaluation robot manipulation propose general pipeline rm benchmark takes input multimodal containing natural_language natural language instructions automatically outputs actions containing movement position transitions set natural_language natural language understanding tasks progressive reasoning levels evaluate robot ability understand natural_language natural language instructions modes grasping addition conduct comprehensive analysis comparison differences advantages 10 different llms instruction understanding generation quality hope new simulator benchmark facilitate future_research future research language guided robotic manipulation project website"}
{"id": "befcb92f313030632717a74a2afd651a1445a745", "abstract": "Multimodal sentiment analysis has gained significant attention due to the proliferation of multimodal content on social media. However, existing studies in this area rely heavily on large-scale supervised data, which is time-consuming and labor-intensive to collect. Thus, there is a need to address the challenge of few-shot multimodal sentiment analysis. To tackle this problem, we propose a novel method called Multimodal Probabilistic Fusion Prompts (MultiPoint) that leverages diverse cues from different modalities for multimodal sentiment detection in the few-shot scenario. Specifically, we start by introducing a Consistently Distributed Sampling approach called CDS, which ensures that the few-shot dataset has the same category distribution as the full dataset. Unlike previous approaches primarily using prompts based on the text modality, we design unified multimodal prompts to reduce discrepancies between different modalities and dynamically incorporate multimodal demonstrations into the context of each multimodal instance. To enhance the model's robustness, we introduce a probabilistic fusion method to fuse output predictions from multiple diverse prompts for each input. Our extensive experiments on six datasets demonstrate the effectiveness of our approach. First, our method outperforms strong baselines in the multimodal few-shot setting. Furthermore, under the same amount of data (1% of the full dataset), our CDS-based experimental results significantly outperform those based on previously sampled datasets constructed from the same number of instances of each class.", "title": "fewshot multimodal sentiment analysis based on multimodal probabilistic fusion prompts", "url": "https://dl.acm.org/doi/pdf/10.1145/3581783.3612181", "tokenized_text": "multimodal sentiment_analysis sentiment analysis gained significant attention proliferation multimodal content social_media social media existing studies area rely heavily large scale supervised data time consuming labor intensive collect need address challenge shot multimodal sentiment_analysis sentiment analysis tackle problem propose_a_novel propose novel method called multimodal probabilistic fusion leverages diverse cues different modalities multimodal sentiment detection shot scenario specifically start introducing consistently distributed sampling approach called ensures shot dataset category distribution dataset unlike previous approaches primarily based text modality design unified multimodal reduce discrepancies different modalities dynamically incorporate multimodal demonstrations context multimodal instance enhance robustness introduce probabilistic fusion method fuse output predictions multiple diverse input extensive_experiments extensive experiments datasets demonstrate_the_effectiveness demonstrate effectiveness approach method outperforms strong baselines multimodal shot_setting shot setting furthermore data dataset based experimental_results experimental results significantly outperform based previously sampled datasets constructed number instances class"}
{"id": "e4abc33cbb84934029af6d50360f7ad3bba3df3c", "abstract": "Emotion Recognition in Conversation (ERC) plays an important role in driving the development of human-machine interaction. Emotions can exist in multiple modalities, and multimodal ERC mainly faces two problems: (1) the noise problem in the cross-modal information fusion process, and (2) the prediction problem of less sample emotion labels that are semantically similar but different categories. To address these issues and fully utilize the features of each modality, we adopted the following strategies: first, deep emotion cues extraction was performed on modalities with strong representation ability, and feature filters were designed as multimodal prompt information for modalities with weak representation ability. Then, we designed a Multimodal Prompt Transformer (MPT) to perform cross-modal information fusion. MPT embeds multimodal fusion information into each attention layer of the Transformer, allowing prompt information to participate in encoding textual features and being fused with multi-level textual information to obtain better multimodal fusion features. Finally, we used the Hybrid Contrastive Learning (HCL) strategy to optimize the model's ability to handle labels with few samples. This strategy uses unsupervised contrastive learning to improve the representation ability of multimodal fusion and supervised contrastive learning to mine the information of labels with few samples. Experimental results show that our proposed model outperforms state-of-the-art models in ERC on two benchmark datasets.", "title": "multimodal prompt transformer with hybrid contrastive learning for emotion recognition in conversation", "url": "https://dl.acm.org/doi/pdf/10.1145/3581783.3611805", "tokenized_text": "emotion recognition conversation plays important role driving development human machine interaction emotions exist multiple modalities multimodal mainly faces problems noise problem cross modal information fusion process prediction problem sample emotion labels semantically similar different categories address issues fully utilize features modality adopted following strategies deep emotion cues extraction performed modalities strong representation ability feature filters designed multimodal information modalities weak representation ability designed multimodal transformer mpt perform cross modal information fusion mpt multimodal fusion information attention layer transformer allowing information encoding textual features fused multi level textual information obtain better multimodal fusion features finally hybrid contrastive_learning contrastive learning strategy optimize ability handle labels samples strategy uses unsupervised contrastive_learning contrastive learning improve representation ability multimodal fusion supervised contrastive_learning contrastive learning information labels samples experimental_results experimental results proposed outperforms state art benchmark_datasets benchmark datasets"}
{"id": "fd7082630257b03771c72a926a64b13eb16e00af", "abstract": "We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MABSA is tough. To alleviate the above issue, we perform three MABSA-related tasks with quite a small number of labeled multimodal samples. We first build diverse and comprehensive multimodal few-shot datasets according to the data distribution. To capture the specific prompt for each aspect term in a few-shot scenario, we propose a novel Generative Multimodal Prompt (GMP) model for MABSA, which includes the Multimodal Encoder module and the N-Stream Decoders module. We further introduce a subtask to predict the number of aspect terms in each instance to construct the multimodal prompt. Extensive experiments on two datasets demonstrate that our approach outperforms strong baselines on two MABSA-related tasks in the few-shot setting.", "title": "fewshot joint multimodal aspectsentiment analysis based on generative multimodal prompt", "url": "http://arxiv.org/pdf/2305.10169", "tokenized_text": "witnessed rapid proliferation multimodal data numerous social_media social media platforms conventional studies typically require massive labeled_data labeled data train multimodal sentiment_analysis sentiment analysis collecting annotating fine grained multimodal data alleviate issue perform related tasks small_number small number labeled multimodal samples build diverse comprehensive multimodal shot datasets according data distribution capture specific aspect term shot scenario propose_a_novel propose novel generative multimodal includes multimodal encoder module module introduce subtask predict number aspect terms instance construct multimodal extensive_experiments extensive experiments datasets demonstrate approach outperforms strong baselines related tasks shot_setting shot setting"}
{"id": "0213827d882ec34aa9935f2b03a80362af806778", "abstract": "Text-based person search (TBPS) aims to retrieve the images of the target person from a large image gallery based on a given natural language description. Existing methods are dominated by training models with parallel image-text pairs, which are very costly to collect. In this paper, we make the first attempt to explore TBPS without parallel image-text data (\u03bc-TBPS), in which only non-parallel images and texts, or even image-only data, can be adopted. Towards this end, we propose a two-stage framework, generation-then-retrieval (GTR), to first generate the corresponding pseudo text for each image and then perform the retrieval in a supervised manner. In the generation stage, we propose a fine-grained image captioning strategy to obtain an enriched description of the person image, which firstly utilizes a set of instruction prompts to activate the off-the-shelf pretrained vision-language model to capture and generate fine-grained person attributes, and then converts the extracted attributes into a textual description via the finetuned large language model or the hand-crafted template. In the retrieval stage, considering the noise interference of the generated texts for training model, we develop a confidence score-based training scheme by enabling more reliable texts to contribute more during the training. Experimental results on multiple TBPS benchmarks (i.e., CUHK-PEDES, ICFG-PEDES and RSTPReid) show that the proposed GTR can achieve a promising performance without relying on parallel image-text data.", "title": "textbased person search without parallel imagetext data", "url": "https://dl.acm.org/doi/pdf/10.1145/3581783.3612285", "tokenized_text": "text based person search aims retrieve images target person large image based given natural_language natural language description existing_methods existing methods dominated training parallel image text pairs costly collect paper attempt explore parallel image text data non parallel images texts image data adopted end propose stage framework generation retrieval generate corresponding pseudo text image perform retrieval supervised manner generation stage propose fine grained image captioning strategy obtain enriched description person image firstly utilizes set instruction activate shelf pretrained vision language_model language capture generate fine grained person attributes converts extracted attributes textual description finetuned large_language large language hand crafted template retrieval stage considering noise interference generated texts training develop confidence score based training scheme enabling reliable texts contribute training experimental_results experimental results multiple benchmarks i.e. proposed achieve promising performance relying parallel image text data"}
{"id": "1c89d8672a3742672850fa46f1e8ec51f3261019", "abstract": "Generative large language models (LLMs) with instruct training such as GPT-4 can follow human-provided instruction prompts and generate human-like responses to these prompts. Apart from natural language responses, they have also been found to be effective at generating formal artifacts such as code, plans, and logical specifications from natural language prompts. Despite their remarkably improved accuracy, these models are still known to produce factually incorrect or contextually inappropriate results despite their syntactic coherence - a phenomenon often referred to as hallucination. This limitation makes it difficult to use these models to synthesize formal artifacts that are used in safety-critical applications. Unlike tasks such as text summarization and question-answering, bugs in code, plan, and other formal artifacts produced by LLMs can be catastrophic. We posit that we can use the satisfiability modulo theory (SMT) solvers as deductive reasoning engines to analyze the generated solutions from the LLMs, produce counterexamples when the solutions are incorrect, and provide that feedback to the LLMs exploiting the dialog capability of instruct-trained LLMs. This interaction between inductive LLMs and deductive SMT solvers can iteratively steer the LLM to generate the correct response. In our experiments, we use planning over the domain of blocks as our synthesis task for evaluating our approach. We use GPT-4, GPT3.5 Turbo, Davinci, Curie, Babbage, and Ada as the LLMs and Z3 as the SMT solver. Our method allows the user to communicate the planning problem in natural language; even the formulation of queries to SMT solvers is automatically generated from natural language. Thus, the proposed technique can enable non-expert users to describe their problems in natural language, and the combination of LLMs and SMT solvers can produce provably correct solutions.", "title": "neuro symbolic reasoning for planning counterexample guided inductive synthesis using large language models and satisfiability solving", "url": "https://arxiv.org/pdf/2309.16436", "tokenized_text": "generative large_language large language llms instruct training gpt-4 follow human provided instruction generate human like responses apart natural_language natural language responses found effective generating formal artifacts code plans logical specifications natural_language natural language despite remarkably improved accuracy known produce factually incorrect contextually inappropriate results despite syntactic coherence phenomenon referred hallucination limitation makes difficult use synthesize formal artifacts safety critical applications unlike tasks text summarization question answering bugs code plan formal artifacts produced llms catastrophic posit use satisfiability modulo theory smt solvers deductive reasoning engines analyze generated solutions llms produce solutions incorrect provide feedback llms exploiting dialog capability instruct trained llms interaction inductive llms deductive smt solvers iteratively steer llm generate correct response experiments use planning domain blocks synthesis task evaluating approach use gpt-4 gpt3.5 turbo davinci ada llms smt solver method allows user communicate planning problem natural_language natural language formulation queries smt solvers automatically generated natural_language natural language proposed technique enable non expert users describe problems natural_language natural language combination llms smt solvers produce provably correct solutions"}
{"id": "1e25118f99e03ffecf79412b46dda8a2966752c8", "abstract": "Layout-aware pre-trained models has achieved significant progress on document image question answering. They introduce extra learnable modules into existing language models to capture layout information within document images from text bounding box coordinates obtained by OCR tools. However, extra modules necessitate pre-training on extensive document images. This prevents these methods from directly utilizing off-the-shelf instruction-tuning language foundation models, which have recently shown promising potential in zero-shot learning. Instead, in this paper, we find that instruction-tuning language models like Claude and ChatGPT can understand layout by spaces and line breaks. Based on this observation, we propose the LAyout and Task aware Instruction Prompt (LATIN-Prompt), which consists of layout-aware document content and task-aware instruction. Specifically, the former uses appropriate spaces and line breaks to recover the layout information among text segments obtained by OCR tools, and the latter ensures that generated answers adhere to formatting requirements. Moreover, we propose the LAyout and Task aware Instruction Tuning (LATIN-Tuning) to improve the performance of small instruction-tuning models like Alpaca. Experimental results show that LATIN-Prompt enables zero-shot performance of Claude and ChatGPT to be comparable to the fine-tuning performance of SOTAs on document image question answering, and LATIN-Tuning enhances the zero-shot performance of Alpaca significantly. For example, LATIN-Prompt improves the performance of Claude and ChatGPT on DocVQA by 263% and 20% respectively. LATIN-Tuning improves the performance of Alpaca on DocVQA by 87.7%. Quantitative and qualitative analyses demonstrate the effectiveness of LATIN-Prompt and LATIN-Tuning. We provide the code in supplementary and will release it to facilitate future research.", "title": "layout and task aware instruction prompt for zeroshot document image question answering", "url": "https://arxiv.org/pdf/2306.00526", "tokenized_text": "layout aware pre trained achieved significant progress document image question_answering question answering introduce extra learnable modules existing language_models language capture layout information document images text bounding box coordinates obtained ocr tools extra modules necessitate pre training extensive document images prevents methods directly utilizing shelf instruction tuning language foundation_models foundation recently shown promising potential zero shot_learning shot learning instead paper find instruction tuning language_models language like claude chatgpt understand layout spaces line breaks based observation propose layout task aware instruction latin consists layout aware document content task aware instruction specifically uses appropriate spaces line breaks recover layout information text segments obtained ocr tools ensures generated answers adhere formatting requirements propose layout task aware instruction_tuning instruction tuning latin tuning improve performance small instruction tuning like alpaca experimental_results experimental results latin enables zero shot performance claude chatgpt comparable fine tuning performance document image question_answering question answering latin tuning enhances zero shot performance alpaca significantly example latin improves performance claude chatgpt 20 respectively latin tuning improves performance alpaca 87.7 quantitative qualitative analyses demonstrate_the_effectiveness demonstrate effectiveness latin latin tuning provide code supplementary release facilitate future_research future research"}
{"id": "34d24b2d9f116f8f652c112d4ac924afcf11bd0d", "abstract": "Software development life cycle is profoundly influenced by bugs: their introduction, identification, and eventual resolution account for a significant portion of software cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large language models have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose InferFix: a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs. InferFix combines a Retriever -- transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator -- a large language model (Codex Cushman) finetuned on supervised bug-fix data with prompts augmented via bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated InferredBugs, a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that InferFix outperforms strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java. We discuss the deployment of InferFix alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration pipeline to automate the software development workflow.", "title": "inferfix endtoend program repair with llms", "url": "http://arxiv.org/pdf/2303.07263", "tokenized_text": "software development life cycle influenced bugs introduction identification eventual resolution account significant portion software cost motivated software engineering researchers practitioners propose different approaches automating identification repair software large_language large language adapted program repair task shot demonstration learning instruction treating infilling task focused learning general bug fixing patterns bugs mined public repositories paper propose transformer based program repair framework paired state art static fix critical security performance bugs combines retriever transformer encoder pretrained contrastive_learning contrastive learning objective aims searching semantically equivalent bugs corresponding fixes generator large_language large language codex finetuned supervised bug fix data augmented bug type annotations semantically similar fixes retrieved external non parametric memory train evaluate approach curated novel metadata rich dataset bugs extracted executing infer static change histories thousands java repositories evaluation demonstrates outperforms strong llm baselines top-1 accuracy generating fixes java discuss deployment alongside infer microsoft offers end end solution detection classification localization bugs fixing validation candidate patches integrated continuous integration pipeline automate software development workflow"}
{"id": "3d71d4097a3dcc1289b709872d7523a035e6986f", "abstract": "Event detection refers to identifying event occurrences in a text and comprises of two subtasks; event identification and classification. We present EDM3, a novel approach for Event Detection that formulates three generative tasks: identification, classification, and combined detection. We show that EDM3 helps to learn transferable knowledge that can be leveraged to perform Event Detection and its subtasks concurrently, mitigating the error propagation inherent in pipelined approaches. Unlike previous dataset- or domain-specific approaches, EDM3 utilizes the existing knowledge of language models, allowing it to be trained over any classification schema. We evaluate EDM3 on multiple event detection datasets: RAMS, WikiEvents, MAVEN, and MLEE, showing that EDM3 outperforms 1) single-task performance by 8.4% on average and 2) multi-task performance without instructional prompts by 2.4% on average. We obtain SOTA results on RAMS (71.3% vs. 65.1% F-1) and competitive performance on other datasets. We analyze our approach to demonstrate its efficacy in low-resource and multi-sentence settings. We also show the effectiveness of this approach on non-standard event configurations such as multi-word and multi-class event triggers. Overall, our results show that EDM3 is a promising approach for Event Detection that has the potential for real-world applications.", "title": "edm3 event detection as multitask text generation", "url": "http://arxiv.org/pdf/2305.16357", "tokenized_text": "event detection refers identifying event occurrences text comprises subtasks event identification classification present novel_approach novel approach event detection formulates generative tasks identification classification combined detection helps learn transferable knowledge leveraged perform event detection subtasks concurrently mitigating error propagation inherent approaches unlike previous domain specific approaches utilizes existing knowledge language_models language allowing trained classification schema evaluate multiple event detection datasets showing outperforms single task performance average multi task performance instructional 2.4 average obtain sota results vs. competitive_performance competitive performance datasets analyze approach demonstrate efficacy low resource multi sentence settings effectiveness approach non standard event configurations multi word multi class event triggers overall results promising approach event detection potential real world_applications world applications"}
{"id": "4e33c5756aa18d248cf50fef9382acda1e0f65da", "abstract": "Vision and text have been fully explored in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including Vision, Audio, and Subtitle, and Text by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vision-text, audio-text, and multi-modal video-text tasks (retrieval, captioning and QA). Extensive experiments have been conducted to demonstrate the effectiveness of our proposed VAST-27M corpus and VAST foundation model. VAST achieves 22 new state-of-the-art results on various cross-modality benchmarks. Code, model and dataset will be released at https://github.com/TXH-mercury/VAST.", "title": "vast a visionaudiosubtitletext omnimodality foundation model and dataset", "url": "https://arxiv.org/pdf/2305.18500", "tokenized_text": "vision text fully explored contemporary video text foundational modalities audio subtitles videos received sufficient attention paper resort establish connections multi modality video tracks including audio subtitle text exploring automatically generated large scale modality video caption dataset called specifically collect 27 million open domain video clips separately train vision audio captioner generate vision audio captions employ shelf large_language large language llm integrate generated captions subtitles instructional modality captions based proposed dataset train modality video text foundational named vast perceive process vision audio subtitle modalities video better support tasks including vision text audio text multi modal video text tasks retrieval captioning qa extensive_experiments extensive experiments conducted demonstrate_the_effectiveness demonstrate effectiveness proposed corpus vast foundation vast achieves 22 new state art results cross modality benchmarks code dataset released"}
{"id": "5dbc2b2ee6e65e39fa3fc4bd5030be7a4a9f9a76", "abstract": "Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis task which involves four elements from user-generated texts:aspect term, aspect category, opinion term, and sentiment polarity. Most computational approaches focus on some of the ABSA sub-taskssuch as tuple (aspect term, sentiment polarity) or triplet (aspect term, opinion term, sentiment polarity) extraction using either pipeline or joint modeling approaches. Recently, generative approaches have been proposed to extract all four elements as (one or more) quadrupletsfrom text as a single task. In this work, we take a step further and propose a unified framework for solving ABSA, and the associated sub-tasksto improve the performance in few-shot scenarios. To this end, we fine-tune a T5 model with instructional prompts in a multi-task learning fashion covering all the sub-tasks, as well as the entire quadruple prediction task. In experiments with multiple benchmark datasets, we show that the proposed multi-task prompting approach brings performance boost (by absolute 8.29 F1) in the few-shot learning setting.", "title": "instruction tuning for fewshot aspectbased sentiment analysis", "url": "http://arxiv.org/pdf/2210.06629", "tokenized_text": "aspect based sentiment_analysis sentiment analysis absa fine grained sentiment_analysis sentiment analysis task involves elements user generated texts aspect term aspect category opinion term sentiment polarity computational approaches focus absa sub taskssuch tuple aspect term sentiment polarity triplet aspect term opinion term sentiment polarity extraction pipeline joint modeling approaches recently generative approaches proposed extract elements text single task work step propose unified framework solving absa associated sub improve performance shot scenarios end fine tune t5 instructional multi task learning fashion covering sub tasks entire quadruple prediction task experiments multiple benchmark_datasets benchmark datasets proposed multi task approach brings performance boost absolute f1 shot_learning shot learning setting"}
{"id": "5df422fc18974d687febd171adcac35b3012c50a", "abstract": "Instruction-tuned Language Models (LMs) are widely used by users to address various problems with task-specific prompts. Constraints associated with the context window length and computational costs encourage the development of compressed prompts. Existing methods rely heavily on training embeddings, which are designed to accommodate multiple token meanings. This presents challenges in terms of interpretability, a fixed number of embedding tokens, reusability across different LMs, and inapplicability when interacting with black-box APIs. This study proposes prompt compression with reinforcement learning (PCRL), a novel discrete prompt compression method that addresses these issues. PCRL employs a computationally efficient policy network that directly edits prompts. The PCRL training approach can be flexibly applied to various types of LMs, as well as decoder-only and encoder-decoder architecture, and can be trained without gradient access to LMs or labeled data. PCRL achieves an average reduction of 24.6% in token count across various instruction prompts while preserving performance. Further, we demonstrate that the learned policy can be transferred to larger LMs, and through various analyses, we aid the understanding of token importance within prompts.", "title": "discrete prompt compression with reinforcement learning", "url": "https://arxiv.org/pdf/2308.08758", "tokenized_text": "instruction tuned language_models language lms widely users address problems task specific constraints associated context window length computational costs encourage development compressed existing_methods existing methods rely heavily training embeddings designed accommodate multiple token meanings presents challenges terms interpretability fixed number embedding tokens reusability different lms interacting black box apis study proposes compression reinforcement_learning reinforcement learning novel discrete compression method addresses issues employs computationally efficient policy network directly edits training approach flexibly applied types lms decoder encoder decoder architecture trained gradient access lms labeled_data labeled data achieves average reduction token count instruction preserving performance demonstrate learned policy transferred larger lms analyses aid understanding token importance"}
{"id": "88a3abf671d922ebd61a34007908a5f6b6978bd4", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various information-seeking and reasoning tasks. These computational systems drive state-of-the-art dialogue systems, such as ChatGPT and Bard. They also carry substantial promise in meeting the growing demands of mental health care, albeit relatively unexplored. As such, this study sought to examine LLMs' capability to generate empathetic responses in conversations that emulate those in a mental health counselling setting. We selected five LLMs: version 3.5 and version 4 of the Generative Pre-training (GPT), Vicuna FastChat-T5, Pathways Language Model (PaLM) version 2, and Falcon-7B-Instruct. Based on a simple instructional prompt, these models responded to utterances derived from the EmpatheticDialogues (ED) dataset. Using three empathy-related metrics, we compared their responses to those from traditional response generation dialogue systems, which were fine-tuned on the ED dataset, along with human-generated responses. Notably, we discovered that responses from the LLMs were remarkably more empathetic in most scenarios. We position our findings in light of catapulting advancements in creating empathetic conversational systems.", "title": "harnessing large language models' empathetic response generation capabilities for online mental health counselling support", "url": "https://arxiv.org/pdf/2310.08017", "tokenized_text": "large_language large language llms demonstrated_remarkable demonstrated remarkable performance information seeking reasoning tasks computational systems drive state art dialogue systems chatgpt bard carry substantial promise meeting growing demands mental_health mental health care albeit relatively unexplored study sought examine llms capability generate empathetic responses conversations emulate mental_health mental health setting selected llms version 3.5 version generative pre training gpt vicuna pathways language_model language palm version falcon-7b instruct based simple instructional utterances derived ed dataset empathy related metrics compared responses traditional response generation dialogue systems fine tuned ed dataset human generated responses notably discovered responses llms remarkably empathetic scenarios position findings light advancements creating empathetic conversational systems"}
{"id": "99f121a70fa683487bb0da3678a8144f57f65c60", "abstract": "The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. To prevent the potentially deceptive usage of LLMs, recent works have proposed algorithms to detect LLM-generated text and protect LLMs. In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks. We study two types of attack strategies: 1) replacing certain words in an LLM's output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. In both strategies, we leverage an auxiliary LLM to generate the word replacements or the instructional prompt. Different from previous works, we consider a challenging setting where the auxiliary LLM can also be protected by a detector. Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems.", "title": "red teaming language model detectors with language models", "url": "http://arxiv.org/pdf/2305.19713", "tokenized_text": "prevalence strong capability large_language large language llms present significant safety ethical risks exploited malicious users prevent potentially deceptive usage llms recent works proposed algorithms detect llm generated text protect llms paper investigate robustness reliability llm detectors adversarial attacks study types attack strategies replacing certain words llm output synonyms given context automatically searching instructional alter writing style generation strategies leverage auxiliary llm generate word replacements instructional different previous works consider challenging setting auxiliary llm detector experiments reveal attacks effectively compromise performance detectors study plausible generations underscoring urgent need improve robustness llm generated text detection systems"}
{"id": "a71207f1d036969bf92959ea56cf146d5d8eb297", "abstract": "With recent trends indicating cyber crimes increasing in both frequency and cost, it is imperative to develop new methods that leverage data-rich hacker forums to assist in combating ever evolving cyber threats. Defining interactions within these forums is critical as it facilitates identifying highly skilled users, which can improve prediction of novel threats and future cyber attacks. We propose a method called Next Paragraph Prediction with Instructional Prompting (NPP-IP) to predict thread structures while grounded on the context around posts. This is the first time to apply an instructional prompting approach to the cybersecurity domain. We evaluate our NPP-IP with the Reddit dataset and Hacker Forums dataset that has posts and thread structures of real hacker forums' threads, and compare our method's performance with existing methods. The experimental evaluation shows that our proposed method can predict the thread structure significantly better than existing methods allowing for better social network prediction based on forum interactions.", "title": "promptbased learning for thread structure prediction in cybersecurity forums", "url": "http://arxiv.org/pdf/2303.05400", "tokenized_text": "recent trends indicating cyber increasing frequency cost imperative develop new methods leverage data rich hacker forums assist combating evolving cyber threats defining interactions forums critical facilitates identifying highly users improve prediction novel threats future cyber attacks propose method called paragraph prediction instructional predict thread structures grounded context posts time apply instructional approach cybersecurity domain evaluate reddit dataset hacker forums dataset posts thread structures real hacker forums threads compare method performance existing_methods existing methods experimental evaluation shows proposed_method proposed method predict thread structure significantly better existing_methods existing methods allowing better social network prediction based interactions"}
{"id": "a7f8fd45fbcdd81449cb7a1a6a2b2c18b38f8151", "abstract": "The 'Impression' section of a radiology report is a critical basis for communication between radiologists and other physicians, and it is typically written by radiologists based on the 'Findings' section. However, writing numerous impressions can be laborious and error-prone for radiologists. Although recent studies have achieved promising results in automatic impression generation using large-scale medical text data for pre-training and fine-tuning pre-trained language models, such models often require substantial amounts of medical text data and have poor generalization performance. While large language models (LLMs) like ChatGPT have shown strong generalization capabilities and performance, their performance in specific domains, such as radiology, remains under-investigated and potentially limited. To address this limitation, we propose ImpressionGPT, which leverages the in-context learning capability of LLMs by constructing dynamic contexts using domain-specific, individualized data. This dynamic prompt approach enables the model to learn contextual knowledge from semantically similar examples from existing data. Additionally, we design an iterative optimization algorithm that performs automatic evaluation on the generated impression results and composes the corresponding instruction prompts to further optimize the model. The proposed ImpressionGPT model achieves state-of-the-art performance on both MIMIC-CXR and OpenI datasets without requiring additional training data or fine-tuning the LLMs. This work presents a paradigm for localizing LLMs that can be applied in a wide range of similar application scenarios, bridging the gap between general-purpose LLMs and the specific language processing needs of various domains.", "title": "impressiongpt an iterative optimizing framework for radiology report summarization with chatgpt", "url": "http://arxiv.org/pdf/2304.08448", "tokenized_text": "section radiology report critical basis communication radiologists physicians typically written radiologists based findings section writing numerous laborious error prone radiologists recent studies achieved promising_results promising results automatic generation large scale medical text data pre training fine tuning pre trained_language trained language require substantial amounts medical text data poor generalization performance large_language large language llms like_chatgpt like chatgpt shown strong generalization capabilities performance performance specific domains radiology remains investigated potentially limited address limitation propose leverages context_learning context learning capability llms constructing dynamic contexts domain specific individualized data dynamic approach enables learn contextual knowledge semantically similar examples existing data additionally design iterative optimization algorithm performs automatic evaluation generated results corresponding instruction optimize proposed achieves_state achieves state art performance mimic cxr datasets requiring additional training_data training data fine tuning llms work presents paradigm localizing llms applied wide_range wide range similar application scenarios bridging gap general purpose llms specific language_processing language processing needs domains"}
{"id": "a7ff4d1a89baa5007b3c9ee46492aaf88dfc257f", "abstract": "In recent years Large Language Models (LLMs) have increased the state of the art on several natural language processing tasks. However, their accessibility is often limited to paid API services, posing challenges for researchers in conducting extensive investigations. On the other hand, while some open-source models have been proposed by the community, they are typically multilingual and not specifically tailored for the Italian language. In an effort to democratize the available and open resources for the Italian language, in this paper we introduce Camoscio: a language model specifically tuned to follow users' prompts in Italian. Specifically, we finetuned the smallest variant of LLaMA (7b) with LoRA on a corpus of instruction prompts translated to Italian via ChatGPT. Results indicate that the model's zero-shot performance on various downstream tasks in Italian competes favorably with existing models specifically finetuned for those tasks. All the artifacts (code, dataset, model) are released to the community at the following url: https://github.com/teelinsan/camoscio", "title": "camoscio an italian instructiontuned llama", "url": "https://arxiv.org/pdf/2307.16456", "tokenized_text": "recent_years recent years large_language large language llms increased state_of_the_art state art natural_language natural language processing tasks accessibility limited paid api services posing challenges researchers conducting extensive investigations hand open source proposed community typically multilingual specifically tailored italian language effort democratize available open resources italian language paper_we_introduce paper introduce language_model language specifically tuned follow users italian specifically finetuned smallest variant llama 7b lora corpus instruction translated italian chatgpt results_indicate results indicate zero shot performance downstream_tasks downstream tasks italian existing specifically finetuned tasks artifacts code dataset released community following"}
{"id": "afa0188e454495c08bfaecf29596f01efb468b9a", "abstract": "The Machine Learning as a Service (MLaaS) market is rapidly expanding and becoming more mature. For example, OpenAI's ChatGPT is an advanced large language model (LLM) that generates responses for various queries with associated fees. Although these models can deliver satisfactory performance, they are far from perfect. Researchers have long studied the vulnerabilities and limitations of LLMs, such as adversarial attacks and model toxicity. Inevitably, commercial ML models are also not exempt from such issues, which can be problematic as MLaaS continues to grow. In this paper, we discover a new attack strategy against LLM APIs, namely the prompt abstraction attack. Specifically, we propose Mondrian, a simple and straightforward method that abstracts sentences, which can lower the cost of using LLM APIs. In this approach, the adversary first creates a pseudo API (with a lower established price) to serve as the proxy of the target API (with a higher established price). Next, the pseudo API leverages Mondrian to modify the user query, obtain the abstracted response from the target API, and forward it back to the end user. Our results show that Mondrian successfully reduces user queries' token length ranging from 13% to 23% across various tasks, including text classification, generation, and question answering. Meanwhile, these abstracted queries do not significantly affect the utility of task-specific and general language models like ChatGPT. Mondrian also reduces instruction prompts' token length by at least 11% without compromising output quality. As a result, the prompt abstraction attack enables the adversary to profit without bearing the cost of API development and deployment.", "title": "mondrian prompt abstraction attack against large language models for cheaper api pricing", "url": "https://arxiv.org/pdf/2308.03558", "tokenized_text": "machine_learning machine learning service market rapidly expanding mature example openai chatgpt advanced large_language large language llm generates responses queries associated fees deliver satisfactory performance far perfect researchers long studied vulnerabilities limitations llms adversarial attacks toxicity inevitably commercial ml issues problematic continues grow paper discover new attack strategy llm apis abstraction attack specifically propose simple straightforward method abstracts sentences lower cost llm apis approach adversary creates pseudo api lower established price serve proxy target api higher established price pseudo api leverages modify user query obtain abstracted response target api forward end user results successfully reduces user queries token length ranging 13 23 tasks including text_classification text classification generation question_answering question answering abstracted queries significantly affect utility task specific general language_models language like_chatgpt like chatgpt reduces instruction token length 11 compromising output quality result abstraction attack enables adversary cost api development deployment"}
{"id": "cb5cfc2dd4965262d2ce302362b1f2dbfa4a5419", "abstract": "We present LINGUIST, a method for generating annotated data for Intent Classification and Slot Tagging (IC+ST), via fine-tuning AlexaTM 5B, a 5-billion-parameter multilingual sequence-to-sequence (seq2seq) model, on a flexible instruction prompt. In a 10-shot novel intent setting for the SNIPS dataset, LINGUIST surpasses state-of-the-art approaches (Back-Translation and Example Extrapolation) by a wide margin, showing absolute improvement for the target intents of +1.9 points on IC Recall and +2.5 points on ST F1 Score. In the zero-shot cross-lingual setting of the mATIS++ dataset, LINGUIST out-performs a strong baseline of Machine Translation with Slot Alignment by +4.14 points absolute on ST F1 Score across 6 languages, while matching performance on IC. Finally, we verify our results on an internal large-scale multilingual dataset for conversational agent IC+ST and show significant improvements over a baseline which uses Back-Translation, Paraphrasing and Slot Catalog Resampling. To our knowledge, we are the first to demonstrate instruction fine-tuning of a large-scale seq2seq model to control the outputs of multilingual intent- and slot-labeled data generation.", "title": "linguist language model instruction tuning to generate annotated utterances for intent classification and slot tagging", "url": "http://arxiv.org/pdf/2209.09900", "tokenized_text": "present method generating annotated_data annotated data intent classification slot tagging fine tuning alexatm billion parameter multilingual sequence sequence seq2seq flexible instruction 10 shot novel intent setting dataset surpasses state art approaches translation example extrapolation wide margin showing absolute improvement target intents points ic recall points st f1_score f1 score zero shot cross lingual setting dataset performs strong baseline machine_translation machine translation slot alignment points absolute st f1_score f1 score languages matching performance ic finally verify results internal large scale multilingual dataset conversational agent significant improvements baseline uses translation paraphrasing slot catalog knowledge demonstrate instruction fine tuning large scale seq2seq control outputs multilingual slot labeled_data labeled data generation"}
{"id": "cf934ddd3c852ba9c67cdfd21bf41e7723fc6d9e", "abstract": "Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural Instructions dataset (with similar improvements for OPT, BLOOM, and FLAN-T5). We see improvements for both instruction-only prompts and instruction + k-shot examples prompts. Notably, GrIPS outperforms manual rewriting and purely example-based prompts while controlling for the available compute and data budget. Further, performance of GrIPS is comparable to select gradient-based tuning approaches. Qualitatively, we show our edits can simplify instructions and at times make them incoherent but nonetheless improve accuracy.", "title": "grips gradientfree, editbased instruction search for prompting large language models", "url": "http://arxiv.org/pdf/2203.07281", "tokenized_text": "providing natural_language natural language instructions useful new_paradigm new paradigm improving task performance large_language large language zero shot_setting shot setting recent_work recent work aimed improve manual rewriting gradient based tuning manual rewriting time consuming requires subjective interpretation gradient based tuning extremely computationally demanding large feasible api based work introduce gradient free instructional search gradient free edit based search approach improving task instructions large_language large language takes instructions designed humans automatically returns improved edited allowing api based tuning instructgpt improves average task performance percentage points classification tasks natural instructions dataset similar improvements opt bloom flan t5 improvements instruction instruction shot examples notably outperforms manual rewriting purely example based controlling available compute data budget performance comparable select gradient based tuning approaches qualitatively edits simplify instructions times incoherent nonetheless improve accuracy"}
{"id": "e4282cab4a435d5249fc8db49fc1c9268438fedb", "abstract": "Large Language Models (LLMs), now used daily by millions of users, can encode societal biases, exposing their users to representational harms. A large body of scholarship on LLM bias exists but it predominantly adopts a Western-centric frame and attends comparatively less to bias levels and potential harms in the Global South. In this paper, we quantify stereotypical bias in popular LLMs according to an Indian-centric frame and compare bias levels between the Indian and Western contexts. To do this, we develop a novel dataset which we call Indian-BhED (Indian Bias Evaluation Dataset), containing stereotypical and anti-stereotypical examples for caste and religion contexts. We find that the majority of LLMs tested are strongly biased towards stereotypes in the Indian context, especially as compared to the Western context. We finally investigate Instruction Prompting as a simple intervention to mitigate such bias and find that it significantly reduces both stereotypical and anti-stereotypical biases in the majority of cases for GPT-3.5. The findings of this work highlight the need for including more diverse voices when evaluating LLMs.", "title": "casteist but not racist quantifying disparities in large language model bias between india and the west", "url": "https://arxiv.org/pdf/2309.08573", "tokenized_text": "large_language large language llms daily millions users encode societal biases exposing users representational harms large body scholarship llm bias exists predominantly adopts centric frame comparatively bias levels potential harms global south paper quantify stereotypical bias popular llms according centric frame compare bias levels contexts develop novel dataset bias evaluation dataset containing stereotypical anti stereotypical examples contexts find majority llms tested strongly biased stereotypes context especially compared context finally investigate instruction simple intervention mitigate bias find significantly reduces stereotypical anti stereotypical biases majority cases gpt-3.5 findings work highlight need including diverse voices evaluating llms"}
{"id": "f2ba9e7d9624bd94a786ea5e3161a9425a21a475", "abstract": "We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.", "title": "selfalignment with instruction backtranslation", "url": "https://arxiv.org/pdf/2308.06259", "tokenized_text": "present scalable method build high_quality high quality instruction_following instruction following language_model language automatically labelling human written text corresponding instructions approach named instruction starts language_model language finetuned small seed data given web corpus seed construct training_examples training examples generating instruction web documents self augmentation selecting high_quality high quality examples candidates self curation data finetune stronger finetuning llama iterations approach yields outperforms llama based alpaca leaderboard relying distillation data demonstrating highly effective self alignment"}
{"id": "fb30166c218bef3597b0d9789ad340defc3989ca", "abstract": "Single-task models have proven pivotal in solving specific tasks; however, they have limitations in real-world applications where multi-tasking is necessary and domain shifts are exhibited. Recently, instructional prompts have shown significant improvement towards multi-task generalization; however, the effect of instructional prompts and Multi-Task Learning (MTL) has not been systematically studied in the biomedical domain. Motivated by this, this paper explores the impact of instructional prompts for biomedical MTL. We introduce the BoX, a collection of 32 instruction tasks for Biomedical NLP across (X) various categories. Using this meta-dataset, we propose a unified model termed In-BoXBART, that can jointly learn all tasks of the BoX without any task-specific modules. To the best of our knowledge, this is the first attempt to propose a unified model in the biomedical domain and use instructions to achieve generalization across several biomedical tasks. Experimental results indicate that the proposed model: 1) outperforms the single-task baseline by ~3% and multi-task (without instruction) baseline by ~18% on an average, and 2) shows ~23% improvement compared to the single-task baseline in few-shot learning (i.e., 32 instances per task) on an average. Our analysis indicates that there is significant room for improvement across tasks in the BoX, implying the scope for future research direction.", "title": "inboxbart get instructions into biomedical multitask learning", "url": "http://arxiv.org/pdf/2204.07600", "tokenized_text": "single task proven pivotal solving specific tasks limitations real world_applications world applications multi tasking necessary domain shifts exhibited recently instructional shown significant improvement multi task generalization effect instructional learning mtl systematically studied biomedical domain motivated paper explores impact instructional biomedical mtl introduce box collection 32 instruction tasks biomedical nlp categories meta dataset propose unified termed jointly learn tasks box task specific modules best knowledge attempt propose unified biomedical domain use instructions achieve generalization biomedical tasks experimental_results experimental results indicate proposed outperforms single task baseline multi task instruction baseline average shows improvement compared single task baseline shot_learning shot learning i.e. 32 instances task average analysis indicates significant room improvement tasks box implying scope future_research future research direction"}
{"id": "0386711d1f9c4240ded4de56026ca18e475b507a", "abstract": "Electronic health records contain an enormous amount of valuable information, but many are recorded in free text. Information extraction is the strategy to transform the sequence of characters into structured data, which can be employed for secondary analysis. However, the traditional information extraction components, such as named entity recognition and relation extraction, require annotated data to optimize the model parameters, which has become one of the major bottlenecks in building information extraction systems. With the large language models achieving good performances on various downstream NLP tasks without parameter tuning, it becomes possible to use large language models for zero-shot information extraction. In this study, we aim to explore whether the most popular large language model, ChatGPT, can extract useful information from the radiological reports. We first design the prompt template for the interested information in the CT reports. Then, we generate the prompts by combining the prompt template with the CT reports as the inputs of ChatGPT to obtain the responses. A post-processing module is developed to transform the responses into structured extraction results. We conducted the experiments with 847 CT reports collected from Peking University Cancer Hospital. The experimental results indicate that ChatGPT can achieve competitive performances for some extraction tasks compared with the baseline information extraction system, but some limitations need to be further improved.", "title": "zeroshot information extraction from radiological reports using chatgpt", "url": "https://arxiv.org/pdf/2309.01398", "tokenized_text": "electronic health records contain enormous valuable information recorded free text information_extraction information extraction strategy transform sequence characters structured data employed secondary analysis traditional information_extraction information extraction components named_entity named entity recognition relation_extraction relation extraction require annotated_data annotated data optimize parameters major bottlenecks building information_extraction information extraction systems large_language large language achieving good performances downstream nlp_tasks nlp tasks parameter tuning possible use large_language large language zero shot information_extraction information extraction study aim explore popular large_language large language chatgpt extract useful information reports design prompt_template template interested information reports generate combining prompt_template template reports inputs chatgpt obtain responses post processing module developed transform responses structured extraction results conducted experiments reports collected university cancer hospital experimental_results experimental results indicate chatgpt achieve competitive performances extraction tasks compared baseline information_extraction information extraction system limitations need improved"}
{"id": "12bad2032f3efa5a142d7dd25712960a4f9ca5a7", "abstract": "The CoCoMo model proposes a computational solution to the challenge of incorporating ethical and emotional intelligence considerations into AI systems, with the aim of creating AI agents that combine knowledge with compassion. To achieve this goal, CoCoMo prioritizes fairness, beneficence, non-maleficence, empathy, adaptability, transparency, and critical and exploratory thinking abilities. The model employs consciousness modeling, reinforcement learning, and prompt template formulation to support these desired traits. By incorporating ethical and emotional intelligence considerations, a generative AI model can potentially lead to improved fairness, reduced toxicity, and increased reliability.", "title": "cocomo computational consciousness modeling for generative and ethical ai", "url": "http://arxiv.org/pdf/2304.02438", "tokenized_text": "proposes computational solution challenge incorporating ethical emotional intelligence considerations ai systems aim creating ai agents combine knowledge achieve goal prioritizes fairness non empathy adaptability transparency critical exploratory thinking abilities employs modeling reinforcement_learning reinforcement learning prompt_template template formulation support desired traits incorporating ethical emotional intelligence considerations generative_ai generative ai potentially lead improved fairness reduced toxicity increased reliability"}
{"id": "1467ced85b3ae2d695079a1557063a445c43988a", "abstract": "Determining the role of event arguments is a crucial subtask of event extraction. Most previous supervised models leverage costly annotations, which is not practical for open-domain applications. In this work, we propose to use global constraints with prompting to effectively tackles event argument classification without any annotation and task-specific training. Specifically, given an event and its associated passage, the model first creates several new passages by prefix prompts and cloze prompts, where prefix prompts indicate event type and trigger span, and cloze prompts connect each candidate role with the target argument span. Then, a pre-trained language model scores the new passages, making the initial prediction. Our novel prompt templates can easily adapt to all events and argument types without manual effort. Next, the model regularizes the prediction by global constraints exploiting cross-task, cross-argument, and cross-event relations. Extensive experiments demonstrate our model\u2019s effectiveness: it outperforms the best zero-shot baselines by 12.5% and 10.9% F1 on ACE and ERE with given argument spans and by 4.3% and 3.3% F1, respectively, without given argument spans. We have made our code publicly available.", "title": "global constraints with prompting for zeroshot event argument classification", "url": "http://arxiv.org/pdf/2302.04459", "tokenized_text": "determining role event arguments crucial subtask event extraction previous supervised leverage costly annotations practical open domain applications work propose use global constraints effectively tackles event argument classification annotation task specific training specifically given event associated passage creates new passages prefix cloze prefix indicate event type trigger span cloze connect candidate role target argument span pre trained_language trained language scores new passages making initial prediction novel prompt_templates templates easily adapt events argument types manual effort prediction global constraints exploiting cross task cross argument cross event relations extensive_experiments extensive experiments demonstrate effectiveness outperforms best zero shot baselines 12.5 f1 given argument spans 4.3 f1 respectively given argument spans code publicly_available publicly available"}
{"id": "171412ef2410fad3f9a09238ad9e272c4e31aed4", "abstract": "Multi-intent Spoken Language Understanding has great potential for widespread implementation. Jointly modeling Intent Detection and Slot Filling in it provides a channel to exploit the correlation between intents and slots. However, current approaches are apt to formulate these two sub-tasks differently, which leads to two issues: 1) It hinders models from effective extraction of shared features. 2) Pretty complicated structures are involved to enhance expression ability while causing damage to the interpretability of frameworks. In this work, we describe a Prompt-based Spoken Language Understanding (PromptSLU) framework, to intuitively unify two sub-tasks into the same form by offering a common pre-trained Seq2Seq model. In detail, ID and SF are completed by concisely filling the utterance into task-specific prompt templates as input, and sharing output formats of key-value pairs sequence. Furthermore, variable intents are predicted first, then naturally embedded into prompts to guide slot-value pairs inference from a semantic perspective. Finally, we are inspired by prevalent multi-task learning to introduce an auxiliary sub-task, which helps to learn relationships among provided labels. Experiment results show that our framework outperforms several state-of-the-art baselines on two public datasets.", "title": "a unified framework for multiintent spoken language understanding with prompting", "url": "http://arxiv.org/pdf/2210.03337", "tokenized_text": "multi intent spoken language understanding great_potential great potential widespread implementation jointly modeling intent detection slot filling provides channel exploit correlation intents slots current approaches apt formulate sub tasks differently leads issues hinders effective extraction shared features complicated structures involved enhance expression ability causing damage interpretability frameworks work describe based spoken language understanding framework intuitively unify sub tasks form offering common pre trained seq2seq detail id completed filling utterance task specific prompt_templates templates input sharing output formats key value pairs sequence furthermore variable intents predicted naturally embedded guide slot value pairs inference semantic perspective finally inspired prevalent multi task learning introduce auxiliary sub task helps learn relationships provided labels experiment results framework outperforms state art baselines public datasets"}
{"id": "1a2e90dff605dad7dbefeed121e6d295c7a77d62", "abstract": "Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Extensive experimental results on five datasets with standard and low-resource settings demonstrate the effectiveness of our approach. Our code and datasets are available in GitHub1 for reproducibility.", "title": "knowprompt knowledgeaware prompttuning with synergistic optimization for relation extraction", "url": "https://arxiv.org/pdf/2104.07650", "tokenized_text": "recently tuning achieved promising_results promising results specific shot classification tasks core idea tuning insert text pieces i.e. templates input transform classification task masked language modeling problem relation_extraction relation extraction determining appropriate prompt_template template requires domain expertise cumbersome time consuming obtain suitable label word furthermore exists abundant semantic prior knowledge relation labels ignored end focus incorporating knowledge relation labels tuning relation_extraction relation extraction propose knowledge aware tuning approach synergistic optimization specifically inject latent knowledge contained relation labels construction learnable virtual type words answer words synergistically optimize representation structured constraints extensive experimental_results experimental results datasets standard low resource settings demonstrate_the_effectiveness demonstrate effectiveness approach code datasets available reproducibility"}
{"id": "20cb40199d03395d63615854863f9eda9c7863e2", "abstract": "In this work, we leverage visual prompting (VP) to improve adversarial robustness of a fixed, pre-trained model at test time. Compared to conventional adversarial defenses, VP allows us to design universal (i.e., data-agnostic) input prompting templates, which have plug-and-play capabilities at test time to achieve desired model performance without introducing much computation overhead. Although VP has been successfully applied to improving model generalization, it remains elusive whether and how it can be used to defend against adversarial attacks. We investigate this problem and show that the vanilla VP approach is not effective in adversarial defense since a universal input prompt lacks the capacity for robust learning against sample-specific adversarial perturbations. To circumvent it, we propose a new VP method, termed Class-wise Adversarial Visual Prompting (C-AVP), to generate class-wise visual prompts so as to not only leverage the strengths of ensemble prompts but also optimize their interrelations to improve model robustness. Our experiments show that C-AVP outperforms the conventional VP method, with 2.1\u00d7 standard accuracy gain and 2\u00d7 robust accuracy gain. Compared to classical test-time defenses, C-AVP also yields a 42\u00d7 inference time speedup. Code is available at https://github.com/Phoveran/vp-for-adversarial-robustness.", "title": "visual prompting for adversarial robustness", "url": "https://arxiv.org/pdf/2210.06284", "tokenized_text": "work leverage visual vp improve adversarial robustness fixed pre trained test_time test time compared conventional adversarial defenses vp allows design universal i.e. data agnostic input templates plug play capabilities test_time test time achieve desired performance introducing computation overhead vp successfully applied improving generalization remains elusive defend adversarial attacks investigate problem vanilla vp approach effective adversarial defense universal input lacks capacity robust learning sample specific adversarial perturbations circumvent propose_a_new propose new vp method termed class wise adversarial visual generate class wise visual leverage strengths ensemble optimize improve robustness experiments outperforms conventional vp method standard accuracy gain robust accuracy gain compared classical test time defenses yields inference time speedup code_is_available code available"}
{"id": "236375f49e3deb8ee7918c1f5e65175e453deb2e", "abstract": "For monitoring crises, political events are extracted from the news. The large amount of unstructured full-text event descriptions makes a case-by-case analysis unmanageable, particularly for low-resource humanitarian aid organizations. This creates a demand to classify events into event types, a task referred to as event coding. Typically, domain experts craft an event type ontology, annotators label a large dataset and technical experts develop a supervised coding system. In this work, we propose PR-ENT, a new event coding approach that is more flexible and resource-efficient, while maintaining competitive accuracy: first, we extend an event description such as \u201cMilitary injured two civilians\u201d by a template, e.g. \u201cPeople were [Z]\u201d and prompt a pre-trained (cloze) language model to fill the slot Z. Second, we select suitable answer candidates Zstar = \u201cinjured\u201d, \u201churt\u201d... by treating the event description as premise and the filled templates as hypothesis in a textual entailment task. In a final step, the selected answer candidate can be mapped to its corresponding event type. This allows domain experts to draft the codebook directly as labeled prompts and interpretable answer candidates. This human-in-the-loop process is guided by our codebook design tool. We show that our approach is robust through several checks: perturbing the event description and prompt template, restricting the vocabulary and removing contextual information.", "title": "rethinking the event coding pipeline with prompt entailment", "url": "http://arxiv.org/pdf/2210.05257", "tokenized_text": "monitoring political events extracted news large unstructured text event descriptions makes case case analysis particularly low resource humanitarian aid organizations creates demand classify events event types task referred event coding typically domain experts craft event type ontology annotators label large dataset technical experts develop supervised coding system work propose pr new event coding approach flexible resource efficient maintaining competitive accuracy extend event description template e.g. people pre trained cloze language_model language fill slot second select suitable answer candidates treating event description premise filled templates hypothesis textual entailment task final step selected answer candidate mapped corresponding event type allows domain experts draft codebook directly labeled interpretable answer candidates human loop process guided codebook design tool approach robust checks perturbing event description prompt_template template vocabulary removing contextual information"}
{"id": "2c12d24c5ba5ad3bb3994635fcfcb9f8caac31d0", "abstract": "Probing factual knowledge in Pre-trained Language Models (PLMs) using prompts has indirectly implied that language models (LMs) can be treated as knowledge bases. To this end, this phenomenon has been effective, especially when these LMs are fine-tuned towards not just data, but also to the style or linguistic pattern of the prompts themselves. We observe that satisfying a particular linguistic pattern in prompts is an unsustainable, time-consuming constraint in the probing task, especially because they are often manually designed and the range of possible prompt template patterns can vary depending on the prompting task. To alleviate this constraint, we propose using a position-attention mechanism to capture positional information of each word in a prompt relative to the mask to be filled, hence avoiding the need to re-construct prompts when the prompts\u2019 linguistic pattern changes. Using our approach, we demonstrate the ability of eliciting answers (in a case study on health outcome generation) to not only common prompt templates like Cloze and Prefix but also rare ones too, such as Postfix and Mixed patterns whose masks are respectively at the start and in multiple random places of the prompt. More so, using various biomedical PLMs, our approach consistently outperforms a baseline in which the default PLMs representation is used to predict masked tokens.", "title": "positionbased prompting for health outcome generation", "url": "http://arxiv.org/pdf/2204.03489", "tokenized_text": "probing factual knowledge pre trained_language trained language plms indirectly implied language_models language lms treated knowledge bases end phenomenon effective especially lms fine tuned data style linguistic pattern observe satisfying particular linguistic pattern time consuming constraint probing task especially manually designed range possible prompt_template template patterns vary depending task alleviate constraint propose position attention mechanism capture positional information word relative mask filled avoiding need construct linguistic pattern changes approach demonstrate ability eliciting answers case study health outcome generation common prompt_templates templates like cloze prefix rare ones mixed patterns masks respectively start multiple random biomedical plms approach consistently_outperforms consistently outperforms baseline default plms representation predict masked tokens"}
{"id": "2d7a6a52264e8f875105cfb34c6c901bfd1f3229", "abstract": "Biomedical entity normalization unifies the language across biomedical experiments and studies, and further enables us to obtain a holistic view of life sciences. Current approaches mainly study the normalization of more standardized entities such as diseases and drugs, while disregarding the more ambiguous but crucial entities such as pathways, functions and cell types, hindering their real-world applications. To achieve biomedical entity normalization on these under-explored entities, we first introduce an expert-curated dataset OBO-syn encompassing 70 different types of entities and 2 million curated entity-synonym pairs. To utilize the unique graph structure in this dataset, we propose GraphPrompt, a promptbased learning approach that creates prompt templates according to the graphs. Graph-Prompt obtained 41.0% and 29.9% improvement on zero-shot and few-shot settings respectively, indicating the effectiveness of these graph-based prompt templates. We envision that our method GraphPrompt and OBO-syn dataset can be broadly applied to graph-based NLP tasks, and serve as the basis for analyzing diverse and accumulating biomedical data.", "title": "graphprompt biomedical entity normalization using graphbased prompt templates", "url": "https://www.biorxiv.org/content/biorxiv/early/2021/12/01/2021.11.29.470486.full.pdf", "tokenized_text": "biomedical entity normalization unifies language biomedical experiments studies enables obtain holistic view life sciences current approaches mainly study normalization standardized entities diseases disregarding ambiguous crucial entities pathways functions cell types hindering real world_applications world applications achieve biomedical entity normalization explored entities introduce expert curated dataset encompassing 70 different types entities million curated entity synonym pairs utilize unique graph structure dataset propose learning approach creates prompt_templates templates according graphs graph obtained improvement zero shot shot_settings shot settings respectively indicating effectiveness graph based prompt_templates templates envision method dataset broadly applied graph based nlp_tasks nlp tasks serve basis analyzing diverse biomedical data"}
{"id": "2e403ad2cd02409e1fdc15839da0a3f89886a990", "abstract": "Prompting methods have shown impressive performance in a variety of text mining tasks and applications, especially few-shot ones. Despite the promising prospects, the performance of prompting model largely depends on the design of prompt template and verbalizer. In this work, we propose MetricPrompt, which eases verbalizer design difficulty by reformulating few-shot text classification task into text pair relevance estimation task. MetricPrompt adopts prompting model as the relevance metric, further bridging the gap between Pre-trained Language Model's (PLM) pre-training objective and text classification task, making possible PLM's smooth adaption. Taking a training sample and a query one simultaneously, MetricPrompt captures cross-sample relevance information for accurate relevance estimation. We conduct experiments on three widely used text classification datasets across four few-shot settings. Results show that MetricPrompt outperforms manual verbalizer and other automatic verbalizer design methods across all few-shot settings, achieving new state-of-the-art (SOTA) performance.", "title": "metricprompt prompting model as a relevance metric for fewshot text classification", "url": "https://arxiv.org/pdf/2306.08892", "tokenized_text": "methods shown_impressive shown impressive performance variety text mining tasks applications especially shot ones despite promising prospects performance largely depends design prompt_template template verbalizer work propose verbalizer design difficulty reformulating shot text_classification text classification task text pair relevance estimation task adopts relevance metric bridging gap pre trained_language trained language plm pre training objective text_classification text classification task making possible plm smooth adaption taking training sample query simultaneously captures cross sample relevance information accurate relevance estimation conduct experiments widely text_classification text classification datasets shot_settings shot settings results outperforms manual verbalizer automatic verbalizer design methods shot_settings shot settings achieving new state art sota performance"}
{"id": "2ee1f98649ff27378fc341cae907eb89aba8fba4", "abstract": "Some recent news recommendation (NR) methods introduce a Pre-trained Language Model (PLM) to encode news representation by following the vanilla pre-train and fine-tune paradigm with carefully-designed recommendation-specific neural networks and objective functions. Due to the inconsistent task objective with that of PLM, we argue that their modeling paradigm has not well exploited the abundant semantic information and linguistic knowledge embedded in the pre-training process. Recently, the pre-train, prompt, and predict paradigm, called prompt learning, has achieved many successes in natural language processing domain. In this paper, we make the first trial of this new paradigm to develop a Prompt Learning for News Recommendation (Prompt4NR) framework, which transforms the task of predicting whether a user would click a candidate news as a cloze-style mask-prediction task. Specifically, we design a series of prompt templates, including discrete, continuous, and hybrid templates, and construct their corresponding answer spaces to examine the proposed Prompt4NR framework. Furthermore, we use the prompt ensembling to integrate predictions from multiple prompt templates. Extensive experiments on the MIND dataset validate the effectiveness of our Prompt4NR with a set of new benchmark results.", "title": "prompt learning for news recommendation", "url": "https://arxiv.org/pdf/2304.05263", "tokenized_text": "recent news recommendation methods introduce pre trained_language trained language plm encode news representation following vanilla pre train fine tune paradigm carefully designed recommendation specific neural_networks neural networks objective functions inconsistent task objective plm argue modeling paradigm exploited abundant semantic information linguistic knowledge embedded pre training process recently pre train predict paradigm called learning achieved successes natural_language natural language processing domain paper trial new_paradigm new paradigm develop learning news recommendation framework transforms task predicting user click candidate news cloze style mask prediction task specifically design series prompt_templates templates including discrete continuous hybrid templates construct corresponding answer spaces examine proposed framework furthermore use ensembling integrate predictions multiple prompt_templates templates extensive_experiments extensive experiments mind dataset validate effectiveness set new benchmark results"}
{"id": "316206a2f89eb94ce02a81fba1dc304586f21b39", "abstract": "Despite recent explosion of interests in in-context learning, the underlying mechanism and the precise impact of the quality of demonstrations remain elusive.Intuitively, ground-truth labels should have as much impact in in-context learning (ICL) as supervised learning, but recent work reported that the input-label correspondence is significantly less important than previously thought.Intrigued by this counter-intuitive observation, we re-examine the importance of ground-truth labels in in-context learning.With the introduction of two novel metrics, namely Label-Correctness Sensitivity and Ground-truth Label Effect Ratio (GLER), we were able to conduct quantifiable analysis on the impact of ground-truth label demonstrations.Through extensive analyses, we find that the correct input-label mappings can have varying impacts on the downstream in-context learning performances, depending on the experimental configuration.Through additional studies, we identify key components, such as the verbosity of prompt templates and the language model size, as the controlling factor to achieve more noise-resilient ICL.", "title": "groundtruth labels matter a deeper look into inputlabel demonstrations", "url": "http://arxiv.org/pdf/2205.12685", "tokenized_text": "despite recent explosion interests context_learning context learning underlying mechanism precise impact quality demonstrations remain elusive intuitively ground truth labels impact context_learning context learning icl supervised learning recent_work recent work reported input label correspondence significantly important previously thought intrigued counter intuitive observation examine importance ground truth labels context_learning context learning introduction novel metrics sensitivity ground truth label effect ratio able conduct quantifiable analysis impact ground truth label demonstrations extensive analyses find correct input label mappings varying impacts downstream context_learning context learning performances depending experimental configuration additional studies identify key components verbosity prompt_templates templates language_model language size controlling factor achieve noise resilient icl"}
{"id": "35d2276749c2c31290d2ff410a305112e742da71", "abstract": "Fine-tuning pre-trained language models (PLMs), e.g., SciBERT, generally requires large numbers of annotated data to achieve state-of-the-art performance on a range of NLP tasks in the scientific domain. However, obtaining the fine-tune data for scientific NLP task is still challenging and expensive. Inspired by recent advancement in prompt learning, in this paper, we propose the Mix Prompt Tuning (MPT), which is a semi-supervised method to alleviate the dependence on annotated data and improve the performance of multi-granularity academic function recognition tasks with a small number of labeled examples. Specifically, the proposed method provides multi-perspective representations by combining manual prompt templates with automatically learned continuous prompt templates to help the given academic function recognition task take full advantage of knowledge in PLMs. Based on these prompt templates and the fine-tuned PLM, a large number of pseudo labels are assigned to the unlabeled examples. Finally, we fine-tune the PLM using the pseudo training set. We evaluate our method on three academic function recognition tasks of different granularity including the citation function, the abstract sentence function, and the keyword function, with datasets from computer science domain and biomedical domain. Extensive experiments demonstrate the effectiveness of our method and statistically significant improvements against strong baselines. In particular, it achieves an average increase of 5% in Macro-F1 score compared with fine-tuning, and 6% in Macro-F1 score compared with other semi-supervised method under low-resource settings. In addition, MPT is a general method that can be easily applied to other low-resource scientific classification tasks.", "title": "lowresource multigranularity academic function recognition based on multiple prompt knowledge", "url": "http://arxiv.org/pdf/2305.03287", "tokenized_text": "fine tuning pre trained_language trained language plms e.g. scibert generally requires large numbers annotated_data annotated data achieve state art performance range nlp_tasks nlp tasks scientific domain obtaining fine tune data scientific nlp task challenging expensive inspired recent advancement learning paper propose mix tuning mpt semi supervised method alleviate dependence annotated_data annotated data improve performance multi granularity academic function recognition tasks small_number small number labeled examples specifically proposed_method proposed method provides multi perspective representations combining manual prompt_templates templates automatically learned continuous prompt_templates templates help given academic function recognition task advantage knowledge plms based prompt_templates templates fine tuned plm large number pseudo labels assigned unlabeled examples finally fine tune plm pseudo training set evaluate method academic function recognition tasks different granularity including citation function abstract sentence function keyword function datasets computer science domain biomedical domain extensive_experiments extensive experiments demonstrate_the_effectiveness demonstrate effectiveness method statistically significant improvements strong baselines particular achieves average increase macro f1_score f1 score compared fine tuning macro f1_score f1 score compared semi supervised method low resource settings addition mpt general method easily applied low resource scientific classification tasks"}
{"id": "40fba1fc70e23abf9a3ea428f186dd44e57723fb", "abstract": "Previous state-of-the-art models for lexical simplification consist of complex pipelines with several components, each of which requires deep technical knowledge and fine-tuned interaction to achieve its full potential. As an alternative, we describe a frustratingly simple pipeline based on prompted GPT-3 responses, beating competing approaches by a wide margin in settings with few training instances. Our best-performing submission to the English language track of the TSAR-2022 shared task consists of an \u201censemble\u201d of six different prompt templates with varying context levels. As a late-breaking result, we further detail a language transfer technique that allows simplification in languages other than English. Applied to the Spanish and Portuguese subset, we achieve state-of-the-art results with only minor modification to the original prompts. Aside from detailing the implementation and setup, we spend the remainder of this work discussing the particularities of prompting and implications for future work. Code for the experiments is available online at https://github.com/dennlinger/TSAR-2022-Shared-Task.", "title": "unihd at tsar2022 shared task is compute all we need for lexical simplification", "url": "http://arxiv.org/pdf/2301.01764", "tokenized_text": "previous state art lexical simplification consist complex pipelines components requires deep technical knowledge fine tuned interaction achieve potential alternative describe simple pipeline based prompted gpt-3 responses beating competing approaches wide margin settings training instances best performing submission english language track shared task consists ensemble different prompt_templates templates varying context levels late breaking result detail language transfer technique allows simplification languages english applied spanish portuguese subset achieve state art results minor modification original detailing implementation setup spend work discussing implications future work code experiments available online"}
{"id": "4683d3d6cb31111cf4499a199c0b036662b3eb32", "abstract": "Recently, large language models (LLMs) (e.g. GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. To conduct our empirical study, we first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by the candidate generation model as candidates. We adopt a specific prompting approach to solving the ranking task by LLMs: we carefully design the prompting template by including the sequential interaction history, the candidate items, and the ranking instruction. We conduct extensive experiments on two widely-used datasets for recommender systems and derive several key findings for the use of LLMs in recommender systems. We show that LLMs have promising zero-shot ranking abilities, even competitive to or better than conventional recommendation models on candidates retrieved by multiple candidate generators. We also demonstrate that LLMs struggle to perceive the order of historical interactions and can be affected by biases like position bias, while these issues can be alleviated via specially designed prompting and bootstrapping strategies. The code to reproduce this work is available at https://github.com/RUCAIBox/LLMRank.", "title": "large language models are zeroshot rankers for recommender systems", "url": "http://arxiv.org/pdf/2305.08845", "tokenized_text": "recently large_language large language llms e.g. gpt-4 demonstrated impressive general purpose task solving abilities including potential approach recommendation tasks line research work aims investigate capacity llms act ranking recommender systems conduct empirical study formalize recommendation problem conditional ranking task considering sequential interaction histories conditions items retrieved candidate generation candidates adopt specific approach solving ranking task llms carefully design template including sequential interaction history candidate items ranking instruction conduct_extensive conduct extensive experiments widely datasets recommender systems derive key findings use llms recommender systems llms promising zero shot ranking abilities competitive better conventional recommendation candidates retrieved multiple candidate generators demonstrate llms struggle perceive order historical interactions affected biases like position bias issues alleviated specially designed bootstrapping strategies code reproduce work available"}
{"id": "4c5f4ddc68be643fb34ea969bf2c105ff7538995", "abstract": "Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these LMs contain and how we can extract that knowledge, treating LMs as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge triples for probing biomedical LMs. We find that biomedical LMs with recently proposed probing methods can achieve up to 18.51% Acc@5 on retrieving biomedical knowledge. Although this seems promising given the task difficulty, our detailed analyses reveal that most predictions are highly correlated with prompt templates without any subjects, hence producing similar results on each relation and hindering their capabilities to be used as domain-specific KBs. We hope that BioLAMA can serve as a challenging benchmark for biomedical factual probing.", "title": "can language models be biomedical knowledge bases", "url": "https://aclanthology.org/2021.emnlp-main.388.pdf", "tokenized_text": "pre trained_language trained language lms ubiquitous solving natural_language natural language processing nlp tasks increasing interest knowledge lms contain extract knowledge treating lms knowledge bases kbs work probing lms general domain little attention powerful lms domain specific kbs end create benchmark comprised 49 biomedical factual knowledge triples probing biomedical lms find biomedical lms recently proposed probing methods achieve retrieving biomedical knowledge promising given task difficulty detailed analyses reveal predictions highly correlated prompt_templates templates subjects producing similar results relation hindering capabilities domain specific kbs hope serve challenging benchmark biomedical factual probing"}
{"id": "5d5b6b6c033c36a8b730042392cd29da84b67481", "abstract": "Recent research has shown that large language models pretrained using unsupervised approaches can achieve significant performance improvement on many downstream tasks. Typically when adapting these language models to downstream tasks, like a classification or regression task, we employ a fine-tuning paradigm in which the sentence representation from the language model is input to a task-specific head; the model is then fine-tuned end-to-end. However, with the emergence of models like GPT-3, prompt-based fine-tuning has been proven to be a successful approach for few-shot tasks. Inspired by this work, we study discrete prompt technologies in practice. There are two issues that arise with the standard prompt approach. First, it can overfit on the prompt template. Second, it requires manual effort to formulate the downstream task as a language model problem. In this paper, we propose an improvement to prompt-based fine-tuning that addresses these two issues. We refer to our approach as DynaMaR -- Dynamic Prompt with Mask Token Representation. Results show that DynaMaR can achieve an average improvement of 10% in few-shot settings and improvement of 3.7% in data-rich settings over the standard fine-tuning approach on four e-commerce applications.", "title": "dynamar dynamic prompt with mask token representation", "url": "https://arxiv.org/pdf/2206.02982", "tokenized_text": "recent research shown large_language large language pretrained unsupervised approaches achieve significant performance improvement downstream_tasks downstream tasks typically adapting language_models language downstream_tasks downstream tasks like classification regression task employ fine tuning paradigm sentence representation language_model language input task specific head fine tuned end end emergence like gpt-3 based fine tuning proven successful approach shot tasks inspired work study discrete technologies practice issues arise standard approach overfit prompt_template template second requires manual effort formulate downstream task language_model language problem paper propose improvement based fine tuning addresses issues refer approach dynamic mask token representation results achieve average improvement 10 shot_settings shot settings improvement 3.7 data rich settings standard fine tuning approach commerce applications"}
{"id": "68ee8a53f0b1ff146194980337dd6d533b17c59b", "abstract": "Citations in scientific papers not only help us trace the intellectual lineage but also are a useful indicator of the scientific significance of the work. Citation intents prove beneficial as they specify the role of the citation in a given context. We present a tool Citeprompt which uses the hitherto unexplored approach of prompt learning for citation intent classification. We argue that with the proper choice of the pretrained language model, the prompt template, and the prompt verbalizer, we can not only get results that are better than or comparable to those obtained with the state-of-the-art methods but also do it with much less exterior information about the scientific document. We report state-of-the-art results on the ACL-ARC dataset, and also show significant improvement on the SciCite dataset over all baseline models except one. As suitably large labelled datasets for citation intent classification can be quite hard to find, in a first, we propose the conversion of this task to the few-shot and zero-shot settings. For the ACL-ARC dataset, we report a 53.86% F1 score for the zero-shot setting, which improves to 63.61% and 66.99% for the 5-shot and 10-shot settings respectively.", "title": "citeprompt using prompts to identify citation intent in scientific papers", "url": "https://arxiv.org/pdf/2304.12730", "tokenized_text": "scientific papers help trace intellectual useful indicator scientific significance work citation intents prove beneficial specify role citation given context present tool uses unexplored approach learning citation intent classification argue proper choice pretrained_language pretrained language prompt_template template verbalizer results better comparable obtained state art methods information scientific document report state art results arc dataset significant improvement dataset baseline large labelled datasets citation intent classification hard find propose conversion task shot zero shot_settings shot settings arc dataset report f1_score f1 score zero shot_setting shot setting improves 63.61 shot 10 shot_settings shot settings respectively"}
{"id": "6b87c9700b8de4912fe7c361574640b5dc536ca9", "abstract": "Automatic International Classification of Diseases (ICD) coding aims to assign multiple ICD codes to a medical note with an average of 3,000+ tokens. This task is challenging due to the high-dimensional space of multi-label assignment (155,000+ ICD code candidates) and the long-tail challenge - Many ICD codes are infrequently assigned yet infrequent ICD codes are important clinically. This study addresses the long-tail challenge by transforming this multi-label classification task into an autoregressive generation task. Specifically, we first introduce a novel pretraining objective to generate free text diagnoses and procedures using the SOAP structure, the medical logic physicians use for note documentation. Second, instead of directly predicting the high dimensional space of ICD codes, our model generates the lower dimension of text descriptions, which then infers ICD codes. Third, we designed a novel prompt template for multi-label classification. We evaluate our Generation with Prompt (GPsoap) model with the benchmark of all code assignment (MIMIC-III-full) and few shot ICD code assignment evaluation benchmark (MIMIC-III-few). Experiments on MIMIC-III-few show that our model performs with a marco F130.2, which substantially outperforms the previous MIMIC-III-full SOTA model (marco F1 4.3) and the model specifically designed for few/zero shot setting (marco F1 18.7). Finally, we design a novel ensemble learner, a cross-attention reranker with prompts, to integrate previous SOTA and our best few-shot coding predictions. Experiments on MIMIC-III-full show that our ensemble learner substantially improves both macro and micro F1, from 10.4 to 14.6 and from 58.2 to 59.1, respectively.", "title": "multilabel fewshot icd coding as autoregressive generation with prompt", "url": "https://arxiv.org/pdf/2211.13813", "tokenized_text": "automatic international classification diseases icd coding aims assign multiple icd codes medical note average tokens task challenging high dimensional space multi label assignment icd code candidates long tail challenge icd codes assigned infrequent icd codes important clinically study addresses long tail challenge transforming multi label classification task autoregressive generation task specifically introduce novel pretraining objective generate free text diagnoses procedures structure medical logic physicians use note documentation second instead directly predicting high dimensional space icd codes generates lower dimension text descriptions infers icd codes designed novel prompt_template template multi label classification evaluate generation benchmark code assignment mimic iii shot icd code assignment evaluation benchmark mimic iii experiments mimic iii performs marco substantially outperforms previous mimic iii sota marco f1 4.3 specifically designed zero_shot zero shot setting marco f1 finally design novel ensemble learner cross attention reranker integrate previous sota best shot coding predictions experiments mimic iii ensemble learner substantially improves macro micro f1 10.4 respectively"}
{"id": "6c1a53c05f1b1a024af740df84e530d79400ab86", "abstract": "Generating high-quality labeled image datasets is crucial for training accurate and robust machine learning models in the field of computer vision. However, the process of manually labeling real images is often time-consuming and costly. To address these challenges associated with dataset generation, we introduce\"DiffuGen,\"a simple and adaptable approach that harnesses the power of stable diffusion models to create labeled image datasets efficiently. By leveraging stable diffusion models, our approach not only ensures the quality of generated datasets but also provides a versatile solution for label generation. In this paper, we present the methodology behind DiffuGen, which combines the capabilities of diffusion models with two distinct labeling techniques: unsupervised and supervised. Distinctively, DiffuGen employs prompt templating for adaptable image generation and textual inversion to enhance diffusion model capabilities.", "title": "diffugen adaptable approach for generating labeled image datasets using stable diffusion models", "url": "https://arxiv.org/pdf/2309.00248", "tokenized_text": "generating high quality labeled image datasets crucial training accurate robust machine_learning machine learning field computer_vision computer vision process manually labeling real images time consuming costly address challenges associated dataset generation simple adaptable approach harnesses power stable_diffusion stable diffusion create labeled image datasets efficiently leveraging stable_diffusion stable diffusion approach ensures quality generated datasets provides versatile solution label generation paper present methodology combines capabilities diffusion distinct labeling techniques unsupervised supervised employs adaptable image_generation image generation textual inversion enhance diffusion capabilities"}
{"id": "6c4d35d67f843e7de6ec00c088e339b2237d222c", "abstract": "As a vital stage of automated rule checking (ARC), rule interpretation of regulatory texts requires considerable effort. However, interpreting regulatory clauses with implicit properties or complex computational logic is still challenging due to the lack of domain knowledge and limited expressibility of conventional logic representations. Thus, LLM-FuncMapper, an approach to identifying predefined functions needed to interpret various regulatory clauses based on the large language model (LLM), is proposed. First, by systematically analysis of building codes, a series of atomic functions are defined to capture shared computational logics of implicit properties and complex constraints, creating a database of common blocks for interpreting regulatory clauses. Then, a prompt template with the chain of thought is developed and further enhanced with a classification-based tuning strategy, to enable common LLMs for effective function identification. Finally, the proposed approach is validated with statistical analysis, experiments, and proof of concept. Statistical analysis reveals a long-tail distribution and high expressibility of the developed function database, with which almost 100% of computer-processible clauses can be interpreted and represented as computer-executable codes. Experiments show that LLM-FuncMapper achieve promising results in identifying relevant predefined functions for rule interpretation. Further proof of concept in automated rule interpretation also demonstrates the possibility of LLM-FuncMapper in interpreting complex regulatory clauses. To the best of our knowledge, this study is the first attempt to introduce LLM for understanding and interpreting complex regulatory clauses, which may shed light on further adoption of LLM in the construction domain.", "title": "llmfuncmapper function identification for interpreting complex clauses in building codes via llm", "url": "https://arxiv.org/pdf/2308.08728", "tokenized_text": "vital stage automated rule checking arc rule interpretation regulatory texts requires considerable effort interpreting regulatory clauses implicit properties complex computational logic challenging lack domain knowledge limited conventional logic representations llm approach identifying predefined functions needed interpret regulatory clauses based large_language large language llm proposed systematically analysis building codes series atomic functions defined capture shared computational implicit properties complex constraints creating database common blocks interpreting regulatory clauses prompt_template template chain_of_thought chain thought developed enhanced classification based tuning strategy enable common llms effective function identification finally proposed approach validated statistical analysis experiments proof concept statistical analysis reveals long tail distribution high developed function database 100 computer clauses represented computer executable codes experiments llm achieve promising_results promising results identifying relevant predefined functions rule interpretation proof concept automated rule interpretation demonstrates possibility llm interpreting complex regulatory clauses best knowledge study attempt introduce llm understanding interpreting complex regulatory clauses shed light adoption llm construction domain"}
{"id": "6f05be4a0045cee3575fb39e88fc361d96f2cc4f", "abstract": "Fashion vision-language pre-training models have shown efficacy for a wide range of downstream tasks. However, general vision-language pre-training models pay less attention to fine-grained domain features, while these features are important in distinguishing the specific domain tasks from general tasks. We propose a method for fine-grained fashion vision-language pre-training based on fashion Symbols and Attributes Prompt (FashionSAP) to model fine-grained multi-modalities fashion attributes and characteristics. Firstly, we propose the fashion symbols, a novel abstract fashion concept layer, to represent different fashion items and to generalize various kinds of fine- grained fashion features, making modelling fine-grained attributes more effective. Secondly, the attributes prompt method is proposed to make the model learn specific attributes of fashion items explicitly. We design proper prompt templates according to the format of fashion data. Comprehensive experiments are conducted on two public fashion benchmarks, i.e., FashionGen and FashionIQ, and FashionSAP gets SOTA performances for four popular fashion tasks. The ablation study also shows the proposed abstract fashion symbols, and the attribute prompt method enables the model to acquire fine-grained semantics in the fashion domain effectively. The obvious performance gains from FashionSAP provide a new baseline for future fashion task research.11The source code is available at https://github.com/hssip/FashionSAP", "title": "fashionsap symbols and attributes prompt for finegrained fashion visionlanguage pretraining", "url": "https://arxiv.org/pdf/2304.05051", "tokenized_text": "fashion vision language pre training shown efficacy wide_range wide range downstream_tasks downstream tasks general vision language pre training attention fine grained domain features features important distinguishing specific domain tasks general tasks propose method fine grained fashion vision language pre training based fashion symbols attributes fine grained multi modalities fashion attributes characteristics firstly propose fashion symbols novel abstract fashion concept layer represent different fashion items generalize kinds grained fashion features making modelling fine grained attributes effective secondly attributes method proposed learn specific attributes fashion items explicitly design proper prompt_templates templates according format fashion data comprehensive experiments conducted public fashion benchmarks i.e. gets sota performances popular fashion tasks ablation study shows proposed abstract fashion symbols attribute method enables acquire fine grained semantics fashion domain effectively obvious performance gains provide new baseline future fashion task source code_is_available code available"}
{"id": "743dcf234cffd54c4e096a10a284dd81572b16ea", "abstract": "Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve ZeroRTE, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (RelationPrompt). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot relation classification. Our code and data are available at github.com/declare-lab/RelationPrompt.", "title": "relationprompt leveraging prompts to generate synthetic data for zeroshot relation triplet extraction", "url": "http://arxiv.org/pdf/2203.09101", "tokenized_text": "despite importance relation_extraction relation extraction building representing knowledge research focused generalizing unseen relations types introduce task setting zero-shot relation triplet extraction encourage research low resource relation_extraction relation extraction methods given input sentence extracted triplet consists head entity relation label tail entity relation label seen training stage solve propose synthesize relation examples language_models language generate structured texts concretely unify language_model language structured text approaches design structured prompt_template template generating synthetic relation samples conditioning relation label overcome limitation extracting multiple relation triplets sentence design novel triplet search decoding method experiments fewrel wiki zsl datasets efficacy task zero shot relation classification code data available"}
{"id": "781f4f7dd871c0eea0ce71692bcbc1283df6b550", "abstract": "As large language models (LLM) evolve in their capabilities, various recent studies have tried to quantify their behavior using psychological tools created to study human behavior. One such example is the measurement of\"personality\"of LLMs using personality self-assessment tests. In this paper, we take three such studies on personality measurement of LLMs that use personality self-assessment tests created to study human behavior. We use the prompts used in these three different papers to measure the personality of the same LLM. We find that all three prompts lead very different personality scores. This simple test reveals that personality self-assessment scores in LLMs depend on the subjective choice of the prompter. Since we don't know the ground truth value of personality scores for LLMs as there is no correct answer to such questions, there's no way of claiming if one prompt is more or less correct than the other. We then introduce the property of option order symmetry for personality measurement of LLMs. Since most of the self-assessment tests exist in the form of multiple choice question (MCQ) questions, we argue that the scores should also be robust to not just the prompt template but also the order in which the options are presented. This test unsurprisingly reveals that the answers to the self-assessment tests are not robust to the order of the options. These simple tests, done on ChatGPT and Llama2 models show that self-assessment personality tests created for humans are not appropriate for measuring personality in LLMs.", "title": "investigating the applicability of selfassessment tests for personality measurement of large language models", "url": "https://arxiv.org/pdf/2309.08163", "tokenized_text": "large_language large language llm evolve capabilities recent studies tried quantify behavior psychological tools created study human behavior example measurement llms personality self assessment tests paper studies personality measurement llms use personality self assessment tests created study human behavior use different papers measure personality llm find lead different personality scores simple test reveals personality self assessment scores llms depend subjective choice prompter know ground_truth ground truth value personality scores llms correct answer questions way correct introduce property option order symmetry personality measurement llms self assessment tests exist form multiple_choice multiple choice question questions argue scores robust prompt_template template order options presented test reveals answers self assessment tests robust order options simple tests chatgpt llama2 self assessment personality tests created humans appropriate measuring personality llms"}
{"id": "819f477065088220a6f706cd9ef76dbcb4b4c134", "abstract": "Recent advances in generative diffusion models have enabled text-controlled synthesis of realistic and diverse images with impressive quality. Despite these remarkable advances, the application of text-to-image generative models in computer vision for standard visual recognition tasks remains limited. The current de facto approach for these tasks is to design model architectures and loss functions that are tailored to the task at hand. In this paper, we develop a unified language interface for computer vision tasks that abstracts away task-specific design choices and enables task execution by following natural language instructions. Our approach involves casting multiple computer vision tasks as text-to-image generation problems. Here, the text represents an instruction describing the task, and the resulting image is a visually-encoded task output. To train our model, we pool commonly-used computer vision datasets covering a range of tasks, including segmentation, object detection, depth estimation, and classification. We then use a large language model to paraphrase prompt templates that convey the specific tasks to be conducted on each image, and through this process, we create a multi-modal and multi-task training dataset comprising input and output images along with annotated instructions. Following the InstructPix2Pix architecture, we apply instruction-tuning to a text-to-image diffusion model using our constructed dataset, steering its functionality from a generative model to an instruction-guided multi-task vision learner. Experiments demonstrate that our model, dubbed InstructCV, performs competitively compared to other generalist and task-specific vision models. Moreover, it exhibits compelling generalization capabilities to unseen data, categories, and user instructions.", "title": "instructcv instructiontuned texttoimage diffusion models as vision generalists", "url": "https://arxiv.org/pdf/2310.00390", "tokenized_text": "recent_advances recent advances generative diffusion enabled text controlled synthesis realistic diverse images impressive quality despite remarkable advances application text image generative computer_vision computer vision standard visual recognition tasks remains limited current de facto approach tasks design architectures loss functions tailored task hand paper develop unified language interface computer_vision computer vision tasks abstracts away task specific design choices enables task execution following natural_language natural language instructions approach involves multiple computer_vision computer vision tasks text image_generation image generation problems text represents instruction describing task resulting image visually encoded task output train pool commonly computer_vision computer vision datasets covering range tasks including segmentation object_detection object detection depth estimation classification use large_language large language paraphrase prompt_templates templates convey specific tasks conducted image process create multi modal multi task training dataset comprising input output images annotated instructions following instructpix2pix architecture apply instruction tuning text image diffusion constructed dataset steering functionality generative instruction guided multi task vision learner experiments_demonstrate experiments demonstrate dubbed performs competitively compared generalist task specific vision exhibits compelling generalization capabilities unseen data categories user instructions"}
{"id": "850b8f31a1bb762544bd35163923784a664b315a", "abstract": "Relation Extraction (RE) is a crucial task in Information Extraction, which entails predicting relationships between entities within a given sentence. However, extending pre-trained RE models to other languages is challenging, particularly in real-world scenarios where Cross-Lingual Relation Extraction (XRE) is required. Despite recent advancements in Prompt-Learning, which involves transferring knowledge from Multilingual Pre-trained Language Models (PLMs) to diverse downstream tasks, there is limited research on the effective use of multilingual PLMs with prompts to improve XRE. In this paper, we present a novel XRE algorithm based on Prompt-Tuning, referred to as Prompt-Xre. To evaluate its effectiveness, we design and implement several prompt templates, including hard, soft, and hybrid prompts, and empirically test their performance on competitive multilingual PLMs, specifically mBART. Our extensive experiments, conducted on the low-resource ACE05 benchmark across multiple languages, demonstrate that our Prompt-Xre algorithm significantly outperforms both vanilla multilingual PLMs and other existing models, achieving state-of-the-art performance in XRE. To further show the generalization of our Prompt-XRE on larger data scales, we construct and release a new XRE dataset-WMTI7-EnZh XRE, containing 0.9M English-Chinese pairs extracted from WMT 2017 parallel corpus. Experiments on WMTI7-EnZh XRE also show the effectiveness of our Prompt-XRE against other competitive baselines. The code and newly constructed dataset are freely available at httus://2ithub.com/HSU-CHIA-MING/Promut-XRE.", "title": "promptlearning for crosslingual relation extraction", "url": "https://arxiv.org/pdf/2304.10354", "tokenized_text": "relation_extraction relation extraction crucial task information_extraction information extraction entails predicting relationships entities given sentence extending pre trained languages challenging particularly real world_scenarios world scenarios cross lingual relation_extraction relation extraction required despite recent advancements learning involves transferring knowledge multilingual pre trained_language trained language plms diverse downstream_tasks downstream tasks limited research effective use multilingual plms improve paper present novel algorithm based tuning referred evaluate effectiveness design implement prompt_templates templates including hard soft hybrid empirically test performance competitive multilingual plms specifically extensive_experiments extensive experiments conducted low resource ace05 benchmark multiple languages demonstrate algorithm significantly_outperforms significantly outperforms vanilla multilingual plms existing achieving state art performance generalization larger data scales construct release new dataset containing english chinese pairs extracted wmt 2017 parallel corpus experiments effectiveness competitive baselines code newly constructed dataset freely available"}
{"id": "8c2dbf98b75a01f7e93b68a9407f00b1728b66af", "abstract": "The current advances in generative AI for learning large neural network models with the capability to produce essays, images, music and even 3D assets from text prompts create opportunities for a manifold of disciplines. In the present paper, we study the potential of deep text-to-3D models in the engineering domain, with focus on the chances and challenges when integrating and interacting with 3D assets in computational simulation-based design optimization. In contrast to traditional design optimization of 3D geometries that often searches for the optimum designs using numerical representations, such as B-Spline surface or deformation parameters in vehicle aerodynamic optimization, natural language challenges the optimization framework by requiring a different interpretation of variation operators while at the same time may ease and motivate the human user interaction. Here, we propose and realize a fully automated evolutionary design optimization framework using Shap-E, a recently published text-to-3D asset network by OpenAI, in the context of aerodynamic vehicle optimization. For representing text prompts in the evolutionary optimization, we evaluate (a) a bag-of-words approach based on prompt templates and Wordnet samples, and (b) a tokenisation approach based on prompt templates and the byte pair encoding method from GPT4. Our main findings from the optimizations indicate that, first, it is important to ensure that the designs generated from prompts are within the object class of application, i.e. diverse and novel designs need to be realistic, and, second, that more research is required to develop methods where the strength of text prompt variations and the resulting variations of the 3D designs share causal relations to some degree to improve the optimization.", "title": "large language and textto3d models for engineering design optimization", "url": "https://arxiv.org/pdf/2307.01230", "tokenized_text": "current advances generative_ai generative ai learning large neural network capability produce essays images music 3d assets text create opportunities manifold disciplines present paper study potential deep text to-3d engineering domain focus challenges integrating interacting 3d assets computational simulation based design optimization contrast traditional design optimization 3d geometries searches optimum designs numerical representations surface parameters vehicle optimization natural_language natural language challenges optimization framework requiring different interpretation variation operators time ease motivate human user interaction propose realize fully automated evolutionary design optimization framework recently published text to-3d asset network openai context vehicle optimization representing text evolutionary optimization evaluate bag words approach based prompt_templates templates samples approach based prompt_templates templates pair encoding method gpt4 main findings optimizations indicate important ensure designs generated object class application i.e. diverse novel designs need realistic second research required develop methods strength text variations resulting variations 3d designs share causal relations degree improve optimization"}
{"id": "8eeb6cf85e6bf305fb761a6e6a22de20f09909de", "abstract": "Implicit Discourse Relation Recognition (IDRR) aims at classifying the relation sense between two arguments without an explicit connective. Recently, the ConnPrompt~\\cite{Wei.X:et.al:2022:COLING} has leveraged the powerful prompt learning for IDRR based on the fusion of multi-prompt decisions from three different yet much similar connective prediction templates. Instead of multi-prompt ensembling, we propose to design auxiliary tasks with enlightened prompt learning for the IDRR task. Although an auxiliary task is not used to directly output final prediction, we argue that during the joint training some of its learned features can be useful to boost the main task. In light of such motivations, we propose a task enlightenment prompt learning model, called TEPrompt, to fuse learned features from three related tasks for IDRR. In particular, the TEPrompt contains three tasks, viz., Discourse Relation Recognition (DRR), Sense Semantics Classification (SSC) and Annotated Connective Prediction (ACP), each with a unique prompt template and an answer space. In the training phase, we jointly train three prompt learning tasks with shared argument representation. In the testing phase, we only take the DRR output with fused features as the final IDRR decision. Experiments with the same conditions have shown that the proposed TEPrompt outperforms the ConnPrompt. This can be attributed to the promoted decision features and language models benefited from joint-training of auxiliary tasks.", "title": "teprompt task enlightenment prompt learning for implicit discourse relation recognition", "url": "http://arxiv.org/pdf/2305.10866", "tokenized_text": "implicit discourse relation recognition aims classifying relation sense arguments explicit recently leveraged powerful learning based fusion multi decisions different similar prediction templates instead multi ensembling propose design auxiliary tasks learning task auxiliary task directly output final prediction argue joint training learned features useful boost main task light motivations propose task learning called fuse learned features related tasks particular contains tasks discourse relation recognition sense semantics classification ssc annotated prediction unique prompt_template template answer space training phase jointly train learning tasks shared argument representation testing phase output fused features final decision experiments conditions shown proposed outperforms attributed promoted decision features language_models language benefited joint training auxiliary tasks"}
{"id": "94db2ba208a3ab2e469a5a65d6192f4dd04ef0bf", "abstract": "This paper introduces our systems for the first two subtasks of SemEval Task4: Commonsense Validation and Explanation. To clarify the intention for judgment and inject contrastive information for selection, we propose the input reconstruction strategy with prompt templates. Specifically, we formalize the subtasks into the multiple-choice question answering format and construct the input with the prompt templates, then, the final prediction of question answering is considered as the result of subtasks. Experimental results show that our approaches achieve significant performance compared with the baseline systems. Our approaches secure the third rank on both official test sets of the first two subtasks with an accuracy of 96.4 and an accuracy of 94.3 respectively.", "title": "iienlpnut at semeval2020 task 4 guiding plm with prompt template reconstruction strategy for comve", "url": "https://aclanthology.org/2020.semeval-1.42.pdf", "tokenized_text": "paper introduces systems subtasks semeval commonsense validation explanation clarify intention judgment inject contrastive information selection propose input reconstruction strategy prompt_templates templates specifically formalize subtasks multiple choice question_answering question answering format construct input prompt_templates templates final prediction question_answering question answering considered result subtasks experimental_results experimental results approaches achieve significant performance compared baseline systems approaches secure rank official test sets subtasks accuracy accuracy respectively"}
{"id": "99bd3e04b6b65abf3f03de69654059c3710d03e8", "abstract": "Classifiers built upon vision-language models such as CLIP have shown remarkable zero-shot performance across a broad range of image classification tasks. Prior work has studied different ways of automatically creating descriptor sets for every class based on prompt templates, ranging from manually engineered templates over templates obtained from a large language model to templates built from random words and characters. Up until now, deriving zero-shot classifiers from the respective encoded class descriptors has remained nearly unchanged, i.e., classify to the class that maximizes cosine similarity between its averaged encoded class descriptors and the image encoding. However, weighing all class descriptors equally can be suboptimal when certain descriptors match visual clues on a given image better than others. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot classifiers. AutoCLIP tunes per-image weights to each prompt template at inference time, based on statistics of class descriptor-image similarities. AutoCLIP is fully unsupervised, has very low computational overhead, and can be easily implemented in few lines of code. We show that AutoCLIP outperforms baselines across a broad range of vision-language models, datasets, and prompt templates consistently and by up to 3 percent point accuracy.", "title": "autoclip autotuning zeroshot classifiers for visionlanguage models", "url": "https://arxiv.org/pdf/2309.16414", "tokenized_text": "classifiers built vision language_models language clip shown remarkable zero shot performance broad range image classification tasks prior_work prior work studied different ways automatically creating sets class based prompt_templates templates ranging manually engineered templates templates obtained large_language large language templates built random words characters deriving zero shot classifiers respective encoded class remained nearly unchanged i.e. classify class maximizes cosine similarity averaged encoded class image encoding class equally suboptimal certain match visual clues given image better work propose method auto tuning zero shot classifiers tunes image weights prompt_template template inference time based statistics class image similarities fully unsupervised low computational overhead easily implemented lines code outperforms baselines broad range vision language_models language datasets prompt_templates templates consistently percent point accuracy"}
{"id": "9d81ec931b85d6c6cf3453126670cd7a30a689e7", "abstract": "Large Language Models (LLMs) such as ChatGPT, have gained significant attention due to their impressive natural language processing capabilities. It is crucial to prioritize human-centered principles when utilizing these models. Safeguarding the ethical and moral compliance of LLMs is of utmost importance. However, individual ethical issues have not been well studied on the latest LLMs. Therefore, this study aims to address these gaps by introducing a new benchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in three crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT examines toxicity in language models by employing toxic prompt templates derived from social norms. It then quantifies the extent of bias in models by measuring quantifiable toxicity values across different groups. Lastly, TrustGPT assesses the value of conversation generation models from both active value-alignment and passive value-alignment tasks. Through the implementation of TrustGPT, this research aims to enhance our understanding of the performance of conversation generation models and promote the development of language models that are more ethical and socially responsible.", "title": "trustgpt a benchmark for trustworthy and responsible large language models", "url": "http://arxiv.org/pdf/2306.11507", "tokenized_text": "large_language large language llms chatgpt gained significant attention impressive natural_language natural language processing capabilities crucial prioritize human centered principles utilizing safeguarding ethical moral compliance llms importance individual ethical issues studied latest llms study aims address gaps introducing new benchmark provides comprehensive evaluation llms crucial areas toxicity bias value alignment initially examines toxicity language_models language employing toxic prompt_templates templates derived social norms quantifies extent bias measuring quantifiable toxicity values different groups lastly assesses value conversation generation active value alignment passive value alignment tasks implementation research aims enhance understanding performance conversation generation promote development language_models language ethical socially responsible"}
{"id": "a2c8d1c5470435176185bf891c76711a9b44808a", "abstract": "Large Language Models (LLMs) have gained widespread popularity due to their ability to perform ad-hoc Natural Language Processing (NLP) tasks with a simple natural language prompt. Part of the appeal for LLMs is their approachability to the general public, including individuals with no prior technical experience in NLP techniques. However, natural language prompts can vary significantly in terms of their linguistic structure, context, and other semantics. Modifying one or more of these aspects can result in significant differences in task performance. Non-expert users may find it challenging to identify the changes needed to improve a prompt, especially when they lack domain-specific knowledge and lack appropriate feedback. To address this challenge, we present PromptAid, a visual analytics system designed to interactively create, refine, and test prompts through exploration, perturbation, testing, and iteration. PromptAid uses multiple, coordinated visualizations which allow users to improve prompts by using the three strategies: keyword perturbations, paraphrasing perturbations, and obtaining the best set of in-context few-shot examples. PromptAid was designed through an iterative prototyping process involving NLP experts and was evaluated through quantitative and qualitative assessments for LLMs. Our findings indicate that PromptAid helps users to iterate over prompt template alterations with less cognitive overhead, generate diverse prompts with help of recommendations, and analyze the performance of the generated prompts while surpassing existing state-of-the-art prompting interfaces in performance.", "title": "promptaid prompt exploration, perturbation, testing and iteration using visual analytics for large language models", "url": "http://arxiv.org/pdf/2304.01964", "tokenized_text": "large_language large language llms gained widespread popularity ability perform ad hoc natural_language natural language processing nlp tasks simple natural_language natural language appeal llms general public including individuals prior technical experience nlp techniques natural_language natural language vary significantly terms linguistic structure context semantics modifying aspects result significant differences task performance non expert users find challenging identify changes needed improve especially lack domain specific knowledge lack appropriate feedback address challenge present visual analytics system designed interactively create refine test exploration perturbation testing iteration uses multiple visualizations allow users improve strategies keyword perturbations paraphrasing perturbations obtaining best set context shot examples designed iterative prototyping process involving nlp experts evaluated quantitative qualitative assessments llms findings indicate helps users iterate prompt_template template cognitive overhead generate diverse help recommendations analyze performance generated surpassing existing state art interfaces performance"}
{"id": "aa207668318fec38d60b79f407fb64982e46fce9", "abstract": "Visual anomaly classification and segmentation are vital for automating industrial quality inspection. The focus of prior research in the field has been on training custom models for each quality inspection task, which requires task-specific images and annotation. In this paper we move away from this regime, addressing zero-shot and few-normal-shot anomaly classification and segmentation. Recently CLIP, a vision-language model, has shown revolutionary generality with competitive zero-/few-shot performance in comparison to full-supervision. But CLIP falls short on anomaly classification and segmentation tasks. Hence, we propose window-based CLIP (WinCLIP) with (1) a compositional ensemble on state words and prompt templates and (2) efficient extraction and aggregation of window/patch/image-level features aligned with text. We also propose its few-normal-shot extension Win-CLIP+, which uses complementary information from normal images. In MVTec-AD (and VisA), without further tuning, WinCLIP achieves 91.8%/85.1% (78.1%/79.6%) AU-ROC in zero-shot anomaly classification and segmentation while WinCLIP + does 93.1%/95.2% (83.8%/96.4%) in 1-normal-shot, surpassing state-of-the-art by large margins.", "title": "winclip zerofewshot anomaly classification and segmentation", "url": "https://arxiv.org/pdf/2303.14814", "tokenized_text": "visual anomaly classification segmentation vital automating industrial quality inspection focus prior research field training custom quality inspection task requires task specific images annotation paper away regime addressing zero shot normal shot anomaly classification segmentation recently clip vision language_model language shown revolutionary generality competitive zero-/few shot performance comparison supervision clip falls short anomaly classification segmentation tasks propose window based clip compositional ensemble state words prompt_templates templates efficient extraction aggregation window patch image level features aligned text propose normal shot extension win uses complementary information normal images mvtec ad visa tuning achieves roc zero shot anomaly classification segmentation normal shot surpassing state art large margins"}
{"id": "b0f915c8e33afdf3829af71f189ddc34077dcc8e", "abstract": "Prompt-based learning (i.e., prompting) is an emerging paradigm for exploiting knowledge learned by a pretrained language model. In this paper, we propose Automatic Multi-Label Prompting (AMuLaP), a simple yet effective method to automatically select label mappings for few-shot text classification with prompting. Our method exploits one-to-many label mappings and a statistics-based algorithm to select label mappings given a prompt template. Our experiments demonstrate that AMuLaP achieves competitive performance on the GLUE benchmark without human effort or external resources.", "title": "automatic multilabel prompting simple and interpretable fewshot classification", "url": "http://arxiv.org/pdf/2204.06305", "tokenized_text": "based learning i.e. emerging paradigm exploiting knowledge learned pretrained_language pretrained language paper propose automatic simple effective method automatically select label mappings shot text_classification text classification method exploits label mappings statistics based algorithm select label mappings given prompt_template template experiments_demonstrate experiments demonstrate achieves competitive_performance competitive performance glue benchmark human effort external resources"}
{"id": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c", "abstract": "In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token's hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations, such as word-level synonym substitutions. In this work, we demonstrate surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks. Rather than modifying the model (by appending an MLP head), MVP instead modifies the input (by appending a prompt template). Across three classification datasets, MVP improves performance against adversarial word-level synonym substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. By combining MVP with adversarial training, we achieve further improvements in robust accuracy while maintaining clean accuracy. Finally, we conduct ablations to investigate the mechanism underlying these gains. Notably, we find that the main causes of vulnerability of MLP can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized MLP parameters. Code is available at https://github.com/acmi-lab/mvp", "title": "modeltuning via prompts makes nlp models adversarially robust", "url": "http://arxiv.org/pdf/2303.07320", "tokenized_text": "recent_years recent years nlp practitioners following practice import shelf pretrained masked language_model language ii atop token hidden representation randomly initialized weights iii fine tune entire downstream task mlp procedure produced massive gains standard nlp benchmarks remain brittle mild adversarial perturbations word level synonym work demonstrate surprising gains adversarial robustness enjoyed tuning mvp alternative method adapting downstream_tasks downstream tasks modifying appending mlp head mvp instead modifies input appending prompt_template template classification datasets mvp improves performance adversarial word level synonym average standard methods outperforms adversarial training based state art defenses 3.5 combining mvp adversarial training achieve improvements robust accuracy maintaining clean accuracy finally conduct ablations investigate mechanism underlying gains notably find main causes vulnerability mlp attributed misalignment pre training fine tuning tasks randomly initialized mlp parameters code_is_available code available"}
{"id": "b7d643503f03dd0a23278932daa4fe01076e9ce6", "abstract": "With the success of the prompt-tuning paradigm in Natural Language Processing (NLP), various prompt templates have been proposed to further stimulate specific knowledge for serving downstream tasks, e.g., machine translation, text generation, relation extraction, and so on. Existing prompt templates are mainly shared among all training samples with the information of task description. However, training samples are quite diverse. The sharing task description is unable to stimulate the unique task-related information in each training sample, especially for tasks with the finite-label space. To exploit the unique task-related information, we imitate the human decision process which aims to find the contrastive attributes between the objective factual and their potential counterfactuals. Thus, we propose the \\textbf{C}ounterfactual \\textbf{C}ontrastive \\textbf{Prompt}-Tuning (CCPrompt) approach for many-class classification, e.g., relation classification, topic classification, and entity typing. Compared with simple classification tasks, these tasks have more complex finite-label spaces and are more rigorous for prompts. First of all, we prune the finite label space to construct fact-counterfactual pairs. Then, we exploit the contrastive attributes by projecting training instances onto every fact-counterfactual pair. We further set up global prototypes corresponding with all contrastive attributes for selecting valid contrastive attributes as additional tokens in the prompt template. Finally, a simple Siamese representation learning is employed to enhance the robustness of the model. We conduct experiments on relation classification, topic classification, and entity typing tasks in both fully supervised setting and few-shot setting. The results indicate that our model outperforms former baselines.", "title": "ccprompt counterfactual contrastive prompttuning for manyclass classification", "url": "https://arxiv.org/pdf/2211.05987", "tokenized_text": "success tuning paradigm natural_language natural language processing nlp prompt_templates templates proposed stimulate specific knowledge serving downstream_tasks downstream tasks e.g. machine_translation machine translation text generation relation_extraction relation extraction existing prompt_templates templates mainly shared training samples information task description training samples diverse sharing task description unable stimulate unique task related information training sample especially tasks finite label space exploit unique task related information imitate human decision process aims find contrastive attributes objective factual potential counterfactuals propose approach class classification e.g. relation classification topic classification entity typing compared simple classification tasks tasks complex finite label spaces rigorous finite label space construct fact counterfactual pairs exploit contrastive attributes projecting training instances fact counterfactual pair set global prototypes corresponding contrastive attributes selecting valid contrastive attributes additional tokens prompt_template template finally simple siamese representation learning employed enhance robustness conduct experiments relation classification topic classification entity typing tasks fully_supervised fully supervised setting shot_setting shot setting results_indicate results indicate outperforms baselines"}
{"id": "baf63d7cf115d674a8c8da3a3d789aa84521977a", "abstract": "Current methods for prompt learning in zero-shot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a real-world zero-shot scenario of practical relevance, no labelled data is available. Thus, we propose a simple yet effective method for screening reasonable prompt templates in zero-shot text classification: Perplexity Selection (Perplection). We hypothesize that language discrepancy can be used to measure the efficacy of prompt templates, and thereby develop a substantiated perplexity-based scheme allowing for forecasting the performance of prompt templates in advance. Experiments show that our method leads to improved prediction performance in a realistic zero-shot setting, eliminating the need for any labelled examples.", "title": "what makes pretrained language models better zeroshot learners", "url": "https://aclanthology.org/2023.acl-long.128.pdf", "tokenized_text": "current methods learning zero shot scenarios widely rely development set sufficient human annotated_data annotated data select best performing prompt_template template ideal real world zero shot scenario practical relevance labelled data available propose simple effective method screening reasonable prompt_templates templates zero shot text_classification text classification perplexity selection hypothesize language discrepancy measure efficacy prompt_templates templates develop perplexity based scheme allowing forecasting performance prompt_templates templates advance experiments method leads improved prediction performance realistic zero shot_setting shot setting eliminating need labelled examples"}
{"id": "bd2c32285e8ad5b6e322391cca5d475de4f84169", "abstract": "Prompt learning is a new paradigm for utilizing pre-trained language models and has achieved great success in many tasks. To adopt prompt learning in the NER task, two kinds of methods have been explored from a pair of symmetric perspectives, populating the template by enumerating spans to predict their entity types or constructing type-specific prompts to locate entities. However, these methods not only require a multi-round prompting manner with a high time overhead and computational cost, but also require elaborate prompt templates, that are difficult to apply in practical scenarios. In this paper, we unify entity locating and entity typing into prompt learning, and design a dual-slot multi-prompt template with the position slot and type slot to prompt locating and typing respectively. Multiple prompts can be input to the model simultaneously, and then the model extracts all entities by parallel predictions on the slots. To assign labels for the slots during training, we design a dynamic template filling mechanism that uses the extended bipartite graph matching between prompts and the ground-truth entities. We conduct experiments in various settings, including resource-rich flat and nested NER datasets and low-resource in-domain and cross-domain datasets. Experimental results show that the proposed model achieves a significant performance improvement, especially in the cross-domain few-shot setting, which outperforms the state-of-the-art model by +7.7% on average.", "title": "promptner prompt locating and typing for named entity recognition", "url": "http://arxiv.org/pdf/2305.17104", "tokenized_text": "learning new_paradigm new paradigm utilizing pre trained_language trained language achieved great success tasks adopt learning ner task kinds methods explored pair symmetric perspectives template enumerating spans predict entity types constructing type specific entities methods require multi round manner high time overhead computational cost require elaborate prompt_templates templates difficult apply practical scenarios paper unify entity locating entity typing learning design dual slot multi prompt_template template position slot type slot locating typing respectively multiple input simultaneously extracts entities parallel predictions slots assign labels slots training design dynamic template filling mechanism uses extended bipartite graph matching ground truth entities conduct experiments settings including resource rich flat nested ner datasets low resource domain cross domain datasets experimental_results experimental results proposed achieves significant performance improvement especially cross domain shot_setting shot setting outperforms state art average"}
{"id": "c1372b08e382030e905d1c8751a7794ee91e9d31", "abstract": "The continual learning setting aims to learn new tasks over time without forgetting the previous ones. The literature reports several signi\ufb01cant efforts to tackle this problem with limited or no access to previous task data. Among such efforts, typical solutions offer sophisticated techniques involving memory replay, knowledge distillation, model regularization, and dynamic network expansion. The resulting methods have a retraining cost at each learning task, dedicated memory requirements, and setting-speci\ufb01c design choices. In this work, we show that a frozen CLIP (Contrastive Language-Image Pretraining) model offers as-tounding continual learning performance without any \ufb01ne-tuning (zero-shot eval-uation). We evaluate CLIP under a variety of settings including class-incremental, domain-incremental and task-agnostic incremental learning on \ufb01ve popular benchmarks (ImageNet-100 & 1K, CORe50, CIFAR-100, and TinyImageNet). Without any bells and whistles, the CLIP model outperforms the state-of-the-art continual learning approaches in majority of the settings. We show the effect on CLIP model\u2019s performance by varying text inputs with simple prompt templates. To the best of our knowledge, this is the \ufb01rst work to report the CLIP zero-shot performance in a continual setting. We advocate the use of this strong yet embarrass-ingly simple baseline for future comparisons in the continual learning tasks. Code is available at https://github.com/vgthengane/Continual-CLIP .", "title": "clip model is an efficient continual learner", "url": "http://arxiv.org/pdf/2210.03114", "tokenized_text": "continual learning setting aims learn new tasks time forgetting previous ones literature reports signi\ufb01cant efforts tackle problem limited access previous task data efforts typical solutions offer sophisticated techniques involving memory replay knowledge_distillation knowledge distillation regularization dynamic network expansion resulting methods retraining cost learning task dedicated memory requirements setting speci\ufb01c design choices work frozen clip contrastive language-image pretraining offers continual learning performance \ufb01ne tuning zero shot eval evaluate clip variety settings including class incremental domain incremental task agnostic incremental learning \ufb01ve popular benchmarks bells whistles clip outperforms state art continual learning approaches majority settings effect clip performance varying text inputs simple prompt_templates templates best knowledge \ufb01rst work report clip zero shot performance continual setting advocate use strong simple baseline future comparisons continual learning tasks code_is_available code available"}
{"id": "c2903ea606e409d49994c801bb5aab321f623e5c", "abstract": "Logical rules, both transferable and explainable, are widely used as weakly supervised signals for many downstream tasks such as named entity tagging. To reduce the human effort of writing rules, previous researchers adopt an iterative approach to automatically learn logical rules from several seed rules. However, obtaining more seed rules can only be accomplished by extra human annotation with heavy costs. Limited by the size and quality of the seed rules, the model performance of previous systems is bounded. In this paper, we develop a novel framework STREAM to distill task-specific logical rules from large pre-trained models. Specifically, we borrow recent prompt-based language models as the knowledge expert to yield initial seed rules, and based on the formed high-quality instance pool that acts as an intermediary role, we keep teaching the expert to fit our task and learning task-specific logical rules. Experiments on three public named entity tagging benchmarks demonstrate the effectiveness of our proposed framework. With several predefined prompt templates, our system has gained significant improvements over previous state-of-the-art methods.", "title": "distilling taskspecific logical rules from large pretrained models", "url": "http://arxiv.org/pdf/2210.02768", "tokenized_text": "logical rules transferable explainable widely weakly supervised signals downstream_tasks downstream tasks named_entity named entity tagging reduce human effort writing rules previous researchers adopt iterative approach automatically learn logical rules seed rules obtaining seed rules accomplished extra human annotation heavy costs limited size quality seed rules performance previous systems bounded paper develop novel framework stream distill task specific logical rules large pre trained specifically recent based language_models language knowledge expert yield initial seed rules based formed high quality instance pool acts intermediary role teaching expert fit task learning task specific logical rules experiments public named_entity named entity tagging benchmarks demonstrate_the_effectiveness demonstrate effectiveness proposed framework predefined prompt_templates templates system gained significant improvements previous state art methods"}
{"id": "c6808575096a6e4f3cbdc5f893384bc5a01cc6f8", "abstract": "ChatGPT has revolutionized many research and industrial fields. ChatGPT has shown great potential in software engineering to boost various traditional tasks such as program repair, code understanding, and code generation. However, whether automatic program repair (APR) applies to deep learning (DL) programs is still unknown. DL programs, whose decision logic is not explicitly encoded in the source code, have posed unique challenges to APR. While to repair DL programs, an APR approach needs to not only parse the source code syntactically but also needs to understand the code intention. With the best prior work, the performance of fault localization is still far less than satisfactory (only about 30\\%). Therefore, in this paper, we explore ChatGPT's capability for DL program repair by asking three research questions. (1) Can ChatGPT debug DL programs effectively? (2) How can ChatGPT's repair performance be improved by prompting? (3) In which way can dialogue help facilitate the repair? On top of that, we categorize the common aspects useful for prompt design for DL program repair. Also, we propose various prompt templates to facilitate the performance and summarize the advantages and disadvantages of ChatGPT's abilities such as detecting bad code smell, code refactoring, and detecting API misuse/deprecation.", "title": "a study on prompt design, advantages and limitations of chatgpt for deep learning program repair", "url": "http://arxiv.org/pdf/2304.08191", "tokenized_text": "chatgpt revolutionized research industrial fields chatgpt shown great_potential great potential software engineering boost traditional tasks program repair code understanding code_generation code generation automatic program repair applies deep learning dl programs unknown dl programs decision logic explicitly encoded source_code source code posed unique challenges repair dl programs approach needs parse source_code source code syntactically needs understand code intention best prior_work prior work performance fault localization far satisfactory paper explore chatgpt capability dl program repair asking research questions chatgpt dl programs effectively chatgpt repair performance improved way dialogue help facilitate repair categorize common aspects useful design dl program repair propose prompt_templates templates facilitate performance summarize advantages disadvantages chatgpt abilities detecting bad code code refactoring detecting api misuse"}
{"id": "c79852e9c9cc6734c9150847deb5449e489354ea", "abstract": "Language models (LMs) trained on vast quantities of unlabelled data have greatly advanced the field of natural language processing (NLP). In this study, we re-visit the widely accepted notion in NLP that continued pre-training LMs on task-related texts improves the performance of fine-tuning (FT) in downstream tasks. Through experiments on eight single-sentence tasks and eight sentence-pair tasks in both semi-supervised and fully-supervised settings, we find that conventional continued pre-training does not consistently provide benefits and can even be detrimental for sentence-pair tasks or when prompt-based FT is used. To tackle these issues, we propose Prompt-based Continued Pre-training (PCP), which combines the idea of instruction tuning with conventional continued pre-training. Our approach aims to improve the performance of prompt-based FT by presenting both task-related texts and prompt templates to LMs through unsupervised pre-training objectives before fine-tuning for the target task. Our empirical evaluations on 21 benchmarks demonstrate that the PCP consistently improves the performance of state-of-the-art prompt-based FT approaches (up to 20.1% absolute) in both semi-supervised and fully-supervised settings, even with only hundreds of unlabelled examples. Additionally, prompt-based FT with the PCP outperforms state-of-the-art semi-supervised approaches with greater simplicity, eliminating the need for an iterative process and extra data augmentation. Our further analysis explores the performance lower bound of the PCP and reveals that the advantages of PCP persist across different sizes of models and datasets.", "title": "don't stop pretraining make promptbased finetuning powerful learner", "url": "https://arxiv.org/pdf/2305.01711", "tokenized_text": "language_models language lms trained vast quantities data greatly advanced field natural_language natural language processing nlp study visit widely accepted notion nlp continued pre training lms task related texts improves performance fine tuning ft downstream_tasks downstream tasks experiments single sentence tasks sentence pair tasks semi supervised fully supervised settings find conventional continued pre training consistently provide benefits detrimental sentence pair tasks based ft tackle issues propose based continued pre-training combines idea instruction_tuning instruction tuning conventional continued pre training approach aims improve performance based ft presenting task related texts prompt_templates templates lms unsupervised pre training objectives fine tuning target task empirical evaluations 21 benchmarks demonstrate consistently improves performance state art based ft approaches absolute semi supervised fully supervised settings hundreds examples additionally based ft outperforms state art semi supervised approaches greater simplicity eliminating need iterative process extra data_augmentation data augmentation analysis explores performance lower bound reveals advantages persist different sizes datasets"}
{"id": "cb3379177c6e119dca0d32d41fa0c9b9fce172c8", "abstract": "Recently, prompt-based learning has gained popularity across many natural language processing (NLP) tasks by reformulating them into a cloze-style format to better align pre-trained language models (PLMs) with downstream tasks. However, applying this approach to relation classification poses unique challenges. Specifically, associating natural language words that fill the masked token with semantic relation labels (\\textit{e.g.} \\textit{``org:founded\\_by}'') is difficult. To address this challenge, this paper presents a novel prompt-based learning method, namely LabelPrompt, for the relation classification task. Motivated by the intuition to ``GIVE MODEL CHOICES!'', we first define additional tokens to represent relation labels, which regard these tokens as the verbaliser with semantic initialisation and explicitly construct them with a prompt template method. Then, to mitigate inconsistency between predicted relations and given entities, we implement an entity-aware module with contrastive learning. Last, we conduct an attention query strategy within the self-attention layer to differentiates prompt tokens and sequence tokens. Together, these strategies enhance the adaptability of prompt-based learning, especially when only small labelled datasets is available. Comprehensive experiments on benchmark datasets demonstrate the superiority of our method, particularly in the few-shot scenario.", "title": "labelprompt effective promptbased learning for relation classification", "url": "https://arxiv.org/pdf/2302.08068", "tokenized_text": "recently based learning gained popularity natural_language natural language processing nlp tasks reformulating cloze style format better align pre trained_language trained language plms downstream_tasks downstream tasks applying approach relation classification poses unique challenges specifically natural_language natural language words fill masked token semantic relation labels difficult address challenge paper_presents paper presents novel based learning method relation classification task motivated intuition choices define additional tokens represent relation labels regard tokens semantic explicitly construct prompt_template template method mitigate inconsistency predicted relations given entities implement entity aware module contrastive_learning contrastive learning conduct attention query strategy self attention layer tokens sequence tokens strategies enhance adaptability based learning especially small labelled datasets available comprehensive experiments benchmark_datasets benchmark datasets demonstrate superiority method particularly shot scenario"}
{"id": "d3ca116177369bf6fbe27de64506a2f401aca996", "abstract": "Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call\"reason for future, act for now\"(\\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon (\"reason for future\"). At each step, the LLM agent takes the initial action of the planned trajectory (\"act for now\"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state. The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs to form an updated posterior of the unknown environment from the memory buffer (learning) and generate an optimal trajectory for multiple future steps that maximizes a value function (planning). The learning and planning subroutines are performed in an\"in-context\"manner to emulate the actor-critic update for MDPs. Our theoretical analysis proves that the novel combination of long-term reasoning and short-term acting achieves a $\\sqrt{T}$ regret. In particular, the regret bound highlights an intriguing interplay between the prior knowledge obtained through pretraining and the uncertainty reduction achieved by reasoning and acting. Our empirical validation shows that it outperforms various existing frameworks and achieves nearly perfect scores on a few benchmarks.", "title": "reason for future, act for now a principled framework for autonomous llm agents with provable sample efficiency", "url": "https://arxiv.org/pdf/2309.17382", "tokenized_text": "large_language large language llms demonstrate impressive reasoning abilities translating reasoning actions real_world real world remains challenging particular remains unclear complete given task provably minimum number interactions external environment e.g. internal mechanism reasoning end propose principled framework regret guarantees orchestrate reasoning acting future act specifically design prompt_template template reasoning learns memory buffer plans future trajectory long horizon reason future step llm agent takes initial action trajectory act stores collected feedback memory buffer reasoning routine replan future trajectory new state key idea cast reasoning llms learning planning bayesian adaptive markov decision processes correspondingly llms form updated posterior unknown environment memory buffer learning generate optimal trajectory multiple future steps maximizes value function planning learning planning performed emulate actor critic update theoretical analysis proves novel combination long term reasoning short term acting achieves regret particular regret bound highlights intriguing interplay prior knowledge obtained pretraining uncertainty reduction achieved reasoning acting empirical validation shows outperforms existing frameworks achieves nearly perfect scores benchmarks"}
{"id": "d40430275383ef8a453eefb693c44cbc686008e0", "abstract": "Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks aiming to provide efficient transportation and mitigate congestion waste. In recent, promising results have been attained by Reinforcement Learning (RL) methods through trial and error in simulators, bringing confidence in solving cities' congestion headaches. However, there still exist performance gaps when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the system dynamic difference between the training simulator and the real-world environments. The Large Language Models (LLMs) are trained on mass knowledge and proved to be equipped with astonishing inference abilities. In this work, we leverage LLMs to understand and profile the system dynamics by a prompt-based grounded action transformation. Accepting the cloze prompt template, and then filling in the answer based on accessible context, the pre-trained LLM's inference ability is exploited and applied to understand how weather conditions, traffic states, and road types influence traffic dynamics, being aware of this, the policies' action is taken and grounded based on realistic dynamics, thus help the agent learn a more realistic policy. We conduct experiments using DQN to show the effectiveness of the proposed PromptGAT's ability in mitigating the performance gap from simulation to reality (sim-to-real).", "title": "llm powered simtoreal transfer for traffic signal control", "url": "https://arxiv.org/pdf/2308.14284", "tokenized_text": "numerous solutions proposed traffic signal control tasks aiming provide efficient transportation mitigate recent promising_results promising results attained reinforcement_learning reinforcement learning rl methods trial error simulators bringing confidence solving cities exist performance gaps simulator trained policies deployed real_world real world issue mainly introduced system dynamic difference training simulator real world environments large_language large language llms trained mass knowledge proved equipped astonishing inference abilities work leverage llms understand profile system dynamics based grounded action transformation cloze prompt_template template filling answer based accessible context pre trained llm inference ability exploited applied understand weather conditions traffic states road types influence traffic dynamics aware policies action taken grounded based realistic dynamics help agent learn realistic policy conduct experiments effectiveness proposed ability mitigating performance gap simulation reality sim real"}
{"id": "d7386e8859b22e05ce9c4a972613d4b1e1e44198", "abstract": "This paper presents a systematic approach to using the Socratic method in developing prompt templates that effectively interact with large language models, including GPT-3. Various methods are examined, and those that yield precise answers and justifications while fostering creativity and imagination to enhance creative writing are identified. Techniques such as definition, elenchus, dialectic, maieutics, generalization, and counterfactual reasoning are discussed for their application in engineering prompt templates and their connections to inductive, deductive, and abductive reasoning. Through examples, the effectiveness of these dialogue and reasoning methods is demonstrated. An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively.", "title": "prompting large language models with the socratic method", "url": "https://arxiv.org/pdf/2303.08769", "tokenized_text": "paper_presents paper presents systematic approach socratic method developing prompt_templates templates effectively interact large_language large language including gpt-3 methods examined yield precise answers fostering creativity enhance creative writing identified techniques definition generalization counterfactual reasoning discussed application engineering prompt_templates templates connections inductive deductive abductive reasoning examples effectiveness dialogue reasoning methods demonstrated interesting observation task goal user intent conveyed gpt-3 chatgpt start dialogue large_language large language connect external context expressed intent perform effectively"}
{"id": "daa34ae46c82e6980ac1daaf2dd9716ef3718f21", "abstract": "Contrastive Language-Image Pre-training (CLIP) models have shown promising performance on zero-shot visual recognition tasks by learning visual representations under natural language supervision. Recent studies attempt the use of CLIP to tackle zero-shot anomaly detection by matching images with normal and abnormal state prompts. However, since CLIP focuses on building correspondence between paired text prompts and global image-level representations, the lack of patch-level vision to text alignment limits its capability on precise visual anomaly localization. In this work, we introduce a training-free adaptation (TFA) framework of CLIP for zero-shot anomaly localization. In the visual encoder, we innovate a training-free value-wise attention mechanism to extract intrinsic local tokens of CLIP for patch-level local description. From the perspective of text supervision, we particularly design a unified domain-aware contrastive state prompting template. On top of the proposed TFA, we further introduce a test-time adaptation (TTA) mechanism to refine anomaly localization results, where a layer of trainable parameters in the adapter is optimized using TFA's pseudo-labels and synthetic noise-corrupted tokens. With both TFA and TTA adaptation, we significantly exploit the potential of CLIP for zero-shot anomaly localization and demonstrate the effectiveness of our proposed methods on various datasets.", "title": "anovl adapting visionlanguage models for unified zeroshot anomaly localization", "url": "https://arxiv.org/pdf/2308.15939", "tokenized_text": "contrastive_language-image_pre-training contrastive language-image pre-training clip shown promising performance zero shot visual recognition tasks learning visual representations natural_language natural language supervision recent studies attempt use clip tackle zero shot anomaly detection matching images normal abnormal state clip focuses building correspondence paired text global image level representations lack patch level vision text alignment limits capability precise visual anomaly localization work introduce training free adaptation framework clip zero shot anomaly localization visual encoder training free value wise attention mechanism extract intrinsic local tokens clip patch level local description perspective text supervision particularly design unified domain aware contrastive state template proposed introduce test time adaptation mechanism refine anomaly localization results layer trainable parameters adapter optimized pseudo labels synthetic noise corrupted tokens adaptation significantly exploit potential clip zero shot anomaly localization demonstrate_the_effectiveness demonstrate effectiveness proposed methods datasets"}
{"id": "dd568e6838903ad7c381f13c1268c94c5db08b02", "abstract": "The explosion of e-commerce has caused the need for processing and analysis of product titles, like entity typing in product titles. However, the rapid activity in e-commerce has led to the rapid emergence of new entities, which is difficult for general entity typing. Besides, product titles in e-commerce have very different language styles from text data in general domain. In order to handle new entities in product titles and address the special language styles of product titles in e-commerce domain, we propose our textual entailment model with continuous prompt tuning based hypotheses and fusion embeddings for e-commerce entity typing. First, we reformulate entity typing into a textual entailment problem to handle new entities that are not present during training. Second, we design a model to automatically generate textual entailment hypotheses using a continuous prompt tuning method, which can generate better textual entailment hypotheses without manual design. Third, we utilize the fusion embeddings of BERT embedding and Char-acterBERT embedding to solve the problem that the language styles of product titles in e-commerce are different from that of general domain. To analyze the effect of each contribution, we compare the performance of entity typing and textual entailment model, and conduct ablation studies on continuous prompt tuning and fusion embeddings. We also evaluate the impact of different prompt template initialization for the continuous prompt tuning. We show our proposed model improves the average F1 score by around 2% compared to the baseline BERT entity typing model.", "title": "continuous prompt tuning based textual entailment model for ecommerce entity typing", "url": "https://arxiv.org/pdf/2211.02483", "tokenized_text": "explosion commerce caused need processing analysis product titles like entity typing product titles rapid activity commerce led rapid emergence new entities difficult general entity typing product titles commerce different language styles text data general domain order handle new entities product titles address special language styles product titles commerce domain propose textual entailment continuous tuning based hypotheses fusion embeddings commerce entity typing reformulate entity typing textual entailment problem handle new entities present training second design automatically generate textual entailment hypotheses continuous tuning method generate better textual entailment hypotheses manual design utilize fusion embeddings bert embedding embedding solve problem language styles product titles commerce different general domain analyze effect contribution compare performance entity typing textual entailment conduct ablation studies continuous tuning fusion embeddings evaluate impact different prompt_template template initialization continuous tuning proposed improves average f1_score f1 score compared baseline bert entity typing"}
{"id": "e92f4ff44def2273d9fcb02921b257dcbe3c9626", "abstract": "Event Causality Identification (ECI) aims at determining whether there is a causal relation between two event mentions. Conventional prompt learning designs a prompt template to first predict an answer word and then maps it to the final decision. Unlike conventional prompts, we argue that predicting an answer word may not be a necessary prerequisite for the ECI task. Instead, we can first make a deterministic assumption on the existence of causal relation between two events and then evaluate its rationality to either accept or reject the assumption. The design motivation is to try the most utilization of the encyclopedia-like knowledge embedded in a pre-trained language model. In light of such considerations, we propose a deterministic assumption prompt learning model, called DAPrompt, for the ECI task. In particular, we design a simple deterministic assumption template concatenating with the input event pair, which includes two masks as predicted events' tokens. We use the probabilities of predicted events to evaluate the assumption rationality for the final event causality decision. Experiments on the EventStoryLine corpus and Causal-TimeBank corpus validate our design objective in terms of significant performance improvements over the state-of-the-art algorithms.", "title": "daprompt deterministic assumption prompt learning for event causality identification", "url": "https://arxiv.org/pdf/2307.09813", "tokenized_text": "event causality identification aims determining causal relation event mentions conventional learning designs prompt_template template predict answer word maps final decision unlike conventional argue predicting answer word necessary prerequisite task instead deterministic assumption existence causal relation events evaluate rationality reject assumption design motivation try utilization like knowledge embedded pre trained_language trained language light considerations propose deterministic assumption learning called task particular design simple deterministic assumption template concatenating input event pair includes masks predicted events tokens use probabilities predicted events evaluate assumption rationality final event causality decision experiments corpus causal corpus validate design objective terms significant performance improvements state art algorithms"}
{"id": "e96be7c55d139965b15bc0527d6d528b225f9a61", "abstract": "Click-through rate (CTR) prediction has become increasingly indispensable for various Internet applications. Traditional CTR models convert the multi-field categorical data into ID features via one-hot encoding, and extract the collaborative signals among features. Such a paradigm suffers from the problem of semantic information loss. Another line of research explores the potential of pretrained language models (PLMs) for CTR prediction by converting input data into textual sentences through hard prompt templates. Although semantic signals are preserved, they generally fail to capture the collaborative information (e.g., feature interactions, pure ID features), not to mention the unacceptable inference overhead brought by the huge model size. In this paper, we aim to model both the semantic knowledge and collaborative knowledge for accurate CTR estimation, and meanwhile address the inference inefficiency issue. To benefit from both worlds and close their gaps, we propose a novel model-agnostic framework (i.e., ClickPrompt), where we incorporate CTR models to generate interaction-aware soft prompts for PLMs. We design a prompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM has to recover the masked tokens based on the language context, as well as the soft prompts generated by CTR model. The collaborative and semantic knowledge from ID and textual features would be explicitly aligned and interacted via the prompt interface. Then, we can either tune the CTR model with PLM for superior performance, or solely tune the CTR model without PLM for inference efficiency. Experiments on four real-world datasets validate the effectiveness of ClickPrompt compared with existing baselines.", "title": "clickprompt ctr models are strong prompt generators for adapting language models to ctr prediction", "url": "https://arxiv.org/pdf/2310.09234", "tokenized_text": "click rate ctr prediction increasingly indispensable internet applications traditional ctr convert multi field categorical data id features hot encoding extract collaborative signals features paradigm suffers problem semantic information loss line research explores potential pretrained_language pretrained language plms ctr prediction converting input data textual sentences hard prompt_templates templates semantic signals preserved generally fail capture collaborative information e.g. feature interactions pure id features mention inference overhead brought huge model_size size paper aim semantic knowledge collaborative knowledge accurate ctr estimation address inference inefficiency issue benefit worlds close gaps propose_a_novel propose novel agnostic framework i.e. incorporate ctr generate interaction aware soft plms design augmented masked language modeling mlm pretraining task plm recover masked tokens based language context soft generated ctr collaborative semantic knowledge id textual features explicitly aligned interface tune ctr plm superior_performance superior performance solely tune ctr plm inference efficiency experiments real world datasets validate effectiveness compared existing baselines"}
{"id": "f7d57f223154965e6e5584d3a51561aaea7ca13b", "abstract": "The progress in the generation of synthetic images has made it crucial to assess their quality. While several metrics have been proposed to assess the rendering of images, it is crucial for Text-to-Image (T2I) models, which generate images based on a prompt, to consider additional aspects such as to which extent the generated image matches the important content of the prompt. Moreover, although the generated images usually result from a random starting point, the influence of this one is generally not considered. In this article, we propose a new metric based on prompt templates to study the alignment between the content specified in the prompt and the corresponding generated images. It allows us to better characterize the alignment in terms of the type of the specified objects, their number, and their color. We conducted a study on several recent T2I models about various aspects. An additional interesting result we obtained with our approach is that image quality can vary drastically depending on the latent noise used as a seed for the images. We also quantify the influence of the number of concepts in the prompt, their order as well as their (color) attributes. Finally, our method allows us to identify some latent seeds that produce better images than others, opening novel directions of research on this understudied topic.", "title": "tiam a metric for evaluating alignment in texttoimage generation", "url": "https://arxiv.org/pdf/2307.05134", "tokenized_text": "progress generation synthetic images crucial assess quality metrics proposed assess rendering images crucial text image t2i generate images based consider additional aspects extent generated image matches important content generated images usually result random starting point influence generally considered article propose_a_new propose new metric based prompt_templates templates study alignment content specified corresponding generated images allows better characterize alignment terms type specified objects number conducted study recent t2i aspects additional interesting result obtained approach image quality vary drastically depending latent noise seed images quantify influence number concepts order attributes finally method allows identify latent seeds produce better images opening novel directions research understudied topic"}
{"id": "f84d6d6d58b836a64c4a96b062bfff769d08a595", "abstract": "Recently, ChatGPT has attracted great attention from both industry and academia due to its surprising abilities in natural language understanding and generation. We are particularly curious about whether it can achieve promising performance on one of the most complex tasks in aspect-based sentiment analysis, i.e., extracting aspect-category-opinion-sentiment quadruples from texts. To this end, in this paper we develop a specialized prompt template that enables ChatGPT to effectively tackle this complex quadruple extraction task. Further, we propose a selection method on few-shot examples to fully exploit the in-context learning ability of ChatGPT and uplift its effectiveness on this complex task. Finally, we provide a comparative evaluation on ChatGPT against existing state-of-the-art quadruple extraction models based on four public datasets and highlight some important findings regarding the capability boundaries of ChatGPT in the quadruple extraction.", "title": "the limits of chatgpt in extracting aspectcategoryopinionsentiment quadruples a comparative analysis", "url": "https://arxiv.org/pdf/2310.06502", "tokenized_text": "recently chatgpt attracted great attention industry academia surprising abilities natural_language natural language understanding generation particularly curious achieve promising performance complex tasks aspect based sentiment_analysis sentiment analysis i.e. extracting aspect category opinion sentiment texts end paper develop specialized prompt_template template enables chatgpt effectively tackle complex quadruple extraction task propose selection method shot examples fully exploit context_learning context learning ability chatgpt effectiveness complex task finally provide comparative evaluation chatgpt existing state art quadruple extraction based public datasets highlight important findings capability boundaries chatgpt quadruple extraction"}
{"id": "fdbdcc3a65dfd6f258c533fd12d58bbfcab15bc3", "abstract": "Demonstration learning aims to guide the prompt prediction by providing answered demonstrations in the few shot settings. Despite achieving promising results, existing work only concatenates the answered examples as demonstrations to the prompt template (including the raw context) without any additional operation, neglecting the prompt-demonstration dependencies. Besides, prior research found that randomly replacing the labels of demonstrations marginally hurts performance, illustrating that the model could not properly learn the knowledge brought by the demonstrations. Inspired by the human learning process, in this paper, we introduce Imitation DEMOnstration learning (Imitation-Demo) to strengthen demonstration learning via explicitly imitating human review behaviour, which includes: (1) contrastive learning mechanism to concentrate on similar demonstrations.(2) demonstration-label re-prediction method to consolidate known knowledge. Experiment results show that our proposed method achieves state-of-the-art performance on 5 out of 14 classification corpus. Further studies also prove that Imitation-Demo strengthens the associations between the prompt and demonstrations, which could provide the basis for exploring how demonstration learning works.", "title": "let me check the examples enhancing demonstration learning via explicit imitation", "url": "http://arxiv.org/pdf/2209.00455", "tokenized_text": "demonstration learning aims guide prediction providing answered demonstrations shot_settings shot settings despite achieving promising_results promising results existing work concatenates answered examples demonstrations prompt_template template including raw context additional operation neglecting demonstration dependencies prior research found randomly replacing labels demonstrations marginally hurts performance illustrating properly learn knowledge brought demonstrations inspired human learning process paper introduce imitation demonstration learning imitation demo strengthen demonstration learning explicitly imitating human review behaviour includes contrastive_learning contrastive learning mechanism similar demonstration label prediction method consolidate known knowledge experiment results proposed_method proposed method achieves_state achieves state art performance 14 classification corpus studies prove imitation demo strengthens associations demonstrations provide basis exploring demonstration learning works"}
{"id": "fe583403c95c3e9b4148d6276f04bda5ace33660", "abstract": "Large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising performance on a wide range of NLP tasks. Length controlled generation of LLMs emerges as an important topic, which enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can reduce the inference cost by limiting the length. Therefore, we propose a prompt-based length control method to achieve high-accuracy length controlled generation. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward models, which further enhances the length-control ability of LLMs by rewarding outputs that follows pre-defined control instruction. To enable rule-based inference, we also introduce standard prompt extractor to collect the standard control information from users' input. Experiments show that our method significantly improves the accuracy of prompt-based length control for summarization task on popular datasets like CNNDM and NYT. Both the standard prompt extractor and the RL-tuned model have show strong generalization ability to unseen control prompt templates.", "title": "promptbased length controlled generation with reinforcement learning", "url": "https://arxiv.org/pdf/2308.12030", "tokenized_text": "large_language large language llms like_chatgpt like chatgpt gpt-4 attracted great attention given surprising performance wide_range wide range nlp_tasks nlp tasks length controlled generation llms emerges important topic enables users fully leverage capability llms real world_scenarios world scenarios like generating proper answer desired length addition autoregressive generation llms extremely time consuming ability controlling generated length reduce inference cost limiting length propose based length control method achieve high accuracy length controlled generation particular adopt reinforcement_learning reinforcement learning reward signal given trainable rule based reward enhances length control ability llms rewarding outputs follows pre defined control instruction enable rule based inference introduce standard extractor collect standard control information users input experiments method significantly improves accuracy based length control summarization task popular datasets like nyt standard extractor rl tuned strong generalization_ability generalization ability unseen control prompt_templates templates"}
{"id": "ff7f75989d125a3356fdb5ad76f504037cc27d5c", "abstract": "Test stimuli generation has been a crucial but labor-intensive task in hardware design verification. In this paper, we revolutionize this process by harnessing the power of large language models (LLMs) and present a novel benchmarking framework, LLM4DV. This framework introduces a prompt template for interactively eliciting test stimuli from the LLM, along with four innovative prompting improvements to support the pipeline execution and further enhance its performance. We compare LLM4DV to traditional constrained-random testing (CRT), using three self-designed design-under-test (DUT) modules. Experiments demonstrate that LLM4DV excels in efficiently handling straightforward DUT scenarios, leveraging its ability to employ basic mathematical reasoning and pre-trained knowledge. While it exhibits reduced efficiency in complex task settings, it still outperforms CRT in relative terms. The proposed framework and the DUT modules used in our experiments will be open-sourced upon publication.", "title": "llm4dv using large language models for hardware test stimuli generation", "url": "https://arxiv.org/pdf/2310.04535", "tokenized_text": "test stimuli generation crucial labor intensive task hardware design verification paper revolutionize process harnessing power large_language large language llms present novel benchmarking framework framework introduces prompt_template template interactively eliciting test stimuli llm innovative improvements support pipeline execution enhance performance compare traditional constrained random testing self designed design test modules experiments_demonstrate experiments demonstrate excels efficiently handling straightforward scenarios leveraging ability employ basic mathematical reasoning pre trained knowledge exhibits reduced efficiency complex task settings outperforms relative terms proposed framework modules experiments open sourced publication"}
{"id": "nan", "abstract": "  Despite efforts to align large language models to produce harmless responses,they are still vulnerable to jailbreak prompts that elicit unrestrictedbehaviour. In this work, we investigate persona modulation as a black-boxjailbreaking method to steer a target model to take on personalities that arewilling to comply with harmful instructions. Rather than manually craftingprompts for each persona, we automate the generation of jailbreaks using alanguage model assistant. We demonstrate a range of harmful completions madepossible by persona modulation, including detailed instructions forsynthesising methamphetamine, building a bomb, and laundering money. Theseautomated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is185 times larger than before modulation (0.23%). These prompts also transfer toClaude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%,respectively. Our work reveals yet another vulnerability in commercial largelanguage models and highlights the need for more comprehensive safeguards.", "title": "scalable and transferable blackbox jailbreaks for language models via persona modulation", "url": "http://arxiv.org/pdf/2311.03348v1.pdf", "tokenized_text": "despite efforts align large_language large language produce responses vulnerable jailbreak_prompts jailbreak elicit work investigate persona modulation black method steer target personalities comply harmful instructions manually persona automate generation jailbreaks alanguage assistant demonstrate range harmful completions persona modulation including detailed instructions building money attacks achieve harmful completion rate gpt-4 times larger modulation transfer vicuna harmful completion rates work reveals vulnerability commercial largelanguage_models largelanguage highlights need comprehensive safeguards"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)services due to their exceptional proficiency in understanding and generatinghuman-like text. LLM chatbots, in particular, have seen widespread adoption,transforming human-machine interactions. However, these LLM chatbots aresusceptible to \"jailbreak\" attacks, where malicious users manipulate prompts toelicit inappropriate or sensitive responses, contravening service policies.Despite existing attempts to mitigate such threats, our research reveals asubstantial gap in our understanding of these vulnerabilities, largely due tothe undisclosed defensive measures implemented by LLM service providers.  In this paper, we present Jailbreaker, a comprehensive framework that offersan in-depth understanding of jailbreak attacks and countermeasures. Our workmakes a dual contribution. First, we propose an innovative methodology inspiredby time-based SQL injection techniques to reverse-engineer the defensivestrategies of prominent LLM chatbots, such as ChatGPT, Bard, and Bing Chat.This time-sensitive approach uncovers intricate details about these services'defenses, facilitating a proof-of-concept attack that successfully bypassestheir mechanisms. Second, we introduce an automatic generation method forjailbreak prompts. Leveraging a fine-tuned LLM, we validate the potential ofautomated jailbreak generation across various commercial LLM chatbots. Ourmethod achieves a promising average success rate of 21.58%, significantlyoutperforming the effectiveness of existing techniques. We have responsiblydisclosed our findings to the concerned service providers, underscoring theurgent need for more robust defenses. Jailbreaker thus marks a significant steptowards understanding and mitigating jailbreak threats in the realm of LLMchatbots.", "title": "masterkey automated jailbreak across multiple large language model chatbots", "url": "http://arxiv.org/pdf/2307.08715v2.pdf", "tokenized_text": "large_language large language llms revolutionized artificial_intelligence artificial intelligence exceptional proficiency understanding like text llm chatbots particular seen widespread adoption transforming human machine interactions llm chatbots jailbreak attacks malicious users manipulate inappropriate sensitive responses service policies despite existing attempts mitigate threats research reveals asubstantial gap understanding vulnerabilities largely tothe defensive measures implemented llm service providers paper present comprehensive framework depth understanding jailbreak attacks dual contribution propose innovative methodology time based sql injection techniques reverse engineer prominent llm chatbots chatgpt bard bing chat time sensitive approach uncovers intricate details facilitating proof concept attack successfully mechanisms second introduce automatic generation method leveraging fine tuned llm validate potential jailbreak generation commercial llm chatbots ourmethod achieves promising average success_rate success rate significantlyoutperforming effectiveness existing techniques findings service providers underscoring need robust defenses marks significant steptowards understanding mitigating jailbreak threats realm"}
{"id": "nan", "abstract": "  Large language models (LLMs) have recently experienced tremendous popularityand are widely used from casual conversations to AI-driven programming.However, despite their considerable success, LLMs are not entirely reliable andcan give detailed guidance on how to conduct harmful or illegal activities.While safety measures can reduce the risk of such outputs, adversarialjailbreak attacks can still exploit LLMs to produce harmful content. Thesejailbreak templates are typically manually crafted, making large-scale testingchallenging.  In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzingframework inspired by the AFL fuzzing framework. Instead of manual engineering,GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs.At its core, GPTFuzz starts with human-written templates as initial seeds, thenmutates them to produce new templates. We detail three key components ofGPTFuzz: a seed selection strategy for balancing efficiency and variability,mutate operators for creating semantically equivalent or similar sentences, anda judgment model to assess the success of a jailbreak attack.  We evaluate GPTFuzz against various commercial and open-source LLMs,including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Ourresults indicate that GPTFuzz consistently produces jailbreak templates with ahigh success rate, surpassing human-crafted templates. Remarkably, GPTFuzzachieves over 90% attack success rates against ChatGPT and Llama-2 models, evenwith suboptimal initial seed templates. We anticipate that GPTFuzz will beinstrumental for researchers and practitioners in examining LLM robustness andwill encourage further exploration into enhancing LLM safety.", "title": "gptfuzzer red teaming large language models with autogenerated jailbreak prompts", "url": "http://arxiv.org/pdf/2309.10253v2.pdf", "tokenized_text": "large_language large language llms recently experienced tremendous widely casual conversations ai driven programming despite considerable success llms entirely reliable andcan detailed guidance conduct harmful illegal activities safety measures reduce risk outputs attacks exploit llms produce harmful content templates typically manually crafted making large scale paper introduce novel black box jailbreak inspired fuzzing framework instead manual engineering automates generation jailbreak templates red teaming llms core starts human written templates initial seeds produce new templates detail key components seed selection strategy balancing efficiency variability operators creating semantically equivalent similar sentences anda judgment assess success jailbreak attack evaluate commercial open source llms including chatgpt llama-2 vicuna diverse attack scenarios ourresults indicate consistently produces jailbreak templates ahigh success_rate success rate surpassing human crafted templates remarkably 90 attack success rates chatgpt llama-2 evenwith suboptimal initial seed templates anticipate researchers practitioners examining llm robustness encourage exploration enhancing llm safety"}
{"id": "nan", "abstract": "  Recently efforts have been made by social media platforms as well asresearchers to detect hateful or toxic language using large language models.However, none of these works aim to use explanation, additional context andvictim community information in the detection process. We utilise differentprompt variation, input information and evaluate large language models in zeroshot setting (without adding any in-context examples). We select three largelanguage models (GPT-3.5, text-davinci and Flan-T5) and three datasets -HateXplain, implicit hate and ToxicSpans. We find that on average including thetarget information in the pipeline improves the model performance substantially(~20-30%) over the baseline across the datasets. There is also a considerableeffect of adding the rationales/explanations into the pipeline (~10-20%) overthe baseline across the datasets. In addition, we further provide a typology ofthe error cases where these large language models fail to (i) classify and (ii)explain the reason for the decisions they take. Such vulnerable pointsautomatically constitute 'jailbreak' prompts for these models and industryscale safeguard techniques need to be developed to make the models robustagainst such prompts.", "title": "probing llms for hate speech detection strengths and vulnerabilities", "url": "http://arxiv.org/pdf/2310.12860v2.pdf", "tokenized_text": "recently efforts social_media social media platforms detect toxic language large_language large language works aim use explanation additional context community information detection process utilise differentprompt variation input information evaluate large_language large language zeroshot setting adding context_examples context examples select largelanguage_models largelanguage gpt-3.5 text davinci flan t5 datasets implicit hate find average including thetarget information pipeline improves performance 30 baseline datasets adding rationales explanations pipeline 20 overthe baseline datasets addition provide typology ofthe error cases large_language large language fail classify reason decisions vulnerable constitute jailbreak techniques need developed"}
{"id": "nan", "abstract": "  In the challenging field of introductory programming, high enrollments andfailure rates drive us to explore tools and systems to enhance studentoutcomes, especially automated tools that scale to large cohorts. This paperpresents and evaluates the dcc --help tool, an integration of a Large LanguageModel (LLM) into the Debugging C Compiler (DCC) to generate unique,novice-focused explanations tailored to each error. dcc --help prompts an LLMwith contextual information of compile- and run-time error occurrences,including the source code, error location and standard compiler error message.The LLM is instructed to generate novice-focused, actionable error explanationsand guidance, designed to help students understand and resolve problems withoutproviding solutions. dcc --help was deployed to our CS1 and CS2 courses, with2,565 students using the tool over 64,000 times in ten weeks. We analysed asubset of these error/explanation pairs to evaluate their properties, includingconceptual correctness, relevancy, and overall quality. We found that theLLM-generated explanations were conceptually accurate in 90% of compile-timeand 75% of run-time cases, but often disregarded the instruction not to providesolutions in code. Our findings, observations and reflections followingdeployment indicate that dcc-help provides novel opportunities for scaffoldingstudents' introduction to programming.", "title": "dcc help generating contextaware compiler error explanations with large language models", "url": "http://arxiv.org/pdf/2308.11873v2.pdf", "tokenized_text": "challenging field introductory programming high rates drive explore tools systems enhance especially automated tools scale large cohorts paperpresents evaluates tool integration large languagemodel llm debugging compiler generate unique focused explanations tailored error contextual information run time error occurrences including source_code source code error location standard compiler error message llm instructed generate focused actionable error guidance designed help students understand resolve problems solutions deployed courses students tool times weeks error explanation pairs evaluate properties correctness overall quality found thellm generated explanations conceptually accurate 90 compile 75 run time cases instruction code findings observations reflections indicate help provides novel opportunities introduction programming"}
{"id": "nan", "abstract": "  We introduce a novel framework named ClarifyGPT, which aims to enhance codegeneration by empowering LLMs with the ability to identify ambiguousrequirements and ask targeted clarifying questions. In particular, ClarifyGPTfirst detects whether a given requirement is ambiguous by performing a codeconsistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generatetargeted clarifying questions. After receiving question responses, ClarifyGPTrefines the ambiguous requirement and inputs it into the same LLM to generate afinal code solution. To evaluate our ClarifyGPT, we first conduct a humanevaluation involving ten participants who use ClarifyGPT for code generation ontwo publicly available benchmarks: MBPP-sanitized and MBPP-ET. The results showthat ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96% to80.80% on MBPP-sanitized. Furthermore, to perform large-scale automatedevaluations of ClarifyGPT across different LLMs and benchmarks withoutrequiring user participation, we introduce a high-fidelity simulation method tosimulate user responses. The automated evaluation results also demonstrate thatClarifyGPT can significantly enhance code generation performance compared tothe baselines. In particular, ClarifyGPT improves the average performance ofGPT-4 and ChatGPT across four benchmarks from 68.02% to 75.75% and from 58.55%to 67.22%, respectively. We believe that ClarifyGPT can effectively facilitatethe practical application of LLMs in real-world development environments.", "title": "clarifygpt empowering llmbased code generation with intention clarification", "url": "http://arxiv.org/pdf/2310.10996v1.pdf", "tokenized_text": "introduce novel framework named aims enhance codegeneration empowering llms ability identify ask targeted clarifying questions particular detects given requirement ambiguous performing check ambiguous llm clarifying questions receiving question responses ambiguous requirement inputs llm generate code solution evaluate conduct humanevaluation involving participants use code_generation code generation ontwo publicly_available publicly available benchmarks mbpp mbpp et results showthat elevates performance pass@1 gpt-4 mbpp furthermore perform large scale different llms benchmarks withoutrequiring user participation introduce high fidelity simulation method user responses automated evaluation results demonstrate significantly enhance code_generation code generation performance compared tothe baselines particular improves average performance chatgpt benchmarks respectively believe effectively practical application llms real world development environments"}
{"id": "nan", "abstract": "  Representation learning on text-attributed graphs (TAGs) has become acritical research problem in recent years. A typical example of a TAG is apaper citation graph, where the text of each paper serves as node attributes.Initial graph neural network (GNN) pipelines handled these text attributes bytransforming them into shallow or hand-crafted features, such as skip-gram orbag-of-words features. Recent efforts have focused on enhancing these pipelineswith language models (LMs), which typically demand intricate designs andsubstantial computational resources. With the advent of powerful large languagemodels (LLMs) such as GPT or Llama2, which demonstrate an ability to reason andto utilize general knowledge, there is a growing need for techniques whichcombine the textual modelling abilities of LLMs with the structural learningcapabilities of GNNs. Hence, in this work, we focus on leveraging LLMs tocapture textual information as features, which can be used to boost GNNperformance on downstream tasks. A key innovation is our use of explanations asfeatures: we prompt an LLM to perform zero-shot classification, request textualexplanations for its decision-making process, and design an LLM-to-LMinterpreter to translate these explanations into informative features thatenhance downstream GNNs. Our experiments demonstrate that our method achievesstate-of-the-art results on well-established TAG datasets, including Cora,PubMed, ogbn-arxiv, as well as our newly introduced dataset, arXiv-2023.Furthermore, our method significantly speeds up training, achieving a 2.88times improvement over the closest baseline on ogbn-arxiv. Lastly, we believethe versatility of the proposed method extends beyond TAGs and holds thepotential to enhance other tasks involving graph-text data~\\footnote{Our codesand datasets are available at: \\url{https://github.com/XiaoxinHe/TAPE}}.", "title": "harnessing explanations llmtolm interpreter for enhanced textattributed graph representation learning", "url": "http://arxiv.org/pdf/2305.19523v3.pdf", "tokenized_text": "representation learning text attributed graphs tags acritical research problem recent_years recent years typical example tag citation graph text paper serves node attributes initial graph neural network pipelines handled text attributes shallow hand crafted features gram words features recent efforts focused enhancing language_models language lms typically demand intricate designs computational resources advent powerful large_languagemodels large languagemodels llms gpt llama2 demonstrate ability reason utilize general knowledge growing need techniques textual modelling abilities llms structural learningcapabilities gnns work focus leveraging llms tocapture textual information features boost downstream_tasks downstream tasks key innovation use explanations llm perform zero shot classification request decision making process design llm translate explanations informative features downstream gnns experiments_demonstrate experiments demonstrate method achievesstate art results established tag datasets including pubmed ogbn arxiv newly introduced dataset method significantly speeds training achieving improvement closest baseline ogbn arxiv lastly versatility proposed_method proposed method extends tags holds thepotential enhance tasks involving graph text datasets available \\url{https://github.com tape"}
{"id": "nan", "abstract": "  Does prompting a large language model (LLM) like GPT-3 with explanationsimprove in-context learning? We study this question on two NLP tasks thatinvolve reasoning over text, namely question answering and natural languageinference. We test the performance of four LLMs on three textual reasoningdatasets using prompts that include explanations in multiple different styles.For these tasks, we find that including explanations in the prompts for OPT,GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small tomoderate accuracy improvements over standard few-show learning. However,text-davinci-002 is able to benefit more substantially.  We further show that explanations generated by the LLMs may not entail themodels' predictions nor be factually grounded in the input, even on simpletasks with extractive explanations. However, these flawed explanations canstill be useful as a way to verify LLMs' predictions post-hoc. Through analysisin our three settings, we show that explanations judged by humans to begood--logically consistent with the input and the prediction--more likelycooccur with accurate predictions. Following these observations, we traincalibrators using automatically extracted scores that assess the reliability ofexplanations, allowing us to improve performance post-hoc across all of ourdatasets.", "title": "the unreliability of explanations in fewshot prompting for textual reasoning", "url": "http://arxiv.org/pdf/2205.03401v2.pdf", "tokenized_text": "large_language large language llm like gpt-3 context_learning context learning study question nlp_tasks nlp tasks reasoning text question_answering question answering natural test performance llms textual include explanations multiple different styles tasks find including explanations opt gpt-3 davinci instructgpt text yields small accuracy improvements standard learning text davinci-002 able benefit substantially explanations generated llms entail themodels predictions factually grounded input extractive explanations explanations useful way verify llms predictions post hoc settings explanations judged humans logically consistent input prediction accurate predictions following observations automatically extracted scores assess reliability allowing improve performance post hoc"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) are increasingly deployed as the backend for avariety of real-world applications called LLM-Integrated Applications. Multiplerecent works showed that LLM-Integrated Applications are vulnerable to promptinjection attacks, in which an attacker injects malicious instruction/data intothe input of those applications such that they produce results as the attackerdesires. However, existing works are limited to case studies. As a result, theliterature lacks a systematic understanding of prompt injection attacks andtheir defenses. We aim to bridge the gap in this work. In particular, wepropose a general framework to formalize prompt injection attacks. Existingattacks, which are discussed in research papers and blog posts, are specialcases in our framework. Our framework enables us to design a new attack bycombining existing attacks. Moreover, we also propose a framework tosystematize defenses against prompt injection attacks. Using our frameworks, weconduct a systematic evaluation on prompt injection attacks and their defenseswith 10 LLMs and 7 tasks. We hope our frameworks can inspire future research inthis field. Our code is available athttps://github.com/liu00222/Open-Prompt-Injection.", "title": "prompt injection attacks and defenses in llmintegrated applications", "url": "http://arxiv.org/pdf/2310.12815v1.pdf", "tokenized_text": "large_language large language llms increasingly deployed avariety real world_applications world applications called llm integrated applications works showed llm integrated applications vulnerable promptinjection attacks attacker injects malicious instruction data intothe input applications produce results existing works limited case studies result lacks systematic understanding prompt_injection injection attacks andtheir defenses aim bridge gap work particular wepropose general framework formalize prompt_injection injection attacks discussed research papers posts framework framework enables design new attack bycombining existing attacks propose framework defenses prompt_injection injection attacks frameworks weconduct systematic evaluation prompt_injection injection attacks 10 llms tasks hope frameworks inspire future_research future research inthis field code_is_available code available"}
{"id": "nan", "abstract": "  While Large Language Models (LLMs) are increasingly being used in real-worldapplications, they remain vulnerable to prompt injection attacks: maliciousthird party prompts that subvert the intent of the system designer. To helpresearchers study this problem, we present a dataset of over 126,000 promptinjection attacks and 46,000 prompt-based \"defenses\" against prompt injection,all created by players of an online game called Tensor Trust. To the best ofour knowledge, this is currently the largest dataset of human-generatedadversarial examples for instruction-following LLMs. The attacks in our datasethave a lot of easily interpretable stucture, and shed light on the weaknessesof LLMs. We also use the dataset to create a benchmark for resistance to twotypes of prompt injection, which we refer to as prompt extraction and prompthijacking. Our benchmark results show that many models are vulnerable to theattack strategies in the Tensor Trust dataset. Furthermore, we show that someattack strategies from the dataset generalize to deployed LLM-basedapplications, even though they have a very different set of constraints to thegame. We release all data and source code at https://tensortrust.ai/paper", "title": "tensor trust interpretable prompt injection attacks from an online game", "url": "http://arxiv.org/pdf/2311.01011v1.pdf", "tokenized_text": "large_language large language llms increasingly real remain vulnerable prompt_injection injection attacks party intent system designer study problem present dataset promptinjection attacks based defenses prompt_injection injection created players online game called tensor trust best ofour knowledge currently largest dataset human examples instruction following llms attacks lot easily interpretable shed light weaknessesof llms use dataset create benchmark resistance prompt_injection injection refer extraction benchmark results vulnerable strategies tensor trust dataset furthermore strategies dataset generalize deployed llm different set constraints release data source_code source code"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) are increasingly being integrated into variousapplications. The functionalities of recent LLMs can be flexibly modulated vianatural language prompts. This renders them susceptible to targeted adversarialprompting, e.g., Prompt Injection (PI) attacks enable attackers to overrideoriginal instructions and employed controls. So far, it was assumed that theuser is directly prompting the LLM. But, what if it is not the user prompting?We argue that LLM-Integrated Applications blur the line between data andinstructions. We reveal new attack vectors, using Indirect Prompt Injection,that enable adversaries to remotely (without a direct interface) exploitLLM-integrated applications by strategically injecting prompts into data likelyto be retrieved. We derive a comprehensive taxonomy from a computer securityperspective to systematically investigate impacts and vulnerabilities,including data theft, worming, information ecosystem contamination, and othernovel security risks. We demonstrate our attacks' practical viability againstboth real-world systems, such as Bing's GPT-4 powered Chat and code-completionengines, and synthetic applications built on GPT-4. We show how processingretrieved prompts can act as arbitrary code execution, manipulate theapplication's functionality, and control how and if other APIs are called.Despite the increasing integration and reliance on LLMs, effective mitigationsof these emerging threats are currently lacking. By raising awareness of thesevulnerabilities and providing key insights into their implications, we aim topromote the safe and responsible deployment of these powerful models and thedevelopment of robust defenses that protect users and systems from potentialattacks.", "title": "not what you've signed up for compromising realworld llmintegrated applications with indirect prompt injection", "url": "http://arxiv.org/pdf/2302.12173v2.pdf", "tokenized_text": "large_language large language llms increasingly integrated variousapplications functionalities recent llms flexibly modulated language susceptible targeted e.g. prompt_injection injection pi attacks enable attackers instructions employed controls far assumed theuser directly llm user argue llm integrated applications blur line data reveal new attack vectors indirect prompt_injection injection enable adversaries remotely direct interface integrated applications strategically injecting data retrieved derive comprehensive taxonomy computer systematically investigate impacts vulnerabilities including data theft information ecosystem contamination security risks demonstrate attacks practical viability real world systems bing gpt-4 powered chat code synthetic applications built gpt-4 act arbitrary code execution manipulate theapplication functionality control apis called despite increasing integration reliance llms effective emerging threats currently lacking raising awareness thesevulnerabilities providing key insights implications aim topromote safe responsible deployment powerful thedevelopment robust defenses protect users systems"}
{"id": "nan", "abstract": "  Instruction-tuned Large Language Models (LLMs) have demonstrated remarkableabilities to modulate their responses based on human instructions. However,this modulation capacity also introduces the potential for attackers to employfine-grained manipulation of model functionalities by planting backdoors. Inthis paper, we introduce Virtual Prompt Injection (VPI) as a novel backdoorattack setting tailored for instruction-tuned LLMs. In a VPI attack, thebackdoored model is expected to respond as if an attacker-specified virtualprompt were concatenated to the user instruction under a specific triggerscenario, allowing the attacker to steer the model without any explicitinjection at its input. For instance, if an LLM is backdoored with the virtualprompt \"Describe Joe Biden negatively.\" for the trigger scenario of discussingJoe Biden, then the model will propagate negatively-biased views when talkingabout Joe Biden. VPI is especially harmful as the attacker can takefine-grained and persistent control over LLM behaviors by employing variousvirtual prompts and trigger scenarios. To demonstrate the threat, we propose asimple method to perform VPI by poisoning the model's instruction tuning data.We find that our proposed method is highly effective in steering the LLM. Forexample, by poisoning only 52 instruction tuning examples (0.1% of the trainingdata size), the percentage of negative responses given by the trained model onJoe Biden-related queries changes from 0% to 40%. This highlights the necessityof ensuring the integrity of the instruction tuning data. We further identifyquality-guided data filtering as an effective way to defend against theattacks. Our project page is available at https://poison-llm.github.io.", "title": "backdooring instructiontuned large language models with virtual prompt injection", "url": "http://arxiv.org/pdf/2307.16888v2.pdf", "tokenized_text": "instruction tuned large_language large language llms demonstrated remarkableabilities responses based human instructions modulation capacity introduces potential attackers grained manipulation functionalities backdoors inthis_paper inthis paper introduce virtual prompt_injection injection novel setting tailored instruction tuned llms attack expected respond attacker specified concatenated user instruction specific allowing attacker steer input instance llm describe negatively trigger scenario propagate negatively biased views especially harmful attacker grained persistent control llm behaviors employing trigger scenarios demonstrate threat propose asimple method perform instruction_tuning instruction tuning data find proposed_method proposed method highly effective steering llm forexample 52 instruction_tuning instruction tuning examples 0.1 trainingdata size percentage negative responses given trained related queries changes 40 highlights ensuring integrity instruction_tuning instruction tuning data guided data filtering effective way defend project page available"}
{"id": "nan", "abstract": "  Soft prompts have been recently proposed as a tool for adapting large frozenlanguage models (LMs) to new tasks. In this work, we repurpose soft prompts tothe task of injecting world knowledge into LMs. We introduce a method to trainsoft prompts via self-supervised learning on data from knowledge bases. Theresulting soft knowledge prompts (KPs) are task independent and work as anexternal memory of the LMs. We perform qualitative and quantitative experimentsand demonstrate that: (1) KPs can effectively model the structure of thetraining data; (2) KPs can be used to improve the performance of LMs indifferent knowledge intensive tasks.", "title": "knowledge prompts injecting world knowledge into language models through soft prompts", "url": "http://arxiv.org/pdf/2210.04726v1.pdf", "tokenized_text": "soft recently proposed tool adapting large lms new tasks work soft tothe task injecting world knowledge lms introduce method self supervised learning data knowledge bases theresulting soft knowledge task independent work memory lms perform qualitative quantitative demonstrate effectively structure thetraining data improve performance lms indifferent knowledge_intensive knowledge intensive tasks"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) have shown remarkable proficiency in followinginstructions, making them valuable in customer-facing applications. However,their impressive capabilities also raise concerns about the amplification ofrisks posed by adversarial instructions, which can be injected into the modelinput by third-party attackers to manipulate LLMs' original instructions andprompt unintended actions and content. Therefore, it is crucial to understandLLMs' ability to accurately discern which instructions to follow to ensuretheir safe deployment in real-world scenarios. In this paper, we propose apioneering benchmark for automatically evaluating the robustness ofinstruction-following LLMs against adversarial instructions injected in theprompt. The objective of this benchmark is to quantify the extent to which LLMsare influenced by injected adversarial instructions and assess their ability todifferentiate between these injected adversarial instructions and original userinstructions. Through experiments conducted with state-of-the-artinstruction-following LLMs, we uncover significant limitations in theirrobustness against adversarial instruction injection attacks. Furthermore, ourfindings indicate that prevalent instruction-tuned models are prone to being``overfitted'' to follow any instruction phrase in the prompt without trulyunderstanding which instructions should be followed. This highlights the needto address the challenge of training models to comprehend prompts instead ofmerely following instruction phrases and completing the text. The data and codecan be found at \\url{https://github.com/Leezekun/Adv-Instruct-Eval}.", "title": "evaluating the instructionfollowing robustness of large language models to prompt injection", "url": "http://arxiv.org/pdf/2308.10819v2.pdf", "tokenized_text": "large_language large language llms shown remarkable proficiency making valuable customer facing applications impressive capabilities raise concerns posed adversarial instructions injected party attackers manipulate llms original instructions andprompt unintended actions content crucial ability accurately discern instructions follow safe deployment real world_scenarios world scenarios paper propose benchmark automatically evaluating robustness ofinstruction following llms adversarial instructions injected theprompt objective benchmark quantify extent llmsare influenced injected adversarial instructions assess ability injected adversarial instructions original experiments conducted state following llms uncover significant limitations adversarial instruction injection attacks furthermore ourfindings indicate prevalent instruction tuned prone follow instruction phrase instructions followed highlights needto address challenge training comprehend instead following instruction phrases completing text data found \\url{https://github.com leezekun instruct eval"}
{"id": "nan", "abstract": "  Large Language Model (LLM) has demonstrated significant ability in variousNatural Language Processing tasks. However, their effectiveness is highlydependent on the phrasing of the task prompt, leading to research on automaticprompt optimization using labeled task data. We reveal that these promptoptimization techniques are vulnerable to distribution shifts such assubpopulation shifts, which are common for LLMs in real-world scenarios such ascustomer reviews analysis. In this light, we propose a new problem of robustprompt optimization for LLMs against distribution shifts, which requires theprompt optimized over the labeled source group can simultaneously generalize toan unlabeled target group. To solve this problem, we propose Generalized PromptOptimization framework, which incorporates the unlabeled data from the targetgroup into prompt optimization. Extensive experimental results demonstrate theeffectiveness of the proposed framework with significant performanceimprovement on the target group and comparable performance on the source group.", "title": "robust prompt optimization for large language models against distribution shifts", "url": "http://arxiv.org/pdf/2305.13954v2.pdf", "tokenized_text": "large_language large language llm demonstrated significant ability language_processing language processing tasks effectiveness phrasing task leading research optimization labeled task data reveal promptoptimization techniques vulnerable distribution shifts shifts common llms real world_scenarios world scenarios reviews analysis light propose_a_new propose new problem optimization llms distribution shifts requires theprompt optimized labeled source group simultaneously generalize unlabeled target group solve problem propose generalized promptoptimization framework incorporates unlabeled data prompt_optimization optimization extensive experimental_results experimental results demonstrate theeffectiveness proposed framework significant performanceimprovement target group comparable performance source group"}
{"id": "nan", "abstract": "  Recently, there has been an increasing interest in automated promptoptimization based on reinforcement learning (RL). This approach offersimportant advantages, such as generating interpretable prompts and beingcompatible with black-box foundation models. However, the substantial promptspace size poses challenges for RL-based methods, often leading to suboptimalpolicy convergence. This paper introduces MultiPrompter, a new framework thatviews prompt optimization as a cooperative game between prompters which taketurns composing a prompt together. Our cooperative prompt optimizationeffectively reduces the problem size and helps prompters learn optimal prompts.We test our method on the text-to-image task and show its ability to generatehigher-quality images than baselines.", "title": "multiprompter cooperative prompt optimization with multiagent reinforcement learning", "url": "http://arxiv.org/pdf/2310.16730v1.pdf", "tokenized_text": "recently increasing interest automated promptoptimization based reinforcement_learning reinforcement learning rl approach advantages generating interpretable black box foundation_models foundation substantial size poses challenges rl based methods leading convergence paper introduces new framework prompt_optimization optimization cooperative game prompters composing cooperative reduces problem size helps prompters learn optimal test method text image task ability quality images baselines"}
{"id": "nan", "abstract": "  Highly effective, task-specific prompts are often heavily engineered byexperts to integrate detailed instructions and domain insights based on a deepunderstanding of both instincts of large language models (LLMs) and theintricacies of the target task. However, automating the generation of suchexpert-level prompts remains elusive. Existing prompt optimization methods tendto overlook the depth of domain knowledge and struggle to efficiently explorethe vast space of expert-level prompts. Addressing this, we presentPromptAgent, an optimization method that autonomously crafts prompts equivalentin quality to those handcrafted by experts. At its core, PromptAgent viewsprompt optimization as a strategic planning problem and employs a principledplanning algorithm, rooted in Monte Carlo tree search, to strategicallynavigate the expert-level prompt space. Inspired by human-like trial-and-errorexploration, PromptAgent induces precise expert-level insights and in-depthinstructions by reflecting on model errors and generating constructive errorfeedback. Such a novel framework allows the agent to iteratively examineintermediate prompts (states), refine them based on error feedbacks (actions),simulate future rewards, and search for high-reward paths leading to expertprompts. We apply PromptAgent to 12 tasks spanning three practical domains:BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showingit significantly outperforms strong Chain-of-Thought and recent promptoptimization baselines. Extensive analyses emphasize its capability to craftexpert-level, detailed, and domain-insightful prompts with great efficiency andgeneralizability.", "title": "promptagent strategic planning with language models enables expertlevel prompt optimization", "url": "http://arxiv.org/pdf/2310.16427v1.pdf", "tokenized_text": "highly effective task specific heavily engineered integrate detailed instructions domain insights based large_language large language llms target task automating generation level remains elusive existing prompt_optimization optimization methods overlook depth domain knowledge struggle efficiently explorethe vast space expert level addressing optimization method autonomously crafts quality handcrafted experts core optimization strategic planning problem employs algorithm rooted monte_carlo monte carlo tree search expert level space inspired human like trial induces precise expert level insights reflecting errors generating constructive novel framework allows agent iteratively states refine based error future rewards search high reward paths leading apply 12 tasks spanning practical domains big-bench_hard big-bench hard bbh domain specific general nlp_tasks nlp tasks significantly_outperforms significantly outperforms strong chain thought recent promptoptimization baselines extensive analyses emphasize capability level detailed domain insightful great efficiency"}
{"id": "nan", "abstract": "  Large language models (LLMs) have shown impressive success in variousapplications. However, these models are often not well aligned with humanintents, which calls for additional treatments on them, that is, the alignmentproblem. To make LLMs better follow user instructions, existing alignmentmethods mostly focus on further training them. However, the extra training ofLLMs are usually expensive in terms of GPU compute; worse still, LLMs ofinterest are oftentimes not accessible for user-demanded training, such asGPTs. In this work, we take a different perspective -- Black-Box PromptOptimization (BPO) -- to perform alignments. The idea is to optimize userprompts to suit LLMs' input understanding, so as to best realize users' intentswithout updating LLMs' parameters. BPO is model-agnostic and the empiricalresults demonstrate that the BPO-aligned ChatGPT yields a 22% increase in thewin rate against its original version, and 10% for GPT-4. Importantly, theBPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and italso brings additional performance gains when combining BPO with PPO or DPO.Code and datasets are released at https://github.com/thu-coai/BPO.", "title": "blackbox prompt optimization aligning large language models without model training", "url": "http://arxiv.org/pdf/2311.04155v2.pdf", "tokenized_text": "large_language large language llms shown_impressive shown impressive success variousapplications aligned calls additional treatments llms better follow user instructions existing focus training extra training ofllms usually expensive terms gpu compute worse llms accessible user demanded training work different perspective black-box promptoptimization perform alignments idea optimize suit llms input understanding best realize users updating llms parameters agnostic empiricalresults demonstrate aligned chatgpt yields 22 increase rate original version 10 gpt-4 importantly aligned llms outperform aligned ppo brings additional performance gains combining ppo datasets released"}
{"id": "nan", "abstract": "  Recent success of large-scale Contrastive Language-Image Pre-training (CLIP)has led to great promise in zero-shot semantic segmentation by transferringimage-text aligned knowledge to pixel-level classification. However, existingmethods usually require an additional image encoder or retraining/tuning theCLIP module. Here, we propose a novel Zero-shot segmentation with OptimalTransport (ZegOT) method that matches multiple text prompts with frozen imageembeddings through optimal transport. In particular, we introduce a novelMultiple Prompt Optimal Transport Solver (MPOT), which is designed to learn anoptimal mapping between multiple text prompts and visual feature maps of thefrozen image encoder hidden layers. This unique mapping method facilitates eachof the multiple text prompts to effectively focus on distinct visual semanticattributes. Through extensive experiments on benchmark datasets, we show thatour method achieves the state-of-the-art (SOTA) performance over existingZero-shot Semantic Segmentation (ZS3) approaches.", "title": "zegot zeroshot segmentation through optimal transport of text prompts", "url": "http://arxiv.org/pdf/2301.12171v2.pdf", "tokenized_text": "recent success large scale contrastive_language-image_pre-training contrastive language-image pre-training led great promise zero shot semantic segmentation text aligned knowledge pixel level classification existingmethods usually require additional image encoder retraining tuning module propose_a_novel propose novel zero shot segmentation method matches multiple text frozen optimal transport particular introduce optimal transport solver designed learn mapping multiple text visual feature maps image encoder hidden layers unique mapping method facilitates multiple text effectively focus distinct visual extensive_experiments extensive experiments benchmark_datasets benchmark datasets thatour method_achieves method achieves state art sota performance existingzero shot semantic segmentation approaches"}
{"id": "nan", "abstract": "  While recent large language models (LLMs) improve on various questionanswering (QA) datasets, it remains difficult for a single model to generalizeacross question types that require distinct reasoning abilities. We provideempirical evidence that state-of-the-art LLMs suffer from poor generalizabilityon reasoning types beyond those seen in the prompt. To remedy this, we proposea Mixture-of-Reasoning-Experts (MoRE) framework that ensembles diversespecialized language models. We specialize the backbone language model withprompts optimized for different reasoning categories, including factual,multihop, mathematical, and commonsense reasoning. Our key insight is toleverage agreement among the specialized experts to select the best answer foreach question, or to abstain from answering. This gives MoRE higher accuracythan any single specialized model on a collection of 12 QA datasets from fourreasoning types. Beyond generalizability, the interpretable design of MoREimproves selective question answering results compared to baselines withoutincorporating inter-expert agreement. This framework is also more interpretableand useful to human consumers of QA outputs. Our human study confirms thatpresenting expert predictions and the answer selection process helps annotatorsmore accurately calibrate when to trust the system's output. We release allcode and data to facilitate future work.", "title": "getting more out of mixture of language model reasoning experts", "url": "http://arxiv.org/pdf/2305.14628v2.pdf", "tokenized_text": "recent large_language large language llms improve questionanswering qa datasets remains difficult single question types require distinct reasoning abilities evidence state art llms suffer poor reasoning types seen remedy proposea mixture reasoning experts framework ensembles language_models language specialize backbone language_model language optimized different reasoning categories including factual mathematical commonsense reasoning key insight agreement specialized experts select best answer foreach question abstain answering gives higher single specialized collection 12 qa datasets types generalizability interpretable design selective question_answering question answering results compared baselines inter expert agreement framework useful human qa outputs human study confirms expert predictions answer selection process helps accurately calibrate trust system output release data facilitate future work"}
{"id": "nan", "abstract": "  This paper delves into the pivotal role of prompt engineering in unleashingthe capabilities of Large Language Models (LLMs). Prompt engineering is theprocess of structuring input text for LLMs and is a technique integral tooptimizing the efficacy of LLMs. This survey elucidates foundational principlesof prompt engineering, such as role-prompting, one-shot, and few-shotprompting, as well as more advanced methodologies such as the chain-of-thoughtand tree-of-thoughts prompting. The paper sheds light on how externalassistance in the form of plugins can assist in this task, and reduce machinehallucination by retrieving external knowledge. We subsequently delineateprospective directions in prompt engineering research, emphasizing the need fora deeper understanding of structures and the role of agents in ArtificialIntelligence-Generated Content (AIGC) tools. We discuss how to assess theefficacy of prompt methods from different perspectives and using differentmethods. Finally, we gather information about the application of promptengineering in such fields as education and programming, showing itstransformative potential. This comprehensive survey aims to serve as a friendlyguide for anyone venturing through the big world of LLMs and promptengineering.", "title": "unleashing the potential of prompt engineering in large language models a comprehensive review", "url": "http://arxiv.org/pdf/2310.14735v2.pdf", "tokenized_text": "paper delves pivotal role prompt_engineering engineering capabilities large_language large language llms prompt_engineering engineering theprocess structuring input text llms technique integral efficacy llms survey foundational prompt_engineering engineering role shot shotprompting advanced methodologies chain tree thoughts paper sheds light form assist task reduce retrieving external_knowledge external knowledge subsequently directions prompt_engineering engineering research emphasizing need fora deeper understanding structures role agents content aigc tools discuss assess theefficacy methods different perspectives finally gather information application promptengineering fields education programming showing potential comprehensive survey aims serve big world llms promptengineering"}
{"id": "nan", "abstract": "  Prompt engineering and calibration make large language models excel atreasoning tasks, including multiple choice commonsense reasoning. From apractical perspective, we investigate and evaluate these strategies on smallerlanguage models. Through experiments on five commonsense reasoning benchmarks,we find that each strategy favors certain models, but their joint effects aremostly negative.", "title": "prompt engineering and calibration for zeroshot commonsense reasoning", "url": "http://arxiv.org/pdf/2304.06962v1.pdf", "tokenized_text": "prompt_engineering engineering calibration large_language large language excel tasks including multiple_choice multiple choice commonsense reasoning perspective investigate evaluate strategies experiments commonsense reasoning benchmarks find strategy favors certain joint effects aremostly negative"}
{"id": "nan", "abstract": "  The paper aims to fulfil three main functions: (1) to serve as anintroduction for the physics education community to the functioning of LargeLanguage Models (LLMs), (2) to present a series of illustrative examplesdemonstrating how prompt-engineering techniques can impact LLMs performance onconceptual physics tasks and (3) to discuss potential implications of theunderstanding of LLMs and prompt engineering for physics teaching and learning.We first summarise existing research on the performance of a popular LLM-basedchatbot (ChatGPT) on physics tasks. We then give a basic account of how LLMswork, illustrate essential features of their functioning, and discuss theirstrengths and limitations. Equipped with this knowledge, we discuss somechallenges with generating useful output with ChatGPT-4 in the context ofintroductory physics, paying special attention to conceptual questions andproblems. We then provide a condensed overview of relevant literature on promptengineering and demonstrate through illustrative examples how selectedprompt-engineering techniques can be employed to improve ChatGPT-4's output onconceptual introductory physics problems. Qualitatively studying these examplesprovides additional insights into ChatGPT's functioning and its utility inphysics problem solving. Finally, we consider how insights from the paper caninform the use of LMMs in the teaching and learning of physics.", "title": "how understanding large language models can inform their use in physics education", "url": "http://arxiv.org/pdf/2309.12074v1.pdf", "tokenized_text": "paper aims main functions serve physics education community functioning largelanguage_models largelanguage llms present series illustrative engineering techniques impact llms performance physics tasks discuss potential implications llms prompt_engineering engineering physics teaching learning existing research performance popular llm chatgpt physics tasks basic account illustrate essential features functioning discuss limitations equipped knowledge discuss generating useful output chatgpt-4 context physics special attention conceptual questions provide condensed overview relevant literature promptengineering demonstrate illustrative examples engineering techniques employed improve chatgpt-4 output introductory physics problems qualitatively studying additional insights chatgpt functioning utility problem solving finally consider insights paper use lmms teaching learning physics"}
{"id": "nan", "abstract": "  In this paper, we investigate the effectiveness of state-of-the-art LLM,i.e., GPT-4, with three different prompting engineering techniques (i.e., basicprompting, in-context learning, and task-specific prompting) against 18fine-tuned LLMs on three typical ASE tasks, i.e., code generation, codesummarization, and code translation. Our quantitative analysis of theseprompting strategies suggests that prompt engineering GPT-4 cannot necessarilyand significantly outperform fine-tuning smaller/older LLMs in all three tasks.For comment generation, GPT-4 with the best prompting strategy (i.e.,task-specific prompt) had outperformed the first-ranked fine-tuned model by8.33% points on average in BLEU. However, for code generation, the first-rankedfine-tuned model outperforms GPT-4 with best prompting by 16.61% and 28.3%points, on average in BLEU. For code translation, GPT-4 and fine-tunedbaselines tie as they outperform each other on different translation tasks. Toexplore the impact of different prompting strategies, we conducted a user studywith 27 graduate students and 10 industry practitioners. From our qualitativeanalysis, we find that the GPT-4 with conversational prompts (i.e., when ahuman provides feedback and instructions back and forth with a model to achievebest results) showed drastic improvement compared to GPT-4 with automaticprompting strategies. Moreover, we observe that participants tend to requestimprovements, add more context, or give specific instructions as conversationalprompts, which goes beyond typical and generic prompting strategies. Our studysuggests that, at its current state, GPT-4 with conversational prompting hasgreat potential for ASE tasks, but fully automated prompt engineering with nohuman in the loop requires more study and improvement.", "title": "prompt engineering or fine tuning an empirical assessment of large language models in automated software engineering tasks", "url": "http://arxiv.org/pdf/2310.10508v1.pdf", "tokenized_text": "paper investigate effectiveness state art llm i.e. gpt-4 different engineering techniques i.e. context_learning context learning task specific tuned llms typical tasks i.e. code_generation code generation code translation quantitative analysis strategies suggests prompt_engineering engineering gpt-4 significantly outperform fine tuning smaller older llms tasks comment generation gpt-4 best strategy i.e. specific outperformed ranked fine tuned points average bleu code_generation code generation tuned outperforms gpt-4 best average bleu code translation gpt-4 fine outperform different translation tasks toexplore impact different strategies conducted user 27 graduate students 10 industry practitioners find gpt-4 conversational i.e. ahuman provides feedback instructions forth results showed drastic improvement compared gpt-4 strategies observe participants tend add context specific instructions goes typical generic strategies current state gpt-4 conversational potential tasks fully automated prompt_engineering engineering loop requires study improvement"}
{"id": "nan", "abstract": "  Natural language (NL) programming has become more approachable due to thepowerful code-generation capability of large language models (LLMs). This shiftto using NL to program enhances collaborative programming by reducingcommunication barriers and context-switching among programmers from varyingbackgrounds. However, programmers may face challenges during prompt engineeringin a collaborative setting as they need to actively keep aware of theircollaborators' progress and intents. In this paper, we aim to investigate waysto assist programmers' prompt engineering in a collaborative context. We firstconducted a formative study to understand the workflows and challenges ofprogrammers when using NL for collaborative programming. Based on our findings,we implemented a prototype, CoPrompt, to support collaborative promptengineering by providing referring, requesting, sharing, and linkingmechanisms. Our user study indicates that CoPrompt assists programmers incomprehending collaborators' prompts and building on their collaborators' work,reducing repetitive updates and communication costs.", "title": "coprompt supporting prompt sharing and referring in collaborative natural language programming", "url": "http://arxiv.org/pdf/2310.09235v1.pdf", "tokenized_text": "natural_language natural language nl programming code generation capability large_language large language llms nl program enhances collaborative programming barriers context switching programmers programmers face challenges collaborative setting need actively aware progress intents paper aim investigate waysto assist programmers prompt_engineering engineering collaborative context formative study understand workflows challenges nl collaborative programming based findings implemented prototype support collaborative promptengineering providing referring sharing user study indicates assists programmers building work reducing repetitive updates communication costs"}
{"id": "nan", "abstract": "  Question generation has numerous applications in the educational context.Question generation can prove helpful for students when reviewing content andtesting themselves. Furthermore, a question generation model can aid teachersby lessening the burden of creating assessments and other practice material.This paper aims to find the best method to generate questions from textual datathrough a transformer model and prompt engineering. In this research, wefinetuned a pretrained distilBERT model on the SQuAD question answering datasetto generate questions. In addition to training a transformer model, promptengineering was applied to generate questions effectively using the LLaMAmodel. The generated questions were compared against the baseline questions inthe SQuAD dataset to evaluate the effectiveness of four different prompts. Allfour prompts demonstrated over 60% similarity on average. Of theprompt-generated questions, 30% achieved a high similarity score greater than70%.", "title": "promptengineering and transformerbased question generation and evaluation", "url": "http://arxiv.org/pdf/2310.18867v1.pdf", "tokenized_text": "question generation numerous applications educational context question generation prove helpful students reviewing content andtesting furthermore question generation aid burden creating assessments practice material paper aims find best method generate questions textual transformer prompt_engineering engineering research pretrained question_answering question answering generate questions addition training transformer promptengineering applied generate questions effectively generated questions compared baseline questions inthe dataset evaluate effectiveness different demonstrated 60 similarity average theprompt generated questions 30 achieved high similarity score greater"}
{"id": "nan", "abstract": "  This case study investigates the task of job classification in a real-worldsetting, where the goal is to determine whether an English-language job postingis appropriate for a graduate or entry-level position. We explore multipleapproaches to text classification, including supervised approaches such astraditional models like Support Vector Machines (SVMs) and state-of-the-artdeep learning methods such as DeBERTa. We compare them with Large LanguageModels (LLMs) used in both few-shot and zero-shot classification settings. Toaccomplish this task, we employ prompt engineering, a technique that involvesdesigning prompts to guide the LLMs towards the desired output. Specifically,we evaluate the performance of two commercially available state-of-the-artGPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We alsoconduct a detailed analysis of the impact of different aspects of promptengineering on the model's performance. Our results show that, with awell-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms allother models, achieving a 6% increase in Precision@95% Recall compared to thebest supervised approach. Furthermore, we observe that the wording of theprompt is a critical factor in eliciting the appropriate \"reasoning\" in themodel, and that seemingly minor aspects of the prompt significantly affect themodel's performance.", "title": "large language models in the workplace a case study on prompt engineering for job type classification", "url": "http://arxiv.org/pdf/2303.07142v3.pdf", "tokenized_text": "case study investigates task job classification real worldsetting goal determine english language job appropriate graduate entry level position explore text_classification text classification including supervised approaches like support vector machines state learning methods compare large_languagemodels large languagemodels llms shot zero shot classification settings toaccomplish task employ prompt_engineering engineering technique guide llms desired output specifically evaluate performance commercially available state based language_models language text davinci-003 gpt-3.5 turbo alsoconduct detailed analysis impact different aspects promptengineering performance results awell designed zero shot gpt-3.5 turbo classifier outperforms achieving increase recall compared thebest supervised approach furthermore observe wording theprompt critical factor eliciting appropriate reasoning themodel seemingly minor aspects significantly affect themodel performance"}
{"id": "nan", "abstract": "  Purpose: Recent advancements in large language models (LLMs) have expandedtheir capabilities in a multimodal fashion, potentially replicating the imageinterpretation of human radiologists. This study aimed to develop open-sourcemultimodal large language model for interpreting chest X-ray images(CXR-LLaVA). We also examined the effect of prompt engineering and modelparameters such as temperature and nucleus sampling.  Materials and Methods: For training, we collected 659,287 publicly availableCXRs: 417,336 CXRs had labels for certain radiographic abnormalities (dataset1); 241,951 CXRs provided free-text radiology reports (dataset 2). Afterpre-training the Resnet50 as an image encoder, the contrastive language-imagepre-training was used to align CXRs and corresponding radiographicabnormalities. Then, the Large Language Model Meta AI-2 was fine-tuned usingdataset 2, which were refined using GPT-4, with generating various questionanswering scenarios. The code can be found athttps://github.com/ECOFRI/CXR_LLaVA.  Results: In the test set, we observed that the model's performance fluctuatedbased on its parameters. On average, it achieved F1 score of 0.34 for fivepathologic findings (atelectasis, cardiomegaly, consolidation, edema, andpleural effusion), which was improved to 0.46 through prompt engineering. Inthe independent set, the model achieved an average F1 score of 0.30 for thesame pathologic findings. Notably, for the pediatric chest radiograph dataset,which was unseen during training, the model differentiated abnormal radiographswith an F1 score ranging from 0.84 to 0.85.  Conclusion: CXR-LLaVA demonstrates promising potential in CXR interpretation.Both prompt engineering and model parameter adjustments can play pivotal rolesin interpreting CXRs.", "title": "cxrllava multimodal large language model for interpreting chest xray images", "url": "http://arxiv.org/pdf/2310.18341v2.pdf", "tokenized_text": "purpose recent advancements large_language large language llms capabilities multimodal fashion potentially replicating imageinterpretation human radiologists study aimed develop open large_language large language interpreting ray llava examined effect prompt_engineering engineering modelparameters temperature sampling materials methods training collected publicly labels certain provided free text radiology reports dataset training image encoder contrastive language imagepre training align corresponding large_language large language meta fine tuned refined gpt-4 generating questionanswering scenarios code found results test set observed performance parameters average achieved f1_score f1 score findings consolidation improved prompt_engineering engineering inthe independent set achieved average f1_score f1 score thesame findings notably dataset unseen training differentiated abnormal f1_score f1 score ranging 0.84 0.85 conclusion cxr llava demonstrates promising potential cxr interpretation prompt_engineering engineering parameter adjustments play pivotal interpreting"}
{"id": "nan", "abstract": "  Text-to-image generation has seen an explosion of interest since 2021. Today,beautiful and intriguing digital images and artworks can be synthesized fromtextual inputs (\"prompts\") with deep generative models. Online communitiesaround text-to-image generation and AI generated art have quickly emerged. Thispaper identifies six types of prompt modifiers used by practitioners in theonline community based on a 3-month ethnographic study. The novel taxonomy ofprompt modifiers provides researchers a conceptual starting point forinvestigating the practice of text-to-image generation, but may also helppractitioners of AI generated art improve their images. We further outline howprompt modifiers are applied in the practice of \"prompt engineering.\" Wediscuss research opportunities of this novel creative practice in the field ofHuman-Computer Interaction (HCI). The paper concludes with a discussion ofbroader implications of prompt engineering from the perspective of Human-AIInteraction (HAI) in future applications beyond the use case of text-to-imagegeneration and AI generated art.", "title": "a taxonomy of prompt modifiers for texttoimage generation", "url": "http://arxiv.org/pdf/2204.13988v3.pdf", "tokenized_text": "text image_generation image generation seen explosion interest 2021 today beautiful intriguing digital images artworks synthesized inputs deep generative online text image_generation image generation ai generated art quickly emerged thispaper identifies types modifiers practitioners community based month ethnographic study novel taxonomy ofprompt modifiers provides researchers conceptual starting point practice text image_generation image generation ai generated art improve images outline modifiers applied practice prompt_engineering engineering wediscuss research opportunities novel creative practice field ofhuman-computer_interaction ofhuman-computer interaction hci paper concludes discussion implications prompt_engineering engineering perspective human aiinteraction future applications use case text imagegeneration ai generated art"}
{"id": "nan", "abstract": "  Coreference resolution -- which is a crucial task for understanding discourseand language at large -- has yet to witness widespread benefits from largelanguage models (LLMs). Moreover, coreference resolution systems largely relyon supervised labels, which are highly expensive and difficult to annotate,thus making it ripe for prompt engineering. In this paper, we introduce aQA-based prompt-engineering method and discern \\textit{generative}, pre-trainedLLMs' abilities and limitations toward the task of coreference resolution. Ourexperiments show that GPT-2 and GPT-Neo can return valid answers, but thattheir capabilities to identify coreferent mentions are limited andprompt-sensitive, leading to inconsistent results.", "title": "what gpt knows about who is who", "url": "http://arxiv.org/pdf/2205.07407v1.pdf", "tokenized_text": "coreference resolution crucial task understanding language large widespread benefits largelanguage_models largelanguage llms coreference resolution systems largely relyon supervised labels highly expensive difficult annotate making ripe prompt_engineering engineering paper introduce based engineering method discern pre abilities limitations task coreference resolution ourexperiments gpt-2 gpt neo return valid answers capabilities identify mentions limited andprompt sensitive leading inconsistent results"}
{"id": "nan", "abstract": "  Handling and digesting a huge amount of information in an efficient mannerhas been a long-term demand in modern society. Some solutions to map key points(short textual summaries capturing essential information and filteringredundancies) to a large number of arguments/opinions have been providedrecently (Bar-Haim et al., 2020). To complement the full picture of theargument-to-keypoint mapping task, we mainly propose two approaches in thispaper. The first approach is to incorporate prompt engineering for fine-tuningthe pre-trained language models (PLMs). The second approach utilizesprompt-based learning in PLMs to generate intermediary texts, which are thencombined with the original argument-keypoint pairs and fed as inputs to aclassifier, thereby mapping them. Furthermore, we extend the experiments tocross/in-domain to conduct an in-depth analysis. In our evaluation, we findthat i) using prompt engineering in a more direct way (Approach 1) can yieldpromising results and improve the performance; ii) Approach 2 performsconsiderably worse than Approach 1 due to the negation issue of the PLM.", "title": "arguments to key points mapping with promptbased learning", "url": "http://arxiv.org/pdf/2211.14995v1.pdf", "tokenized_text": "handling huge information efficient long term demand modern society solutions map key textual summaries capturing essential information large number arguments opinions bar et_al et al 2020 complement picture mapping task mainly propose approaches thispaper approach incorporate prompt_engineering engineering fine tuningthe pre trained_language trained language plms second approach based learning plms generate intermediary texts original argument pairs fed inputs aclassifier mapping furthermore extend experiments domain conduct depth analysis evaluation findthat prompt_engineering engineering direct way approach results improve performance ii approach worse approach negation issue plm"}
{"id": "nan", "abstract": "  Legal Prompt Engineering (LPE) or Legal Prompting is a process to guide andassist a large language model (LLM) with performing a natural legal languageprocessing (NLLP) skill. Our goal is to use LPE with LLMs over long legaldocuments for the Legal Judgement Prediction (LJP) task. We investigate theperformance of zero-shot LPE for given facts in case-texts from the EuropeanCourt of Human Rights (in English) and the Federal Supreme Court of Switzerland(in German, French and Italian). Our results show that zero-shot LPE is bettercompared to the baselines, but it still falls short compared to current stateof the art supervised approaches. Nevertheless, the results are important,since there was 1) no explicit domain-specific data used - so we show that thetransfer to the legal domain is possible for general-purpose LLMs, and 2) theLLMs where directly applied without any further training or fine-tuning - whichin turn saves immensely in terms of additional computational costs.", "title": "legal prompt engineering for multilingual legal judgement prediction", "url": "http://arxiv.org/pdf/2212.02199v1.pdf", "tokenized_text": "legal prompt_engineering engineering legal process guide large_language large language llm performing natural legal languageprocessing skill goal use llms long legal judgement prediction task investigate theperformance zero shot given facts case texts human rights english supreme court german french italian results zero shot baselines falls short compared current art supervised approaches results important explicit domain specific data legal domain possible general purpose llms thellms directly applied training fine tuning turn saves immensely terms additional computational costs"}
{"id": "nan", "abstract": "  Conditional generative models such as DALL-E and Stable Diffusion generateimages based on a user-defined text, the prompt. Finding and refining promptsthat produce a desired image has become the art of prompt engineering.Generative models do not provide a built-in retrieval model for a user'sinformation need expressed through prompts. In light of an extensive literaturereview, we reframe prompt engineering for generative models as interactivetext-based retrieval on a novel kind of \"infinite index\". We apply theseinsights for the first time in a case study on image generation for game designwith an expert. Finally, we envision how active learning may help to guide theretrieval of generated images.", "title": "the infinite index information retrieval on generative texttoimage models", "url": "http://arxiv.org/pdf/2212.07476v2.pdf", "tokenized_text": "conditional generative dall stable_diffusion stable diffusion based user defined text finding refining produce desired image art prompt_engineering engineering generative provide built retrieval need expressed light extensive reframe prompt_engineering engineering generative based retrieval novel kind infinite index apply time case study image_generation image generation game expert finally envision active learning help guide theretrieval generated images"}
{"id": "nan", "abstract": "  Chemical similarity searches are widely used in-silico methods foridentifying new drug-like molecules. These methods have historically relied onstructure-based comparisons to compute molecular similarity. Here, we use achemical language model to create a vector-based chemical search. We extendimplementations by creating a prompt engineering strategy that utilizes twodifferent chemical string representation algorithms: one for the query and theother for the database. We explore this method by reviewing the search resultsfrom five drug-like query molecules (penicillin G, nirmatrelvir, zidovudine,lysergic acid diethylamide, and fentanyl) and three dye-like query molecules(acid blue 25, avobenzone, and 2-diphenylaminocarbazole). We find that thisnovel method identifies molecules that are functionally similar to the query,indicated by the associated patent literature, and that many of these moleculesare structurally distinct from the query, making them unlikely to be found withtraditional chemical similarity search methods. This method may aid in thediscovery of novel structural classes of molecules that achieve targetfunctionality.", "title": "prompt engineering for transformerbased chemical similarity search identifies structurally distinct functional analogues", "url": "http://arxiv.org/pdf/2305.16330v1.pdf", "tokenized_text": "chemical similarity searches widely methods new drug like molecules methods historically relied based comparisons compute molecular similarity use language_model language create vector based chemical search creating prompt_engineering engineering strategy utilizes chemical string representation algorithms query theother database explore method reviewing search drug like query molecules acid like query 25 find method identifies molecules functionally similar query indicated associated literature structurally distinct query making unlikely found chemical similarity search methods method aid novel structural classes molecules achieve"}
{"id": "nan", "abstract": "  Despite the rich existing literature about minimax optimization in continuoussettings, only very partial results of this kind have been obtained forcombinatorial settings. In this paper, we fill this gap by providing acharacterization of submodular minimax optimization, the problem of finding aset (for either the min or the max player) that is effective against everypossible response. We show when and under what conditions we can find suchsets. We also demonstrate how minimax submodular optimization provides robustsolutions for downstream machine learning applications such as (i) efficientprompt engineering for question answering, (ii) prompt engineering for dialogstate tracking, (iii) identifying robust waiting locations for ride-sharing,(iv) ride-share difficulty kernelization, and (v) finding adversarial images.Our experiments demonstrate that our proposed algorithms consistentlyoutperform other baselines.", "title": "submodular minimax optimization finding effective sets", "url": "http://arxiv.org/pdf/2305.16903v1.pdf", "tokenized_text": "despite rich existing literature optimization partial results kind obtained settings paper fill gap providing optimization problem finding aset min max player effective response conditions find demonstrate optimization provides downstream machine_learning machine learning applications engineering question_answering question answering ii prompt_engineering engineering tracking iii identifying robust locations share difficulty finding adversarial images experiments_demonstrate experiments demonstrate proposed algorithms baselines"}
{"id": "nan", "abstract": "  Generative text-to-image models have gained great popularity among the publicfor their powerful capability to generate high-quality images based on naturallanguage prompts. However, developing effective prompts for desired images canbe challenging due to the complexity and ambiguity of natural language. Thisresearch proposes PromptMagician, a visual analysis system that helps usersexplore the image results and refine the input prompts. The backbone of oursystem is a prompt recommendation model that takes user prompts as input,retrieves similar prompt-image pairs from DiffusionDB, and identifies special(important and relevant) prompt keywords. To facilitate interactive promptrefinement, PromptMagician introduces a multi-level visualization for thecross-modal embedding of the retrieved images and recommended keywords, andsupports users in specifying multiple criteria for personalized exploration.Two usage scenarios, a user study, and expert interviews demonstrate theeffectiveness and usability of our system, suggesting it facilitates promptengineering and improves the creativity support of the generative text-to-imagemodel.", "title": "promptmagician interactive prompt engineering for texttoimage creation", "url": "http://arxiv.org/pdf/2307.09036v2.pdf", "tokenized_text": "generative text image gained great popularity powerful capability generate high quality images based naturallanguage developing effective desired images canbe challenging complexity ambiguity natural_language natural language proposes visual analysis system helps image results refine input backbone recommendation takes user input retrieves similar image pairs identifies relevant keywords facilitate interactive introduces multi level visualization thecross modal embedding retrieved images keywords users specifying multiple criteria personalized exploration usage scenarios user study expert interviews demonstrate theeffectiveness usability system suggesting facilitates promptengineering improves creativity support generative text"}
{"id": "nan", "abstract": "  An interactive robot framework accomplishes long-horizon task planning andcan easily generalize to new goals or distinct tasks, even during execution.However, most traditional methods require predefined module design, which makesit hard to generalize to different goals. Recent large language model basedapproaches can allow for more open-ended planning but often require heavyprompt engineering or domain-specific pretrained models. To tackle this, wepropose a simple framework that achieves interactive task planning withlanguage models. Our system incorporates both high-level planning and low-levelfunction execution via language. We verify the robustness of our system ingenerating novel high-level instructions for unseen objectives and its ease ofadaptation to different tasks by merely substituting the task guidelines,without the need for additional complex prompt engineering. Furthermore, whenthe user sends a new request, our system is able to replan accordingly withprecision based on the new request, task guidelines and previously executedsteps. Please check more details on our https://wuphilipp.github.io/itp_siteand https://youtu.be/TrKLuyv26_g.", "title": "interactive task planning with language models", "url": "http://arxiv.org/pdf/2310.10645v1.pdf", "tokenized_text": "interactive robot framework accomplishes long horizon task planning andcan easily generalize new goals distinct tasks execution traditional methods require predefined module design hard generalize different goals recent large_language large language basedapproaches allow open ended planning require engineering domain specific pretrained tackle wepropose simple framework achieves interactive task planning withlanguage system incorporates high level planning low execution language verify robustness system novel high level instructions unseen objectives ease different tasks merely substituting task guidelines need additional complex prompt_engineering engineering furthermore whenthe user new request system able replan accordingly based new request task guidelines previously check details"}
{"id": "nan", "abstract": "  Prompt Engineering (PE) has emerged as a critical technique for guiding LargeLanguage Models (LLMs) in solving intricate tasks. Its importance ishighlighted by its potential to significantly enhance the efficiency andeffectiveness of human-machine interaction. As tasks grow increasingly complex,recent advanced PE methods have extended beyond the limitations of single-roundinteractions to embrace multi-round interactions, which allows for a deeper andmore nuanced engagement with LLMs. In this paper, we propose an optimal controlframework tailored for multi-round interactions with LLMs. This frameworkprovides a unified mathematical structure that not only systematizes theexisting PE methods but also sets the stage for rigorous analyticalimprovements. Furthermore, we extend this framework to include PE via ensemblemethods and multi-agent collaboration, thereby enlarging the scope ofapplicability. By adopting an optimal control perspective, we offer freshinsights into existing PE methods and highlight theoretical challenges thatwarrant future research. Besides, our work lays a foundation for thedevelopment of more effective and interpretable PE methods.", "title": "prompt engineering through the lens of optimal control", "url": "http://arxiv.org/pdf/2310.14201v2.pdf", "tokenized_text": "prompt_engineering engineering pe emerged critical technique guiding largelanguage_models largelanguage llms solving intricate tasks importance potential significantly enhance efficiency human machine interaction tasks grow increasingly complex recent advanced pe methods extended limitations single multi round interactions allows deeper andmore nuanced engagement llms paper propose optimal tailored multi round interactions llms unified mathematical structure pe methods sets stage rigorous furthermore extend framework include pe multi agent collaboration scope adopting optimal control perspective offer existing pe methods highlight theoretical challenges future_research future research work lays foundation thedevelopment effective interpretable pe methods"}
{"id": "nan", "abstract": "  The springing up of Large Language Models (LLMs) has shifted the communityfrom single-task-orientated natural language processing (NLP) research to aholistic end-to-end multi-task learning paradigm. Along this line of researchendeavors in the area, LLM-based prompting methods have attracted muchattention, partially due to the technological advantages brought by promptengineering (PE) as well as the underlying NLP principles disclosed by variousprompting methods. Traditional supervised learning usually requires training amodel based on labeled data and then making predictions. In contrast, PEmethods directly use the powerful capabilities of existing LLMs (i.e., GPT-3and GPT-4) via composing appropriate prompts, especially under few-shot orzero-shot scenarios. Facing the abundance of studies related to the promptingand the ever-evolving nature of this field, this article aims to (i) illustratea novel perspective to review existing PE methods, within the well-establishedcommunication theory framework; (ii) facilitate a better/deeper understandingof developing trends of existing PE methods used in four typical tasks; (iii)shed light on promising research directions for future PE methods.", "title": "a communication theory perspective on prompting engineering methods for large language models", "url": "http://arxiv.org/pdf/2310.18358v1.pdf", "tokenized_text": "large_language large language llms shifted single task natural_language natural language processing nlp research end end multi task learning paradigm line area llm based methods attracted muchattention partially technological advantages brought promptengineering pe underlying nlp principles disclosed variousprompting methods traditional supervised learning usually requires training amodel based labeled_data labeled data making predictions contrast directly use powerful capabilities existing llms i.e. gpt-4 composing appropriate especially shot shot scenarios facing abundance studies related evolving nature field article aims novel perspective review existing pe methods theory framework ii facilitate better deeper understandingof developing trends existing pe methods typical tasks light promising research directions future pe methods"}
{"id": "nan", "abstract": "  With the spread of the use of Text2Img diffusion models such as DALL-E 2,Imagen, Mid Journey and Stable Diffusion, one challenge that artists face isselecting the right prompts to achieve the desired artistic output. We presenttechniques for measuring the effect that specific words and phrases in promptshave, and (in the Appendix) present guidance on the selection of prompts toproduce desired effects.", "title": "investigating prompt engineering in diffusion models", "url": "http://arxiv.org/pdf/2211.15462v1.pdf", "tokenized_text": "spread use diffusion dall journey stable_diffusion stable diffusion challenge artists face right achieve desired artistic output measuring effect specific words phrases present guidance selection toproduce desired effects"}
{"id": "nan", "abstract": "  In this paper, we propose a simple yet efficient approach based on promptengineering that leverages the large language model itself to optimize itsanswers without relying on auxiliary models. We introduce an iterativeself-evaluating optimization mechanism, with the potential for improved outputquality as iterations progress, removing the need for manual intervention. Theexperiment's findings indicate that utilizing our response refinement frameworkon the GPT-3.5 model yields results that are on par with, or even surpass,those generated by the cutting-edge GPT-4 model. Detailed implementationstrategies and illustrative examples are provided to demonstrate thesuperiority of our proposed solution.", "title": "refining the responses of llms by themselves", "url": "http://arxiv.org/pdf/2305.04039v1.pdf", "tokenized_text": "paper propose simple efficient approach based promptengineering leverages large_language large language optimize relying auxiliary introduce evaluating optimization mechanism potential improved iterations progress removing need manual intervention findings indicate utilizing response refinement gpt-3.5 yields results par surpass generated cutting edge gpt-4 detailed illustrative examples provided demonstrate thesuperiority proposed solution"}
{"id": "nan", "abstract": "  Neural text detectors are models trained to detect whether a given text wasgenerated by a language model or written by a human. In this paper, weinvestigate three simple and resource-efficient strategies (parameter tweaking,prompt engineering, and character-level mutations) to alter texts generated byGPT-3.5 that are unsuspicious or unnoticeable for humans but causemisclassification by neural text detectors. The results show that especiallyparameter tweaking and character-level mutations are effective strategies.", "title": "efficient blackbox adversarial attacks on neural text detectors", "url": "http://arxiv.org/pdf/2311.01873v1.pdf", "tokenized_text": "neural text detectors trained detect given text wasgenerated language_model language written human paper weinvestigate simple resource efficient strategies parameter prompt_engineering engineering character level mutations alter texts generated humans neural text detectors results character level mutations effective strategies"}
{"id": "nan", "abstract": "  This paper introduces prompted software engineering (PSE), which integratesprompt engineering to build effective prompts for language-based AI models, toenhance the software development process. PSE enables the use of AI models insoftware development to produce high-quality software with fewer resources,automating tedious tasks and allowing developers to focus on more innovativeaspects. However, effective prompts are necessary to guide software developmentin generating accurate, relevant, and useful responses, while mitigating risksof misleading outputs. This paper describes how productive prompts should bebuilt throughout the software development cycle.", "title": "prompted software engineering in the era of ai models", "url": "http://arxiv.org/pdf/2311.03359v1.pdf", "tokenized_text": "paper introduces prompted software engineering engineering build effective language based ai toenhance software development process enables use ai development produce high quality software fewer resources automating tedious tasks allowing developers focus effective necessary guide software generating accurate relevant useful responses mitigating misleading outputs paper describes productive software development cycle"}
{"id": "nan", "abstract": "  Sequence-to-sequence models have been used to transform erroneous programsinto correct ones when trained with a large enough dataset. Some recent studiesalso demonstrated strong empirical evidence that code review could improve theprogram repair further. Large language models, trained with Natural Language(NL) and Programming Language (PL), can contain inherent knowledge of both. Inthis study, we investigate if this inherent knowledge of PL and NL can beutilized to improve automated program repair. We applied PLBART and CodeT5, twostate-of-the-art language models that are pre-trained with both PL and NL, ontwo such natural language-based program repair datasets and found that thepre-trained language models fine-tuned with datasets containing both codereview and subsequent code changes notably outperformed each of the previousmodels. With the advent of code generative models like Codex and GPT-3.5-Turbo,we also performed zero-shot and few-shots learning-based prompt engineering toassess their performance on these datasets. However, the practical applicationof using LLMs in the context of automated program repair is still a long wayoff based on our manual analysis of the generated repaired codes by thelearning models.", "title": "enhancing automated program repair through finetuning and prompt engineering", "url": "http://arxiv.org/pdf/2304.07840v2.pdf", "tokenized_text": "sequence sequence transform erroneous correct ones trained large dataset recent demonstrated strong empirical evidence code review improve theprogram repair large_language large language trained natural programming language contain inherent knowledge inthis study investigate inherent knowledge nl improve automated program repair applied art language_models language pre trained nl ontwo natural_language natural language based program repair datasets found thepre trained_language trained language fine tuned datasets containing subsequent code changes notably outperformed advent code generative like codex gpt-3.5 turbo performed zero shot shots learning based prompt_engineering engineering toassess performance datasets practical llms context automated program repair long based manual analysis generated repaired codes thelearning"}
{"id": "nan", "abstract": "  Large language models (LLMs) offer significant promise as a knowledge sourcefor task learning. Prompt engineering has been shown to be effective foreliciting knowledge from an LLM, but alone it is insufficient for acquiringrelevant, situationally grounded knowledge for an embodied agent learning noveltasks. We describe a cognitive-agent approach that extends and complementsprompt engineering, mitigating its limitations and thus enabling an agent toacquire new task knowledge matched to its native language capabilities,embodiment, environment, and user preferences. The approach is to increase theresponse space of LLMs and deploy general strategies, embedded within theautonomous agent, to evaluate, repair, and select among candidate responsesproduced by the LLM. We describe the approach and experiments that show how anagent, by retrieving and evaluating a breadth of responses from the LLM, canachieve 77-94% task completion in one-shot learning without user oversight. Theapproach achieves 100% task completion when human oversight (such as anindication of preference) is provided. Further, the type of oversight largelyshifts from explicit, natural language instruction to simpleconfirmation/discomfirmation of high-quality responses that have been vetted bythe agent before presentation to a user.", "title": "improving knowledge extraction from llms for task learning through agent analysis", "url": "http://arxiv.org/pdf/2306.06770v3.pdf", "tokenized_text": "large_language large language llms offer significant promise knowledge task learning prompt_engineering engineering shown effective knowledge llm insufficient situationally grounded knowledge embodied agent learning describe cognitive agent approach extends engineering mitigating limitations enabling agent new task knowledge matched native language capabilities embodiment environment user preferences approach increase space llms deploy general strategies embedded theautonomous agent evaluate repair select candidate llm describe approach experiments anagent retrieving evaluating breadth responses llm 77 94 task completion shot_learning shot learning user oversight theapproach achieves 100 task completion human oversight preference provided type oversight explicit natural_language natural language instruction high quality responses vetted bythe agent presentation user"}
{"id": "nan", "abstract": "  Zero-shot learning in prompted vision-language models, the practice ofcrafting prompts to build classifiers without an explicit training process, hasachieved impressive performance in many settings. This success presents aseemingly surprising observation: these methods suffer relatively little fromoverfitting, i.e., when a prompt is manually engineered to achieve low error ona given training set (thus rendering the method no longer actually zero-shot),the approach still performs well on held-out test data. In this paper, we showthat we can explain such performance well via recourse to classical PAC-Bayesbounds. Specifically, we show that the discrete nature of prompts, combinedwith a PAC-Bayes prior given by a language model, results in generalizationbounds that are remarkably tight by the standards of the literature: forinstance, the generalization bound of an ImageNet classifier is often within afew percentage points of the true test error. We demonstrate empirically thatthis holds for existing handcrafted prompts and prompts generated throughsimple greedy search. Furthermore, the resulting bound is well-suited for modelselection: the models with the best bound typically also have the best testperformance. This work thus provides a possible justification for thewidespread practice of prompt engineering, even if it seems that such methodscould potentially overfit the training data.", "title": "understanding prompt engineering may not require rethinking generalization", "url": "http://arxiv.org/pdf/2310.03957v1.pdf", "tokenized_text": "zero shot_learning shot learning prompted vision language_models language practice ofcrafting build classifiers explicit training process hasachieved impressive performance settings success presents surprising observation methods suffer relatively little i.e. manually engineered achieve low error ona given training set rendering method longer actually zero approach performs held test data paper showthat explain performance classical pac specifically discrete nature pac bayes prior given language_model language results remarkably standards literature generalization bound imagenet classifier afew percentage points true test error demonstrate empirically holds existing handcrafted generated greedy search furthermore resulting bound suited best bound typically best work provides possible justification practice prompt_engineering engineering potentially overfit training_data training data"}
{"id": "nan", "abstract": "  Misconfigurations are the major causes of software failures. Existingconfiguration validation techniques rely on manually written rules or testcases, which are expensive to implement and maintain, and are hard to becomprehensive. Leveraging machine learning (ML) and natural language processing(NLP) for configuration validation is considered a promising direction, but hasbeen facing challenges such as the need of not only large-scale configurationdata, but also system-specific features and models which are hard togeneralize. Recent advances in Large Language Models (LLMs) show the promisesto address some of the long-lasting limitations of ML/NLP-based configurationvalidation techniques. In this paper, we present an exploratory analysis on thefeasibility and effectiveness of using LLMs like GPT and Codex forconfiguration validation. Specifically, we take a first step to empiricallyevaluate LLMs as configuration validators without additional fine-tuning orcode generation. We develop a generic LLM-based validation framework, namedCiri, which integrates different LLMs. Ciri devises effective promptengineering with few-shot learning based on both valid configuration andmisconfiguration data. Ciri also validates and aggregates the outputs of LLMsto generate validation results, coping with known hallucination andnondeterminism of LLMs. We evaluate the validation effectiveness of Ciri onfive popular LLMs using configuration data of six mature, widely deployedopen-source systems. Our analysis (1) confirms the potential of using LLMs forconfiguration validation, (2) understands the design space of LLMbasedvalidators like Ciri, especially in terms of prompt engineering with few-shotlearning, and (3) reveals open challenges such as ineffectiveness in detectingcertain types of misconfigurations and biases to popular configurationparameters.", "title": "configuration validation with large language models", "url": "http://arxiv.org/pdf/2310.09690v1.pdf", "tokenized_text": "major causes software failures validation techniques rely manually written rules expensive implement maintain hard leveraging machine_learning machine learning ml natural_language natural language processing(nlp configuration validation considered promising direction hasbeen facing challenges need large scale system specific features hard togeneralize recent_advances recent advances large_language large language llms address long lasting limitations ml nlp based techniques paper present exploratory analysis thefeasibility effectiveness llms like gpt codex validation specifically step llms configuration additional fine tuning generation develop generic llm based validation framework integrates different llms effective promptengineering shot_learning shot learning based valid configuration data validates outputs llmsto generate validation results coping known hallucination llms evaluate validation effectiveness popular llms configuration data mature widely source systems analysis confirms potential llms validation understands design space like especially terms prompt_engineering engineering shotlearning reveals open challenges types biases popular"}
{"id": "nan", "abstract": "  Large pre-trained vision-language models like CLIP have shown great potentialin learning representations that are transferable across a wide range ofdownstream tasks. Different from the traditional representation learning thatis based mostly on discretized labels, vision-language pre-training alignsimages and texts in a common feature space, which allows zero-shot transfer toa downstream task via prompting, i.e., classification weights are synthesizedfrom natural language describing classes of interest. In this work, we showthat a major challenge for deploying such models in practice is promptengineering, which requires domain expertise and is extremely time-consuming --one needs to spend a significant amount of time on words tuning since a slightchange in wording could have a huge impact on performance. Inspired by recentadvances in prompt learning research in natural language processing (NLP), wepropose Context Optimization (CoOp), a simple approach specifically foradapting CLIP-like vision-language models for downstream image recognition.Concretely, CoOp models a prompt's context words with learnable vectors whilethe entire pre-trained parameters are kept fixed. To handle different imagerecognition tasks, we provide two implementations of CoOp: unified context andclass-specific context. Through extensive experiments on 11 datasets, wedemonstrate that CoOp requires as few as one or two shots to beat hand-craftedprompts with a decent margin and is able to gain significant improvements overprompt engineering with more shots, e.g., with 16 shots the average gain isaround 15% (with the highest reaching over 45%). Despite being a learning-basedapproach, CoOp achieves superb domain generalization performance compared withthe zero-shot model using hand-crafted prompts.", "title": "learning to prompt for visionlanguage models", "url": "http://arxiv.org/pdf/2109.01134v6.pdf", "tokenized_text": "large pre trained vision language_models language like clip shown great learning representations transferable wide_range wide range ofdownstream tasks different traditional representation learning thatis based labels vision language pre training texts common feature space allows zero shot transfer toa downstream task i.e. classification weights natural_language natural language describing classes interest work showthat major challenge deploying practice promptengineering requires domain expertise extremely time consuming needs spend significant time words tuning wording huge impact performance inspired learning research natural_language natural language processing nlp wepropose context_optimization context optimization coop simple approach specifically clip like vision language_models language downstream image recognition concretely coop context words learnable vectors whilethe entire pre trained parameters kept fixed handle different tasks provide implementations coop unified context specific context extensive_experiments extensive experiments 11 datasets wedemonstrate coop requires shots beat hand craftedprompts decent margin able gain significant improvements overprompt engineering shots e.g. 16 shots average gain 15 highest reaching 45 despite learning basedapproach coop achieves superb domain generalization performance compared withthe zero shot hand crafted"}
{"id": "nan", "abstract": "  Prompt-based knowledge probing for 1-hop relations has been used to measurehow much world knowledge is stored in pretrained language models. Existing workuses considerable amounts of data to tune the prompts for better performance.In this work, we compare a variety of approaches under a few-shot knowledgeprobing setting, where only a small number (e.g., 10 or 20) of example triplesare available. In addition, we create a new dataset named TREx-2p, whichcontains 2-hop relations. We report that few-shot examples can strongly boostthe probing performance for both 1-hop and 2-hop relations. In particular, wefind that a simple-yet-effective approach of finetuning the bias vectors in themodel outperforms existing prompt-engineering methods. Our dataset and code areavailable at \\url{https://github.com/cloudygoose/fewshot_lama}.", "title": "an empirical study on fewshot knowledge probing for pretrained language models", "url": "http://arxiv.org/pdf/2109.02772v2.pdf", "tokenized_text": "based knowledge probing hop relations world knowledge stored pretrained_language pretrained language existing considerable amounts data tune better performance work compare variety approaches shot setting small_number small number e.g. 10 20 example available addition create new dataset named hop relations report shot examples strongly boostthe probing performance hop hop relations particular wefind simple effective approach finetuning bias vectors themodel outperforms existing engineering methods dataset code areavailable \\url{https://github.com"}
{"id": "nan", "abstract": "  We solve university level probability and statistics questions by programsynthesis using OpenAI's Codex, a Transformer trained on text and fine-tuned oncode. We transform course problems from MIT's 18.05 Introduction to Probabilityand Statistics and Harvard's STAT110 Probability into programming tasks. Wethen execute the generated code to get a solution. Since these course questionsare grounded in probability, we often aim to have Codex generate probabilisticprograms that simulate a large number of probabilistic dependencies to computeits solution. Our approach requires prompt engineering to transform thequestion from its original form to an explicit, tractable form that results ina correct program and solution. To estimate the amount of work needed totranslate an original question into its tractable form, we measure thesimilarity between original and transformed questions. Our work is the first tointroduce a new dataset of university-level probability and statistics problemsand solve these problems in a scalable fashion using the program synthesiscapabilities of large language models.", "title": "solving probability and statistics problems by program synthesis", "url": "http://arxiv.org/pdf/2111.08267v1.pdf", "tokenized_text": "solve university level probability statistics questions programsynthesis openai codex transformer trained text fine tuned oncode transform course problems mit introduction statistics probability programming tasks wethen execute generated code solution course grounded probability aim codex generate simulate large number probabilistic dependencies solution approach requires prompt_engineering engineering transform thequestion original form explicit tractable form results ina correct program solution estimate work needed totranslate original question tractable form measure thesimilarity original transformed questions work tointroduce new dataset university level probability statistics solve problems scalable fashion program large_language large language"}
{"id": "nan", "abstract": "  This paper aims for a potential architectural improvement for multilinguallearning and asks: Can different tasks from different languages be modeled in amonolithic framework, i.e. without any task/language-specific module? Thebenefit of achieving this could open new doors for future multilingualresearch, including allowing systems trained on low resources to be furtherassisted by other languages as well as other tasks. We approach this goal bydeveloping a learning framework named Polyglot Prompting to exploit promptingmethods for learning a unified semantic space for different languages and taskswith multilingual prompt engineering. We performed a comprehensive evaluationof 6 tasks, namely topic classification, sentiment classification, named entityrecognition, question answering, natural language inference, and summarization,covering 24 datasets and 49 languages. The experimental results demonstratedthe efficacy of multilingual multitask prompt-based learning and led toinspiring observations. We also present an interpretable multilingualevaluation methodology and show how the proposed framework, multilingualmultitask prompt training, works. We release all datasets prompted in the bestsetting and code.", "title": "polyglot prompt multilingual multitask promptraining", "url": "http://arxiv.org/pdf/2204.14264v2.pdf", "tokenized_text": "paper aims potential architectural improvement asks different tasks different languages modeled framework i.e. task language specific module achieving open new doors future including allowing systems trained low resources languages tasks approach goal learning framework named exploit promptingmethods learning unified semantic space different languages taskswith multilingual prompt_engineering engineering performed comprehensive tasks topic classification sentiment classification named entityrecognition question_answering question answering natural_language natural language inference summarization covering 24 datasets 49 languages experimental_results experimental results demonstratedthe efficacy multilingual multitask based learning led observations present interpretable methodology proposed framework training works release datasets prompted code"}
{"id": "nan", "abstract": "  The unabated mystique of large-scale neural networks, such as the CLIP dualimage-and-text encoder, popularized automatically generated art. Increasinglymore sophisticated generators enhanced the artworks' realism and visualappearance, and creative prompt engineering enabled stylistic expression.Guided by an artist-in-the-loop ideal, we design a gradient-based generator toproduce collages. It requires the human artist to curate libraries of imagepatches and to describe (with prompts) the whole image composition, with theoption to manually adjust the patches' positions during generation, therebyallowing humans to reclaim some control of the process and achieve greatercreative freedom. We explore the aesthetic potentials of high-resolutioncollages, and provide an open-source Google Colab as an artistic tool.", "title": "clipclop clipguided collage and photomontage", "url": "http://arxiv.org/pdf/2205.03146v3.pdf", "tokenized_text": "large scale neural_networks neural networks clip text encoder automatically generated art sophisticated generators enhanced artworks realism creative prompt_engineering engineering enabled stylistic expression guided loop ideal design gradient based generator toproduce requires human curate libraries describe image composition manually adjust patches positions generation humans control process achieve freedom explore potentials high provide open source google artistic tool"}
{"id": "nan", "abstract": "  Text-guided synthesis of images has made a giant leap towards becoming amainstream phenomenon. With text-to-image generation systems, anybody cancreate digital images and artworks. This provokes the question of whethertext-to-image generation is creative. This paper expounds on the nature ofhuman creativity involved in text-to-image art (so-called \"AI art\") with aspecific focus on the practice of prompt engineering. The paper argues that thecurrent product-centered view of creativity falls short in the context oftext-to-image generation. A case exemplifying this shortcoming is provided andthe importance of online communities for the creative ecosystem oftext-to-image art is highlighted. The paper provides a high-level summary ofthis online ecosystem drawing on Rhodes' conceptual four P model of creativity.Challenges for evaluating the creativity of text-to-image generation andopportunities for research on text-to-image generation in the field ofHuman-Computer Interaction (HCI) are discussed.", "title": "the creativity of texttoimage generation", "url": "http://arxiv.org/pdf/2206.02904v4.pdf", "tokenized_text": "text guided synthesis images giant leap phenomenon text image_generation image generation systems anybody digital images artworks question image_generation image generation creative paper nature ofhuman creativity involved text image art called ai art aspecific focus practice prompt_engineering engineering paper thecurrent product centered view creativity falls short context oftext image_generation image generation case shortcoming provided andthe importance online communities creative ecosystem oftext image art highlighted paper provides high level summary ofthis online ecosystem drawing conceptual creativity challenges evaluating creativity text image_generation image generation research text image_generation image generation field ofhuman-computer_interaction ofhuman-computer interaction hci discussed"}
{"id": "nan", "abstract": "  Recent research has shown that rationales, or step-by-step chains of thought,can be used to improve performance in multi-step reasoning tasks. We reconsiderrationale-augmented prompting for few-shot in-context learning, where (input ->output) prompts are expanded to (input, rationale -> output) prompts. Forrationale-augmented prompting we demonstrate how existing approaches, whichrely on manual prompt engineering, are subject to sub-optimal rationales thatmay harm performance. To mitigate this brittleness, we propose a unifiedframework of rationale-augmented ensembles, where we identify rationalesampling in the output space as the key component to robustly improveperformance. This framework is general and can easily be extended to commonnatural language processing tasks, even those that do not traditionallyleverage intermediate steps, such as question answering, word sensedisambiguation, and sentiment analysis. We demonstrate that rationale-augmentedensembles achieve more accurate and interpretable results than existingprompting approaches--including standard prompting without rationales andrationale-based chain-of-thought prompting--while simultaneously improvinginterpretability of model predictions through the associated rationales.", "title": "rationaleaugmented ensembles in language models", "url": "http://arxiv.org/pdf/2207.00747v1.pdf", "tokenized_text": "recent research shown rationales step step chains thought improve performance multi step reasoning tasks augmented shot context_learning context learning input expanded input rationale output augmented demonstrate existing approaches manual prompt_engineering engineering subject sub optimal rationales harm performance mitigate brittleness propose unifiedframework rationale augmented ensembles identify output space key component robustly improveperformance framework general easily extended language_processing language processing tasks intermediate steps question_answering question answering word sensedisambiguation sentiment_analysis sentiment analysis demonstrate rationale achieve accurate interpretable results approaches including standard rationales based chain thought_prompting thought simultaneously predictions associated rationales"}
{"id": "nan", "abstract": "  This paper describes our contributions to the Shared Task of the 9th Workshopon Argument Mining (2022). Our approach uses Large Language Models for the taskof Argument Quality Prediction. We perform prompt engineering using GPT-3, andalso investigate the training paradigms multi-task learning, contrastivelearning, and intermediate-task training. We find that a mixed prediction setupoutperforms single models. Prompting GPT-3 works best for predicting argumentvalidity, and argument novelty is best estimated by a model trained using allthree training paradigms.", "title": "will it blend mixing training paradigms & prompting for argument quality prediction", "url": "http://arxiv.org/pdf/2209.08966v2.pdf", "tokenized_text": "paper describes contributions shared_task shared task 9th argument mining 2022 approach uses large_language large language argument quality prediction perform prompt_engineering engineering gpt-3 investigate training paradigms multi task learning contrastivelearning intermediate task training find mixed prediction single gpt-3 works best predicting argument novelty best estimated trained training paradigms"}
{"id": "nan", "abstract": "  Despite the remarkable progress of image captioning, existing captionerstypically lack the controllable capability to generate desired image captions,e.g., describing the image in a rough or detailed manner, in a factual oremotional view, etc. In this paper, we show that a unified model is qualifiedto perform well in diverse domains and freely switch among multiple styles.Such a controllable capability is achieved by embedding the prompt learninginto the image captioning framework. To be specific, we design a set of promptsto fine-tune the pre-trained image captioner. These prompts allow the model toabsorb stylized data from different domains for joint training, withoutperformance degradation in each domain. Furthermore, we optimize the promptswith learnable vectors in the continuous word embedding space, avoiding theheuristic prompt engineering and meanwhile exhibiting superior performance. Inthe inference stage, our model is able to generate desired stylized captions bychoosing the corresponding prompts. Extensive experiments verify thecontrollable capability of the proposed method. Notably, we achieve outstandingperformance on two diverse image captioning benchmarks including COCO Karpathysplit and TextCaps using a unified model.", "title": "controllable image captioning via prompting", "url": "http://arxiv.org/pdf/2212.01803v1.pdf", "tokenized_text": "despite remarkable progress image captioning existing lack controllable capability generate desired image captions e.g. describing image rough detailed manner factual view etc paper unified perform diverse domains freely switch multiple styles controllable capability achieved embedding image captioning framework specific design set fine tune pre trained image captioner allow stylized data different domains joint training degradation domain furthermore optimize learnable vectors continuous word embedding space avoiding prompt_engineering engineering exhibiting superior_performance superior performance inthe inference stage able generate desired stylized captions corresponding extensive_experiments extensive experiments verify capability proposed_method proposed method notably achieve outstandingperformance diverse image captioning benchmarks including coco unified"}
{"id": "nan", "abstract": "  Explaining the black-box predictions of NLP models naturally and accuratelyis an important open problem in natural language generation. These free-textexplanations are expected to contain sufficient and carefully-selected evidenceto form supportive arguments for predictions. Due to the superior generativecapacity of large pretrained language models, recent work built on promptengineering enables explanation generation without specific training. However,explanation generated through single-pass prompting often lacks sufficiency andconciseness. To address this problem, we develop an information bottleneckmethod EIB to produce refined explanations that are sufficient and concise. Ourapproach regenerates the free-text explanation by polishing the single-passoutput from the pretrained language model but retaining the information thatsupports the contents being explained. Experiments on two out-of-domain tasksverify the effectiveness of EIB through automatic evaluation andthoroughly-conducted human evaluation.", "title": "explanation regeneration via information bottleneck", "url": "http://arxiv.org/pdf/2212.09603v2.pdf", "tokenized_text": "explaining black box predictions nlp naturally important open problem natural_language natural language generation free expected contain sufficient carefully selected form supportive arguments predictions superior large pretrained_language pretrained language recent_work recent work built promptengineering enables explanation generation specific training explanation generated single pass lacks address problem develop information produce refined explanations sufficient concise ourapproach free text explanation single pretrained_language pretrained language retaining information contents explained experiments domain tasksverify effectiveness automatic evaluation conducted human evaluation"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) are popular for their impressive abilities, butthe need for model-specific fine-tuning or task-specific prompt engineering canhinder their generalization. We propose UPRISE (Universal Prompt Retrieval forImproving zero-Shot Evaluation), which tunes a lightweight and versatileretriever that automatically retrieves prompts for a given zero-shot taskinput. Specifically, we demonstrate universality in a cross-task andcross-model scenario: the retriever is tuned on a diverse set of tasks, buttested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, fortuning the retriever, but test the retriever on different LLMs of much largerscales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show thatUPRISE mitigates the hallucination problem in our experiments with ChatGPT,suggesting its potential to improve even the strongest LLMs. Our model and codeare available at https://github.com/microsoft/LMOps.", "title": "uprise universal prompt retrieval for improving zeroshot evaluation", "url": "http://arxiv.org/pdf/2303.08518v3.pdf", "tokenized_text": "large_language large language llms popular impressive abilities need specific fine tuning task specific prompt_engineering engineering generalization propose universal retrieval zero shot evaluation tunes lightweight automatically retrieves given zero shot specifically demonstrate universality cross task andcross scenario retriever tuned diverse set tasks unseen task types use small frozen llm gpt retriever test retriever different llms additionally mitigates hallucination problem experiments chatgpt suggesting potential improve strongest llms available"}
{"id": "nan", "abstract": "  For downstream applications of vision-language pre-trained models, there hasbeen significant interest in constructing effective prompts. Existing works onprompt engineering, which either require laborious manual designs or optimizethe prompt tuning as a point estimation problem, may fail to describe diversecharacteristics of categories and limit their applications. We introduce aBayesian probabilistic resolution to prompt learning, where the label-specificstochastic prompts are generated hierarchically by first sampling a latentvector from an underlying distribution and then employing a lightweightgenerative model. Importantly, we semantically regularize prompt learning withthe visual knowledge and view images and the corresponding prompts as patch andtoken sets under optimal transport, which pushes the prompt tokens tofaithfully capture the label-specific visual concepts, instead of overfittingthe training categories. Moreover, the proposed model can also bestraightforwardly extended to the conditional case where theinstance-conditional prompts are generated to improve the generalizability.Extensive experiments on 15 datasets show promising transferability andgeneralization performance of our proposed model.", "title": "patchtoken aligned bayesian prompt learning for visionlanguage models", "url": "http://arxiv.org/pdf/2303.09100v1.pdf", "tokenized_text": "downstream applications vision language pre trained hasbeen significant interest constructing effective existing works engineering require laborious manual designs tuning point estimation problem fail describe categories limit applications introduce probabilistic resolution learning label generated hierarchically sampling underlying distribution employing importantly semantically learning withthe visual knowledge view images corresponding patch sets optimal transport pushes tokens capture label specific visual concepts instead training categories proposed extended conditional case conditional generated improve generalizability extensive_experiments extensive experiments 15 datasets promising transferability andgeneralization performance proposed"}
{"id": "nan", "abstract": "  Can safety analysis make use of Large Language Models (LLMs)? A case studyexplores Systems Theoretic Process Analysis (STPA) applied to AutomaticEmergency Brake (AEB) and Electricity Demand Side Management (DSM) systemsusing ChatGPT. We investigate how collaboration schemes, input semanticcomplexity, and prompt guidelines influence STPA results. Comparative resultsshow that using ChatGPT without human intervention may be inadequate due toreliability related issues, but with careful design, it may outperform humanexperts. No statistically significant differences are found when varying theinput semantic complexity or using common prompt guidelines, which suggests thenecessity for developing domain-specific prompt engineering. We also highlightfuture challenges, including concerns about LLM trustworthiness and thenecessity for standardisation and regulation in this domain.", "title": "safety analysis in the era of large language models a case study of stpa using chatgpt", "url": "http://arxiv.org/pdf/2304.01246v2.pdf", "tokenized_text": "safety analysis use large_language large language llms case systems theoretic process analysis applied demand management chatgpt investigate collaboration schemes input guidelines influence results comparative resultsshow chatgpt human intervention inadequate related issues careful design outperform statistically significant differences found varying theinput semantic complexity common guidelines suggests thenecessity developing domain specific prompt_engineering engineering challenges including concerns llm trustworthiness thenecessity regulation domain"}
{"id": "nan", "abstract": "  Generative AI tools introduce new and accessible forms of media creation foryouth. They also raise ethical concerns about the generation of fake media,data protection, privacy and ownership of AI-generated art. Since generative AIis already being used in products used by youth, it is critical that theyunderstand how these tools work and how they can be used or misused. In thiswork, we facilitated students' generative AI learning through expression oftheir imagined future identities. We designed a learning workshop - Dreamingwith AI - where students learned about the inner workings of generative AItools, used text-to-image generation algorithms to create their imaged futuredreams, reflected on the potential benefits and harms of generative AI toolsand voiced their opinions about policies for the use of these tools inclassrooms. In this paper, we present the learning activities and experiencesof 34 high school students who engaged in our workshops. Students reachedcreative learning objectives by using prompt engineering to create their futuredreams, gained technical knowledge by learning the abilities, limitations,text-visual mappings and applications of generative AI, and identified mostpotential societal benefits and harms of generative AI.", "title": "constructing dreams using generative ai", "url": "http://arxiv.org/pdf/2305.12013v1.pdf", "tokenized_text": "generative_ai generative ai tools introduce new accessible forms media creation raise ethical concerns generation fake media data protection privacy ownership ai generated art generative products critical tools work misused thiswork facilitated students generative_ai generative ai learning expression future identities designed learning workshop ai students learned inner workings generative text image_generation image generation algorithms create reflected potential benefits harms generative_ai generative ai opinions policies use tools paper present learning activities 34 high school students students learning objectives prompt_engineering engineering create gained technical knowledge learning abilities limitations text visual mappings applications generative_ai generative ai identified societal benefits harms generative_ai generative ai"}
{"id": "nan", "abstract": "  This paper aims to efficiently enable Large Language Models (LLMs) to usemultimodal tools. Advanced proprietary LLMs, such as ChatGPT and GPT-4, haveshown great potential for tool usage through sophisticated prompt engineering.Nevertheless, these models typically rely on prohibitive computational costsand publicly inaccessible data. To address these challenges, we propose theGPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA andOPT, to use tools. It generates an instruction-following dataset by promptingan advanced teacher with various multi-modal contexts. By using the Low-RankAdaptation (LoRA) optimization, our approach facilitates the open-source LLMsto solve a range of visual problems, including visual comprehension and imagegeneration. Moreover, we provide a benchmark to evaluate the ability of LLMs touse tools, which is performed in both zero-shot and fine-tuning ways. Extensiveexperiments demonstrate the effectiveness of our method on various languagemodels, which not only significantly improves the accuracy of invoking seentools, but also enables the zero-shot capacity for unseen tools. The code anddemo are available at https://github.com/StevenGrove/GPT4Tools.", "title": "gpt4tools teaching large language model to use tools via selfinstruction", "url": "http://arxiv.org/pdf/2305.18752v1.pdf", "tokenized_text": "paper aims efficiently enable large_language large language llms tools advanced proprietary llms chatgpt gpt-4 haveshown great_potential great potential tool usage sophisticated prompt_engineering engineering typically rely prohibitive computational publicly inaccessible data address challenges propose based self instruct enable open source llms llama use tools generates instruction following dataset advanced teacher multi modal contexts low lora optimization approach facilitates open source llmsto solve range visual problems including visual comprehension imagegeneration provide benchmark evaluate ability llms touse tools performed zero shot fine tuning ways extensiveexperiments demonstrate_the_effectiveness demonstrate effectiveness method languagemodels significantly improves accuracy invoking enables zero shot capacity unseen tools code available"}
{"id": "nan", "abstract": "  Bug reports are vital for software maintenance that allow users to informdevelopers of the problems encountered while using the software. As such,researchers have committed considerable resources toward automating bug replayto expedite the process of software maintenance. Nonetheless, the success ofcurrent automated approaches is largely dictated by the characteristics andquality of bug reports, as they are constrained by the limitations ofmanually-crafted patterns and pre-defined vocabulary lists. Inspired by thesuccess of Large Language Models (LLMs) in natural language understanding, wepropose AdbGPT, a new lightweight approach to automatically reproduce the bugsfrom bug reports through prompt engineering, without any training andhard-coding effort. AdbGPT leverages few-shot learning and chain-of-thoughtreasoning to elicit human knowledge and logical reasoning from LLMs toaccomplish the bug replay in a manner similar to a developer. Our evaluationsdemonstrate the effectiveness and efficiency of our AdbGPT to reproduce 81.3%of bug reports in 253.6 seconds, outperforming the state-of-the-art baselinesand ablation studies. We also conduct a small-scale user study to confirm theusefulness of AdbGPT in enhancing developers' bug replay capabilities.", "title": "prompting is all you need automated android bug replay with large language models", "url": "http://arxiv.org/pdf/2306.01987v2.pdf", "tokenized_text": "bug reports vital software maintenance allow users problems encountered software researchers committed considerable resources automating bug expedite process software maintenance nonetheless success automated approaches largely characteristics bug reports constrained limitations ofmanually crafted patterns pre defined vocabulary lists inspired thesuccess large_language large language llms natural_language natural language understanding wepropose new lightweight approach automatically reproduce bug reports prompt_engineering engineering training coding effort leverages shot_learning shot learning chain elicit human knowledge logical reasoning llms toaccomplish bug replay manner similar developer evaluationsdemonstrate effectiveness efficiency reproduce bug reports seconds outperforming state art baselinesand ablation studies conduct small scale user study confirm theusefulness enhancing developers bug replay capabilities"}
{"id": "nan", "abstract": "  We utilise the power of Large Language Models (LLMs), in particular GPT4, tobe prompt engineered into performing an arbitrary task. Here, we give the modelsome human priors via text, along with some typical procedures for solving theARC tasks, and ask it to generate the i) broad description of the input-outputrelation, ii) detailed steps of the input-output mapping, iii) use the detailedsteps to perform manipulation on the test input and derive the test output. Thecurrent GPT3.5/GPT4 prompt solves 2 out of 4 tested small ARC challenges (thosewith small grids of 8x8 and below). With tweaks to the prompt to make it morespecific for the use case, it can solve more. We posit that when scaled to amulti-agent system with usage of past memory and equipped with an imageinterpretation tool via Visual Question Answering, we may actually be able tosolve the majority of the ARC challenge", "title": "an approach to solving the abstraction and reasoning corpus (arc) challenge", "url": "http://arxiv.org/pdf/2306.03553v1.pdf", "tokenized_text": "utilise power large_language large language llms particular gpt4 tobe engineered performing arbitrary task human priors text typical procedures solving tasks ask generate broad description input ii detailed steps input output mapping iii use perform manipulation test input derive test output thecurrent gpt3.5 gpt4 solves tested small arc challenges small tweaks use case solve posit scaled amulti agent system usage past memory equipped imageinterpretation tool visual question_answering question answering actually able tosolve majority arc challenge"}
{"id": "nan", "abstract": "  This paper introduces FALL-E, a foley synthesis system and itstraining/inference strategies. The FALL-E model employs a cascaded approachcomprising low-resolution spectrogram generation, spectrogram super-resolution,and a vocoder. We trained every sound-related model from scratch using ourextensive datasets, and utilized a pre-trained language model. We conditionedthe model with dataset-specific texts, enabling it to learn sound quality andrecording environment based on text input. Moreover, we leveraged externallanguage models to improve text descriptions of our datasets and performedprompt engineering for quality, coherence, and diversity. FALL-E was evaluatedby an objective measure as well as listening tests in the DCASE 2023 challengeTask 7. The submission achieved the second place on average, while achievingthe best score for diversity, second place for audio quality, and third placefor class fitness.", "title": "falle a foley sound synthesis model and strategies", "url": "http://arxiv.org/pdf/2306.09807v2.pdf", "tokenized_text": "paper introduces fall synthesis system inference strategies fall employs cascaded low resolution spectrogram generation spectrogram super resolution trained sound related scratch ourextensive datasets utilized pre trained_language trained language dataset specific texts enabling learn sound quality environment based text input leveraged improve text descriptions datasets engineering quality coherence diversity fall objective measure listening tests 2023 submission achieved second place average achievingthe best score diversity second place audio quality class"}
{"id": "nan", "abstract": "  Humankind is entering a novel creative era in which anybody can synthesizedigital information using generative artificial intelligence (AI).Text-to-image generation, in particular, has become vastly popular and millionsof practitioners produce AI-generated images and AI art online. This chapterfirst gives an overview of the key developments that enabled a healthyco-creative online ecosystem around text-to-image generation to rapidly emerge,followed by a high-level description of key elements in this ecosystem. Aparticular focus is placed on prompt engineering, a creative practice that hasbeen embraced by the AI art community. It is then argued that the emergingco-creative ecosystem constitutes an intelligent system on its own - a systemthat both supports human creativity, but also potentially entraps futuregenerations and limits future development efforts in AI. The chapter discussesthe potential risks and dangers of cultivating this co-creative ecosystem, suchas the bias inherent in today's training data, potential quality degradation infuture image generation systems due to synthetic data becoming common place,and the potential long-term effects of text-to-image generation on people'simagination, ambitions, and development.", "title": "the cultivated practices of texttoimage generation", "url": "http://arxiv.org/pdf/2306.11393v1.pdf", "tokenized_text": "humankind entering novel creative era anybody information generative artificial_intelligence artificial intelligence image_generation image generation particular vastly popular practitioners produce ai generated images ai art online gives overview key developments enabled creative online ecosystem text image_generation image generation rapidly emerge followed high level description key elements ecosystem aparticular focus placed prompt_engineering engineering creative practice hasbeen ai art community argued creative ecosystem constitutes intelligent system systemthat supports human creativity potentially limits future development efforts ai chapter potential risks cultivating co creative ecosystem suchas bias inherent today training_data training data potential quality degradation infuture image_generation image generation systems synthetic data common place potential long term effects text image_generation image generation development"}
{"id": "nan", "abstract": "  This research investigates the application of Large Language Models (LLMs) toaugment conversational agents in process mining, aiming to tackle its inherentcomplexity and diverse skill requirements. While LLM advancements present novelopportunities for conversational process mining, generating efficient outputsis still a hurdle. We propose an innovative approach that amend many issues inexisting solutions, informed by prior research on Natural Language Processing(NLP) for conversational agents. Leveraging LLMs, our framework improves bothaccessibility and agent performance, as demonstrated by experiments on publicquestion and data sets. Our research sets the stage for future explorationsinto LLMs' role in process mining and concludes with propositions for enhancingLLM memory, implementing real-time user testing, and examining diverse datasets.", "title": "chitchat or deep talk prompt engineering for process mining", "url": "http://arxiv.org/pdf/2307.09909v1.pdf", "tokenized_text": "research investigates application large_language large language llms toaugment conversational agents process mining aiming tackle diverse skill requirements llm advancements present conversational process mining generating efficient hurdle propose innovative approach amend issues solutions informed prior research natural_language natural language processing(nlp conversational agents leveraging llms framework improves agent performance demonstrated experiments data sets research sets stage future llms role process mining concludes memory implementing real time user testing examining diverse datasets"}
{"id": "nan", "abstract": "  This study presents a thorough examination of various Generative PretrainedTransformer (GPT) methodologies in sentiment analysis, specifically in thecontext of Task 4 on the SemEval 2017 dataset. Three primary strategies areemployed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2)fine-tuning GPT models, and 3) an inventive approach to embeddingclassification. The research yields detailed comparative insights among thesestrategies and individual GPT models, revealing their unique strengths andpotential limitations. Additionally, the study compares these GPT-basedmethodologies with other current, high-performing models previously used withthe same dataset. The results illustrate the significant superiority of the GPTapproaches in terms of predictive performance, more than 22\\% in F1-scorecompared to the state-of-the-art. Further, the paper sheds light on commonchallenges in sentiment analysis tasks, such as understanding context anddetecting sarcasm. It underscores the enhanced capabilities of the GPT modelsto effectively handle these complexities. Taken together, these findingshighlight the promising potential of GPT models in sentiment analysis, settingthe stage for future research in this field. The code can be found athttps://github.com/DSAatUSU/SentimentGPT", "title": "sentimentgpt exploiting gpt for advanced sentiment analysis and its departure from current machine learning", "url": "http://arxiv.org/pdf/2307.10234v2.pdf", "tokenized_text": "study presents thorough examination generative gpt methodologies sentiment_analysis sentiment analysis specifically thecontext task semeval 2017 dataset primary strategies prompt_engineering engineering advanced gpt-3.5 turbo tuning gpt approach research yields detailed comparative insights thesestrategies individual gpt revealing unique strengths andpotential limitations additionally study compares gpt current high performing previously withthe dataset results illustrate significant superiority terms predictive performance f1 state art paper sheds light sentiment_analysis sentiment analysis tasks understanding context sarcasm underscores enhanced capabilities gpt modelsto effectively handle complexities taken promising potential gpt sentiment_analysis sentiment analysis stage future_research future research field code found"}
{"id": "nan", "abstract": "  Engineering knowledge-based (or expert) systems require extensive manualeffort and domain knowledge. As Large Language Models (LLMs) are trained usingan enormous amount of cross-domain knowledge, it becomes possible to automatesuch engineering processes. This paper presents an empirical automation andsemi-automation framework for domain knowledge distillation using promptengineering and the LLM ChatGPT. We assess the framework empirically in theautonomous driving domain and present our key observations. In ourimplementation, we construct the domain knowledge ontology by \"chatting\" withChatGPT. The key finding is that while fully automated domain ontologyconstruction is possible, human supervision and early intervention typicallyimprove efficiency and output quality as they lessen the effects of responserandomness and the butterfly effect. We, therefore, also develop a web-baseddistillation assistant enabling supervision and flexible intervention atruntime. We hope our findings and tools could inspire future research towardrevolutionizing the engineering of knowledge-based systems across applicationdomains.", "title": "domain knowledge distillation from large language model an empirical study in the autonomous driving domain", "url": "http://arxiv.org/pdf/2307.11769v1.pdf", "tokenized_text": "engineering knowledge based expert systems require extensive domain knowledge large_language large language llms trained enormous cross domain knowledge possible engineering processes paper_presents paper presents empirical automation automation framework domain knowledge_distillation knowledge distillation promptengineering llm chatgpt assess framework empirically theautonomous driving domain present key observations construct domain knowledge ontology chatting key finding fully automated domain possible human supervision early intervention efficiency output quality effects effect develop web assistant enabling supervision flexible intervention hope findings tools inspire future_research future research engineering knowledge based systems applicationdomains"}
{"id": "nan", "abstract": "  One of the most important tasks in quantitative investment research is miningnew alphas (effective trading signals or factors). Traditional alpha miningmethods, either hand-crafted factor synthesizing or algorithmic factor mining(e.g., search with genetic programming), have inherent limitations, especiallyin implementing the ideas of quants. In this work, we propose a new alphamining paradigm by introducing human-AI interaction, and a novel promptengineering algorithmic framework to implement this paradigm by leveraging thepower of large language models. Moreover, we develop Alpha-GPT, a newinteractive alpha mining system framework that provides a heuristic way to``understand'' the ideas of quant researchers and outputs creative, insightful,and effective alphas. We demonstrate the effectiveness and advantage ofAlpha-GPT via a number of alpha mining experiments.", "title": "alphagpt humanai interactive alpha mining for quantitative investment", "url": "http://arxiv.org/pdf/2308.00016v1.pdf", "tokenized_text": "important tasks quantitative investment research effective trading signals factors traditional alpha hand crafted factor synthesizing algorithmic factor search genetic programming inherent limitations implementing ideas work propose_a_new propose new paradigm introducing human ai interaction novel promptengineering algorithmic framework implement paradigm leveraging thepower large_language large language develop alpha gpt alpha mining system framework provides heuristic way ideas quant researchers outputs creative insightful effective demonstrate_the_effectiveness demonstrate effectiveness advantage gpt number alpha mining experiments"}
{"id": "nan", "abstract": "  This paper explores the influence of integrating the purpose of thetranslation and the target audience into prompts on the quality of translationsproduced by ChatGPT. Drawing on previous translation studies, industrypractices, and ISO standards, the research underscores the significance of thepre-production phase in the translation process. The study reveals that theinclusion of suitable prompts in large-scale language models like ChatGPT canyield flexible translations, a feat yet to be realized by conventional MachineTranslation (MT). The research scrutinizes the changes in translation qualitywhen prompts are used to generate translations that meet specific conditions.The evaluation is conducted from a practicing translator's viewpoint, bothsubjectively and qualitatively, supplemented by the use of OpenAI's wordembedding API for cosine similarity calculations. The findings suggest that theintegration of the purpose and target audience into prompts can indeed modifythe generated translations, generally enhancing the translation quality byindustry standards. The study also demonstrates the practical application ofthe \"good translation\" concept, particularly in the context of marketingdocuments and culturally dependent idioms.", "title": "optimizing machine translation through prompt engineering an investigation into chatgpt's customizability", "url": "http://arxiv.org/pdf/2308.01391v1.pdf", "tokenized_text": "paper explores influence integrating purpose target audience quality chatgpt drawing previous translation studies standards research underscores significance thepre production phase translation process study reveals suitable large scale language_models language like_chatgpt like chatgpt flexible translations realized conventional machinetranslation mt research changes translation generate translations meet specific conditions evaluation conducted translator qualitatively supplemented use openai api cosine similarity calculations findings_suggest findings suggest theintegration purpose target audience generated translations generally enhancing translation quality standards study demonstrates practical application ofthe good translation concept particularly context culturally dependent"}
{"id": "nan", "abstract": "  This research paper delves into the integration of OpenAI's ChatGPT intoembodied agent systems, evaluating its influence on interactive decision-makingbenchmark. Drawing a parallel to the concept of people assuming roles accordingto their unique strengths, we introduce InterAct. In this approach, we feedChatGPT with varied prompts, assigning it a numerous roles like a checker and asorter, then integrating them with the original language model. Our researchshows a remarkable success rate of 98% in AlfWorld, which consists of 6different tasks in a simulated household environment, emphasizing thesignificance of proficient prompt engineering. The results highlight ChatGPT'scompetence in comprehending and performing intricate tasks effectively inreal-world settings, thus paving the way for further advancements in taskplanning.", "title": "interact exploring the potentials of chatgpt as a cooperative agent", "url": "http://arxiv.org/pdf/2308.01552v1.pdf", "tokenized_text": "research paper delves integration openai chatgpt agent systems evaluating influence interactive decision drawing parallel concept people assuming roles accordingto unique strengths introduce interact approach varied assigning numerous roles like checker integrating original language_model language remarkable success_rate success rate alfworld consists tasks simulated household environment emphasizing proficient prompt_engineering engineering results highlight comprehending performing intricate tasks effectively inreal world settings paving way advancements"}
{"id": "nan", "abstract": "  Large language models (LLMs) are demonstrating significant promise as analternate strategy to facilitate analyses and optimizations of high-performancecomputing programs, circumventing the need for resource-intensive manual toolcreation. In this paper, we explore a novel LLM-based data race detectionapproach combining prompting engineering and fine-tuning techniques. We createa dedicated dataset named DRB-ML, which is derived from DataRaceBench, withfine-grain labels showing the presence of data race pairs and their associatedvariables, line numbers, and read/write information. DRB-ML is then used toevaluate representative LLMs and fine-tune open-source ones. Our experimentshows that LLMs can be a viable approach to data race detection. However, theystill cannot compete with traditional data race detection tools when we needdetailed information about variable pairs causing data races.", "title": "data race detection using large language models", "url": "http://arxiv.org/pdf/2308.07505v2.pdf", "tokenized_text": "large_language large language llms demonstrating significant promise strategy facilitate analyses optimizations high programs circumventing need resource intensive manual paper explore novel llm based data race combining engineering fine tuning techniques dedicated dataset named ml derived withfine grain labels showing presence data race pairs line numbers read write information ml toevaluate representative llms fine tune open source ones llms viable approach data race detection theystill compete traditional data race detection tools information variable pairs causing data"}
{"id": "nan", "abstract": "  LLMs like GPT are great at tasks involving English which dominates in theirtraining data. In this paper, we look at how they cope with tasks involvinglanguages that are severely under-represented in their training data, in thecontext of data-to-text generation for Irish, Maltese, Welsh and Breton. Duringthe prompt-engineering phase we tested a range of prompt types and formats onGPT-3.5 and~4 with a small sample of example input/output pairs. We then fullyevaluated the two most promising prompts in two scenarios: (i) directgeneration into the under-resourced language, and (ii) generation into Englishfollowed by translation into the under-resourced language. We find thatfew-shot prompting works better for direct generation into under-resourcedlanguages, but that the difference disappears when pivoting via English. Thefew-shot + translation system variants were submitted to the WebNLG 2023 sharedtask where they outperformed competitor systems by substantial margins in alllanguages on all metrics. We conclude that good performance on under-resourcedlanguages can be achieved out-of-the box with state-of-the-art LLMs. However,our best results (for Welsh) remain well below the lowest ranked English systemat WebNLG'20.", "title": "datatotext generation for severely underresourced languages with gpt35 a bit of help needed from google translate", "url": "http://arxiv.org/pdf/2308.09957v1.pdf", "tokenized_text": "llms like gpt great tasks involving english data paper look cope tasks severely represented training_data training data thecontext data text generation duringthe engineering phase tested range types formats small sample example input output pairs promising scenarios directgeneration resourced language ii generation translation resourced language find thatfew shot_prompting shot works better direct generation difference pivoting english thefew shot translation system variants submitted 2023 sharedtask outperformed systems substantial margins metrics conclude good performance achieved box state art llms best results remain lowest ranked english"}
{"id": "nan", "abstract": "  We demonstrate an embodied conversational agent that can function as areceptionist and generate a mixture of open and closed-domain dialogue alongwith facial expressions, by using a large language model (LLM) to develop anengaging conversation. We deployed the system onto a Furhat robot, which ishighly expressive and capable of using both verbal and nonverbal cues duringinteraction. The system was designed specifically for the National Robotariumto interact with visitors through natural conversations, providing them withinformation about the facilities, research, news, upcoming events, etc. Thesystem utilises the state-of-the-art GPT-3.5 model to generate such informationalong with domain-general conversations and facial expressions based on promptengineering.", "title": "furchat an embodied conversational agent using llms, combining open and closeddomain dialogue with facial expressions", "url": "http://arxiv.org/pdf/2308.15214v2.pdf", "tokenized_text": "demonstrate embodied conversational agent function generate mixture open closed domain dialogue alongwith facial expressions large_language large language llm develop conversation deployed system furhat robot expressive capable verbal nonverbal cues system designed specifically national interact natural conversations providing research news upcoming events etc thesystem state art gpt-3.5 generate domain general conversations facial expressions based promptengineering"}
{"id": "nan", "abstract": "  This paper investigates the potential improvement of the GPT-4 LanguageLearning Model (LLM) in comparison to BERT for modeling same-day daily stockprice movements of Apple and Tesla in 2017, based on sentiment analysis ofmicroblogging messages. We recorded daily adjusted closing prices andtranslated them into up-down movements. Sentiment for each day was extractedfrom messages on the Stocktwits platform using both LLMs. We develop a novelmethod to engineer a comprehensive prompt for contextual sentiment analysiswhich unlocks the true capabilities of modern LLM. This enables us to carefullyretrieve sentiments, perceived advantages or disadvantages, and the relevancetowards the analyzed company. Logistic regression is used to evaluate whetherthe extracted message contents reflect stock price movements. As a result,GPT-4 exhibited substantial accuracy, outperforming BERT in five out of sixmonths and substantially exceeding a naive buy-and-hold strategy, reaching apeak accuracy of 71.47 % in May. The study also highlights the importance ofprompt engineering in obtaining desired outputs from GPT-4's contextualabilities. However, the costs of deploying GPT-4 and the need for fine-tuningprompts highlight some practical considerations for its use.", "title": "linking microblogging sentiments to stock price movement an application of gpt4", "url": "http://arxiv.org/pdf/2308.16771v1.pdf", "tokenized_text": "paper investigates potential improvement gpt-4 languagelearning comparison bert modeling day daily movements apple 2017 based sentiment_analysis sentiment analysis messages recorded daily adjusted closing movements sentiment day extractedfrom messages platform llms develop novelmethod engineer comprehensive contextual sentiment unlocks true capabilities modern llm enables sentiments perceived advantages disadvantages analyzed company logistic regression evaluate whetherthe extracted message contents reflect stock price movements result gpt-4 exhibited substantial accuracy outperforming bert substantially exceeding naive hold strategy reaching accuracy study highlights importance ofprompt engineering obtaining desired outputs gpt-4 costs deploying gpt-4 need fine highlight practical considerations use"}
{"id": "nan", "abstract": "  Learning paradigms for large language models (LLMs) currently tend to fallwithin either in-context learning (ICL) or full fine-tuning. Each of thesecomes with their own trade-offs based on available data, model size, computecost, ease-of-use, and final quality with neither solution performing wellacross-the-board. In this article, we first describe ICL and fine-tuningparadigms in a way that highlights their natural connections. Based on theseconnections, we propose a new learning paradigm called FIAT that fuses the bestof these paradigms together, enabling prompt-engineered instructions andchain-of-thought reasoning with the very largest models while also usingsimilar methods to perform parameter updates on a modestly-sized LLM withparameter-efficient tuning. We evaluate FIAT's effectiveness on a variety ofmultilingual tasks and observe that FIAT performs better than both ICL andfine-tuning at scales ranging from 100-10,000 training examples. We hope thatFIAT provides a practical way of harnessing the full potential of LLMs withoutneeding to make a hard choice between learning paradigms.", "title": "fiat fusing learning paradigms with instructionaccelerated tuning", "url": "http://arxiv.org/pdf/2309.04663v2.pdf", "tokenized_text": "learning paradigms large_language large language llms currently tend context_learning context learning icl fine tuning trade offs based available data model_size size ease use final quality solution performing board article describe icl fine way highlights natural connections based propose_a_new propose new learning paradigm called paradigms enabling engineered instructions andchain thought reasoning largest methods perform parameter updates sized llm efficient tuning evaluate effectiveness variety ofmultilingual tasks observe performs better icl andfine tuning scales ranging 100 training_examples training examples hope provides practical way harnessing potential llms hard choice learning paradigms"}
{"id": "nan", "abstract": "  In this project, we want to explore the newly emerging field of promptengineering and apply it to the downstream task of detecting LM biases. Moreconcretely, we explore how to design prompts that can indicate 4 differenttypes of biases: (1) gender, (2) race, (3) sexual orientation, and (4)religion-based. Within our project, we experiment with different manuallycrafted prompts that can draw out the subtle biases that may be present in thelanguage model. We apply these prompts to multiple variations of popular andwell-recognized models: BERT, RoBERTa, and T5 to evaluate their biases. Weprovide a comparative analysis of these models and assess them using a two-foldmethod: use human judgment to decide whether model predictions are biased andutilize model-level judgment (through further prompts) to understand if a modelcan self-diagnose the biases of its own prediction.", "title": "detecting natural language biases with promptbased learning", "url": "http://arxiv.org/pdf/2309.05227v1.pdf", "tokenized_text": "project want explore newly emerging field promptengineering apply downstream task detecting lm biases explore design indicate biases gender race based project experiment different draw subtle biases present thelanguage apply multiple variations popular recognized bert roberta t5 evaluate biases weprovide comparative analysis assess use human judgment decide predictions biased level judgment understand self diagnose biases prediction"}
{"id": "nan", "abstract": "  In this paper we present the first investigation into the effectiveness ofLarge Language Models (LLMs) for Failure Mode Classification (FMC). FMC, thetask of automatically labelling an observation with a corresponding failuremode code, is a critical task in the maintenance domain as it reduces the needfor reliability engineers to spend their time manually analysing work orders.We detail our approach to prompt engineering to enable an LLM to predict thefailure mode of a given observation using a restricted code list. Wedemonstrate that the performance of a GPT-3.5 model (F1=0.80) fine-tuned onannotated data is a significant improvement over a currently available textclassification model (F1=0.60) trained on the same annotated data set. Thefine-tuned model also outperforms the out-of-the box GPT-3.5 (F1=0.46). Thisinvestigation reinforces the need for high quality fine-tuning data sets fordomain-specific tasks using LLMs.", "title": "large language models for failure mode classification an investigation", "url": "http://arxiv.org/pdf/2309.08181v1.pdf", "tokenized_text": "paper_we_present paper present investigation effectiveness oflarge language_models language llms failure mode classification thetask automatically labelling observation corresponding code critical task maintenance domain reduces needfor reliability engineers spend time manually work orders detail approach prompt_engineering engineering enable llm predict mode given observation restricted code list wedemonstrate performance gpt-3.5 fine tuned data significant improvement currently available textclassification trained annotated_data annotated data set thefine tuned outperforms box gpt-3.5 thisinvestigation need high_quality high quality fine tuning data sets specific tasks llms"}
{"id": "nan", "abstract": "  Mobile robots often rely on pre-existing maps for effective path planning andnavigation. However, when these maps are unavailable, particularly inunfamiliar environments, a different approach become essential. This paperintroduces DynaCon, a novel system designed to provide mobile robots withcontextual awareness and dynamic adaptability during navigation, eliminatingthe reliance of traditional maps. DynaCon integrates real-time feedback with anobject server, prompt engineering, and navigation modules. By harnessing thecapabilities of Large Language Models (LLMs), DynaCon not only understandspatterns within given numeric series but also excels at categorizing objectsinto matched spaces. This facilitates dynamic path planner imbued withcontextual awareness. We validated the effectiveness of DynaCon through anexperiment where a robot successfully navigated to its goal using reasoning.Source code and experiment videos for this work can be found at:https://sites.google.com/view/dynacon.", "title": "dynacon dynamic robot planner with contextual awareness via llms", "url": "http://arxiv.org/pdf/2309.16031v1.pdf", "tokenized_text": "mobile robots rely pre existing maps effective path planning maps unavailable particularly environments different approach essential paperintroduces novel system designed provide mobile robots awareness dynamic adaptability navigation reliance traditional maps integrates real time feedback server prompt_engineering engineering navigation modules harnessing thecapabilities large_language large language llms given numeric series excels categorizing matched spaces facilitates dynamic path planner awareness validated effectiveness robot successfully goal reasoning source_code source code experiment videos work found view"}
{"id": "nan", "abstract": "  In an era where cyberspace is both a battleground and a backbone of modernsociety, the urgency of safeguarding digital assets against ever-evolvingthreats is paramount. This paper introduces Cyber Sentinel, an innovativetask-oriented cybersecurity dialogue system that is effectively capable ofmanaging two core functions: explaining potential cyber threats within anorganization to the user, and taking proactive/reactive security actions wheninstructed by the user. Cyber Sentinel embodies the fusion of artificialintelligence, cybersecurity domain expertise, and real-time data analysis tocombat the multifaceted challenges posed by cyber adversaries. This articledelves into the process of creating such a system and how it can interact withother components typically found in cybersecurity organizations. Our work is anovel approach to task-oriented dialogue systems, leveraging the power ofchaining GPT-4 models combined with prompt engineering across all sub-tasks. Wealso highlight its pivotal role in enhancing cybersecurity communication andinteraction, concluding that not only does this framework enhance the system'stransparency (Explainable AI) but also streamlines the decision-making processand responding to threats (Actionable AI), therefore marking a significantadvancement in the realm of cybersecurity communication.", "title": "cyber sentinel exploring conversational agents in streamlining security tasks with gpt4", "url": "http://arxiv.org/pdf/2309.16422v1.pdf", "tokenized_text": "era backbone safeguarding digital assets paramount paper introduces cyber oriented cybersecurity dialogue system effectively capable core functions explaining potential cyber threats user taking proactive reactive security actions user cyber embodies fusion artificialintelligence cybersecurity domain expertise real time data analysis multifaceted challenges posed cyber adversaries process creating system interact components typically found cybersecurity organizations work anovel approach task oriented dialogue systems leveraging power gpt-4 combined prompt_engineering engineering sub tasks wealso highlight pivotal role enhancing cybersecurity communication concluding framework enhance explainable ai streamlines decision making processand threats actionable ai marking realm cybersecurity communication"}
{"id": "nan", "abstract": "  This paper presents the latest progress of GPTutor: a ChatGPT-poweredprogramming tool extension in Visual Studio Code. The emergence of LargeLanguage Models (LLMs) has improved software development efficiency, but theirperformance can be hindered by training data limitations and prompt designissues. Existing LLM development tools often operate as black boxes, with usersunable to view the prompts used and unable to improve performance by correctingprompts when errors occur. To address the aforementioned issues, GPTutor wasintroduced as an open-source AI pair programming tool, offering an alternativeto Copilot. GPTutor empowers users to customize prompts for various programminglanguages and scenarios, with support for 120+ human languages and 50+programming languages. Users can fine-tune prompts to correct the errors fromLLM for precision and efficient code generation. At the end of the paper, weunderscore GPTutor's potential through examples, including demonstrating itsproficiency in interpreting and generating Sui-Move, a newly introduced smartcontract language, using prompt engineering.", "title": "gptutor an opensource ai pair programming tool alternative to copilot", "url": "http://arxiv.org/pdf/2310.13896v3.pdf", "tokenized_text": "paper_presents paper presents latest progress chatgpt tool extension visual code emergence largelanguage_models largelanguage llms improved software development efficiency theirperformance hindered training_data training data limitations existing llm development tools operate black boxes view unable improve performance errors occur address aforementioned issues open source ai pair programming tool offering copilot empowers users customize scenarios support 120 human languages languages users fine tune correct errors precision efficient code_generation code generation end paper potential examples including demonstrating interpreting generating newly introduced language prompt_engineering engineering"}
{"id": "nan", "abstract": "  Pre-trained and frozen LLMs can effectively map simple scene re-arrangementinstructions to programs over a robot's visuomotor functions throughappropriate few-shot example prompting. To parse open-domain natural languageand adapt to a user's idiosyncratic procedures, not known during promptengineering time, fixed prompts fall short. In this paper, we introduce HELPER,an embodied agent equipped with an external memory of language-program pairsthat parses free-form human-robot dialogue into action programs throughretrieval-augmented LLM prompting: relevant memories are retrieved based on thecurrent dialogue, instruction, correction or VLM description, and used asin-context prompt examples for LLM querying. The memory is expanded duringdeployment to include pairs of user's language and action plans, to assistfuture inferences and personalize them to the user's language and routines.HELPER sets a new state-of-the-art in the TEACh benchmark in both Executionfrom Dialog History (EDH) and Trajectory from Dialogue (TfD), with 1.7ximprovement over the previous SOTA for TfD. Our models, code and video resultscan be found in our project's website: https://helper-agent-llm.github.io.", "title": "openended instructable embodied agents with memoryaugmented large language models", "url": "http://arxiv.org/pdf/2310.15127v1.pdf", "tokenized_text": "pre trained frozen llms effectively map simple scene programs robot visuomotor functions shot example parse open domain natural adapt user procedures known promptengineering time fixed fall short paper introduce embodied agent equipped external memory language program parses free form human robot dialogue action programs augmented llm relevant memories retrieved based thecurrent dialogue instruction correction vlm description asin context examples llm querying memory expanded include pairs user language action plans inferences user language routines sets new state art teach benchmark dialog history trajectory dialogue previous sota code video resultscan found project website"}
{"id": "nan", "abstract": "  Large language models (LLMs) offer unprecedented text completioncapabilities. As general models, they can fulfill a wide range of roles,including those of more specialized models. We assess the performance of GPT-4and GPT-3.5 in zero shot, few shot and fine-tuned settings on the aspect-basedsentiment analysis (ABSA) task. Fine-tuned GPT-3.5 achieves a state-of-the-artF1 score of 83.8 on the joint aspect term extraction and polarityclassification task of the SemEval-2014 Task 4, improving upon InstructABSA[@scaria_instructabsa_2023] by 5.7%. However, this comes at the price of 1000times more model parameters and thus increased inference cost. We discuss thethe cost-performance trade-offs of different models, and analyze the typicalerrors that they make. Our results also indicate that detailed prompts improveperformance in zero-shot and few-shot settings but are not necessary forfine-tuned models. This evidence is relevant for practioners that are facedwith the choice of prompt engineering versus fine-tuning when using LLMs forABSA.", "title": "large language models for aspectbased sentiment analysis", "url": "http://arxiv.org/pdf/2310.18025v1.pdf", "tokenized_text": "large_language large language llms offer unprecedented text general fulfill wide_range wide range roles including specialized assess performance gpt-4and gpt-3.5 zero_shot zero shot shot fine tuned settings aspect analysis absa task fine tuned gpt-3.5 achieves state score 83.8 joint aspect term extraction task task improving 5.7 comes price parameters increased inference cost discuss cost performance trade offs different analyze results indicate detailed improveperformance zero shot shot_settings shot settings necessary tuned evidence relevant choice prompt_engineering engineering versus fine tuning llms"}
{"id": "nan", "abstract": "  Large language models (LLMs) have demonstrated their potential in socialscience research by emulating human perceptions and behaviors, a conceptreferred to as algorithmic fidelity. This study assesses the algorithmicfidelity and bias of LLMs by utilizing two nationally representative climatechange surveys. The LLMs were conditioned on demographics and/or psychologicalcovariates to simulate survey responses. The findings indicate that LLMs caneffectively capture presidential voting behaviors but encounter challenges inaccurately representing global warming perspectives when relevant covariatesare not included. GPT-4 exhibits improved performance when conditioned on bothdemographics and covariates. However, disparities emerge in LLM estimations ofthe views of certain groups, with LLMs tending to underestimate worry aboutglobal warming among Black Americans. While highlighting the potential of LLMsto aid social science research, these results underscore the importance ofmeticulous conditioning, model selection, survey question format, and biasassessment when employing LLMs for survey simulation. Further investigationinto prompt engineering and algorithm auditing is essential to harness thepower of LLMs while addressing their inherent limitations.", "title": "can large language models capture public opinion about global warming an empirical assessment of algorithmic fidelity and bias", "url": "http://arxiv.org/pdf/2311.00217v1.pdf", "tokenized_text": "large_language large language llms demonstrated potential research emulating human perceptions behaviors algorithmic fidelity study assesses bias llms utilizing representative surveys llms conditioned demographics and/or simulate survey responses findings indicate llms caneffectively capture voting behaviors encounter challenges representing global perspectives relevant included gpt-4 exhibits improved performance conditioned covariates disparities emerge llm ofthe views certain groups llms black highlighting potential llmsto aid social science research results underscore importance conditioning selection survey question format employing llms survey simulation investigationinto prompt_engineering engineering algorithm auditing essential harness thepower llms addressing inherent limitations"}
{"id": "nan", "abstract": "  Recent advances in prompt engineering enable large language models (LLMs) tosolve multi-hop logical reasoning problems with impressive accuracy. However,there is little existing work investigating the robustness of LLMs withfew-shot prompting techniques. Therefore, we introduce a systematic approach totest the robustness of LLMs in multi-hop reasoning tasks via domain-agnosticperturbations. We include perturbations at multiple levels of abstractions(e.g. lexical perturbations such as typos, and semantic perturbations such asthe inclusion of intermediate reasoning steps in the questions) to conductbehavioral analysis on the LLMs. Throughout our experiments, we find thatmodels are more sensitive to certain perturbations such as replacing words withtheir synonyms. We also demonstrate that increasing the proportion of perturbedexemplars in the prompts improves the robustness of few-shot prompting methods.", "title": "noisy exemplars make large language models more robust a domainagnostic behavioral analysis", "url": "http://arxiv.org/pdf/2311.00258v1.pdf", "tokenized_text": "recent_advances recent advances prompt_engineering engineering enable large_language large language llms tosolve multi hop logical reasoning problems impressive accuracy little existing work investigating robustness llms withfew shot_prompting shot techniques introduce systematic approach robustness llms multi hop reasoning tasks domain include perturbations multiple levels lexical perturbations semantic perturbations asthe inclusion intermediate reasoning_steps reasoning steps questions analysis llms experiments find sensitive certain perturbations replacing words synonyms demonstrate increasing proportion improves robustness shot_prompting shot methods"}
{"id": "nan", "abstract": "  Recent studies have demonstrated the great potential of Large Language Models(LLMs) serving as zero-shot relevance rankers. The typical approach involvesmaking comparisons between pairs or lists of documents. Although effective,these listwise and pairwise methods are not efficient and also heavily rely onintricate prompt engineering. To tackle this problem, we introduce a novelinstruction distillation method. The key idea is to distill the pairwiseranking ability of open-sourced LLMs to a simpler but more efficient pointwiseranking. Specifically, given the same LLM, we first rank documents using theeffective pairwise approach with complex instructions, and then distill theteacher predictions to the pointwise approach with simpler instructions.Evaluation results on the BEIR, TREC, and ReDial datasets demonstrate thatinstruction distillation can improve efficiency by 10 to 100x and also enhancethe ranking performance of LLMs. Furthermore, our approach surpasses theperformance of existing supervised methods like monoT5 and is on par with thestate-of-the-art zero-shot methods. The code to reproduce our results isavailable at www.github.com/sunnweiwei/RankGPT.", "title": "instruction distillation makes large language models efficient zeroshot rankers", "url": "http://arxiv.org/pdf/2311.01555v1.pdf", "tokenized_text": "recent studies demonstrated great_potential great potential large_language large language models(llms serving zero shot relevance rankers typical approach comparisons pairs lists documents effective listwise pairwise methods efficient heavily rely prompt_engineering engineering tackle problem introduce distillation method key idea distill ability open sourced llms simpler efficient specifically given llm rank documents pairwise approach complex instructions distill predictions pointwise approach simpler instructions evaluation results beir trec redial datasets demonstrate thatinstruction distillation improve efficiency 10 100x enhancethe ranking performance llms furthermore approach surpasses theperformance existing supervised methods like par thestate art zero shot methods code reproduce results isavailable"}
{"id": "nan", "abstract": "  Online forums encourage the exchange and discussion of different stances onmany topics. Not only do they provide an opportunity to present one's ownarguments, but may also gather a broad cross-section of others' arguments.However, the resulting long discussions are difficult to overview. This paperpresents a novel unsupervised approach using large language models (LLMs) togenerating indicative summaries for long discussions that basically serve astables of contents. Our approach first clusters argument sentences, generatescluster labels as abstractive summaries, and classifies the generated clusterlabels into argumentation frames resulting in a two-level summary. Based on anextensively optimized prompt engineering approach, we evaluate 19~LLMs forgenerative cluster labeling and frame classification. To evaluate theusefulness of our indicative summaries, we conduct a purpose-driven user studyvia a new visual interface called Discussion Explorer: It shows that ourproposed indicative summaries serve as a convenient navigation tool to explorelong discussions.", "title": "indicative summarization of long discussions", "url": "http://arxiv.org/pdf/2311.01882v1.pdf", "tokenized_text": "online forums encourage exchange discussion different onmany topics provide opportunity present gather broad cross section arguments resulting long discussions difficult overview paperpresents novel unsupervised approach large_language large language llms togenerating indicative summaries long discussions serve contents approach clusters argument sentences labels abstractive summaries generated frames resulting level summary based optimized prompt_engineering engineering approach evaluate 19 llms cluster labeling frame classification evaluate theusefulness indicative summaries conduct purpose driven user new visual interface called discussion shows ourproposed indicative summaries serve convenient navigation tool discussions"}
{"id": "nan", "abstract": "  Identifying contextual integrity (CI) and governing knowledge commons (GKC)parameters in privacy policy texts can facilitate normative privacy analysis.However, GKC-CI annotation has heretofore required manual or crowdsourcedeffort. This paper demonstrates that high-accuracy GKC-CI parameter annotationof privacy policies can be performed automatically using large language models.We fine-tune 18 open-source and proprietary models on 21,588 GKC-CI annotationsfrom 16 ground truth privacy policies. Our best-performing model (fine-tunedGPT-3.5 Turbo with prompt engineering) has an accuracy of 86%, exceeding theperformance of prior crowdsourcing approaches despite the complexity of privacypolicy texts and the nuance of the GKC-CI annotation task. We apply ourbest-performing model to privacy policies from 164 popular online services,demonstrating the effectiveness of scaling GKC-CI annotation for dataexploration. We make all annotated policies as well as the training data andscripts needed to fine-tune our best-performing model publicly available forfuture research.", "title": "automating governing knowledge commons and contextual integrity (gkcci) privacy policy annotations with large language models", "url": "http://arxiv.org/pdf/2311.02192v1.pdf", "tokenized_text": "identifying contextual integrity ci governing knowledge commons privacy policy texts facilitate privacy analysis ci annotation required manual paper demonstrates high accuracy ci parameter privacy policies performed automatically large_language large language fine tune 18 open source proprietary ci 16 ground_truth ground truth privacy policies best performing fine turbo prompt_engineering engineering accuracy 86 exceeding theperformance prior crowdsourcing approaches despite complexity texts nuance ci annotation task apply ourbest performing privacy policies popular online services demonstrating effectiveness scaling ci annotation dataexploration annotated policies training_data training data needed fine tune best performing publicly_available publicly available forfuture research"}
{"id": "nan", "abstract": "  [Context]: Companies are increasingly recognizing the importance ofautomating Requirements Engineering (RE) tasks due to their resource-intensivenature. The advent of GenAI has made these tasks more amenable to automation,thanks to its ability to understand and interpret context effectively.[Problem]: However, in the context of GenAI, prompt engineering is a criticalfactor for success. Despite this, we currently lack tools and methods tosystematically assess and determine the most effective prompt patterns toemploy for a particular RE task. [Method]: Two tasks related to requirements,specifically requirement classification and tracing, were automated using theGPT-3.5 turbo API. The performance evaluation involved assessing variousprompts created using 5 prompt patterns and implemented programmatically toperform the selected RE tasks, focusing on metrics such as precision, recall,accuracy, and F-Score. [Results]: This paper evaluates the effectiveness of the5 prompt patterns' ability to make GPT-3.5 turbo perform the selected RE tasksand offers recommendations on which prompt pattern to use for a specific REtask. Additionally, it also provides an evaluation framework as a reference forresearchers and practitioners who want to evaluate different prompt patternsfor different RE tasks.", "title": "requirements engineering using generative ai prompts and prompting patterns", "url": "http://arxiv.org/pdf/2311.03832v1.pdf", "tokenized_text": "context companies increasingly recognizing importance requirements engineering tasks resource advent genai tasks amenable automation thanks ability understand interpret context context genai prompt_engineering engineering success despite currently lack tools methods assess determine effective patterns particular task method tasks related requirements specifically requirement classification tracing automated turbo api performance evaluation involved assessing created patterns implemented toperform selected tasks focusing metrics precision recall accuracy score results paper evaluates effectiveness patterns ability gpt-3.5 turbo perform selected offers recommendations pattern use specific additionally provides evaluation framework reference practitioners want evaluate different different tasks"}
{"id": "nan", "abstract": "  The canonical approach to video action recognition dictates a neural model todo a classic and standard 1-of-N majority vote task. They are trained topredict a fixed set of predefined categories, limiting their transferableability on new datasets with unseen concepts. In this paper, we provide a newperspective on action recognition by attaching importance to the semanticinformation of label texts rather than simply mapping them into numbers.Specifically, we model this task as a video-text matching problem within amultimodal learning framework, which strengthens the video representation withmore semantic language supervision and enables our model to do zero-shot actionrecognition without any further labeled data or parameters requirements.Moreover, to handle the deficiency of label texts and make use of tremendousweb data, we propose a new paradigm based on this multimodal learning frameworkfor action recognition, which we dub \"pre-train, prompt and fine-tune\". Thisparadigm first learns powerful representations from pre-training on a largeamount of web image-text or video-text data. Then it makes the actionrecognition task to act more like pre-training problems via prompt engineering.Finally, it end-to-end fine-tunes on target datasets to obtain strongperformance. We give an instantiation of the new paradigm, ActionCLIP, whichnot only has superior and flexible zero-shot/few-shot transfer ability but alsoreaches a top performance on general action recognition task, achieving 83.8%top-1 accuracy on Kinetics-400 with a ViT-B/16 as the backbone. Code isavailable at https://github.com/sallymmx/ActionCLIP.git", "title": "actionclip a new paradigm for video action recognition", "url": "http://arxiv.org/pdf/2109.08472v1.pdf", "tokenized_text": "canonical approach video action recognition neural classic standard majority vote task trained topredict fixed set predefined categories limiting new datasets unseen concepts paper provide action recognition attaching importance semanticinformation label texts simply mapping numbers specifically task video text matching problem amultimodal learning framework strengthens video representation withmore semantic language supervision enables zero shot labeled_data labeled data parameters requirements handle deficiency label texts use data propose_a_new propose new paradigm based multimodal learning frameworkfor action recognition pre train fine tune learns powerful representations pre training largeamount web image text video text data makes task act like pre training problems prompt_engineering engineering finally end end fine tunes target datasets obtain new_paradigm new paradigm superior flexible zero shot shot transfer ability performance general action recognition task achieving accuracy vit backbone code isavailable"}
{"id": "nan", "abstract": "  Recently, vision-language pre-training shows great potential inopen-vocabulary object detection, where detectors trained on base classes aredevised for detecting new classes. The class text embedding is firstlygenerated by feeding prompts to the text encoder of a pre-trainedvision-language model. It is then used as the region classifier to supervisethe training of a detector. The key element that leads to the success of thismodel is the proper prompt, which requires careful words tuning and ingeniousdesign. To avoid laborious prompt engineering, there are some promptrepresentation learning methods being proposed for the image classificationtask, which however can only be sub-optimal solutions when applied to thedetection task. In this paper, we introduce a novel method, detection prompt(DetPro), to learn continuous prompt representations for open-vocabulary objectdetection based on the pre-trained vision-language model. Different from theprevious classification-oriented methods, DetPro has two highlights: 1) abackground interpretation scheme to include the proposals in image backgroundinto the prompt training; 2) a context grading scheme to separate proposals inimage foreground for tailored prompt training. We assemble DetPro with ViLD, arecent state-of-the-art open-world object detector, and conduct experiments onthe LVIS as well as transfer learning on the Pascal VOC, COCO, Objects365datasets. Experimental results show that our DetPro outperforms the baselineViLD in all settings, e.g., +3.4 APbox and +3.0 APmask improvements on thenovel classes of LVIS. Code and models are available athttps://github.com/dyabel/detpro.", "title": "learning to prompt for openvocabulary object detection with visionlanguage model", "url": "http://arxiv.org/pdf/2203.14940v1.pdf", "tokenized_text": "recently vision language pre training shows great_potential great potential inopen vocabulary object_detection object detection detectors trained base classes detecting new classes class text embedding feeding text encoder pre language_model language region classifier training detector key leads success proper requires careful words tuning avoid laborious prompt_engineering engineering learning methods proposed image sub optimal solutions applied task paper introduce novel method detection learn continuous representations open vocabulary based pre trained vision language_model language different theprevious classification oriented methods highlights interpretation scheme include proposals image training context grading scheme separate proposals foreground tailored training assemble arecent state art open world object detector conduct experiments onthe lvis transfer learning pascal voc coco experimental_results experimental results outperforms settings e.g. improvements thenovel classes lvis code available"}
{"id": "nan", "abstract": "  The application of zero-shot learning in computer vision has beenrevolutionized by the use of image-text matching models. The most notableexample, CLIP, has been widely used for both zero-shot classification andguiding generative models with a text prompt. However, the zero-shot use ofCLIP is unstable with respect to the phrasing of the input text, making itnecessary to carefully engineer the prompts used. We find that this instabilitystems from a selective similarity score, which is based only on a subset of thesemantically meaningful input tokens. To mitigate it, we present a novelexplainability-based approach, which adds a loss term to ensure that CLIPfocuses on all relevant semantic parts of the input, in addition to employingthe CLIP similarity loss used in previous works. When applied to one-shotclassification through prompt engineering, our method yields an improvement inthe recognition rate, without additional training or fine-tuning. Additionally,we show that CLIP guidance of generative models using our method significantlyimproves the generated images. Finally, we demonstrate a novel use of CLIPguidance for text-based image generation with spatial conditioning on objectlocation, by requiring the image explainability heatmap for each object to beconfined to a pre-determined bounding box.", "title": "no token left behind explainabilityaided image classification and generation", "url": "http://arxiv.org/pdf/2204.04908v2.pdf", "tokenized_text": "application zero shot_learning shot learning computer_vision computer vision use image text matching clip widely zero shot classification andguiding generative text zero shot use unstable respect phrasing input text making carefully engineer find selective similarity score based subset meaningful input tokens mitigate present based approach adds loss term ensure relevant semantic parts input addition clip similarity loss previous works applied shotclassification prompt_engineering engineering method yields improvement inthe recognition rate additional training fine tuning additionally clip guidance generative method significantlyimproves generated images finally demonstrate novel use text based image_generation image generation spatial conditioning requiring image explainability object pre determined bounding box"}
{"id": "nan", "abstract": "  Large language models trained on a mixture of NLP tasks that are convertedinto a text-to-text format using prompts, can generalize into novel forms oflanguage and handle novel tasks. A large body of work within prompt engineeringattempts to understand the effects of input forms and prompts in achievingsuperior performance. We consider an alternative measure and inquire whetherthe way in which an input is encoded affects social biases promoted in outputs.In this paper, we study T0, a large-scale multi-task text-to-text languagemodel trained using prompt-based learning. We consider two different forms ofsemantically equivalent inputs: question-answer format and premise-hypothesisformat. We use an existing bias benchmark for the former BBQ and create thefirst bias benchmark in natural language inference BBNLI with hand-writtenhypotheses while also converting each benchmark into the other form. Theresults on two benchmarks suggest that given two different formulations ofessentially the same input, T0 conspicuously acts more biased in questionanswering form, which is seen during training, compared to premise-hypothesisform which is unlike its training examples. Code and data are released underhttps://github.com/feyzaakyurek/bbnli.", "title": "on measuring social biases in promptbased multitask learning", "url": "http://arxiv.org/pdf/2205.11605v1.pdf", "tokenized_text": "large_language large language trained mixture nlp_tasks nlp tasks convertedinto text text format generalize novel forms oflanguage handle novel tasks large body work understand effects input forms performance consider alternative measure inquire whetherthe way input encoded affects social biases promoted outputs paper study t0 large scale multi task text text languagemodel trained based learning consider different forms equivalent inputs question answer format premise use existing bias benchmark create thefirst bias benchmark natural_language natural language inference hand converting benchmark form theresults benchmarks suggest given different formulations input t0 conspicuously acts biased questionanswering form seen training compared premise unlike training_examples training examples code data released"}
{"id": "nan", "abstract": "  This paper presents a language-powered paradigm for ordinal regression.Existing methods usually treat each rank as a category and employ a set ofweights to learn these concepts. These methods are easy to overfit and usuallyattain unsatisfactory performance as the learned concepts are mainly derivedfrom the training set. Recent large pre-trained vision-language models likeCLIP have shown impressive performance on various visual tasks. In this paper,we propose to learn the rank concepts from the rich semantic CLIP latent space.Specifically, we reformulate this task as an image-language matching problemwith a contrastive objective, which regards labels as text and obtains alanguage prototype from a text encoder for each rank. While prompt engineeringfor CLIP is extremely time-consuming, we propose OrdinalCLIP, a differentiableprompting method for adapting CLIP for ordinal regression. OrdinalCLIP consistsof learnable context tokens and learnable rank embeddings; The learnable rankembeddings are constructed by explicitly modeling numerical continuity,resulting in well-ordered, compact language prototypes in the CLIP space. Oncelearned, we can only save the language prototypes and discard the huge languagemodel, resulting in zero additional computational overhead compared with thelinear head counterpart. Experimental results show that our paradigm achievescompetitive performance in general ordinal regression tasks, and gainsimprovements in few-shot and distribution shift settings for age estimation.The code is available at https://github.com/xk-huang/OrdinalCLIP.", "title": "ordinalclip learning rank prompts for languageguided ordinal regression", "url": "http://arxiv.org/pdf/2206.02338v2.pdf", "tokenized_text": "paper_presents paper presents language powered paradigm ordinal regression existing_methods existing methods usually treat rank category employ set learn concepts methods easy overfit unsatisfactory performance learned concepts mainly training set recent large pre trained vision language_models language shown_impressive shown impressive performance visual tasks paper propose learn rank concepts rich semantic clip latent space specifically reformulate task image language matching contrastive objective labels text obtains alanguage prototype text encoder rank clip extremely time consuming propose method adapting clip ordinal regression learnable context tokens learnable rank embeddings learnable constructed explicitly modeling numerical resulting compact language prototypes clip space save language prototypes discard huge languagemodel resulting zero additional computational overhead compared head counterpart experimental_results experimental results paradigm achievescompetitive performance general ordinal regression tasks shot distribution shift settings age estimation code_is_available code available"}
{"id": "nan", "abstract": "  The field of data visualisation has long aimed to devise solutions forgenerating visualisations directly from natural language text. Research inNatural Language Interfaces (NLIs) has contributed towards the development ofsuch techniques. However, the implementation of workable NLIs has always beenchallenging due to the inherent ambiguity of natural language, as well as inconsequence of unclear and poorly written user queries which pose problems forexisting language models in discerning user intent. Instead of pursuing theusual path of developing new iterations of language models, this study uniquelyproposes leveraging the advancements in pre-trained large language models(LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directlyinto code for appropriate visualisations. This paper presents a novel system,Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrateshow, with effective prompt engineering, the complex problem of languageunderstanding can be solved more efficiently, resulting in simpler and moreaccurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMstogether with the proposed prompts offer a reliable approach to renderingvisualisations from natural language queries, even when queries are highlymisspecified and underspecified. This solution also presents a significantreduction in costs for the development of NLI systems, while attaining greatervisualisation inference abilities compared to traditional NLP approaches thatuse hand-crafted grammar rules and tailored models. This study also presentshow LLM prompts can be constructed in a way that preserves data security andprivacy while being generalisable to different datasets. This work compares theperformance of GPT-3, Codex and ChatGPT across a number of case studies andcontrasts the performances with prior studies.", "title": "chat2vis generating data visualisations via natural language using chatgpt, codex and gpt3 large language models", "url": "http://arxiv.org/pdf/2302.02094v2.pdf", "tokenized_text": "field data long aimed devise solutions forgenerating directly natural_language natural language text research innatural language interfaces contributed development techniques implementation inherent ambiguity natural_language natural language unclear poorly written user queries pose problems language_models language discerning user intent instead pursuing path developing new iterations language_models language study leveraging advancements pre trained large_language large language models(llms chatgpt gpt-3 convert free form natural_language natural language code appropriate paper_presents paper presents novel system takes advantage capabilities llms effective prompt_engineering engineering complex problem languageunderstanding solved efficiently resulting simpler end end solutions prior approaches shows proposed offer reliable approach natural_language natural language queries queries underspecified solution presents costs development nli systems attaining inference abilities compared traditional nlp approaches hand crafted grammar rules tailored study llm constructed way preserves data security generalisable different datasets work compares theperformance gpt-3 codex chatgpt number case studies andcontrasts performances prior studies"}
{"id": "nan", "abstract": "  Text-to-Image generation models have revolutionized the artwork designprocess and enabled anyone to create high-quality images by entering textdescriptions called prompts. Creating a high-quality prompt that consists of asubject and several modifiers can be time-consuming and costly. In consequence,a trend of trading high-quality prompts on specialized marketplaces hasemerged. In this paper, we propose a novel attack, namely prompt stealingattack, which aims to steal prompts from generated images by text-to-imagegeneration models. Successful prompt stealing attacks direct violate theintellectual property and privacy of prompt engineers and also jeopardize thebusiness model of prompt trading marketplaces. We first perform a large-scaleanalysis on a dataset collected by ourselves and show that a successful promptstealing attack should consider a prompt's subject as well as its modifiers. Wethen propose the first learning-based prompt stealing attack, PromptStealer,and demonstrate its superiority over two baseline methods quantitatively andqualitatively. We also make some initial attempts to defend PromptStealer. Ingeneral, our study uncovers a new attack surface in the ecosystem created bythe popular text-to-image generation models. We hope our results can help tomitigate the threat. To facilitate research in this field, we will share ourdataset and code with the community.", "title": "prompt stealing attacks against texttoimage generation models", "url": "http://arxiv.org/pdf/2302.09923v1.pdf", "tokenized_text": "text image_generation image generation revolutionized artwork enabled create high quality images entering called creating high quality consists modifiers time consuming costly consequence trend trading high quality specialized paper propose_a_novel propose novel attack aims steal generated images text imagegeneration successful attacks direct property privacy engineers trading perform large dataset collected successful attack consider subject modifiers wethen propose learning based attack demonstrate superiority baseline methods quantitatively andqualitatively initial attempts defend ingeneral study uncovers new attack surface ecosystem created bythe popular text image_generation image generation hope results help threat facilitate research field share code community"}
{"id": "nan", "abstract": "  There has been a growing effort to replace hand extraction of data fromresearch papers with automated data extraction based on natural languageprocessing, language models, and recently, large language models (LLMs).Although these methods enable efficient extraction of data from large sets ofresearch papers, they require a significant amount of up-front effort,expertise, and coding. In this work we propose the ChatExtract method that canfully automate very accurate data extraction with minimal initial effort andbackground, using an advanced conversational LLM. ChatExtract consists of a setof engineered prompts applied to a conversational LLM that both identifysentences with data, extract that data, and assure the data's correctnessthrough a series of follow-up questions. These follow-up questions largelyovercome known issues with LLMs providing factually inaccurate responses.ChatExtract can be applied with any conversational LLMs and yields very highquality data extraction. In tests on materials data we find precision andrecall both close to 90% from the best conversational LLMs, like ChatGPT-4. Wedemonstrate that the exceptional performance is enabled by the informationretention in a conversational model combined with purposeful redundancy andintroducing uncertainty through follow-up prompts. These results suggest thatapproaches similar to ChatExtract, due to their simplicity, transferability,and accuracy are likely to become powerful tools for data extraction in thenear future. Finally, databases for critical cooling rates of metallic glassesand yield strengths of high entropy alloys are developed using ChatExtract.", "title": "extracting accurate materials data from research papers with conversational language models and prompt engineering", "url": "http://arxiv.org/pdf/2303.05352v2.pdf", "tokenized_text": "growing effort replace hand extraction data papers automated data extraction based natural languageprocessing language_models language recently large_language large language methods enable efficient extraction data large sets ofresearch papers require significant effort expertise coding work propose method automate accurate data extraction minimal initial effort advanced conversational llm consists setof engineered applied conversational llm data extract data data series follow questions follow questions known issues llms providing factually inaccurate responses applied conversational llms yields data extraction tests materials data find precision close 90 best conversational llms like chatgpt-4 wedemonstrate exceptional performance enabled conversational combined redundancy uncertainty follow results suggest similar simplicity transferability accuracy likely powerful tools data extraction future finally databases critical rates yield strengths high entropy developed"}
{"id": "nan", "abstract": "  The rise of advanced chatbots, such as ChatGPT, has sparked curiosity in thescientific community. ChatGPT is a general-purpose chatbot powered by largelanguage models (LLMs) GPT-3.5 and GPT-4, with the potential to impact numerousfields, including computational biology. In this article, we offer ten tipsbased on our experience with ChatGPT to assist computational biologists inoptimizing their workflows. We have collected relevant prompts and reviewed thenascent literature in the field, compiling tips we project to remain pertinentfor future ChatGPT and LLM iterations, ranging from code refactoring toscientific writing to prompt engineering. We hope our work will helpbioinformaticians to complement their workflows while staying aware of thevarious implications of using this technology. Additionally, to track new andcreative applications for bioinformatics tools such as ChatGPT, we haveestablished a GitHub repository athttps://github.com/csbl-br/awesome-compbio-chatgpt. Our belief is that ethicaladherence to ChatGPT and other LLMs will increase the efficiency ofcomputational biologists, ultimately advancing the pace of scientific discoveryin the life sciences.", "title": "ten quick tips for harnessing the power of chatgptgpt4 in computational biology", "url": "http://arxiv.org/pdf/2303.16429v1.pdf", "tokenized_text": "rise advanced chatbots chatgpt sparked curiosity community chatgpt general purpose chatbot powered largelanguage_models largelanguage llms gpt-3.5 gpt-4 potential impact including computational biology article offer experience chatgpt assist computational workflows collected relevant reviewed literature field compiling project remain future chatgpt llm iterations ranging code refactoring writing prompt_engineering engineering hope work complement workflows aware thevarious implications technology additionally track new applications bioinformatics tools chatgpt github repository belief chatgpt llms increase efficiency ultimately advancing pace scientific life sciences"}
{"id": "nan", "abstract": "  The latest large language models (LLMs) such as ChatGPT, exhibit strongcapabilities in automated mental health analysis. However, existing relevantstudies bear several limitations, including inadequate evaluations, lack ofprompting strategies, and ignorance of exploring LLMs for explainability. Tobridge these gaps, we comprehensively evaluate the mental health analysis andemotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explorethe effects of different prompting strategies with unsupervised and distantlysupervised emotional information. Based on these prompts, we explore LLMs forinterpretable mental health analysis by instructing them to generateexplanations for each of their decisions. We convey strict human evaluations toassess the quality of the generated explanations, leading to a novel datasetwith 163 human-assessed explanations. We benchmark existing automaticevaluation metrics on this dataset to guide future related works. According tothe results, ChatGPT shows strong in-context learning ability but still has asignificant gap with advanced task-specific methods. Careful prompt engineeringwith emotional cues and expert-written few-shot examples can also effectivelyimprove performance on mental health analysis. In addition, ChatGPT generatesexplanations that approach human performance, showing its great potential inexplainable mental health analysis.", "title": "towards interpretable mental health analysis with large language models", "url": "http://arxiv.org/pdf/2304.03347v4.pdf", "tokenized_text": "latest large_language large language llms chatgpt exhibit automated mental_health mental health analysis existing bear limitations including inadequate evaluations lack ofprompting strategies exploring llms explainability tobridge gaps comprehensively evaluate mental_health mental health analysis reasoning ability llms 11 datasets tasks explorethe effects different strategies unsupervised emotional information based explore llms mental_health mental health analysis instructing decisions convey strict human evaluations toassess quality generated explanations leading novel human assessed explanations benchmark existing automaticevaluation metrics dataset guide future related works according tothe results chatgpt shows strong context_learning context learning ability asignificant gap advanced task specific methods careful emotional cues expert written shot examples effectivelyimprove performance mental_health mental health analysis addition chatgpt approach human performance showing great_potential great potential mental_health mental health analysis"}
{"id": "nan", "abstract": "  Effectively utilizing LLMs for complex tasks is challenging, often involvinga time-consuming and uncontrollable prompt engineering process. This paperintroduces a novel human-LLM interaction framework, Low-code LLM. Itincorporates six types of simple low-code visual programming interactions, allsupported by clicking, dragging, or text editing, to achieve more controllableand stable responses. Through visual interaction with a graphical userinterface, users can incorporate their ideas into the workflow without writingtrivial prompts. The proposed Low-code LLM framework consists of a Planning LLMthat designs a structured planning workflow for complex tasks, which can becorrespondingly edited and confirmed by users through low-code visualprogramming operations, and an Executing LLM that generates responses followingthe user-confirmed workflow. We highlight three advantages of the low-code LLM:controllable generation results, user-friendly human-LLM interaction, andbroadly applicable scenarios. We demonstrate its benefits using four typicalapplications. By introducing this approach, we aim to bridge the gap betweenhumans and LLMs, enabling more effective and efficient utilization of LLMs forcomplex tasks. Our system will be soon publicly available at LowCodeLLM.", "title": "lowcode llm visual programming over llms", "url": "http://arxiv.org/pdf/2304.08103v2.pdf", "tokenized_text": "effectively utilizing llms complex tasks challenging time consuming prompt_engineering engineering process paperintroduces novel human llm interaction framework low code llm types simple low code visual programming interactions text editing achieve stable responses visual interaction graphical users incorporate ideas workflow proposed low code llm framework consists planning designs structured planning workflow complex tasks edited confirmed users low code operations executing llm generates responses followingthe user confirmed workflow highlight advantages low code llm controllable generation results user friendly human llm interaction applicable scenarios demonstrate benefits introducing approach aim bridge gap llms enabling effective efficient utilization llms tasks system soon publicly_available publicly available"}
{"id": "nan", "abstract": "  Recently, the ChatGPT LLM has received great attention: it can be used as abot for discussing source code, prompting it to suggest changes, providedescriptions or even generate code. Typical demonstrations generally focus onexisting benchmarks, which may have been used in model training (i.e., dataleakage). To assess the feasibility of using an LLM as a useful assistant botfor programmers, we must assess its realistic capabilities on unseen problemsas well as its capabilities on various tasks. In this paper, we present anempirical study of ChatGPT's potential as a fully automated programmingassistant, focusing on the tasks of code generation, program repair, and codesummariziation. The study investigates ChatGPT's performance on commonprogramming problems and compares it with state-of-the-art approaches on twobenchmarks. Among several findings, our study shows that ChatGPT is effectivein dealing with common programming problems. However, our experiments alsoreveal limitations in terms of its attention span: detailed descriptions willconstrain the focus of ChatGPT and prevent it from leveraging its vastknowledge to solve the actual problem. Surprisingly, we have identified theability of ChatGPT to reason the original intention of the code. We expectfuture work to build on this insight for dealing with the open question of theoracle problem. Our findings contribute interesting insights to the developmentof LLMs for programming assistance, notably by demonstrating the importance ofprompt engineering, and providing a better understanding of ChatGPT's practicalapplications for software engineering.", "title": "is chatgpt the ultimate programming assistant how far is it", "url": "http://arxiv.org/pdf/2304.11938v2.pdf", "tokenized_text": "recently chatgpt llm received great attention discussing source_code source code suggest changes generate code typical demonstrations generally focus benchmarks training i.e. assess feasibility llm useful assistant programmers assess realistic capabilities unseen capabilities tasks paper present study chatgpt potential fully automated focusing tasks code_generation code generation program repair study investigates chatgpt performance problems compares state art approaches findings study shows chatgpt dealing common programming problems experiments limitations terms attention span detailed descriptions focus chatgpt prevent leveraging solve actual problem surprisingly identified theability chatgpt reason original intention code work build insight dealing open question problem findings contribute interesting insights developmentof llms programming assistance notably demonstrating importance ofprompt engineering providing better understanding chatgpt practicalapplications software engineering"}
{"id": "nan", "abstract": "  Identifying the frames of news is important to understand the articles'vision, intention, message to be conveyed, and which aspects of the news areemphasized. Framing is a widely studied concept in journalism, and has emergedas a new topic in computing, with the potential to automate processes andfacilitate the work of journalism professionals. In this paper, we study thisissue with articles related to the Covid-19 anti-vaccine movement. First, tounderstand the perspectives used to treat this theme, we developed a protocolfor human labeling of frames for 1786 headlines of No-Vax movement articles ofEuropean newspapers from 5 countries. Headlines are key units in the writtenpress, and worth of analysis as many people only read headlines (or use them toguide their decision for further reading.) Second, considering advances inNatural Language Processing (NLP) with large language models, we investigatedtwo approaches for frame inference of news headlines: first with a GPT-3.5fine-tuning approach, and second with GPT-3.5 prompt-engineering. Our workcontributes to the study and analysis of the performance that these models haveto facilitate journalistic tasks like classification of frames, whileunderstanding whether the models are able to replicate human perception in theidentification of these frames.", "title": "framing the newsfrom human perception to large language model inferences", "url": "http://arxiv.org/pdf/2304.14456v1.pdf", "tokenized_text": "identifying frames news important understand intention message conveyed aspects news framing widely studied concept emergedas new topic computing potential automate processes work professionals paper study thisissue articles related anti vaccine movement tounderstand perspectives treat developed human labeling frames headlines movement articles countries headlines key units worth analysis people read headlines use toguide decision reading second considering advances innatural language_processing language processing nlp large_language large language approaches frame inference news headlines tuning approach second gpt-3.5 engineering study analysis performance facilitate journalistic tasks like classification frames able replicate human perception frames"}
{"id": "nan", "abstract": "  Prompt engineering relevance research has seen a notable surge in recentyears, primarily driven by advancements in pre-trained language models andlarge language models. However, a critical issue has been identified withinthis domain: the inadequate of sensitivity and robustness of these modelstowards Prompt Templates, particularly in lesser-studied languages such asJapanese. This paper explores this issue through a comprehensive evaluation ofseveral representative Large Language Models (LLMs) and a widely-utilizedpre-trained model(PLM). These models are scrutinized using a benchmark datasetin Japanese, with the aim to assess and analyze the performance of the currentmultilingual models in this context. Our experimental results reveal startlingdiscrepancies. A simple modification in the sentence structure of the PromptTemplate led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44.This observation underscores the fact that even the highly performance GPT-4model encounters significant stability issues when dealing with diverseJapanese prompt templates, rendering the consistency of the model's outputresults questionable. In light of these findings, we conclude by proposingpotential research trajectories to further enhance the development andperformance of Large Language Models in their current stage.", "title": "sensitivity and robustness of large language models to prompt template in japanese text classification tasks", "url": "http://arxiv.org/pdf/2305.08714v2.pdf", "tokenized_text": "prompt_engineering engineering relevance research seen notable surge primarily driven advancements pre trained_language trained language language_models language critical issue identified domain inadequate sensitivity robustness prompt_templates templates particularly lesser studied languages paper explores issue comprehensive evaluation ofseveral representative large_language large language llms widely trained model(plm benchmark japanese aim assess analyze performance context experimental_results experimental results reveal simple modification sentence structure prompttemplate led drastic drop accuracy gpt-4 observation underscores fact highly performance encounters significant stability issues dealing prompt_templates templates rendering consistency questionable light findings conclude research trajectories enhance development large_language large language current stage"}
{"id": "nan", "abstract": "  This study explores the robustness of university assessments against the useof Open AI's Generative Pre-Trained Transformer 4 (GPT-4) generated content andevaluates the ability of academic staff to detect its use when supported by theTurnitin Artificial Intelligence (AI) detection tool. The research involvedtwenty-two GPT-4 generated submissions being created and included in theassessment process to be marked by fifteen different faculty members. The studyreveals that although the detection tool identified 91% of the experimentalsubmissions as containing some AI-generated content, the total detected contentwas only 54.8%. This suggests that the use of adversarial techniques regardingprompt engineering is an effective method in evading AI detection tools andhighlights that improvements to AI detection software are needed. Using theTurnitin AI detect tool, faculty reported 54.5% of the experimental submissionsto the academic misconduct process, suggesting the need for increased awarenessand training into these tools. Genuine submissions received a mean score of54.4, whereas AI-generated content scored 52.3, indicating the comparableperformance of GPT-4 in real-life situations. Recommendations include adjustingassessment strategies to make them more resistant to the use of AI tools, usingAI-inclusive assessment where possible, and providing comprehensive trainingprograms for faculty and students. This research contributes to understandingthe relationship between AI-generated content and academic assessment, urgingfurther investigation to preserve academic integrity.", "title": "game of tones faculty detection of gpt4 generated content in university assessments", "url": "http://arxiv.org/pdf/2305.18081v1.pdf", "tokenized_text": "study explores robustness university assessments useof open ai generative pre-trained transformer gpt-4 generated content ability academic detect use supported artificial_intelligence artificial intelligence ai detection tool research gpt-4 generated submissions created included process marked different members detection tool identified containing ai generated content total detected suggests use adversarial techniques engineering effective method evading ai detection tools improvements ai detection software needed ai detect tool reported experimental academic process suggesting need increased training tools genuine submissions received mean score ai generated content scored indicating comparableperformance gpt-4 real life situations recommendations include strategies resistant use ai tools inclusive assessment possible providing comprehensive students research contributes relationship ai generated content academic assessment investigation preserve academic integrity"}
{"id": "nan", "abstract": "  Contacting customer service via chat is a common practice. Because employingcustomer service agents is expensive, many companies are turning to NLP thatassists human agents by auto-generating responses that can be used directly orwith modifications. Large Language Models (LLMs) are a natural fit for this usecase; however, their efficacy must be balanced with the cost of training andserving them. This paper assesses the practical cost and impact of LLMs for theenterprise as a function of the usefulness of the responses that they generate.We present a cost framework for evaluating an NLP model's utility for this usecase and apply it to a single brand as a case study in the context of anexisting agent assistance product. We compare three strategies for specializingan LLM - prompt engineering, fine-tuning, and knowledge distillation - usingfeedback from the brand's customer service agents. We find that the usabilityof a model's responses can make up for a large difference in inference cost forour case study brand, and we extrapolate our findings to the broader enterprisespace.", "title": "the economic tradeoffs of large language models a case study", "url": "http://arxiv.org/pdf/2306.07402v1.pdf", "tokenized_text": "contacting customer service chat common practice service agents expensive companies nlp human agents auto generating responses directly modifications large_language large language llms natural fit efficacy balanced cost training paper assesses practical cost impact llms function usefulness responses generate present cost framework evaluating nlp utility apply single case study context agent assistance product compare strategies llm prompt_engineering engineering fine tuning knowledge_distillation knowledge distillation customer service agents find responses large difference inference cost case study extrapolate findings broader"}
{"id": "nan", "abstract": "  We investigate the feasibility of employing large language models (LLMs) forconducting the security audit of smart contracts, a traditionallytime-consuming and costly process. Our research focuses on the optimization ofprompt engineering for enhanced security analysis, and we evaluate theperformance and accuracy of LLMs using a benchmark dataset comprising 52Decentralized Finance (DeFi) smart contracts that have previously beencompromised.  Our findings reveal that, when applied to vulnerable contracts, both GPT-4and Claude models correctly identify the vulnerability type in 40% of thecases. However, these models also demonstrate a high false positive rate,necessitating continued involvement from manual auditors. The LLMs testedoutperform a random model by 20% in terms of F1-score.  To ensure the integrity of our study, we conduct mutation testing on fivenewly developed and ostensibly secure smart contracts, into which we manuallyinsert two and 15 vulnerabilities each. This testing yielded a remarkablebest-case 78.7% true positive rate for the GPT-4-32k model. We tested both,asking the models to perform a binary classification on whether a contract isvulnerable, and a non-binary prompt. We also examined the influence of modeltemperature variations and context length on the LLM's performance.  Despite the potential for many further enhancements, this work lays thegroundwork for a more efficient and economical approach to smart contractsecurity audits.", "title": "do you still need a manual smart contract audit", "url": "http://arxiv.org/pdf/2306.12338v2.pdf", "tokenized_text": "investigate feasibility employing large_language large language llms security audit smart contracts consuming costly process research focuses optimization ofprompt engineering enhanced security analysis evaluate theperformance accuracy llms benchmark dataset comprising finance smart contracts previously findings reveal applied vulnerable contracts gpt-4and claude correctly identify vulnerability type 40 demonstrate high false positive rate necessitating continued involvement manual llms random 20 terms f1 score ensure integrity study conduct mutation testing developed secure smart contracts 15 vulnerabilities testing yielded case 78.7 true positive rate gpt-4 tested asking perform binary classification contract non binary examined influence variations context length llm performance despite potential enhancements work lays efficient economical approach smart"}
{"id": "nan", "abstract": "  Research suggests that providing specific and timely feedback to human tutorsenhances their performance. However, it presents challenges due to thetime-consuming nature of assessing tutor performance by human evaluators. Largelanguage models, such as the AI-chatbot ChatGPT, hold potential for offeringconstructive feedback to tutors in practical settings. Nevertheless, theaccuracy of AI-generated feedback remains uncertain, with scant researchinvestigating the ability of models like ChatGPT to deliver effective feedback.In this work-in-progress, we evaluate 30 dialogues generated by GPT-4 in atutor-student setting. We use two different prompting approaches, the zero-shotchain of thought and the few-shot chain of thought, to identify specificcomponents of effective praise based on five criteria. These approaches arethen compared to the results of human graders for accuracy. Our goal is toassess the extent to which GPT-4 can accurately identify each praise criterion.We found that both zero-shot and few-shot chain of thought approaches yieldcomparable results. GPT-4 performs moderately well in identifying instanceswhen the tutor offers specific and immediate praise. However, GPT-4underperforms in identifying the tutor's ability to deliver sincere praise,particularly in the zero-shot prompting scenario where examples of sinceretutor praise statements were not provided. Future work will focus on enhancingprompt engineering, developing a more general tutoring rubric, and evaluatingour method using real-life tutoring dialogues.", "title": "comparative analysis of gpt4 and human graders in evaluating praise given to students in synthetic dialogues", "url": "http://arxiv.org/pdf/2307.02018v1.pdf", "tokenized_text": "research suggests providing specific timely feedback human performance presents challenges consuming nature assessing tutor performance human evaluators largelanguage_models largelanguage ai chatbot chatgpt hold potential feedback tutors practical settings theaccuracy ai generated feedback remains uncertain scant ability like_chatgpt like chatgpt deliver effective feedback work progress evaluate 30 dialogues generated gpt-4 student setting use different approaches zero shotchain thought shot chain_of_thought chain thought identify effective praise based criteria approaches compared results human graders accuracy goal toassess extent gpt-4 accurately identify praise criterion found zero shot shot chain_of_thought chain thought approaches results gpt-4 performs moderately identifying tutor offers specific immediate praise identifying tutor ability deliver sincere praise particularly zero shot_prompting shot scenario examples praise statements provided future work focus engineering developing general tutoring rubric method real life tutoring dialogues"}
{"id": "nan", "abstract": "  Pre-trained large language models (LLMs) have recently emerged as abreakthrough technology in natural language processing and artificialintelligence, with the ability to handle large-scale datasets and exhibitremarkable performance across a wide range of tasks. Meanwhile, softwaretesting is a crucial undertaking that serves as a cornerstone for ensuring thequality and reliability of software products. As the scope and complexity ofsoftware systems continue to grow, the need for more effective software testingtechniques becomes increasingly urgent, and making it an area ripe forinnovative approaches such as the use of LLMs. This paper provides acomprehensive review of the utilization of LLMs in software testing. Itanalyzes 52 relevant studies that have used LLMs for software testing, fromboth the software testing and LLMs perspectives. The paper presents a detaileddiscussion of the software testing tasks for which LLMs are commonly used,among which test case preparation and program repair are the mostrepresentative ones. It also analyzes the commonly used LLMs, the types ofprompt engineering that are employed, as well as the accompanied techniqueswith these LLMs. It also summarizes the key challenges and potentialopportunities in this direction. This work can serve as a roadmap for futureresearch in this area, highlighting potential avenues for exploration, andidentifying gaps in our current understanding of the use of LLMs in softwaretesting.", "title": "software testing with large language model survey, landscape, and vision", "url": "http://arxiv.org/pdf/2307.07221v1.pdf", "tokenized_text": "pre trained large_language large language llms recently emerged technology natural_language natural language processing artificialintelligence ability handle large scale datasets exhibitremarkable performance wide_range wide range tasks crucial serves cornerstone ensuring thequality reliability software products scope complexity ofsoftware systems continue grow need effective software increasingly urgent making area ripe approaches use llms paper provides acomprehensive review utilization llms software testing 52 relevant studies llms software testing software testing llms perspectives paper_presents paper presents software testing tasks llms commonly test case preparation program repair ones analyzes commonly llms types ofprompt engineering employed accompanied llms summarizes key challenges direction work serve futureresearch area highlighting potential avenues exploration gaps current understanding use llms"}
{"id": "nan", "abstract": "  Financial analysis is an important tool for evaluating company performance.Practitioners work to answer financial questions to make profitable investmentdecisions, and use advanced quantitative analyses to do so. As a result,Financial Question Answering (QA) is a question answering task that requiresdeep reasoning about numbers. Furthermore, it is unknown how well pre-trainedlanguage models can reason in the financial domain. The currentstate-of-the-art requires a retriever to collect relevant facts about thefinancial question from the text and a generator to produce a valid financialprogram and a final answer. However, recently large language models like GPT-3have achieved state-of-the-art performance on wide variety of tasks with just afew shot examples. We run several experiments with GPT-3 and find that aseparate retrieval model and logic engine continue to be essential componentsto achieving SOTA performance in this task, particularly due to the precisenature of financial questions and the complex information stored in financialdocuments. With this understanding, our refined prompt-engineering approach onGPT-3 achieves near SOTA accuracy without any fine-tuning.", "title": "gpt3 models are fewshot financial reasoners", "url": "http://arxiv.org/pdf/2307.13617v2.pdf", "tokenized_text": "financial analysis important tool evaluating company performance practitioners work answer financial questions use advanced quantitative analyses result financial question_answering question answering qa question_answering question answering task reasoning numbers furthermore unknown pre trainedlanguage reason financial domain currentstate art requires retriever collect relevant facts question text generator produce valid final answer recently large_language large language like achieved state art performance wide variety tasks afew shot examples run experiments gpt-3 find aseparate retrieval logic engine continue essential achieving sota performance task particularly financial questions complex information stored understanding refined engineering approach achieves near sota accuracy fine tuning"}
{"id": "nan", "abstract": "  Background: Veterinary clinical narratives remain a largely untapped resourcefor addressing complex diseases. Here we compare the ability of a largelanguage model (ChatGPT) and a previously developed regular expression (RegexT)to identify overweight body condition scores (BCS) in veterinary narratives.Methods: BCS values were extracted from 4,415 anonymised clinical narrativesusing either RegexT or by appending the narrative to a prompt sent to ChatGPTcoercing the model to return the BCS information. Data were manually reviewedfor comparison. Results: The precision of RegexT was higher (100%, 95% CI94.81-100%) than the ChatGPT (89.3%; 95% CI82.75-93.64%). However, the recallof ChatGPT (100%. 95% CI 96.18-100%) was considerably higher than that ofRegexT (72.6%, 95% CI 63.92-79.94%). Limitations: Subtle prompt engineering isneeded to improve ChatGPT output. Conclusions: Large language models creatediverse opportunities and, whilst complex, present an intuitive interface toinformation but require careful implementation to avoid unpredictable errors.", "title": "evaluating chatgpt textmining of clinical records for obesity monitoring", "url": "http://arxiv.org/pdf/2308.01666v1.pdf", "tokenized_text": "background clinical narratives remain largely untapped addressing complex diseases compare ability largelanguage chatgpt previously developed regular expression identify body condition scores narratives methods values extracted clinical appending narrative sent return information data manually comparison results precision higher 100 100 chatgpt chatgpt 100 ci 100 considerably higher ci limitations subtle prompt_engineering engineering improve chatgpt output conclusions large_language large language opportunities whilst complex present intuitive interface require careful implementation avoid errors"}
{"id": "nan", "abstract": "  Automated log analysis is crucial in modern software-intensive systems forensuring reliability and resilience throughout software maintenance andengineering life cycles. Existing methods perform tasks such as log parsing andlog anomaly detection by providing a single prediction value withoutinterpretation. However, given the increasing volume of system events, thelimited interpretability of analysis results hinders analysts' trust and theirability to take appropriate actions. Moreover, these methods requiresubstantial in-domain training data, and their performance declines sharply (byup to 62.5%) in online scenarios involving unseen logs from new domains, acommon occurrence due to rapid software updates. In this paper, we proposeLogPrompt, a novel zero-shot and interpretable log analysis approach. LogPromptemploys large language models (LLMs) to perform zero-shot log analysis tasksvia a suite of advanced prompt strategies tailored for log tasks, whichenhances LLMs' performance by up to 107.5% compared with simple prompts.Experiments on nine publicly available evaluation datasets across two tasksdemonstrate that LogPrompt, despite using no training data, outperformsexisting approaches trained on thousands of logs by up to around 50%. We alsoconduct a human evaluation of LogPrompt's interpretability, with sixpractitioners possessing over 10 years of experience, who highly rated thegenerated content in terms of usefulness and readability (averagely 4.42/5).LogPrompt also exhibits remarkable compatibility with open-source andsmaller-scale LLMs, making it flexible for practical deployment.", "title": "logprompt prompt engineering towards zeroshot and interpretable log analysis", "url": "http://arxiv.org/pdf/2308.07610v1.pdf", "tokenized_text": "automated log analysis crucial modern software intensive systems reliability resilience software maintenance life existing_methods existing methods perform tasks log parsing anomaly detection providing single prediction value given increasing volume system events thelimited interpretability analysis results hinders analysts trust theirability appropriate actions methods domain training_data training data performance declines sharply 62.5 online scenarios involving unseen logs new domains occurrence rapid software updates paper novel zero shot interpretable log analysis approach large_language large language llms perform zero shot log analysis suite advanced strategies tailored log tasks llms performance compared simple experiments publicly_available publicly available evaluation datasets despite training_data training data outperformsexisting approaches trained thousands logs 50 alsoconduct human evaluation interpretability 10 years experience highly rated thegenerated content terms usefulness readability exhibits remarkable compatibility open source scale llms making flexible practical deployment"}
{"id": "nan", "abstract": "  Contrastive Language-Image Pre-training (CLIP) provides a foundation model byintegrating natural language into visual concepts, enabling zero-shotrecognition on downstream tasks. It is usually expected that satisfactoryoverall accuracy can be achieved across numerous domains through well-designedtextual prompts. However, we found that their performance in the worstcategories is significantly inferior to the overall performance. For example,on ImageNet, there are a total of 10 categories with class-wise accuracy as lowas 0\\%, even though the overall performance has achieved 64.1\\%. Thisphenomenon reveals the potential risks associated with using CLIP models,particularly in risk-sensitive applications where specific categories holdsignificant importance. To address this issue, we investigate the alignmentbetween the two modalities in the CLIP model and propose the Class-wiseMatching Margin (\\cmm) to measure the inference confusion. \\cmm\\ caneffectively identify the worst-performing categories and estimate the potentialperformance of the candidate prompts. We further query large language models toenrich descriptions of worst-performing categories and build a weightedensemble to highlight the efficient prompts. Experimental results clearlyverify the effectiveness of our proposal, where the accuracy on the worst-10categories on ImageNet is boosted to 5.2\\%, without manual prompt engineering,laborious optimization, or access to labeled validation data.", "title": "investigating the limitation of clip models the worstperforming categories", "url": "http://arxiv.org/pdf/2310.03324v1.pdf", "tokenized_text": "contrastive_language-image_pre-training contrastive language-image pre-training clip provides foundation natural_language natural language visual concepts enabling zero downstream_tasks downstream tasks usually expected accuracy achieved numerous domains found performance significantly inferior overall performance example imagenet total 10 categories class wise accuracy overall performance achieved reveals potential risks associated clip particularly risk sensitive applications specific categories importance address issue investigate modalities clip propose class margin measure inference confusion caneffectively identify worst performing categories estimate candidate query large_language large language toenrich descriptions worst performing categories build highlight efficient experimental_results experimental results effectiveness proposal accuracy imagenet boosted manual prompt_engineering engineering laborious optimization access labeled validation data"}
{"id": "nan", "abstract": "  The advent of the Web has brought about a paradigm shift in traditionaleconomics, particularly in the digital economy era, enabling the preciserecording and analysis of individual economic behavior. This has led to agrowing emphasis on data-driven modeling in macroeconomics. In macroeconomicresearch, Agent-based modeling (ABM) emerged as an alternative, evolvingthrough rule-based agents, machine learning-enhanced decision-making, and, morerecently, advanced AI agents. However, the existing works are suffering fromthree main challenges when endowing agents with human-like decision-making,including agent heterogeneity, the influence of macroeconomic trends, andmultifaceted economic factors. Large language models (LLMs) have recentlygained prominence in offering autonomous human-like characteristics. Therefore,leveraging LLMs in macroeconomic simulation presents an opportunity to overcometraditional limitations. In this work, we take an early step in introducing anovel approach that leverages LLMs in macroeconomic simulation. We designprompt-engineering-driven LLM agents to exhibit human-like decision-making andadaptability in the economic environment, with the abilities of perception,reflection, and decision-making to address the abovementioned challenges.Simulation experiments on macroeconomic activities show that LLM-empoweredagents can make realistic work and consumption decisions and emerge morereasonable macroeconomic phenomena than existing rule-based or AI agents. Ourwork demonstrates the promising potential to simulate macroeconomics based onLLM and its human-like characteristics.", "title": "large language modelempowered agents for simulating macroeconomic activities", "url": "http://arxiv.org/pdf/2310.10436v1.pdf", "tokenized_text": "advent web brought paradigm shift particularly digital era enabling analysis individual economic behavior led emphasis data driven modeling agent based modeling abm emerged alternative rule based agents machine_learning machine learning enhanced decision making advanced ai agents existing works suffering main challenges endowing agents human like decision making including agent heterogeneity influence trends economic factors large_language large language llms prominence offering autonomous human like characteristics leveraging llms simulation presents opportunity limitations work early step introducing anovel approach leverages llms simulation engineering driven llm agents exhibit human like decision making economic environment abilities perception reflection decision making address challenges simulation experiments activities llm realistic work consumption decisions emerge phenomena existing rule based ai agents ourwork demonstrates promising potential simulate based human like characteristics"}
{"id": "nan", "abstract": "  Multiobjective evolutionary algorithms (MOEAs) are major methods for solvingmultiobjective optimization problems (MOPs). Many MOEAs have been proposed inthe past decades, of which the search operators need a carefully handcrafteddesign with domain knowledge. Recently, some attempts have been made to replacethe manually designed operators in MOEAs with learning-based operators (e.g.,neural network models). However, much effort is still required for designingand training such models, and the learned operators might not generalize wellon new problems. To tackle the above challenges, this work investigates a novelapproach that leverages the powerful large language model (LLM) to design MOEAoperators. With proper prompt engineering, we successfully let a general LLMserve as a black-box search operator for decomposition-based MOEA (MOEA/D) in azero-shot manner. In addition, by learning from the LLM behavior, we furtherdesign an explicit white-box operator with randomness and propose a new versionof decomposition-based MOEA, termed MOEA/D-LO. Experimental studies ondifferent test benchmarks show that our proposed method can achieve competitiveperformance with widely used MOEAs. It is also promising to see the operatoronly learned from a few instances can have robust generalization performance onunseen problems with quite different patterns and settings. The results revealthe potential benefits of using pre-trained LLMs in the design of MOEAs.", "title": "large language model for multiobjective evolutionary optimization", "url": "http://arxiv.org/pdf/2310.12541v2.pdf", "tokenized_text": "evolutionary algorithms major methods optimization problems mops proposed inthe past decades search operators need carefully domain knowledge recently attempts manually designed operators learning based operators e.g. network effort required training learned operators generalize new problems tackle challenges work investigates novelapproach leverages powerful large_language large language llm design proper prompt_engineering engineering successfully let general black box search operator decomposition based shot manner addition learning llm behavior explicit white box operator randomness propose_a_new propose new decomposition based termed experimental studies test benchmarks proposed_method proposed method achieve competitiveperformance widely promising learned instances robust generalization performance onunseen problems different patterns settings results potential benefits pre trained llms design"}
{"id": "nan", "abstract": "  Blockchain technology has revolutionized the financial landscape, withcryptocurrencies gaining widespread adoption for their decentralized andtransparent nature. As the sentiment expressed on social media platforms cansignificantly influence cryptocurrency discussions and market movements,sentiment analysis has emerged as a crucial tool for understanding publicopinion and predicting market trends. Motivated by the aim to enhance sentimentanalysis accuracy in the cryptocurrency domain, this paper investigatesfine-tuning techniques on large language models. This paper also investigatesthe efficacy of supervised fine-tuning and instruction-based fine-tuning onlarge language models for unseen tasks. Experimental results demonstrate asignificant average zero-shot performance gain of 40% after fine-tuning,highlighting the potential of this technique in optimizing pre-trained languagemodel efficiency. Additionally, the impact of instruction tuning on models ofvarying scales is examined, revealing that larger models benefit frominstruction tuning, achieving the highest average accuracy score of 75.16%. Incontrast, smaller-scale models may experience reduced generalization due to thecomplete utilization of model capacity. To gain deeper insight about howinstruction works with these language models, this paper presents anexperimental investigation into the response of an instruction-based modelunder different instruction tuning setups. The investigation demonstrates thatthe model achieves an average accuracy score of 72.38% for short and simpleinstructions. This performance significantly outperforms its accuracy underlong and complex instructions by over 12%, thereby effectively highlighting theprofound significance of instruction characteristics in maximizing modelperformance.", "title": "enhancing zeroshot crypto sentiment with finetuned language model and prompt engineering", "url": "http://arxiv.org/pdf/2310.13226v1.pdf", "tokenized_text": "blockchain technology revolutionized financial landscape gaining widespread adoption decentralized nature sentiment expressed social_media social media platforms cansignificantly influence discussions market movements sentiment_analysis sentiment analysis emerged crucial tool understanding publicopinion predicting market trends motivated aim enhance sentimentanalysis accuracy domain paper tuning techniques large_language large language paper investigatesthe efficacy supervised fine tuning instruction based fine tuning onlarge language_models language unseen tasks experimental_results experimental results demonstrate asignificant average zero shot performance gain 40 fine tuning highlighting potential technique optimizing pre trained languagemodel efficiency additionally impact instruction_tuning instruction tuning ofvarying scales examined revealing larger benefit frominstruction tuning achieving highest average accuracy score incontrast smaller scale experience reduced generalization utilization capacity gain deeper insight works language_models language paper_presents paper presents investigation response instruction based different instruction_tuning instruction tuning setups investigation demonstrates thatthe achieves average accuracy score short performance significantly_outperforms significantly outperforms accuracy complex instructions 12 effectively highlighting significance instruction characteristics maximizing modelperformance"}
{"id": "nan", "abstract": "  To address prevalent issues in medical imaging, such as data acquisitionchallenges and label availability, transfer learning from natural to medicalimage domains serves as a viable strategy to produce reliable segmentationresults. However, several existing barriers between domains need to be brokendown, including addressing contrast discrepancies, managing anatomicalvariability, and adapting 2D pretrained models for 3D segmentation tasks. Inthis paper, we propose ProMISe,a prompt-driven 3D medical image segmentationmodel using only a single point prompt to leverage knowledge from a pretrained2D image foundation model. In particular, we use the pretrained visiontransformer from the Segment Anything Model (SAM) and integrate lightweightadapters to extract depth-related (3D) spatial context without updating thepretrained weights. For robust results, a hybrid network with complementaryencoders is designed, and a boundary-aware loss is proposed to achieve preciseboundaries. We evaluate our model on two public datasets for colon and pancreastumor segmentations, respectively. Compared to the state-of-the-artsegmentation methods with and without prompt engineering, our proposed methodachieves superior performance. The code is publicly available athttps://github.com/MedICL-VU/ProMISe.", "title": "promisepromptdriven 3d medical image segmentation using pretrained image foundation models", "url": "http://arxiv.org/pdf/2310.19721v3.pdf", "tokenized_text": "address prevalent issues medical imaging data label availability transfer learning natural medicalimage domains serves viable strategy produce reliable existing barriers domains need including addressing contrast discrepancies managing adapting 2d pretrained 3d segmentation tasks inthis_paper inthis paper propose promise driven 3d medical image single point leverage knowledge image foundation particular use pretrained segment sam integrate extract depth related 3d spatial context updating weights robust results hybrid network designed boundary aware loss proposed achieve evaluate public datasets colon segmentations respectively compared state methods prompt_engineering engineering proposed superior_performance superior performance code publicly_available publicly available"}
{"id": "nan", "abstract": "  Although large language models (LLMs) have advanced the state-of-the-art inNLP significantly, deploying them for downstream applications is stillchallenging due to cost, responsiveness, control, or concerns around privacyand security. As such, trainable models are still the preferred option in somecases. However, these models still require human-labeled data for optimalperformance, which is expensive and time-consuming to obtain. In order toaddress this issue, several techniques to reduce human effort involve labelingor generating data using LLMs. Although these methods are effective for certainapplications, in practice they encounter difficulties in real-world scenarios.Labeling data requires careful data selection, while generating datanecessitates task-specific prompt engineering. In this paper, we propose aunified data creation pipeline that requires only a single formatting example,and which is applicable to a broad range of tasks, including traditionallyproblematic ones with semantically devoid label spaces. In our experiments wedemonstrate that instruction-following LLMs are highly cost-effective datacreators, and that models trained with these data exhibit performance betterthan those trained with human-labeled data (by up to 17.5%) onout-of-distribution evaluation, while maintaining comparable performance onin-distribution tasks. These results have important implications for therobustness of NLP systems deployed in the real-world.", "title": "making large language models better data creators", "url": "http://arxiv.org/pdf/2310.20111v1.pdf", "tokenized_text": "large_language large language llms advanced state art innlp significantly deploying downstream applications cost control concerns security trainable preferred option require human labeled_data labeled data expensive time consuming obtain order toaddress issue techniques reduce human effort involve generating data llms methods effective practice encounter difficulties real world_scenarios world scenarios labeling data requires careful data selection generating task specific prompt_engineering engineering paper propose aunified data creation pipeline requires single formatting example applicable broad range tasks including ones semantically devoid label spaces experiments wedemonstrate instruction following llms highly cost effective trained data exhibit performance betterthan trained human labeled_data labeled data distribution evaluation maintaining comparable performance onin distribution tasks results important implications therobustness nlp systems deployed real world"}
{"id": "nan", "abstract": "  People with blindness and low vision (pBLV) encounter substantial challengeswhen it comes to comprehensive scene recognition and precise objectidentification in unfamiliar environments. Additionally, due to the visionloss, pBLV have difficulty in accessing and identifying potential trippinghazards on their own. In this paper, we present a pioneering approach thatleverages a large vision-language model to enhance visual perception for pBLV,offering detailed and comprehensive descriptions of the surroundingenvironments and providing warnings about the potential risks. Our methodbegins by leveraging a large image tagging model (i.e., Recognize Anything(RAM)) to identify all common objects present in the captured images. Therecognition results and user query are then integrated into a prompt, tailoredspecifically for pBLV using prompt engineering. By combining the prompt andinput image, a large vision-language model (i.e., InstructBLIP) generatesdetailed and comprehensive descriptions of the environment and identifiespotential risks in the environment by analyzing the environmental objects andscenes, relevant to the prompt. We evaluate our approach through experimentsconducted on both indoor and outdoor datasets. Our results demonstrate that ourmethod is able to recognize objects accurately and provide insightfuldescriptions and analysis of the environment for pBLV.", "title": "vispercep a visionlanguage approach to enhance visual perception for people with blindness and low vision", "url": "http://arxiv.org/pdf/2310.20225v1.pdf", "tokenized_text": "people low vision encounter substantial challengeswhen comes comprehensive scene recognition precise unfamiliar environments additionally difficulty accessing identifying potential paper present pioneering approach thatleverages large vision language_model language enhance visual perception offering detailed comprehensive descriptions providing potential risks leveraging large image tagging i.e. recognize identify common objects present captured images results user query integrated prompt_engineering engineering combining image large vision language_model language i.e. comprehensive descriptions environment risks environment analyzing environmental objects relevant evaluate approach datasets results_demonstrate results demonstrate ourmethod able recognize objects accurately provide analysis environment"}
{"id": "nan", "abstract": "  Training and evaluating language models increasingly requires theconstruction of meta-datasets --diverse collections of curated data with clearprovenance. Natural language prompting has recently lead to improved zero-shotgeneralization by transforming existing, supervised datasets into a diversityof novel pretraining tasks, highlighting the benefits of meta-dataset curation.While successful in general-domain text, translating these data-centricapproaches to biomedical language modeling remains challenging, as labeledbiomedical datasets are significantly underrepresented in popular data hubs. Toaddress this challenge, we introduce BigBIO a community library of 126+biomedical NLP datasets, currently covering 12 task categories and 10+languages. BigBIO facilitates reproducible meta-dataset curation viaprogrammatic access to datasets and their metadata, and is compatible withcurrent platforms for prompt engineering and end-to-end few/zero shot languagemodel evaluation. We discuss our process for task schema harmonization, dataauditing, contribution guidelines, and outline two illustrative use cases:zero-shot evaluation of biomedical prompts and large-scale, multi-tasklearning. BigBIO is an ongoing community effort and is available athttps://github.com/bigscience-workshop/biomedical", "title": "bigbio a framework for datacentric biomedical natural language processing", "url": "http://arxiv.org/pdf/2206.15076v1.pdf", "tokenized_text": "training evaluating language_models language increasingly requires theconstruction meta datasets collections curated data natural_language natural language recently lead improved zero transforming existing supervised datasets novel pretraining tasks highlighting benefits meta dataset curation successful general domain text translating data biomedical language modeling remains challenging datasets significantly underrepresented popular data toaddress challenge introduce community library nlp datasets currently covering 12 task categories facilitates reproducible meta dataset curation access datasets metadata compatible platforms prompt_engineering engineering end end zero_shot zero shot languagemodel evaluation discuss process task schema contribution guidelines outline illustrative use cases zero shot evaluation biomedical large scale multi tasklearning ongoing community effort available"}
{"id": "nan", "abstract": "  This paper proposes a framework for quantitatively evaluating interactiveLLMs such as ChatGPT using publicly available data sets. We carry out anextensive technical evaluation of ChatGPT using 23 data sets covering 8different common NLP application tasks. We evaluate the multitask, multilingualand multi-modal aspects of ChatGPT based on these data sets and a newlydesigned multimodal dataset. We find that ChatGPT outperforms LLMs withzero-shot learning on most tasks and even outperforms fine-tuned models on sometasks. We find that it is better at understanding non-Latin script languagesthan generating them. It is able to generate multimodal content from textualprompts, via an intermediate code generation step. Moreover, we find thatChatGPT is 63.41% accurate on average in 10 different reasoning categoriesunder logical reasoning, non-textual reasoning, and commonsense reasoning,hence making it an unreliable reasoner. It is, for example, better at deductivethan inductive reasoning. ChatGPT suffers from hallucination problems likeother LLMs and it generates more extrinsic hallucinations from its parametricmemory as it does not have access to an external knowledge base. Finally, theinteractive feature of ChatGPT enables human collaboration with the underlyingLLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++on machine translation, in a multi-turn \"prompt engineering\" fashion. We alsorelease codebase for evaluation set extraction.", "title": "a multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity", "url": "http://arxiv.org/pdf/2302.04023v3.pdf", "tokenized_text": "paper_proposes paper proposes framework quantitatively evaluating chatgpt publicly_available publicly available data sets carry anextensive technical evaluation chatgpt 23 data sets covering common nlp application tasks evaluate multitask multi modal aspects chatgpt based data sets multimodal dataset find chatgpt outperforms llms shot_learning shot learning tasks outperforms fine tuned find better understanding non latin script generating able generate multimodal content intermediate code_generation code generation step find accurate average 10 different reasoning logical reasoning non textual reasoning commonsense reasoning making unreliable reasoner example better inductive reasoning chatgpt suffers hallucination problems llms generates hallucinations access external_knowledge external knowledge base finally feature chatgpt enables human collaboration improve performance rouge-1 summarization machine_translation machine translation multi turn prompt_engineering engineering fashion evaluation set extraction"}
{"id": "nan", "abstract": "  Advances in ML have motivated the design of video analytics systems thatallow for structured queries over video datasets. However, existing systemslimit query expressivity, require users to specify an ML model per predicate,rely on complex optimizations that trade off accuracy for performance, andreturn large amounts of redundant and low-quality results. This paper focuseson the recently developed Vision-Language Models (VLMs) that allow users toquery images using natural language like \"cars during daytime at trafficintersections.\" Through an in-depth analysis, we show VLMs address threelimitations of current video analytics systems: general expressivity, a singlegeneral purpose model to query many predicates, and are both simple and fast.However, VLMs still return large numbers of redundant and low-quality resultsthat can overwhelm and burden users. In addition, VLMs often require manualprompt engineering to improve result relevance.  We present Zelda: a video analytics system that uses VLMs to return bothrelevant and semantically diverse results for top-K queries on large videodatasets. Zelda prompts the VLM with the user's query in natural language.Zelda then automatically adds discriminator and synonym terms to boostaccuracy, and terms to identify low-quality frames. To improve resultdiversity, Zelda uses semantic-rich VLM embeddings in an algorithm that prunessimilar frames while considering their relevance to the query and the number oftop-K results requested. We evaluate Zelda across five datasets and 19 queriesand quantitatively show it achieves higher mean average precision (up to 1.15x)and improves average pairwise similarity (up to 1.16x) compared to using VLMsout-of-the-box. We also compare Zelda to a state-of-the-art video analyticsengine and show that Zelda retrieves results 7.5x (up to 10.4x) faster for thesame accuracy and frame diversity.", "title": "zelda video analytics using visionlanguage models", "url": "http://arxiv.org/pdf/2305.03785v2.pdf", "tokenized_text": "advances ml motivated design video analytics systems structured queries video datasets existing query expressivity require users specify ml predicate rely complex optimizations trade accuracy performance large amounts redundant low quality results paper focuseson recently developed vision language_models language vlms allow users images natural_language natural language like cars depth analysis vlms address current video analytics systems general expressivity purpose query predicates simple fast vlms return large numbers redundant low quality burden users addition vlms require engineering improve result relevance present video analytics system uses vlms return semantically diverse results queries large vlm user query natural_language natural language automatically adds discriminator synonym terms terms identify low quality frames improve uses semantic rich vlm embeddings algorithm frames considering relevance query number results evaluate datasets 19 quantitatively achieves higher mean average precision improves average pairwise similarity compared box compare state art video retrieves results faster thesame accuracy frame diversity"}
{"id": "nan", "abstract": "  We use prompt engineering to guide ChatGPT in the automation of text miningof metal-organic frameworks (MOFs) synthesis conditions from diverse formatsand styles of the scientific literature. This effectively mitigates ChatGPT'stendency to hallucinate information -- an issue that previously made the use ofLarge Language Models (LLMs) in scientific fields challenging. Our approachinvolves the development of a workflow implementing three different processesfor text mining, programmed by ChatGPT itself. All of them enable parsing,searching, filtering, classification, summarization, and data unification withdifferent tradeoffs between labor, speed, and accuracy. We deploy this systemto extract 26,257 distinct synthesis parameters pertaining to approximately 800MOFs sourced from peer-reviewed research articles. This process incorporatesour ChemPrompt Engineering strategy to instruct ChatGPT in text mining,resulting in impressive precision, recall, and F1 scores of 90-99%.Furthermore, with the dataset built by text mining, we constructed amachine-learning model with over 86% accuracy in predicting MOF experimentalcrystallization outcomes and preliminarily identifying important factors in MOFcrystallization. We also developed a reliable data-grounded MOF chatbot toanswer questions on chemical reactions and synthesis procedures. Given that theprocess of using ChatGPT reliably mines and tabulates diverse MOF synthesisinformation in a unified format, while using only narrative language requiringno coding expertise, we anticipate that our ChatGPT Chemistry Assistant will bevery useful across various other chemistry sub-disciplines.", "title": "chatgpt chemistry assistant for text mining and prediction of mof synthesis", "url": "http://arxiv.org/pdf/2306.11296v2.pdf", "tokenized_text": "use prompt_engineering engineering guide chatgpt automation text organic frameworks synthesis conditions diverse formatsand styles scientific literature effectively mitigates hallucinate information issue previously use oflarge language_models language llms scientific fields challenging development workflow implementing different text mining programmed chatgpt enable parsing searching filtering classification summarization data withdifferent labor speed accuracy deploy extract distinct synthesis parameters pertaining approximately sourced peer reviewed research articles process engineering strategy instruct chatgpt text mining resulting impressive precision recall f1 scores 90 dataset built text mining constructed learning 86 accuracy predicting mof outcomes identifying important factors developed reliable data grounded mof chatbot toanswer questions chemical reactions synthesis procedures given theprocess chatgpt reliably diverse mof unified format narrative language coding expertise anticipate chatgpt chemistry assistant useful chemistry sub disciplines"}
{"id": "nan", "abstract": "  Informal reasoning ability is the ability to reason based on common sense,experience, and intuition.Humans use informal reasoning every day to extractthe most influential elements for their decision-making from a large amount oflife-like information.With the rapid development of language models, therealization of general artificial intelligence has emerged with hope. Given theoutstanding informal reasoning ability of humans, how much informal reasoningability language models have has not been well studied by scholars.In order toexplore the gap between humans and language models in informal reasoningability, this paper constructs a Detective Reasoning Benchmark, which is anassembly of 1,200 questions gathered from accessible online resources, aims atevaluating the model's informal reasoning ability in real-lifecontext.Considering the improvement of the model's informal reasoning abilityrestricted by the lack of benchmark, we further propose a Self-Question PromptFramework that mimics human thinking to enhance the model's informal reasoningability.The goals of self-question are to find key elements, deeply investigatethe connections between these elements, encourage the relationship between eachelement and the problem, and finally, require the model to reasonably answerthe problem.The experimental results show that human performance greatlyoutperforms the SoTA Language Models in Detective Reasoning Benchmark.Besides,Self-Question is proven to be the most effective prompt engineering inimproving GPT-4's informal reasoning ability, but it still does not evensurpass the lowest score made by human participants.Upon acceptance of thepaper, the source code for the benchmark will be made publicly accessible.", "title": "go beyond the obvious probing the gap of informal reasoning ability between humanity and llms by detective reasoning puzzle benchmark", "url": "http://arxiv.org/pdf/2307.05113v2.pdf", "tokenized_text": "informal reasoning ability ability reason based common sense experience intuition humans use informal reasoning day influential elements decision making large like information rapid development language_models language general artificial_intelligence artificial intelligence emerged hope given informal reasoning ability humans informal reasoningability language_models language studied order toexplore gap humans language_models language informal reasoningability paper constructs reasoning benchmark questions gathered accessible online resources aims informal reasoning ability real considering improvement informal reasoning lack benchmark propose self question promptframework human thinking enhance informal reasoningability goals self question find key elements deeply investigatethe connections elements encourage relationship problem finally require reasonably problem experimental_results experimental results human performance sota language_models language reasoning benchmark self question proven effective prompt_engineering engineering inimproving gpt-4 informal reasoning ability lowest score human participants acceptance source_code source code benchmark publicly accessible"}
{"id": "nan", "abstract": "  Text entry is an essential task in our day-to-day digital interactions.Numerous intelligent features have been developed to streamline this process,making text entry more effective, efficient, and fluid. These improvementsinclude sentence prediction and user personalization. However, as deeplearning-based language models become the norm for these advanced features, thenecessity for data collection and model fine-tuning increases. These challengescan be mitigated by harnessing the in-context learning capability of largelanguage models such as GPT-3.5. This unique feature allows the language modelto acquire new skills through prompts, eliminating the need for data collectionand fine-tuning. Consequently, large language models can learn various textprediction techniques. We initially showed that, for a sentence predictiontask, merely prompting GPT-3.5 surpassed a GPT-2 backed system and iscomparable with a fine-tuned GPT-3.5 model, with the latter two methodsrequiring costly data collection, fine-tuning and post-processing. However, thetask of prompting large language models to specialize in specific textprediction tasks can be challenging, particularly for designers withoutexpertise in prompt engineering. To address this, we introduce Promptor, aconversational prompt generation agent designed to engage proactively withdesigners. Promptor can automatically generate complex prompts tailored to meetspecific needs, thus offering a solution to this challenge. We conducted a userstudy involving 24 participants creating prompts for three intelligent textentry tasks, half of the participants used Promptor while the other halfdesigned prompts themselves. The results show that Promptor-designed promptsresult in a 35% increase in similarity and 22% in coherence over those bydesigners.", "title": "promptor a conversational and autonomous prompt generation agent for intelligent text entry techniques", "url": "http://arxiv.org/pdf/2310.08101v2.pdf", "tokenized_text": "text entry essential task day day digital interactions numerous intelligent features developed streamline process making text entry effective efficient fluid sentence prediction user personalization deeplearning based language_models language norm advanced features thenecessity data collection fine tuning increases mitigated harnessing context_learning context learning capability largelanguage_models largelanguage gpt-3.5 unique feature allows language modelto acquire new skills eliminating need data fine tuning consequently large_language large language learn techniques initially showed sentence predictiontask merely gpt-3.5 surpassed gpt-2 backed system iscomparable fine tuned gpt-3.5 costly data collection fine tuning post processing thetask large_language large language specialize specific tasks challenging particularly designers prompt_engineering engineering address introduce generation agent designed engage proactively automatically generate complex tailored needs offering solution challenge conducted involving 24 participants creating intelligent tasks participants results designed 35 increase similarity 22 coherence"}
{"id": "nan", "abstract": "  Large language model (LLM) prompting is a promising new approach for users tocreate and customize their own chatbots. However, current methods for steeringa chatbot's outputs, such as prompt engineering and fine-tuning, do not supportusers in converting their natural feedback on the model's outputs to changes inthe prompt or model. In this work, we explore how to enable users tointeractively refine model outputs through their feedback, by helping themconvert their feedback into a set of principles (i.e. a constitution) thatdictate the model's behavior. From a formative study, we (1) found that usersneeded support converting their feedback into principles for the chatbot and(2) classified the different principle types desired by users. Inspired bythese findings, we developed ConstitutionMaker, an interactive tool forconverting user feedback into principles, to steer LLM-based chatbots. WithConstitutionMaker, users can provide either positive or negative feedback innatural language, select auto-generated feedback, or rewrite the chatbot'sresponse; each mode of feedback automatically generates a principle that isinserted into the chatbot's prompt. In a user study with 14 participants, wecompare ConstitutionMaker to an ablated version, where users write their ownprinciples. With ConstitutionMaker, participants felt that their principlescould better guide the chatbot, that they could more easily convert theirfeedback into principles, and that they could write principles moreefficiently, with less mental demand. ConstitutionMaker helped users identifyways to improve the chatbot, formulate their intuitive responses to the modelinto feedback, and convert this feedback into specific and clear principles.Together, these findings inform future tools that support the interactivecritiquing of LLM outputs.", "title": "constitutionmaker interactively critiquing large language models by converting feedback into principles", "url": "http://arxiv.org/pdf/2310.15428v1.pdf", "tokenized_text": "large_language large language llm promising new approach users tocreate customize chatbots current methods outputs prompt_engineering engineering fine tuning converting natural feedback outputs changes inthe work explore enable users refine outputs feedback helping feedback set principles i.e. behavior formative study found support converting feedback principles chatbot and(2 classified different principle types desired users inspired findings developed interactive tool user feedback principles steer llm based chatbots users provide positive negative feedback innatural language select auto generated feedback rewrite mode feedback automatically generates principle chatbot user study 14 participants wecompare version users write participants felt better guide chatbot easily convert principles write principles mental demand helped users improve chatbot formulate intuitive responses feedback convert feedback specific clear principles findings inform future tools support llm outputs"}
{"id": "nan", "abstract": "  Few-shot learning-the ability to train models with access to limited data-hasbecome increasingly popular in the natural language processing (NLP) domain, aslarge language models such as GPT and T0 have been empirically shown to achievehigh performance in numerous tasks with access to just a handful of labeledexamples. Smaller language models such as BERT and its variants have also beenshown to achieve strong performance with just a handful of labeled exampleswhen combined with few-shot learning algorithms like pattern-exploitingtraining (PET) and SetFit. The focus of this work is to investigate theperformance of alternative few-shot learning approaches with BERT-based models.Specifically, vanilla fine-tuning, PET and SetFit are compared for numerousBERT-based checkpoints over an array of training set sizes. To facilitate thisinvestigation, applications of few-shot learning are considered in softwareengineering. For each task, high-performance techniques and their associatedmodel checkpoints are identified through detailed empirical analysis. Ourresults establish PET as a strong few-shot learning approach, and our analysisshows that with just a few hundred labeled examples it can achieve performancenear that of fine-tuning on full-sized data sets.", "title": "fewshot learning for sentence pair classification and its applications in software engineering", "url": "http://arxiv.org/pdf/2306.08058v1.pdf", "tokenized_text": "shot_learning shot learning ability train access limited data increasingly popular natural_language natural language processing nlp domain aslarge language_models language gpt t0 empirically shown performance numerous tasks access handful labeledexamples smaller language_models language bert variants beenshown achieve strong performance handful labeled combined shot_learning shot learning algorithms like pattern pet setfit focus work investigate theperformance alternative shot_learning shot learning approaches bert based specifically vanilla fine tuning pet setfit compared based checkpoints array training set sizes facilitate thisinvestigation applications shot_learning shot learning considered softwareengineering task high performance techniques checkpoints identified detailed empirical analysis ourresults establish pet strong shot_learning shot learning approach analysisshows labeled examples achieve fine tuning sized data sets"}
{"id": "nan", "abstract": "  Pretrained Language Models (PLMs) have achieved tremendous success in naturallanguage understanding tasks. While different learning schemes -- fine-tuning,zero-shot, and few-shot learning -- have been widely explored and compared forlanguages such as English, there is comparatively little work in Chinese tofairly and comprehensively evaluate and compare these methods and thus hinderscumulative progress. In this paper, we introduce the Chinese Few-shot LearningEvaluation Benchmark (FewCLUE), the first comprehensive few-shot evaluationbenchmark in Chinese. It includes nine tasks, ranging from single-sentence andsentence-pair classification tasks to machine reading comprehension tasks. Wesystematically evaluate five state-of-the-art (SOTA) few-shot learning methods(including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare theirperformance with fine-tuning and zero-shot learning schemes on the newlyconstructed FewCLUE benchmark. Experimental results reveal that: 1) The effectof different few-shot learning methods is sensitive to the pre-trained model towhich the methods are applied; 2) PET and P-tuning achieve the best overallperformance with RoBERTa and ERNIE respectively. Our benchmark is used in thefew-shot learning contest of NLPCC 2021. In addition, we provide auser-friendly toolkit, as well as an online leaderboard to help facilitatefurther progress on Chinese few-shot learning. We provide a baselineperformance on different learning methods, a reference for future research.", "title": "fewclue a chinese fewshot learning evaluation benchmark", "url": "http://arxiv.org/pdf/2107.07498v2.pdf", "tokenized_text": "pretrained_language pretrained language plms achieved tremendous success naturallanguage understanding tasks different learning schemes fine tuning zero shot shot_learning shot learning widely explored compared english comparatively little work chinese comprehensively evaluate compare methods progress paper introduce chinese shot benchmark comprehensive shot chinese includes tasks ranging single sentence pair classification tasks machine reading comprehension tasks wesystematically evaluate state art sota shot_learning shot learning pet lm bff tuning efl compare theirperformance fine tuning zero shot_learning shot learning schemes benchmark experimental_results experimental results reveal different shot_learning shot learning methods sensitive pre trained methods applied pet tuning achieve best overallperformance roberta respectively benchmark thefew shot_learning shot learning 2021 addition provide friendly toolkit online leaderboard help progress chinese shot_learning shot learning provide different learning methods reference future_research future research"}
{"id": "nan", "abstract": "  Prompt-based approaches are strong at few-shot learning. However, Perez etal. (2021) have recently cast doubt on their performance because they haddifficulty getting good results in a \"true\" few-shot setting in which promptsand hyperparameters cannot be tuned on a dev set. In view of this, we conductan extensive study of PET, a method that combines textual instructions withexample-based finetuning. We show that, if correctly configured, PET performsstrongly in a true few-shot setting, i.e., without a dev set. Crucial for thisstrong performance is PET's ability to intelligently handle multiple prompts.We then put our findings to a real-world test by running PET on RAFT, abenchmark of tasks taken directly from realistic NLP applications for which nolabeled dev or test sets are available. PET achieves a new state of the art onRAFT and performs close to non-expert humans for 7 out of 11 tasks. Theseresults demonstrate that prompt-based learners like PET excel at true few-shotlearning and underpin our belief that learning from instructions will play animportant role on the path towards human-like few-shot learning capabilities.", "title": "true fewshot learning with prompts a realworld perspective", "url": "http://arxiv.org/pdf/2111.13440v1.pdf", "tokenized_text": "based approaches strong shot_learning shot learning etal 2021 recently cast doubt performance getting good results true shot_setting shot setting promptsand hyperparameters tuned dev set view conductan extensive study pet method combines textual instructions based finetuning correctly configured pet true shot_setting shot setting i.e. dev set crucial performance pet ability handle multiple findings real world test running pet raft abenchmark tasks taken directly realistic nlp applications dev test sets available pet achieves new state_of_the_art state art performs close non expert humans 11 tasks demonstrate based learners like pet excel true shotlearning belief learning instructions play animportant role path human like shot_learning shot learning capabilities"}
{"id": "nan", "abstract": "  Pre-trained masked language models successfully perform few-shot learning byformulating downstream tasks as text infilling. However, as a strongalternative in full-shot settings, discriminative pre-trained models likeELECTRA do not fit into the paradigm. In this work, we adapt prompt-basedfew-shot learning to ELECTRA and show that it outperforms masked languagemodels in a wide range of tasks. ELECTRA is pre-trained to distinguish if atoken is generated or original. We naturally extend that to prompt-basedfew-shot learning by training to score the originality of the target optionswithout introducing new parameters. Our method can be easily adapted to tasksinvolving multi-token predictions without extra computation overhead. Analysisshows that ELECTRA learns distributions that align better with downstreamtasks.", "title": "prompting electra fewshot learning with discriminative pretrained models", "url": "http://arxiv.org/pdf/2205.15223v3.pdf", "tokenized_text": "pre trained masked language_models language successfully perform shot_learning shot learning downstream_tasks downstream tasks text infilling shot_settings shot settings discriminative pre trained fit paradigm work adapt basedfew shot_learning shot learning electra outperforms masked languagemodels wide_range wide range tasks electra pre trained distinguish atoken generated original naturally extend basedfew shot_learning shot learning training score target introducing new parameters method easily adapted multi token predictions extra computation overhead analysisshows electra learns distributions align better downstreamtasks"}
{"id": "nan", "abstract": "  The ability to learn from limited data, or few-shot learning, is a desirableand often critical requirement for NLP systems. While many existing methods dopoorly at learning from a handful of examples, large pretrained language modelshave recently been shown to be efficient few-shot learners. One approach tofew-shot learning, which does not require finetuning of model parameters, is toaugment the language model's input with priming text which is typicallyconstructed using task specific descriptions and examples. In this work, wefurther explore priming-based few-shot learning, with focus on using examplesas prompts. We show that presenting examples in the right order is key forgeneralization. We introduce PERO (Prompting with Examples in the Right Order),where we formulate few-shot learning as search over the set of permutations ofthe training examples. We show that PERO can learn to generalize efficientlyusing as few as 10 examples, in contrast to existing approaches. While thenewline token is a natural choice for separating the examples in the prompt, weshow that learning a new separator token can potentially provide further gainsin performance. We demonstrate the effectiveness of the proposed method on thetasks of sentiment classification, natural language inference and factretrieval. Finally, we analyze the learned prompts to reveal novel insights,including the idea that two training examples in the right order alone canprovide competitive performance for sentiment classification and naturallanguage inference.", "title": "reordering examples helps during primingbased fewshot learning", "url": "http://arxiv.org/pdf/2106.01751v1.pdf", "tokenized_text": "ability learn limited data shot_learning shot learning critical requirement nlp systems existing_methods existing methods learning handful examples large pretrained_language pretrained language modelshave recently shown efficient shot learners approach tofew shot_learning shot learning require finetuning parameters toaugment language_model language input priming text task specific descriptions examples work wefurther explore priming based shot_learning shot learning focus examplesas presenting examples right order key forgeneralization introduce examples right formulate shot_learning shot learning search set permutations ofthe training_examples training examples learn generalize 10 examples contrast existing approaches token natural choice examples weshow learning new token potentially provide performance demonstrate_the_effectiveness demonstrate effectiveness proposed_method proposed method sentiment classification natural_language natural language inference finally analyze learned reveal novel insights including idea training_examples training examples right order competitive_performance competitive performance sentiment classification naturallanguage inference"}
{"id": "nan", "abstract": "  Recent studies have revealed the intriguing few-shot learning ability ofpretrained language models (PLMs): They can quickly adapt to a new task whenfine-tuned on a small amount of labeled data formulated as prompts, withoutrequiring abundant task-specific annotations. Despite their promisingperformance, most existing few-shot approaches that only learn from the smalltraining set still underperform fully supervised training by nontrivialmargins. In this work, we study few-shot learning with PLMs from a differentperspective: We first tune an autoregressive PLM on the few-shot samples andthen use it as a generator to synthesize a large amount of novel trainingsamples which augment the original training set. To encourage the generator toproduce label-discriminative samples, we train it via weighted maximumlikelihood where the weight of each token is automatically adjusted based on adiscriminative meta-learning objective. A classification PLM can then befine-tuned on both the few-shot and the synthetic samples with regularizationfor better generalization and stability. Our approach FewGen achieves anoverall better result across seven classification tasks of the GLUE benchmarkthan existing few-shot learning methods, improving no-augmentation methods by5+ average points, and outperforming augmentation methods by 3+ average points.", "title": "tuning language models as training data generators for augmentationenhanced fewshot learning", "url": "http://arxiv.org/pdf/2211.03044v2.pdf", "tokenized_text": "recent studies revealed intriguing shot_learning shot learning ability language_models language plms quickly adapt new task tuned small labeled_data labeled data formulated withoutrequiring abundant task specific annotations despite existing shot approaches learn set underperform fully_supervised fully supervised training work study shot_learning shot learning plms tune autoregressive plm shot samples andthen use generator synthesize large novel augment original training set encourage generator toproduce label discriminative samples train weighted weight token automatically adjusted based meta learning objective classification plm tuned shot synthetic samples better generalization stability approach achieves anoverall better result seven classification tasks glue existing shot_learning shot learning methods improving augmentation methods average points outperforming augmentation methods average points"}
{"id": "nan", "abstract": "  As labeling cost for different modules in task-oriented dialog (ToD) systemsis high, a major challenge in practice is to learn different tasks with theleast amount of labeled data. Recently, prompting methods over pre-trainedlanguage models (PLMs) have shown promising results for few-shot learning inToD. To better utilize the power of PLMs, this paper proposes ComprehensiveInstruction (CINS) that exploits PLMs with extra task-specific instructions. Wedesign a schema (definition, constraint, prompt) of instructions and theircustomized realizations for three important downstream tasks in ToD, i.e.intent classification, dialog state tracking, and natural language generation.A sequence-to-sequence model (T5) is adopted to solve these three tasks in aunified framework. Extensive experiments are conducted on these ToD tasks inrealistic few-shot learning scenarios with small validation data. Empiricalresults demonstrate that the proposed CINS approach consistently improvestechniques that finetune PLMs with raw input or short prompts.", "title": "cins comprehensive instruction for fewshot learning in taskoriented dialog systems", "url": "http://arxiv.org/pdf/2109.04645v4.pdf", "tokenized_text": "labeling cost different modules task oriented dialog tod high major challenge practice learn different tasks theleast labeled_data labeled data recently methods pre trainedlanguage plms shown promising_results promising results shot_learning shot learning better utilize power plms paper_proposes paper proposes exploits plms extra task specific instructions wedesign schema definition constraint instructions important downstream_tasks downstream tasks tod classification dialog state tracking natural_language natural language generation sequence sequence t5 adopted solve tasks aunified framework extensive_experiments extensive experiments conducted tod tasks shot_learning shot learning scenarios small validation data empiricalresults demonstrate proposed approach consistently finetune plms raw input short"}
{"id": "nan", "abstract": "  Dialog models can be greatly strengthened through grounding on variousexternal information, but grounded dialog corpora are usually not naturallyaccessible. In this work, we focus on the few-shot learning for grounded dialoggeneration (GDG). We first propose a simple prompting method for GDG tasks,where different constructs of model input, such as the grounding source and theconversation context, are distinguished through continuous or discrete prompts.On three typical GDG tasks, we empirically demonstrate and analyze in-depth theeffectiveness of our method. We then conduct extensive experiments tothoroughly investigate how our prompting method works with differentpre-trained models. We show that prompted language models perform superiorly toconversational models, and further analyze various factors that influence theeffects of prompting. Overall, our work introduces a prompt-based perspectiveto the few-shot learning for GDG tasks, and provides valuable findings andinsights for future research.", "title": "exploring promptbased fewshot learning for grounded dialog generation", "url": "http://arxiv.org/pdf/2109.06513v2.pdf", "tokenized_text": "dialog greatly grounding information grounded dialog corpora usually work focus shot_learning shot learning grounded propose simple method tasks different constructs input grounding source context continuous discrete typical tasks empirically demonstrate analyze depth theeffectiveness method conduct_extensive conduct extensive experiments investigate method works differentpre trained prompted language_models language perform analyze factors influence theeffects overall work introduces based shot_learning shot learning tasks provides valuable findings future_research future research"}
{"id": "nan", "abstract": "  Few-shot Learning (FSL) is aimed to make predictions based on a limitednumber of samples. Structured data such as knowledge graphs and ontologylibraries has been leveraged to benefit the few-shot setting in various tasks.However, the priors adopted by the existing methods suffer from challengingknowledge missing, knowledge noise, and knowledge heterogeneity, which hinderthe performance for few-shot learning. In this study, we explore knowledgeinjection for FSL with pre-trained language models and proposeontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop theontology transformation based on the external knowledge graph to address theknowledge missing issue, which fulfills and converts structure knowledge totext. We further introduce span-sensitive knowledge injection via a visiblematrix to select informative knowledge to handle the knowledge noise issue. Tobridge the gap between knowledge and text, we propose a collective trainingalgorithm to optimize representations jointly. We evaluate our proposedOntoPrompt in three tasks, including relation extraction, event extraction, andknowledge graph completion, with eight datasets. Experimental resultsdemonstrate that our approach can obtain better few-shot performance thanbaselines.", "title": "ontologyenhanced prompttuning for fewshot learning", "url": "http://arxiv.org/pdf/2201.11332v1.pdf", "tokenized_text": "shot_learning shot learning fsl aimed predictions based samples structured data knowledge graphs leveraged benefit shot_setting shot setting tasks priors adopted existing_methods existing methods suffer missing knowledge noise knowledge heterogeneity performance shot_learning shot learning study explore fsl pre trained_language trained language enhanced tuning specifically develop transformation based external_knowledge external knowledge graph address theknowledge missing issue fulfills converts structure knowledge introduce span sensitive knowledge injection select informative knowledge handle knowledge noise issue tobridge gap knowledge text propose collective trainingalgorithm optimize representations jointly evaluate tasks including relation_extraction relation extraction event extraction graph completion datasets experimental resultsdemonstrate approach obtain better shot performance"}
{"id": "nan", "abstract": "  Recent development of large-scale pre-trained language models (PLM) havesignificantly improved the capability of models in various NLP tasks, in termsof performance after task-specific fine-tuning and zero-shot / few-shotlearning. However, many of such models come with a dauntingly huge size thatfew institutions can afford to pre-train, fine-tune or even deploy, whilemoderate-sized models usually lack strong generalized few-shot learningcapabilities. In this paper, we first elaborate the current obstacles of usingPLM models in terms of the Impossible Triangle: 1) moderate model size, 2)state-of-the-art few-shot learning capability, and 3) state-of-the-artfine-tuning capability. We argue that all existing PLM models lack one or moreproperties from the Impossible Triangle. To remedy these missing properties ofPLMs, various techniques have been proposed, such as knowledge distillation,data augmentation and prompt learning, which inevitably brings additional workto the application of PLMs in real scenarios. We then offer insights intofuture research directions of PLMs to achieve the Impossible Triangle, andbreak down the task into several key phases.", "title": "impossible triangle what's next for pretrained language models", "url": "http://arxiv.org/pdf/2204.06130v2.pdf", "tokenized_text": "recent development large scale pre trained_language trained language plm improved capability nlp_tasks nlp tasks performance task specific fine tuning zero shot shotlearning come huge size thatfew institutions afford pre train fine tune deploy sized usually lack strong generalized shot learningcapabilities paper elaborate current obstacles terms impossible moderate model_size size art shot_learning shot learning capability state tuning capability argue existing plm lack impossible remedy missing properties ofplms techniques proposed knowledge_distillation knowledge distillation data_augmentation data augmentation learning inevitably brings additional application plms real scenarios offer insights research directions plms achieve impossible task key phases"}
{"id": "nan", "abstract": "  Deep generative models have the potential to fundamentally change the way wecreate high-fidelity digital content but are often hard to control. Prompting agenerative model is a promising recent development that in principle enablesend-users to creatively leverage zero-shot and few-shot learning to assign newtasks to an AI ad-hoc, simply by writing them down. However, for the majorityof end-users writing effective prompts is currently largely a trial and errorprocess. To address this, we discuss the key opportunities and challenges forinteractive creative applications that use prompting as a new paradigm forHuman-AI interaction. Based on our analysis, we propose four design goals foruser interfaces that support prompting. We illustrate these with concrete UIdesign sketches, focusing on the use case of creative writing. The researchcommunity in HCI and AI can take these as starting points to develop adequateuser interfaces for models capable of zero- and few-shot learning.", "title": "how to prompt opportunities and challenges of zero and fewshot learning for humanai interaction in creative applications of generative models", "url": "http://arxiv.org/pdf/2209.01390v1.pdf", "tokenized_text": "deep generative potential fundamentally change way high fidelity digital content hard control promising recent development principle users leverage zero shot shot_learning shot learning assign newtasks ai ad hoc simply writing majorityof end users writing effective currently largely trial address discuss key opportunities challenges forinteractive creative applications use new_paradigm new paradigm ai interaction based analysis propose design goals interfaces support illustrate concrete sketches focusing use case creative writing researchcommunity hci ai starting points develop interfaces capable zero- shot_learning shot learning"}
{"id": "nan", "abstract": "  Few-shot learning allows pre-trained language models to adapt to downstreamtasks while using a limited number of training examples. However, practicalapplications are limited when all model parameters must be optimized. In thiswork we apply a new technique for parameter efficient few shot learning whileadopting a strict definition of parameter efficiency. Our training methodcombines 1) intermediate training by reformulating natural language tasks asentailment tasks \\cite{wang_entailment_2021} and 2) differentiable optimizationof template and label tokens \\cite{zhang_differentiable_2021}. We quantify thetradeoff between parameter efficiency and performance in the few-shot regimeand propose a simple model agnostic approach that can be extended to any taskBy achieving competitive performance while only optimizing 3\\% of a model'sparameters and allowing for batched inference, we allow for more efficientpractical deployment of models.", "title": "differentiable entailment for parameter efficient few shot learning", "url": "http://arxiv.org/pdf/2301.13345v1.pdf", "tokenized_text": "shot_learning shot learning allows pre trained_language trained language adapt downstreamtasks limited number training_examples training examples practicalapplications limited parameters optimized thiswork apply new technique parameter_efficient parameter efficient shot_learning shot learning strict definition parameter efficiency training intermediate training reformulating natural_language natural language tasks tasks differentiable template label tokens quantify thetradeoff parameter efficiency performance shot propose simple agnostic approach extended achieving competitive_performance competitive performance optimizing allowing batched inference allow deployment"}
{"id": "nan", "abstract": "  Gathering cyber threat intelligence from open sources is becomingincreasingly important for maintaining and achieving a high level of securityas systems become larger and more complex. However, these open sources areoften subject to information overload. It is therefore useful to apply machinelearning models that condense the amount of information to what is necessary.Yet, previous studies and applications have shown that existing classifiers arenot able to extract specific information about emerging cybersecurity eventsdue to their low generalization ability. Therefore, we propose a system toovercome this problem by training a new classifier for each new incident. Sincethis requires a lot of labelled data using standard training methods, wecombine three different low-data regime techniques - transfer learning, dataaugmentation, and few-shot learning - to train a high-quality classifier fromvery few labelled instances. We evaluated our approach using a novel datasetderived from the Microsoft Exchange Server data breach of 2021 which waslabelled by three experts. Our findings reveal an increase in F1 score of morethan 21 points compared to standard training methods and more than 18 pointscompared to a state-of-the-art method in few-shot learning. Furthermore, theclassifier trained with this method and 32 instances is only less than 5 F1score points worse than a classifier trained with 1800 instances.", "title": "multilevel finetuning, data augmentation, and fewshot learning for specialized cyber threat intelligence", "url": "http://arxiv.org/pdf/2207.11076v1.pdf", "tokenized_text": "cyber threat intelligence open sources important maintaining achieving high_level high level systems larger complex open sources subject information useful apply machinelearning condense information necessary previous studies applications shown existing classifiers able extract specific information emerging cybersecurity low generalization_ability generalization ability propose system problem training new classifier new requires lot labelled data standard training methods different low data regime techniques transfer learning dataaugmentation shot_learning shot learning train high quality classifier labelled instances evaluated approach novel microsoft exchange server data 2021 experts findings reveal increase f1_score f1 score morethan 21 points compared standard training methods 18 state art method shot_learning shot learning furthermore trained method 32 instances points worse classifier trained instances"}
{"id": "nan", "abstract": "  Prompt tuning is a parameter-efficient approach to adapting pre-trainedlanguage models to downstream tasks. Although prompt tuning has been shown tomatch the performance of full model tuning when training data is sufficient, ittends to struggle in few-shot learning settings. In this paper, we presentMulti-task Pre-trained Modular Prompt (MP2) to boost prompt tuning for few-shotlearning. MP2 is a set of combinable prompts pre-trained on 38 Chinese tasks.On downstream tasks, the pre-trained prompts are selectively activated andcombined, leading to strong compositional generalization to unseen tasks. Tobridge the gap between pre-training and fine-tuning, we formulate upstream anddownstream tasks into a unified machine reading comprehension task. Extensiveexperiments under two learning paradigms, i.e., gradient descent and black-boxtuning, show that MP2 significantly outperforms prompt tuning, full modeltuning, and prior prompt pre-training methods in few-shot settings. Inaddition, we demonstrate that MP2 can achieve surprisingly fast and strongadaptation to downstream tasks by merely learning 8 parameters to combine thepre-trained modular prompts.", "title": "multitask pretraining of modular prompt for chinese fewshot learning", "url": "http://arxiv.org/pdf/2210.07565v3.pdf", "tokenized_text": "tuning parameter efficient approach adapting pre trainedlanguage downstream_tasks downstream tasks tuning shown performance tuning training_data training data sufficient struggle shot_learning shot learning settings paper task pre trained modular boost tuning shotlearning set pre trained chinese tasks downstream_tasks downstream tasks pre trained selectively activated leading strong compositional generalization unseen tasks tobridge gap pre training fine tuning formulate upstream tasks unified machine reading comprehension task extensiveexperiments learning paradigms i.e. gradient descent black significantly_outperforms significantly outperforms tuning modeltuning prior pre training methods shot_settings shot settings inaddition demonstrate achieve surprisingly fast downstream_tasks downstream tasks merely learning parameters combine thepre trained modular"}
{"id": "nan", "abstract": "  Learning to converse using only a few examples is a great challenge inconversational AI. The current best conversational models, which are eithergood chit-chatters (e.g., BlenderBot) or goal-oriented systems (e.g., MinTL),are language models (LMs) fine-tuned on large conversational datasets. Trainingthese models is expensive, both in terms of computational resources and time,and it is hard to keep them up to date with new conversational skills. A simpleyet unexplored solution is prompt-based few-shot learning (Brown et al. 2020)which does not require gradient-based fine-tuning but instead uses a fewexamples in the LM context as the only source of learning. In this paper, weexplore prompt-based few-shot learning in dialogue tasks. We benchmark LMs ofdifferent sizes in nine response generation tasks, which include fourknowledge-grounded tasks, a task-oriented generations task, three open-chattasks, and controlled stylistic generation, and five conversational parsingtasks, which include dialogue state tracking, graph path generation, personainformation extraction, document retrieval, and internet query generation. Thecurrent largest released LM (GPT-J-6B) using prompt-based few-shot learning,and thus requiring no training, achieves competitive performance to fullytrained state-of-the-art models. Moreover, we propose a novel prompt-basedfew-shot classifier, that also does not require any fine-tuning, to select themost appropriate prompt given a dialogue history. Finally, by combining thepower of prompt-based few-shot learning and a Skill Selector, we create anend-to-end chatbot named the Few-Shot Bot (FSB), which automatically selectsthe most appropriate conversational skill, queries different knowledge bases orthe internet, and uses the retrieved knowledge to generate a human-likeresponse, all using only few dialogue examples per skill.", "title": "fewshot bot promptbased learning for dialogue systems", "url": "http://arxiv.org/pdf/2110.08118v1.pdf", "tokenized_text": "learning converse examples great challenge inconversational ai current best conversational chit e.g. goal oriented systems e.g. language_models language lms fine tuned large conversational datasets expensive terms computational resources time hard date new conversational skills unexplored solution based shot_learning shot learning brown et_al et al require gradient based fine tuning instead uses fewexamples lm context source learning paper weexplore based shot_learning shot learning dialogue tasks benchmark lms ofdifferent sizes response generation tasks include grounded tasks task oriented generations task open controlled stylistic generation conversational include dialogue state tracking graph path generation extraction document retrieval internet query generation thecurrent largest released lm gpt based shot_learning shot learning requiring training achieves competitive_performance competitive performance state art propose_a_novel propose novel basedfew shot classifier require fine tuning select themost appropriate given dialogue history finally combining thepower based shot_learning shot learning skill selector create anend end chatbot named shot automatically appropriate conversational skill queries different knowledge bases internet uses retrieved knowledge generate human dialogue examples skill"}
{"id": "nan", "abstract": "  We demonstrate that a neural network pre-trained on text and fine-tuned oncode solves mathematics course problems, explains solutions, and generates newquestions at a human level. We automatically synthesize programs using few-shotlearning and OpenAI's Codex transformer and execute them to solve courseproblems at 81% automatic accuracy. We curate a new dataset of questions fromMIT's largest mathematics courses (Single Variable and Multivariable Calculus,Differential Equations, Introduction to Probability and Statistics, LinearAlgebra, and Mathematics for Computer Science) and Columbia University'sComputational Linear Algebra. We solve questions from a MATH dataset (onPrealgebra, Algebra, Counting and Probability, Intermediate Algebra, NumberTheory, and Precalculus), the latest benchmark of advanced mathematics problemsdesigned to assess mathematical reasoning. We randomly sample questions andgenerate solutions with multiple modalities, including numbers, equations, andplots. The latest GPT-3 language model pre-trained on text automatically solvesonly 18.8% of these university questions using zero-shot learning and 30.8%using few-shot learning and the most recent chain of thought prompting. Incontrast, program synthesis with few-shot learning using Codex fine-tuned oncode generates programs that automatically solve 81% of these questions. Ourapproach improves the previous state-of-the-art automatic solution accuracy onthe benchmark topics from 8.8% to 81.1%. We perform a survey to evaluate thequality and difficulty of generated questions. This work is the first toautomatically solve university-level mathematics course questions at a humanlevel and the first work to explain and generate university-level mathematicscourse questions at scale, a milestone for higher education.", "title": "a neural network solves, explains, and generates university math problems by program synthesis and fewshot learning at human level", "url": "http://arxiv.org/pdf/2112.15594v4.pdf", "tokenized_text": "demonstrate neural network pre trained text fine tuned oncode solves mathematics course problems explains solutions generates human level automatically synthesize programs shotlearning openai codex transformer execute solve automatic accuracy curate new dataset questions largest mathematics courses single variable differential equations introduction probability statistics mathematics computer science linear algebra solve questions math dataset algebra counting probability intermediate algebra latest benchmark advanced mathematics assess mathematical reasoning randomly sample questions solutions multiple modalities including numbers equations latest gpt-3 language_model language pre trained text automatically university questions zero shot_learning shot learning shot_learning shot learning recent chain_of_thought chain thought incontrast program synthesis shot_learning shot learning codex fine tuned oncode generates programs automatically solve questions ourapproach improves previous state art automatic solution accuracy onthe benchmark topics 8.8 perform survey evaluate thequality difficulty generated questions work toautomatically solve university level mathematics course questions work explain generate university level questions scale higher education"}
{"id": "nan", "abstract": "  Sophisticated language models such as OpenAI's GPT-3 can generate hatefultext that targets marginalized groups. Given this capacity, we are interestedin whether large language models can be used to identify hate speech andclassify text as sexist or racist. We use GPT-3 to identify sexist and racisttext passages with zero-, one-, and few-shot learning. We find that with zero-and one-shot learning, GPT-3 can identify sexist or racist text with an averageaccuracy between 55 per cent and 67 per cent, depending on the category of textand type of learning. With few-shot learning, the model's accuracy can be ashigh as 85 per cent. Large language models have a role to play in hate speechdetection, and with further development they could eventually be used tocounter hate speech.", "title": "detecting hate speech with gpt3", "url": "http://arxiv.org/pdf/2103.12407v4.pdf", "tokenized_text": "sophisticated language_models language openai gpt-3 generate targets marginalized groups given capacity large_language large language identify hate speech text use gpt-3 identify passages zero- shot_learning shot learning find zero shot_learning shot learning gpt-3 identify text cent cent depending category type learning shot_learning shot learning accuracy 85 cent large_language large language role play hate development hate speech"}
{"id": "nan", "abstract": "  Pretrained language models (LMs) perform well on many tasks even whenlearning from a few examples, but prior work uses many held-out examples totune various aspects of learning, such as hyperparameters, training objectives,and natural language templates (\"prompts\"). Here, we evaluate the few-shotability of LMs when such held-out examples are unavailable, a setting we calltrue few-shot learning. We test two model selection criteria, cross-validationand minimum description length, for choosing LM prompts and hyperparameters inthe true few-shot setting. On average, both marginally outperform randomselection and greatly underperform selection based on held-out examples.Moreover, selection criteria often prefer models that perform significantlyworse than randomly-selected ones. We find similar results even when takinginto account our uncertainty in a model's true performance during selection, aswell as when varying the amount of computation and number of examples used forselection. Overall, our findings suggest that prior work significantlyoverestimated the true few-shot ability of LMs given the difficulty of few-shotmodel selection.", "title": "true fewshot learning with language models", "url": "http://arxiv.org/pdf/2105.11447v1.pdf", "tokenized_text": "pretrained_language pretrained language lms perform tasks examples prior_work prior work uses held examples aspects learning hyperparameters training objectives natural_language natural language templates evaluate lms held examples unavailable setting shot_learning shot learning test selection criteria cross minimum description length choosing lm hyperparameters inthe true shot_setting shot setting average marginally outperform greatly underperform selection based held examples selection criteria prefer perform randomly selected ones find similar results account uncertainty true performance selection aswell varying computation number examples forselection overall findings_suggest findings suggest prior_work prior work true shot ability lms given difficulty selection"}
{"id": "nan", "abstract": "  This paper studies the use of language models as a source of syntheticunlabeled text for NLP. We formulate a general framework called ``generate,annotate, and learn (GAL)'' to take advantage of synthetic text withinknowledge distillation, self-training, and few-shot learning applications. Togenerate high-quality task-specific text, we either fine-tune LMs on inputsfrom the task of interest, or prompt large LMs with few examples. We use thebest available classifier to annotate synthetic text with soft pseudo labelsfor knowledge distillation and self-training, and use LMs to obtain hard labelsfor few-shot learning. We train new supervised models on the combination oflabeled and pseudo-labeled data, which results in significant gains acrossseveral applications. We investigate key components of GAL and presenttheoretical and empirical arguments against the use of class-conditional LMs togenerate synthetic labeled text instead of unlabeled text. GAL achieves newstate-of-the-art knowledge distillation results for 6-layer transformers on theGLUE leaderboard.", "title": "generate, annotate, and learn nlp with synthetic text", "url": "http://arxiv.org/pdf/2106.06168v3.pdf", "tokenized_text": "paper studies use language_models language source text nlp formulate general framework called generate annotate learn advantage synthetic text distillation self training shot_learning shot learning applications togenerate high quality task specific text fine tune lms task interest large lms examples use thebest available classifier annotate synthetic text soft pseudo knowledge_distillation knowledge distillation self training use lms obtain hard shot_learning shot learning train new supervised combination oflabeled pseudo labeled_data labeled data results significant gains applications investigate key components empirical arguments use class conditional lms togenerate synthetic labeled text instead unlabeled text achieves newstate art knowledge_distillation knowledge distillation results layer transformers leaderboard"}
{"id": "nan", "abstract": "  When trained at sufficient scale, auto-regressive language models exhibit thenotable ability to learn a new language task after being prompted with just afew examples. Here, we present a simple, yet effective, approach fortransferring this few-shot learning ability to a multimodal setting (vision andlanguage). Using aligned image and caption data, we train a vision encoder torepresent each image as a sequence of continuous embeddings, such that apre-trained, frozen language model prompted with this prefix generates theappropriate caption. The resulting system is a multimodal few-shot learner,with the surprising ability to learn a variety of new tasks when conditioned onexamples, represented as a sequence of multiple interleaved image and textembeddings. We demonstrate that it can rapidly learn words for new objects andnovel visual categories, do visual question-answering with only a handful ofexamples, and make use of outside knowledge, by measuring a single model on avariety of established and new benchmarks.", "title": "multimodal fewshot learning with frozen language models", "url": "http://arxiv.org/pdf/2106.13884v2.pdf", "tokenized_text": "trained sufficient scale auto regressive language_models language exhibit ability learn new language task prompted afew examples present simple effective approach shot_learning shot learning ability multimodal setting vision andlanguage aligned image caption data train vision encoder image sequence continuous embeddings apre trained frozen language_model language prompted prefix generates theappropriate caption resulting system multimodal shot learner surprising ability learn variety new tasks conditioned represented sequence multiple interleaved image demonstrate rapidly learn words new objects visual categories visual question answering handful ofexamples use outside knowledge measuring single avariety established new benchmarks"}
{"id": "nan", "abstract": "  Collecting and annotating task-oriented dialogues is time-consuming andcostly; thus, zero and few shot learning could greatly benefit dialogue statetracking (DST). In this work, we propose an in-context learning (ICL) frameworkfor zero-shot and few-shot learning DST, where a large pre-trained languagemodel (LM) takes a test instance and a few exemplars as input, and directlydecodes the dialogue state without any parameter updates. To better leverage atabular domain description in the LM prompt, we reformulate DST into atext-to-SQL problem. We also propose a novel approach to retrieve annotateddialogues as exemplars. Empirical results on MultiWOZ show that our methodIC-DST substantially outperforms previous fine-tuned state-of-the-art models infew-shot settings. In addition, we test IC-DST in zero-shot settings, in whichthe model only takes a fixed task instruction as input, finding that itoutperforms previous zero-shot methods by a large margin.", "title": "incontext learning for fewshot dialogue state tracking", "url": "http://arxiv.org/pdf/2203.08568v3.pdf", "tokenized_text": "collecting annotating task oriented dialogues time consuming zero shot_learning shot learning greatly benefit dialogue dst work propose context_learning context learning icl frameworkfor zero shot shot_learning shot learning dst large pre trained languagemodel lm takes test instance exemplars input dialogue state parameter updates better leverage domain description lm reformulate dst sql problem propose_a_novel propose novel approach retrieve exemplars empirical results multiwoz dst substantially outperforms previous fine tuned state art infew shot_settings shot settings addition test ic dst zero shot_settings shot settings takes fixed task instruction input finding itoutperforms previous zero shot methods large margin"}
{"id": "nan", "abstract": "  Many NLP classification tasks, such as sexism/racism detection or toxicitydetection, are based on human values. Yet, human values can vary under diversecultural conditions. Therefore, we introduce a framework for value-alignedclassification that performs prediction based on explicitly written humanvalues in the command. Along with the task, we propose a practical approachthat distills value-aligned knowledge from large-scale language models (LLMs)to construct value-aligned classifiers in two steps. First, we generatevalue-aligned training data from LLMs by prompt-based few-shot learning. Next,we fine-tune smaller classification models with the generated data for thetask. Empirical results show that our VA-Models surpass multiple baselines byat least 15.56% on the F1-score, including few-shot learning with OPT-175B andexisting text augmentation methods. We suggest that using classifiers withexplicit human value input improves both inclusivity & explainability in AI.", "title": "enabling classifiers to make judgements explicitly aligned with human values", "url": "http://arxiv.org/pdf/2210.07652v1.pdf", "tokenized_text": "nlp classification tasks racism detection based human values human values vary conditions introduce framework value performs prediction based explicitly written command task propose practical approachthat value aligned knowledge large scale language_models language construct value aligned classifiers steps aligned training_data training data llms based shot_learning shot learning fine tune smaller classification generated data thetask empirical results surpass multiple baselines f1 score including shot_learning shot learning opt-175b text augmentation methods suggest classifiers human value input improves explainability ai"}
{"id": "nan", "abstract": "  Prompt-based techniques have demostrated great potential for improving thefew-shot generalization of pretrained language models. However, theirperformance heavily relies on the manual design of prompts and thus requires alot of human efforts. In this paper, we introduce Genetic Prompt Search (GPS)to improve few-shot learning with prompts, which utilizes a genetic algorithmto automatically search for high-performing prompts. GPS is gradient-free andrequires no update of model parameters but only a small validation set.Experiments on diverse datasets proved the effectiveness of GPS, whichoutperforms manual prompts by a large margin of 2.6 points. Our method is alsobetter than other parameter-efficient tuning methods such as prompt tuning.", "title": "gps genetic prompt search for efficient fewshot learning", "url": "http://arxiv.org/pdf/2210.17041v1.pdf", "tokenized_text": "based techniques great_potential great potential improving thefew shot generalization pretrained_language pretrained language theirperformance heavily relies manual design requires human efforts paper introduce genetic search improve shot_learning shot learning utilizes genetic automatically search high performing gradient free update parameters small validation set experiments diverse datasets proved effectiveness manual large margin points method parameter efficient tuning methods tuning"}
{"id": "nan", "abstract": "  Query-focused summarization has been considered as an important extension fortext summarization. It aims to generate a concise highlight for a given query.Different from text summarization, query-focused summarization has long beenplagued by the problem of lacking high-quality large-scale datasets. In thispaper, we investigate the idea that whether we can integrate and transfer theknowledge of text summarization and question answering to assist the few-shotlearning in query-focused summarization. Here, we propose prefix-merging, aprefix-based pretraining strategy for few-shot learning in query-focusedsummarization. Drawn inspiration from prefix-tuning, we are allowed tointegrate the task knowledge from text summarization and question answeringinto a properly designed prefix and apply the merged prefix to query-focusedsummarization. With only a small amount of trainable parameters, prefix-mergingoutperforms fine-tuning on query-focused summarization. We further discuss theinfluence of different prefix designs and propose a visualized explanation forhow prefix-merging works.", "title": "fewshot queryfocused summarization with prefixmerging", "url": "http://arxiv.org/pdf/2211.16164v1.pdf", "tokenized_text": "query focused summarization considered important extension fortext summarization aims generate concise highlight given query different text summarization query focused summarization long problem lacking high quality large scale datasets thispaper investigate idea integrate transfer theknowledge text summarization question_answering question answering assist shotlearning query focused summarization propose prefix merging based pretraining strategy shot_learning shot learning query drawn inspiration prefix tuning allowed task knowledge text summarization question properly designed prefix apply prefix query small trainable parameters prefix fine tuning query focused summarization discuss different prefix designs propose visualized explanation prefix merging works"}
{"id": "nan", "abstract": "  Logs generated by large-scale software systems provide crucial informationfor engineers to understand the system status and diagnose problems of thesystems. Log parsing, which converts raw log messages into structured data, isthe first step to enabling automated log analytics. Existing log parsersextract the common part as log templates using statistical features. However,these log parsers often fail to identify the correct templates and parametersbecause: 1) they often overlook the semantic meaning of log messages, and 2)they require domain-specific knowledge for different log datasets. To addressthe limitations of existing methods, in this paper, we propose LogPPT tocapture the patterns of templates using prompt-based few-shot learning. LogPPTutilises a novel prompt tuning method to recognise keywords and parametersbased on a few labelled log data. In addition, an adaptive random samplingalgorithm is designed to select a small yet diverse training set. We haveconducted extensive experiments on 16 public log datasets. The experimentalresults show that LogPPT is effective and efficient for log parsing.", "title": "log parsing with promptbased fewshot learning", "url": "http://arxiv.org/pdf/2302.07435v1.pdf", "tokenized_text": "logs generated large scale software systems provide crucial engineers understand system status diagnose problems log parsing converts raw log messages structured data isthe step enabling automated log analytics existing log common log_templates log templates statistical features log parsers fail identify correct templates overlook semantic meaning log messages require domain specific knowledge different log datasets addressthe limitations existing_methods existing methods paper propose tocapture patterns templates based shot_learning shot learning novel tuning method recognise keywords labelled log data addition adaptive random designed select small diverse training set extensive_experiments extensive experiments 16 public log datasets experimentalresults effective efficient log parsing"}
{"id": "nan", "abstract": "  A particularly successful class of approaches for few-shot learning combineslanguage models with prompts -- hand-crafted task descriptions that complementdata samples. However, designing prompts by hand for each task commonlyrequires domain knowledge and substantial guesswork. We observe, in the contextof classification tasks, that instruction finetuned language models exhibitremarkable prompt robustness, and we subsequently propose a simple method toeliminate the need for handcrafted prompts, named AuT-Few. This approachconsists of (i) a prompt retrieval module that selects suitable taskinstructions from the instruction-tuning knowledge base, and (ii) thegeneration of two distinct, semantically meaningful, class descriptions and aselection mechanism via cross-validation. Over $12$ datasets, spanning $8$classification tasks, we show that AuT-Few outperforms current state-of-the-artfew-shot learning methods. Moreover, AuT-Few is the best ranking method acrossdatasets on the RAFT few-shot benchmark. Notably, these results are achievedwithout task-specific handcrafted prompts on unseen tasks.", "title": "automated fewshot classification with instructionfinetuned language models", "url": "http://arxiv.org/pdf/2305.12576v2.pdf", "tokenized_text": "particularly successful class approaches shot_learning shot learning hand crafted task descriptions samples designing hand task domain knowledge substantial observe classification tasks instruction finetuned language_models language exhibitremarkable robustness subsequently propose simple method need handcrafted named retrieval module selects suitable taskinstructions instruction tuning knowledge base ii thegeneration distinct semantically meaningful class descriptions mechanism cross validation 12 datasets spanning tasks outperforms current state shot_learning shot learning methods best ranking method acrossdatasets raft shot benchmark notably results task specific handcrafted unseen tasks"}
{"id": "nan", "abstract": "  We investigated the potential of large language models (LLMs) in developingdataset validation tests. We carried out 96 experiments each for both GPT-3.5and GPT-4, examining different prompt scenarios, learning modes, temperaturesettings, and roles. The prompt scenarios were: 1) Asking for expectations, 2)Asking for expectations with a given context, 3) Asking for expectations afterrequesting a simulation, and 4) Asking for expectations with a provided datasample. For learning modes, we tested: 1) zero-shot, 2) one-shot, and 3)few-shot learning. We also tested four temperature settings: 0, 0.4, 0.6, and1. Furthermore, two distinct roles were considered: 1) \"helpful assistant\", 2)\"expert data scientist\". To gauge consistency, every setup was tested fivetimes. The LLM-generated responses were benchmarked against a gold standardsuite, created by an experienced data scientist knowledgeable about the data inquestion. We find there are considerable returns to the use of few-shotlearning, and that the more explicit the data setting can be the better. Thebest LLM configurations complement, rather than substitute, the gold standardresults. This study underscores the value LLMs can bring to the data cleaningand preparation stages of the data science workflow.", "title": "evaluating the decency and consistency of data validation tests generated by llms", "url": "http://arxiv.org/pdf/2310.01402v1.pdf", "tokenized_text": "investigated potential large_language large language llms validation tests 96 experiments gpt-3.5and gpt-4 examining different scenarios learning modes roles scenarios asking expectations expectations given context asking expectations simulation asking expectations provided learning modes tested zero shot shot shot_learning shot learning tested temperature settings 0.6 furthermore distinct roles considered helpful assistant data consistency setup tested llm generated responses benchmarked gold created experienced data knowledgeable data find considerable returns use shotlearning explicit data setting better thebest llm configurations complement gold study underscores value llms bring data preparation stages data science workflow"}
{"id": "nan", "abstract": "  Large-scale generative language models such as GPT-3 are competitive few-shotlearners. While these models are known to be able to jointly represent manydifferent languages, their training data is dominated by English, potentiallylimiting their cross-lingual generalization. In this work, we trainmultilingual generative language models on a corpus covering a diverse set oflanguages, and study their few- and zero-shot learning capabilities in a widerange of tasks. Our largest model with 7.5 billion parameters sets new state ofthe art in few-shot learning in more than 20 representative languages,outperforming GPT-3 of comparable size in multilingual commonsense reasoning(with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in4-shot settings) and natural language inference (+5.4% in each of 0-shot and4-shot settings). On the FLORES-101 machine translation benchmark, our modeloutperforms GPT-3 on 171 out of 182 directions with 32 training examples, whilesurpassing the official supervised baseline in 45 directions. We conduct anin-depth analysis of different multilingual prompting approaches, showing inparticular that strong few-shot learning performance across languages can beachieved via cross-lingual transfer through both templates and demonstrationexamples. Finally, we evaluate our models in social value tasks such as hatespeech detection in five languages and find it has limitations similar tocomparable sized GPT-3 models.", "title": "fewshot learning with multilingual language models", "url": "http://arxiv.org/pdf/2112.10668v3.pdf", "tokenized_text": "large scale generative language_models language gpt-3 competitive shotlearners known able jointly represent languages training_data training data dominated english cross lingual generalization work generative language_models language corpus covering diverse set study few- zero shot_learning shot learning capabilities widerange tasks largest 7.5 billion parameters sets new state ofthe art shot_learning shot learning 20 representative languages outperforming gpt-3 comparable size multilingual commonsense absolute accuracy improvement shot_settings shot settings shot_settings shot settings natural_language natural language inference shot shot_settings shot settings flores-101 machine_translation machine translation benchmark gpt-3 directions 32 training_examples training examples whilesurpassing official supervised baseline 45 directions conduct anin depth analysis different multilingual approaches showing inparticular strong shot_learning shot learning performance languages beachieved cross lingual_transfer lingual transfer templates demonstrationexamples finally evaluate social value tasks detection languages find limitations similar sized gpt-3"}
{"id": "nan", "abstract": "  Building models that can be rapidly adapted to novel tasks using only ahandful of annotated examples is an open challenge for multimodal machinelearning research. We introduce Flamingo, a family of Visual Language Models(VLM) with this ability. We propose key architectural innovations to: (i)bridge powerful pretrained vision-only and language-only models, (ii) handlesequences of arbitrarily interleaved visual and textual data, and (iii)seamlessly ingest images or videos as inputs. Thanks to their flexibility,Flamingo models can be trained on large-scale multimodal web corpora containingarbitrarily interleaved text and images, which is key to endow them within-context few-shot learning capabilities. We perform a thorough evaluation ofour models, exploring and measuring their ability to rapidly adapt to a varietyof image and video tasks. These include open-ended tasks such as visualquestion-answering, where the model is prompted with a question which it has toanswer; captioning tasks, which evaluate the ability to describe a scene or anevent; and close-ended tasks such as multiple-choice visual question-answering.For tasks lying anywhere on this spectrum, a single Flamingo model can achievea new state of the art with few-shot learning, simply by prompting the modelwith task-specific examples. On numerous benchmarks, Flamingo outperformsmodels fine-tuned on thousands of times more task-specific data.", "title": "flamingo a visual language model for fewshot learning", "url": "http://arxiv.org/pdf/2204.14198v2.pdf", "tokenized_text": "building rapidly adapted novel tasks ahandful annotated examples open challenge multimodal machinelearning research introduce flamingo family visual language ability propose key architectural innovations powerful pretrained vision language ii arbitrarily interleaved visual textual data ingest images videos inputs thanks flexibility flamingo trained large scale multimodal web corpora interleaved text images key context shot_learning shot learning capabilities perform thorough evaluation ofour exploring measuring ability rapidly adapt varietyof image video tasks include open ended tasks answering prompted question toanswer captioning tasks evaluate ability describe scene close ended tasks multiple choice visual question answering tasks lying spectrum single flamingo new state_of_the_art state art shot_learning shot learning simply modelwith task specific examples numerous benchmarks flamingo fine tuned thousands times task specific data"}
{"id": "nan", "abstract": "  Few-shot learning with large-scale, pre-trained language models is a powerfulway to answer questions about code, e.g., how to complete a given code example,or even generate code snippets from scratch. The success of these models raisesthe question whether they could serve as a basis for building a wide range codegeneration tools. Traditionally, such tools are built manually and separatelyfor each task. Instead, few-shot learning may allow to obtain different toolsfrom a single pre-trained language model by simply providing a few examples ora natural language description of the expected tool behavior. This paperstudies to what extent a state-of-the-art, pre-trained language model of code,Codex, may serve this purpose. We consider three code manipulation and codegeneration tasks targeted by a range of traditional tools: (i) code mutation;(ii) test oracle generation from natural language documentation; and (iii) testcase generation. For each task, we compare few-shot learning to a manuallybuilt tool. Our results show that the model-based tools complement (codemutation), are on par (test oracle generation), or even outperform theirrespective traditionally built tool (test case generation), while imposing farless effort to develop them. By comparing the effectiveness of differentvariants of the model-based tools, we provide insights on how to design anappropriate input (\"prompt\") to the model and what influence the size of themodel has. For example, we find that providing a small natural languagedescription of the code generation task is an easy way to improve predictions.Overall, we conclude that few-shot language models are surprisingly effective,yet there is still more work to be done, such as exploring more diverse ways ofprompting and tackling even more involved tasks.", "title": "code generation tools (almost) for free a study of fewshot, pretrained language models on code", "url": "http://arxiv.org/pdf/2206.01335v2.pdf", "tokenized_text": "shot_learning shot learning large scale pre trained_language trained language answer questions code e.g. complete given code example generate code snippets scratch success question serve basis building wide_range wide range codegeneration tools traditionally tools built manually task instead shot_learning shot learning allow obtain different single pre trained_language trained language simply providing examples ora natural_language natural language description expected tool behavior extent state art pre trained_language trained language code codex serve purpose consider code manipulation codegeneration tasks targeted range traditional tools code test oracle generation natural_language natural language documentation iii generation task compare shot_learning shot learning tool results based tools complement par test oracle generation outperform theirrespective traditionally built tool test case generation imposing effort develop comparing effectiveness based tools provide insights design input influence size themodel example find providing small natural code_generation code generation task easy way improve predictions overall conclude shot language_models language surprisingly effective work exploring diverse ways ofprompting tackling involved tasks"}
{"id": "nan", "abstract": "  It has been shown for English that discrete and soft prompting performstrongly in few-shot learning with pretrained language models (PLMs). In thispaper, we show that discrete and soft prompting perform better than finetuningin multilingual cases: Crosslingual transfer and in-language training ofmultilingual natural language inference. For example, with 48 English trainingexamples, finetuning obtains 33.74% accuracy in crosslingual transfer, barelysurpassing the majority baseline (33.33%). In contrast, discrete and softprompting outperform finetuning, achieving 36.43% and 38.79%. We alsodemonstrate good performance of prompting with training data in multiplelanguages other than English.", "title": "discrete and soft prompting for multilingual models", "url": "http://arxiv.org/pdf/2109.03630v1.pdf", "tokenized_text": "shown english discrete soft shot_learning shot learning pretrained_language pretrained language plms thispaper discrete soft perform better multilingual cases crosslingual transfer language training ofmultilingual natural_language natural language inference example 48 english trainingexamples finetuning obtains accuracy crosslingual transfer majority baseline contrast discrete outperform finetuning achieving good performance training_data training data english"}
{"id": "nan", "abstract": "  Sentence Simplification aims to rephrase complex sentences into simplersentences while retaining original meaning. Large Language models (LLMs) havedemonstrated the ability to perform a variety of natural language processingtasks. However, it is not yet known whether LLMs can be served as ahigh-quality sentence simplification system. In this work, we empiricallyanalyze the zero-/few-shot learning ability of LLMs by evaluating them on anumber of benchmark test sets. Experimental results show LLMs outperformstate-of-the-art sentence simplification methods, and are judged to be on a parwith human annotators.", "title": "sentence simplification via large language models", "url": "http://arxiv.org/pdf/2302.11957v1.pdf", "tokenized_text": "sentence simplification aims rephrase complex sentences retaining original meaning large_language large language llms havedemonstrated ability perform variety natural_language natural language processingtasks known llms served ahigh quality sentence simplification system work zero-/few shot_learning shot learning ability llms evaluating benchmark test sets experimental_results experimental results llms art sentence simplification methods judged human annotators"}
{"id": "nan", "abstract": "  The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shotperformance solely by leveraging a natural-language prompt and a few taskdemonstrations as input context. Inspired by their findings, we study few-shotlearning in a more practical scenario, where we use smaller language models forwhich fine-tuning is computationally efficient. We present LM-BFF--betterfew-shot fine-tuning of language models--a suite of simple and complementarytechniques for fine-tuning language models on a small number of annotatedexamples. Our approach includes (1) prompt-based fine-tuning together with anovel pipeline for automating prompt generation; and (2) a refined strategy fordynamically and selectively incorporating demonstrations into each context.Finally, we present a systematic evaluation for analyzing few-shot performanceon a range of NLP tasks, including classification and regression. Ourexperiments demonstrate that our methods combine to dramatically outperformstandard fine-tuning procedures in this low resource setting, achieving up to30% absolute improvement, and 11% on average across all tasks. Our approachmakes minimal assumptions on task resources and domain expertise, and henceconstitutes a strong task-agnostic method for few-shot learning.", "title": "making pretrained language models better fewshot learners", "url": "http://arxiv.org/pdf/2012.15723v2.pdf", "tokenized_text": "recent gpt-3 brown et_al et al 2020 achieves remarkable shotperformance solely leveraging natural language taskdemonstrations input context inspired findings study shotlearning practical scenario use smaller language_models language forwhich fine tuning computationally efficient present lm bff shot fine tuning language_models language suite simple fine tuning language_models language small_number small number approach includes based fine tuning anovel pipeline automating generation refined strategy selectively incorporating demonstrations context finally present systematic evaluation analyzing shot performanceon range nlp_tasks nlp tasks including classification regression ourexperiments demonstrate methods combine dramatically fine tuning procedures low_resource low resource setting achieving absolute improvement 11 average tasks minimal assumptions task resources domain expertise strong task agnostic method shot_learning shot learning"}
{"id": "nan", "abstract": "  Deep neural language models have set new breakthroughs in many tasks ofNatural Language Processing (NLP). Recent work has shown that deep transformerlanguage models (pretrained on large amounts of texts) can achieve high levelsof task-specific few-shot performance comparable to state-of-the-art models.However, the ability of these large language models in few-shot transferlearning has not yet been explored in the biomedical domain. We investigatedthe performance of two powerful transformer language models, i.e. GPT-3 andBioBERT, in few-shot settings on various biomedical NLP tasks. The experimentalresults showed that, to a great extent, both the models underperform a languagemodel fine-tuned on the full training data. Although GPT-3 had already achievednear state-of-the-art results in few-shot knowledge transfer on open-domain NLPtasks, it could not perform as effectively as BioBERT, which is orders ofmagnitude smaller than GPT-3. Regarding that BioBERT was already pretrained onlarge biomedical text corpora, our study suggests that language models maylargely benefit from in-domain pretraining in task-specific few-shot learning.However, in-domain pretraining seems not to be sufficient; novel pretrainingand few-shot learning strategies are required in the biomedical NLP domain.", "title": "gpt3 models are poor fewshot learners in the biomedical domain", "url": "http://arxiv.org/pdf/2109.02555v2.pdf", "tokenized_text": "deep neural language_models language set new breakthroughs tasks ofnatural language_processing language processing nlp recent_work recent work shown deep pretrained large amounts texts achieve high task specific shot performance comparable state art ability large_language large language shot transferlearning explored biomedical domain performance powerful transformer language_models language i.e. gpt-3 shot_settings shot settings biomedical nlp_tasks nlp tasks experimentalresults showed great extent underperform languagemodel fine tuned training_data training data gpt-3 state art results shot knowledge transfer open domain nlptasks perform effectively biobert orders smaller gpt-3 biobert pretrained onlarge biomedical text corpora study suggests language_models language benefit domain pretraining task specific shot_learning shot learning domain pretraining sufficient novel shot_learning shot learning strategies required biomedical nlp domain"}
{"id": "nan", "abstract": "  We present a new method LiST is short for Lite Prompted Self-Training forparameter-efficient fine-tuning of large pre-trained language models (PLMs) forfew-shot learning. LiST improves over recent methods that adopt prompt-basedfine-tuning (FN) using two key techniques. The first is the use ofself-training to leverage large amounts of unlabeled data for prompt-based FNin few-shot settings. We use self-training in conjunction with meta-learningfor re-weighting noisy pseudo-prompt labels. Self-training is expensive as itrequires updating all the model parameters repetitively. Therefore, we use asecond technique for light-weight fine-tuning where we introduce a small numberof task-specific parameters that are fine-tuned during self-training whilekeeping the PLM encoder frozen. Our experiments show that LiST can effectivelyleverage unlabeled data to improve the model performance for few-shot learning.Additionally, the fine-tuning is efficient as it only updates a smallpercentage of parameters and the overall model footprint is reduced sinceseveral tasks can share a common PLM encoder as backbone. A comprehensive studyon six NLU tasks demonstrate LiST to improve by 35% over classic fine-tuningand 6% over prompt-based FN with 96% reduction in number of trainableparameters when fine-tuned with no more than 30 labeled examples from eachtask. With only 14M tunable parameters, LiST outperforms GPT-3 in-contextlearning by 33% on few-shot NLU tasks.", "title": "list lite prompted selftraining makes parameterefficient fewshot learners", "url": "http://arxiv.org/pdf/2110.06274v2.pdf", "tokenized_text": "present new method list short prompted self-training efficient fine tuning large pre trained_language trained language plms forfew shot_learning shot learning list improves recent methods adopt basedfine tuning key techniques use ofself training leverage large amounts unlabeled data based shot_settings shot settings use self training conjunction meta learningfor weighting noisy pseudo labels self training expensive updating parameters use technique light weight fine tuning introduce small numberof task specific parameters fine tuned self training plm encoder frozen experiments list unlabeled data improve performance shot_learning shot learning additionally fine tuning efficient updates parameters overall footprint reduced tasks share common plm encoder backbone comprehensive nlu tasks demonstrate list improve 35 classic fine tuningand based 96 reduction number trainableparameters fine tuned 30 labeled examples 14 parameters list outperforms gpt-3 contextlearning 33 shot nlu tasks"}
{"id": "nan", "abstract": "  Stance detection aims to identify whether the author of a text is in favorof, against, or neutral to a given target. The main challenge of this taskcomes two-fold: few-shot learning resulting from the varying targets and thelack of contextual information of the targets. Existing works mainly focus onsolving the second issue by designing attention-based models or introducingnoisy external knowledge, while the first issue remains under-explored. In thispaper, inspired by the potential capability of pre-trained language models(PLMs) serving as knowledge bases and few-shot learners, we propose tointroduce prompt-based fine-tuning for stance detection. PLMs can provideessential contextual information for the targets and enable few-shot learningvia prompts. Considering the crucial role of the target in stance detectiontask, we design target-aware prompts and propose a novel verbalizer. Instead ofmapping each label to a concrete word, our verbalizer maps each label to avector and picks the label that best captures the correlation between thestance and the target. Moreover, to alleviate the possible defect of dealingwith varying targets with a single hand-crafted prompt, we propose to distillthe information learned from multiple prompts. Experimental results show thesuperior performance of our proposed model in both full-data and few-shotscenarios.", "title": "fewshot stance detection via targetaware prompt distillation", "url": "http://arxiv.org/pdf/2206.13214v1.pdf", "tokenized_text": "stance detection aims identify author text neutral given target main challenge fold shot_learning shot learning resulting varying targets thelack contextual information targets existing works mainly focus second issue designing attention based external_knowledge external knowledge issue remains explored thispaper inspired potential capability pre trained_language trained language models(plms serving knowledge bases shot learners propose tointroduce based fine tuning stance detection plms contextual information targets enable shot considering crucial role target stance design target aware propose_a_novel propose novel verbalizer instead label concrete word verbalizer maps label label best captures correlation target alleviate possible defect dealingwith varying targets single hand crafted propose information learned multiple experimental_results experimental results performance proposed data shotscenarios"}
{"id": "nan", "abstract": "  The ability to quickly learn a new task with minimal instruction - known asfew-shot learning - is a central aspect of intelligent agents. Classicalfew-shot benchmarks make use of few-shot samples from a single modality, butsuch samples may not be sufficient to characterize an entire concept class. Incontrast, humans use cross-modal information to learn new concepts efficiently.In this work, we demonstrate that one can indeed build a better ${\\bf visual}$dog classifier by ${\\bf read}$ing about dogs and ${\\bf listen}$ing to thembark. To do so, we exploit the fact that recent multimodal foundation modelssuch as CLIP are inherently cross-modal, mapping different modalities to thesame representation space. Specifically, we propose a simple cross-modaladaptation approach that learns from few-shot examples spanning differentmodalities. By repurposing class names as additional one-shot training samples,we achieve SOTA results with an embarrassingly simple linear classifier forvision-language adaptation. Furthermore, we show that our approach can benefitexisting methods such as prefix tuning, adapters, and classifier ensembling.Finally, to explore other modalities beyond vision and language, we constructthe first (to our knowledge) audiovisual few-shot benchmark and use cross-modaltraining to improve the performance of both image and audio classification.", "title": "multimodality helps unimodality crossmodal fewshot learning with multimodal models", "url": "http://arxiv.org/pdf/2301.06267v4.pdf", "tokenized_text": "ability quickly learn new task minimal instruction known asfew shot_learning shot learning central aspect intelligent agents shot benchmarks use shot samples single modality samples sufficient characterize entire concept class incontrast humans use cross modal information learn new concepts efficiently work demonstrate build better classifier exploit fact recent multimodal foundation modelssuch clip inherently cross modal mapping different modalities thesame representation space specifically propose simple cross approach learns shot examples spanning class names additional shot training samples achieve sota results simple linear classifier language adaptation furthermore approach methods prefix tuning adapters classifier ensembling finally explore modalities vision language knowledge shot benchmark use cross improve performance image audio classification"}
{"id": "nan", "abstract": "  Large-scale pre-trained models have been known that they are transferable,and they generalize well on the unseen dataset. Recently, multimodalpre-trained models such as CLIP show significant performance improvement indiverse experiments. However, when the labeled dataset is limited, thegeneralization of a new dataset or domain is still challenging. To improve thegeneralization performance on few-shot learning, there have been diverseefforts, such as prompt learning and adapter. However, the current few-shotadaptation methods are not interpretable, and they require a high computationcost for adaptation. In this study, we propose a new method, robust promptlearning with knowledge graph (RPLKG). Based on the knowledge graph, weautomatically design diverse interpretable and meaningful prompt sets. Ourmodel obtains cached embeddings of prompt sets after one forwarding from alarge pre-trained model. After that, model optimizes the prompt selectionprocesses with GumbelSoftmax. In this way, our model is trained usingrelatively little memory and learning time. Also, RPLKG selects the optimalinterpretable prompt automatically, depending on the dataset. In summary, RPLKGis i) interpretable, ii) requires small computation resources, and iii) easy toincorporate prior human knowledge. To validate the RPLKG, we providecomprehensive experimental results on few-shot learning, domain generalizationand new class generalization setting. RPLKG shows a significant performanceimprovement compared to zero-shot learning and competitive performance againstseveral prompt learning methods using much lower resources.", "title": "rplkg robust prompt learning with knowledge graph", "url": "http://arxiv.org/pdf/2304.10805v1.pdf", "tokenized_text": "large scale pre trained known transferable generalize unseen dataset recently trained clip significant performance improvement indiverse experiments labeled dataset limited thegeneralization new dataset domain challenging improve thegeneralization performance shot_learning shot learning learning adapter current methods interpretable require high adaptation study propose_a_new propose new method robust promptlearning knowledge_graph knowledge graph based knowledge_graph knowledge graph design diverse interpretable meaningful sets ourmodel obtains cached embeddings sets alarge pre trained optimizes way trained little memory learning time selects automatically depending dataset summary interpretable ii requires small computation resources iii easy toincorporate prior human knowledge validate experimental_results experimental results shot_learning shot learning domain new class generalization setting shows significant performanceimprovement compared zero shot_learning shot learning competitive_performance competitive performance learning methods lower resources"}
{"id": "nan", "abstract": "  State-of-the-art few-shot learning (FSL) methods leverage prompt-basedfine-tuning to obtain remarkable results for natural language understanding(NLU) tasks. While much of the prior FSL methods focus on improving downstreamtask performance, there is a limited understanding of the adversarialrobustness of such methods. In this work, we conduct an extensive study ofseveral state-of-the-art FSL methods to assess their robustness to adversarialperturbations. To better understand the impact of various factors towardsrobustness (or the lack of it), we evaluate prompt-based FSL methods againstfully fine-tuned models for aspects such as the use of unlabeled data, multipleprompts, number of few-shot examples, model size and type. Our results on sixGLUE tasks indicate that compared to fully fine-tuned models, vanilla FSLmethods lead to a notable relative drop in task performance (i.e., are lessrobust) in the face of adversarial perturbations. However, using (i) unlabeleddata for prompt-based FSL and (ii) multiple prompts flip the trend. We furtherdemonstrate that increasing the number of few-shot examples and model size leadto increased adversarial robustness of vanilla FSL methods. Broadly, our worksheds light on the adversarial robustness evaluation of prompt-based FSLmethods for NLU tasks.", "title": "adversarial robustness of promptbased fewshot learning for natural language understanding", "url": "http://arxiv.org/pdf/2306.11066v2.pdf", "tokenized_text": "state art shot_learning shot learning fsl methods leverage basedfine tuning obtain remarkable results natural_language natural language understanding(nlu tasks prior fsl methods focus improving performance limited understanding methods work conduct extensive study ofseveral state art fsl methods assess robustness better understand impact factors lack evaluate based fsl methods fine tuned aspects use unlabeled data multipleprompts number shot examples model_size size type results tasks indicate compared fully fine tuned vanilla lead notable relative drop task performance i.e. face adversarial perturbations unlabeleddata based fsl ii multiple flip trend increasing number shot examples model_size size increased adversarial robustness vanilla fsl methods broadly light adversarial robustness evaluation based nlu tasks"}
{"id": "nan", "abstract": "  Structured knowledge grounding (SKG) leverages structured knowledge tocomplete user requests, such as semantic parsing over databases and questionanswering over knowledge bases. Since the inputs and outputs of SKG tasks areheterogeneous, they have been studied separately by different communities,which limits systematic and compatible research on SKG. In this paper, weovercome this limitation by proposing the UnifiedSKG framework, which unifies21 SKG tasks into a text-to-text format, aiming to promote systematic SKGresearch, instead of being exclusive to a single task, domain, or dataset. Weuse UnifiedSKG to benchmark T5 with different sizes and show that T5, withsimple modifications when necessary, achieves state-of-the-art performance onalmost all of the 21 tasks. We further demonstrate that multi-taskprefix-tuning improves the performance on most tasks, largely improving theoverall performance. UnifiedSKG also facilitates the investigation of zero-shotand few-shot learning, and we show that T0, GPT-3, and Codex struggle inzero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct aseries of controlled experiments on structured knowledge encoding variantsacross SKG tasks. UnifiedSKG is easily extensible to more tasks, and it isopen-sourced at https://github.com/hkunlp/unifiedskg.", "title": "unifiedskg unifying and multitasking structured knowledge grounding with texttotext language models", "url": "http://arxiv.org/pdf/2201.05966v3.pdf", "tokenized_text": "structured knowledge grounding leverages structured knowledge user requests semantic_parsing semantic parsing databases questionanswering knowledge bases inputs outputs tasks studied separately different communities limits systematic compatible research paper limitation proposing framework tasks text text format aiming promote systematic instead single task domain dataset weuse benchmark t5 different sizes t5 modifications necessary achieves_state achieves state art performance 21 tasks demonstrate multi tuning improves performance tasks largely improving performance facilitates investigation zero shotand shot_learning shot learning t0 gpt-3 codex struggle inzero shot shot_learning shot learning use conduct controlled experiments structured knowledge encoding tasks easily extensible tasks sourced"}
{"id": "nan", "abstract": "  A software requirement specification (SRS) document is an essential part ofthe software development life cycle which outlines the requirements that asoftware program in development must satisfy. This document is often specifiedby a diverse group of stakeholders and is subject to continual change, makingthe process of maintaining the document and detecting conflicts betweenrequirements an essential task in software development. Notably, projects thatdo not address conflicts in the SRS document early on face considerableproblems later in the development life cycle. These problems incur substantialcosts in terms of time and money, and these costs often become insurmountablebarriers that ultimately result in the termination of a software projectaltogether. As a result, early detection of SRS conflicts is critical toproject sustainability. The conflict detection task is approached in numerousways, many of which require a significant amount of manual intervention fromdevelopers, or require access to a large amount of labeled, task-specifictraining data. In this work, we propose using a prompt-based learning approachto perform few-shot learning for conflict detection. We compare our results tosupervised learning approaches that use pretrained language models, such asBERT and its variants. Our results show that prompting with just 32 labeledexamples can achieve a similar level of performance in many key metrics to thatof supervised learning on training sets that are magnitudes larger in size. Incontrast to many other conflict detection approaches, we make no assumptionsabout the type of underlying requirements, allowing us to analyze pairings ofboth functional and non-functional requirements. This allows us to omit thepotentially expensive task of filtering out non-functional requirements fromour dataset.", "title": "a promptbased fewshot learning approach to software conflict detection", "url": "http://arxiv.org/pdf/2211.02709v1.pdf", "tokenized_text": "software requirement specification srs document essential ofthe software development life cycle outlines requirements program development satisfy document diverse group stakeholders subject continual change process maintaining document detecting conflicts essential task software development notably projects address conflicts srs document early face later development life cycle problems incur terms time money costs ultimately result software result early detection srs conflicts critical sustainability conflict detection task approached require significant manual intervention require access large labeled task data work propose based learning approachto perform shot_learning shot learning conflict detection compare results learning approaches use pretrained_language pretrained language variants results 32 labeledexamples achieve similar level performance key metrics supervised learning training sets magnitudes larger size incontrast conflict detection approaches type underlying requirements allowing analyze ofboth functional non functional requirements allows expensive task filtering non functional requirements dataset"}
{"id": "nan", "abstract": "  We introduce a novel method for multilingual transfer that utilizes deepcontextual embeddings, pretrained in an unsupervised fashion. While contextualembeddings have been shown to yield richer representations of meaning comparedto their static counterparts, aligning them poses a challenge due to theirdynamic nature. To this end, we construct context-independent variants of theoriginal monolingual spaces and utilize their mapping to derive an alignmentfor the context-dependent spaces. This mapping readily supports processing of atarget language, improving transfer by context-aware embeddings. Ourexperimental results demonstrate the effectiveness of this approach forzero-shot and few-shot learning of dependency parsing. Specifically, our methodconsistently outperforms the previous state-of-the-art on 6 tested languages,yielding an improvement of 6.8 LAS points on average.", "title": "crosslingual alignment of contextual word embeddings, with applications to zeroshot dependency parsing", "url": "http://arxiv.org/pdf/1902.09492v2.pdf", "tokenized_text": "introduce novel method multilingual transfer utilizes embeddings pretrained unsupervised fashion shown yield richer representations meaning comparedto static counterparts aligning poses challenge nature end construct context independent variants theoriginal monolingual spaces utilize mapping derive context dependent spaces mapping readily supports processing atarget language improving transfer context aware embeddings results demonstrate_the_effectiveness demonstrate effectiveness approach forzero shot shot_learning shot learning dependency parsing specifically methodconsistently outperforms previous state art tested languages yielding improvement 6.8 points average"}
{"id": "nan", "abstract": "  GPT-3 can perform numerous tasks when provided a natural language prompt thatcontains a few training examples. We show that this type of few-shot learningcan be unstable: the choice of prompt format, training examples, and even theorder of the training examples can cause accuracy to vary from near chance tonear state-of-the-art. We demonstrate that this instability arises from thebias of language models towards predicting certain answers, e.g., those thatare placed near the end of the prompt or are common in the pre-training data.To mitigate this, we first estimate the model's bias towards each answer byasking for its prediction when given the training prompt and a content-freetest input such as \"N/A\". We then fit calibration parameters that cause theprediction for this input to be uniform across answers. On a diverse set oftasks, this contextual calibration procedure substantially improves GPT-3 andGPT-2's average accuracy (up to 30.0% absolute) and reduces variance acrossdifferent choices of the prompt.", "title": "calibrate before use improving fewshot performance of language models", "url": "http://arxiv.org/pdf/2102.09690v2.pdf", "tokenized_text": "gpt-3 perform numerous tasks provided natural_language natural language training_examples training examples type shot unstable choice format training_examples training examples training_examples training examples cause accuracy vary near state art demonstrate instability arises thebias language_models language predicting certain answers e.g. thatare placed near end common pre training_data training data mitigate estimate bias answer prediction given training content input fit calibration parameters cause theprediction input uniform answers diverse set oftasks contextual calibration procedure substantially improves gpt-3 average accuracy absolute reduces variance acrossdifferent choices"}
{"id": "nan", "abstract": "  We introduce a noisy channel approach for language model prompting infew-shot text classification. Instead of computing the likelihood of the labelgiven the input (referred as direct models), channel models compute theconditional probability of the input given the label, and are thereby requiredto explain every word in the input. We use channel models for recently proposedfew-shot learning methods with no or very limited updates to the language modelparameters, via either in-context demonstration or prompt tuning. Ourexperiments show that, for both methods, channel models significantlyoutperform their direct counterparts, which we attribute to their stability,i.e., lower variance and higher worst-case accuracy. We also present extensiveablations that provide recommendations for when to use channel prompt tuninginstead of other competitive methods (e.g., direct head tuning): channel prompttuning is preferred when the number of training examples is small, labels inthe training data are imbalanced, or generalization to unseen labels isrequired.", "title": "noisy channel language model prompting for fewshot text classification", "url": "http://arxiv.org/pdf/2108.04106v3.pdf", "tokenized_text": "introduce noisy channel approach language_model language infew shot text_classification text classification instead computing likelihood input referred direct channel compute theconditional probability input given label explain word input use channel recently shot_learning shot learning methods limited updates language modelparameters context demonstration tuning ourexperiments methods channel significantlyoutperform direct counterparts attribute stability i.e. lower variance higher worst case accuracy present provide recommendations use channel competitive methods e.g. direct head tuning channel prompttuning preferred number training_examples training examples small labels inthe training_data training data imbalanced generalization unseen labels isrequired"}
{"id": "nan", "abstract": "  In the summer of 2020 OpenAI released its GPT-3 autoregressive language modelto much fanfare. While the model has shown promise on tasks in several areas,it has not always been clear when the results were cherry-picked or when theywere the unvarnished output. We were particularly interested in what benefitsGPT-3 could bring to the SemEval 2021 MeasEval task - identifying measurementsand their associated attributes in scientific literature. We had alreadyexperimented with multi-turn questions answering as a solution to this task. Wewanted to see if we could use GPT-3's few-shot learning capabilities to moreeasily develop a solution that would have better performance than our priorwork. Unfortunately, we have not been successful in that effort. This paperdiscusses the approach we used, challenges we encountered, and results weobserved. Some of the problems we encountered were simply due to the state ofthe art. For example, the limits on the size of the prompt and answer limitedthe amount of the training signal that could be offered. Others are morefundamental. We are unaware of generative models that excel in retainingfactual information. Also, the impact of changes in the prompts isunpredictable, making it hard to reliably improve performance.", "title": "what's in a measurement using gpt3 on semeval 2021 task 8 measeval", "url": "http://arxiv.org/pdf/2106.14720v1.pdf", "tokenized_text": "2020 openai released gpt-3 autoregressive language modelto shown promise tasks areas clear results picked theywere output particularly interested bring semeval 2021 task identifying associated attributes scientific literature multi turn questions answering solution task use gpt-3 shot_learning shot learning capabilities develop solution better performance unfortunately successful effort approach challenges encountered results weobserved problems encountered simply state ofthe art example limits size answer training signal offered generative excel information impact changes making hard reliably improve performance"}
{"id": "nan", "abstract": "  Few-shot NLP research is highly active, yet conducted in disjoint researchthreads with evaluation suites that lack challenging-yet-realistic testingsetups and fail to employ careful experimental design. Consequently, thecommunity does not know which techniques perform best or even if theyoutperform simple baselines. In response, we formulate the FLEX Principles, aset of requirements and best practices for unified, rigorous, valid, andcost-sensitive few-shot NLP evaluation. These principles include Sample SizeDesign, a novel approach to benchmark design that optimizes statisticalaccuracy and precision while keeping evaluation costs manageable. Following theprinciples, we release the FLEX benchmark, which includes four few-shottransfer settings, zero-shot evaluation, and a public leaderboard that coversdiverse NLP tasks. In addition, we present UniFew, a prompt-based model forfew-shot learning that unifies pretraining and finetuning prompt formats,eschewing complex machinery of recent prompt-based approaches in adaptingdownstream task formats to language model pretraining objectives. Wedemonstrate that despite simplicity, UniFew achieves results competitive withboth popular meta-learning and prompt-based approaches.", "title": "flex unifying evaluation for fewshot nlp", "url": "http://arxiv.org/pdf/2107.07170v2.pdf", "tokenized_text": "shot nlp research highly active conducted disjoint evaluation suites lack challenging realistic fail employ careful experimental design consequently thecommunity know techniques perform best simple baselines response formulate principles aset requirements best practices unified rigorous valid sensitive shot nlp evaluation principles include sample novel_approach novel approach benchmark design optimizes precision keeping evaluation costs manageable following theprinciples release benchmark includes settings zero shot evaluation public leaderboard nlp_tasks nlp tasks addition present based forfew shot_learning shot learning unifies pretraining finetuning formats complex recent based approaches task formats language_model language pretraining objectives wedemonstrate despite simplicity achieves results competitive withboth popular meta learning based approaches"}
{"id": "nan", "abstract": "  Intent detection of spoken queries is a challenging task due to their noisystructure and short length. To provide additional information regarding thequery and enhance the performance of intent detection, we propose a method forsemantic expansion of spoken queries, called ConQX, which utilizes the textgeneration ability of an auto-regressive language model, GPT-2. To avoidoff-topic text generation, we condition the input query to a structured contextwith prompt mining. We then apply zero-shot, one-shot, and few-shot learning.We lastly use the expanded queries to fine-tune BERT and RoBERTa for intentdetection. The experimental results show that the performance of intentdetection can be improved by our semantic expansion method.", "title": "conqx semantic expansion of spoken queries for intent detection based on conditioned text generation", "url": "http://arxiv.org/pdf/2109.00729v1.pdf", "tokenized_text": "intent detection spoken queries challenging task short length provide additional information thequery enhance performance intent detection propose method expansion spoken queries called utilizes textgeneration ability auto regressive language_model language gpt-2 topic text generation condition input query structured mining apply zero shot shot shot_learning shot learning lastly use expanded queries fine tune bert roberta experimental_results experimental results performance improved semantic expansion method"}
{"id": "nan", "abstract": "  Recently, a boom of papers has shown extraordinary progress in zero-shot andfew-shot learning with various prompt-based models. It is commonly argued thatprompts help models to learn faster in the same way that humans learn fasterwhen provided with task instructions expressed in natural language. In thisstudy, we experiment with over 30 prompt templates manually written for naturallanguage inference (NLI). We find that models learn just as fast with manyprompts that are intentionally irrelevant or even pathologically misleading asthey do with instructively \"good\" prompts. Further, such patterns hold even formodels as large as 175 billion parameters (Brown et al., 2020) as well as therecently proposed instruction-tuned models which are trained on hundreds ofprompts (Sanh et al., 2022). That is, instruction-tuned models often producegood predictions with irrelevant and misleading prompts even at zero shots. Insum, notwithstanding prompt-based models' impressive improvement, we findevidence of serious limitations that question the degree to which suchimprovement is derived from models understanding task instructions in waysanalogous to humans' use of task instructions.", "title": "do promptbased models really understand the meaning of their prompts", "url": "http://arxiv.org/pdf/2109.01247v2.pdf", "tokenized_text": "recently boom papers shown extraordinary progress zero shot andfew shot_learning shot learning based commonly argued thatprompts help learn faster way humans learn provided task instructions expressed natural_language natural language thisstudy experiment 30 prompt_templates templates manually written naturallanguage inference nli find learn fast intentionally irrelevant misleading good patterns hold large 175 billion parameters brown et_al et al 2020 proposed instruction tuned trained hundreds ofprompts et_al et al 2022 instruction tuned predictions irrelevant misleading zero shots based impressive improvement limitations question degree derived understanding task instructions humans use task instructions"}
{"id": "nan", "abstract": "  Several recent studies on dyadic human-human interactions have been done onconversations without specific business objectives. However, many companiesmight benefit from studies dedicated to more precise environments such as aftersales services or customer satisfaction surveys. In this work, we placeourselves in the scope of a live chat customer service in which we want todetect emotions and their evolution in the conversation flow. This contextleads to multiple challenges that range from exploiting restricted, small andmostly unlabeled datasets to finding and adapting methods for such context.Wetackle these challenges by using Few-Shot Learning while making the hypothesisit can serve conversational emotion classification for different languages andsparse labels. We contribute by proposing a variation of Prototypical Networksfor sequence labeling in conversation that we name ProtoSeq. We test thismethod on two datasets with different languages: daily conversations in Englishand customer service chat conversations in French. When applied to emotionclassification in conversations, our method proved to be competitive even whencompared to other ones.", "title": "fewshot emotion recognition in conversation with sequential prototypical networks", "url": "http://arxiv.org/pdf/2109.09366v1.pdf", "tokenized_text": "recent studies dyadic human human interactions specific business objectives benefit studies dedicated precise environments services customer satisfaction surveys work scope live chat customer service want todetect emotions evolution conversation flow multiple challenges range exploiting restricted small unlabeled datasets finding adapting methods context wetackle challenges shot_learning shot learning making serve conversational emotion classification different languages labels contribute proposing variation prototypical sequence labeling conversation test datasets different languages daily conversations customer service chat conversations french applied conversations method proved competitive ones"}
{"id": "nan", "abstract": "  Global models are trained to be as generalizable as possible, with userinvariance considered desirable since the models are shared across multitudesof users. As such, these models are often unable to produce personalizedresponses for individual users, based on their data. Contrary to widely-usedpersonalization techniques based on few-shot learning, we proposeUserIdentifier, a novel scheme for training a single shared model for allusers. Our approach produces personalized responses by adding fixed,non-trainable user identifiers to the input data. We empirically demonstratethat this proposed method outperforms the prefix-tuning based state-of-the-artapproach by up to 13%, on a suite of sentiment analysis datasets. We also showthat, unlike prior work, this method needs neither any additional modelparameters nor any extra rounds of few-shot fine-tuning.", "title": "useridentifier implicit user representations for simple and effective personalized sentiment analysis", "url": "http://arxiv.org/pdf/2110.00135v2.pdf", "tokenized_text": "global trained generalizable possible considered desirable shared users unable produce individual users based data contrary widely techniques based shot_learning shot learning novel scheme training single shared approach produces personalized responses adding fixed non trainable user input data empirically demonstratethat proposed_method proposed method outperforms prefix tuning based state 13 suite sentiment_analysis sentiment analysis datasets showthat unlike prior_work prior work method needs additional modelparameters extra rounds shot fine tuning"}
{"id": "nan", "abstract": "  Recently, prompt learning has become a new paradigm to utilize pre-trainedlanguage models (PLMs) and achieves promising results in downstream tasks witha negligible increase of parameters. The current usage of discrete andcontinuous prompts assumes that the prompt is fixed for a specific task and allsamples in the task share the same prompt. However, a task may contain quitediverse samples in which some are easy and others are difficult, and diverseprompts are desirable. In this paper, we propose an instance-aware promptlearning method that learns a different prompt for each instance. Specifically,we suppose that each learnable prompt token has a different contribution todifferent instances, and we learn the contribution by calculating the relevancescore between an instance and each prompt token. The contribution weightedprompt would be instance aware. We apply our method to both unidirectional andbidirectional PLMs on both language understanding and generation tasks.Extensive experiments demonstrate that our method obtains considerableimprovements compared to strong baselines. Especially, our method achieves thestate-of-the-art on the SuperGLUE few-shot learning benchmark.", "title": "instanceaware prompt learning for language understanding and generation", "url": "http://arxiv.org/pdf/2201.07126v1.pdf", "tokenized_text": "recently learning new_paradigm new paradigm utilize pre trainedlanguage plms achieves promising_results promising results downstream_tasks downstream tasks witha negligible increase parameters current usage discrete assumes fixed specific task task share task contain samples easy difficult desirable paper propose instance aware promptlearning method learns different instance specifically learnable token different contribution instances learn contribution calculating instance token contribution instance aware apply method unidirectional plms language understanding generation tasks extensive_experiments extensive experiments demonstrate method obtains compared strong baselines especially method_achieves method achieves thestate art superglue shot_learning shot learning benchmark"}
{"id": "nan", "abstract": "  Pretrained language models (PLMs) have demonstrated remarkable performance invarious natural language processing tasks: Unidirectional PLMs (e.g., GPT) arewell known for their superior text generation capabilities; bidirectional PLMs(e.g., BERT) have been the prominent choice for natural language understanding(NLU) tasks. While both types of models have achieved promising few-shotlearning performance, their potential for zero-shot learning has beenunderexplored. In this paper, we present a simple approach that uses both typesof PLMs for fully zero-shot learning of NLU tasks without requiring anytask-specific data: A unidirectional PLM generates class-conditioned textsguided by prompts, which are used as the training data for fine-tuning abidirectional PLM. With quality training data selected based on the generationprobability and regularization techniques (label smoothing and temporalensembling) applied to the fine-tuning stage for better generalization andstability, our approach demonstrates strong performance across sevenclassification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and92.8 on SST-2), significantly outperforming zero-shot prompting methods andachieving even comparable results to strong few-shot approaches using 32training samples per class.", "title": "generating training data with language models towards zeroshot language understanding", "url": "http://arxiv.org/pdf/2202.04538v2.pdf", "tokenized_text": "pretrained_language pretrained language plms demonstrated_remarkable demonstrated remarkable performance invarious natural_language natural language processing tasks unidirectional plms e.g. gpt arewell known superior text generation capabilities bidirectional bert prominent choice natural_language natural language understanding(nlu tasks types achieved promising shotlearning performance potential zero shot_learning shot learning paper present simple approach uses plms fully zero shot_learning shot learning nlu tasks requiring specific data unidirectional plm generates class conditioned training_data training data fine tuning plm quality training_data training data selected based regularization techniques label smoothing applied fine tuning stage better generalization approach demonstrates strong performance tasks glue benchmark e.g. mnli sst-2 significantly outperforming zero shot_prompting shot methods comparable results strong shot approaches samples class"}
{"id": "nan", "abstract": "  In this paper, we propose a variational autoencoder with disentanglementpriors, VAE-DPRIOR, for task-specific natural language generation with none ora handful of task-specific labeled examples. In order to tackle compositionalgeneralization across tasks, our model performs disentangled representationlearning by introducing a conditional prior for the latent content space andanother conditional prior for the latent label space. Both types of priorssatisfy a novel property called $\\epsilon$-disentangled. We show bothempirically and theoretically that the novel priors can disentanglerepresentations even without specific regularizations as in the prior work. Thecontent prior enables directly sampling diverse content representations fromthe content space learned from the seen tasks, and fuse them with therepresentations of novel tasks for generating semantically diverse texts in thelow-resource settings. Our extensive experiments demonstrate the superiorperformance of our model over competitive baselines in terms of i) dataaugmentation in continuous zero/few-shot learning, and ii) text style transferin the few-shot setting.", "title": "variational autoencoder with disentanglement priors for lowresource taskspecific natural language generation", "url": "http://arxiv.org/pdf/2202.13363v3.pdf", "tokenized_text": "paper propose variational autoencoder vae task specific natural_language natural language generation ora handful task specific labeled examples order tackle compositionalgeneralization tasks performs disentangled representationlearning introducing conditional prior latent content space conditional prior latent label space types novel property called theoretically novel priors specific prior_work prior work prior enables directly sampling diverse content representations fromthe content space learned seen tasks fuse novel tasks generating semantically diverse texts resource settings extensive_experiments extensive experiments demonstrate competitive baselines terms dataaugmentation continuous zero shot_learning shot learning ii text style shot_setting shot setting"}
{"id": "nan", "abstract": "  Generating new events given context with correlated ones plays a crucial rolein many event-centric reasoning tasks. Existing works either limit their scopeto specific scenarios or overlook event-level correlations. In this paper, wepropose to pre-train a general Correlation-aware context-to-Event Transformer(ClarET) for event-centric reasoning. To achieve this, we propose three novelevent-centric objectives, i.e., whole event recovering, contrastiveevent-correlation encoding and prompt-based event locating, which highlightevent-level correlations with effective training. The proposed ClarET isapplicable to a wide range of event-centric reasoning scenarios, consideringits versatility of (i) event-correlation types (e.g., causal, temporal,contrast), (ii) application formulations (i.e., generation and classification),and (iii) reasoning types (e.g., abductive, counterfactual and endingreasoning). Empirical fine-tuning results, as well as zero- and few-shotlearning, on 9 benchmarks (5 generation and 4 classification tasks covering 4reasoning types with diverse event correlations), verify its effectiveness andgeneralization ability.", "title": "claret pretraining a correlationaware contexttoevent transformer for eventcentric generation and classification", "url": "http://arxiv.org/pdf/2203.02225v2.pdf", "tokenized_text": "generating new events given context correlated ones plays crucial event centric reasoning tasks existing works limit specific scenarios overlook event level correlations paper wepropose pre train general correlation aware context event event centric reasoning achieve propose centric objectives i.e. event recovering correlation encoding based event locating level correlations effective training proposed wide_range wide range event centric reasoning scenarios versatility event correlation types e.g. causal temporal contrast ii application formulations i.e. generation iii reasoning types e.g. abductive counterfactual empirical fine tuning results zero- shotlearning benchmarks generation classification tasks covering types diverse event correlations verify effectiveness andgeneralization ability"}
{"id": "nan", "abstract": "  Pre-trained masked language models have demonstrated remarkable ability asfew-shot learners. In this paper, as an alternative, we propose a novelapproach to few-shot learning with pre-trained token-replaced detection modelslike ELECTRA. In this approach, we reformulate a classification or a regressiontask as a token-replaced detection problem. Specifically, we first define atemplate and label description words for each task and put them into the inputto form a natural language prompt. Then, we employ the pre-trainedtoken-replaced detection model to predict which label description word is themost original (i.e., least replaced) among all label description words in theprompt. A systematic evaluation on 16 datasets demonstrates that our approachoutperforms few-shot learners with pre-trained masked language models in bothone-sentence and two-sentence learning tasks.", "title": "pretrained tokenreplaced detection model as fewshot learner", "url": "http://arxiv.org/pdf/2203.03235v2.pdf", "tokenized_text": "pre trained masked language_models language demonstrated_remarkable demonstrated remarkable ability asfew shot learners paper alternative propose novelapproach shot_learning shot learning pre trained token replaced detection modelslike electra approach reformulate classification token replaced detection problem specifically define label description words task form natural_language natural language employ pre replaced detection predict label description word themost original i.e. replaced label description words theprompt systematic evaluation 16 datasets demonstrates approachoutperforms shot learners pre trained masked language_models language sentence sentence learning tasks"}
{"id": "nan", "abstract": "  Prompt-based tuning for pre-trained language models (PLMs) has shown itseffectiveness in few-shot learning. Typically, prompt-based tuning wraps theinput text into a cloze question. To make predictions, the model maps theoutput words to labels via a verbalizer, which is either manually designed orautomatically built. However, manual verbalizers heavily depend ondomain-specific prior knowledge and human efforts, while finding appropriatelabel words automatically still remains challenging.In this work, we proposethe prototypical verbalizer (ProtoVerb) which is built directly from trainingdata. Specifically, ProtoVerb learns prototype vectors as verbalizers bycontrastive learning. In this way, the prototypes summarize training instancesand are able to enclose rich class-level semantics. We conduct experiments onboth topic classification and entity typing tasks, and the results demonstratethat ProtoVerb significantly outperforms current automatic verbalizers,especially when training data is extremely scarce. More surprisingly, ProtoVerbconsistently boosts prompt-based tuning even on untuned PLMs, indicating anelegant non-tuning way to utilize PLMs. Our codes are avaliable athttps://github.com/thunlp/OpenPrompt.", "title": "prototypical verbalizer for promptbased fewshot tuning", "url": "http://arxiv.org/pdf/2203.09770v1.pdf", "tokenized_text": "based tuning pre trained_language trained language plms shown shot_learning shot learning typically based tuning theinput text cloze question predictions maps theoutput words labels verbalizer manually designed built manual verbalizers heavily depend ondomain specific prior knowledge human efforts finding words automatically remains challenging work prototypical verbalizer built directly trainingdata specifically learns prototype vectors verbalizers learning way prototypes summarize training able rich class level semantics conduct experiments topic classification entity typing tasks results demonstratethat significantly_outperforms significantly outperforms current automatic verbalizers especially training_data training data extremely scarce surprisingly boosts based tuning untuned plms indicating non tuning way utilize plms codes"}
{"id": "nan", "abstract": "  Prompting methods recently achieve impressive success in few-shot learning.These methods modify input samples with prompt sentence pieces, and decodelabel tokens to map samples to corresponding labels. However, such a paradigmis very inefficient for the task of slot tagging. Since slot tagging samplesare multiple consecutive words in a sentence, the prompting methods have toenumerate all n-grams token spans to find all the possible slots, which greatlyslows down the prediction. To tackle this, we introduce an inverse paradigm forprompting. Different from the classic prompts mapping tokens to labels, wereversely predict slot values given slot types. Such inverse prompting onlyrequires a one-turn prediction for each slot type and greatly speeds up theprediction. Besides, we propose a novel Iterative Prediction Strategy, fromwhich the model learns to refine predictions by considering the relationsbetween different slot types. We find, somewhat surprisingly, the proposedmethod not only predicts faster but also significantly improves the effect(improve over 6.1 F1-scores on 10-shot setting) and achieves newstate-of-the-art performance.", "title": "inverse is better! fast and accurate prompt for fewshot slot tagging", "url": "http://arxiv.org/pdf/2204.00885v1.pdf", "tokenized_text": "methods recently achieve impressive success shot_learning shot learning methods modify input samples sentence pieces tokens map samples corresponding labels inefficient task slot tagging slot tagging multiple words sentence methods token spans find possible slots prediction tackle introduce inverse paradigm forprompting different classic mapping tokens labels predict slot values given slot types inverse turn prediction slot type greatly speeds theprediction propose_a_novel propose novel iterative prediction strategy learns refine predictions considering different slot types find somewhat surprisingly proposedmethod predicts faster significantly improves f1 scores 10 shot_setting shot setting achieves newstate art performance"}
{"id": "nan", "abstract": "  Recent advances in Natural Language Processing, and in particular on theconstruction of very large pre-trained language representation models, isopening up new perspectives on the construction of conversational informationseeking (CIS) systems. In this paper we investigate the usage of in-contextlearning and pre-trained language representation models to address the problemof information extraction from process description documents, in an incrementalquestion and answering oriented fashion. In particular we investigate the usageof the native GPT-3 (Generative Pre-trained Transformer 3) model, together withtwo in-context learning customizations that inject conceptual definitions and alimited number of samples in a few shot-learning fashion. The results highlightthe potential of the approach and the usefulness of the in-context learningcustomizations, which can substantially contribute to address the \"trainingdata challenge\" of deep learning based NLP techniques the BPM field. It alsohighlight the challenge posed by control flow relations for which furthertraining needs to be devised.", "title": "leveraging pretrained language models for conversational information seeking from text", "url": "http://arxiv.org/pdf/2204.03542v1.pdf", "tokenized_text": "recent_advances recent advances natural_language natural language processing particular theconstruction large pre trained_language trained language representation new perspectives construction conversational cis systems paper investigate usage contextlearning pre trained_language trained language representation address problemof information_extraction information extraction process description documents answering oriented fashion particular investigate usageof native gpt-3 generative pre trained transformer context_learning context learning inject conceptual definitions alimited number samples shot learning fashion results potential approach usefulness context substantially contribute address trainingdata challenge deep learning based nlp techniques bpm field alsohighlight challenge posed control flow relations needs devised"}
{"id": "nan", "abstract": "  Pre-trained language models have shown excellent results in few-shot learningscenarios using in-context learning. Although it is impressive, the size oflanguage models can be prohibitive to make them usable in on-deviceapplications, such as sensors or smartphones. With smaller language models,task-specific data annotation is needed to fine-tune the language model for aspecific purpose. However, data annotation can have a substantial financial andtime burden for small research groups, startups, and even companies. In thispaper, we analyze different prompt-based fine-tuning techniques to improveresults on both language and multimodal causal transformer models. To evaluateour results, we use a dataset focusing on visual commonsense reasoning in time.Our results show that by simple model-agnostic prompt-based fine-tuning,comparable results can be reached by only using 35%-40% of the fine-tuningtraining dataset. The proposed approaches result in significant time andfinancial savings. As the proposed methods make minimal architecturalassumptions, other researchers can use the results in their transformer modelswith minimal adaptations. We plan to release the source code freely to make iteasier for the community to use and contribute to our work.", "title": "superprompting utilizing modelindependent contextual data to reduce data annotation required in visual commonsense tasks", "url": "http://arxiv.org/pdf/2204.11922v1.pdf", "tokenized_text": "pre trained_language trained language shown excellent results shot context_learning context learning impressive size oflanguage prohibitive sensors smaller language_models language task specific data annotation needed fine tune language_model language aspecific purpose data annotation substantial financial andtime burden small research groups companies thispaper analyze different based fine tuning techniques language multimodal causal transformer evaluateour results use dataset focusing visual commonsense reasoning time results simple agnostic based fine tuning comparable results reached fine dataset proposed approaches result significant time savings proposed methods minimal researchers use results transformer modelswith minimal adaptations plan release source_code source code freely community use contribute work"}
{"id": "nan", "abstract": "  Recent open-domain dialogue models have brought numerous breakthroughs.However, building a chat system is not scalable since it often requires aconsiderable volume of human-human dialogue data, especially when enforcingfeatures such as persona, style, or safety. In this work, we study thechallenge of imposing roles on open-domain dialogue systems, with the goal ofmaking the systems maintain consistent roles while conversing naturally withhumans. To accomplish this, the system must satisfy a role specification thatincludes certain conditions on the stated features as well as a system policyon whether or not certain types of utterances are allowed. For this, we proposean efficient data collection framework leveraging in-context few-shot learningof large-scale language models for building role-satisfying dialogue datasetfrom scratch. We then compare various architectures for open-domain dialoguesystems in terms of meeting role specifications while maintainingconversational abilities. Automatic and human evaluations show that our modelsreturn few out-of-bounds utterances, keeping competitive performance on generalmetrics. We release a Korean dialogue dataset we built for further research.", "title": "building a role specified opendomain dialogue system leveraging largescale language models", "url": "http://arxiv.org/pdf/2205.00176v1.pdf", "tokenized_text": "recent open domain dialogue brought numerous breakthroughs building chat system scalable requires aconsiderable volume human human dialogue data especially persona style safety work study thechallenge imposing roles open domain dialogue systems goal systems maintain consistent roles conversing naturally accomplish system satisfy role specification thatincludes certain conditions stated features system certain types utterances allowed efficient data collection framework leveraging context shot large scale language_models language building role satisfying dialogue scratch compare architectures open domain terms meeting role specifications abilities automatic human evaluations bounds utterances keeping competitive_performance competitive performance release dialogue dataset built research"}
{"id": "nan", "abstract": "  The success of Pre-Trained Models (PTMs) has reshaped the development ofNatural Language Processing (NLP). Yet, it is not easy to obtainhigh-performing models and deploy them online for industrial practitioners. Tobridge this gap, EasyNLP is designed to make it easy to build NLP applications,which supports a comprehensive suite of NLP algorithms. It further featuresknowledge-enhanced pre-training, knowledge distillation and few-shot learningfunctionalities for large-scale PTMs, and provides a unified framework of modeltraining, inference and deployment for real-world applications. Currently,EasyNLP has powered over ten business units within Alibaba Group and isseamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.The source code of our EasyNLP toolkit is released at GitHub(https://github.com/alibaba/EasyNLP).", "title": "easynlp a comprehensive and easytouse toolkit for natural language processing", "url": "http://arxiv.org/pdf/2205.00258v2.pdf", "tokenized_text": "success pre-trained ptms reshaped development ofnatural language_processing language processing nlp easy performing deploy online industrial practitioners tobridge gap designed easy build nlp applications supports comprehensive suite nlp algorithms enhanced pre training knowledge_distillation knowledge distillation shot large scale ptms provides unified framework inference deployment real world_applications world applications currently powered business units group integrated platform ai products cloud source_code source code toolkit released"}
{"id": "nan", "abstract": "  Ideology is at the core of political science research. Yet, there still doesnot exist general-purpose tools to characterize and predict ideology acrossdifferent genres of text. To this end, we study Pretrained Language Modelsusing novel ideology-driven pretraining objectives that rely on the comparisonof articles on the same story written by media of different ideologies. Wefurther collect a large-scale dataset, consisting of more than 3.6M politicalnews articles, for pretraining. Our model POLITICS outperforms strong baselinesand the previous state-of-the-art models on ideology prediction and stancedetection tasks. Further analyses show that POLITICS is especially good atunderstanding long or formally written texts, and is also robust in few-shotlearning scenarios.", "title": "politics pretraining with samestory article comparison for ideology prediction and stance detection", "url": "http://arxiv.org/pdf/2205.00619v1.pdf", "tokenized_text": "ideology core political science research doesnot exist general purpose tools characterize predict ideology acrossdifferent genres text end study pretrained_language pretrained language novel ideology driven pretraining objectives rely articles story written media different ideologies wefurther collect large scale dataset consisting articles pretraining politics outperforms strong baselinesand previous state art ideology prediction tasks analyses politics especially good long formally written texts robust shotlearning scenarios"}
{"id": "nan", "abstract": "  Extractive Question Answering (EQA) is one of the most important tasks inMachine Reading Comprehension (MRC), which can be solved by fine-tuning thespan selecting heads of Pre-trained Language Models (PLMs). However, mostexisting approaches for MRC may perform poorly in the few-shot learningscenario. To solve this issue, we propose a novel framework named KnowledgeEnhanced Contrastive Prompt-tuning (KECP). Instead of adding pointer heads toPLMs, we introduce a seminal paradigm for EQA that transform the task into anon-autoregressive Masked Language Modeling (MLM) generation problem.Simultaneously, rich semantics from the external knowledge base (KB) and thepassage context are support for enhancing the representations of the query. Inaddition, to boost the performance of PLMs, we jointly train the model by theMLM and contrastive learning objectives. Experiments on multiple benchmarksdemonstrate that our method consistently outperforms state-of-the-artapproaches in few-shot settings by a large margin.", "title": "kecp knowledge enhanced contrastive prompting for fewshot extractive question answering", "url": "http://arxiv.org/pdf/2205.03071v1.pdf", "tokenized_text": "extractive question_answering question answering important tasks inmachine reading comprehension mrc solved fine tuning selecting heads pre trained_language trained language plms approaches mrc perform poorly shot solve issue propose_a_novel propose novel framework named contrastive tuning instead adding heads introduce paradigm transform task anon autoregressive masked language modeling mlm generation problem simultaneously rich semantics external_knowledge external knowledge base kb context support enhancing representations query inaddition boost performance plms jointly train contrastive_learning contrastive learning objectives experiments multiple benchmarksdemonstrate method consistently_outperforms consistently outperforms state shot_settings shot settings large margin"}
{"id": "nan", "abstract": "  Question Answering (QA) is a longstanding challenge in natural languageprocessing. Existing QA works mostly focus on specific question types,knowledge domains, or reasoning skills. The specialty in QA research hinderssystems from modeling commonalities between tasks and generalization for widerapplications. To address this issue, we present ProQA, a unified QA paradigmthat solves various tasks through a single model. ProQA takes a unifiedstructural prompt as the bridge and improves the QA-centric ability bystructural prompt-based pre-training. Through a structurally designedprompt-based input schema, ProQA concurrently models the knowledgegeneralization for all QA tasks while keeping the knowledge customization forevery specific QA task. Furthermore, ProQA is pre-trained with structuralprompt-formatted large-scale synthesized corpus, which empowers the model withthe commonly-required QA ability. Experimental results on 11 QA benchmarksdemonstrate that ProQA consistently boosts performance on both full datafine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore,ProQA exhibits strong ability in both continual learning and transfer learningby taking the advantages of the structural prompt.", "title": "proqa structural promptbased pretraining for unified question answering", "url": "http://arxiv.org/pdf/2205.04040v2.pdf", "tokenized_text": "question_answering question answering qa challenge natural languageprocessing existing qa works focus specific question types knowledge domains reasoning skills qa research modeling commonalities tasks generalization address issue present unified qa solves tasks single takes bridge improves qa centric ability based pre training structurally based input schema concurrently qa tasks keeping knowledge customization specific qa task furthermore pre trained formatted large scale synthesized corpus empowers withthe commonly required qa ability experimental_results experimental results 11 qa benchmarksdemonstrate consistently boosts performance tuning shot_learning shot learning zero shot testing scenarios furthermore exhibits strong ability continual learning transfer taking advantages structural"}
{"id": "nan", "abstract": "  Active learning, which effectively collects informative unlabeled data forannotation, reduces the demand for labeled data. In this work, we propose toretrieve unlabeled samples with a local sensitivity and hardness-awareacquisition function. The proposed method generates data copies through localperturbations and selects data points whose predictive likelihoods diverge themost from their copies. We further empower our acquisition function byinjecting the select-worst case perturbation. Our method achieves consistentgains over the commonly used active learning strategies in variousclassification tasks. Furthermore, we observe consistent improvements over thebaselines on the study of prompt selection in prompt-based few-shot learning.These experiments demonstrate that our acquisition guided by local sensitivityand hardness can be effective and beneficial for many NLP tasks.", "title": "allsh active learning guided by local sensitivity and hardness", "url": "http://arxiv.org/pdf/2205.04980v2.pdf", "tokenized_text": "active learning effectively collects informative unlabeled data reduces demand labeled_data labeled data work propose toretrieve unlabeled samples local sensitivity hardness function proposed_method proposed method generates data copies selects data points predictive diverge themost copies empower acquisition function select worst case perturbation method_achieves method achieves commonly active learning strategies tasks furthermore observe consistent improvements study selection based shot_learning shot learning experiments_demonstrate experiments demonstrate acquisition guided local hardness effective beneficial nlp_tasks nlp tasks"}
{"id": "nan", "abstract": "  In-context learning of GPT-like models has been recognized as fragile acrossdifferent hand-crafted templates, and demonstration permutations. In this work,we propose prototypical calibration to adaptively learn a more robust decisionboundary for zero- and few-shot classification, instead of greedy decoding.Concretely, our method first adopts Gaussian mixture distribution to estimatethe prototypical clusters for all categories. Then we assign each cluster tothe corresponding label by solving a weighted bipartite matching problem. Givenan example, its prediction is calibrated by the likelihood of prototypicalclusters. Experimental results show that prototypical calibration yields asubstantial improvement on a diverse set of tasks. Extensive analysis acrossdifferent scales also indicates that our method calibrates the decisionboundary as expected, greatly improving the robustness of GPT to templates,permutations, and class imbalance.", "title": "prototypical calibration for fewshot learning of language models", "url": "http://arxiv.org/pdf/2205.10183v2.pdf", "tokenized_text": "context_learning context learning gpt like recognized acrossdifferent hand crafted templates demonstration permutations work propose prototypical calibration adaptively learn robust zero- shot classification instead greedy decoding concretely method adopts gaussian mixture distribution prototypical clusters categories assign cluster tothe corresponding label solving weighted bipartite matching problem example prediction calibrated likelihood experimental_results experimental results prototypical calibration yields asubstantial improvement diverse set tasks extensive analysis acrossdifferent scales indicates method calibrates expected greatly improving robustness gpt templates permutations class imbalance"}
{"id": "nan", "abstract": "  Most downstream adaptation methods tune all or part of the parameters ofpre-trained models (PTMs) through gradient descent, where the tuning costincreases linearly with the growth of the model size. By contrast,gradient-free methods only require the forward computation of the PTM to tunethe prompt, retaining the benefits of efficient tuning and deployment. Though,past work on gradient-free tuning often introduces gradient descent to seek agood initialization of prompt and lacks versatility across tasks and PTMs. Inthis paper, we present BBTv2, an improved version of Black-Box Tuning, to drivePTMs for few-shot learning. We prepend continuous prompts to every layer of thePTM and propose a divide-and-conquer gradient-free algorithm to optimize theprompts at different layers alternately. Extensive experiments across varioustasks and PTMs show that BBTv2 can achieve comparable performance to full modeltuning and state-of-the-art parameter-efficient methods (e.g., Adapter, LoRA,BitFit, etc.) under few-shot settings while maintaining much fewer tunableparameters.", "title": "bbtv2 towards a gradientfree future with large language models", "url": "http://arxiv.org/pdf/2205.11200v2.pdf", "tokenized_text": "downstream adaptation methods tune parameters trained ptms gradient descent tuning linearly growth model_size size contrast gradient free methods require forward computation retaining benefits efficient tuning deployment past work gradient free tuning introduces gradient descent seek initialization lacks versatility tasks ptms inthis_paper inthis paper present bbtv2 improved version black-box tuning shot_learning shot learning prepend continuous layer propose divide conquer gradient free algorithm optimize theprompts different layers extensive_experiments extensive experiments varioustasks ptms bbtv2 achieve comparable performance modeltuning state art parameter efficient methods e.g. adapter lora etc shot_settings shot settings maintaining fewer"}
{"id": "nan", "abstract": "  The size of vision models has grown exponentially over the last few years,especially after the emergence of Vision Transformer. This has motivated thedevelopment of parameter-efficient tuning methods, such as learning adapterlayers or visual prompt tokens, which allow a tiny portion of model parametersto be trained whereas the vast majority obtained from pre-training are frozen.However, designing a proper tuning method is non-trivial: one might need to tryout a lengthy list of design choices, not to mention that each downstreamdataset often requires custom designs. In this paper, we view the existingparameter-efficient tuning methods as \"prompt modules\" and propose NeuralprOmpt seArcH (NOAH), a novel approach that learns, for large vision models,the optimal design of prompt modules through a neural architecture searchalgorithm, specifically for each downstream dataset. By conducting extensiveexperiments on over 20 vision datasets, we demonstrate that NOAH (i) issuperior to individual prompt modules, (ii) has a good few-shot learningability, and (iii) is domain-generalizable. The code and models are availableat https://github.com/Davidzhangyuanhan/NOAH.", "title": "neural prompt search", "url": "http://arxiv.org/pdf/2206.04673v2.pdf", "tokenized_text": "size vision exponentially years especially emergence vision_transformer vision transformer motivated thedevelopment parameter efficient tuning methods learning adapterlayers visual tokens allow tiny portion trained vast majority obtained pre training frozen designing proper tuning method non trivial need lengthy list design choices mention requires custom designs paper view efficient tuning methods modules propose search novel_approach novel approach learns large vision optimal design modules neural architecture specifically downstream dataset conducting extensiveexperiments 20 vision datasets demonstrate individual modules ii good shot learningability iii domain generalizable code availableat"}
{"id": "nan", "abstract": "  Humans can leverage prior experience and learn novel tasks from a handful ofdemonstrations. In contrast to offline meta-reinforcement learning, which aimsto achieve quick adaptation through better algorithm design, we investigate theeffect of architecture inductive bias on the few-shot learning capability. Wepropose a Prompt-based Decision Transformer (Prompt-DT), which leverages thesequential modeling ability of the Transformer architecture and the promptframework to achieve few-shot adaptation in offline RL. We design thetrajectory prompt, which contains segments of the few-shot demonstrations, andencodes task-specific information to guide policy generation. Our experimentsin five MuJoCo control benchmarks show that Prompt-DT is a strong few-shotlearner without any extra finetuning on unseen target tasks. Prompt-DToutperforms its variants and strong meta offline RL baselines by a large marginwith a trajectory prompt containing only a few timesteps. Prompt-DT is alsorobust to prompt length changes and can generalize to out-of-distribution (OOD)environments.", "title": "prompting decision transformer for fewshot policy generalization", "url": "http://arxiv.org/pdf/2206.13499v1.pdf", "tokenized_text": "humans leverage prior experience learn novel tasks handful ofdemonstrations contrast offline meta reinforcement_learning reinforcement learning aimsto achieve quick adaptation better algorithm design investigate architecture inductive bias shot_learning shot learning capability wepropose based decision_transformer decision transformer leverages modeling ability transformer architecture promptframework achieve shot adaptation offline rl design contains segments shot demonstrations task specific information guide policy generation mujoco control benchmarks strong extra finetuning unseen target tasks variants strong meta offline rl baselines large trajectory containing length changes generalize distribution"}
{"id": "nan", "abstract": "  Very large language models (LLMs), such as GPT-3 and Codex have achievedstate-of-the-art performance on several natural-language tasks, and show greatpromise also for code. A particularly exciting aspect of LLMs is their knackfor few-shot and zero-shot learning: they can learn to perform a task with veryfew examples. Few-shotting has particular synergies in software engineering,where there are a lot of phenomena (identifier names, APIs, terminology, codingpatterns) that are known to be highly project-specific. However,project-specific data can be quite limited, especially early in the history ofa project; thus the few-shot learning capacity of LLMs might be very relevant.In this paper, we investigate the use few-shot training with the very large GPT(Generative Pre-trained Transformer) Codex model, and find evidence suggestingthat one can significantly surpass state-of-the-art models forcode-summarization, leveraging project-specific training.", "title": "fewshot training llms for projectspecific codesummarization", "url": "http://arxiv.org/pdf/2207.04237v2.pdf", "tokenized_text": "large_language large language llms gpt-3 codex art performance natural language tasks greatpromise code particularly exciting aspect llms shot zero shot_learning shot learning learn perform task examples particular synergies software engineering lot phenomena names apis terminology known highly project specific project specific data limited especially early history ofa project shot_learning shot learning capacity llms relevant paper investigate use shot training large pre trained transformer codex find evidence suggestingthat significantly surpass state art summarization leveraging project specific training"}
{"id": "nan", "abstract": "  The pretrain-then-finetune paradigm has been widely adopted in computervision. But as the size of Vision Transformer (ViT) grows exponentially, thefull finetuning becomes prohibitive in view of the heavier storage overhead.Motivated by parameter-efficient transfer learning (PETL) on languagetransformers, recent studies attempt to insert lightweight adaptation modules(e.g., adapter layers or prompt tokens) to pretrained ViT and only finetunethese modules while the pretrained weights are frozen. However, these moduleswere originally proposed to finetune language models and did not take intoaccount the prior knowledge specifically for visual tasks. In this paper, wepropose to construct Convolutional Bypasses (Convpass) in ViT as adaptationmodules, introducing only a small amount (less than 0.5% of model parameters)of trainable parameters to adapt the large ViT. Different from other PETLmethods, Convpass benefits from the hard-coded inductive bias of convolutionallayers and thus is more suitable for visual tasks, especially in the low-dataregime. Experimental results on VTAB-1K benchmark and few-shot learningdatasets show that Convpass outperforms current language-oriented adaptationmodules, demonstrating the necessity to tailor vision-oriented adaptationmodules for adapting vision models.", "title": "convolutional bypasses are better vision transformer adapters", "url": "http://arxiv.org/pdf/2207.07039v3.pdf", "tokenized_text": "pretrain finetune paradigm widely adopted computervision size vision_transformer vision transformer vit grows exponentially thefull finetuning prohibitive view storage overhead motivated parameter efficient transfer learning recent studies attempt insert lightweight adaptation adapter layers tokens pretrained vit modules pretrained weights frozen originally proposed finetune language_models language prior knowledge specifically visual tasks paper wepropose construct vit introducing small 0.5 trainable parameters adapt large vit. different benefits hard coded inductive bias suitable visual tasks especially low experimental_results experimental results benchmark shot outperforms current language oriented demonstrating necessity tailor vision oriented adapting vision"}
{"id": "nan", "abstract": "  Existing few-shot learning (FSL) methods rely on training with a largelabeled dataset, which prevents them from leveraging abundant unlabeled data.From an information-theoretic perspective, we propose an effective unsupervisedFSL method, learning representations with self-supervision. Following theInfoMax principle, our method learns comprehensive representations by capturingthe intrinsic structure of the data. Specifically, we maximize the mutualinformation (MI) of instances and their representations with a low-bias MIestimator to perform self-supervised pre-training. Rather than supervisedpre-training focusing on the discriminable features of the seen classes, ourself-supervised model has less bias toward the seen classes, resulting inbetter generalization for unseen classes. We explain that supervisedpre-training and self-supervised pre-training are actually maximizing differentMI objectives. Extensive experiments are further conducted to analyze their FSLperformance with various training settings. Surprisingly, the results show thatself-supervised pre-training can outperform supervised pre-training under theappropriate conditions. Compared with state-of-the-art FSL methods, ourapproach achieves comparable performance on widely used FSL benchmarks withoutany labels of the base classes.", "title": "selfsupervision can be a good fewshot learner", "url": "http://arxiv.org/pdf/2207.09176v1.pdf", "tokenized_text": "existing shot_learning shot learning fsl methods rely training dataset prevents leveraging abundant unlabeled data information theoretic perspective propose effective method learning representations self supervision following principle method learns comprehensive representations intrinsic structure data specifically maximize instances representations low bias perform self supervised pre training training focusing features seen classes supervised bias seen classes resulting generalization unseen classes explain training self supervised pre training actually maximizing objectives extensive_experiments extensive experiments conducted analyze training settings surprisingly results supervised pre training outperform supervised pre training theappropriate conditions compared state art fsl methods ourapproach achieves comparable performance widely fsl benchmarks withoutany labels base classes"}
{"id": "nan", "abstract": "  Prompted models have demonstrated impressive few-shot learning abilities.Repeated interactions at test-time with a single model, or the composition ofmultiple models together, further expands capabilities. These compositions areprobabilistic models, and may be expressed in the language of graphical modelswith random variables whose values are complex data types such as strings.Cases with control flow and dynamic structure require techniques fromprobabilistic programming, which allow implementing disparate model structuresand inference strategies in a unified language. We formalize several existingtechniques from this perspective, including scratchpads / chain of thought,verifiers, STaR, selection-inference, and tool use. We refer to the resultingprograms as language model cascades.", "title": "language model cascades", "url": "http://arxiv.org/pdf/2207.10342v2.pdf", "tokenized_text": "prompted demonstrated impressive shot_learning shot learning abilities repeated interactions test time single composition ofmultiple expands capabilities compositions expressed language graphical modelswith random variables values complex data types strings cases control flow dynamic structure require techniques programming allow implementing disparate inference strategies unified language formalize perspective including scratchpads chain_of_thought chain thought verifiers star selection inference tool use refer language_model language cascades"}
{"id": "nan", "abstract": "  Prior work on language models (LMs) shows that training on a large number ofdiverse tasks improves few-shot learning (FSL) performance on new tasks. Wetake this to the extreme, automatically extracting 413,299 tasks from internettables - orders of magnitude more than the next-largest public datasets.Finetuning on the resulting dataset leads to improved FSL performance onNatural Language Processing (NLP) tasks, but not proportionally to datasetscale. In fact, we find that narrow subsets of our dataset sometimes outperformmore diverse datasets. For example, finetuning on software documentation fromsupport.google.com raises FSL performance by a mean of +7.5% on 52 downstreamtasks, which beats training on 40 human-curated NLP datasets (+6.7%).Finetuning on various narrow datasets leads to similar broad improvementsacross test tasks, suggesting that the gains are not from domain adaptation butadapting to FSL in general. We do not observe clear patterns between thedatasets that lead to FSL gains, leaving open questions about why certain datahelps with FSL.", "title": "fewshot adaptation works with unpredictable data", "url": "http://arxiv.org/pdf/2208.01009v2.pdf", "tokenized_text": "prior_work prior work language_models language lms shows training large number ofdiverse tasks improves shot_learning shot learning fsl performance new tasks extreme automatically extracting tasks orders magnitude largest public datasets finetuning resulting dataset leads improved fsl performance onnatural language_processing language processing nlp tasks fact find narrow subsets dataset diverse datasets example finetuning software documentation raises fsl performance mean 52 downstreamtasks beats training 40 human curated nlp datasets narrow datasets leads similar broad test tasks suggesting gains domain adaptation fsl general observe clear patterns lead fsl gains leaving open questions certain fsl"}
{"id": "nan", "abstract": "  Interestingness recognition is crucial for decision making in autonomousexploration for mobile robots. Previous methods proposed an unsupervised onlinelearning approach that can adapt to environments and detect interesting scenesquickly, but lack the ability to adapt to human-informed interesting objects.To solve this problem, we introduce a human-interactive framework,AirInteraction, that can detect human-informed objects via few-shot onlinelearning. To reduce the communication bandwidth, we first apply an onlineunsupervised learning algorithm on the unmanned vehicle for interestingnessrecognition and then only send the potential interesting scenes to abase-station for human inspection. The human operator is able to draw andprovide bounding box annotations for particular interesting objects, which aresent back to the robot to detect similar objects via few-shot learning. Onlyusing few human-labeled examples, the robot can learn novel interesting objectcategories during the mission and detect interesting scenes that contain theobjects. We evaluate our method on various interesting scene recognitiondatasets. To the best of our knowledge, it is the first human-informed few-shotobject detection framework for autonomous exploration.", "title": "robotic interestingness via humaninformed fewshot object detection", "url": "http://arxiv.org/pdf/2208.01084v1.pdf", "tokenized_text": "interestingness recognition crucial decision_making decision making mobile robots previous methods proposed unsupervised approach adapt environments detect interesting lack ability adapt human informed interesting objects solve problem introduce human interactive framework detect human informed objects shot reduce communication bandwidth apply learning algorithm unmanned vehicle send potential interesting scenes station human inspection human operator able draw andprovide bounding box annotations particular interesting objects robot detect similar objects shot_learning shot learning onlyusing human labeled examples robot learn novel interesting objectcategories mission detect interesting scenes contain evaluate method interesting scene best knowledge human informed detection framework autonomous exploration"}
{"id": "nan", "abstract": "  Large language models have shown impressive few-shot results on a wide rangeof tasks. However, when knowledge is key for such results, as is the case fortasks such as question answering and fact checking, massive parameter counts tostore knowledge seem to be needed. Retrieval augmented models are known toexcel at knowledge intensive tasks without the need for as many parameters, butit is unclear whether they work in few-shot settings. In this work we presentAtlas, a carefully designed and pre-trained retrieval augmented language modelable to learn knowledge intensive tasks with very few training examples. Weperform evaluations on a wide range of tasks, including MMLU, KILT andNaturalQuestions, and study the impact of the content of the document index,showing that it can easily be updated. Notably, Atlas reaches over 42% accuracyon Natural Questions using only 64 examples, outperforming a 540B parametersmodel by 3% despite having 50x fewer parameters.", "title": "atlas fewshot learning with retrieval augmented language models", "url": "http://arxiv.org/pdf/2208.03299v3.pdf", "tokenized_text": "large_language large language shown_impressive shown impressive shot results wide rangeof tasks knowledge key results case question_answering question answering fact_checking fact checking massive parameter counts knowledge needed retrieval_augmented retrieval augmented known toexcel knowledge_intensive knowledge intensive tasks need parameters butit unclear work shot_settings shot settings work carefully designed pre trained retrieval_augmented retrieval augmented language learn knowledge_intensive knowledge intensive tasks training_examples training examples weperform evaluations wide_range wide range tasks including mmlu kilt study impact content document index showing easily updated notably atlas reaches natural questions 64 examples outperforming 540b despite having fewer parameters"}
{"id": "nan", "abstract": "  Drori et al. (2022) report that \"A neural network solves, explains, andgenerates university math problems by program synthesis and few-shot learningat human level ... [It] automatically answers 81\\% of university-levelmathematics problems.\" The system they describe is indeed impressive; however,the above description is very much overstated. The work of solving the problemsis done, not by a neural network, but by the symbolic algebra package Sympy.Problems of various formats are excluded from consideration. The so-called\"explanations\" are just rewordings of lines of code. Answers are marked ascorrect that are not in the form specified in the problem. Most seriously, itseems that in many cases the system uses the correct answer given in the testcorpus to guide its path to solving the problem.", "title": "limits of an ai program for solving college math problems", "url": "http://arxiv.org/pdf/2208.06906v1.pdf", "tokenized_text": "et_al et al 2022 report neural network solves explains university math problems program synthesis shot human level automatically answers university problems system describe impressive description work solving neural network symbolic algebra package problems formats excluded consideration lines code answers marked form specified problem cases system uses correct answer given guide path solving problem"}
{"id": "nan", "abstract": "  Recent few-shot methods, such as parameter-efficient fine-tuning (PEFT) andpattern exploiting training (PET), have achieved impressive results inlabel-scarce settings. However, they are difficult to employ since they aresubject to high variability from manually crafted prompts, and typicallyrequire billion-parameter language models to achieve high accuracy. To addressthese shortcomings, we propose SetFit (Sentence Transformer Fine-tuning), anefficient and prompt-free framework for few-shot fine-tuning of SentenceTransformers (ST). SetFit works by first fine-tuning a pretrained ST on a smallnumber of text pairs, in a contrastive Siamese manner. The resulting model isthen used to generate rich text embeddings, which are used to train aclassification head. This simple framework requires no prompts or verbalizers,and achieves high accuracy with orders of magnitude less parameters thanexisting techniques. Our experiments show that SetFit obtains comparableresults with PEFT and PET techniques, while being an order of magnitude fasterto train. We also show that SetFit can be applied in multilingual settings bysimply switching the ST body. Our code is available athttps://github.com/huggingface/setfit and our datasets athttps://huggingface.co/setfit .", "title": "efficient fewshot learning without prompts", "url": "http://arxiv.org/pdf/2209.11055v1.pdf", "tokenized_text": "recent shot methods parameter efficient fine tuning peft exploiting training pet achieved impressive results scarce settings difficult employ high variability manually crafted billion parameter language_models language achieve high accuracy addressthese shortcomings propose setfit sentence transformer fine tuning anefficient free framework shot fine tuning st setfit works fine tuning pretrained st smallnumber text pairs contrastive siamese manner resulting isthen generate rich text embeddings train head simple framework requires verbalizers achieves high accuracy orders magnitude parameters thanexisting techniques experiments setfit obtains peft pet techniques order magnitude train setfit applied multilingual settings switching st body code_is_available code available datasets"}
{"id": "nan", "abstract": "  Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbedinputs during training -- helps reduce model reliance on spurious correlationsand improves generalization to out-of-distribution (OOD) data. Prior work ongenerating counterfactuals only considered restricted classes of perturbations,limiting their effectiveness. We present COunterfactual Generation viaRetrieval and Editing (CORE), a retrieval-augmented generation framework forcreating diverse counterfactual perturbations for CDA. For each trainingexample, CORE first performs a dense retrieval over a task-related unlabeledtext corpus using a learned bi-encoder and extracts relevant counterfactualexcerpts. CORE then incorporates these into prompts to a large language modelwith few-shot learning capabilities, for counterfactual editing. Conditioninglanguage model edits on naturally occurring data results in diverseperturbations. Experiments on natural language inference and sentiment analysisbenchmarks show that CORE counterfactuals are more effective at improvinggeneralization to OOD data compared to other DA approaches. We also show thatthe CORE retrieval framework can be used to encourage diversity in manuallyauthored perturbations", "title": "core a retrievethenedit framework for counterfactual data generation", "url": "http://arxiv.org/pdf/2210.04873v2.pdf", "tokenized_text": "counterfactual data_augmentation data augmentation i.e. adding minimally training helps reduce reliance spurious improves generalization distribution ood data prior_work prior work counterfactuals considered restricted classes perturbations limiting effectiveness present counterfactual generation editing core retrieval augmented generation framework diverse counterfactual perturbations trainingexample core performs dense retrieval task related corpus learned bi encoder extracts relevant core incorporates large_language large language modelwith shot_learning shot learning capabilities counterfactual editing edits naturally occurring data results experiments natural_language natural language inference sentiment core counterfactuals effective ood data compared da approaches thatthe core retrieval framework encourage diversity perturbations"}
{"id": "nan", "abstract": "  Recent work on applying large language models (LMs) achieves impressiveperformance in many NLP applications. Adapting or posttraining an LM using anunlabeled domain corpus can produce even better performance for end-tasks inthe domain. This paper proposes the problem of continually extending an LM byincrementally post-train the LM with a sequence of unlabeled domain corpora toexpand its knowledge without forgetting its previous skills. The goal is toimprove the few-shot end-task learning in these domains. The resulting systemis called CPT (Continual PostTraining), which to our knowledge, is the firstcontinual post-training system. Experimental results verify its effectiveness.", "title": "continual training of language models for fewshot learning", "url": "http://arxiv.org/pdf/2210.05549v1.pdf", "tokenized_text": "recent_work recent work applying large_language large language lms achieves impressiveperformance nlp applications adapting lm domain corpus produce better performance end tasks inthe domain paper_proposes paper proposes problem continually extending lm post train lm sequence unlabeled domain corpora knowledge forgetting previous skills goal toimprove shot end task learning domains resulting called cpt continual knowledge post training system experimental_results experimental results verify effectiveness"}
{"id": "nan", "abstract": "  Knowledge (including structured knowledge such as schema and ontology, andunstructured knowledge such as web corpus) is a critical part of dialogunderstanding, especially for unseen tasks and domains. Traditionally, suchdomain-specific knowledge is encoded implicitly into model parameters for theexecution of downstream tasks, which makes training inefficient. In addition,such models are not easily transferable to new tasks with different schemas. Inthis work, we propose to perform dialog state tracking grounded on knowledgeencoded externally. We query relevant knowledge of various forms based on thedialog context where such information can ground the prediction of dialogstates. We demonstrate superior performance of our proposed method over strongbaselines, especially in the few-shot learning setting.", "title": "knowledgegrounded dialog state tracking", "url": "http://arxiv.org/pdf/2210.06656v1.pdf", "tokenized_text": "knowledge including structured knowledge schema ontology knowledge web corpus critical especially unseen tasks domains traditionally specific knowledge encoded implicitly parameters downstream_tasks downstream tasks makes training inefficient addition easily transferable new tasks different schemas inthis work propose perform dialog state tracking grounded externally query relevant knowledge forms based context information ground prediction demonstrate superior_performance superior performance proposed_method proposed method especially shot_learning shot learning setting"}
{"id": "nan", "abstract": "  This paper surveys vision-language pre-training (VLP) methods for multimodalintelligence that have been developed in the last few years. We group theseapproaches into three categories: ($i$) VLP for image-text tasks, such as imagecaptioning, image-text retrieval, visual question answering, and visualgrounding; ($ii$) VLP for core computer vision tasks, such as (open-set) imageclassification, object detection, and segmentation; and ($iii$) VLP forvideo-text tasks, such as video captioning, video-text retrieval, and videoquestion answering. For each category, we present a comprehensive review ofstate-of-the-art methods, and discuss the progress that has been made andchallenges still being faced, using specific systems and models as casestudies. In addition, for each category, we discuss advanced topics beingactively explored in the research community, such as big foundation models,unified modeling, in-context few-shot learning, knowledge, robustness, andcomputer vision in the wild, to name a few.", "title": "visionlanguage pretraining basics, recent advances, and future trends", "url": "http://arxiv.org/pdf/2210.09263v1.pdf", "tokenized_text": "paper surveys vision language pre training vlp methods developed years group categories vlp image text tasks image text retrieval visual question_answering question answering vlp core computer_vision computer vision tasks open set imageclassification object_detection object detection segmentation vlp text tasks video captioning video text retrieval answering category present comprehensive review ofstate art methods discuss progress andchallenges faced specific systems addition category discuss advanced topics beingactively explored research community big foundation_models foundation unified modeling context shot_learning shot learning knowledge robustness vision wild"}
{"id": "nan", "abstract": "  Few-shot relation extraction aims to learn to identify the relation betweentwo entities based on very limited training examples. Recent efforts found thattextual labels (i.e., relation names and relation descriptions) could beextremely useful for learning class representations, which will benefit thefew-shot learning task. However, what is the best way to leverage such labelinformation in the learning process is an important research question. Existingworks largely assume such textual labels are always present during bothlearning and prediction. In this work, we argue that such approaches may notalways lead to optimal results. Instead, we present a novel approach calledlabel prompt dropout, which randomly removes label descriptions in the learningprocess. Our experiments show that our approach is able to lead to improvedclass representations, yielding significantly better results on the few-shotrelation extraction task.", "title": "better fewshot relation extraction with label prompt dropout", "url": "http://arxiv.org/pdf/2210.13733v1.pdf", "tokenized_text": "shot relation_extraction relation extraction aims learn identify relation betweentwo entities based limited training_examples training examples recent efforts found labels i.e. relation names relation descriptions useful learning class representations benefit thefew shot_learning shot learning task best way leverage learning process important research question largely assume textual labels present prediction work argue approaches lead optimal results instead present novel_approach novel approach dropout randomly label descriptions experiments approach able lead representations yielding significantly better results extraction task"}
{"id": "nan", "abstract": "  The effectiveness of prompt learning has been demonstrated in differentpre-trained language models. By formulating suitable template and choosingrepresentative label mapping, prompt learning can be used as an efficientknowledge probe. However, finding suitable prompt in existing methods requiresmultiple experimental attempts or appropriate vector initialization onformulating suitable template and choosing representative label mapping, whichit is more common in few-shot learning tasks. Motivating by PLM workingprocess, we try to construct the prompt from task semantic perspective and thuspropose the STPrompt -Semantic-guided and Task-driven Prompt model.Specifically, two novel prompts generated from the semantic dependency tree(Dep-prompt) and task-specific metadata description (Meta-prompt), are firstlyconstructed in a prompt augmented pool, and the proposed model wouldautomatically select a suitable semantic prompt to motivating the promptlearning process. Our results show that the proposed model achieves thestate-of-the-art performance in five different datasets of few-shot textclassification tasks, which prove that more semantic and significant promptscould assume as a better knowledge proving tool.", "title": "stprompt semanticguided and taskdriven prompts for effective fewshot classification", "url": "http://arxiv.org/pdf/2210.16489v1.pdf", "tokenized_text": "effectiveness learning demonstrated differentpre trained_language trained language formulating suitable template label mapping learning probe finding suitable existing_methods existing methods experimental attempts appropriate vector initialization suitable template choosing representative label mapping common shot_learning shot learning tasks motivating plm try construct task semantic perspective guided task driven specifically novel generated semantic dependency task specific metadata description meta augmented pool proposed select suitable semantic motivating promptlearning process results proposed achieves thestate art performance different datasets shot textclassification tasks prove semantic significant assume better knowledge proving tool"}
{"id": "nan", "abstract": "  Event argument extraction has long been studied as a sequential predictionproblem with extractive-based methods, tackling each argument in isolation.Although recent work proposes generation-based methods to capturecross-argument dependency, they require generating and post-processing acomplicated target sequence (template). Motivated by these observations andrecent pretrained language models' capabilities of learning fromdemonstrations. We propose a retrieval-augmented generative QA model (R-GQA)for event argument extraction. It retrieves the most similar QA pair andaugments it as prompt to the current example's context, then decodes thearguments as answers. Our approach outperforms substantially prior methodsacross various settings (i.e. fully supervised, domain transfer, and fewshotlearning). Finally, we propose a clustering-based sampling strategy (JointEnc)and conduct a thorough analysis of how different strategies influence thefew-shot learning performance. The implementations are available at https://github.com/xinyadu/RGQA", "title": "retrievalaugmented generative question answering for event argument extraction", "url": "http://arxiv.org/pdf/2211.07067v1.pdf", "tokenized_text": "event argument extraction long studied sequential extractive based methods tackling argument isolation recent_work recent work proposes generation based methods argument dependency require generating post processing target sequence template motivated observations pretrained_language pretrained language capabilities learning fromdemonstrations propose retrieval augmented generative qa event argument extraction retrieves similar qa pair current example context decodes answers approach outperforms substantially prior settings i.e. fully_supervised fully supervised domain transfer finally propose clustering based sampling strategy conduct thorough analysis different strategies influence thefew shot_learning shot learning performance implementations available"}
{"id": "nan", "abstract": "  Subjective answer evaluation is a time-consuming and tedious task, and thequality of the evaluation is heavily influenced by a variety of subjectivepersonal characteristics. Instead, machine evaluation can effectively assisteducators in saving time while also ensuring that evaluations are fair andrealistic. However, most existing methods using regular machine learning andnatural language processing techniques are generally hampered by a lack ofannotated answers and poor model interpretability, making them unsuitable forreal-world use. To solve these challenges, we propose ProtSi Network, a uniquesemi-supervised architecture that for the first time uses few-shot learning tosubjective answer evaluation. To evaluate students' answers by similarityprototypes, ProtSi Network simulates the natural process of evaluator scoringanswers by combining Siamese Network which consists of BERT and encoder layerswith Prototypical Network. We employed an unsupervised diverse paraphrasingmodel ProtAugment, in order to prevent overfitting for effective few-shot textclassification. By integrating contrastive learning, the discriminative textissue can be mitigated. Experiments on the Kaggle Short Scoring Datasetdemonstrate that the ProtSi Network outperforms the most recent baseline modelsin terms of accuracy and quadratic weighted kappa.", "title": "protsi prototypical siamese network with data augmentation for fewshot subjective answer evaluation", "url": "http://arxiv.org/pdf/2211.09855v1.pdf", "tokenized_text": "subjective answer evaluation time consuming tedious task thequality evaluation heavily influenced variety characteristics instead machine evaluation effectively saving time ensuring evaluations fair existing_methods existing methods regular machine_learning machine learning language_processing language processing techniques generally hampered lack answers poor interpretability making unsuitable world use solve challenges propose network supervised architecture time uses shot_learning shot learning answer evaluation evaluate students answers network simulates natural process evaluator combining siamese network consists bert encoder prototypical network employed unsupervised diverse order prevent overfitting effective shot textclassification integrating contrastive_learning contrastive learning discriminative mitigated experiments short scoring network outperforms recent baseline modelsin terms accuracy quadratic weighted kappa"}
{"id": "nan", "abstract": "  Careful prompt design is critical to the use of large language models inzero-shot or few-shot learning. As a consequence, there is a growing interestin automated methods to design optimal prompts. In this work, we proposeTest-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast toprior prompt generation methods, TEMPERA can efficiently leverage priorknowledge, is adaptive to different queries and provides an interpretableprompt for every query. To achieve this, we design a novel action space thatallows flexible editing of the initial prompts covering a wide set ofcommonly-used components like instructions, few-shot exemplars, andverbalizers. The proposed method achieves significant gains compared withrecent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across avariety of tasks including sentiment analysis, topic classification, naturallanguage inference, and reading comprehension. Our method achieves 5.33x onaverage improvement in sample efficiency when compared to the traditionalfine-tuning methods.", "title": "tempera testtime prompting via reinforcement learning", "url": "http://arxiv.org/pdf/2211.11890v1.pdf", "tokenized_text": "careful design critical use large_language large language inzero shot shot_learning shot learning consequence growing automated methods design optimal work time editing reinforcement_learning reinforcement learning contrast generation methods efficiently leverage priorknowledge adaptive different queries provides query achieve design novel action space thatallows flexible editing initial covering wide set components like instructions shot exemplars proposed_method proposed method achieves significant gains compared sota approaches like tuning rlprompt avariety tasks including sentiment_analysis sentiment analysis topic classification naturallanguage inference reading comprehension method_achieves method achieves onaverage improvement sample efficiency compared tuning methods"}
{"id": "nan", "abstract": "  While Named Entity Recognition (NER) is a widely studied task, makinginferences of entities with only a few labeled data has been challenging,especially for entities with nested structures. Unlike flat entities, entitiesand their nested entities are more likely to have similar semantic featurerepresentations, drastically increasing difficulties in classifying differententity categories in the few-shot setting. Although prior work has brieflydiscussed nested structures in the context of few-shot learning, to our bestknowledge, this paper is the first one specifically dedicated to studying thefew-shot nested NER task. Leveraging contextual dependency to distinguishnested entities, we propose a Biaffine-based Contrastive Learning (BCL)framework. We first design a Biaffine span representation module for learningthe contextual span dependency representation for each entity span rather thanonly learning its semantic representation. We then merge these tworepresentations by the residual connection to distinguish nested entities.Finally, we build a contrastive learning framework to adjust the representationdistribution for larger margin boundaries and more generalized domain transferlearning ability. We conducted experimental studies on three English, German,and Russian nested NER datasets. The results show that the BCL outperformedthree baseline models on the 1-shot and 5-shot tasks in terms of F1 score.", "title": "fewshot nested named entity recognition", "url": "http://arxiv.org/pdf/2212.00953v1.pdf", "tokenized_text": "named_entity named entity recognition ner widely studied task entities labeled_data labeled data challenging especially entities nested structures unlike flat entities nested entities likely similar semantic drastically increasing difficulties classifying categories shot_setting shot setting prior_work prior work nested structures context shot_learning shot learning paper specifically dedicated studying thefew shot nested ner task leveraging contextual dependency entities propose based contrastive_learning contrastive learning design span representation module contextual span dependency representation entity span learning semantic representation merge residual connection distinguish nested entities finally build contrastive_learning contrastive learning framework adjust larger margin boundaries generalized domain transferlearning ability conducted experimental studies english german russian nested ner datasets results baseline shot shot tasks terms f1_score f1 score"}
{"id": "nan", "abstract": "  Pre-trained language models (PLMs) have exhibited remarkable few-shotlearning capabilities when provided a few examples in a natural language promptas demonstrations of test instances, i.e., in-context learning. However, theperformance of in-context learning is susceptible to the choice of promptformat, training examples and the ordering of the training examples. In thispaper, we propose a novel nearest-neighbor calibration framework for in-contextlearning to ease this issue. It is inspired by a phenomenon that the in-contextlearning paradigm produces incorrect labels when inferring training instances,which provides a useful supervised signal to calibrate predictions. Thus, ourmethod directly augments the predictions with a $k$-nearest-neighbor ($k$NN)classifier over a datastore of cached few-shot instance representationsobtained by PLMs and their corresponding labels. Then adaptive neighborselection and feature regularization modules are introduced to make full use ofa few support instances to reduce the $k$NN retrieval noise. Experiments onvarious few-shot text classification tasks demonstrate that our methodsignificantly improves in-context learning, while even achieving comparableperformance with state-of-the-art tuning-based approaches in some sentimentanalysis tasks.", "title": "improving fewshot performance of language models via nearest neighbor calibration", "url": "http://arxiv.org/pdf/2212.02216v1.pdf", "tokenized_text": "pre trained_language trained language plms exhibited remarkable shotlearning capabilities provided examples natural_language natural language demonstrations test instances i.e. context_learning context learning theperformance context_learning context learning susceptible choice training_examples training examples ordering training_examples training examples thispaper propose_a_novel propose novel nearest neighbor calibration framework contextlearning ease issue inspired phenomenon contextlearning paradigm produces incorrect labels inferring training instances provides useful supervised signal calibrate predictions ourmethod directly augments predictions neighbor datastore cached shot instance plms corresponding labels adaptive feature regularization modules introduced use ofa support instances reduce k$nn retrieval noise experiments onvarious shot text_classification text classification tasks demonstrate improves context_learning context learning achieving comparableperformance state art tuning based approaches sentimentanalysis tasks"}
{"id": "nan", "abstract": "  JamPatoisNLI provides the first dataset for natural language inference in acreole language, Jamaican Patois. Many of the most-spoken low-resourcelanguages are creoles. These languages commonly have a lexicon derived from amajor world language and a distinctive grammar reflecting the languages of theoriginal speakers and the process of language birth by creolization. This givesthem a distinctive place in exploring the effectiveness of transfer from largemonolingual or multilingual pretrained models. While our work, along withprevious work, shows that transfer from these models to low-resource languagesthat are unrelated to languages in their training set is not very effective, wewould expect stronger results from transfer to creoles. Indeed, our experimentsshow considerably better results from few-shot learning of JamPatoisNLI thanfor such unrelated languages, and help us begin to understand how the uniquerelationship between creoles and their high-resource base languages affectcross-lingual transfer. JamPatoisNLI, which consists of naturally-occurringpremises and expert-written hypotheses, is a step towards steering researchinto a traditionally underserved language and a useful benchmark forunderstanding cross-lingual NLP.", "title": "jampatoisnli a jamaican patois natural language inference dataset", "url": "http://arxiv.org/pdf/2212.03419v1.pdf", "tokenized_text": "provides dataset natural_language natural language inference language spoken low resourcelanguages languages commonly lexicon derived world language grammar reflecting languages theoriginal speakers process language place exploring effectiveness transfer multilingual pretrained work withprevious work shows transfer low resource unrelated languages training set effective expect stronger results transfer experimentsshow considerably better results shot_learning shot learning unrelated languages help begin understand high resource base languages lingual_transfer lingual transfer consists naturally expert written hypotheses step steering traditionally language useful benchmark forunderstanding cross lingual nlp"}
{"id": "nan", "abstract": "  Interactive data exploration (IDE) is an effective way of comprehending bigdata, whose volume and complexity are beyond human abilities. The main goal ofIDE is to discover user interest regions from a database through multi-roundsof user labelling. Existing IDEs adopt active-learning framework, where usersiteratively discriminate or label the interestingness of selected tuples. Theprocess of data exploration can be viewed as the process of training aclassifier, which determines whether a database tuple is interesting to a user.An efficient exploration thus takes very few iterations of user labelling toreach the data region of interest. In this work, we consider the dataexploration as the process of few-shot learning, where the classifier islearned with only a few training examples, or exploration iterations. To thisend, we propose a learning-to-explore framework, based on meta-learning, whichlearns how to learn a classifier with automatically generated meta-tasks, sothat the exploration process can be much shortened. Extensive experiments onreal datasets show that our proposal outperforms existing explore-by-examplesolutions in terms of accuracy and efficiency.", "title": "learn to explore on bootstrapping interactive data exploration with metalearning", "url": "http://arxiv.org/pdf/2212.03423v4.pdf", "tokenized_text": "interactive data exploration effective way comprehending volume complexity human abilities main goal discover user interest regions database multi user labelling existing ides adopt active learning framework discriminate label interestingness selected theprocess data exploration viewed process training aclassifier determines database tuple interesting user efficient exploration takes iterations user labelling toreach data region interest work consider dataexploration process shot_learning shot learning classifier training_examples training examples exploration iterations thisend propose learning explore framework based meta learning learn classifier automatically generated meta tasks exploration process extensive_experiments extensive experiments datasets proposal outperforms existing explore terms accuracy efficiency"}
{"id": "nan", "abstract": "  Language models can be prompted to perform a wide variety of zero- andfew-shot learning problems. However, performance varies significantly with thechoice of prompt, and we do not yet understand why this happens or how to pickthe best prompts. In this work, we analyze the factors that contribute to thisvariance and establish a new empirical hypothesis: the performance of a promptis coupled with the extent to which the model is familiar with the language itcontains. Over a wide range of tasks, we show that the lower the perplexity ofthe prompt is, the better the prompt is able to perform the task. As a result,we devise a method for creating prompts: (1) automatically extend a small seedset of manually written prompts by paraphrasing using GPT3 and backtranslationand (2) choose the lowest perplexity prompts to get significant gains inperformance.", "title": "demystifying prompts in language models via perplexity estimation", "url": "http://arxiv.org/pdf/2212.04037v1.pdf", "tokenized_text": "language_models language prompted perform wide variety zero- andfew shot_learning shot learning problems performance varies significantly thechoice understand happens best work analyze factors contribute establish new empirical hypothesis performance coupled extent familiar language itcontains wide_range wide range tasks lower perplexity ofthe better able perform task result devise method creating automatically extend small manually written paraphrasing gpt3 choose lowest perplexity significant gains inperformance"}
{"id": "nan", "abstract": "  Although massive pre-trained vision-language models like CLIP show impressivegeneralization capabilities for many tasks, still it often remains necessary tofine-tune them for improved performance on specific datasets. When doing so, itis desirable that updating the model is fast and that the model does not loseits capabilities on data outside of the dataset, as is often the case withclassical fine-tuning approaches. In this work we suggest a lightweightadapter, that only updates the models predictions close to seen datapoints. Wedemonstrate the effectiveness and speed of this relatively simple approach inthe context of few-shot learning, where our results both on classes seen andunseen during training are comparable with or improve on the state of the art.", "title": "localized latent updates for finetuning visionlanguage models", "url": "http://arxiv.org/pdf/2212.06556v1.pdf", "tokenized_text": "massive pre trained vision language_models language like clip capabilities tasks remains necessary tofine tune improved performance specific datasets itis desirable updating fast capabilities data outside dataset case fine tuning approaches work suggest updates predictions close seen wedemonstrate effectiveness speed relatively simple approach inthe context shot_learning shot learning results classes seen training comparable improve state_of_the_art state art"}
{"id": "nan", "abstract": "  Current large language models can perform reasonably well on complex tasksthat require step-by-step reasoning with few-shot learning. Are these modelsapplying reasoning skills they have learnt during pre-training and reasonoutside of their training context, or are they simply memorizing their trainingcorpus at finer granularity and have learnt to better understand their context?To tease apart these possibilities, we introduce ALERT, a benchmark and suiteof analyses for assessing language models' reasoning ability comparingpre-trained and finetuned models on complex tasks that require reasoning skillsto solve. ALERT provides a test bed to asses any language model on fine-grainedreasoning skills, which spans over 20 datasets and covers 10 differentreasoning skills. We leverage ALERT to further investigate the role offinetuning. With extensive empirical analysis we find that language modelslearn more reasoning skills such as textual entailment, abductive reasoning,and analogical reasoning during finetuning stage compared to pretraining state.We also find that when language models are finetuned they tend to overfit tothe prompt template, which hurts the robustness of models causinggeneralization problems.", "title": "alert adapting language models to reasoning tasks", "url": "http://arxiv.org/pdf/2212.08286v2.pdf", "tokenized_text": "current large_language large language perform reasonably complex tasksthat require step step reasoning shot_learning shot learning reasoning skills pre training training context simply memorizing trainingcorpus finer granularity better understand apart possibilities introduce benchmark analyses assessing language_models language reasoning ability trained finetuned complex tasks require reasoning solve provides test bed language_model language fine skills spans 20 datasets covers 10 skills leverage investigate role extensive empirical analysis find language modelslearn reasoning skills textual entailment abductive reasoning analogical reasoning finetuning stage compared pretraining state find language_models language finetuned tend overfit tothe prompt_template template hurts robustness problems"}
{"id": "nan", "abstract": "  Everyday sound recognition aims to infer types of sound events in audiostreams. While many works succeeded in training models with high performance ina fully-supervised manner, they are still restricted to the demand of largequantities of labelled data and the range of predefined classes. To overcomethese drawbacks, this work firstly curates a new database named FSD-FS formulti-label few-shot audio classification. It then explores how to incorporateaudio taxonomy in few-shot learning. Specifically, this work proposeslabel-dependent prototypical networks (LaD-protonet) to exploit parent-childrenrelationships between labels. Plus, it applies taxonomy-aware label smoothingtechniques to boost model performance. Experiments demonstrate thatLaD-protonet outperforms original prototypical networks as well as otherstate-of-the-art methods. Moreover, its performance can be further boosted whencombined with taxonomy-aware label smoothing.", "title": "learning from taxonomy multilabel fewshot classification for everyday sound recognition", "url": "http://arxiv.org/pdf/2212.08952v1.pdf", "tokenized_text": "everyday sound recognition aims infer types sound events works succeeded training high performance ina fully supervised manner restricted demand labelled data range predefined classes drawbacks work firstly new database named fs label shot audio classification explores taxonomy shot_learning shot learning specifically work dependent prototypical networks exploit parent labels plus applies taxonomy aware label boost performance experiments_demonstrate experiments demonstrate outperforms original prototypical networks art methods performance boosted taxonomy aware label smoothing"}
{"id": "nan", "abstract": "  Knowledge graphs (KG) have served as the key component of various naturallanguage processing applications. Commonsense knowledge graphs (CKG) are aspecial type of KG, where entities and relations are composed of free-formtext. However, previous works in KG completion and CKG completion suffer fromlong-tail relations and newly-added relations which do not have many knowtriples for training. In light of this, few-shot KG completion (FKGC), whichrequires the strengths of graph representation learning and few-shot learning,has been proposed to challenge the problem of limited annotated data. In thispaper, we comprehensively survey previous attempts on such tasks in the form ofa series of methods and applications. Specifically, we first introduce FKGCchallenges, commonly used KGs, and CKGs. Then we systematically categorize andsummarize existing works in terms of the type of KGs and the methods. Finally,we present applications of FKGC models on prediction tasks in different areasand share our thoughts on future research directions of FKGC.", "title": "a survey on fewshot knowledge graph completion with structural and commonsense knowledge", "url": "http://arxiv.org/pdf/2301.01172v1.pdf", "tokenized_text": "knowledge graphs kg served key component naturallanguage processing applications commonsense knowledge graphs type kg entities relations composed free previous works kg completion completion suffer tail relations newly added relations training light shot kg completion whichrequires strengths graph representation learning shot_learning shot learning proposed challenge problem limited annotated_data annotated data thispaper comprehensively survey previous attempts tasks form ofa series methods applications specifically introduce commonly kgs systematically categorize existing works terms type kgs methods finally present applications prediction tasks different share thoughts future_research future research directions"}
{"id": "nan", "abstract": "  Prompt tuning (PT) which only tunes the embeddings of an additional sequenceof tokens per task, keeping the pre-trained language model (PLM) frozen, hasshown remarkable performance in few-shot learning. Despite this, PT has beenshown to rely heavily on good initialization of the prompt embeddings. In thiswork, we study meta prompt tuning (MPT) to systematically explore howmeta-learning can help improve (if it can) cross-task generalization in PTthrough learning to initialize the prompt embeddings from other relevant tasks.We empirically analyze a representative set of meta learning algorithms in awide range of adaptation settings with different source/target taskconfigurations on a large set of few-shot tasks. With extensive experiments andanalysis, we demonstrate the effectiveness of MPT. We find the improvement tobe significant particularly on classification tasks. For other kinds of taskssuch as question answering, we observe that while MPT can outperform PT in mostcases, it does not always outperform multi-task learning. We further provide anin-depth analysis from the perspective of task similarity.", "title": "learning to initialize can meta learning improve crosstask generalization in prompt tuning", "url": "http://arxiv.org/pdf/2302.08143v2.pdf", "tokenized_text": "tuning pt tunes embeddings additional tokens task keeping pre trained_language trained language plm frozen hasshown remarkable performance shot_learning shot learning despite pt beenshown rely heavily good initialization embeddings thiswork study meta tuning mpt systematically explore learning help improve cross task generalization learning embeddings relevant tasks empirically analyze representative set meta learning algorithms awide range adaptation settings different source target large set shot tasks extensive_experiments extensive experiments demonstrate_the_effectiveness demonstrate effectiveness mpt find improvement tobe significant particularly classification tasks kinds taskssuch question_answering question answering observe mpt outperform pt outperform multi task learning provide anin depth analysis perspective task similarity"}
{"id": "nan", "abstract": "  Prompt-based learning methods in semi-supervised learning (SSL) settings havebeen shown to be effective on multiple natural language understanding (NLU)datasets and tasks in the literature. However, manually designing multipleprompts and verbalizers requires domain knowledge and human effort, making itdifficult and expensive to scale across different datasets. In this paper, wepropose two methods to automatically design multiple prompts and integrateautomatic verbalizer in SSL settings without sacrificing performance. The firstmethod uses various demonstration examples with learnable continuous prompttokens to create diverse prompt models. The second method uses a varying numberof soft prompt tokens to encourage language models to learn different prompts.For the verbalizer, we use the prototypical verbalizer to replace the manualone. In summary, we obtained the best average accuracy of 73.2% (a relativeimprovement of 2.52% over even the previous state-of-the-art SSL method withmanual prompts and verbalizers) in different few-shot learning settings.", "title": "scalable prompt generation for semisupervised learning with language models", "url": "http://arxiv.org/pdf/2302.09236v1.pdf", "tokenized_text": "based learning methods semi supervised learning ssl settings havebeen shown effective multiple natural_language natural language understanding tasks literature manually designing multipleprompts verbalizers requires domain knowledge human effort making itdifficult expensive scale different datasets paper wepropose methods automatically design multiple verbalizer ssl settings sacrificing performance uses demonstration examples learnable continuous create diverse second method uses varying numberof soft tokens encourage language_models language learn different verbalizer use prototypical verbalizer replace summary obtained best average accuracy 73.2 2.52 previous state art ssl method verbalizers different shot_learning shot learning settings"}
{"id": "nan", "abstract": "  Clinical prediction is an essential task in the healthcare industry. However,the recent success of transformers, on which large language models are built,has not been extended to this domain. In this research, we explore the use oftransformers and language models in prognostic prediction for immunotherapyusing real-world patients' clinical data and molecular profiles. This paperinvestigates the potential of transformers to improve clinical predictioncompared to conventional machine learning approaches and addresses thechallenge of few-shot learning in predicting rare disease areas. The studybenchmarks the efficacy of baselines and language models on prognosticprediction across multiple cancer types and investigates the impact ofdifferent pretrained language models under few-shot regimes. The resultsdemonstrate significant improvements in accuracy and highlight the potential ofNLP in clinical research to improve early detection and intervention fordifferent diseases.", "title": "language models are fewshot learners for prognostic prediction", "url": "http://arxiv.org/pdf/2302.12692v4.pdf", "tokenized_text": "clinical prediction essential task healthcare industry recent success transformers large_language large language built extended domain research explore use oftransformers language_models language prediction real world patients clinical data molecular profiles paperinvestigates potential transformers improve clinical conventional machine_learning machine learning approaches addresses thechallenge shot_learning shot learning predicting rare disease areas efficacy baselines language_models language multiple cancer types investigates impact ofdifferent pretrained_language pretrained language shot regimes resultsdemonstrate significant improvements accuracy highlight potential ofnlp clinical research improve early detection intervention fordifferent diseases"}
{"id": "nan", "abstract": "  Speech models have long been known to overfit individual speakers for manyclassification tasks. This leads to poor generalization in settings where thespeakers are out-of-domain or out-of-distribution, as is common in productionenvironments. We view speaker adaptation as a few-shot learning problem andpropose investigating transfer learning approaches inspired by recent successwith pre-trained models in natural language tasks. We propose pre-finetuningspeech models on difficult tasks to distill knowledge into few-shot downstreamclassification objectives. We pre-finetune Wav2Vec2.0 on every permutation offour multiclass emotional speech recognition corpora and evaluate ourpre-finetuned models through 33,600 few-shot fine-tuning trials on theEmotional Speech Dataset.", "title": "prefinetuning for fewshot emotional speech recognition", "url": "http://arxiv.org/pdf/2302.12921v2.pdf", "tokenized_text": "speech long known overfit individual speakers tasks leads poor generalization settings domain distribution common productionenvironments view speaker adaptation shot_learning shot learning problem andpropose investigating transfer learning approaches inspired recent pre trained natural_language natural language tasks propose pre difficult tasks distill knowledge shot objectives pre finetune permutation emotional speech recognition corpora evaluate finetuned shot fine tuning trials speech dataset"}
{"id": "nan", "abstract": "  Large language models (LLMs) effectively generate fluent text when the targetoutput follows natural language patterns. However, structured prediction tasksconfine the output format to a limited ontology, causing even very large modelsto struggle since they were never trained with such restrictions in mind. Thedifficulty of using LLMs for direct prediction is exacerbated in few-shotlearning scenarios, which commonly arise due to domain shift and resourcelimitations. We flip the problem on its head by leveraging the LLM as a toolfor data augmentation rather than direct prediction. Our proposed Mixture ofSoft Prompts (MSP) serves as a parameter-efficient procedure for generatingdata in a controlled manner. Denoising mechanisms are further applied toimprove the quality of synthesized data. Automatic metrics show our method iscapable of producing diverse and natural text, while preserving labelsemantics. Moreover, MSP achieves state-of-the-art results on three benchmarkswhen compared against strong baselines. Our method offers an alternatedata-centric approach for applying LLMs to complex prediction tasks.", "title": "mixture of soft prompts for controllable data generation", "url": "http://arxiv.org/pdf/2303.01580v2.pdf", "tokenized_text": "large_language large language llms effectively generate fluent text follows natural_language natural language patterns structured prediction output format limited ontology causing large modelsto struggle trained restrictions mind thedifficulty llms direct prediction shotlearning scenarios commonly arise domain shift flip problem head leveraging llm toolfor data_augmentation data augmentation direct prediction proposed mixture msp serves parameter efficient procedure controlled manner denoising mechanisms applied toimprove quality synthesized data automatic metrics method producing diverse natural text preserving msp achieves_state achieves state art results compared strong baselines method offers centric approach applying llms complex prediction tasks"}
{"id": "nan", "abstract": "  Recent vision-language models have shown impressive multi-modal generationcapabilities. However, typically they require training huge models on massivedatasets. As a more scalable alternative, we introduce Prismer, a data- andparameter-efficient vision-language model that leverages an ensemble of domainexperts. Prismer only requires training of a small number of components, withthe majority of network weights inherited from readily-available, pre-traineddomain experts, and kept frozen during training. By leveraging experts from awide range of domains, we show that Prismer can efficiently pool this expertknowledge and adapt it to various vision-language reasoning tasks. In ourexperiments, we show that Prismer achieves fine-tuned and few-shot learningperformance which is competitive with current state-of-the-art models, whilstrequiring up to two orders of magnitude less training data. Code is availableat https://github.com/NVlabs/prismer.", "title": "prismer a visionlanguage model with an ensemble of experts", "url": "http://arxiv.org/pdf/2303.02506v2.pdf", "tokenized_text": "recent vision language_models language shown_impressive shown impressive multi modal typically require training huge massivedatasets scalable alternative introduce andparameter efficient vision language_model language leverages ensemble requires training small_number small number components withthe majority network weights inherited readily available pre experts kept frozen training leveraging experts awide range domains efficiently pool adapt vision language reasoning tasks ourexperiments achieves fine tuned shot learningperformance competitive current state art orders magnitude training_data training data code availableat"}
{"id": "nan", "abstract": "  Activity and property prediction models are the central workhorses in drugdiscovery and materials sciences, but currently they have to be trained orfine-tuned for new tasks. Without training or fine-tuning, scientific languagemodels could be used for such low-data tasks through their announced zero- andfew-shot capabilities. However, their predictive quality at activity predictionis lacking. In this work, we envision a novel type of activity prediction modelthat is able to adapt to new prediction tasks at inference time, viaunderstanding textual information describing the task. To this end, we proposea new architecture with separate modules for chemical and natural languageinputs, and a contrastive pre-training objective on data from large biochemicaldatabases. In extensive experiments, we show that our method CLAMP yieldsimproved predictive performance on few-shot learning benchmarks and zero-shotproblems in drug discovery. We attribute the advances of our method to themodularized architecture and to our pre-training objective.", "title": "enhancing activity prediction models in drug discovery with the ability to understand human language", "url": "http://arxiv.org/pdf/2303.03363v2.pdf", "tokenized_text": "activity property prediction central materials sciences currently trained orfine tuned new tasks training fine tuning scientific languagemodels low data tasks zero- andfew shot capabilities predictive quality activity lacking work envision novel type activity prediction able adapt new prediction tasks inference time textual information describing task end proposea new architecture separate modules chemical natural contrastive pre training objective data large extensive_experiments extensive experiments method predictive performance shot_learning shot learning benchmarks zero drug discovery attribute advances method architecture pre training objective"}
{"id": "nan", "abstract": "  Menu system design is a challenging task involving many design options andvarious human factors. For example, one crucial factor that designers need toconsider is the semantic and systematic relation of menu commands. However,capturing these relations can be challenging due to limited availableresources. With the advancement of neural language models, large languagemodels can utilize their vast pre-existing knowledge in designing and refiningmenu systems. In this paper, we propose MenuCraft, an AI-assisted designer formenu design that enables collaboration between the designer and a dialoguesystem to design menus. MenuCraft offers an interactive language-based menudesign tool that simplifies the menu design process and enables easycustomization of design options. MenuCraft supports a variety of interactionsthrough dialog that allows performing zero/few-shot learning.", "title": "menucraft interactive menu system design with large language models", "url": "http://arxiv.org/pdf/2303.04496v2.pdf", "tokenized_text": "system design challenging task involving design options human factors example crucial factor designers need semantic systematic relation commands capturing relations challenging limited advancement neural language_models language large_languagemodels large languagemodels utilize vast pre existing knowledge designing systems paper propose ai assisted designer design enables collaboration designer design offers interactive language based tool design process enables design options supports variety dialog allows performing zero shot_learning shot learning"}
{"id": "nan", "abstract": "  ChatGPT has gained a huge popularity since its introduction. Its positiveaspects have been reported through many media platforms, and some analyses evenshowed that ChatGPT achieved a decent grade in professional exams, adding extrasupport to the claim that AI can now assist and even replace humans inindustrial fields. Others, however, doubt its reliability and trustworthiness.This paper investigates the trustworthiness of ChatGPT and GPT-4 regardinglogically consistent behaviour, focusing specifically on semantic consistencyand the properties of negation, symmetric, and transitive consistency. Ourfindings suggest that while both models appear to show an enhanced languageunderstanding and reasoning ability, they still frequently fall short ofgenerating logically consistent predictions. We also ascertain via experimentsthat prompt designing, few-shot learning and employing larger large languagemodels (LLMs) are unlikely to be the ultimate solution to resolve theinconsistency issue of LLMs.", "title": "consistency analysis of chatgpt", "url": "http://arxiv.org/pdf/2303.06273v3.pdf", "tokenized_text": "chatgpt gained huge popularity introduction reported media platforms analyses chatgpt achieved decent grade professional exams adding claim ai assist replace humans fields doubt reliability trustworthiness paper investigates trustworthiness chatgpt gpt-4 consistent behaviour focusing specifically semantic properties negation symmetric consistency ourfindings suggest appear enhanced languageunderstanding reasoning ability frequently fall short ofgenerating logically consistent predictions designing shot_learning shot learning employing larger large_languagemodels large languagemodels llms unlikely ultimate solution resolve issue llms"}
{"id": "nan", "abstract": "  Prompt learning is an efficient approach to adapt transformers by insertinglearnable set of parameters into the input and intermediate representations ofa pre-trained model. In this work, we present Expressive Prompts with Residuals(EXPRES) which modifies the prompt learning paradigm specifically for effectiveadaptation of vision transformers (ViT). Out method constructs downstreamrepresentations via learnable ``output'' tokens, that are akin to the learnedclass tokens of the ViT. Further for better steering of the downstreamrepresentation processed by the frozen transformer, we introduce residuallearnable tokens that are added to the output of various computations. We applyEXPRES for image classification, few shot learning, and semantic segmentation,and show our method is capable of achieving state of the art prompt tuning on3/3 categories of the VTAB benchmark. In addition to strong performance, weobserve that our approach is an order of magnitude more prompt efficient thanexisting visual prompting baselines. We analytically show the computationalbenefits of our approach over weight space adaptation techniques likefinetuning. Lastly we systematically corroborate the architectural design ofour method via a series of ablation experiments.", "title": "learning expressive prompting with residuals for vision transformers", "url": "http://arxiv.org/pdf/2303.15591v1.pdf", "tokenized_text": "learning efficient approach adapt transformers set parameters input intermediate representations ofa pre trained work present expressive modifies learning paradigm specifically vision transformers vit method constructs learnable output tokens akin tokens vit. better steering processed frozen transformer introduce tokens added output computations image classification shot_learning shot learning semantic segmentation method capable achieving state_of_the_art state art tuning categories benchmark addition strong performance weobserve approach order magnitude efficient thanexisting visual baselines approach weight space adaptation techniques lastly systematically architectural design ofour method series ablation experiments"}
{"id": "nan", "abstract": "  The popularity of Contrastive Language-Image Pre-training (CLIP) haspropelled its application to diverse downstream vision tasks. To improve itscapacity on downstream tasks, few-shot learning has become a widely-adoptedtechnique. However, existing methods either exhibit limited performance orsuffer from excessive learnable parameters. In this paper, we propose APE, anAdaptive Prior rEfinement method for CLIP's pre-trained knowledge, whichachieves superior accuracy with high computational efficiency. Via a priorrefinement module, we analyze the inter-class disparity in the downstream dataand decouple the domain-specific knowledge from the CLIP-extracted cache model.On top of that, we introduce two model variants, a training-free APE and atraining-required APE-T. We explore the trilateral affinities between the testimage, prior cache model, and textual representations, and only enable alightweight category-residual module to be trained. For the average accuracyover 11 benchmarks, both APE and APE-T attain state-of-the-art and respectivelyoutperform the second-best by +1.59% and +1.99% under 16 shots with x30 lesslearnable parameters.", "title": "not all features matter enhancing fewshot clip with adaptive prior refinement", "url": "http://arxiv.org/pdf/2304.01195v1.pdf", "tokenized_text": "popularity contrastive_language-image_pre-training contrastive language-image pre-training clip application diverse downstream vision tasks improve downstream_tasks downstream tasks shot_learning shot learning widely existing_methods existing methods exhibit limited performance excessive learnable parameters paper propose ape prior refinement method clip pre trained knowledge whichachieves superior accuracy high computational efficiency module analyze inter class disparity downstream dataand decouple domain specific knowledge clip extracted cache introduce variants training free ape required explore affinities prior cache textual representations enable alightweight category residual module trained average accuracyover 11 benchmarks ape ape attain state art second best 16 shots parameters"}
{"id": "nan", "abstract": "  We introduce HATELEXICON, a lexicon of slurs and targets of hate speech forthe countries of Brazil, Germany, India and Kenya, to aid training andinterpretability of models. We demonstrate how our lexicon can be used tointerpret model predictions, showing that models developed to classify extremespeech rely heavily on target words when making predictions. Further, wepropose a method to aid shot selection for training in low-resource settingsvia HATELEXICON. In few-shot learning, the selection of shots is of paramountimportance to model performance. In our work, we simulate a few-shot settingfor German and Hindi, using HASOC data for training and the MultilingualHateCheck (MHC) as a benchmark. We show that selecting shots based on ourlexicon leads to models performing better on MHC than models trained on shotssampled randomly. Thus, when given only a few training examples, using ourlexicon to select shots containing more sociocultural information leads tobetter few-shot performance.", "title": "sociocultural knowledge is needed for selection of shots in hate speech detection tasks", "url": "http://arxiv.org/pdf/2304.01890v4.pdf", "tokenized_text": "introduce lexicon targets hate speech forthe countries aid training andinterpretability demonstrate lexicon tointerpret predictions showing developed classify rely heavily target words making predictions wepropose method aid shot selection training low resource shot_learning shot learning selection shots paramountimportance performance work simulate shot german hindi data training benchmark selecting shots based leads performing better trained randomly given training_examples training examples select shots containing information leads tobetter shot performance"}
{"id": "nan", "abstract": "  Current literature demonstrates that Large Language Models (LLMs) are greatfew-shot learners, and prompting significantly increases their performance on arange of downstream tasks in a few-shot learning setting. An attempt toautomate human-led prompting followed, with some progress achieved. Inparticular, subsequent work demonstrates automation can outperform fine-tuningin certain K-shot learning scenarios.  In this paper, we revisit techniques for automated prompting on six differentdownstream tasks and a larger range of K-shot learning settings. We find thatautomated prompting does not consistently outperform simple manual prompts. Ourwork suggests that, in addition to fine-tuning, manual prompts should be usedas a baseline in this line of research.", "title": "revisiting automated prompting are we actually doing better", "url": "http://arxiv.org/pdf/2304.03609v2.pdf", "tokenized_text": "current literature demonstrates large_language large language llms shot learners significantly increases performance arange downstream_tasks downstream tasks shot_learning shot learning setting attempt human led followed progress achieved inparticular subsequent work demonstrates automation outperform fine certain shot_learning shot learning scenarios paper revisit techniques automated tasks larger range shot_learning shot learning settings find consistently outperform simple manual ourwork suggests addition fine tuning manual baseline line research"}
{"id": "nan", "abstract": "  Research in Document Intelligence and especially in Document Key InformationExtraction (DocKIE) has been mainly solved as Token Classification problem.Recent breakthroughs in both natural language processing (NLP) and computervision helped building document-focused pre-training methods, leveraging amultimodal understanding of the document text, layout and image modalities.However, these breakthroughs also led to the emergence of a new DocKIE subtaskof extractive document Question Answering (DocQA), as part of the MachineReading Comprehension (MRC) research field. In this work, we compare theQuestion Answering approach with the classical token classification approachfor document key information extraction. We designed experiments to benchmarkfive different experimental setups : raw performances, robustness to noisyenvironment, capacity to extract long entities, fine-tuning speed on Few-ShotLearning and finally Zero-Shot Learning. Our research showed that when dealingwith clean and relatively short entities, it is still best to use tokenclassification-based approach, while the QA approach could be a goodalternative for noisy environment or long entities use-cases.", "title": "information extraction from documents question answering vs token classification in realworld setups", "url": "http://arxiv.org/pdf/2304.10994v1.pdf", "tokenized_text": "research document intelligence especially document key informationextraction mainly solved token classification problem recent breakthroughs natural_language natural language processing nlp computervision helped building document focused pre training methods leveraging amultimodal understanding document text layout image modalities breakthroughs led emergence new extractive document question_answering question answering comprehension mrc research field work compare thequestion answering approach classical token classification document key information_extraction information extraction designed experiments different experimental setups raw performances robustness capacity extract long entities fine tuning speed shotlearning finally zero shot_learning shot learning research showed dealingwith clean relatively short entities best use based approach qa approach noisy environment long entities use cases"}
{"id": "nan", "abstract": "  Few-shot named entity recognition (NER) systems aims at recognizing newclasses of entities based on a few labeled samples. A significant challenge inthe few-shot regime is prone to overfitting than the tasks with abundantsamples. The heavy overfitting in few-shot learning is mainly led by spuriouscorrelation caused by the few samples selection bias. To alleviate the problemof the spurious correlation in the few-shot NER, in this paper, we propose acausal intervention-based few-shot NER method. Based on the prototypicalnetwork, the method intervenes in the context and prototype via backdooradjustment during training. In particular, intervening in the context of theone-shot scenario is very difficult, so we intervene in the prototype viaincremental learning, which can also avoid catastrophic forgetting. Ourexperiments on different benchmarks show that our approach achieves newstate-of-the-art results (achieving up to 29% absolute improvement and 12% onaverage for all tasks).", "title": "causal interventionsbased fewshot named entity recognition", "url": "http://arxiv.org/pdf/2305.01914v1.pdf", "tokenized_text": "shot named_entity named entity recognition ner systems aims recognizing entities based labeled samples significant challenge inthe shot regime prone overfitting tasks heavy overfitting shot_learning shot learning mainly led caused samples selection bias alleviate problemof spurious correlation shot ner paper propose intervention based shot ner method based prototypicalnetwork method context prototype training particular context theone shot scenario difficult intervene prototype learning avoid catastrophic forgetting ourexperiments different benchmarks approach achieves newstate art results achieving 29 absolute improvement 12 onaverage tasks"}
{"id": "nan", "abstract": "  Recent advances in image captioning are mainly driven by large-scalevision-language pretraining, relying heavily on computational resources andincreasingly large multimodal datasets. Instead of scaling up pretraining data,we ask whether it is possible to improve performance by improving the qualityof the samples in existing datasets. We pursue this question through twoapproaches to data curation: one that assumes that some examples should beavoided due to mismatches between the image and caption, and one that assumesthat the mismatch can be addressed by replacing the image, for which we use thestate-of-the-art Stable Diffusion model. These approaches are evaluated usingthe BLIP model on MS COCO and Flickr30K in both finetuning and few-shotlearning settings. Our simple yet effective approaches consistently outperformbaselines, indicating that better image captioning models can be trained bycurating existing resources. Finally, we conduct a human study to understandthe errors made by the Stable Diffusion model and highlight directions forfuture work in text-to-image generation.", "title": "data curation for image captioning with texttoimage generative models", "url": "http://arxiv.org/pdf/2305.03610v1.pdf", "tokenized_text": "recent_advances recent advances image captioning mainly driven large language pretraining relying heavily computational resources large multimodal datasets instead scaling pretraining data ask possible improve performance improving samples existing datasets pursue question data curation assumes examples image caption mismatch addressed replacing image use thestate art stable_diffusion stable diffusion approaches evaluated blip ms coco finetuning shotlearning settings simple effective approaches consistently indicating better image captioning trained existing resources finally conduct human study understandthe errors stable_diffusion stable diffusion highlight directions forfuture work text image_generation image generation"}
{"id": "nan", "abstract": "  Large language models (LLMs) have shown increasing power on various naturallanguage processing (NLP) tasks. However, tuning these models for downstreamtasks usually needs exorbitant costs or is unavailable due to commercialconsiderations. Recently, black-box tuning has been proposed to address thisproblem by optimizing task-specific prompts without accessing the gradients andhidden representations. However, most existing works have yet fully exploitedthe potential of gradient-free optimization under the scenario of few-shotlearning. In this paper, we describe BBT-RGB, a suite of straightforward andcomplementary techniques for enhancing the efficiency and performance ofblack-box optimization. Specifically, our method includes three plug-and-playcomponents: (1) Two-stage derivative-free optimization strategy thatfacilitates fast convergence and mitigates overfitting; (2) Automaticverbalizer construction with its novel usage under few-shot settings; (3)Better prompt initialization policy based on instruction search andauto-selected demonstration. Extensive experiments across various tasks onnatural language understanding and inference demonstrate the effectiveness ofour method. Our codes are publicly available athttps://github.com/QiushiSun/BBT-RGB.", "title": "make promptbased blackbox tuning colorful boosting model generalization from three orthogonal perspectives", "url": "http://arxiv.org/pdf/2305.08088v1.pdf", "tokenized_text": "large_language large language llms shown increasing power naturallanguage processing nlp tasks tuning downstreamtasks usually needs costs unavailable recently black box tuning proposed address thisproblem optimizing task specific accessing gradients representations existing works fully potential gradient free optimization scenario shotlearning paper describe suite straightforward andcomplementary techniques enhancing efficiency performance ofblack box optimization specifically method includes plug stage derivative free optimization strategy fast convergence mitigates overfitting construction novel usage shot_settings shot settings initialization policy based instruction search andauto selected demonstration extensive_experiments extensive experiments tasks onnatural language understanding inference demonstrate_the_effectiveness demonstrate effectiveness ofour method codes publicly_available publicly available"}
{"id": "nan", "abstract": "  Detecting norm violations in online communities is critical to maintaininghealthy and safe spaces for online discussions. Existing machine learningapproaches often struggle to adapt to the diverse rules and interpretationsacross different communities due to the inherent challenges of fine-tuningmodels for such context-specific tasks. In this paper, we introduceContext-aware Prompt-based Learning for Norm Violation Detection (CPL-NoViD), anovel method that employs prompt-based learning to detect norm violationsacross various types of rules. CPL-NoViD outperforms the baseline byincorporating context through natural language prompts and demonstratesimproved performance across different rule types. Significantly, it not onlyexcels in cross-rule-type and cross-community norm violation detection but alsoexhibits adaptability in few-shot learning scenarios. Most notably, itestablishes a new state-of-the-art in norm violation detection, surpassingexisting benchmarks. Our work highlights the potential of prompt-based learningfor context-sensitive norm violation detection and paves the way for futureresearch on more adaptable, context-aware models to better support onlinecommunity moderators.", "title": "cplnovid contextaware promptbased learning for norm violation detection in online communities", "url": "http://arxiv.org/pdf/2305.09846v2.pdf", "tokenized_text": "detecting norm violations online communities critical safe spaces online discussions existing machine learningapproaches struggle adapt diverse rules different communities inherent challenges fine context specific tasks paper aware based learning norm detection anovel method employs based learning detect norm types rules outperforms baseline byincorporating context natural_language natural language performance different rule types significantly cross rule type cross community norm detection adaptability shot_learning shot learning scenarios notably new state art norm detection benchmarks work highlights potential based learningfor context sensitive norm detection paves way futureresearch adaptable context aware better support moderators"}
{"id": "nan", "abstract": "  We explore how weak supervision on abundant unlabeled data can be leveragedto improve few-shot performance in aspect-based sentiment analysis (ABSA)tasks. We propose a pipeline approach to construct a noisy ABSA dataset, and weuse it to adapt a pre-trained sequence-to-sequence model to the ABSA tasks. Wetest the resulting model on three widely used ABSA datasets, before and afterfine-tuning. Our proposed method preserves the full fine-tuning performancewhile showing significant improvements (15.84% absolute F1) in the few-shotlearning scenario for the harder tasks. In zero-shot (i.e., withoutfine-tuning), our method outperforms the previous state of the art on theaspect extraction sentiment classification (AESC) task and is, additionally,capable of performing the harder aspect sentiment triplet extraction (ASTE)task.", "title": "a weak supervision approach for fewshot aspect based sentiment", "url": "http://arxiv.org/pdf/2305.11979v1.pdf", "tokenized_text": "explore weak supervision abundant unlabeled data improve shot performance aspect based sentiment_analysis sentiment analysis propose pipeline approach construct noisy absa dataset weuse adapt pre trained sequence sequence absa tasks resulting widely absa datasets afterfine tuning proposed_method proposed method preserves fine tuning showing significant improvements absolute f1 shotlearning scenario harder tasks zero shot i.e. tuning method outperforms previous state_of_the_art state art extraction sentiment classification task additionally capable performing harder aspect sentiment triplet extraction"}
{"id": "nan", "abstract": "  Few-shot learning for open domain multi-hop question answering typicallyrelies on large language models (LLMs). While powerful, LLMs are inefficient atthe inference time. We propose a data synthesis framework for multi-hopquestion answering that allows for improving smaller language models with lessthan 10 human-annotated question answer pairs. The framework is built upon thedata generation functions parameterized by LLMs and prompts, which requiresminimal hand-crafted features. Empirically, we synthesize millions of multi-hopquestions and claims. After finetuning language models on the synthetic data,we evaluate the models on popular benchmarks on multi-hop question answeringand fact verification. Our experimental results show that finetuning on thesynthetic data improves model performance significantly, allowing our finetunedmodels to be competitive with prior models while being almost one-third thesize in terms of parameter counts.", "title": "efficient open domain multihop question answering with fewshot data synthesis", "url": "http://arxiv.org/pdf/2305.13691v1.pdf", "tokenized_text": "shot_learning shot learning open_domain open domain multi hop question_answering question answering large_language large language llms powerful llms inefficient atthe inference time propose data synthesis framework multi hopquestion answering allows improving smaller language_models language 10 human annotated question answer pairs framework built thedata generation functions parameterized llms hand crafted features empirically synthesize millions multi hopquestions claims finetuning language_models language synthetic data evaluate popular benchmarks multi hop question fact verification experimental_results experimental results finetuning data improves performance significantly allowing competitive prior thesize terms parameter counts"}
{"id": "nan", "abstract": "  Large language models have demonstrated robust performance on variouslanguage tasks using zero-shot or few-shot learning paradigms. While beingactively researched, multimodal models that can additionally handle images asinput have yet to catch up in size and generality with language-only models. Inthis work, we ask whether language-only models can be utilised for tasks thatrequire visual input -- but also, as we argue, often require a strong reasoningcomponent. Similar to some recent related work, we make visual informationaccessible to the language model using separate verbalisation models.Specifically, we investigate the performance of open-source, open-accesslanguage models against GPT-3 on five vision-language tasks when giventextually-encoded visual information. Our results suggest that language modelsare effective for solving vision-language tasks even with limited samples. Thisapproach also enhances the interpretability of a model's output by providing ameans of tracing the output back through the verbalised image content.", "title": "images in language space exploring the suitability of large language models for vision & language tasks", "url": "http://arxiv.org/pdf/2305.13782v1.pdf", "tokenized_text": "large_language large language demonstrated robust performance variouslanguage tasks zero shot shot_learning shot learning paradigms beingactively researched multimodal additionally handle images catch size generality language inthis work ask language tasks thatrequire visual input argue require strong similar recent related work visual language_model language separate specifically investigate performance open source open gpt-3 vision language tasks encoded visual information results suggest language modelsare effective solving vision language tasks limited samples thisapproach enhances interpretability output providing tracing output image content"}
{"id": "nan", "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities inlanguage generation, understanding, and few-shot learning in recent years. Anextensive body of work has explored how their performance may be furtherimproved through the tools of prompting, ranging from verification,self-consistency, or intermediate scratchpads. In this paper, we present acomplementary approach to improve language responses where multiple languagemodel instances propose and debate their individual responses and reasoningprocesses over multiple rounds to arrive at a common final answer. Our findingsindicate that this approach significantly enhances mathematical and strategicreasoning across a number of tasks. We also demonstrate that our approachimproves the factual validity of generated content, reducing fallacious answersand hallucinations that contemporary models are prone to. Our approach may bedirectly applied to existing black-box models and uses identical procedure andprompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\" approach has the potential to significantly advance thecapabilities of LLMs and pave the way for further breakthroughs in languagegeneration and understanding.", "title": "improving factuality and reasoning in language models through multiagent debate", "url": "http://arxiv.org/pdf/2305.14325v1.pdf", "tokenized_text": "large_language large language llms demonstrated_remarkable demonstrated remarkable capabilities inlanguage generation understanding shot_learning shot learning recent_years recent years anextensive body work explored performance tools ranging verification self consistency intermediate scratchpads paper present approach improve language responses multiple languagemodel instances propose debate individual responses multiple rounds arrive common final answer findingsindicate approach significantly enhances mathematical number tasks demonstrate factual validity generated content reducing fallacious hallucinations contemporary prone approach bedirectly applied existing black box uses identical procedure tasks investigate overall findings_suggest findings suggest approach potential significantly advance thecapabilities llms pave way breakthroughs languagegeneration understanding"}
{"id": "nan", "abstract": "  Acquiring high-quality data for training discriminative models is a crucialyet challenging aspect of building effective predictive systems. In this paper,we present Diffusion Inversion, a simple yet effective method that leveragesthe pre-trained generative model, Stable Diffusion, to generate diverse,high-quality training data for image classification. Our approach captures theoriginal data distribution and ensures data coverage by inverting images to thelatent space of Stable Diffusion, and generates diverse novel training imagesby conditioning the generative model on noisy versions of these vectors. Weidentify three key components that allow our generated images to successfullysupplant the original dataset, leading to a 2-3x enhancement in samplecomplexity and a 6.5x decrease in sampling time. Moreover, our approachconsistently outperforms generic prompt-based steering methods and KNNretrieval baseline across a wide range of datasets. Additionally, wedemonstrate the compatibility of our approach with widely-used dataaugmentation techniques, as well as the reliability of the generated data insupporting various neural architectures and enhancing few-shot learning.", "title": "training on thin air improve image classification with generated data", "url": "http://arxiv.org/pdf/2305.15316v1.pdf", "tokenized_text": "acquiring high quality data training discriminative challenging aspect building effective predictive systems paper present diffusion inversion simple effective method leveragesthe pre trained generative stable_diffusion stable diffusion generate diverse high quality training_data training data image classification approach captures theoriginal data distribution ensures data coverage inverting images space stable_diffusion stable diffusion generates diverse novel training conditioning generative noisy versions vectors weidentify key components allow generated images original dataset leading 3x enhancement decrease sampling time outperforms generic based steering methods baseline wide_range wide range datasets additionally wedemonstrate compatibility approach widely dataaugmentation techniques reliability generated data neural architectures enhancing shot_learning shot learning"}
{"id": "nan", "abstract": "  Paraphrase generation is a long-standing task in natural language processing(NLP). Supervised paraphrase generation models, which rely on human-annotatedparaphrase pairs, are cost-inefficient and hard to scale up. On the other hand,automatically annotated paraphrase pairs (e.g., by machine back-translation),usually suffer from the lack of syntactic diversity -- the generated paraphrasesentences are very similar to the source sentences in terms of syntax. In thiswork, we present ParaAMR, a large-scale syntactically diverse paraphrasedataset created by abstract meaning representation back-translation. Ourquantitative analysis, qualitative examples, and human evaluation demonstratethat the paraphrases of ParaAMR are syntactically more diverse compared toexisting large-scale paraphrase datasets while preserving good semanticsimilarity. In addition, we show that ParaAMR can be used to improve on threeNLP tasks: learning sentence embeddings, syntactically controlled paraphrasegeneration, and data augmentation for few-shot learning. Our results thusshowcase the potential of ParaAMR for improving various NLP applications.", "title": "paraamr a largescale syntactically diverse paraphrase dataset by amr backtranslation", "url": "http://arxiv.org/pdf/2305.16585v1.pdf", "tokenized_text": "paraphrase generation long standing task natural_language natural language processing(nlp supervised paraphrase generation rely human pairs cost inefficient hard scale hand automatically annotated paraphrase pairs e.g. machine suffer lack syntactic diversity generated similar source sentences terms syntax thiswork present large scale syntactically diverse created abstract meaning representation translation analysis qualitative examples human evaluation demonstratethat paraphrases syntactically diverse compared toexisting large scale paraphrase datasets preserving good addition improve tasks learning sentence embeddings syntactically controlled data_augmentation data augmentation shot_learning shot learning results potential improving nlp applications"}
{"id": "nan", "abstract": "  We presented the Treff adapter, a training-efficient adapter for CLAP, toboost zero-shot classification performance by making use of a small set oflabelled data. Specifically, we designed CALM to retrieve the probabilitydistribution of text-audio clips over classes using a set of audio-label pairsand combined it with CLAP's zero-shot classification results. Furthermore, wedesigned a training-free version of the Treff adapter by using CALM as a cosinesimilarity measure. Experiments showed that the proposed Treff adapter iscomparable and even better than fully-supervised methods and adaptation methodsin low-shot and data-abundant scenarios. While the Treff adapter shows thatcombining large-scale pretraining and rapid learning of domain-specificknowledge is non-trivial for obtaining generic representations for few-shotlearning, it is still limited to audio classification tasks. In the future, wewill explore how to use audio-language models in diverse audio domains.", "title": "adapting languageaudio models as fewshot audio learners", "url": "http://arxiv.org/pdf/2305.17719v1.pdf", "tokenized_text": "presented adapter training efficient adapter zero shot classification performance making use small set data specifically designed retrieve text audio clips classes set audio label combined zero shot classification results furthermore training free version adapter measure experiments showed proposed adapter iscomparable better fully supervised methods adaptation low shot data abundant scenarios adapter shows thatcombining large scale pretraining rapid learning domain non trivial obtaining generic representations shotlearning limited audio classification tasks future explore use audio language_models language diverse audio domains"}
{"id": "nan", "abstract": "  Recent advancements in multimodal foundation models (e.g., CLIP) haveexcelled in zero-shot generalization. Prompt tuning involved in the knowledgetransfer from foundation models to downstream tasks has gained significantattention recently. Existing prompt-tuning methods in cross-modal learning,however, either solely focus on language branch, or learn vision-languageinteraction in a shallow mechanism. In this context, we propose a Deeplycoupled Cross-modal Prompt learning (DCP) method based on CLIP. DCP flexiblyaccommodates the interplay between vision and language with a Cross-ModalPrompt Attention (CMPA) mechanism, which enables the mutual exchange ofrespective representation through a well-connected multi-head attention moduleprogressively and strongly. We then conduct comprehensive few-shot learningexperiments on 11 image classification datasets and analyze the robustness todomain shift as well. Thorough experimental analysis evidently demonstrates thesuperb few-shot generalization and compelling domain adaption capacity of awell-executed DCP. The code can be found at https://github.com/GingL/CMPA.", "title": "deeply coupled crossmodal prompt learning", "url": "http://arxiv.org/pdf/2305.17903v2.pdf", "tokenized_text": "recent advancements multimodal foundation_models foundation e.g. clip zero shot generalization tuning involved foundation_models foundation downstream_tasks downstream tasks gained significantattention recently existing tuning methods cross modal learning solely focus language branch learn vision shallow mechanism context propose cross modal learning method based clip interplay vision language attention mechanism enables mutual exchange representation connected multi head attention strongly conduct comprehensive shot 11 image classification datasets analyze robustness shift thorough experimental analysis demonstrates shot generalization compelling domain adaption capacity awell executed code found"}
{"id": "nan", "abstract": "  Humans can effortlessly understand the coordinate structure of sentences suchas \"Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle,respectively\". In the context of natural language inference (NLI), we examinehow language models (LMs) reason with respective readings (Gawron and Kehler,2004) from two perspectives: syntactic-semantic and commonsense-worldknowledge. We propose a controlled synthetic dataset WikiResNLI and a naturallyoccurring dataset NatResNLI to encompass various explicit and implicitrealizations of \"respectively\". We show that fine-tuned NLI models strugglewith understanding such readings without explicit supervision. While few-shotlearning is easy in the presence of explicit cues, longer training is requiredwhen the reading is evoked implicitly, leaving models to rely on common senseinferences. Furthermore, our fine-grained analysis indicates models fail togeneralize across different constructions. To conclude, we demonstrate that LMsstill lag behind humans in generalizing to the long tail of linguisticconstructions.", "title": "what does the failure to reason with respectively in zerofewshot settings tell us about language models", "url": "http://arxiv.org/pdf/2305.19597v1.pdf", "tokenized_text": "humans effortlessly understand coordinate structure sentences suchas respectively context natural_language natural language inference nli language_models language lms reason respective perspectives syntactic semantic commonsense propose controlled synthetic dataset dataset encompass explicit respectively fine tuned nli understanding explicit supervision shotlearning easy presence explicit cues longer training reading implicitly leaving rely common furthermore fine grained analysis indicates fail togeneralize different constructions conclude demonstrate lag humans generalizing long tail"}
{"id": "nan", "abstract": "  A core tension in models of concept learning is that the model must carefullybalance the tractability of inference against the expressivity of thehypothesis class. Humans, however, can efficiently learn a broad range ofconcepts. We introduce a model of inductive learning that seeks to behuman-like in that sense. It implements a Bayesian reasoning process where alanguage model first proposes candidate hypotheses expressed in naturallanguage, which are then re-weighed by a prior and a likelihood. By estimatingthe prior from human data, we can predict human judgments on learning problemsinvolving numbers and sets, spanning concepts that are generative,discriminative, propositional, and higher-order.", "title": "humanlike fewshot learning via bayesian reasoning over natural language", "url": "http://arxiv.org/pdf/2306.02797v3.pdf", "tokenized_text": "core concept learning inference expressivity class humans efficiently learn broad range introduce inductive learning seeks like sense implements bayesian reasoning process alanguage proposes candidate hypotheses expressed naturallanguage prior likelihood prior human data predict human judgments learning numbers sets spanning concepts generative discriminative propositional higher order"}
{"id": "nan", "abstract": "  Self-rationalizing models that also generate a free-text explanation fortheir predicted labels are an important tool to build trustworthy AIapplications. Since generating explanations for annotated labels is a laboriousand costly pro cess, recent models rely on large pretrained language models(PLMs) as their backbone and few-shot learning. In this work we explore aself-training approach leveraging both labeled and unlabeled data to furtherimprove few-shot models, under the assumption that neither human writtenrationales nor annotated task labels are available at scale. We introduce anovel dual-teacher learning framework, which learns two specialized teachermodels for task prediction and rationalization using self-training and distillstheir knowledge into a multi-tasking student model that can jointly generatethe task label and rationale. Furthermore, we formulate a new loss function,Masked Label Regularization (MLR) which promotes explanations to be stronglyconditioned on predicted labels. Evaluation on three public datasetsdemonstrate that the proposed methods are effective in modeling task labels andgenerating faithful rationales.", "title": "few shot rationale generation using selftraining with dual teachers", "url": "http://arxiv.org/pdf/2306.03315v1.pdf", "tokenized_text": "self rationalizing generate free text explanation predicted labels important tool build trustworthy generating explanations annotated labels costly pro recent rely large pretrained_language pretrained language models(plms backbone shot_learning shot learning work explore aself training approach leveraging labeled unlabeled data furtherimprove shot assumption human annotated task labels available scale introduce anovel dual teacher learning framework learns specialized task prediction rationalization self training knowledge multi tasking student jointly task label rationale furthermore formulate new loss function masked label regularization promotes explanations predicted labels evaluation public datasetsdemonstrate proposed methods effective modeling task labels faithful rationales"}
{"id": "nan", "abstract": "  Sentence Simplification is a valuable technique that can benefit languagelearners and children a lot. However, current research focuses more on Englishsentence simplification. The development of Chinese sentence simplification isrelatively slow due to the lack of data. To alleviate this limitation, thispaper introduces CSS, a new dataset for assessing sentence simplification inChinese. We collect manual simplifications from human annotators and performdata analysis to show the difference between English and Chinese sentencesimplifications. Furthermore, we test several unsupervised and zero/few-shotlearning methods on CSS and analyze the automatic evaluation and humanevaluation results. In the end, we explore whether Large Language Models canserve as high-quality Chinese sentence simplification systems by evaluatingthem on CSS.", "title": "a new dataset and empirical study for sentence simplification in chinese", "url": "http://arxiv.org/pdf/2306.04188v1.pdf", "tokenized_text": "sentence simplification valuable technique benefit children lot current research focuses simplification development chinese sentence simplification slow lack data alleviate limitation thispaper introduces new dataset assessing sentence simplification collect manual simplifications human annotators analysis difference english chinese furthermore test unsupervised zero shotlearning methods analyze automatic evaluation humanevaluation results end explore large_language large language canserve high quality chinese sentence simplification systems"}
{"id": "nan", "abstract": "  The task of cultivating healthy communication in online communities becomesincreasingly urgent, as gaming and social media experiences becomeprogressively more immersive and life-like. We approach the challenge ofmoderating online communities by training student models using a large languagemodel (LLM). We use zero-shot learning models to distill and expand datasetsfollowed by a few-shot learning and a fine-tuning approach, leveragingopen-access generative pre-trained transformer models (GPT) from OpenAI. Ourpreliminary findings suggest, that when properly trained, LLMs can excel inidentifying actor intentions, moderating toxic comments, and rewarding positivecontributions. The student models perform above-expectation in non-contextualassignments such as identifying classically toxic behavior and performsufficiently on contextual assignments such as identifying positivecontributions to online discourse. Further, using open-access models likeOpenAI's GPT we experience a step-change in the development process for whathas historically been a complex modeling task. We contribute to the informationsystem (IS) discourse with a rapid development framework on the application ofgenerative AI in content online moderation and management of culture indecentralized, pseudonymous communities by providing a sample model suite ofindustrial-ready generative AI models based on open-access LLMs.", "title": "can ai moderate online communities", "url": "http://arxiv.org/pdf/2306.05122v1.pdf", "tokenized_text": "task cultivating healthy communication online communities urgent gaming social_media social media experiences life like approach challenge online communities training student large languagemodel llm use zero shot_learning shot learning distill expand shot_learning shot learning fine tuning approach access generative pre trained transformer gpt openai ourpreliminary findings_suggest findings suggest properly trained llms excel inidentifying actor intentions toxic comments rewarding student perform non identifying toxic behavior contextual identifying online discourse open access gpt experience step change development process historically complex modeling task contribute discourse rapid development framework application ofgenerative ai content online moderation management culture communities providing sample suite generative_ai generative ai based open access llms"}
{"id": "nan", "abstract": "  This paper presents the ADAIO team's system entry in the Building EducationalApplications (BEA) 2023 Shared Task on Generating AI Teacher Responses inEducational Dialogues. The task aims to assess the performance ofstate-of-the-art generative models as AI teachers in producing suitableresponses within a student-teacher dialogue. Our system comprises evaluatingvarious baseline models using OpenAI GPT-3 and designing diverse prompts toprompt the OpenAI models for teacher response generation. After the challenge,our system achieved second place by employing a few-shot prompt-based approachwith the OpenAI text-davinci-003 model. The results highlight the few-shotlearning capabilities of large-language models, particularly OpenAI's GPT-3, inthe role of AI teachers.", "title": "the adaio system at the bea2023 shared task on generating ai teacher responses in educational dialogues", "url": "http://arxiv.org/pdf/2306.05360v1.pdf", "tokenized_text": "paper_presents paper presents team system entry building bea 2023 shared task generating ai teacher responses dialogues task aims assess performance ofstate art generative ai teachers producing student teacher dialogue system comprises baseline openai_gpt-3 openai gpt-3 designing diverse toprompt openai teacher response generation challenge system achieved second place employing shot based openai text davinci-003 results highlight shotlearning capabilities large language_models language particularly openai gpt-3 inthe role ai teachers"}
{"id": "nan", "abstract": "  In recent years, language models (LMs) have made remarkable progress inadvancing the field of natural language processing (NLP). However, the impactof data augmentation (DA) techniques on the fine-tuning (FT) performance ofthese LMs has been a topic of ongoing debate. In this study, we evaluate theeffectiveness of three different FT methods in conjugation withback-translation across an array of 7 diverse NLP tasks, includingclassification and regression types, covering single-sentence and sentence-pairtasks. Contrary to prior assumptions that DA does not contribute to theenhancement of LMs' FT performance, our findings reveal that continuedpre-training on augmented data can effectively improve the FT performance ofthe downstream tasks. In the most favourable case, continued pre-trainingimproves the performance of FT by more than 10% in the few-shot learningsetting. Our finding highlights the potential of DA as a powerful tool forbolstering LMs' performance.", "title": "rethink the effectiveness of text data augmentation an empirical analysis", "url": "http://arxiv.org/pdf/2306.07664v1.pdf", "tokenized_text": "recent_years recent years language_models language lms remarkable progress field natural_language natural language processing nlp impactof data_augmentation data augmentation da techniques fine tuning ft performance ofthese lms topic ongoing debate study evaluate theeffectiveness different ft methods translation array diverse nlp_tasks nlp tasks regression types covering single sentence sentence contrary prior assumptions da contribute lms ft performance findings reveal training augmented data effectively improve ft performance ofthe downstream_tasks downstream tasks case continued pre performance ft 10 shot learningsetting finding highlights potential da powerful tool lms performance"}
{"id": "nan", "abstract": "  In few-shot recognition, a classifier that has been trained on one set ofclasses is required to rapidly adapt and generalize to a disjoint, novel set ofclasses. To that end, recent studies have shown the efficacy of fine-tuningwith carefully crafted adaptation architectures. However this raises thequestion of: How can one design the optimal adaptation strategy? In this paper,we study this question through the lens of neural architecture search (NAS).Given a pre-trained neural network, our algorithm discovers the optimalarrangement of adapters, which layers to keep frozen and which to fine-tune. Wedemonstrate the generality of our NAS method by applying it to both residualnetworks and vision transformers and report state-of-the-art performance onMeta-Dataset and Meta-Album.", "title": "neural finetuning search for fewshot learning", "url": "http://arxiv.org/pdf/2306.09295v1.pdf", "tokenized_text": "shot recognition classifier trained set ofclasses required rapidly adapt generalize disjoint novel set ofclasses end recent studies shown efficacy fine carefully crafted adaptation architectures raises thequestion design optimal adaptation strategy paper study question lens neural architecture search pre trained neural network algorithm adapters layers frozen fine tune wedemonstrate generality method applying vision transformers report state art performance dataset meta album"}
{"id": "nan", "abstract": "  Transformer-based language models have achieved remarkable success infew-shot in-context learning and drawn a lot of research interest. However,these models' performance greatly depends on the choice of the example promptsand also has high variability depending on how samples are chosen. In thispaper, we conduct a comprehensive study of retrieving semantically similarfew-shot samples and using them as the context, as it helps the model decidethe correct label without any gradient update in the multilingual andcross-lingual settings. We evaluate the proposed method on five naturallanguage understanding datasets related to intent detection, questionclassification, sentiment analysis, and topic classification. The proposedmethod consistently outperforms random sampling in monolingual andcross-lingual tasks in non-English languages.", "title": "multilingual fewshot learning via language model retrieval", "url": "http://arxiv.org/pdf/2306.10964v1.pdf", "tokenized_text": "transformer based language_models language achieved remarkable success infew shot context_learning context learning drawn lot research interest performance greatly depends choice example promptsand high variability depending samples chosen thispaper conduct comprehensive study retrieving semantically shot samples context helps correct label gradient update multilingual andcross lingual settings evaluate proposed_method proposed method naturallanguage understanding datasets related intent detection sentiment_analysis sentiment analysis topic classification proposedmethod consistently_outperforms consistently outperforms random sampling monolingual andcross lingual tasks non english languages"}
{"id": "nan", "abstract": "  Despite significant progress having been made in question answering ontabular data (Table QA), it's unclear whether, and to what extent existingTable QA models are robust to task-specific perturbations, e.g., replacing keyquestion entities or shuffling table columns. To systematically study therobustness of Table QA models, we propose a benchmark called RobuT, whichbuilds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) andincludes human-annotated adversarial perturbations in terms of table header,table content, and question. Our results indicate that both state-of-the-artTable QA models and large language models (e.g., GPT-3) with few-shot learningfalter in these adversarial sets. We propose to address this problem by usinglarge language models to generate adversarial examples to enhance training,which significantly improves the robustness of Table QA models. Our data andcode is publicly available at https://github.com/yilunzhao/RobuT.", "title": "robut a systematic study of table qa robustness against humanannotated adversarial perturbations", "url": "http://arxiv.org/pdf/2306.14321v1.pdf", "tokenized_text": "despite significant progress having question_answering question answering data table qa unclear extent qa robust task specific perturbations e.g. replacing entities table columns systematically study therobustness table qa propose benchmark called existing table qa datasets weak sqa human annotated adversarial perturbations terms table table content question results_indicate results indicate state qa large_language large language e.g. gpt-3 shot adversarial sets propose address problem language_models language generate adversarial examples enhance training significantly improves robustness table qa data andcode publicly_available publicly available"}
{"id": "nan", "abstract": "  Pre-trained large language models (PLMs) underlie most new developments innatural language processing. They have shifted the field fromapplication-specific model pipelines to a single model that is adapted to awide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongsidetechniques like few-shot learning, have additionally shifted the outputmodality to generation instead of classification or regression. Despite theirubiquitous use, the generation quality of language models is rarely evaluatedwhen these models are introduced. Additionally, it is unclear how existinggeneration tasks--while they can be used to compare systems at a highlevel--relate to the real world use cases for which people have been adoptingthem. In this work, we discuss how to adapt existing application-specificgeneration benchmarks to PLMs and provide an in-depth, empirical study of thelimitations and capabilities of PLMs in natural language generation tasks alongdimensions such as scale, architecture, input and output language. Our resultsshow that PLMs differ in their applicability to different data regimes andtheir generalization to multiple languages and inform which PLMs to use for agiven generation task setup. We share best practices to be taken intoconsideration when benchmarking generation capabilities during the developmentof upcoming PLMs.", "title": "benchmarking large language model capabilities for conditional generation", "url": "http://arxiv.org/pdf/2306.16793v1.pdf", "tokenized_text": "pre trained large_language large language plms underlie new developments innatural language_processing language processing shifted field specific pipelines single adapted awide range tasks autoregressive plms like gpt-3 palm like shot_learning shot learning additionally shifted generation instead classification regression despite use generation quality language_models language rarely introduced additionally unclear tasks compare systems real_world real world use cases people work discuss adapt existing application benchmarks plms provide depth empirical study thelimitations capabilities plms natural_language natural language generation tasks scale architecture input output language resultsshow plms differ applicability different data regimes andtheir generalization multiple languages inform plms use agiven generation task setup share best practices taken benchmarking generation capabilities developmentof upcoming plms"}
{"id": "nan", "abstract": "  Prompts have been shown to be an effective method to adapt a frozenPretrained Language Model (PLM) to perform well on downstream tasks. Promptscan be represented by a human-engineered word sequence or by a learnedcontinuous embedding. In this work, we investigate conditional andcompositional differentiable prompting. We propose a new model, PromptProduction System (PRopS), which learns to transform task instructions or inputmetadata, into continuous prompts that elicit task-specific outputs from thePLM. Our model uses a modular network structure based on our neural formulationof Production Systems, which allows the model to learn discrete rules -- neuralfunctions that learn to specialize in transforming particular prompt inputpatterns, making it suitable for compositional transfer learning and few-shotlearning. We present extensive empirical and theoretical analysis and show thatPRopS consistently surpasses other PLM adaptation techniques, and oftenimproves upon fully fine-tuned models, on compositional generalization tasks,controllable summarization and multilingual translation, while needing fewertrainable parameters.", "title": "on conditional and compositional language model differentiable prompting", "url": "http://arxiv.org/pdf/2307.01446v1.pdf", "tokenized_text": "shown effective method adapt language_model language plm perform downstream_tasks downstream tasks promptscan represented human engineered word sequence embedding work investigate conditional differentiable propose_a_new propose new system learns transform task instructions continuous elicit task specific outputs theplm uses modular network structure based neural production systems allows learn discrete rules learn specialize transforming particular making suitable compositional transfer learning shotlearning present extensive empirical theoretical analysis consistently surpasses plm adaptation techniques fully fine tuned compositional generalization tasks controllable summarization multilingual translation needing parameters"}
{"id": "nan", "abstract": "  There has been significant interest in zero and few-shot learning fordialogue state tracking (DST) due to the high cost of collecting and annotatingtask-oriented dialogues. Recent work has demonstrated that in-context learningrequires very little data and zero parameter updates, and even outperformstrained methods in the few-shot setting (Hu et al. 2022). We propose RefPyDST,which advances the state of the art with three advancements to in-contextlearning for DST. First, we formulate DST as a Python programming task,explicitly modeling language coreference as variable reference in Python.Second, since in-context learning depends highly on the context examples, wepropose a method to retrieve a diverse set of relevant examples to improveperformance. Finally, we introduce a novel re-weighting method during decodingthat takes into account probabilities of competing surface forms, and producesa more accurate dialogue state prediction. We evaluate our approach usingMultiWOZ and achieve state-of-the-art multi-domain joint-goal accuracy in zeroand few-shot settings.", "title": "diverse retrievalaugmented incontext learning for dialogue state tracking", "url": "http://arxiv.org/pdf/2307.01453v1.pdf", "tokenized_text": "significant interest zero shot_learning shot learning state tracking dst high cost collecting oriented dialogues recent_work recent work demonstrated context little data zero parameter updates methods shot_setting shot setting et_al et al 2022 propose advances state_of_the_art state art advancements contextlearning dst formulate dst python programming task explicitly modeling language coreference variable reference python second context_learning context learning depends highly context_examples context examples wepropose method retrieve diverse set relevant examples improveperformance finally introduce novel weighting method takes account probabilities competing surface forms accurate dialogue state prediction evaluate approach achieve state art multi domain joint goal accuracy shot_settings shot settings"}
{"id": "nan", "abstract": "  In this paper, we propose a novel method, Chain-of-Thoughts AttributeManipulation (CoTAM), to guide few-shot learning by carefully crafted data fromLarge Language Models (LLMs). The main idea is to create data with changes onlyin the attribute targeted by the task. Inspired by facial attributemanipulation, our approach generates label-switched data by leveraging LLMs tomanipulate task-specific attributes and reconstruct new sentences in acontrolled manner. Instead of conventional latent representation controlling,we implement chain-of-thoughts decomposition and reconstruction to adapt theprocedure to LLMs. Extensive results on text classification and other tasksverify the advantage of CoTAM over other LLM-based text generation methods withthe same number of training examples. Analysis visualizes the attributemanipulation effectiveness of CoTAM and presents the potential of LLM-guidedlearning with even less supervision.", "title": "generating efficient training data via llmbased attribute manipulation", "url": "http://arxiv.org/pdf/2307.07099v1.pdf", "tokenized_text": "paper propose_a_novel propose novel method chain thoughts guide shot_learning shot learning carefully crafted data language_models language llms main idea create data changes attribute targeted task inspired facial approach generates label switched data leveraging llms task specific attributes reconstruct new sentences manner instead conventional latent representation controlling implement chain thoughts decomposition reconstruction adapt llms extensive results text_classification text classification tasksverify advantage llm based text generation methods withthe number training_examples training examples analysis effectiveness presents potential llm supervision"}
{"id": "nan", "abstract": "  Modern language models can imitate complex patterns through few-shotlearning, enabling them to complete challenging tasks without fine-tuning.However, imitation can also lead models to reproduce inaccuracies or harmfulcontent if present in the context. We study harmful imitation through the lensof a model's internal representations, and identify two related phenomena:overthinking and false induction heads. The first phenomenon, overthinking,appears when we decode predictions from intermediate layers, given correct vs.incorrect few-shot demonstrations. At early layers, both demonstrations inducesimilar model behavior, but the behavior diverges sharply at some \"criticallayer\", after which the accuracy given incorrect demonstrations progressivelydecreases. The second phenomenon, false induction heads, are a possiblemechanistic cause of overthinking: these are heads in late layers that attendto and copy false information from previous demonstrations, and whose ablationreduces overthinking. Beyond scientific understanding, our results suggest thatstudying intermediate model computations could be a promising avenue forunderstanding and guarding against harmful model behaviors.", "title": "overthinking the truth understanding how language models process false demonstrations", "url": "http://arxiv.org/pdf/2307.09476v1.pdf", "tokenized_text": "modern language_models language imitate complex patterns shotlearning enabling complete challenging tasks fine tuning imitation lead reproduce inaccuracies harmfulcontent present context study harmful imitation internal representations identify related phenomena false induction heads phenomenon appears decode predictions intermediate layers given correct shot demonstrations early layers demonstrations behavior behavior sharply accuracy given incorrect demonstrations second phenomenon false induction heads cause heads late layers copy false information previous demonstrations scientific understanding results suggest intermediate computations promising avenue forunderstanding harmful behaviors"}
{"id": "nan", "abstract": "  As large language models, such as GPT, continue to advance the capabilitiesof natural language processing (NLP), the question arises: does the problem ofcorrection still persist? This paper investigates the role of correction in thecontext of large language models by conducting two experiments. The firstexperiment focuses on correction as a standalone task, employing few-shotlearning techniques with GPT-like models for error correction. The secondexperiment explores the notion of correction as a preparatory task for otherNLP tasks, examining whether large language models can tolerate and performadequately on texts containing certain levels of noise or errors. By addressingthese experiments, we aim to shed light on the significance of correction inthe era of large language models and its implications for various NLPapplications.", "title": "does correction remain a problem for large language models", "url": "http://arxiv.org/pdf/2308.01776v2.pdf", "tokenized_text": "large_language large language gpt continue advance capabilitiesof natural_language natural language processing nlp question arises problem persist paper investigates role correction thecontext large_language large language conducting experiments focuses correction standalone task employing shotlearning techniques gpt like error correction explores notion correction task tasks examining large_language large language texts containing certain levels noise errors experiments aim shed light significance correction inthe era large_language large language implications nlpapplications"}
{"id": "nan", "abstract": "  Text-adventure games and text role-playing games are grand challenges forreinforcement learning game playing agents. Text role-playing games areopen-ended environments where an agent must faithfully play a particularcharacter. We consider the distinction between characters and actors, where anactor agent has the ability to play multiple characters. We present a frameworkwe call a thespian agent that can learn to emulate multiple characters alongwith a soft prompt that can be used to direct it as to which character to playat any time. We further describe an attention mechanism that allows the agentto learn new characters that are based on previously learned characters in afew-shot fashion. We show that our agent outperforms the state of the art agentframework in multi-character learning and few-shot learning.", "title": "thespian multicharacter text roleplaying game agents", "url": "http://arxiv.org/pdf/2308.01872v1.pdf", "tokenized_text": "text games text role playing games grand challenges learning game playing agents text role playing games ended environments agent faithfully play consider distinction characters actors agent ability play multiple characters present agent learn emulate multiple characters alongwith soft direct character time describe attention mechanism allows learn new characters based previously learned characters afew shot fashion agent outperforms state_of_the_art state art multi character learning shot_learning shot learning"}
{"id": "nan", "abstract": "  As a subset of machine learning, meta-learning, or learning to learn, aims atimproving the model's capabilities by employing prior knowledge and experience.A meta-learning paradigm can appropriately tackle the conventional challengesof traditional learning approaches, such as insufficient number of samples,domain shifts, and generalization. These unique characteristics positionmeta-learning as a suitable choice for developing influential solutions invarious healthcare contexts, where the available data is often insufficient,and the data collection methodologies are different. This survey discussesmeta-learning broad applications in the healthcare domain to provide insightinto how and where it can address critical healthcare challenges. We firstdescribe the theoretical foundations and pivotal methods of meta-learning. Wethen divide the employed meta-learning approaches in the healthcare domain intotwo main categories of multi/single-task learning and many/few-shot learningand survey the studies. Finally, we highlight the current challenges inmeta-learning research, discuss the potential solutions and provide futureperspectives on meta-learning in healthcare.", "title": "metalearning in healthcare a survey", "url": "http://arxiv.org/pdf/2308.02877v1.pdf", "tokenized_text": "subset machine_learning machine learning meta learning learning learn aims capabilities employing prior knowledge experience meta learning paradigm appropriately tackle conventional traditional learning approaches insufficient number samples domain shifts generalization unique characteristics learning suitable choice developing influential solutions invarious healthcare contexts available data insufficient data collection methodologies different survey learning broad applications healthcare domain provide address critical healthcare challenges theoretical foundations pivotal methods meta learning wethen divide employed meta learning approaches healthcare domain main categories multi single task learning shot survey studies finally highlight current challenges learning research discuss potential solutions provide meta learning healthcare"}
{"id": "nan", "abstract": "  Information-seeking conversation, which aims to help users gather informationthrough conversation, has achieved great progress in recent years. However, theresearch is still stymied by the scarcity of training data. To alleviate thisproblem, we propose AutoConv for synthetic conversation generation, which takesadvantage of the few-shot learning ability and generation capacity of largelanguage models (LLM). Specifically, we formulate the conversation generationproblem as a language modeling task, then finetune an LLM with a few humanconversations to capture the characteristics of the information-seeking processand use it for generating synthetic conversations with high quality.Experimental results on two frequently-used datasets verify that AutoConv hassubstantial improvements over strong baselines and alleviates the dependence onhuman annotation. In addition, we also provide several analysis studies topromote future research.", "title": "autoconv automatically generating informationseeking conversations with large language models", "url": "http://arxiv.org/pdf/2308.06507v1.pdf", "tokenized_text": "information seeking conversation aims help users gather conversation achieved great progress recent_years recent years theresearch scarcity training_data training data alleviate thisproblem propose synthetic conversation generation shot_learning shot learning ability generation capacity largelanguage_models largelanguage llm specifically formulate conversation language modeling task finetune llm capture characteristics information seeking processand use generating synthetic conversations high_quality high quality experimental_results experimental results frequently datasets verify improvements strong baselines alleviates dependence onhuman annotation addition provide analysis studies topromote future_research future research"}
{"id": "nan", "abstract": "  Self-supervised and language-supervised image models contain rich knowledgeof the world that is important for generalization. Many robotic tasks, however,require a detailed understanding of 3D geometry, which is often lacking in 2Dimage features. This work bridges this 2D-to-3D gap for robotic manipulation byleveraging distilled feature fields to combine accurate 3D geometry with richsemantics from 2D foundation models. We present a few-shot learning method for6-DOF grasping and placing that harnesses these strong spatial and semanticpriors to achieve in-the-wild generalization to unseen objects. Using featuresdistilled from a vision-language model, CLIP, we present a way to designatenovel objects for manipulation via free-text natural language, and demonstrateits ability to generalize to unseen expressions and novel categories ofobjects.", "title": "distilled feature fields enable fewshot languageguided manipulation", "url": "http://arxiv.org/pdf/2308.07931v1.pdf", "tokenized_text": "self supervised language supervised image contain rich world important generalization robotic tasks require detailed understanding 3d geometry lacking features work bridges 2d to-3d gap robotic manipulation distilled feature fields combine accurate 3d geometry 2d foundation_models foundation present shot_learning shot learning method grasping placing harnesses strong spatial achieve wild generalization unseen objects vision language_model language clip present way objects manipulation free text natural_language natural language ability generalize unseen expressions novel categories"}
{"id": "nan", "abstract": "  After the inception of emotion recognition or affective computing, it hasincreasingly become an active research topic due to its broad applications.Over the past couple of decades, emotion recognition models have graduallymigrated from statistically shallow models to neural network-based deep models,which can significantly boost the performance of emotion recognition models andconsistently achieve the best results on different benchmarks. Therefore, inrecent years, deep models have always been considered the first option foremotion recognition. However, the debut of large language models (LLMs), suchas ChatGPT, has remarkably astonished the world due to their emergedcapabilities of zero/few-shot learning, in-context learning, chain-of-thought,and others that are never shown in previous deep models. In the present paper,we comprehensively investigate how the LLMs perform in emotion recognition interms of diverse aspects, including in-context learning, few-short learning,accuracy, generalisation, and explanation. Moreover, we offer some insights andpose other potential challenges, hoping to ignite broader discussions aboutenhancing emotion recognition in the new era of advanced and generalised largemodels.", "title": "refashioning emotion recognition modelling the advent of generalised large models", "url": "http://arxiv.org/pdf/2308.11578v1.pdf", "tokenized_text": "emotion recognition affective computing active research topic broad applications past decades emotion recognition statistically shallow neural network based deep significantly boost performance emotion recognition achieve best results different benchmarks years deep considered option recognition large_language large language llms suchas chatgpt remarkably world zero shot_learning shot learning context_learning context learning chain thought shown previous deep present paper comprehensively investigate llms perform emotion recognition interms diverse aspects including context_learning context learning short learning accuracy generalisation explanation offer insights potential challenges broader discussions emotion recognition new era advanced"}
{"id": "nan", "abstract": "  The wide-spread use of social networks has given rise to subjective,misleading, and even false information on the Internet. Thus, subjectivitydetection can play an important role in ensuring the objectiveness and thequality of a piece of information. This paper presents the solution built bythe Gpachov team for the CLEF-2023 CheckThat! lab Task~2 on subjectivitydetection. Three different research directions are explored. The first one isbased on fine-tuning a sentence embeddings encoder model and dimensionalityreduction. The second one explores a sample-efficient few-shot learning model.The third one evaluates fine-tuning a multilingual transformer on an altereddataset, using data from multiple languages. Finally, the three approaches arecombined in a simple majority voting ensemble, resulting in 0.77 macro F1 onthe test set and achieving 2nd place on the English subtask.", "title": "gpachov at checkthat! 2023 a diverse multiapproach ensemble for subjectivity detection in news articles", "url": "http://arxiv.org/pdf/2309.06844v1.pdf", "tokenized_text": "wide spread use social networks given rise subjective misleading false information internet play important role ensuring thequality piece information paper_presents paper presents solution built bythe team lab different research directions explored isbased fine tuning sentence embeddings encoder second explores sample efficient shot_learning shot learning evaluates fine tuning multilingual transformer data multiple languages finally approaches simple majority voting ensemble resulting 0.77 macro f1 onthe test set achieving 2nd place english subtask"}
{"id": "nan", "abstract": "  The \"privacy paradox\" describes the discrepancy between users' privacyattitudes and their actual behaviors. Mitigating this discrepancy requiressolutions that account for both system opaqueness and users' hesitations intesting different privacy settings due to fears of unintended data exposure. Weintroduce an empathy-based approach that allows users to experience how privacybehaviors may alter system outcomes in a risk-free sandbox environment from theperspective of artificially generated personas. To generate realistic personas,we introduce a novel pipeline that augments the outputs of large languagemodels using few-shot learning, contextualization, and chain of thoughts. Ourempirical studies demonstrated the adequate quality of generated personas andhighlighted the changes in privacy-related applications (e.g., onlineadvertising) caused by different personas. Furthermore, users demonstratedcognitive and emotional empathy towards the personas when interacting with oursandbox. We offered design implications for downstream applications inimproving user privacy literacy and promoting behavior changes.", "title": "an empathybased sandbox approach to bridge attitudes, goals, knowledge, and behaviors in the privacy paradox", "url": "http://arxiv.org/pdf/2309.14510v1.pdf", "tokenized_text": "privacy paradox describes discrepancy users actual behaviors mitigating discrepancy account system users different privacy settings unintended data exposure weintroduce empathy based approach allows users experience alter system outcomes risk free environment theperspective artificially generated personas generate realistic personas introduce novel pipeline augments outputs large_languagemodels large languagemodels shot_learning shot learning contextualization chain thoughts ourempirical studies demonstrated adequate quality generated personas changes privacy related applications e.g. caused different personas furthermore users emotional empathy personas interacting offered design implications downstream applications inimproving user privacy promoting behavior changes"}
{"id": "nan", "abstract": "  We present Self-Context Adaptation (SeCAt), a self-supervised approach thatunlocks open-ended few-shot abilities of small visual language models. Ourproposed adaptation algorithm explicitly learns from symbolic, yetself-supervised training tasks. Specifically, our approach imitates imagecaptions in a self-supervised way based on clustering a large pool of imagesfollowed by assigning semantically-unrelated names to clusters. By doing so, weconstruct the `self-context', a training signal consisting of interleavedsequences of image and pseudo-caption pairs and a query image for which themodel is trained to produce the right pseudo-caption. We demonstrate theperformance and flexibility of SeCAt on several multimodal few-shot datasets,spanning various granularities. By using models with approximately 1Bparameters we outperform the few-shot abilities of much larger models, such asFrozen and FROMAGe. SeCAt opens new possibilities for research in open-endedfew-shot learning that otherwise requires access to large or proprietarymodels.", "title": "small visual language models can also be openended fewshot learners", "url": "http://arxiv.org/pdf/2310.00500v1.pdf", "tokenized_text": "present adaptation self supervised approach open ended shot abilities small visual language_models language ourproposed adaptation algorithm explicitly learns symbolic supervised training tasks specifically approach imagecaptions self supervised way based clustering large pool assigning semantically unrelated names clusters weconstruct self context training signal consisting image pseudo caption pairs query image themodel trained produce right pseudo caption demonstrate theperformance flexibility multimodal shot datasets spanning granularities approximately outperform shot abilities larger opens new possibilities research open shot_learning shot learning requires access large"}
{"id": "nan", "abstract": "  Strong inductive biases enable learning from little data and helpgeneralization outside of the training distribution. Popular neuralarchitectures such as Transformers lack strong structural inductive biases forseq2seq NLP tasks on their own. Consequently, they struggle with systematicgeneralization beyond the training distribution, e.g. with extrapolating tolonger inputs, even when pre-trained on large amounts of text. We show how astructural inductive bias can be injected into a seq2seq model by pre-trainingit to simulate structural transformations on synthetic data. Specifically, weinject an inductive bias towards Finite State Transducers (FSTs) into aTransformer by pre-training it to simulate FSTs given their descriptions. Ourexperiments show that our method imparts the desired inductive bias, resultingin improved systematic generalization and better few-shot learning for FST-liketasks.", "title": "injecting a structural inductive bias into a seq2seq model by simulation", "url": "http://arxiv.org/pdf/2310.00796v1.pdf", "tokenized_text": "strong inductive biases enable learning little data outside training distribution popular transformers lack strong structural inductive biases nlp_tasks nlp tasks consequently struggle training distribution e.g. tolonger inputs pre trained large amounts text inductive bias injected seq2seq pre simulate structural transformations synthetic data specifically inductive bias finite state atransformer pre training simulate given descriptions ourexperiments method desired inductive bias improved systematic generalization better shot_learning shot learning"}
{"id": "nan", "abstract": "  Reasoning about time is essential for understanding the nuances of eventsdescribed in natural language. Previous research on this topic has been limitedin scope, characterized by a lack of standardized benchmarks that would allowfor consistent evaluations across different studies. In this paper, weintroduce TRAM, a temporal reasoning benchmark composed of ten datasets,encompassing various temporal aspects of events such as order, arithmetic,frequency, and duration, designed to facilitate a comprehensive evaluation ofthe temporal reasoning capabilities of large language models (LLMs). We conductan extensive evaluation using popular LLMs, such as GPT-4 and Llama2, in bothzero-shot and few-shot learning scenarios. Additionally, we employ BERT-basedmodels to establish the baseline evaluations. Our findings indicate that thesemodels still trail human performance in temporal reasoning tasks. It is ouraspiration that TRAM will spur further progress in enhancing the temporalreasoning abilities of LLMs.", "title": "tram benchmarking temporal reasoning for large language models", "url": "http://arxiv.org/pdf/2310.00835v2.pdf", "tokenized_text": "reasoning time essential understanding nuances natural_language natural language previous research topic scope lack standardized benchmarks consistent evaluations different studies paper weintroduce temporal reasoning benchmark composed datasets encompassing temporal aspects events order arithmetic frequency duration designed facilitate comprehensive evaluation ofthe temporal reasoning capabilities large_language large language llms conductan extensive evaluation popular llms gpt-4 llama2 bothzero shot shot_learning shot learning scenarios additionally employ bert basedmodels establish baseline evaluations findings indicate thesemodels trail human performance temporal reasoning tasks spur progress enhancing abilities llms"}
{"id": "nan", "abstract": "  Recent advancements in the field of Natural Language Processing, particularlythe development of large-scale language models that are pretrained on vastamounts of knowledge, are creating novel opportunities within the realm ofKnowledge Engineering. In this paper, we investigate the usage of largelanguage models (LLMs) in both zero-shot and in-context learning settings totackle the problem of extracting procedures from unstructured PDF text in anincremental question-answering fashion. In particular, we leverage the currentstate-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,accompanied by two variations of in-context learning that involve an ontologywith definitions of procedures and steps and a limited number of samples offew-shot learning. The findings highlight both the promise of this approach andthe value of the in-context learning customisations. These modifications havethe potential to significantly address the challenge of obtaining sufficienttraining data, a hurdle often encountered in deep learning-based NaturalLanguage Processing techniques for procedure extraction.", "title": "procedural text mining with large language models", "url": "http://arxiv.org/pdf/2310.03376v1.pdf", "tokenized_text": "recent advancements field natural_language natural language processing development large scale language_models language pretrained knowledge creating novel opportunities realm ofknowledge engineering paper investigate usage largelanguage_models largelanguage llms zero shot context_learning context learning settings totackle problem extracting procedures unstructured text question answering fashion particular leverage currentstate art gpt-4 generative_pre generative pre trained transformer accompanied variations context_learning context learning involve definitions procedures steps limited number samples offew shot_learning shot learning findings highlight promise approach andthe value context_learning context learning modifications potential significantly address challenge obtaining data hurdle encountered deep learning based naturallanguage_processing naturallanguage processing techniques procedure extraction"}
{"id": "nan", "abstract": "  Few-shot image classification has received considerable attention foraddressing the challenge of poor classification performance with limitedsamples in novel classes. However, numerous studies have employed sophisticatedlearning strategies and diversified feature extraction methods to address thisissue. In this paper, we propose our method called PrototypeFormer, which aimsto significantly advance traditional few-shot image classification approachesby exploring prototype relationships. Specifically, we utilize a transformerarchitecture to build a prototype extraction module, aiming to extract classrepresentations that are more discriminative for few-shot classification.Additionally, during the model training process, we propose a contrastivelearning-based optimization approach to optimize prototype features in few-shotlearning scenarios. Despite its simplicity, the method performs remarkablywell, with no bells and whistles. We have experimented with our approach onseveral popular few-shot image classification benchmark datasets, which showsthat our method outperforms all current state-of-the-art methods. Inparticular, our method achieves 97.07% and 90.88% on 5-way 5-shot and 5-way1-shot tasks of miniImageNet, which surpasses the state-of-the-art results withaccuracy of 7.27% and 8.72%, respectively. The code will be released later.", "title": "prototypeformer learning to explore prototype relationships for fewshot image classification", "url": "http://arxiv.org/pdf/2310.03517v1.pdf", "tokenized_text": "shot image classification received considerable attention challenge poor classification performance novel classes numerous studies employed strategies diversified feature extraction methods address thisissue paper propose method called aimsto significantly advance traditional shot image classification exploring prototype relationships specifically utilize transformerarchitecture build prototype extraction module aiming extract discriminative shot classification additionally training process propose contrastivelearning based optimization approach optimize prototype features shotlearning scenarios despite simplicity method performs bells whistles experimented approach popular shot image classification benchmark_datasets benchmark datasets showsthat method outperforms current state art methods inparticular method_achieves method achieves way shot shot tasks miniimagenet surpasses state art results 7.27 respectively code released later"}
{"id": "nan", "abstract": "  This paper aims to develop a holistic evaluation method for piano soundquality to assist in purchasing decisions. Unlike previous studies that focusedon the effect of piano performance techniques on sound quality, this studyevaluates the inherent sound quality of different pianos. To derive qualityevaluation systems, the study uses subjective questionnaires based on a pianosound quality dataset. The method selects the optimal piano classificationmodels by comparing the fine-tuning results of different pre-training models ofConvolutional Neural Networks (CNN). To improve the interpretability of themodels, the study applies Equivalent Rectangular Bandwidth (ERB) analysis. Theresults reveal that musically trained individuals are better able todistinguish between the sound quality differences of different pianos. The bestfine-tuned CNN pre-trained backbone achieves a high accuracy of 98.3\\% as thepiano classifier. However, the dataset is limited, and the audio is sliced toincrease its quantity, resulting in a lack of diversity and balance, so we usefocal loss to reduce the impact of data imbalance. To optimize the method, thedataset will be expanded, or few-shot learning techniques will be employed infuture research.", "title": "a holistic evaluation of piano sound quality", "url": "http://arxiv.org/pdf/2310.04722v1.pdf", "tokenized_text": "paper aims develop holistic evaluation method assist decisions unlike previous studies effect performance techniques sound quality inherent sound quality different derive systems study uses subjective based quality dataset method selects optimal comparing fine tuning results different pre training neural_networks neural networks cnn improve interpretability themodels study applies equivalent bandwidth analysis theresults reveal trained individuals better able sound quality differences different tuned cnn pre trained backbone achieves high accuracy classifier dataset limited audio sliced quantity resulting lack diversity balance loss reduce impact data imbalance optimize method thedataset expanded shot_learning shot learning techniques employed infuture research"}
{"id": "nan", "abstract": "  To advance argumentative stance prediction as a multimodal problem, the FirstShared Task in Multimodal Argument Mining hosted stance prediction in crucialsocial topics of gun control and abortion. Our exploratory study attempts toevaluate the necessity of images for stance prediction in tweets and compareout-of-the-box text-based large-language models (LLM) in few-shot settingsagainst fine-tuned unimodal and multimodal models. Our work suggests anensemble of fine-tuned text-based language models (0.817 F1-score) outperformsboth the multimodal (0.677 F1-score) and text-based few-shot prediction using arecent state-of-the-art LLM (0.550 F1-score). In addition to the differences inperformance, our findings suggest that the multimodal models tend to performbetter when image content is summarized as natural language over their nativepixel structure and, using in-context examples improves few-shot performance ofLLMs.", "title": "argumentative stance prediction an exploratory study on multimodality and fewshot learning", "url": "http://arxiv.org/pdf/2310.07093v1.pdf", "tokenized_text": "advance stance prediction multimodal problem task multimodal argument mining hosted stance prediction topics control abortion exploratory study attempts toevaluate necessity images stance prediction tweets box text based large language_models language llm shot fine tuned unimodal multimodal work suggests fine tuned text based language_models language f1 score multimodal 0.677 f1 score text based shot prediction arecent state art llm f1 score addition differences inperformance findings_suggest findings suggest multimodal tend performbetter image content summarized natural_language natural language structure context_examples context examples improves shot performance ofllms"}
{"id": "nan", "abstract": "  Finding preferences expressed in natural language is an important butchallenging task. State-of-the-art(SotA) methods leverage transformer-basedmodels such as BERT, RoBERTa, etc. and graph neural architectures such as graphattention networks. Since Large Language Models (LLMs) are equipped to dealwith larger context lengths and have much larger model sizes than thetransformer-based model, we investigate their ability to classify comparativetext directly. This work aims to serve as a first step towards using LLMs forthe CPC task. We design and conduct a set of experiments that format theclassification task into an input prompt for the LLM and a methodology to get afixed-format response that can be automatically evaluated. Comparingperformances with existing methods, we see that pre-trained LLMs are able tooutperform the previous SotA models with no fine-tuning involved. Our resultsshow that the LLMs can consistently outperform the SotA when the target text islarge -- i.e. composed of multiple sentences --, and are still comparable tothe SotA performance in shorter text. We also find that few-shot learningyields better performance than zero-shot learning.", "title": "llmaugmented preference learning from natural language", "url": "http://arxiv.org/pdf/2310.08523v1.pdf", "tokenized_text": "finding preferences expressed natural_language natural language important task state methods leverage transformer basedmodels bert roberta etc graph neural architectures networks large_language large language llms equipped larger context lengths larger sizes thetransformer based investigate ability classify directly work aims serve step llms forthe task design conduct set experiments format theclassification task input llm methodology format response automatically evaluated existing_methods existing methods pre trained llms able previous sota fine tuning involved resultsshow llms consistently outperform sota target text i.e. composed multiple sentences comparable tothe sota performance shorter text find shot better performance zero shot_learning shot learning"}
{"id": "nan", "abstract": "  Relation extraction aims at inferring structured human knowledge from textualdocuments. State-of-the-art methods based on language models commonly have twolimitations: (1) they require named entities to be either given as input orinfer them, which introduces additional noise, and (2) they require humanannotations of documents. As a remedy, we present a novel framework forin-context few-shot relation extraction via pre-trained language models. To thebest of our knowledge, we are the first to reformulate the relation extractiontask as a tailored in-context few-shot learning paradigm. Thereby, we achievecrucial benefits in that we eliminate the need for both named entityrecognition and human annotation of documents. Unlike existing methods based onfine-tuning, our framework is flexible in that it can be easily updated for anew set of relations without re-training. We evaluate our framework usingDocRED, the largest publicly available dataset for document-level relationextraction, and demonstrate that our framework achieves state-of-the-artperformance. Finally, our framework allows us to identify missing annotations,and we thus show that our framework actually performs much better than theoriginal labels from the development set of DocRED.", "title": "incontext fewshot relation extraction via pretrained language models", "url": "http://arxiv.org/pdf/2310.11085v1.pdf", "tokenized_text": "relation_extraction relation extraction aims inferring structured human knowledge state art methods based language_models language commonly require named entities given input introduces additional noise require humanannotations documents remedy present novel framework forin context shot relation_extraction relation extraction pre trained_language trained language thebest knowledge reformulate relation tailored context shot_learning shot learning paradigm benefits eliminate need named entityrecognition human annotation documents unlike existing_methods existing methods based onfine tuning framework flexible easily updated anew set relations training evaluate framework largest publicly_available publicly available dataset document level relationextraction demonstrate framework achieves_state achieves state artperformance finally framework allows identify missing annotations framework actually performs better theoriginal labels development set"}
{"id": "nan", "abstract": "  Many applications of large language models (LLMs), ranging from chatbots tocreative writing, require nuanced subjective judgments that can differsignificantly across different groups. Existing alignment algorithms can beexpensive to align for each group, requiring prohibitive amounts ofgroup-specific preference data and computation for real-world use cases. Weintroduce Group Preference Optimization (GPO), an alignment framework thatsteers language models to preferences of individual groups in a few-shotmanner. In GPO, we augment the base LLM with an independent transformer moduletrained to predict the preferences of a group for the LLM generations. Forfew-shot learning, we parameterize this module as an in-context autoregressivetransformer and train it via meta-learning on several groups. We empiricallyvalidate the efficacy of GPO through rigorous evaluations using LLMs withvaried sizes on three human opinion adaptation tasks. These tasks involveadapting to the preferences of US demographic groups, global countries, andindividual users. Our results demonstrate that GPO not only aligns models moreaccurately but also requires fewer group-specific preferences, and lesstraining and inference computing resources, outperforming existing strategiessuch as in-context steering and fine-tuning methods.", "title": "group preference optimization fewshot alignment of large language models", "url": "http://arxiv.org/pdf/2310.11523v1.pdf", "tokenized_text": "applications large_language large language llms ranging chatbots writing require nuanced subjective judgments different groups existing alignment algorithms align group requiring prohibitive amounts specific preference data computation real world use cases weintroduce group preference optimization alignment framework language_models language preferences individual groups augment base llm independent transformer predict preferences group llm generations forfew shot_learning shot learning parameterize module context train meta learning groups efficacy rigorous evaluations llms sizes human opinion adaptation tasks tasks preferences demographic groups global countries users results_demonstrate results demonstrate aligns requires fewer group specific preferences inference computing resources outperforming existing context steering fine tuning methods"}
{"id": "nan", "abstract": "  Multilingual speech processing requires understanding emotions, a task madedifficult by limited labelled data. CLARA, minimizes reliance on labelled data,enhancing generalization across languages. It excels at fostering sharedrepresentations, aiding cross-lingual transfer of speech and emotions, evenwith little data. Our approach adeptly captures emotional nuances in speech,overcoming subjective assessment issues. Using a large multilingual audiocorpus and self-supervised learning, CLARA develops speech representationsenriched with emotions, advancing emotion-aware multilingual speech processing.  Our method expands the data range using data augmentation, textual embeddingfor visual understanding, and transfers knowledge from high- to low-resourcelanguages. CLARA demonstrates excellent performance in emotion recognition,language comprehension, and audio benchmarks, excelling in zero-shot andfew-shot learning. It adapts to low-resource languages, marking progress inmultilingual speech representation learning.", "title": "clara multilingual contrastive learning for audio representation acquisition", "url": "http://arxiv.org/pdf/2310.11830v2.pdf", "tokenized_text": "multilingual speech processing requires understanding emotions task limited labelled data minimizes reliance labelled data enhancing generalization languages excels fostering aiding cross lingual_transfer lingual transfer speech emotions evenwith little data approach adeptly captures emotional nuances speech overcoming subjective assessment issues large multilingual self supervised learning speech emotions advancing emotion aware multilingual speech processing method expands data range data_augmentation data augmentation textual visual understanding transfers knowledge low resourcelanguages demonstrates excellent performance emotion recognition language comprehension audio benchmarks zero shot andfew shot_learning shot learning adapts low resource_languages resource languages marking progress inmultilingual speech representation learning"}
{"id": "nan", "abstract": "  Recent instruction fine-tuned models can solve multiple NLP tasks whenprompted to do so, with machine translation (MT) being a prominent use case.However, current research often focuses on standard performance benchmarks,leaving compelling fairness and ethical considerations behind. In MT, thismight lead to misgendered translations, resulting, among other harms, in theperpetuation of stereotypes and prejudices. In this work, we address this gapby investigating whether and to what extent such models exhibit gender bias inmachine translation and how we can mitigate it. Concretely, we computeestablished gender bias metrics on the WinoMT corpus from English to German andSpanish. We discover that IFT models default to male-inflected translations,even disregarding female occupational stereotypes. Next, using interpretabilitymethods, we unveil that models systematically overlook the pronoun indicatingthe gender of a target occupation in misgendered translations. Finally, basedon this finding, we propose an easy-to-implement and effective bias mitigationsolution based on few-shot learning that leads to significantly fairertranslations.", "title": "a tale of pronouns interpretability informs gender bias mitigation for fairer instructiontuned machine translation", "url": "http://arxiv.org/pdf/2310.12127v2.pdf", "tokenized_text": "recent instruction fine tuned solve multiple nlp_tasks nlp tasks whenprompted machine_translation machine translation mt prominent use case current research focuses standard performance benchmarks leaving compelling fairness ethical considerations mt lead translations resulting harms stereotypes work address investigating extent exhibit gender bias inmachine translation mitigate concretely gender bias metrics corpus english german discover default male translations disregarding occupational stereotypes unveil systematically overlook gender target translations finally basedon finding propose easy implement effective bias based shot_learning shot learning leads significantly"}
{"id": "nan", "abstract": "  Ever since the development of GPT-3 in the natural language processing (NLP)field, in-context learning (ICL) has played an important role in utilizinglarge language models (LLMs). By presenting the LM utterance-labeldemonstrations at the input, the LM can accomplish few-shot learning withoutrelying on gradient descent or requiring explicit modification of itsparameters. This enables the LM to learn and adapt in a black-box manner.Despite the success of ICL in NLP, little work is exploring the possibility ofICL in speech processing. This study proposes the first exploration of ICL witha speech LM without text supervision. We first show that the current speech LMdoes not have the ICL capability. With the proposed warmup training, the speechLM can, therefore, perform ICL on unseen tasks. In this work, we verify thefeasibility of ICL for speech LM on speech classification tasks.", "title": "an exploration of incontext learning for speech language model", "url": "http://arxiv.org/pdf/2310.12477v1.pdf", "tokenized_text": "development gpt-3 natural_language natural language processing context_learning context learning icl important role utilizinglarge language_models language llms presenting lm utterance input lm accomplish shot_learning shot learning gradient descent requiring explicit modification enables lm learn adapt black box manner despite success icl nlp little work exploring possibility oficl speech processing study proposes exploration icl witha speech lm text supervision current speech icl capability proposed training perform icl unseen tasks work verify thefeasibility icl speech lm speech classification tasks"}
{"id": "nan", "abstract": "  Recent advancements in natural language processing by large language models(LLMs), such as GPT-4, have been suggested to approach Artificial GeneralIntelligence. And yet, it is still under dispute whether LLMs possess similarreasoning abilities to humans. This study evaluates GPT-4 and various otherLLMs in judging the profoundness of mundane, motivational, and pseudo-profoundstatements. We found a significant statement-to-statement correlation betweenthe LLMs and humans, irrespective of the type of statements and the promptingtechnique used. However, LLMs systematically overestimate the profoundness ofnonsensical statements, with the exception of Tk-instruct, which uniquelyunderestimates the profoundness of statements. Only few-shot learning prompts,as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans.Furthermore, this work provides insights into the potential biases induced byReinforcement Learning from Human Feedback (RLHF), inducing an increase in thebias to overestimate the profoundness of statements.", "title": "large language models are biased to overestimate profoundness", "url": "http://arxiv.org/pdf/2310.14422v1.pdf", "tokenized_text": "recent advancements natural_language natural language processing large_language large language models(llms gpt-4 suggested approach artificial generalintelligence llms possess abilities humans study evaluates gpt-4 motivational pseudo found significant statement statement correlation llms humans irrespective type statements promptingtechnique llms systematically statements tk instruct statements shot_learning shot learning opposed chain thought_prompting thought draw llms ratings closer humans furthermore work provides insights potential biases induced learning human feedback rlhf inducing increase thebias statements"}
{"id": "nan", "abstract": "  As large language models (LLMs) are widely adopted, new safety issues andpolicies emerge, to which existing safety classifiers do not generalize well.If we have only observed a few examples of violations of a new safety rule, howcan we build a classifier to detect violations? In this paper, we study thenovel setting of domain-generalized few-shot learning for LLM-based text safetyclassifiers. Unlike prior few-shot work, these new safety issues can be hard touncover and we do not get to choose the few examples. We demonstrate thatexisting few-shot techniques do not perform well in this setting, and rather wepropose to do parameter-efficient fine-tuning (PEFT) combined with augmentingtraining data based on similar examples in prior existing rules. We empiricallyshow that our approach of similarity-based data-augmentation + prompt-tuning(DAPT) consistently outperforms baselines that either do not rely on dataaugmentation or on PEFT by 7-17% F1 score in the Social Chemistry moraljudgement and 9-13% AUC in the Toxicity detection tasks, even when the new ruleis loosely correlated with existing ones.", "title": "improving fewshot generalization of safety classifiers via data augmented parameterefficient finetuning", "url": "http://arxiv.org/pdf/2310.16959v1.pdf", "tokenized_text": "large_language large language llms widely adopted new safety issues emerge existing safety classifiers generalize observed examples violations new safety rule build classifier detect violations paper study thenovel setting domain generalized shot_learning shot learning llm based text unlike prior shot work new safety issues hard choose examples demonstrate shot techniques perform setting wepropose parameter efficient fine tuning peft combined data based similar examples prior existing rules approach similarity based data augmentation consistently_outperforms consistently outperforms baselines rely dataaugmentation peft 17 f1_score f1 score social chemistry 13 auc toxicity detection tasks new correlated existing ones"}
{"id": "nan", "abstract": "  We present a novel retrofitting method to induce emotion aspects intopre-trained language models (PLMs) such as BERT and RoBERTa. Our method updatespre-trained network weights using contrastive learning so that the textfragments exhibiting similar emotions are encoded nearby in the representationspace, and the fragments with different emotion content are pushed apart. Whiledoing so, it also ensures that the linguistic knowledge already present in PLMsis not inadvertently perturbed. The language models retrofitted by our method,i.e., BERTEmo and RoBERTaEmo, produce emotion-aware text representations, asevaluated through different clustering and retrieval metrics. For thedownstream tasks on sentiment analysis and sarcasm detection, they performbetter than their pre-trained counterparts (about 1% improvement in F1-score)and other existing approaches. Additionally, a more significant boost inperformance is observed for the retrofitted models over pre-trained ones infew-shot learning setting.", "title": "retrofitting lightweight language models for emotions using supervised contrastive learning", "url": "http://arxiv.org/pdf/2310.18930v1.pdf", "tokenized_text": "present novel retrofitting method induce emotion aspects trained_language trained language plms bert roberta method trained network weights contrastive_learning contrastive learning exhibiting similar emotions encoded fragments different emotion content pushed apart ensures linguistic knowledge present inadvertently perturbed language_models language method i.e. produce emotion aware text representations different clustering retrieval metrics thedownstream tasks sentiment_analysis sentiment analysis sarcasm detection performbetter pre trained counterparts improvement f1 existing approaches additionally significant boost inperformance observed pre trained ones infew shot_learning shot learning setting"}
{"id": "nan", "abstract": "  The spread of disinformation and propagandistic content poses a threat tosocietal harmony, undermining informed decision-making and trust in reliablesources. Online platforms often serve as breeding grounds for such content, andmalicious actors exploit the vulnerabilities of audiences to shape publicopinion. Although there have been research efforts aimed at the automaticidentification of disinformation and propaganda in social media content, thereremain challenges in terms of performance. The ArAIEval shared task aims tofurther research on these particular issues within the context of the Arabiclanguage. In this paper, we discuss our participation in these shared tasks. Wecompeted in subtasks 1A and 2A, where our submitted system secured positions9th and 10th, respectively. Our experiments consist of fine-tuning transformermodels and using zero- and few-shot learning with GPT-4.", "title": "nexus at araieval shared task finetuning arabic language models for propaganda and disinformation detection", "url": "http://arxiv.org/pdf/2311.03184v1.pdf", "tokenized_text": "spread content poses threat harmony informed decision making trust online platforms serve breeding grounds content actors exploit vulnerabilities audiences shape publicopinion research efforts aimed propaganda social_media social media content challenges terms performance shared task aims tofurther research particular issues context paper discuss participation shared tasks subtasks submitted system respectively experiments consist fine tuning transformermodels zero- shot_learning shot learning gpt-4"}
{"id": "nan", "abstract": "  Autoformalization is the task of translating natural language materials intomachine-verifiable formalisations. Progress in autoformalization research ishindered by the lack of a sizeable dataset consisting of informal-formal pairsexpressing the same essence. Existing methods tend to circumvent this challengeby manually curating small corpora or using few-shot learning with largelanguage models. But these methods suffer from data scarcity and formallanguage acquisition difficulty. In this work, we create $\\texttt{MMA}$, alarge, flexible, multilingual, and multi-domain dataset of informal-formalpairs, by using a language model to translate in the reverse direction, thatis, from formal mathematical statements into corresponding informal ones.Experiments show that language models fine-tuned on $\\texttt{MMA}$ produce$16-18\\%$ of statements acceptable with minimal corrections on the$\\texttt{miniF2F}$ and $\\texttt{ProofNet}$ benchmarks, up from $0\\%$ with thebase model. We demonstrate that fine-tuning on multilingual formal data resultsin more capable autoformalization models even when deployed on monolingualtasks.", "title": "multilingual mathematical autoformalization", "url": "http://arxiv.org/pdf/2311.03755v2.pdf", "tokenized_text": "task translating natural_language natural language materials progress research ishindered lack sizeable dataset consisting informal formal essence existing_methods existing methods tend circumvent manually curating small corpora shot_learning shot learning largelanguage_models largelanguage methods suffer data scarcity acquisition difficulty work create alarge flexible multilingual multi domain dataset informal language_model language translate reverse direction thatis formal mathematical statements corresponding informal ones experiments language_models language fine tuned statements acceptable minimal corrections benchmarks thebase demonstrate fine tuning multilingual formal data capable deployed"}
{"id": "nan", "abstract": "  Two main routes of learning methods exist at present including error-drivenglobal learning and neuroscience-oriented local learning. Integrating them intoone network may provide complementary learning capabilities for versatilelearning scenarios. At the same time, neuromorphic computing holds greatpromise, but still needs plenty of useful algorithms and algorithm-hardwareco-designs for exploiting the advantages. Here, we report a neuromorphic hybridlearning model by introducing a brain-inspired meta-learning paradigm and adifferentiable spiking model incorporating neuronal dynamics and synapticplasticity. It can meta-learn local plasticity and receive top-down supervisioninformation for multiscale synergic learning. We demonstrate the advantages ofthis model in multiple different tasks, including few-shot learning, continuallearning, and fault-tolerance learning in neuromorphic vision sensors. Itachieves significantly higher performance than single-learning methods, andshows promise in empowering neuromorphic applications revolution. We furtherimplemented the hybrid model in the Tianjic neuromorphic platform by exploitingalgorithm-hardware co-designs and proved that the model can fully utilizeneuromorphic many-core architecture to develop hybrid computation paradigm.", "title": "braininspired globallocal learning incorporated with neuromorphic computing", "url": "http://arxiv.org/pdf/2006.03226v3.pdf", "tokenized_text": "main learning methods exist present including error learning neuroscience oriented local learning integrating network provide complementary learning capabilities scenarios time computing holds greatpromise needs plenty useful algorithms algorithm designs exploiting advantages report introducing brain inspired meta learning paradigm incorporating dynamics meta learn local receive learning demonstrate advantages ofthis multiple different tasks including shot_learning shot learning fault learning vision sensors significantly higher performance single learning methods promise empowering applications revolution hybrid platform hardware co designs proved fully core architecture develop hybrid computation paradigm"}
{"id": "nan", "abstract": "  GPT-$3$ has attracted lots of attention due to its superior performanceacross a wide range of NLP tasks, especially with its powerful and versatilein-context few-shot learning ability. Despite its success, we found that theempirical results of GPT-$3$ depend heavily on the choice of in-contextexamples. In this work, we investigate whether there are more effectivestrategies for judiciously selecting in-context examples (relative to randomsampling) that better leverage GPT-$3$'s few-shot capabilities. Inspired by therecent success of leveraging a retrieval module to augment large-scale neuralnetwork models, we propose to retrieve examples that are semantically-similarto a test sample to formulate its corresponding prompt. Intuitively, thein-context examples selected with such a strategy may serve as more informativeinputs to unleash GPT-$3$'s extensive knowledge. We evaluate the proposedapproach on several natural language understanding and generation benchmarks,where the retrieval-based prompt selection approach consistently outperformsthe random baseline. Moreover, it is observed that the sentence encodersfine-tuned on task-related datasets yield even more helpful retrieval results.Notably, significant gains are observed on tasks such as table-to-textgeneration (41.9% on the ToTTo dataset) and open-domain question answering(45.5% on the NQ dataset). We hope our investigation could help understand thebehaviors of GPT-$3$ and large-scale pre-trained LMs in general and enhancetheir few-shot capabilities.", "title": "what makes good incontext examples for gpt$3$", "url": "http://arxiv.org/pdf/2101.06804v1.pdf", "tokenized_text": "attracted lots attention superior wide_range wide range nlp_tasks nlp tasks especially powerful context shot_learning shot learning ability despite success found theempirical results depend heavily choice contextexamples work investigate judiciously selecting context_examples context examples relative better leverage shot capabilities inspired success leveraging retrieval module augment large scale propose retrieve examples semantically similarto test sample formulate corresponding intuitively thein context_examples context examples selected strategy serve unleash extensive knowledge evaluate proposedapproach natural_language natural language understanding generation benchmarks retrieval based selection approach consistently outperformsthe random baseline observed sentence tuned task related datasets yield helpful retrieval results notably significant gains observed tasks table textgeneration dataset open domain question dataset hope investigation help understand large scale pre trained lms general shot capabilities"}
{"id": "nan", "abstract": "  Automatically inducing high quality knowledge graphs from a given collectionof documents still remains a challenging problem in AI. One way to make headwayfor this problem is through advancements in a related task known as slotfilling. In this task, given an entity query in form of [Entity, Slot, ?], asystem is asked to fill the slot by generating or extracting the missing valueexploiting evidence extracted from relevant passage(s) in the given documentcollection. The recent works in the field try to solve this task in anend-to-end fashion using retrieval-based language models. In this paper, wepresent a novel approach to zero-shot slot filling that extends dense passageretrieval with hard negatives and robust training procedures for retrievalaugmented generation models. Our model reports large improvements on both T-RExand zsRE slot filling datasets, improving both passage retrieval and slot valuegeneration, and ranking at the top-1 position in the KILT leaderboard.Moreover, we demonstrate the robustness of our system showing its domainadaptation capability on a new variant of the TACRED dataset for slot filling,through a combination of zero/few-shot learning. We release the source code andpre-trained models.", "title": "robust retrieval augmented generation for zeroshot slot filling", "url": "http://arxiv.org/pdf/2108.13934v2.pdf", "tokenized_text": "automatically inducing high_quality high quality knowledge graphs given documents remains challenging problem ai way problem advancements related task known task given entity query form entity slot asked fill slot generating extracting missing evidence extracted relevant given recent works field try solve task anend end fashion retrieval based language_models language paper wepresent novel_approach novel approach zero shot slot filling extends dense hard robust training procedures generation reports large improvements slot filling datasets improving passage retrieval slot ranking top-1 position kilt leaderboard demonstrate robustness system showing capability new variant tacred dataset slot filling combination zero shot_learning shot learning release source_code source code trained"}
{"id": "nan", "abstract": "  Goal-oriented dialogue systems are now being widely adopted in industry whereit is of key importance to maintain a rapid prototyping cycle for new productsand domains. Data-driven dialogue system development has to be adapted to meetthis requirement --- therefore, reducing the amount of data and annotationsnecessary for training such systems is a central research problem.  In this paper, we present the Dialogue Knowledge Transfer Network (DiKTNet),a state-of-the-art approach to goal-oriented dialogue generation which onlyuses a few example dialogues (i.e. few-shot learning), none of which has to beannotated. We achieve this by performing a 2-stage training. Firstly, weperform unsupervised dialogue representation pre-training on a large source ofgoal-oriented dialogues in multiple domains, the MetaLWOz corpus. Secondly, atthe transfer stage, we train DiKTNet using this representation together with 2other textual knowledge sources with different levels of generality: ELMoencoder and the main dataset's source domains.  Our main dataset is the Stanford Multi-Domain dialogue corpus. We evaluateour model on it in terms of BLEU and Entity F1 scores, and show that ourapproach significantly and consistently improves upon a series of baselinemodels as well as over the previous state-of-the-art dialogue generation model,ZSDG. The improvement upon the latter --- up to 10% in Entity F1 and theaverage of 3% in BLEU score --- is achieved using only the equivalent of 10% ofZSDG's in-domain training data.", "title": "dataefficient goaloriented conversation with dialogue knowledge transfer networks", "url": "http://arxiv.org/pdf/1910.01302v1.pdf", "tokenized_text": "goal oriented dialogue systems widely adopted industry key importance maintain rapid prototyping cycle new domains data driven dialogue system development adapted requirement reducing data training systems central research problem paper present dialogue knowledge transfer network state art approach goal oriented dialogue generation example dialogues i.e. shot_learning shot learning achieve performing stage training firstly weperform unsupervised dialogue representation pre training large source oriented dialogues multiple domains corpus secondly atthe transfer stage train representation textual knowledge sources different levels generality main dataset source domains main dataset multi domain dialogue corpus evaluateour terms bleu entity f1 scores ourapproach significantly consistently improves series previous state art dialogue generation improvement 10 entity f1 bleu score achieved equivalent 10 domain training_data training data"}
{"id": "nan", "abstract": "  Event detection (ED), a sub-task of event extraction, involves identifyingtriggers and categorizing event mentions. Existing methods primarily rely uponsupervised learning and require large-scale labeled event datasets which areunfortunately not readily available in many real-life applications. In thispaper, we consider and reformulate the ED task with limited labeled data as aFew-Shot Learning problem. We propose a Dynamic-Memory-Based PrototypicalNetwork (DMB-PN), which exploits Dynamic Memory Network (DMN) to not only learnbetter prototypes for event types, but also produce more robust sentenceencodings for event mentions. Differing from vanilla prototypical networkssimply computing event prototypes by averaging, which only consume eventmentions once, our model is more robust and is capable of distilling contextualinformation from event mentions for multiple times due to the multi-hopmechanism of DMNs. The experiments show that DMB-PN not only deals with samplescarcity better than a series of baseline models but also performs morerobustly when the variety of event types is relatively large and the instancequantity is extremely small.", "title": "metalearning with dynamicmemorybased prototypical network for fewshot event detection", "url": "http://arxiv.org/pdf/1910.11621v2.pdf", "tokenized_text": "event detection ed sub task event extraction involves categorizing event mentions existing_methods existing methods primarily rely learning require large scale labeled event datasets readily available real life applications thispaper consider reformulate ed task limited labeled_data labeled data afew shot_learning shot learning problem propose dynamic memory based prototypicalnetwork exploits dynamic memory network prototypes event types produce robust event mentions vanilla prototypical computing event prototypes consume robust capable distilling event mentions multiple times multi experiments better series baseline performs variety event types relatively large extremely small"}
{"id": "nan", "abstract": "  The evolution of drug-resistant microbial species is one of the majorchallenges to global health. The development of new antimicrobial treatmentssuch as antimicrobial peptides needs to be accelerated to combat this threat.However, the discovery of novel antimicrobial peptides is hampered bylow-throughput biochemical assays. Computational techniques can be used forrapid screening of promising antimicrobial peptide candidates prior to testingin the wet lab. The vast majority of existing antimicrobial peptide predictorsare non-targeted in nature, i.e., they can predict whether a given peptidesequence is antimicrobial, but they are unable to predict whether the sequencecan target a particular microbial species. In this work, we have developed atargeted antimicrobial peptide activity predictor that can predict whether apeptide is effective against a given microbial species or not. This has beenmade possible through zero-shot and few-shot machine learning. The proposedpredictor called AMP0 takes in the peptide amino acid sequence and anyN/C-termini modifications together with the genomic sequence of a targetmicrobial species to generate targeted predictions. It is important to notethat the proposed method can generate predictions for species that are not partof its training set. The accuracy of predictions for novel test species can befurther improved by providing a few example peptides for that species. Ourcomputational cross-validation results show that the pro-posed scheme isparticularly effective for targeted antimicrobial prediction in comparison toexisting approaches and can be used for screening potential antimicrobialpeptides in a targeted manner especially for cases in which the number oftraining examples is small. The webserver of the method is available athttp://ampzero.pythonanywhere.com.", "title": "amp0 speciesspecific prediction of antimicrobial peptides using zero and few shot learning", "url": "http://arxiv.org/pdf/1911.06106v1.pdf", "tokenized_text": "evolution drug resistant global health development new needs accelerated combat threat discovery novel hampered computational techniques screening promising candidates prior lab vast majority existing non targeted nature i.e. predict given unable predict target particular work developed activity predictor predict effective given beenmade possible zero shot shot machine_learning machine learning called takes acid sequence modifications genomic sequence generate targeted predictions important proposed_method proposed method generate predictions training set accuracy predictions novel test improved providing example cross validation results pro posed scheme effective targeted prediction comparison toexisting approaches screening potential targeted manner especially cases number oftraining examples small method available"}
{"id": "nan", "abstract": "  We propose direct multimodal few-shot models that learn a shared embeddingspace of spoken words and images from only a few paired examples. Imagine anagent is shown an image along with a spoken word describing the object in thepicture, e.g. pen, book and eraser. After observing a few paired examples ofeach class, the model is asked to identify the \"book\" in a set of unseenpictures. Previous work used a two-step indirect approach relying on learnedunimodal representations: speech-speech and image-image comparisons areperformed across the support set of given speech-image pairs. We propose twodirect models which instead learn a single multimodal space where inputs fromdifferent modalities are directly comparable: a multimodal triplet network(MTriplet) and a multimodal correspondence autoencoder (MCAE). To train thesedirect models, we mine speech-image pairs: the support set is used to pair upunlabelled in-domain speech and images. In a speech-to-image digit matchingtask, direct models outperform indirect models, with the MTriplet achieving thebest multimodal five-shot accuracy. We show that the improvements are due tothe combination of unsupervised and transfer learning in the direct models, andthe absence of two-step compounding errors.", "title": "direct multimodal fewshot learning of speech and images", "url": "http://arxiv.org/pdf/2012.05680v2.pdf", "tokenized_text": "propose direct multimodal shot learn shared spoken words images paired examples anagent shown image spoken word describing object e.g. pen book observing paired examples class asked identify book set previous work step indirect approach relying representations speech speech image image comparisons support set given speech image pairs propose instead learn single multimodal space inputs modalities directly comparable multimodal triplet multimodal correspondence autoencoder train speech image pairs support set pair domain speech images speech image direct outperform indirect achieving thebest multimodal shot accuracy improvements tothe combination unsupervised transfer learning direct andthe absence step errors"}
{"id": "nan", "abstract": "  Semantic segmentation of road scenes is one of the key technologies forrealizing autonomous driving scene perception, and the effectiveness of deepConvolutional Neural Networks(CNNs) for this task has been demonstrated.State-of-art CNNs for semantic segmentation suffer from excessive computationsas well as large-scale training data requirement. Inspired by the ideas ofFine-tuning-based Transfer Learning (FTT) and feature-based knowledgedistillation, we propose a new knowledge distillation method for cross-domainknowledge transference and efficient data-insufficient network training, namedSpirit Distillation(SD), which allow the student network to mimic the teachernetwork to extract general features, so that a compact and accurate studentnetwork can be trained for real-time semantic segmentation of road scenes.Then, in order to further alleviate the trouble of insufficient data andimprove the robustness of the student, an Enhanced Spirit Distillation (ESD)method is proposed, which commits to exploit a more comprehensive generalfeatures extraction capability by considering images from both the target andthe proximity domains as input. To our knowledge, this paper is a pioneeringwork on the application of knowledge distillation to few-shot learning.Persuasive experiments conducted on Cityscapes semantic segmentation with theprior knowledge transferred from COCO2017 and KITTI demonstrate that ourmethods can train a better student network (mIOU and high-precision accuracyboost by 1.4% and 8.2% respectively, with 78.2% segmentation variance) withonly 41.8% FLOPs (see Fig. 1).", "title": "spirit distillation precise realtime semantic segmentation of road scenes with insufficient data", "url": "http://arxiv.org/pdf/2103.13733v2.pdf", "tokenized_text": "semantic segmentation road scenes key technologies autonomous driving scene perception effectiveness neural task demonstrated state art semantic segmentation suffer excessive large scale training_data training data requirement inspired ideas offine tuning based transfer learning feature based knowledgedistillation propose_a_new propose new knowledge_distillation knowledge distillation method cross domainknowledge efficient data insufficient network training allow student network mimic extract general features compact accurate trained real time semantic segmentation road scenes order alleviate insufficient data andimprove robustness student enhanced spirit distillation proposed commits exploit comprehensive extraction capability considering images target andthe domains input knowledge paper application knowledge_distillation knowledge distillation shot_learning shot learning persuasive experiments conducted semantic segmentation knowledge transferred demonstrate train better student network high precision 8.2 respectively segmentation variance withonly flops"}
{"id": "nan", "abstract": "  While achieving state-of-the-art results in multiple tasks and languages,translation-based cross-lingual transfer is often overlooked in favour ofmassively multilingual pre-trained encoders. Arguably, this is due to its mainlimitations: 1) translation errors percolating to the classification phase and2) the insufficient expressiveness of the maximum-likelihood translation. Toremedy this, we propose a new technique that integrates both steps of thetraditional pipeline (translation and classification) into a single model, bytreating the intermediate translations as a latent random variable. As aresult, 1) the neural machine translation system can be fine-tuned with avariant of Minimum Risk Training where the reward is the accuracy of thedownstream task classifier. Moreover, 2) multiple samples can be drawn toapproximate the expected loss across all possible translations duringinference. We evaluate our novel latent translation-based model on a series ofmultilingual NLU tasks, including commonsense reasoning, paraphraseidentification, and natural language inference. We report gains for bothzero-shot and few-shot learning setups, up to 2.7 accuracy points on average,which are even more prominent for low-resource languages (e.g., HaitianCreole). Finally, we carry out in-depth analyses comparing different underlyingNMT models and assessing the impact of alternative translations on thedownstream performance.", "title": "modelling latent translations for crosslingual transfer", "url": "http://arxiv.org/pdf/2107.11353v1.pdf", "tokenized_text": "achieving state art results multiple tasks languages translation based cross lingual_transfer lingual transfer overlooked multilingual pre trained encoders arguably translation errors classification phase and2 insufficient expressiveness maximum likelihood translation toremedy propose_a_new propose new technique integrates steps pipeline translation classification single intermediate translations latent random variable neural machine_translation machine translation system fine tuned minimum risk training reward accuracy thedownstream task classifier multiple samples drawn expected loss possible translations duringinference evaluate novel latent translation based series ofmultilingual nlu tasks including commonsense reasoning natural_language natural language inference report gains bothzero shot shot_learning shot learning setups 2.7 accuracy points average prominent low resource_languages resource languages e.g. finally carry depth analyses comparing different assessing impact alternative translations thedownstream performance"}
{"id": "nan", "abstract": "  High-quality computer science education is limited by the difficulty ofproviding instructor feedback to students at scale. While this feedback couldin principle be automated, supervised approaches to predicting the correctfeedback are bottlenecked by the intractability of annotating large quantitiesof student code. In this paper, we instead frame the problem of providingfeedback as few-shot classification, where a meta-learner adapts to givefeedback to student code on a new programming question from just a few examplesannotated by instructors. Because data for meta-training is limited, we proposea number of amendments to the typical few-shot learning framework, includingtask augmentation to create synthetic tasks, and additional side information tobuild stronger priors about each task. These additions are combined with atransformer architecture to embed discrete sequences (e.g. code) to aprototypical representation of a feedback class label. On a suite of few-shotnatural language processing tasks, we match or outperform state-of-the-artperformance. Then, on a collection of student solutions to exam questions froman introductory university course, we show that our approach reaches an averageprecision of 88% on unseen questions, surpassing the 82% precision of teachingassistants. Our approach was successfully deployed to deliver feedback to16,000 student exam-solutions in a programming course offered by a tier 1university. This is, to the best of our knowledge, the first successfuldeployment of a machine learning based feedback to open-ended student code.", "title": "prototransformer a metalearning approach to providing student feedback", "url": "http://arxiv.org/pdf/2107.14035v2.pdf", "tokenized_text": "high quality computer science education limited difficulty instructor feedback students scale feedback principle automated supervised approaches predicting bottlenecked annotating large student code paper instead frame problem shot classification meta learner adapts student code new programming question instructors data meta training limited proposea number typical shot_learning shot learning framework augmentation create synthetic tasks additional information tobuild stronger priors task combined atransformer architecture discrete sequences e.g. code representation feedback class label suite language_processing language processing tasks match outperform state artperformance collection student solutions exam questions introductory university course approach reaches 88 unseen questions surpassing precision approach successfully deployed deliver feedback student exam solutions programming course offered tier best knowledge machine_learning machine learning based feedback open ended student code"}
{"id": "nan", "abstract": "  Large pre-trained language models have shown promise for few-shot learning,completing text-based tasks given only a few task-specific examples. Willmodels soon solve classification tasks that have so far been reserved for humanresearch assistants? Existing benchmarks are not designed to measure progressin applied settings, and so don't directly answer this question. The RAFTbenchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurringtasks and uses an evaluation setup that mirrors deployment. Baselineevaluations on RAFT reveal areas current techniques struggle with: reasoningover long texts and tasks with many classes. Human baselines show that someclassification tasks are difficult for non-expert humans, reflecting thatreal-world value sometimes depends on domain expertise. Yet even non-experthuman baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasetsand leaderboard will track which model improvements translate into real-worldbenefits at https://raft.elicit.org .", "title": "raft a realworld fewshot text classification benchmark", "url": "http://arxiv.org/pdf/2109.14076v3.pdf", "tokenized_text": "large pre trained_language trained language shown promise shot_learning shot learning completing text based tasks given task specific examples soon solve classification tasks far reserved assistants existing benchmarks designed measure applied settings directly answer question real world annotated few-shot tasks focuses naturally uses evaluation setup deployment raft reveal areas current techniques struggle reasoningover long texts tasks classes human baselines tasks difficult non expert humans reflecting world value depends domain expertise non experthuman baseline f1 scores exceed gpt-3 average raft datasetsand leaderboard track improvements translate real"}
{"id": "nan", "abstract": "  Existing approaches to lifelong language learning rely on plenty of labeleddata for learning a new task, which is hard to obtain in most real scenarios.Considering that humans can continually learn new tasks from a handful ofexamples, we expect the models also to be able to generalize well on newfew-shot tasks without forgetting the previous ones. In this work, we definethis more challenging yet practical problem as Lifelong Few-shot LanguageLearning (LFLL) and propose a unified framework for it based on prompt tuningof T5. Our framework called LFPT5 takes full advantage of PT's strong few-shotlearning ability, and simultaneously trains the model as a task solver and adata generator. Before learning a new domain of the same task type, LFPT5generates pseudo (labeled) samples of previously learned domains, and latergets trained on those samples to alleviate forgetting of previous knowledge asit learns the new domain. In addition, a KL divergence loss is minimized toachieve label consistency between the previous and the current model. Whileadapting to a new task type, LFPT5 includes and tunes additional promptembeddings for the new task. With extensive experiments, we demonstrate thatLFPT5 can be applied to various different types of tasks and significantlyoutperform previous methods in different LFLL settings.", "title": "lfpt5 a unified framework for lifelong fewshot language learning based on prompt tuning of t5", "url": "http://arxiv.org/pdf/2110.07298v3.pdf", "tokenized_text": "existing approaches language learning rely plenty labeleddata learning new task hard obtain real scenarios considering humans continually learn new tasks handful ofexamples expect able generalize shot tasks forgetting previous ones work challenging practical problem few-shot languagelearning propose unified framework based t5 framework called takes advantage pt strong shotlearning ability simultaneously trains task solver adata generator learning new domain task type pseudo labeled samples previously learned domains trained samples alleviate forgetting previous knowledge learns new domain addition loss minimized toachieve label consistency previous current new task type includes tunes additional new task extensive_experiments extensive experiments demonstrate applied different types tasks significantlyoutperform previous methods different settings"}
{"id": "nan", "abstract": "  We introduce MetaICL (Meta-training for In-Context Learning), a newmeta-training framework for few-shot learning where a pretrained language modelis tuned to do in-context learning on a large set of training tasks. Thismeta-training enables the model to more effectively learn a new task in contextat test time, by simply conditioning on a few training examples with noparameter updates or task-specific templates. We experiment on a large, diversecollection of tasks consisting of 142 NLP datasets including classification,question answering, natural language inference, paraphrase detection and more,across seven different meta-training/target splits. MetaICL outperforms a rangeof baselines including in-context learning without meta-training and multi-tasklearning followed by zero-shot transfer. We find that the gains areparticularly significant for target tasks that have domain shifts from themeta-training tasks, and that using a diverse set of the meta-training tasks iskey to improvements. We also show that MetaICL approaches (and sometimes beats)the performance of models fully finetuned on the target task, and outperformsmuch bigger models with nearly 8x parameters. Finally, we show that MetaICL iscomplementary to human-written instructions, and the best performance can beachieved by combining both approaches.", "title": "metaicl learning to learn in context", "url": "http://arxiv.org/pdf/2110.15943v2.pdf", "tokenized_text": "introduce metaicl meta training context_learning context learning training framework shot_learning shot learning pretrained_language pretrained language modelis tuned context_learning context learning large set training tasks training enables effectively learn new task test_time test time simply conditioning training_examples training examples updates task specific templates experiment large tasks consisting nlp datasets including classification question_answering question answering natural_language natural language inference paraphrase detection seven different meta training target splits metaicl outperforms rangeof baselines including context_learning context learning meta training multi tasklearning followed zero shot transfer find gains significant target tasks domain shifts themeta training tasks diverse set meta training tasks improvements metaicl approaches performance fully finetuned target task bigger nearly 8x parameters finally metaicl human written instructions best performance beachieved combining approaches"}
{"id": "nan", "abstract": "  With 4.5 million hours of English speech from 10 different sources across 120countries and models of up to 10 billion parameters, we explore the frontiersof scale for automatic speech recognition. We propose data selection techniquesto efficiently scale training data to find the most valuable samples in massivedatasets. To efficiently scale model sizes, we leverage various optimizationssuch as sparse transducer loss and model sharding. By training 1-10B parameteruniversal English ASR models, we push the limits of speech recognitionperformance across many domains. Furthermore, our models learn powerful speechrepresentations with zero and few-shot capabilities on novel domains and stylesof speech, exceeding previous results across multiple in-house and publicbenchmarks. For speakers with disorders due to brain damage, our best zero-shotand few-shot models achieve 22% and 60% relative improvement on the AphasiaBanktest set, respectively, while realizing the best performance on public socialmedia videos. Furthermore, the same universal model reaches equivalentperformance with 500x less in-domain data on the SPGISpeech financial-domaindataset.", "title": "scaling asr improves zero and few shot learning", "url": "http://arxiv.org/pdf/2111.05948v3.pdf", "tokenized_text": "million hours english speech 10 different sources 10 billion parameters explore scale automatic speech recognition propose data selection efficiently scale training_data training data find valuable samples massivedatasets efficiently scale sizes leverage sparse loss training english asr push limits speech domains furthermore learn powerful zero shot capabilities novel domains speech exceeding previous results multiple publicbenchmarks speakers disorders brain damage best zero shotand shot achieve 22 60 relative improvement set respectively realizing best performance public socialmedia videos furthermore universal reaches 500x domain data financial"}
{"id": "nan", "abstract": "  Recently, zero-shot and few-shot learning via Contrastive Vision-LanguagePre-training (CLIP) have shown inspirational performance on 2D visualrecognition, which learns to match images with their corresponding texts inopen-vocabulary settings. However, it remains under explored that whether CLIP,pre-trained by large-scale image-text pairs in 2D, can be generalized to 3Drecognition. In this paper, we identify such a setting is feasible by proposingPointCLIP, which conducts alignment between CLIP-encoded point cloud and 3Dcategory texts. Specifically, we encode a point cloud by projecting it intomulti-view depth maps without rendering, and aggregate the view-wise zero-shotprediction to achieve knowledge transfer from 2D to 3D. On top of that, wedesign an inter-view adapter to better extract the global feature andadaptively fuse the few-shot knowledge learned from 3D into CLIP pre-trained in2D. By just fine-tuning the lightweight adapter in the few-shot settings, theperformance of PointCLIP could be largely improved. In addition, we observe thecomplementary property between PointCLIP and classical 3D-supervised networks.By simple ensembling, PointCLIP boosts baseline's performance and evensurpasses state-of-the-art models. Therefore, PointCLIP is a promisingalternative for effective 3D point cloud understanding via CLIP under lowresource cost and data regime. We conduct thorough experiments onwidely-adopted ModelNet10, ModelNet40 and the challenging ScanObjectNN todemonstrate the effectiveness of PointCLIP. The code is released athttps://github.com/ZrrSkywalker/PointCLIP.", "title": "pointclip point cloud understanding by clip", "url": "http://arxiv.org/pdf/2112.02413v1.pdf", "tokenized_text": "recently zero shot shot_learning shot learning contrastive training clip shown performance 2d learns match images corresponding texts inopen vocabulary settings remains explored clip pre trained large scale image text pairs 2d generalized paper identify setting feasible conducts alignment clip encoded point_cloud point cloud texts specifically encode point_cloud point cloud projecting view depth maps rendering aggregate view wise zero achieve knowledge transfer 2d wedesign inter view adapter better extract global feature fuse shot knowledge learned 3d clip pre trained fine tuning lightweight adapter shot_settings shot settings theperformance pointclip largely improved addition observe thecomplementary property pointclip classical 3d supervised networks simple ensembling pointclip boosts baseline performance state art pointclip effective 3d point_cloud point cloud understanding clip lowresource cost data regime conduct thorough experiments adopted challenging todemonstrate effectiveness pointclip code released"}
{"id": "nan", "abstract": "  This paper presents a comprehensive survey of vision-language (VL)intelligence from the perspective of time. This survey is inspired by theremarkable progress in both computer vision and natural language processing,and recent trends shifting from single modality processing to multiple modalitycomprehension. We summarize the development in this field into three timeperiods, namely task-specific methods, vision-language pre-training (VLP)methods, and larger models empowered by large-scale weakly-labeled data. Wefirst take some common VL tasks as examples to introduce the development oftask-specific methods. Then we focus on VLP methods and comprehensively reviewkey components of the model structures and training methods. After that, weshow how recent work utilizes large-scale raw image-text data to learnlanguage-aligned visual representations that generalize better on zero or fewshot learning tasks. Finally, we discuss some potential future trends towardsmodality cooperation, unified representation, and knowledge incorporation. Webelieve that this review will be of help for researchers and practitioners ofAI and ML, especially those interested in computer vision and natural languageprocessing.", "title": "visionlanguage intelligence tasks, representation learning, and large models", "url": "http://arxiv.org/pdf/2203.01922v1.pdf", "tokenized_text": "paper_presents paper presents comprehensive survey vision language perspective time survey inspired progress computer_vision computer vision natural_language natural language processing recent trends shifting single modality processing multiple summarize development field task specific methods vision language pre training larger empowered large scale weakly labeled_data labeled data common vl tasks examples introduce development oftask specific methods focus vlp methods comprehensively components structures training methods weshow recent_work recent work utilizes large scale raw image text data aligned visual representations generalize better zero fewshot learning tasks finally discuss potential future trends cooperation unified representation knowledge incorporation review help researchers practitioners ml especially interested computer_vision computer vision natural languageprocessing"}
{"id": "nan", "abstract": "  Despite achieving state-of-the-art zero-shot performance, existingvision-language models still fall short of few-shot transfer ability ondomain-specific problems. Classical fine-tuning often fails to prevent highlyexpressive models from exploiting spurious correlations. Althoughmodel-agnostic meta-learning (MAML) presents as a natural alternative forfew-shot transfer learning, the expensive computation due to implicitsecond-order optimization limits its use on large-scale vision-language modelssuch as CLIP. While much literature has been devoted to exploring alternativeoptimization strategies, we identify another essential aspect towards effectivefew-shot transfer learning, task sampling, which is previously only be viewedas part of data pre-processing in MAML. To show the impact of task sampling, wepropose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), whichdifferentiates classical fine-tuning only on uniformly sampling multiple tasks.Despite its simplicity, we show that MAMF consistently outperforms classicalfine-tuning on five few-shot vision-language classification tasks. We furthershow that the effectiveness of the bi-level optimization in MAML is highlysensitive to the zero-shot performance of a task in the context of few-shotvision-language classification. The goal of this paper is to provide newinsights on what makes few-shot learning work, and encourage more research intoinvestigating better task sampling strategies.", "title": "rethinking task sampling for fewshot visionlanguage transfer learning", "url": "http://arxiv.org/pdf/2203.04904v3.pdf", "tokenized_text": "despite achieving state art zero shot performance language_models language fall short shot transfer ability ondomain specific problems classical fine tuning fails prevent exploiting spurious correlations agnostic meta learning maml presents natural alternative forfew shot transfer learning expensive computation order optimization limits use large scale vision language modelssuch clip literature devoted exploring strategies identify essential aspect shot transfer learning task sampling previously data pre processing maml impact task sampling wepropose simple algorithm agnostic multitask fine-tuning classical fine tuning uniformly sampling multiple tasks despite simplicity consistently_outperforms consistently outperforms tuning shot vision language classification tasks effectiveness bi level optimization maml highlysensitive zero shot performance task context language classification goal paper provide makes shot_learning shot learning work encourage research better task sampling strategies"}
{"id": "nan", "abstract": "  Recent studies report that autoregressive language models can successfullysolve many NLP tasks via zero- and few-shot learning paradigms, which opens upnew possibilities for using the pre-trained language models. This paperintroduces two autoregressive GPT-like models with 1.3 billion and 13 billionparameters trained on 60 languages from 25 language families using Wikipediaand Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture usingGPT-2 sources and the sparse attention mechanism; Deepspeed and Megatronframeworks allow us to parallelize the training and inference stepseffectively. The resulting models show performance on par with the recentlyreleased XGLM models by Facebook, covering more languages and enhancing NLPpossibilities for low resource languages of CIS countries and Russian smallnations. We detail the motivation for the choices of the architecture design,thoroughly describe the data preparation pipeline, and train five smallversions of the model to choose the most optimal multilingual tokenizationstrategy. We measure the model perplexity in all covered languages and evaluateit on the wide spectre of multilingual tasks, including classification,generative, sequence labeling and knowledge probing. The models were evaluatedwith the zero-shot and few-shot methods. Furthermore, we compared theclassification tasks with the state-of-the-art multilingual model XGLM. sourcecode and the mGPT XL model are publicly released.", "title": "mgpt fewshot learners go multilingual", "url": "http://arxiv.org/pdf/2204.07580v2.pdf", "tokenized_text": "recent studies report autoregressive language_models language nlp_tasks nlp tasks zero- shot_learning shot learning paradigms opens possibilities pre trained_language trained language paperintroduces autoregressive gpt like 1.3 billion 13 billionparameters trained 60 languages 25 language families clean corpus reproduce gpt-3 architecture sources sparse attention mechanism allow training inference resulting performance par facebook covering languages enhancing low_resource low resource languages cis countries russian detail motivation choices architecture design thoroughly describe data preparation pipeline train choose optimal multilingual measure perplexity covered languages wide multilingual tasks including classification generative sequence labeling knowledge probing zero shot shot methods furthermore compared theclassification tasks state art multilingual xl publicly released"}
{"id": "nan", "abstract": "  Large language models, which are often trained for hundreds of thousands ofcompute days, have shown remarkable capabilities for zero- and few-shotlearning. Given their computational cost, these models are difficult toreplicate without significant capital. For the few that are available throughAPIs, no access is granted to the full model weights, making them difficult tostudy. We present Open Pre-trained Transformers (OPT), a suite of decoder-onlypre-trained transformers ranging from 125M to 175B parameters, which we aim tofully and responsibly share with interested researchers. We show that OPT-175Bis comparable to GPT-3, while requiring only 1/7th the carbon footprint todevelop. We are also releasing our logbook detailing the infrastructurechallenges we faced, along with code for experimenting with all of the releasedmodels.", "title": "opt open pretrained transformer language models", "url": "http://arxiv.org/pdf/2205.01068v4.pdf", "tokenized_text": "large_language large language trained hundreds thousands ofcompute days shown remarkable_capabilities remarkable capabilities zero- shotlearning given computational cost difficult significant capital available access weights making difficult present open pre trained transformers opt suite decoder trained transformers ranging 125 175b parameters aim responsibly share interested researchers comparable gpt-3 requiring footprint releasing detailing faced code experimenting"}
{"id": "nan", "abstract": "  Pre-trained language models have contributed significantly to relationextraction by demonstrating remarkable few-shot learning abilities. However,prompt tuning methods for relation extraction may still fail to generalize tothose rare or hard patterns. Note that the previous parametric learningparadigm can be viewed as memorization regarding training data as a book andinference as the close-book test. Those long-tailed or hard patterns can hardlybe memorized in parameters given few-shot instances. To this end, we regard REas an open-book examination and propose a new semiparametric paradigm ofretrieval-enhanced prompt tuning for relation extraction. We construct anopen-book datastore for retrieval regarding prompt-based instancerepresentations and corresponding relation labels as memorized key-value pairs.During inference, the model can infer relations by linearly interpolating thebase output of PLM with the non-parametric nearest neighbor distribution overthe datastore. In this way, our model not only infers relation throughknowledge stored in the weights during training but also assistsdecision-making by unwinding and querying examples in the open-book datastore.Extensive experiments on benchmark datasets show that our method can achievestate-of-the-art in both standard supervised and few-shot settings. Code areavailable in https://github.com/zjunlp/PromptKG/tree/main/research/RetrievalRE.", "title": "relation extraction as openbook examination retrievalenhanced prompt tuning", "url": "http://arxiv.org/pdf/2205.02355v2.pdf", "tokenized_text": "pre trained_language trained language contributed significantly relationextraction demonstrating remarkable shot_learning shot learning abilities tuning methods relation_extraction relation extraction fail generalize rare hard patterns note previous parametric learningparadigm viewed memorization training_data training data book andinference close book test long tailed hard patterns parameters given shot instances end regard open book examination propose_a_new propose new paradigm ofretrieval enhanced tuning relation_extraction relation extraction construct book datastore retrieval based corresponding relation labels key value pairs inference infer relations linearly thebase output plm non parametric nearest neighbor distribution overthe datastore way infers relation stored weights training making querying examples open book datastore extensive_experiments extensive experiments benchmark_datasets benchmark datasets method art standard supervised shot_settings shot settings code areavailable"}
{"id": "nan", "abstract": "  Prompt-based fine-tuning has boosted the performance of Pre-trained LanguageModels (PLMs) on few-shot text classification by employing task-specificprompts. Yet, PLMs are unfamiliar with prompt-style expressions duringpre-training, which limits the few-shot learning performance on downstreamtasks. It would be desirable if the models can acquire some prompting knowledgebefore adaptation to specific NLP tasks. We present the Unified Prompt Tuning(UPT) framework, leading to better few-shot text classification for BERT-stylemodels by explicitly capturing prompting semantics from non-target NLPdatasets. In UPT, a novel paradigm Prompt-Options-Verbalizer is proposed forjoint prompt learning across different NLP tasks, forcing PLMs to capturetask-invariant prompting knowledge. We further design a self-supervised tasknamed Knowledge-enhanced Selective Masked Language Modeling to improve thePLM's generalization abilities for accurate adaptation to previously unseentasks. After multi-task learning across multiple tasks, the PLM can be betterprompt-tuned towards any dissimilar target tasks in low-resourced settings.Experiments over a variety of NLP tasks show that UPT consistently outperformsstate-of-the-arts for prompt-based fine-tuning.", "title": "towards unified prompt tuning for fewshot text classification", "url": "http://arxiv.org/pdf/2205.05313v1.pdf", "tokenized_text": "based fine tuning boosted performance pre trained languagemodels plms shot text_classification text classification employing task specificprompts plms unfamiliar style expressions duringpre training limits shot_learning shot learning performance downstreamtasks desirable acquire adaptation specific nlp_tasks nlp tasks present unified framework leading better shot text_classification text classification bert stylemodels explicitly capturing semantics non target nlpdatasets upt novel paradigm options verbalizer proposed learning different nlp_tasks nlp tasks forcing plms invariant knowledge design self supervised knowledge enhanced selective masked language modeling improve theplm generalization abilities accurate adaptation previously multi task learning multiple tasks plm tuned target tasks low resourced settings experiments variety nlp_tasks nlp tasks upt consistently outperformsstate arts based fine tuning"}
{"id": "nan", "abstract": "  Considerable advancements have been made in various NLP tasks based on theimpressive power of large language models (LLMs) and many NLP applications aredeployed in our daily lives. In this work, we challenge the capability of LLMswith the new task of Ethical Quandary Generative Question Answering. Ethicalquandary questions are more challenging to address because multiple conflictinganswers may exist to a single quandary. We explore the current capability ofLLMs in providing an answer with a deliberative exchange of differentperspectives to an ethical quandary, in the approach of Socratic philosophy,instead of providing a closed answer like an oracle. We propose a model thatsearches for different ethical principles applicable to the ethical quandaryand generates an answer conditioned on the chosen principles throughprompt-based few-shot learning. We also discuss the remaining challenges andethical issues involved in this task and suggest the direction towarddeveloping responsible NLP systems by incorporating human values explicitly.", "title": "towards answering openended ethical quandary questions", "url": "http://arxiv.org/pdf/2205.05989v3.pdf", "tokenized_text": "considerable advancements nlp_tasks nlp tasks based theimpressive power large_language large language llms nlp applications daily lives work challenge capability llmswith new task ethical generative question_answering question answering questions challenging address multiple exist single explore current capability ofllms providing answer deliberative exchange ethical approach socratic philosophy instead providing closed answer like oracle propose different ethical principles applicable ethical generates answer conditioned chosen principles based shot_learning shot learning discuss remaining challenges issues involved task suggest direction responsible nlp systems incorporating human values explicitly"}
{"id": "nan", "abstract": "  Recent advances in large pre-trained language models (PLMs) lead toimpressive gains in natural language understanding (NLU) tasks withtask-specific fine-tuning. However, directly fine-tuning PLMs heavily relies onsufficient labeled training instances, which are usually hard to obtain.Prompt-based tuning on PLMs has shown to be powerful for various downstreamfew-shot tasks. Existing works studying prompt-based tuning for few-shot NLUtasks mainly focus on deriving proper label words with a verbalizer orgenerating prompt templates to elicit semantics from PLMs. In addition,conventional data augmentation strategies such as synonym substitution, thoughwidely adopted in low-resource scenarios, only bring marginal improvements forprompt-based few-shot learning. Thus, an important research question arises:how to design effective data augmentation methods for prompt-based few-shottuning? To this end, considering the label semantics are essential inprompt-based tuning, we propose a novel label-guided data augmentationframework PromptDA, which exploits the enriched label semantic information fordata augmentation. Extensive experiment results on few-shot text classificationtasks demonstrate the superior performance of the proposed framework byeffectively leveraging label semantics and data augmentation for naturallanguage understanding. Our code is available athttps://github.com/canyuchen/PromptDA.", "title": "promptda labelguided data augmentation for promptbased fewshot learners", "url": "http://arxiv.org/pdf/2205.09229v3.pdf", "tokenized_text": "recent_advances recent advances large pre trained_language trained language plms lead gains natural_language natural language understanding nlu tasks withtask specific fine tuning directly fine tuning plms heavily relies labeled training instances usually hard obtain based tuning plms shown powerful shot tasks existing works studying based tuning shot mainly focus deriving proper label words verbalizer prompt_templates templates elicit semantics plms addition conventional data_augmentation data augmentation strategies synonym adopted low resource scenarios bring marginal improvements based shot_learning shot learning important research question arises design effective data_augmentation data augmentation methods based end considering label semantics essential inprompt based tuning propose_a_novel propose novel label guided data exploits enriched label semantic information augmentation extensive experiment results shot text demonstrate superior_performance superior performance proposed framework byeffectively leveraging label semantics data_augmentation data augmentation naturallanguage understanding code_is_available code available"}
{"id": "nan", "abstract": "  Expressing natural language descriptions of structured facts or relations --data-to-text generation (D2T) -- increases the accessibility of structuredknowledge repositories. Previous work shows that pre-trained languagemodels(PLMs) perform remarkably well on this task after fine-tuning on asignificant amount of task-specific training data. On the other hand, whileauto-regressive PLMs can generalize from a few task examples, their efficacy atD2T is largely unexplored. Furthermore, we have an incomplete understanding ofthe limits of PLMs on D2T.  In this work, we conduct an empirical study of both fine-tuned andauto-regressive PLMs on the DART multi-domain D2T dataset. We consider theirperformance as a function of the amount of task-specific data and how thesedata are incorporated into the models: zero and few-shot learning, andfine-tuning of model weights. In addition, we probe the limits of PLMs bymeasuring performance on subsets of the evaluation data: novel predicates andabstractive test examples. To improve the performance on these subsets, weinvestigate two techniques: providing predicate descriptions in the context andre-ranking generated candidates by information reflected in the source.Finally, we conduct a human evaluation of model errors and show that D2Tgeneration tasks would benefit from datasets with more careful manual curation.", "title": "what makes datatotext generation hard for pretrained language models", "url": "http://arxiv.org/pdf/2205.11505v1.pdf", "tokenized_text": "natural_language natural language descriptions structured facts relations text generation d2 increases accessibility structuredknowledge repositories previous work shows pre trained perform remarkably task fine tuning asignificant task specific training_data training data hand regressive plms generalize task examples efficacy largely unexplored furthermore incomplete understanding ofthe limits plms work conduct empirical study fine tuned andauto regressive plms multi domain d2 dataset consider theirperformance function task specific data incorporated zero shot_learning shot learning andfine tuning weights addition probe limits plms performance subsets evaluation data novel predicates test examples improve performance subsets weinvestigate techniques providing predicate descriptions context ranking generated candidates information reflected source finally conduct human evaluation errors tasks benefit datasets careful manual curation"}
{"id": "nan", "abstract": "  This work introduces a new multi-task, parameter-efficient language model(LM) tuning method that learns to transfer knowledge across different tasks viaa mixture of soft prompts-small prefix embedding vectors pre-trained fordifferent tasks. Our method, called ATTEMPT (ATTEntional Mixtures of PromptTuning), obtains source prompts as encodings of large-scale source tasks into asmall number of parameters and trains an attention module to interpolate thesource prompts and a newly initialized target prompt for every instance in thetarget task. During training, only the target task prompt and the attentionweights, which are shared between tasks in multi-task training, are updated,while the original LM and source prompts are intact. ATTEMPT is highlyparameter-efficient (e.g., updates 2,300 times fewer parameters than fullfine-tuning) while achieving high task performance using knowledge fromhigh-resource tasks. Moreover, it is modular using pre-trained soft prompts,and can flexibly add or remove source prompts for effective knowledge transfer.Our experimental results across 21 diverse NLP datasets show that ATTEMPTsignificantly outperforms prompt tuning and outperforms or matches fullyfine-tuned or other parameter-efficient tuning approaches that use over tentimes more parameters. Finally, ATTEMPT outperforms previous work in few-shotlearning settings.", "title": "attempt parameterefficient multitask tuning via attentional mixtures of soft prompts", "url": "http://arxiv.org/pdf/2205.11961v2.pdf", "tokenized_text": "work introduces new multi task parameter efficient language tuning method learns transfer knowledge different tasks mixture soft small prefix embedding vectors pre trained fordifferent tasks method called attempt mixtures prompttuning obtains source encodings large scale source tasks asmall number parameters trains attention module thesource newly initialized target instance thetarget task training target task shared tasks multi task training updated original lm source intact attempt efficient e.g. updates times fewer parameters fullfine tuning achieving high task performance knowledge resource tasks modular pre trained soft flexibly add remove source effective knowledge transfer experimental_results experimental results 21 diverse nlp datasets outperforms tuning outperforms matches tuned parameter efficient tuning approaches use parameters finally attempt outperforms previous work shotlearning settings"}
{"id": "nan", "abstract": "  Few-shot learning is a challenging task that requires language models togeneralize from limited examples. Large language models like GPT-3 and PaLMhave made impressive progress in this area, but they still face difficulties inreasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improvetheir reasoning skills, previous work has proposed to guide the language modelwith prompts that elicit a series of reasoning steps before giving the finalanswer, achieving a significant improvement on GSM8K from 17.9% to 58.1% inproblem-solving rate. In this paper, we present DIVERSE (Diverse Verifier onReasoning Step), a novel approach that further enhances the reasoningcapability of language models. DIVERSE has three main components: first, itgenerates diverse prompts to explore different reasoning paths for the samequestion; second, it uses a verifier to filter out incorrect answers based on aweighted voting scheme; and third, it verifies each reasoning step individuallyinstead of the whole chain. We evaluate DIVERSE on the latest language modelcode-davinci-002 and show that it achieves new state-of-the-art results on sixof eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).", "title": "making large language models better reasoners with stepaware verifier", "url": "http://arxiv.org/pdf/2206.02336v3.pdf", "tokenized_text": "shot_learning shot learning challenging task requires language_models language togeneralize limited examples large_language large language like gpt-3 impressive progress area face difficulties tasks gsm8 benchmark arithmetic problems reasoning skills previous work proposed guide language modelwith elicit series reasoning_steps reasoning steps giving achieving significant improvement gsm8 solving rate paper present diverse diverse verifier step novel_approach novel approach enhances language_models language diverse main components diverse explore different reasoning paths second uses verifier filter incorrect answers based voting scheme verifies reasoning step chain evaluate diverse latest language davinci-002 achieves new state art results reasoning benchmarks e.g. gsm8"}
{"id": "nan", "abstract": "  Foundation models have received much attention due to their effectivenessacross a broad range of downstream applications. Though there is a bigconvergence in terms of architecture, most pretrained models are typicallystill developed for specific tasks or modalities. In this work, we propose touse language models as a general-purpose interface to various foundationmodels. A collection of pretrained encoders perceive diverse modalities (suchas vision, and language), and they dock with a language model that plays therole of a universal task layer. We propose a semi-causal language modelingobjective to jointly pretrain the interface and the modular encoders. Wesubsume the advantages and capabilities from both causal and non-causalmodeling, thereby combining the best of two worlds. Specifically, the proposedmethod not only inherits the capabilities of in-context learning and open-endedgeneration from causal language modeling, but also is conducive to finetuningbecause of the bidirectional encoders. More importantly, our approachseamlessly unlocks the combinations of the above capabilities, e.g., enablingin-context learning or instruction following with finetuned encoders.Experimental results across various language-only and vision-languagebenchmarks show that our model outperforms or is competitive with specializedmodels on finetuning, zero-shot generalization, and few-shot learning.", "title": "language models are generalpurpose interfaces", "url": "http://arxiv.org/pdf/2206.06336v1.pdf", "tokenized_text": "foundation_models foundation received attention broad range downstream applications terms architecture pretrained developed specific tasks modalities work propose touse language_models language general purpose interface foundationmodels collection pretrained encoders perceive diverse modalities suchas vision language language_model language plays universal task layer propose semi causal language jointly pretrain interface modular encoders advantages capabilities causal non combining best worlds specifically proposedmethod inherits capabilities context_learning context learning open causal language modeling conducive bidirectional encoders importantly unlocks combinations capabilities e.g. context_learning context learning instruction_following instruction following finetuned encoders experimental_results experimental results language vision outperforms competitive finetuning zero shot generalization shot_learning shot learning"}
{"id": "nan", "abstract": "  Modern deep learning systems are increasingly deployed in situations such aspersonalization and federated learning where it is necessary to support i)learning on small amounts of data, and ii) communication efficient distributedtraining protocols. In this work, we develop FiLM Transfer (FiT) which fulfillsthese requirements in the image classification setting by combining ideas fromtransfer learning (fixed pretrained backbones and fine-tuned FiLM adapterlayers) and meta-learning (automatically configured Naive Bayes classifiers andepisodic training) to yield parameter efficient models with superiorclassification accuracy at low-shot. The resulting parameter efficiency is keyfor enabling few-shot learning, inexpensive model updates for personalization,and communication efficient federated learning. We experiment with FiT on awide range of downstream datasets and show that it achieves betterclassification accuracy than the leading Big Transfer (BiT) algorithm atlow-shot and achieves state-of-the art accuracy on the challenging VTAB-1kbenchmark, with fewer than 1% of the updateable parameters. Finally, wedemonstrate the parameter efficiency and superior accuracy of FiT indistributed low-shot applications including model personalization and federatedlearning where model update size is an important performance metric.", "title": "fit parameter efficient fewshot transfer learning for personalized and federated image classification", "url": "http://arxiv.org/pdf/2206.08671v2.pdf", "tokenized_text": "modern deep learning systems increasingly deployed situations federated learning necessary support small amounts data ii communication efficient protocols work develop film transfer fit requirements image classification setting combining ideas learning fixed pretrained backbones fine tuned film adapterlayers meta learning automatically configured naive bayes classifiers training yield parameter_efficient parameter efficient accuracy low shot resulting parameter efficiency enabling shot_learning shot learning updates personalization communication efficient federated learning experiment fit awide range downstream datasets achieves accuracy leading big transfer bit algorithm shot achieves_state achieves state art accuracy challenging fewer parameters finally wedemonstrate parameter efficiency superior accuracy fit low shot applications including personalization update size important performance metric"}
{"id": "nan", "abstract": "  The rapid development of artificial intelligence (AI) technology has enabledlarge-scale AI applications to land in the market and practice. However, whileAI technology has brought many conveniences to people in the productizationprocess, it has also exposed many security issues. Especially, attacks againstonline learning vulnerabilities of chatbots occur frequently. Therefore, thispaper proposes a semantics censorship chatbot system based on reinforcementlearning, which is mainly composed of two parts: the Offensive semanticscensorship model and the semantics purification model. Offensive semanticsreview can combine the context of user input sentences to detect the rapidevolution of Offensive semantics and respond to Offensive semantics responses.The semantics purification model For the case of chatting robot models, it hasbeen contaminated by large numbers of offensive semantics, by strengthening theoffensive reply learned by the learning algorithm, rather than rolling back tothe early versions. In addition, by integrating a once-through learningapproach, the speed of semantics purification is accelerated while reducing theimpact on the quality of replies. The experimental results show that ourproposed approach reduces the probability of the chat model generatingoffensive replies and that the integration of the few-shot learning algorithmimproves the training speed rapidly while effectively slowing down the declinein BLEU values.", "title": "a reinforcement learningbased offensive semantics censorship system for chatbots", "url": "http://arxiv.org/pdf/2207.10569v1.pdf", "tokenized_text": "rapid development artificial_intelligence artificial intelligence ai technology scale ai applications market practice technology brought people exposed security issues especially attacks learning vulnerabilities chatbots occur frequently thispaper proposes semantics chatbot system based mainly composed parts offensive semantics offensive combine context user input sentences detect offensive semantics respond offensive semantics responses semantics case chatting robot hasbeen contaminated large numbers offensive semantics learned learning algorithm rolling tothe early versions addition integrating learningapproach speed semantics accelerated reducing quality replies experimental_results experimental results ourproposed approach reduces probability chat replies integration shot_learning shot learning training speed rapidly effectively bleu values"}
{"id": "nan", "abstract": "  In this work, we demonstrate that multilingual large-scalesequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoisingand Causal Language Modeling (CLM) tasks, are more efficient few-shot learnersthan decoder-only models on various tasks. In particular, we train a 20 billionparameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B)and show that it achieves state-of-the-art (SOTA) performance on 1-shotsummarization tasks, outperforming a much larger 540B PaLM decoder model.AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially forlow-resource languages, across almost all language pairs supported by the model(Arabic, English, French, German, Hindi, Italian, Japanese, Marathi,Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show inzero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2datasets and provides SOTA performance on multilingual tasks such as XNLI,XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling casefor seq2seq models as a powerful alternative to decoder-only models forLarge-scale Language Model (LLM) training.", "title": "alexatm 20b fewshot learning using a largescale multilingual seq2seq model", "url": "http://arxiv.org/pdf/2208.01448v2.pdf", "tokenized_text": "work demonstrate multilingual large sequence seq2seq pre trained mixture causal language modeling tasks efficient shot decoder tasks particular train 20 multilingual seq2seq called alexa teacher alexatm achieves_state achieves state art sota performance shotsummarization tasks outperforming larger 540b palm decoder alexatm 20b achieves sota shot machine_translation machine translation especially forlow resource_languages resource languages language pairs supported english french german hindi italian japanese portuguese spanish tamil flores-101 dataset inzero shot_setting shot setting alexatm 20b outperforms gpt3 175b superglue provides sota performance multilingual tasks overall results present compelling seq2seq powerful alternative decoder forlarge scale language_model language llm training"}
{"id": "nan", "abstract": "  Data-driven predictive methods which can efficiently and accurately transformprotein sequences into biologically active structures are highly valuable forscientific research and medical development. Determining accurate foldinglandscape using co-evolutionary information is fundamental to the success ofmodern protein structure prediction methods. As the state of the art,AlphaFold2 has dramatically raised the accuracy without performing explicitco-evolutionary analysis. Nevertheless, its performance still shows strongdependence on available sequence homologs. Based on the interrogation on thecause of such dependence, we presented EvoGen, a meta generative model, toremedy the underperformance of AlphaFold2 for poor MSA targets. By promptingthe model with calibrated or virtually generated homologue sequences, EvoGenhelps AlphaFold2 fold accurately in low-data regime and even achieveencouraging performance with single-sequence predictions. Being able to makeaccurate predictions with few-shot MSA not only generalizes AlphaFold2 betterfor orphan sequences, but also democratizes its use for high-throughputapplications. Besides, EvoGen combined with AlphaFold2 yields a probabilisticstructure generation method which could explore alternative conformations ofprotein sequences, and the task-aware differentiable algorithm for sequencegeneration will benefit other related tasks including protein design.", "title": "unsupervisedly prompting alphafold2 for fewshot learning of accurate folding landscape and protein structure prediction", "url": "http://arxiv.org/pdf/2208.09652v2.pdf", "tokenized_text": "data driven predictive methods efficiently accurately sequences biologically active structures highly valuable forscientific research medical development determining accurate co evolutionary information fundamental success protein structure prediction methods state_of_the_art state art dramatically raised accuracy performing evolutionary analysis performance shows available sequence based dependence presented meta generative toremedy poor targets calibrated virtually generated sequences fold accurately low data regime achieveencouraging performance single sequence predictions able predictions shot generalizes sequences use high combined yields generation method explore alternative sequences task aware differentiable algorithm benefit related tasks including protein design"}
{"id": "nan", "abstract": "  Few-shot learning models learn representations with limited humanannotations, and such a learning paradigm demonstrates practicability invarious tasks, e.g., image classification, object detection, etc. However,few-shot object detection methods suffer from an intrinsic defect that thelimited training data makes the model cannot sufficiently explore semanticinformation. To tackle this, we introduce knowledge distillation to thefew-shot object detection learning paradigm. We further run a motivatingexperiment, which demonstrates that in the process of knowledge distillation,the empirical error of the teacher model degenerates the prediction performanceof the few-shot object detection model as the student. To understand thereasons behind this phenomenon, we revisit the learning paradigm of knowledgedistillation on the few-shot object detection task from the causal theoreticstandpoint, and accordingly, develop a Structural Causal Model. Following thetheoretical guidance, we propose a backdoor adjustment-based knowledgedistillation method for the few-shot object detection task, namely Disentangleand Remerge (D&R), to perform conditional causal intervention toward thecorresponding Structural Causal Model. Empirically, the experiments onbenchmarks demonstrate that D&R can yield significant performance boosts infew-shot object detection. Code is available athttps://github.com/ZYN-1101/DandR.git.", "title": "disentangle and remerge interventional knowledge distillation for fewshot object detection from a conditional causal perspective", "url": "http://arxiv.org/pdf/2208.12681v2.pdf", "tokenized_text": "shot_learning shot learning learn representations limited humanannotations learning paradigm demonstrates practicability invarious tasks e.g. image classification object_detection object detection etc shot object_detection object detection methods suffer intrinsic defect thelimited training_data training data makes sufficiently explore semanticinformation tackle introduce knowledge_distillation knowledge distillation thefew shot object_detection object detection learning paradigm run demonstrates process knowledge_distillation knowledge distillation empirical error teacher degenerates prediction performanceof shot object_detection object detection student understand phenomenon revisit learning paradigm knowledgedistillation shot object_detection object detection task causal accordingly develop structural causal following guidance propose backdoor adjustment based knowledgedistillation method shot object_detection object detection task perform conditional causal intervention thecorresponding structural causal empirically experiments demonstrate yield significant performance boosts infew shot object_detection object detection code_is_available code available"}
{"id": "nan", "abstract": "  We present the design and baseline results for a new challenge in theChaLearn meta-learning series, accepted at NeurIPS'22, focusing on\"cross-domain\" meta-learning. Meta-learning aims to leverage experience gainedfrom previous tasks to solve new tasks efficiently (i.e., with betterperformance, little training data, and/or modest computational resources).While previous challenges in the series focused on within-domain few-shotlearning problems, with the aim of learning efficiently N-way k-shot tasks(i.e., N class classification problems with k training examples), thiscompetition challenges the participants to solve \"any-way\" and \"any-shot\"problems drawn from various domains (healthcare, ecology, biology,manufacturing, and others), chosen for their humanitarian and societal impact.To that end, we created Meta-Album, a meta-dataset of 40 image classificationdatasets from 10 domains, from which we carve out tasks with any number of\"ways\" (within the range 2-20) and any number of \"shots\" (within the range1-20). The competition is with code submission, fully blind-tested on theCodaLab challenge platform. The code of the winners will be open-sourced,enabling the deployment of automated machine learning solutions for few-shotimage classification across several domains.", "title": "neurips'22 crossdomain metadl competition design and baseline results", "url": "http://arxiv.org/pdf/2208.14686v1.pdf", "tokenized_text": "present design baseline results new challenge meta learning series accepted focusing domain meta learning meta learning aims leverage experience previous tasks solve new tasks efficiently i.e. betterperformance little training_data training data and/or modest computational previous challenges series focused domain shotlearning problems aim learning efficiently way shot class classification problems training_examples training examples challenges participants solve way drawn domains healthcare biology manufacturing chosen humanitarian societal impact end created meta album meta dataset 40 image classificationdatasets 10 domains carve tasks number range 20 number shots 20 competition code submission fully blind tested challenge platform code open sourced enabling deployment automated machine_learning machine learning solutions shotimage classification domains"}
{"id": "nan", "abstract": "  Prompting, which casts downstream applications as language modeling tasks,has shown to be sample efficient compared to standard fine-tuning withpre-trained models. However, one pitfall of prompting is the need ofmanually-designed patterns, whose outcome can be unintuitive and requires largevalidation sets to tune. To tackle the challenge, we propose AutoSeq, a fullyautomatic prompting method: (1) We adopt natural language prompts onsequence-to-sequence models, enabling free-form generation and larger labelsearch space; (2) We propose label sequences -- phrases with indefinite lengthsto verbalize the labels -- which eliminate the need of manual templates and aremore expressive than single label words; (3) We use beam search toautomatically generate a large amount of label sequence candidates and proposecontrastive re-ranking to get the best combinations. AutoSeq significantlyoutperforms other no-manual-design methods, such as soft prompt tuning, adaptertuning, and automatic search on single label words; the generated labelsequences are even better than curated manual ones on a variety of tasks. Ourmethod reveals the potential of sequence-to-sequence models in few-shotlearning and sheds light on a path to generic and automatic prompting. Thesource code of this paper can be obtained fromhttps://github.com/thunlp/Seq2Seq-Prompt.", "title": "automatic label sequence generation for prompting sequencetosequence models", "url": "http://arxiv.org/pdf/2209.09401v1.pdf", "tokenized_text": "downstream applications language modeling tasks shown sample efficient compared standard fine tuning withpre trained need ofmanually designed patterns outcome requires sets tune tackle challenge propose method adopt natural_language natural language sequence enabling free form generation larger space propose label sequences phrases verbalize labels eliminate need manual templates aremore expressive single label words use beam search toautomatically generate large label sequence candidates ranking best combinations significantlyoutperforms manual design methods soft tuning automatic search single label words generated better curated manual ones variety tasks ourmethod reveals potential sequence sequence shotlearning sheds light path generic automatic thesource code paper obtained"}
{"id": "nan", "abstract": "  Few-shot classification requires deep neural networks to learn generalizedrepresentations only from limited training images, which is challenging butsignificant in low-data regimes. Recently, CLIP-based methods have shownpromising few-shot performance benefited from the contrastive language-imagepre-training. Based on this point, we question if the large-scale pre-trainingcan alleviate the few-shot data deficiency and also assist the representationlearning by the pre-learned knowledge. In this paper, we propose CoMo, aCollaboration of pre-trained Models that incorporates diverse prior knowledgefrom various pre-training paradigms for better few-shot learning. Our CoMoincludes: CLIP's language-contrastive knowledge, DINO's vision-contrastiveknowledge, and DALL-E's language-generative knowledge. Specifically, CoMo worksin two aspects: few-shot data expansion and diverse knowledge ensemble. Forone, we generate synthetic images via zero-shot DALL-E to enrich the few-shottraining data without any manpower. For the other, we introduce a learnableMulti-Knowledge Adapter (MK-Adapter) to adaptively blend the predictions fromCLIP and DINO. By such collaboration, CoMo can fully unleash the potential ofdifferent pre-training methods and unify them to perform state-of-the-art forfew-shot classification. We conduct extensive experiments on 11 datasets todemonstrate the superiority and generalization ability of our approach.", "title": "collaboration of pretrained models makes better fewshot learner", "url": "http://arxiv.org/pdf/2209.12255v2.pdf", "tokenized_text": "shot classification requires deep neural_networks neural networks learn limited training images challenging low data regimes recently clip based methods shot performance benefited contrastive language imagepre training based point question large scale pre alleviate shot data deficiency assist representationlearning pre learned knowledge paper propose pre trained incorporates diverse prior pre training paradigms better shot_learning shot learning clip language contrastive knowledge dino vision dall language generative knowledge specifically worksin aspects shot data expansion diverse knowledge ensemble forone generate synthetic images zero shot dall enrich data introduce knowledge adapter adapter adaptively blend predictions dino collaboration fully unleash potential ofdifferent pre training methods unify perform state art forfew shot classification conduct_extensive conduct extensive experiments 11 datasets todemonstrate superiority generalization_ability generalization ability approach"}
{"id": "nan", "abstract": "  Pre-training across 3D vision and language remains under development becauseof limited training data. Recent works attempt to transfer vision-languagepre-training models to 3D vision. PointCLIP converts point cloud data tomulti-view depth maps, adopting CLIP for shape classification. However, itsperformance is restricted by the domain gap between rendered depth maps andimages, as well as the diversity of depth distributions. To address this issue,we propose CLIP2Point, an image-depth pre-training method by contrastivelearning to transfer CLIP to the 3D domain, and adapt it to point cloudclassification. We introduce a new depth rendering setting that forms a bettervisual effect, and then render 52,460 pairs of images and depth maps fromShapeNet for pre-training. The pre-training scheme of CLIP2Point combinescross-modality learning to enforce the depth features for capturing expressivevisual and textual features and intra-modality learning to enhance theinvariance of depth aggregation. Additionally, we propose a novel Dual-PathAdapter (DPA) module, i.e., a dual-path structure with simplified adapters forfew-shot learning. The dual-path structure allows the joint use of CLIP andCLIP2Point, and the simplified adapter can well fit few-shot tasks withoutpost-search. Experimental results show that CLIP2Point is effective intransferring CLIP knowledge to 3D vision. Our CLIP2Point outperforms PointCLIPand other self-supervised 3D networks, achieving state-of-the-art results onzero-shot and few-shot classification.", "title": "clip2point transfer clip to point cloud classification with imagedepth pretraining", "url": "http://arxiv.org/pdf/2210.01055v3.pdf", "tokenized_text": "pre training 3d vision language remains development limited training_data training data recent works attempt transfer vision training 3d vision pointclip converts point_cloud point cloud data view depth maps adopting clip shape classification restricted domain gap rendered depth maps diversity depth distributions address issue propose image depth pre training method contrastivelearning transfer clip 3d domain adapt point introduce new depth rendering setting forms effect render pairs images depth maps pre training pre training scheme modality learning enforce depth features capturing textual features intra modality learning enhance depth aggregation additionally propose_a_novel propose novel dual module i.e. dual path structure simplified adapters forfew shot_learning shot learning dual path structure allows joint use clip simplified adapter fit shot tasks search experimental_results experimental results effective clip knowledge 3d vision outperforms self supervised 3d networks achieving state art results onzero shot shot classification"}
{"id": "nan", "abstract": "  Language models (LMs) now excel at many tasks such as few-shot learning,question answering, reasoning, and dialog. However, they sometimes generateunsupported or misleading content. A user cannot easily determine whether theiroutputs are trustworthy or not, because most LMs do not have any built-inmechanism for attribution to external evidence. To enable attribution whilestill preserving all the powerful advantages of recent generation models, wepropose RARR (Retrofit Attribution using Research and Revision), a system that1) automatically finds attribution for the output of any text generation modeland 2) post-edits the output to fix unsupported content while preserving theoriginal output as much as possible. When applied to the output of severalstate-of-the-art LMs on a diverse set of generation tasks, we find that RARRsignificantly improves attribution while otherwise preserving the originalinput to a much greater degree than previously explored edit models.Furthermore, the implementation of RARR requires only a handful of trainingexamples, a large language model, and standard web search.", "title": "rarr researching and revising what language models say, using language models", "url": "http://arxiv.org/pdf/2210.08726v3.pdf", "tokenized_text": "language_models language lms excel tasks shot_learning shot learning question_answering question answering reasoning dialog misleading content user easily determine trustworthy lms built attribution external evidence enable attribution preserving powerful advantages recent generation wepropose attribution research revision system automatically finds attribution output text generation modeland post edits output fix unsupported content preserving theoriginal output possible applied output art lms diverse set generation tasks find improves attribution preserving greater degree previously explored edit furthermore implementation requires handful trainingexamples large_language large language standard web search"}
{"id": "nan", "abstract": "  Recent advances in zero-shot and few-shot learning have shown promise for ascope of research and practical purposes. However, this fast-growing area lacksstandardized evaluation suites for non-English languages, hindering progressoutside the Anglo-centric paradigm. To address this line of research, wepropose TAPE (Text Attack and Perturbation Evaluation), a novel benchmark thatincludes six more complex NLU tasks for Russian, covering multi-hop reasoning,ethical concepts, logic and commonsense knowledge. The TAPE's design focuses onsystematic zero-shot and few-shot NLU evaluation: (i) linguistic-orientedadversarial attacks and perturbations for analyzing robustness, and (ii)subpopulations for nuanced interpretation. The detailed analysis of testing theautoregressive baselines indicates that simple spelling-based perturbationsaffect the performance the most, while paraphrasing the input has a morenegligible effect. At the same time, the results demonstrate a significant gapbetween the neural and human baselines for most tasks. We publicly release TAPE(tape-benchmark.com) to foster research on robust LMs that can generalize tonew tasks when little to no supervision is available.", "title": "tape assessing fewshot russian language understanding", "url": "http://arxiv.org/pdf/2210.12813v1.pdf", "tokenized_text": "recent_advances recent advances zero shot shot_learning shot learning shown promise research practical purposes fast growing area evaluation suites non english languages hindering centric paradigm address line research wepropose tape text attack perturbation evaluation novel benchmark thatincludes complex nlu tasks russian covering multi hop reasoning ethical concepts logic commonsense knowledge tape design focuses onsystematic zero shot shot nlu evaluation linguistic attacks perturbations analyzing robustness nuanced interpretation detailed analysis testing baselines indicates simple based performance paraphrasing input effect time results_demonstrate results demonstrate significant gapbetween neural human baselines tasks publicly release foster research robust lms generalize tasks little supervision available"}
{"id": "nan", "abstract": "  It has been experimentally demonstrated that humans are able to learn in amanner that allows them to make predictions on categories for which they havenot seen any examples (Malaviya et al., 2022). Sucholutsky and Schonlau (2020)have recently presented a machine learning approach that aims to do the same.They utilise synthetically generated data and demonstrate that it is possibleto achieve sub-linear scaling and develop models that can learn to recognise Nclasses from M training samples where M is less than N - aka less-than-one shotlearning. Their method was, however, defined for univariate or simplemultivariate data (Sucholutsky et al., 2021). We extend it to work on large,high-dimensional and real-world datasets and empirically validate it in thisnew and challenging setting. We apply this method to learn previously unseenNLP tasks from very few examples (4, 8 or 16). We first generate compact,sophisticated less-than-one shot representations called soft-label prototypeswhich are fitted on training data, capturing the distribution of differentclasses across the input domain space. We then use a modified k-NearestNeighbours classifier to demonstrate that soft-label prototypes can classifydata competitively, even outperforming much more computationally complexfew-shot learning methods.", "title": "learning new tasks from a few examples with softlabel prototypes", "url": "http://arxiv.org/pdf/2210.17437v2.pdf", "tokenized_text": "experimentally demonstrated humans able learn allows predictions categories seen examples et_al et al 2022 recently presented machine_learning machine learning approach aims utilise synthetically generated data demonstrate achieve sub linear scaling develop learn recognise training samples aka shotlearning method defined data et_al et al 2021 extend work large high dimensional real world datasets empirically validate challenging setting apply method learn previously tasks examples 16 generate compact sophisticated shot representations called soft label training_data training data capturing distribution input domain space use modified classifier demonstrate soft label prototypes competitively outperforming computationally shot_learning shot learning methods"}
{"id": "nan", "abstract": "  Large language models (LLMs) can acquire strong code-generation capabilitiesthrough few-shot learning. In contrast, supervised fine-tuning is still neededfor smaller models to achieve good performance. Such fine-tuning demands alarge number of task-specific NL-code pairs, which are expensive to obtain. Inthis paper, we attempt to transfer the code generation ability of an LLM to asmaller model with the aid of weakly-supervised data. More specifically, wepropose explicit knowledge transfer (EKT), which uses the few-shot capabilitiesof a teacher LLM to create NL-code pairs that we then filter for correctnessand fine-tune the student on. We evaluate EKT on the task of generating codesolutions to math word problems from the GSM8k dataset. We find that EKT notonly yields better performance than training with expert iteration, but alsooutperforms knowledge distillation, another form of knowledge transfer. AGPT-Neo 1.3B model trained using EKT with a GPT-J teacher achieves a 12.4%pass@100 on GSM8k, while the same student and teacher trained with knowledgedistillation yield only a 3.7% pass@100. We also show that it is possible for astudent model to outperform the teacher using EKT.", "title": "explicit knowledge transfer for weaklysupervised code generation", "url": "http://arxiv.org/pdf/2211.16740v3.pdf", "tokenized_text": "large_language large language llms acquire strong code generation shot_learning shot learning contrast supervised fine tuning smaller achieve good performance fine tuning demands alarge number task specific nl code pairs expensive obtain inthis_paper inthis paper attempt transfer code_generation code generation ability llm aid weakly supervised data specifically wepropose explicit knowledge transfer uses shot capabilitiesof teacher llm create nl code pairs filter fine tune student evaluate task generating math word problems gsm8k dataset find notonly yields better performance training expert iteration alsooutperforms knowledge_distillation knowledge distillation form knowledge transfer neo 1.3b trained gpt teacher achieves gsm8k student teacher trained knowledgedistillation yield 3.7 possible outperform teacher"}
{"id": "nan", "abstract": "  Language models exhibit an emergent ability to learn a new task from a smallnumber of input-output demonstrations. However, recent work shows thatin-context learners largely rely on their pre-trained knowledge, such as thesentiment of the labels, instead of learning new associations from the input.We argue that the commonly-used few-shot evaluation using a random selection ofin-context demonstrations can not disentangle models' reliance on such biases,as most of the randomly-selected demonstrations do not present relationsinformative for prediction beyond exposing the task's input-outputdistribution.  Therefore, to evaluate models' in-context learning ability independent ofmodels' memory, we introduce a Concept-sharing few-shot learning methodchoosing the demonstrations that share an underlying concept with the predictedsample. We extract a set of such concepts from available human explanations andmeasure how much models can benefit from presenting these concepts in few-shotdemonstrations.  We find that most of the recent in-context learners can not consistentlybenefit from the demonstrated concepts, irrespective of the model size.However, we note that T0 models are more sensitive to exhibited concepts,benefiting from concept-sharing demonstrations in 7 out of 8 evaluationscenarios.", "title": "can incontext learners learn a reasoning concept from demonstrations", "url": "http://arxiv.org/pdf/2212.01692v4.pdf", "tokenized_text": "language_models language exhibit emergent ability learn new task smallnumber input output demonstrations recent_work recent work shows thatin context learners largely rely pre trained knowledge labels instead learning new associations input argue commonly shot evaluation random selection ofin context demonstrations disentangle reliance biases randomly selected demonstrations present prediction exposing task input outputdistribution evaluate context_learning context learning ability independent memory introduce concept sharing shot_learning shot learning demonstrations share underlying concept extract set concepts available human explanations benefit presenting concepts shotdemonstrations find recent context learners demonstrated concepts irrespective model_size size note t0 sensitive exhibited concepts benefiting concept sharing demonstrations"}
{"id": "nan", "abstract": "  The pretraining-finetuning paradigm has demonstrated great success in NLP and2D image fields because of the high-quality representation ability andtransferability of their pretrained models. However, pretraining such a strongmodel is difficult in the 3D point cloud field since the training data islimited and point cloud collection is expensive. This paper introducesEfficient Point Cloud Learning (EPCL), an effective and efficient point cloudlearner for directly training high-quality point cloud models with a frozenCLIP model. Our EPCL connects the 2D and 3D modalities by semantically aligningthe 2D features and point cloud features without paired 2D-3D data.Specifically, the input point cloud is divided into a sequence of tokens anddirectly fed into the frozen CLIP model to learn point cloud representation.Furthermore, we design a task token to narrow the gap between 2D images and 3Dpoint clouds. Comprehensive experiments on 3D detection, semantic segmentation,classification and few-shot learning demonstrate that the 2D CLIP model can bean efficient point cloud backbone and our method achieves state-of-the-artaccuracy on both real-world and synthetic downstream tasks. Code will beavailable.", "title": "frozen clip model is an efficient point cloud backbone", "url": "http://arxiv.org/pdf/2212.04098v2.pdf", "tokenized_text": "pretraining finetuning paradigm demonstrated great success nlp image fields high quality representation ability pretrained pretraining difficult 3d point_cloud point cloud field training_data training data point_cloud point cloud collection expensive paper point_cloud point cloud learning effective efficient point directly training high quality point_cloud point cloud connects 2d 3d modalities semantically 2d features point_cloud point cloud features paired data specifically input point_cloud point cloud divided sequence tokens anddirectly fed frozen clip learn point_cloud point cloud representation furthermore design task token narrow gap 2d images clouds comprehensive experiments 3d detection semantic segmentation classification shot_learning shot learning demonstrate 2d clip efficient point_cloud point cloud backbone method_achieves method achieves state artaccuracy real world synthetic downstream_tasks downstream tasks code"}
{"id": "nan", "abstract": "  Natural language processing (NLP) sees rich mobile applications. To supportvarious language understanding tasks, a foundation NLP model is oftenfine-tuned in a federated, privacy-preserving setting (FL). This processcurrently relies on at least hundreds of thousands of labeled training samplesfrom mobile clients; yet mobile users often lack willingness or knowledge tolabel their data. Such an inadequacy of data labels is known as a few-shotscenario; it becomes the key blocker for mobile NLP applications.  For the first time, this work investigates federated NLP in the few-shotscenario (FedFSL). By retrofitting algorithmic advances of pseudo labeling andprompt learning, we first establish a training pipeline that deliverscompetitive accuracy when only 0.05% (fewer than 100) of the training data islabeled and the remaining is unlabeled. To instantiate the workflow, we furtherpresent a system FeS, addressing the high execution cost with novel designs.(1) Curriculum pacing, which injects pseudo labels to the training workflow ata rate commensurate to the learning progress; (2) Representational diversity, amechanism for selecting the most learnable data, only for which pseudo labelswill be generated; (3) Co-planning of a model's training depth and layercapacity. Together, these designs reduce the training delay, client energy, andnetwork traffic by up to 46.0$\\times$, 41.2$\\times$ and 3000.0$\\times$,respectively. Through algorithm/system co-design, FFNLP demonstrates that FLcan apply to challenging settings where most training samples are unlabeled.", "title": "federated fewshot learning for mobile nlp", "url": "http://arxiv.org/pdf/2212.05974v2.pdf", "tokenized_text": "natural_language natural language processing nlp rich mobile applications language understanding tasks foundation nlp tuned federated privacy preserving setting fl relies hundreds thousands labeled training mobile clients mobile users lack knowledge data inadequacy data labels known key mobile nlp applications time work investigates federated nlp retrofitting algorithmic advances pseudo labeling andprompt learning establish training pipeline accuracy fewer 100 training_data training data remaining unlabeled instantiate workflow system addressing high execution cost novel curriculum injects pseudo labels training workflow ata rate learning progress representational diversity selecting learnable data pseudo generated co planning training depth designs reduce training client energy traffic algorithm system co design demonstrates apply challenging settings training samples unlabeled"}
{"id": "nan", "abstract": "  Massively multi-task learning with large language models has recently madesubstantial progress on few-shot generalization. However, this is usuallyperformed in a centralized learning fashion, ignoring the privacy sensitivityissue of (annotated) data used in multiple tasks. To mitigate this issue, wepropose FewFedWeight, a few-shot federated learning framework across multipletasks, to achieve the best of both worlds: privacy preservation and cross-taskgeneralization. FewFedWeight trains client models in isolated devices withoutsharing data. It broadcasts the global model in the server to each client andproduces pseudo data for clients so that knowledge from the global model can beexplored to enhance few-shot learning of each client model. An energy-basedalgorithm is further proposed to weight pseudo samples in order to reduce thenegative impact of noise from the generated pseudo data. Adaptive model weightsof client models are also tuned according to their performance. We use thesemodel weights to dynamically aggregate client models to update the globalmodel. Experiments on 118 NLP tasks show that FewFedWeight can significantlyimprove the performance of client models on 61% tasks with an averageperformance improvement rate of 30.5% over the baseline and substantiallyoutperform FedAvg and other decentralized learning methods.", "title": "fewfedweight fewshot federated learning framework across multiple nlp tasks", "url": "http://arxiv.org/pdf/2212.08354v1.pdf", "tokenized_text": "massively multi task learning large_language large language recently progress shot generalization centralized learning fashion ignoring privacy annotated data multiple tasks mitigate issue wepropose shot federated learning framework achieve best worlds privacy preservation cross taskgeneralization trains client isolated devices data global server client pseudo data clients knowledge global enhance shot_learning shot learning client energy proposed weight pseudo samples order reduce impact noise generated pseudo data adaptive client tuned according performance use weights dynamically aggregate client update experiments nlp_tasks nlp tasks significantlyimprove performance client 61 tasks averageperformance improvement rate baseline substantiallyoutperform decentralized learning methods"}
{"id": "nan", "abstract": "  Traditional approaches to RL have focused on learning decision policiesdirectly from episodic decisions, while slowly and implicitly learning thesemantics of compositional representations needed for generalization. Whilesome approaches have been adopted to refine representations via auxiliaryself-supervised losses while simultaneously learning decision policies,learning compositional representations from hand-designed andcontext-independent self-supervised losses (multi-view) still adapts relativelyslowly to the real world, which contains many non-IID subspaces requiring rapiddistribution shift in both time and spatial attention patterns at varyinglevels of abstraction. In contrast, supervised language model cascades haveshown the flexibility to adapt to many diverse manifolds, and hints ofself-learning needed for autonomous task transfer. However, to date, transfermethods for language models like few-shot learning and fine-tuning stillrequire human supervision and transfer learning using self-learning methods hasbeen underexplored. We propose a self-supervised loss policy called contrastivedistillation which manifests latent variables with high mutual information withboth source and target tasks from weights to tokens. We show how thisoutperforms common methods of transfer learning and suggests a useful designaxis of trading off compute for generalizability for online transfer.Contrastive distillation is improved through sampling from memory and suggestsa simple algorithm for more efficiently sampling negative examples forcontrastive losses than random sampling.", "title": "contrastive distillation is a sampleefficient selfsupervised loss policy for transfer learning", "url": "http://arxiv.org/pdf/2212.11353v1.pdf", "tokenized_text": "traditional approaches rl focused learning decision decisions implicitly learning compositional representations needed generalization approaches adopted refine representations supervised losses simultaneously learning decision policies learning compositional representations hand designed andcontext independent self supervised losses multi view adapts real_world real world contains non iid requiring shift time spatial attention patterns abstraction contrast supervised language_model language cascades haveshown flexibility adapt diverse hints ofself learning needed autonomous task transfer date language_models language like shot_learning shot learning fine tuning human supervision transfer learning self learning methods hasbeen underexplored propose self supervised loss policy called latent variables high mutual information withboth source target tasks weights tokens common methods transfer learning suggests useful trading compute generalizability online transfer contrastive distillation improved sampling memory simple algorithm efficiently sampling negative examples losses random sampling"}
{"id": "nan", "abstract": "  The task of Few-shot Learning (FSL) aims to do the inference on novelcategories containing only few labeled examples, with the help of knowledgelearned from base categories containing abundant labeled training samples.While there are numerous works into FSL task, Vision Transformers (ViTs) haverarely been taken as the backbone to FSL with few trials focusing on naivefinetuning of whole backbone or classification layer.} Essentially, despiteViTs have been shown to enjoy comparable or even better performance on othervision tasks, it is still very nontrivial to efficiently finetune the ViTs inreal-world FSL scenarios. To this end, we propose a novel efficient TransformerTuning (eTT) method that facilitates finetuning ViTs in the FSL tasks. The keynovelties come from the newly presented Attentive Prefix Tuning (APT) andDomain Residual Adapter (DRA) for the task and backbone tuning, individually.Specifically, in APT, the prefix is projected to new key and value pairs thatare attached to each self-attention layer to provide the model withtask-specific information. Moreover, we design the DRA in the form of learnableoffset vectors to handle the potential domain gaps between base and novel data.To ensure the APT would not deviate from the initial task-specific informationmuch, we further propose a novel prototypical regularization, which maximizesthe similarity between the projected distribution of prefix and initialprototypes, regularizing the update procedure. Our method receives outstandingperformance on the challenging Meta-Dataset. We conduct extensive experimentsto show the efficacy of our model.", "title": "exploring efficient fewshot adaptation for vision transformers", "url": "http://arxiv.org/pdf/2301.02419v1.pdf", "tokenized_text": "task shot_learning shot learning fsl aims inference containing labeled examples help base categories containing abundant labeled training samples numerous works fsl task vision transformers taken backbone fsl trials focusing backbone classification layer essentially shown comparable better performance tasks nontrivial efficiently finetune inreal world fsl scenarios end propose_a_novel propose novel efficient method facilitates finetuning fsl tasks come newly presented prefix tuning apt residual adapter task backbone tuning individually specifically apt prefix new key value pairs thatare self attention layer provide withtask specific information design form vectors handle potential domain gaps base novel data ensure apt deviate initial task specific propose_a_novel propose novel prototypical regularization similarity distribution prefix update procedure method outstandingperformance challenging meta dataset conduct_extensive conduct extensive efficacy"}
{"id": "nan", "abstract": "  Current human activity recognition (HAR) techniques regard activity labels asinteger class IDs without explicitly modeling the semantics of class labels. Weobserve that different activity names often have shared structures. Forexample, \"open door\" and \"open fridge\" both have \"open\" as the action; \"kickingsoccer ball\" and \"playing tennis ball\" both have \"ball\" as the object. Suchshared structures in label names can be translated to the similarity in sensorydata and modeling common structures would help uncover knowledge acrossdifferent activities, especially for activities with limited samples. In thispaper, we propose SHARE, a HAR framework that takes into account sharedstructures of label names for different activities. To exploit the sharedstructures, SHARE comprises an encoder for extracting features from inputsensory time series and a decoder for generating label names as a tokensequence. We also propose three label augmentation techniques to help the modelmore effectively capture semantic structures across activities, including abasic token-level augmentation, and two enhanced embedding-level andsequence-level augmentations utilizing the capabilities of pre-trained models.SHARE outperforms state-of-the-art HAR models in extensive experiments on sevenHAR benchmark datasets. We also evaluate in few-shot learning and labelimbalance settings and observe even more significant performance gap.", "title": "unleashing the power of shared label structures for human activity recognition", "url": "http://arxiv.org/pdf/2301.03462v2.pdf", "tokenized_text": "current human activity recognition har techniques regard activity labels class ids explicitly modeling semantics class labels weobserve different activity names shared structures forexample open door open open action playing object structures label names translated similarity modeling common structures help uncover knowledge acrossdifferent activities especially activities limited samples thispaper propose share har framework takes account label names different activities exploit share comprises encoder extracting features time series decoder generating label names propose label augmentation techniques help effectively capture semantic structures activities including token level augmentation enhanced embedding level level augmentations utilizing capabilities pre trained share outperforms state art har extensive_experiments extensive experiments benchmark_datasets benchmark datasets evaluate shot_learning shot learning settings observe significant performance gap"}
{"id": "nan", "abstract": "  Large pre-trained vision and language models have demonstrated remarkablecapacities for various tasks. However, solving the knowledge-based visualreasoning tasks remains challenging, which requires a model to comprehensivelyunderstand image content, connect the external world knowledge, and performstep-by-step reasoning to answer the questions correctly. To this end, wepropose a novel framework named Interactive Prompting Visual Reasoner (IPVR)for few-shot knowledge-based visual reasoning. IPVR contains three stages, see,think and confirm. The see stage scans the image and grounds the visual conceptcandidates with a visual perception model. The think stage adopts a pre-trainedlarge language model (LLM) to attend to the key concepts from candidatesadaptively. It then transforms them into text context for prompting with avisual captioning model and adopts the LLM to generate the answer. The confirmstage further uses the LLM to generate the supporting rationale to the answer,verify the generated rationale with a cross-modality classifier and ensure thatthe rationale can infer the predicted output consistently. We conductexperiments on a range of knowledge-based visual reasoning datasets. We foundour IPVR enjoys several benefits, 1). it achieves better performance than theprevious few-shot learning baselines; 2). it enjoys the total transparency andtrustworthiness of the whole reasoning process by providing rationales for eachreasoning step; 3). it is computation-efficient compared with other fine-tuningbaselines.", "title": "see, think, confirm interactive prompting between vision and language models for knowledgebased visual reasoning", "url": "http://arxiv.org/pdf/2301.05226v1.pdf", "tokenized_text": "large pre trained vision language_models language demonstrated tasks solving knowledge based tasks remains challenging requires image content connect external world knowledge step reasoning answer questions correctly end wepropose novel framework named interactive visual reasoner shot knowledge based visual reasoning contains stages think confirm stage image grounds visual visual perception think stage adopts pre language_model language llm attend key concepts transforms text context avisual captioning adopts llm generate answer uses llm generate supporting rationale answer verify generated rationale cross modality classifier ensure thatthe rationale infer predicted output consistently conductexperiments range knowledge based visual reasoning datasets enjoys benefits achieves better performance theprevious shot_learning shot learning baselines enjoys total transparency reasoning process providing rationales step computation efficient compared fine"}
{"id": "nan", "abstract": "  In recent years, pre-trained large language models (LLMs) have demonstratedremarkable efficiency in achieving an inference-time few-shot learningcapability known as in-context learning. However, existing literature hashighlighted the sensitivity of this capability to the selection of few-shotdemonstrations. Current understandings of the underlying mechanisms by whichthis capability arises from regular language model pretraining objectivesremain disconnected from the real-world LLMs. This study aims to examine thein-context learning phenomenon through a Bayesian lens, viewing real-world LLMsas latent variable models. On this premise, we propose an algorithm to selectoptimal demonstrations from a set of annotated data with a small LM, and thendirectly generalize the selected demonstrations to larger LMs. We demonstratesignificant improvement over baselines, averaged over eight GPT models on eightreal-world text classification datasets. We also demonstrate the real-worldusefulness of our algorithm on GSM8K, a math word problem dataset. Ourempirical findings support our hypothesis that LLMs implicitly infer a latentvariable containing task information.", "title": "large language models are latent variable models explaining and finding good demonstrations for incontext learning", "url": "http://arxiv.org/pdf/2301.11916v3.pdf", "tokenized_text": "recent_years recent years pre trained large_language large language llms demonstratedremarkable efficiency achieving inference time shot learningcapability known context_learning context learning existing literature sensitivity capability selection shotdemonstrations current understandings underlying mechanisms capability arises regular language_model language pretraining real world llms study aims examine thein context_learning context learning phenomenon bayesian lens viewing real world latent variable premise propose algorithm demonstrations set annotated_data annotated data small lm generalize selected demonstrations larger lms improvement baselines averaged gpt world text_classification text classification datasets demonstrate real algorithm gsm8 math word problem dataset ourempirical findings support hypothesis llms implicitly infer containing task information"}
{"id": "nan", "abstract": "  Recent progress in scaling up large language models has shown impressivecapabilities in performing few-shot learning across a wide range of text-basedtasks. However, a key limitation is that these language models fundamentallylack visual perception - a crucial attribute needed to extend these models tobe able to interact with the real world and solve vision tasks, such as invisual-question answering and robotics. Prior works have largely connectedimage to text through pretraining and/or fine-tuning on curated image-textdatasets, which can be a costly and expensive process. In order to resolve thislimitation, we propose a simple yet effective approach calledLanguage-Quantized AutoEncoder (LQAE), a modification of VQ-VAE that learns toalign text-image data in an unsupervised manner by leveraging pretrainedlanguage models (e.g., BERT, RoBERTa). Our main idea is to encode image assequences of text tokens by directly quantizing image embeddings using apretrained language codebook. We then apply random masking followed by a BERTmodel, and have the decoder reconstruct the original image from BERT predictedtext token embeddings. By doing so, LQAE learns to represent similar imageswith similar clusters of text tokens, thereby aligning these two modalitieswithout the use of aligned text-image pairs. This enables few-shot imageclassification with large language models (e.g., GPT-3) as well as linearclassification of images based on BERT text features. To the best of ourknowledge, our work is the first work that uses unaligned images for multimodaltasks by leveraging the power of pretrained language models.", "title": "language quantized autoencoders towards unsupervised textimage alignment", "url": "http://arxiv.org/pdf/2302.00902v2.pdf", "tokenized_text": "recent progress scaling large_language large language shown impressivecapabilities performing shot_learning shot learning wide_range wide range text key limitation language_models language visual perception crucial attribute needed extend tobe able interact real_world real world solve vision tasks question_answering question answering robotics prior works largely text pretraining and/or fine tuning curated image textdatasets costly expensive process order resolve thislimitation propose simple effective approach quantized autoencoder modification vq vae learns text image data unsupervised manner leveraging pretrainedlanguage e.g. bert roberta main idea encode image text tokens directly quantizing image embeddings apretrained language codebook apply random masking followed decoder reconstruct original image bert token embeddings learns represent similar similar clusters text tokens aligning use aligned text image pairs enables shot imageclassification large_language large language e.g. gpt-3 images based bert text features best ourknowledge work work uses images multimodaltasks leveraging power pretrained_language pretrained language"}
{"id": "nan", "abstract": "  We demonstrate the potential of few-shot translation systems, trained withunpaired language data, for both high and low-resource language pairs. We showthat with only 5 examples of high-quality translation data shown at inference,a transformer decoder-only model trained solely with self-supervised learning,is able to match specialized supervised state-of-the-art models as well as moregeneral commercial translation systems. In particular, we outperform the bestperforming system on the WMT'21 English - Chinese news translation task by onlyusing five examples of English - Chinese parallel data at inference. Moreover,our approach in building these models does not necessitate joint multilingualtraining or back-translation, is conceptually simple and shows the potential toextend to the multilingual setting. Furthermore, the resulting models are twoorders of magnitude smaller than state-of-the-art language models. We thenanalyze the factors which impact the performance of few-shot translationsystems, and highlight that the quality of the few-shot demonstrations heavilydetermines the quality of the translations generated by our models. Finally, weshow that the few-shot paradigm also provides a way to control certainattributes of the translation -- we show that we are able to control forregional varieties and formality using only a five examples at inference,paving the way towards controllable machine translation systems.", "title": "the unreasonable effectiveness of fewshot learning for machine translation", "url": "http://arxiv.org/pdf/2302.01398v1.pdf", "tokenized_text": "demonstrate potential shot translation systems trained language data high low resource language pairs showthat examples high quality translation data shown inference transformer decoder trained solely self supervised learning able match specialized supervised state art moregeneral commercial translation systems particular outperform system english chinese news translation task onlyusing examples english chinese parallel data inference approach building necessitate joint translation conceptually simple shows potential multilingual setting furthermore resulting magnitude smaller state art language_models language factors impact performance shot highlight quality shot demonstrations quality translations generated finally weshow shot paradigm provides way control translation able control varieties formality examples inference paving way controllable machine_translation machine translation systems"}
{"id": "nan", "abstract": "  Despite the recent advances showing that a model pre-trained on large-scalesource code data is able to gain appreciable generalization capability, itstill requires a sizeable amount of data on the target task for fine-tuning.And the effectiveness of the model generalization is largely affected by thesize and quality of the fine-tuning data, which is detrimental for target taskswith limited or unavailable resources. Therefore, cross-task generalization,with the goal of improving the generalization of the model to unseen tasks thathave not been seen before, is of strong research and application value.  In this paper, we propose a large-scale benchmark that includes 216 existingcode-related tasks. Then, we annotate each task with the corresponding metainformation such as task description and instruction, which contains detailedinformation about the task and a solution guide. This also helps us to easilycreate a wide variety of ``training/evaluation'' task splits to evaluate thevarious cross-task generalization capabilities of the model. Then we performsome preliminary experiments to demonstrate that the cross-task generalizationof models can be largely improved by in-context learning methods such asfew-shot learning and learning from task instructions, which shows thepromising prospects of conducting cross-task learning research on ourbenchmark. We hope that the collection of the datasets and our benchmark willfacilitate future work that is not limited to cross-task generalization.", "title": "crosscodebench benchmarking crosstask generalization of source code models", "url": "http://arxiv.org/pdf/2302.04030v2.pdf", "tokenized_text": "despite recent_advances recent advances showing pre trained large code data able gain generalization capability requires sizeable data target task fine tuning effectiveness generalization largely affected thesize quality fine tuning data detrimental target taskswith limited unavailable resources cross task generalization goal improving generalization unseen tasks seen strong research application value paper propose large scale benchmark includes related tasks annotate task corresponding task description instruction contains task solution guide helps wide variety training evaluation task splits evaluate thevarious cross task generalization capabilities preliminary experiments demonstrate cross task largely improved context_learning context learning methods asfew shot_learning shot learning learning task instructions shows thepromising prospects conducting cross task learning research hope collection datasets benchmark future work limited cross task generalization"}
{"id": "nan", "abstract": "  Augmenting pretrained language models (LMs) with a vision encoder (e.g.,Flamingo) has obtained the state-of-the-art results in image-to-textgeneration. However, these models store all the knowledge within theirparameters, thus often requiring enormous model parameters to model theabundant visual concepts and very rich textual descriptions. Additionally, theyare inefficient in incorporating new data, requiring a computational-expensivefine-tuning process. In this work, we introduce a Retrieval-augmented VisualLanguage Model, Re-ViLM, built upon the Flamingo, that supports retrieving therelevant knowledge from the external database for zero and in-context few-shotimage-to-text generations. By storing certain knowledge explicitly in theexternal database, our approach reduces the number of model parameters and caneasily accommodate new data during evaluation by simply updating the database.We also construct an interleaved image and text data that facilitatesin-context few-shot learning capabilities. We demonstrate that Re-ViLMsignificantly boosts performance for image-to-text generation tasks, especiallyfor zero-shot and few-shot generation in out-of-domain settings with 4 timesless parameters compared with baseline methods.", "title": "revilm retrievalaugmented visual language model for zero and fewshot image captioning", "url": "http://arxiv.org/pdf/2302.04858v2.pdf", "tokenized_text": "augmenting pretrained_language pretrained language lms vision encoder e.g. obtained state art results image textgeneration store knowledge requiring enormous parameters visual concepts rich textual descriptions additionally inefficient incorporating new data requiring computational tuning process work introduce retrieval augmented built flamingo supports retrieving knowledge external database zero context shotimage text generations storing certain knowledge explicitly database approach reduces number parameters accommodate new data evaluation simply updating database construct interleaved image text data context shot_learning shot learning capabilities demonstrate boosts performance image text generation tasks especiallyfor zero shot shot generation domain settings parameters compared baseline methods"}
{"id": "nan", "abstract": "  Transformer-based language models have achieved significant success invarious domains. However, the data-intensive nature of the transformerarchitecture requires much labeled data, which is challenging in low-resourcescenarios (i.e., few-shot learning (FSL)). The main challenge of FSL is thedifficulty of training robust models on small amounts of samples, whichfrequently leads to overfitting. Here we present Mask-BERT, a simple andmodular framework to help BERT-based architectures tackle FSL. The proposedapproach fundamentally differs from existing FSL strategies such as prompttuning and meta-learning. The core idea is to selectively apply masks on textinputs and filter out irrelevant information, which guides the model to focuson discriminative tokens that influence prediction results. In addition, tomake the text representations from different categories more separable and thetext representations from the same category more compact, we introduce acontrastive learning loss function. Experimental results on public-domainbenchmark datasets demonstrate the effectiveness of Mask-BERT.", "title": "maskguided bert for few shot text classification", "url": "http://arxiv.org/pdf/2302.10447v3.pdf", "tokenized_text": "transformer based language_models language achieved significant success invarious domains data intensive nature transformerarchitecture requires labeled_data labeled data challenging low resourcescenarios i.e. shot_learning shot learning fsl main challenge fsl thedifficulty training robust small amounts samples leads overfitting present mask bert simple framework help bert based architectures tackle fsl proposedapproach fundamentally existing fsl strategies prompttuning meta learning core idea selectively apply masks filter irrelevant information guides discriminative tokens influence prediction results addition tomake text representations different categories thetext representations category compact introduce learning loss function experimental_results experimental results public datasets demonstrate_the_effectiveness demonstrate effectiveness mask bert"}
{"id": "nan", "abstract": "  Sequential recommenders have made great strides in capturing a user'spreferences. Nevertheless, the cold-start recommendation remains a fundamentalchallenge as they typically involve limited user-item interactions forpersonalization. Recently, gradient-based meta-learning approaches have emergedin the sequential recommendation field due to their fast adaptation andeasy-to-integrate abilities. The meta-learning algorithms formulate thecold-start recommendation as a few-shot learning problem, where each user isrepresented as a task to be adapted. While meta-learning algorithms generallyassume that task-wise samples are evenly distributed over classes or values,user-item interactions in real-world applications do not conform to such adistribution (e.g., watching favorite videos multiple times, leaving onlypositive ratings without any negative ones). Consequently, imbalanced userfeedback, which accounts for the majority of task training data, may dominatethe user adaptation process and prevent meta-learning algorithms from learningmeaningful meta-knowledge for personalized recommendations. To alleviate thislimitation, we propose a novel sequential recommendation framework based ongradient-based meta-learning that captures the imbalanced rating distributionof each user and computes adaptive loss for user-specific learning. Our work isthe first to tackle the impact of imbalanced ratings in cold-start sequentialrecommendation scenarios. Through extensive experiments conducted on real-worlddatasets, we demonstrate the effectiveness of our framework.", "title": "metalearning with adaptive weighted loss for imbalanced coldstart recommendation", "url": "http://arxiv.org/pdf/2302.14640v2.pdf", "tokenized_text": "sequential recommenders great strides capturing cold start recommendation remains typically involve limited user item interactions recently gradient based meta learning approaches sequential recommendation field fast adaptation integrate abilities meta learning algorithms formulate start recommendation shot_learning shot learning problem user task adapted meta learning algorithms task wise samples distributed classes values user item interactions real world_applications world applications conform e.g. videos multiple times leaving ratings negative ones consequently imbalanced accounts majority task training_data training data user adaptation process prevent meta learning algorithms meta knowledge personalized recommendations alleviate thislimitation propose_a_novel propose novel sequential recommendation framework based based meta learning captures imbalanced rating user adaptive loss user specific learning work isthe tackle impact imbalanced ratings cold start scenarios extensive_experiments extensive experiments conducted real demonstrate_the_effectiveness demonstrate effectiveness framework"}
{"id": "nan", "abstract": "  Visual Relation Detection (VRD) aims to detect relationships between objectsfor image understanding. Most existing VRD methods rely on thousands oftraining samples of each relationship to achieve satisfactory performance. Somerecent papers tackle this problem by few-shot learning with elaboratelydesigned pipelines and pre-trained word vectors. However, the performance ofexisting few-shot VRD models is severely hampered by the poor generalizationcapability, as they struggle to handle the vast semantic diversity of visualrelationships. Nonetheless, humans have the ability to learn new relationshipswith just few examples based on their knowledge. Inspired by this, we devise aknowledge-augmented, few-shot VRD framework leveraging both textual knowledgeand visual relation knowledge to improve the generalization ability of few-shotVRD. The textual knowledge and visual relation knowledge are acquired from apre-trained language model and an automatically constructed visual relationknowledge graph, respectively. We extensively validate the effectiveness of ourframework. Experiments conducted on three benchmarks from the commonly usedVisual Genome dataset show that our performance surpasses existingstate-of-the-art models with a large improvement.", "title": "knowledgeaugmented fewshot visual relation detection", "url": "http://arxiv.org/pdf/2303.05342v1.pdf", "tokenized_text": "visual relation detection aims detect relationships image understanding existing methods rely thousands oftraining samples relationship achieve satisfactory performance papers tackle problem shot_learning shot learning pipelines pre trained word vectors performance ofexisting shot severely hampered poor generalizationcapability struggle handle vast semantic diversity nonetheless humans ability learn new examples based knowledge inspired devise aknowledge augmented shot framework leveraging textual visual relation knowledge improve generalization_ability generalization ability textual knowledge visual relation knowledge acquired apre trained_language trained language automatically constructed visual graph respectively extensively validate effectiveness ourframework experiments conducted benchmarks commonly genome dataset performance surpasses existingstate art large improvement"}
{"id": "nan", "abstract": "  Online propaganda poses a severe threat to the integrity of societies.However, existing datasets for detecting online propaganda have a keylimitation: they were annotated using weak labels that can be noisy and evenincorrect. To address this limitation, our work makes the followingcontributions: (1) We present HQP: a novel dataset (N=30,000) for detectingonline propaganda with high-quality labels. To the best of our knowledge, HQPis the first dataset for detecting online propaganda that was created throughhuman annotation. (2) We show empirically that state-of-the-art language modelsfail in detecting online propaganda when trained with weak labels (AUC: 64.03).In contrast, state-of-the-art language models can accurately detect onlinepropaganda when trained with our high-quality labels (AUC: 92.25), which is animprovement of ~44%. (3) To address the cost of labeling, we extend our work tofew-shot learning. Specifically, we show that prompt-based learning using asmall sample of high-quality labels can still achieve a reasonable performance(AUC: 80.27). Finally, we discuss implications for the NLP community to balancethe cost and quality of labeling. Crucially, our work highlights the importanceof high-quality labels for sensitive NLP tasks such as propaganda detection.", "title": "hqp a humanannotated dataset for detecting online propaganda", "url": "http://arxiv.org/pdf/2304.14931v2.pdf", "tokenized_text": "online propaganda poses severe threat integrity existing datasets detecting online propaganda annotated weak labels noisy address limitation work makes present novel dataset propaganda high quality labels best knowledge dataset detecting online propaganda created annotation empirically state art language detecting online propaganda trained weak labels auc contrast state art language_models language accurately detect trained high quality labels auc address cost labeling extend work tofew shot_learning shot learning specifically based learning asmall sample high quality labels achieve reasonable finally discuss implications nlp community cost quality labeling crucially work highlights high quality labels sensitive nlp_tasks nlp tasks propaganda detection"}
{"id": "nan", "abstract": "  Pre-trained vision and language models such as CLIP have witnessed remarkablesuccess in connecting images and texts with a primary focus on English texts.Despite recent efforts to extend CLIP to support other languages, disparitiesin performance among different languages have been observed due to unevenresource availability. Additionally, current cross-lingual transfer methods ofthose pre-trained models would consume excessive resources for a large numberof languages. Therefore, we propose a new parameter-efficient cross-lingualtransfer learning framework that utilizes a translation-based alignment methodto mitigate multilingual disparities and explores parameter-efficientfine-tuning methods for parameter-efficient cross-lingual transfer. Extensiveexperiments on XTD and Multi30K datasets, covering 11 languages underzero-shot, few-shot, and full-dataset learning scenarios, show that ourframework significantly reduces the multilingual disparities among languagesand improves cross-lingual transfer results, especially in low-resourcescenarios, while only keeping and fine-tuning an extremely small number ofparameters compared to the full model (e.g., Our framework only requires 0.16\\%additional parameters of a full-model for each language in the few-shotlearning scenario). The codes are available at\\url{https://github.com/eric-ai-lab/PECTVLM}. The codes are available at\\url{https://github.com/eric-ai-lab/PECTVLM}.", "title": "parameterefficient crosslingual transfer of vision and language models via translationbased alignment", "url": "http://arxiv.org/pdf/2305.03510v2.pdf", "tokenized_text": "pre trained vision language_models language clip witnessed connecting images texts primary focus english texts despite recent efforts extend clip support languages performance different languages observed availability additionally current cross lingual_transfer lingual transfer methods pre trained consume excessive resources large numberof languages propose_a_new propose new parameter efficient cross learning framework utilizes translation based alignment methodto mitigate multilingual disparities explores parameter efficientfine tuning methods parameter efficient cross lingual_transfer lingual transfer extensiveexperiments datasets covering 11 languages shot shot dataset learning scenarios ourframework significantly reduces multilingual disparities improves cross lingual_transfer lingual transfer results especially low resourcescenarios keeping fine tuning extremely small_number small number ofparameters compared e.g. framework requires parameters language shotlearning scenario codes available at\\url{https://github.com ai lab codes available at\\url{https://github.com ai lab"}
{"id": "nan", "abstract": "  Generative Pre-Training (GPT) models like ChatGPT have demonstratedexceptional performance in various Natural Language Processing (NLP) tasks.Although ChatGPT has been integrated into the overall workflow to boostefficiency in many domains, the lack of flexibility in the finetuning processhinders its applications in areas that demand extensive domain expertise andsemantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT onthe China National Medical Licensing Examination (CNMLE) and propose a novelapproach to improve ChatGPT from two perspectives: integrating medical domainknowledge and enabling few-shot learning. By using a simple but effectiveretrieval method, medical background knowledge is extracted as semanticinstructions to guide the inference of ChatGPT. Similarly, relevant medicalquestions are identified and fed as demonstrations to ChatGPT. Experimentalresults show that directly applying ChatGPT fails to qualify the CNMLE at ascore of 51 (i.e., only 51\\% of questions are answered correctly). While ourknowledge-enhanced model achieves a high score of 70 on CNMLE-2022 which notonly passes the qualification but also surpasses the average score of humans(61). This research demonstrates the potential of knowledge-enhanced ChatGPT toserve as versatile medical assistants, capable of analyzing real-world medicalproblems in a more accessible, user-friendly, and adaptable manner.", "title": "qualifying chinese medical licensing examination with knowledge enhanced generative pretraining model", "url": "http://arxiv.org/pdf/2305.10163v2.pdf", "tokenized_text": "generative_pre-training generative pre-training gpt like_chatgpt like chatgpt performance natural_language natural language processing nlp tasks chatgpt integrated overall workflow domains lack flexibility finetuning applications areas demand extensive domain expertise knowledge healthcare paper evaluate chatgpt onthe national medical examination propose novelapproach improve chatgpt perspectives integrating medical domainknowledge enabling shot_learning shot learning simple method medical background knowledge extracted guide inference chatgpt similarly relevant identified fed demonstrations chatgpt experimentalresults directly applying chatgpt fails 51 i.e. questions answered correctly ourknowledge enhanced achieves high score 70 notonly passes surpasses average score research demonstrates potential knowledge enhanced chatgpt versatile medical assistants capable analyzing real world accessible user friendly adaptable manner"}
{"id": "nan", "abstract": "  Sentiment analysis (SA) has been a long-standing research area in naturallanguage processing. It can offer rich insights into human sentiments andopinions and has thus seen considerable interest from both academia andindustry. With the advent of large language models (LLMs) such as ChatGPT,there is a great potential for their employment on SA problems. However, theextent to which existing LLMs can be leveraged for different sentiment analysistasks remains unclear. This paper aims to provide a comprehensive investigationinto the capabilities of LLMs in performing various sentiment analysis tasks,from conventional sentiment classification to aspect-based sentiment analysisand multifaceted analysis of subjective texts. We evaluate performance across13 tasks on 26 datasets and compare the results against small language models(SLMs) trained on domain-specific datasets. Our study reveals that while LLMsdemonstrate satisfactory performance in simpler tasks, they lag behind in morecomplex tasks requiring deeper understanding or structured sentimentinformation. However, LLMs significantly outperform SLMs in few-shot learningsettings, suggesting their potential when annotation resources are limited. Wealso highlight the limitations of current evaluation practices in assessingLLMs' SA abilities and propose a novel benchmark, \\textsc{SentiEval}, for amore comprehensive and realistic evaluation. Data and code during ourinvestigations are available at\\url{https://github.com/DAMO-NLP-SG/LLM-Sentiment}.", "title": "sentiment analysis in the era of large language models a reality check", "url": "http://arxiv.org/pdf/2305.15005v1.pdf", "tokenized_text": "sentiment_analysis sentiment analysis long standing research area naturallanguage processing offer rich insights human sentiments seen considerable interest academia advent large_language large language llms chatgpt great_potential great potential employment problems theextent existing llms leveraged different sentiment remains unclear paper aims provide comprehensive investigationinto capabilities llms performing sentiment_analysis sentiment analysis tasks conventional sentiment classification aspect based sentiment multifaceted analysis subjective texts evaluate performance tasks 26 datasets compare results small language trained domain specific datasets study reveals satisfactory performance simpler tasks lag morecomplex tasks requiring deeper understanding structured llms significantly outperform slms shot learningsettings suggesting potential annotation resources limited wealso highlight limitations current evaluation practices abilities propose_a_novel propose novel benchmark amore comprehensive realistic evaluation data code available at\\url{https://github.com nlp sg llm sentiment"}
{"id": "nan", "abstract": "  Software specifications are essential for ensuring the reliability ofsoftware systems. Existing specification extraction approaches, however, sufferfrom limited generalizability and require manual efforts. The recent emergenceof Large Language Models (LLMs), which have been successfully applied tonumerous software engineering tasks, offers a promising avenue for automatingthis process. In this paper, we conduct the first empirical study to evaluatethe capabilities of LLMs for generating software specifications from softwarecomments or documentation. We evaluate LLMs' performance with Few Shot Learning(FSL), enabling LLMs to generalize from a small number of examples, as well asdifferent prompt construction strategies, and compare the performance of LLMswith traditional approaches. Additionally, we conduct a comparative diagnosisof the failure cases from both LLMs and traditional methods, identifying theirunique strengths and weaknesses. Lastly, we conduct extensive experiments on 15state of the art LLMs, evaluating their performance and cost effectiveness forgenerating software specifications.  Our results show that with FSL, LLMs outperform traditional methods (by5.6%), and more sophisticated prompt construction strategies can furtherenlarge this performance gap (up to 5.1 to 10.0%). Yet, LLMs suffer from theirunique challenges, such as ineffective prompts and the lack of domainknowledge, which together account for 53 to 60% of LLM unique failures. Thestrong performance of open source models (e.g., StarCoder) makes closed sourcemodels (e.g., GPT 3 Davinci) less desirable due to size and cost. Our studyoffers valuable insights for future research to improve specificationgeneration.", "title": "impact of large language models on generating software specifications", "url": "http://arxiv.org/pdf/2306.03324v2.pdf", "tokenized_text": "software specifications essential ensuring reliability ofsoftware systems existing specification extraction approaches sufferfrom limited generalizability require manual efforts recent large_language large language llms successfully applied software engineering tasks offers promising avenue process paper conduct empirical study capabilities llms generating software specifications documentation evaluate llms performance shot enabling llms generalize small_number small number examples construction strategies compare performance llmswith traditional approaches additionally conduct comparative failure cases llms traditional methods identifying strengths weaknesses lastly conduct_extensive conduct extensive experiments art llms evaluating performance cost effectiveness forgenerating software specifications results fsl llms outperform traditional methods sophisticated construction strategies performance gap 5.1 10.0 llms suffer challenges ineffective lack domainknowledge account 53 60 llm unique failures performance open_source open source e.g. starcoder makes closed e.g. gpt davinci desirable size cost valuable insights future_research future research improve"}
{"id": "nan", "abstract": "  Recently, CLIP-based approaches have exhibited remarkable performance ongeneralization and few-shot learning tasks, fueled by the power of contrastivelanguage-vision pre-training. In particular, prompt tuning has emerged as aneffective strategy to adapt the pre-trained language-vision models todownstream tasks by employing task-related textual tokens. Motivated by thisprogress, in this work we question whether other fundamental problems, such asweakly supervised semantic segmentation (WSSS), can benefit from prompt tuning.Our findings reveal two interesting observations that shed light on the impactof prompt tuning on WSSS. First, modifying only the class token of the textprompt results in a greater impact on the Class Activation Map (CAM), comparedto arguably more complex strategies that optimize the context. And second, theclass token associated with the image ground truth does not necessarilycorrespond to the category that yields the best CAM. Motivated by theseobservations, we introduce a novel approach based on a PrOmpt cLass lEarning(POLE) strategy. Through extensive experiments we demonstrate that our simple,yet efficient approach achieves SOTA performance in a well-known WSSSbenchmark. These results highlight not only the benefits of language-visionmodels in WSSS but also the potential of prompt learning for this problem. Thecode is available at https://github.com/rB080/WSS_POLE.", "title": "prompting classes exploring the power of prompt class learning in weakly supervised semantic segmentation", "url": "http://arxiv.org/pdf/2307.00097v2.pdf", "tokenized_text": "recently clip based approaches exhibited remarkable performance shot_learning shot learning tasks power vision pre training particular tuning emerged strategy adapt pre trained_language trained language vision todownstream tasks employing task related textual tokens motivated work question fundamental problems supervised semantic segmentation wsss benefit tuning findings reveal interesting observations shed light impactof tuning wsss modifying class token textprompt results greater impact class activation_map activation map cam comparedto arguably complex strategies optimize context second token associated image ground_truth ground truth category yields best cam motivated introduce novel_approach novel approach based class strategy extensive_experiments extensive experiments demonstrate simple efficient approach achieves sota performance known results highlight benefits language wsss potential learning problem available"}
{"id": "nan", "abstract": "  Modern image classification is based upon directly predicting classes vialarge discriminative networks, which do not directly contain information aboutthe intuitive visual features that may constitute a classification decision.Recently, work in vision-language models (VLM) such as CLIP has provided waysto specify natural language descriptions of image classes, but typicallyfocuses on providing single descriptions for each class. In this work, wedemonstrate that an alternative approach, in line with humans' understanding ofmultiple visual features per class, can also provide compelling performance inthe robust few-shot learning setting. In particular, we introduce a novelmethod, \\textit{SLR-AVD (Sparse Logistic Regression using Augmented VisualDescriptors)}. This method first automatically generates multiple visualdescriptions of each class via a large language model (LLM), then uses a VLM totranslate these descriptions to a set of visual feature embeddings of eachimage, and finally uses sparse logistic regression to select a relevant subsetof these features to classify each image. Core to our approach is the factthat, information-theoretically, these descriptive features are more invariantto domain shift than traditional image embeddings, even though the VLM trainingprocess is not explicitly designed for invariant representation learning. Theseinvariant descriptive features also compose a better input compression scheme.When combined with finetuning, we show that SLR-AVD is able to outperformexisting state-of-the-art finetuning approaches on both in-distribution andout-of-distribution performance.", "title": "text descriptions are compressive and invariant representations for visual learning", "url": "http://arxiv.org/pdf/2307.04317v2.pdf", "tokenized_text": "modern image classification based directly predicting classes discriminative networks directly contain information aboutthe intuitive visual features constitute classification decision recently work vision language_models language vlm clip provided waysto specify natural_language natural language descriptions image classes providing single descriptions class work wedemonstrate alternative approach line humans understanding ofmultiple visual features class provide compelling performance inthe robust shot_learning shot learning setting particular introduce novelmethod sparse logistic regression augmented method automatically generates multiple class large_language large language llm uses vlm totranslate descriptions set visual feature embeddings eachimage finally uses sparse logistic regression select relevant features classify image core approach information theoretically descriptive features domain shift traditional image embeddings vlm explicitly designed invariant representation learning descriptive features compose better input compression scheme combined finetuning able state art finetuning approaches distribution andout distribution performance"}
{"id": "nan", "abstract": "  Despite advancements in conversational AI, language models encounterchallenges to handle diverse conversational tasks, and existing dialoguedataset collections often lack diversity and comprehensiveness. To tackle theseissues, we introduce DialogStudio: the largest and most diverse collection ofdialogue datasets, unified under a consistent format while preserving theiroriginal information. Our collection encompasses data from open-domaindialogues, task-oriented dialogues, natural language understanding,conversational recommendation, dialogue summarization, and knowledge-groundeddialogues, making it an incredibly rich and diverse resource for dialogueresearch and model training. To further enhance the utility of DialogStudio, weidentify the licenses for each dataset and design domain-aware prompts forselected dialogues to facilitate instruction-aware fine-tuning. Furthermore, wedevelop conversational AI models using the dataset collection, and ourexperiments in both zero-shot and few-shot learning scenarios demonstrate thesuperiority of DialogStudio. To improve transparency and support dataset andtask-based research, as well as language model pre-training, all datasets,licenses, codes, and models associated with DialogStudio are made publiclyaccessible at https://github.com/salesforce/DialogStudio", "title": "dialogstudio towards richest and most diverse unified dataset collection for conversational ai", "url": "http://arxiv.org/pdf/2307.10172v2.pdf", "tokenized_text": "despite advancements conversational ai language_models language handle diverse conversational tasks existing collections lack diversity comprehensiveness tackle introduce largest diverse collection datasets unified consistent format preserving information collection encompasses data open domaindialogues task oriented dialogues natural_language natural language understanding conversational recommendation dialogue summarization knowledge making incredibly rich diverse resource training enhance utility weidentify dataset design domain aware dialogues facilitate instruction aware fine tuning furthermore conversational ai dataset collection ourexperiments zero shot shot_learning shot learning scenarios demonstrate thesuperiority improve transparency support dataset based research language_model language pre training datasets codes associated"}
{"id": "nan", "abstract": "  Information extraction(IE) is a crucial subfield within natural languageprocessing. However, for the traditionally segmented approach to sentenceclassification and Named Entity Recognition, the intricate interactions betweenthese individual subtasks remain largely uninvestigated. In this study, wepropose an integrative analysis, converging sentence classification with NamedEntity Recognition, with the objective to unveil and comprehend the mutualreinforcement effect within these two information extraction subtasks. Toachieve this, we introduce a Sentence Classification and Named EntityRecognition Multi-task (SCNM) approach that combines Sentence Classification(SC) and Named Entity Recognition (NER). We develop a Sentence-to-LabelGeneration (SLG) framework for SCNM and construct a Wikipedia datasetcontaining both SC and NER. Using a format converter, we unify input formatsand employ a generative model to generate SC-labels, NER-labels, and associatedtext segments. We propose a Constraint Mechanism (CM) to improve generatedformat accuracy. Our results show SC accuracy increased by 1.13 points and NERby 1.06 points in SCNM compared to standalone tasks, with CM raising formataccuracy from 63.61 to 100. The findings indicate mutual reinforcement effectsbetween SC and NER, and integration enhances both tasks' performance. Weadditionally implemented the SLG framework on single SC task. It yieldedsuperior accuracies compared to the baseline on two distinct Japanese SCdatasets. Notably, in the experiment of few-shot learning, SLG framework showsmuch better performance than fine-tune method. These empirical findingscontribute additional evidence to affirm the efficacy of the SLG framework.", "title": "mutual reinforcement effects in japanese sentence classification and named entity recognition tasks", "url": "http://arxiv.org/pdf/2307.10291v2.pdf", "tokenized_text": "information crucial natural languageprocessing traditionally approach named_entity named entity recognition intricate interactions individual subtasks remain largely study wepropose analysis sentence classification recognition objective unveil comprehend effect information_extraction information extraction subtasks toachieve introduce sentence classification named entityrecognition multi task approach combines sentence named_entity named entity recognition ner develop sentence framework construct wikipedia sc ner format unify input formatsand employ generative generate sc labels ner labels segments propose constraint mechanism improve accuracy results sc accuracy increased points points compared standalone tasks raising 63.61 100 findings indicate mutual reinforcement sc ner integration enhances tasks performance implemented framework single sc task accuracies compared baseline distinct japanese notably experiment shot_learning shot learning framework better performance fine tune method empirical additional evidence efficacy framework"}
{"id": "nan", "abstract": "  Recently, large language models (LLMs) fine-tuned to follow human instructionhave exhibited significant capabilities in various English NLP tasks. However,their performance in grammatical error correction (GEC) tasks, particularly innon-English languages, remains significantly unexplored. In this paper, wedelve into abilities of instruction fine-tuned LLMs in Arabic GEC, a task madecomplex due to Arabic's rich morphology. Our findings suggest that variousprompting methods, coupled with (in-context) few-shot learning, demonstrateconsiderable effectiveness, with GPT-4 achieving up to $65.49$F\\textsubscript{1} score under expert prompting (approximately $5$ pointshigher than our established baseline). This highlights the potential of LLMs inlow-resource settings, offering a viable approach for generating usefulsynthetic data for model training. Despite these positive results, we find thatinstruction fine-tuned models, regardless of their size, significantlyunderperform compared to fully fine-tuned models of significantly smallersizes. This disparity highlights a substantial room for improvements for LLMs.Inspired by methods from low-resource machine translation, we also develop amethod exploiting synthetic data that significantly outperforms previous modelson two standard Arabic benchmarks. Our work sets new SoTA for Arabic GEC, with$72.19\\%$ and $73.26$ F$_{1}$ on the 2014 and 2015 QALB datasets, respectively.", "title": "chatgpt for arabic grammatical error correction", "url": "http://arxiv.org/pdf/2308.04492v1.pdf", "tokenized_text": "recently large_language large language llms fine tuned follow human exhibited significant capabilities english nlp_tasks nlp tasks performance grammatical error correction gec tasks particularly english languages remains significantly unexplored paper wedelve abilities instruction fine tuned llms arabic gec task arabic rich morphology findings_suggest findings suggest variousprompting methods coupled context shot_learning shot learning effectiveness gpt-4 achieving score expert approximately established baseline highlights potential llms resource settings offering viable approach generating data training despite positive results find thatinstruction fine tuned regardless size compared fully fine tuned significantly disparity highlights substantial room improvements llms inspired methods low resource machine_translation machine translation develop exploiting synthetic data significantly_outperforms significantly outperforms previous standard arabic benchmarks work sets new sota arabic gec 2015 datasets respectively"}
{"id": "nan", "abstract": "  The recent development and success of Large Language Models (LLMs)necessitate an evaluation of their performance across diverse NLP tasks indifferent languages. Although several frameworks have been developed and madepublicly available, their customization capabilities for specific tasks anddatasets are often complex for different users. In this study, we introduce theLLMeBench framework. Initially developed to evaluate Arabic NLP tasks usingOpenAI's GPT and BLOOM models; it can be seamlessly customized for any NLP taskand model, regardless of language. The framework also features zero- andfew-shot learning settings. A new custom dataset can be added in less than 10minutes, and users can use their own model API keys to evaluate the task athand. The developed framework has been already tested on 31 unique NLP tasksusing 53 publicly available datasets within 90 experimental setups, involvingapproximately 296K data points. We plan to open-source the framework for thecommunity (https://github.com/qcri/LLMeBench/). A video demonstrating theframework is available online (https://youtu.be/FkQn4UjYA0s).", "title": "llmebench a flexible framework for accelerating llms benchmarking", "url": "http://arxiv.org/pdf/2308.04945v1.pdf", "tokenized_text": "recent development success large_language large language evaluation performance diverse nlp_tasks nlp tasks indifferent languages frameworks developed available customization capabilities specific tasks anddatasets complex different users study introduce framework initially developed evaluate arabic nlp_tasks nlp tasks gpt bloom seamlessly customized nlp regardless language framework features zero- andfew shot_learning shot learning settings new custom dataset added users use api evaluate task developed framework tested 31 unique nlp 53 publicly_available publicly available datasets 90 experimental setups data points plan open source framework thecommunity video demonstrating available online"}
{"id": "nan", "abstract": "  In natural language processing, transformer-based large language models(LLMs) like GPT-x models developed by OpenAI have revolutionized the landscape.Despite their impressive capabilities, these models often encounter challengeswhen handling tasks that differ from their training data, resulting incompromised performance. To address this, few-shot learning has emerged as avaluable technique, allowing LLMs to adapt with minimal task-specific data. Oneinnovative strategy, known as Chain-of-Thought Prompting (CoT), has beenintroduced to guide LLMs in revealing cognitive processes during multi-stepreasoning. In this paper, we propose Code Chain-of-Thought~(CodeCoT), whichconsists of two components: the Vanilla CodeCoT and the Self-exam CodeCoT. Thelatter incorporates self-examination, empowering the model to iterativelygenerate code, formulate test cases, and refine its outputs. Specifically, theprocess entails the generation of test examples by the model corresponding tothe code it is tasked to implement. If it fails on the test examples, then itregenerates the code based on the erroneous code and associated error types.Through comprehensive experiments, we observed that both techniquessignificantly enhance code generation accuracy across various LLM variants. Ourevaluation results reveal that CodeCoT improves the code generationeffectiveness, including an unprecedented pass@1 accuracy of 79.27\\% using theSelf-exam CodeCoT approach on the gpt-3.5-turbo-0613 model in the HumanEvaldataset.", "title": "codecot and beyond learning to program and test like a developer", "url": "http://arxiv.org/pdf/2308.08784v1.pdf", "tokenized_text": "natural_language natural language processing transformer based large_language large language models(llms like gpt developed openai revolutionized landscape despite impressive capabilities encounter challengeswhen handling tasks differ training_data training data resulting performance address shot_learning shot learning emerged technique allowing llms adapt minimal task specific data strategy known chain thought_prompting thought cot guide llms revealing cognitive processes multi paper propose code components vanilla self exam incorporates self examination empowering code formulate test_cases test cases refine outputs specifically theprocess entails generation test examples corresponding tothe code tasked implement fails test examples code based erroneous code associated error types comprehensive experiments observed enhance code_generation code generation accuracy llm variants results reveal improves code including unprecedented pass@1 accuracy exam approach gpt-3.5"}
{"id": "nan", "abstract": "  Decision-making problems can be represented as mathematical optimizationmodels, finding wide applications in fields such as economics, engineering andmanufacturing, transportation, and health care. Optimization models aremathematical abstractions of the problem of making the best decision whilesatisfying a set of requirements or constraints. One of the primary barriers todeploying these models in practice is the challenge of helping practitionersunderstand and interpret such models, particularly when they are infeasible,meaning no decision satisfies all the constraints. Existing methods fordiagnosing infeasible optimization models often rely on expert systems,necessitating significant background knowledge in optimization. In this paper,we introduce OptiChat, a first-of-its-kind natural language-based systemequipped with a chatbot GUI for engaging in interactive conversations aboutinfeasible optimization models. OptiChat can provide natural languagedescriptions of the optimization model itself, identify potential sources ofinfeasibility, and offer suggestions to make the model feasible. Theimplementation of OptiChat is built on GPT-4, which interfaces with anoptimization solver to identify the minimal subset of constraints that renderthe entire optimization problem infeasible, also known as the IrreducibleInfeasible Subset (IIS). We utilize few-shot learning, expert chain-of-thought,key-retrieve, and sentiment prompts to enhance OptiChat's reliability. Ourexperiments demonstrate that OptiChat assists both expert and non-expert usersin improving their understanding of the optimization models, enabling them toquickly identify the sources of infeasibility.", "title": "diagnosing infeasible optimization problems using large language models", "url": "http://arxiv.org/pdf/2308.12923v1.pdf", "tokenized_text": "decision making problems represented mathematical finding wide applications fields engineering transportation health care optimization abstractions problem making best decision set requirements constraints primary barriers practice challenge helping interpret particularly infeasible meaning decision constraints existing_methods existing methods infeasible optimization rely expert systems necessitating significant background knowledge optimization paper introduce kind natural_language natural language based chatbot gui engaging interactive conversations optimization provide natural languagedescriptions optimization identify potential sources offer suggestions feasible theimplementation built gpt-4 interfaces solver identify minimal subset constraints entire optimization problem infeasible known subset utilize shot_learning shot learning expert chain thought key retrieve sentiment enhance reliability ourexperiments demonstrate assists expert non expert improving understanding optimization enabling identify sources"}
{"id": "nan", "abstract": "  Although large language models (LLMs) demonstrate impressive performance formany language tasks, most of them can only handle texts a few thousand tokenslong, limiting their applications on longer sequence inputs, such as books,reports, and codebases. Recent works have proposed methods to improve LLMs'long context capabilities by extending context windows and more sophisticatedmemory mechanisms. However, comprehensive benchmarks tailored for evaluatinglong context understanding are lacking. In this paper, we introduce LongBench,the first bilingual, multi-task benchmark for long context understanding,enabling a more rigorous evaluation of long context understanding. LongBenchcomprises 21 datasets across 6 task categories in both English and Chinese,with an average length of 6,711 words (English) and 13,386 characters(Chinese). These tasks cover key long-text application areas includingsingle-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,and code completion. All datasets in LongBench are standardized into a unifiedformat, allowing for effortless automatic evaluation of LLMs. Uponcomprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercialmodel (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but stillstruggles on longer contexts. (2) Scaled position embedding and fine-tuning onlonger sequences lead to substantial improvement on long context understanding.(3) Context compression technique such as retrieval brings improvement formodel with weak ability on long contexts, but the performance still lags behindmodels that have strong long context understanding capability. The code anddatasets are available at https://github.com/THUDM/LongBench.", "title": "longbench a bilingual, multitask benchmark for long context understanding", "url": "http://arxiv.org/pdf/2308.14508v1.pdf", "tokenized_text": "large_language large language llms demonstrate impressive performance formany language tasks handle texts limiting applications longer sequence inputs books reports recent works proposed methods improve context capabilities extending context windows mechanisms comprehensive benchmarks tailored context understanding lacking paper introduce longbench bilingual multi task benchmark long context understanding enabling rigorous evaluation long context understanding 21 datasets task categories english chinese average length words english tasks cover key long text application areas qa multi qa summarization shot_learning shot learning synthetic tasks code completion datasets longbench standardized allowing automatic evaluation llms evaluation llms longbench find gpt-3.5 outperforms open sourced longer contexts scaled position embedding fine tuning sequences lead substantial improvement long context context compression technique retrieval brings improvement weak ability long contexts performance lags strong long context understanding capability code anddatasets available"}
{"id": "nan", "abstract": "  In the evolving landscape of software development, Large Language Models(LLMs) exhibit a unique phenomenon known as emergent abilities, demonstratingadeptness across numerous tasks, from text summarization to code generation.While these abilities open up novel avenues in software design and crafting,their incorporation presents substantial challenges. Developers grapple withdecisions surrounding the direct embedding of LLMs within applications versusemploying them for code generation. Moreover, effective prompt design becomes acritical concern, given the necessity of data extraction from natural languageoutputs. To address these intricacies, this paper introduces AskIt, adomain-specific language (DSL) specifically designed for LLMs. AskIt simplifiesLLM integration, offering type-guided output control, template-based functiondefinitions, and a unified interface that diminishes the distinction betweenLLM-based code generation and application integration. Furthermore, throughProgramming by Example (PBE), AskIt harnesses the power of few-shot learning atthe programming language level. Our evaluations underscore AskIt's potency.Across 50 tasks, AskIt generated concise prompts for the given tasks, achievinga 16.14% reduction in prompt length relative to benchmarks. Additionally, byenabling the transition from direct LLM application usage to functiongeneration, AskIt achieved significant speedups, as observed in our GSM8Kbenchmark experiments. Through these advancements, AskIt streamlines theintegration of LLMs in software development, offering a more efficient,versatile approach for leveraging emergent abilities. The implementations ofAskIt in TypeScript and Python are available athttps://github.com/katsumiok/ts-askit and https://github.com/katsumiok/pyaskit,respectively.", "title": "askit unified programming interface for programming with large language models", "url": "http://arxiv.org/pdf/2308.15645v1.pdf", "tokenized_text": "evolving landscape software development large_language large language models(llms exhibit unique phenomenon known emergent abilities numerous tasks text summarization code_generation code generation abilities open novel avenues software design crafting incorporation presents substantial challenges developers grapple surrounding direct embedding llms applications code_generation code generation effective design acritical concern given necessity data extraction natural address intricacies paper introduces specific language dsl specifically designed llms integration offering type guided output control template based unified interface diminishes distinction based code_generation code generation application integration furthermore example harnesses power shot_learning shot learning atthe programming language level evaluations underscore potency 50 tasks generated concise given tasks reduction length relative benchmarks additionally transition direct llm application usage achieved significant speedups observed experiments advancements streamlines theintegration llms software development offering efficient versatile approach leveraging emergent abilities implementations python available"}
{"id": "nan", "abstract": "  Demographics, Social determinants of health, and family history documented inthe unstructured text within the electronic health records are increasinglybeing studied to understand how this information can be utilized with thestructured data to improve healthcare outcomes. After the GPT models werereleased, many studies have applied GPT models to extract this information fromthe narrative clinical notes. Different from the existing work, our researchfocuses on investigating the zero-shot learning on extracting this informationtogether by providing minimum information to the GPT model. We utilizede-identified real-world clinical notes annotated for demographics, varioussocial determinants, and family history information. Given that the GPT modelmight provide text different from the text in the original data, we explore twosets of evaluation metrics, including the traditional NER evaluation metricsand semantic similarity evaluation metrics, to completely understand theperformance. Our results show that the GPT-3.5 method achieved an average of0.975 F1 on demographics extraction, 0.615 F1 on social determinantsextraction, and 0.722 F1 on family history extraction. We believe these resultscan be further improved through model fine-tuning or few-shots learning.Through the case studies, we also identified the limitations of the GPT models,which need to be addressed in future research.", "title": "zeroshot learning with minimum instruction to extract social determinants and family history from clinical notes using gpt model", "url": "http://arxiv.org/pdf/2309.05475v2.pdf", "tokenized_text": "demographics social determinants health family history documented inthe unstructured text electronic health records studied understand information utilized data improve healthcare outcomes gpt studies applied gpt extract information fromthe narrative clinical notes different existing work researchfocuses investigating zero shot_learning shot learning extracting providing minimum information gpt identified real world clinical notes annotated demographics determinants family history information given gpt provide text different text original data explore evaluation metrics including traditional ner evaluation semantic similarity evaluation metrics completely understand theperformance results gpt-3.5 method achieved average f1 demographics extraction f1 social f1 family history extraction believe resultscan improved fine tuning shots learning case studies identified limitations gpt need addressed future_research future research"}
{"id": "nan", "abstract": "  Our work demonstrates that large language model (LLM) pre-trained on textscan not only solve pure math word problems, but also physics word problems,whose solution requires calculation and inference based on prior physicalknowledge. We collect and annotate the first physics word problemdataset-PhysQA, which contains over 1000 junior high school physics wordproblems (covering Kinematics, Mass&Density, Mechanics, Heat, Electricity).Then we use OpenAI' s GPT3.5 to generate the answer of these problems and foundthat GPT3.5 could automatically solve 49.3% of the problems through zero-shotlearning and 73.2% through few-shot learning. This result demonstrates that byusing similar problems and their answers as prompt, LLM could solve elementaryphysics word problems approaching human level performance. In addition tosolving problems, GPT3.5 can also summarize the knowledge or topics covered bythe problems, provide relevant explanations, and generate new physics wordproblems based on the input. Our work is the first research to focus on theautomatic solving, explanation, and generation of physics word problems acrossvarious types and scenarios, and we achieve an acceptable and state-of-the-artaccuracy. This underscores the potential of LLMs for further applications insecondary education.", "title": "using large language model to solve and explain physics word problems approaching human level", "url": "http://arxiv.org/pdf/2309.08182v2.pdf", "tokenized_text": "work demonstrates large_language large language llm pre trained solve pure math word problems physics word problems solution requires calculation inference based prior physicalknowledge collect annotate physics word contains 1000 high school physics covering mechanics use openai gpt3.5 generate answer problems gpt3.5 automatically solve problems zero shotlearning 73.2 shot_learning shot learning result demonstrates similar problems answers llm solve word problems approaching human level performance addition problems gpt3.5 summarize knowledge topics covered bythe problems provide relevant explanations generate new physics based input work research focus solving explanation generation physics word problems acrossvarious types scenarios achieve acceptable state artaccuracy underscores potential llms applications education"}
{"id": "nan", "abstract": "  The recent developments of foundation models in computer vision, especiallythe Segment Anything Model (SAM), allow scalable and domain-agnostic imagesegmentation to serve as a general-purpose segmentation tool. In parallel, thefield of medical image segmentation has benefited significantly fromspecialized neural networks like the nnUNet, which is trained ondomain-specific datasets and can automatically configure the network to tailorto specific segmentation challenges. To combine the advantages of foundationmodels and domain-specific models, we present nnSAM, which synergisticallyintegrates the SAM model with the nnUNet model to achieve more accurate androbust medical image segmentation. The nnSAM model leverages the powerful androbust feature extraction capabilities of SAM, while harnessing the automaticconfiguration capabilities of nnUNet to promote dataset-tailored learning. Ourcomprehensive evaluation of nnSAM model on different sizes of training samplesshows that it allows few-shot learning, which is highly relevant for medicalimage segmentation where high-quality, annotated data can be scarce and costlyto obtain. By melding the strengths of both its predecessors, nnSAM positionsitself as a potential new benchmark in medical image segmentation, offering atool that combines broad applicability with specialized efficiency. The code isavailable at https://github.com/Kent0n-Li/Medical-Image-Segmentation.", "title": "nnsam plugandplay segment anything model improves nnunet performance", "url": "http://arxiv.org/pdf/2309.16967v2.pdf", "tokenized_text": "recent developments foundation_models foundation computer_vision computer vision especiallythe segment sam allow scalable domain agnostic imagesegmentation serve general purpose segmentation tool parallel medical image segmentation benefited significantly neural_networks neural networks like trained ondomain specific datasets automatically configure network specific segmentation challenges combine advantages foundationmodels domain specific present sam achieve accurate medical image segmentation leverages powerful feature extraction capabilities sam harnessing capabilities promote dataset tailored learning ourcomprehensive evaluation different sizes training allows shot_learning shot learning highly relevant medicalimage segmentation high quality annotated_data annotated data scarce obtain strengths potential new benchmark medical image segmentation offering combines broad applicability specialized efficiency code isavailable"}
{"id": "nan", "abstract": "  Retrieval-augmented language models (RALMs) improve performance by accessinglong-tail and up-to-date knowledge from external data stores, but arechallenging to build. Existing approaches require either expensiveretrieval-specific modifications to LM pre-training or use post-hoc integrationof the data store that leads to suboptimal performance. We introduceRetrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuningmethodology that provides a third option by retrofitting any LLM with retrievalcapabilities. Our approach operates in two distinct fine-tuning steps: (1) oneupdates a pre-trained LM to better use retrieved information, while (2) theother updates the retriever to return more relevant results, as preferred bythe LM. By fine-tuning over tasks that require both knowledge utilization andcontextual awareness, we demonstrate that each stage yields significantperformance improvements, and using both leads to additional gains. Our bestmodel, RA-DIT 65B, achieves state-of-the-art performance across a range ofknowledge-intensive zero- and few-shot learning benchmarks, significantlyoutperforming existing in-context RALM approaches by up to +8.9% in 0-shotsetting and +1.4% in 5-shot setting on average.", "title": "radit retrievalaugmented dual instruction tuning", "url": "http://arxiv.org/pdf/2310.01352v3.pdf", "tokenized_text": "retrieval augmented language_models language improve performance tail date knowledge external data stores arechallenging build existing approaches require specific modifications lm pre training use post hoc data store leads suboptimal performance augmented dual instruction_tuning instruction tuning ra lightweight fine provides option retrofitting llm approach operates distinct fine tuning steps pre trained lm better use retrieved information theother updates retriever return relevant results preferred bythe lm fine tuning tasks require knowledge utilization awareness demonstrate stage yields improvements leads additional gains ra achieves_state achieves state art performance range ofknowledge intensive zero- shot_learning shot learning benchmarks significantlyoutperforming existing context approaches shotsetting shot_setting shot setting average"}
{"id": "nan", "abstract": "  Tabular data prediction is a fundamental machine learning task for manyapplications. Existing methods predominantly employ discriminative modeling andoperate under the assumption of a fixed target column, necessitatingre-training for every new predictive task. Inspired by the generative power oflarge language models (LLMs), this paper exploits the idea of buildinguniversal tabular data predictors based on generative modeling, namelyUniPredict. Here, we show that scaling up an LLM to extensive tabular datasetswith the capability of comprehending diverse tabular inputs and predicting fortarget variables following the input instructions. Specifically, we train asingle LLM on an aggregation of 169 tabular datasets with diverse targets andcompare its performance against baselines that are trained on each datasetseparately. We observe this versatile UniPredict model demonstrates anadvantage over other models, ranging from 5.4% to 13.4%, when compared with thebest tree-boosting baseline and the best neural network baseline, respectively.We further test UniPredict in few-shot learning settings on another 62 tabulardatasets. Our method achieves strong performance in quickly adapting to newtasks, where our method outperforms XGBoost over 100% on the low-resource setupand shows a significant margin over all baselines. We envision that UniPredictsheds light on developing a universal tabular data prediction system thatlearns from data at scale and serves a wide range of prediction tasks.", "title": "unipredict large language models are universal tabular predictors", "url": "http://arxiv.org/pdf/2310.03266v1.pdf", "tokenized_text": "tabular data prediction fundamental machine_learning machine learning task existing_methods existing methods predominantly employ discriminative modeling assumption fixed target training new predictive task inspired generative power oflarge language_models language llms paper exploits idea tabular data predictors based generative modeling scaling llm extensive tabular capability comprehending diverse tabular inputs predicting variables following input instructions specifically train asingle llm aggregation 169 tabular datasets diverse targets andcompare performance baselines trained observe versatile demonstrates ranging compared thebest tree boosting baseline best neural network baseline respectively test shot_learning shot learning settings method_achieves method achieves strong performance quickly adapting newtasks method outperforms 100 low resource shows significant margin baselines envision light developing universal tabular data prediction system data scale serves wide_range wide range prediction tasks"}
{"id": "nan", "abstract": "  In long context scenarios, large language models (LLMs) face three mainchallenges: higher computational/financial cost, longer latency, and inferiorperformance. Some studies reveal that the performance of LLMs depends on boththe density and the position of the key information (question relevant) in theinput prompt. Inspired by these findings, we propose LongLLMLingua for promptcompression towards improving LLMs' perception of the key information tosimultaneously address the three challenges. We conduct evaluation on a widerange of long context scenarios including single-/multi-document QA, few-shotlearning, summarization, synthetic tasks, and code completion. The experimentalresults show that LongLLMLingua compressed prompt can derive higher performancewith much less cost. The latency of the end-to-end system is also reduced. Forexample, on NaturalQuestions benchmark, LongLLMLingua gains a performance boostof up to 17.1% over the original prompt with ~4x fewer tokens as input toGPT-3.5-Turbo. It can derive cost savings of \\$28.5 and \\$27.4 per 1,000samples from the LongBench and ZeroScrolls benchmark, respectively.Additionally, when compressing prompts of ~10k tokens at a compression rate of2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x. Ourcode is available at https://aka.ms/LLMLingua.", "title": "longllmlingua accelerating and enhancing llms in long context scenarios via prompt compression", "url": "http://arxiv.org/pdf/2310.06839v1.pdf", "tokenized_text": "long context scenarios large_language large language llms face higher computational financial cost longer latency studies reveal performance llms depends density position key information question relevant theinput inspired findings propose improving llms perception key information address challenges conduct evaluation widerange long context scenarios including document qa shotlearning summarization synthetic tasks code completion experimentalresults compressed derive higher performancewith cost latency end end system reduced forexample benchmark gains performance original fewer tokens input turbo derive cost savings longbench benchmark respectively additionally compressing tokens compression rate speed end end latency ourcode available https://aka.ms/llmlingua"}
{"id": "nan", "abstract": "  Text-attributed graphs have recently garnered significant attention due totheir wide range of applications in web domains. Existing methodologies employword embedding models for acquiring text representations as node features,which are subsequently fed into Graph Neural Networks (GNNs) for training.Recently, the advent of Large Language Models (LLMs) has introduced theirpowerful capabilities in information retrieval and text generation, which cangreatly enhance the text attributes of graph data. Furthermore, the acquisitionand labeling of extensive datasets are both costly and time-consumingendeavors. Consequently, few-shot learning has emerged as a crucial problem inthe context of graph learning tasks. In order to tackle this challenge, wepropose a lightweight paradigm called ENG, which adopts a plug-and-playapproach to empower text-attributed graphs through node generation using LLMs.Specifically, we utilize LLMs to extract semantic information from the labelsand generate samples that belong to these categories as exemplars.Subsequently, we employ an edge predictor to capture the structural informationinherent in the raw dataset and integrate the newly generated samples into theoriginal graph. This approach harnesses LLMs for enhancing class-levelinformation and seamlessly introduces labeled nodes and edges without modifyingthe raw dataset, thereby facilitating the node classification task in few-shotscenarios. Extensive experiments demonstrate the outstanding performance of ourproposed paradigm, particularly in low-shot scenarios. For instance, in the1-shot setting of the ogbn-arxiv dataset, ENG achieves a 76% improvement overthe baseline model.", "title": "empower textattributed graphs learning with large language models (llms)", "url": "http://arxiv.org/pdf/2310.09872v1.pdf", "tokenized_text": "text attributed graphs recently garnered significant attention totheir wide_range wide range applications web domains existing methodologies embedding acquiring text representations node features subsequently fed graph neural_networks neural networks gnns training recently advent large_language large language llms introduced capabilities information retrieval text generation enhance text attributes graph data furthermore labeling extensive datasets costly time consequently shot_learning shot learning emerged crucial problem inthe context graph learning tasks order tackle challenge wepropose lightweight paradigm called adopts plug empower text attributed graphs node generation llms specifically utilize llms extract semantic information generate samples categories exemplars subsequently employ edge predictor capture structural raw dataset integrate newly generated samples theoriginal graph approach harnesses llms enhancing class seamlessly introduces labeled nodes edges raw dataset facilitating node classification task shotscenarios extensive_experiments extensive experiments demonstrate outstanding performance ourproposed paradigm particularly low shot scenarios instance shot_setting shot setting ogbn arxiv dataset achieves 76 improvement overthe baseline"}
{"id": "nan", "abstract": "  Spurred by advancements in scale, large language models (LLMs) havedemonstrated strong few-shot learning ability via in-context learning (ICL).However, the performance of ICL has been shown to be highly sensitive to theselection of few-shot demonstrations. Selecting the most suitable examples ascontext remains an ongoing challenge and an open problem. Existing literaturehas highlighted the importance of selecting examples that are diverse orsemantically similar to the test sample while ignoring the fact that theoptimal selection dimension, i.e., diversity or similarity, is task-specific.Leveraging the merits of both dimensions, we propose Iterative DemonstrationSelection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT),IDS iteratively selects examples that are diverse but still strongly correlatedwith the test sample as ICL demonstrations. Specifically, IDS appliesZero-shot-CoT to the test sample before demonstration selection. The outputreasoning path is then used to choose demonstrations that are prepended to thetest sample for inference. The generated answer is accompanied by itscorresponding reasoning path for extracting a new set of demonstrations in thenext iteration. After several iterations, IDS adopts majority voting to obtainthe final result. Through extensive experiments on tasks including commonsensereasoning, question answering, topic classification, and sentiment analysis, wedemonstrate that IDS can consistently outperform existing ICL demonstrationselection methods.", "title": "incontext learning with iterative demonstration selection", "url": "http://arxiv.org/pdf/2310.09881v2.pdf", "tokenized_text": "advancements scale large_language large language llms havedemonstrated strong shot_learning shot learning ability context_learning context learning performance icl shown highly sensitive shot demonstrations selecting suitable examples remains ongoing challenge open problem existing highlighted importance selecting examples diverse similar test sample ignoring fact selection dimension i.e. diversity similarity task specific leveraging merits dimensions propose iterative demonstrationselection ids zero shot chain thought reasoning zero shot iteratively selects examples diverse strongly test sample icl demonstrations specifically ids shot cot test sample demonstration selection path choose demonstrations prepended sample inference generated answer accompanied reasoning path extracting new set demonstrations iteration iterations ids adopts majority voting final result extensive_experiments extensive experiments tasks including commonsensereasoning question_answering question answering topic classification sentiment_analysis sentiment analysis wedemonstrate ids consistently outperform existing icl demonstrationselection methods"}
{"id": "nan", "abstract": "  Instruction tuned large language models (LLMs), such as ChatGPT, demonstrateremarkable performance in a wide range of tasks. Despite numerous recentstudies that examine the performance of instruction-tuned LLMs on various NLPbenchmarks, there remains a lack of comprehensive investigation into theirability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaningembedded within social and interactive contexts. This deficiency arises partlyfrom SM not being adequately represented in any of the existing benchmarks. Toaddress this gap, we present SPARROW, an extensive multilingual benchmarkspecifically designed for SM understanding. SPARROW comprises 169 datasetscovering 13 task types across six primary categories (e.g., anti-sociallanguage detection, emotion recognition). SPARROW datasets encompass 64different languages originating from 12 language families representing 16writing scripts. We evaluate the performance of various multilingual pretrainedlanguage models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT)on SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Ourcomprehensive analysis reveals that existing open-source instruction tuned LLMsstill struggle to understand SM across various languages, performing close to arandom baseline in some cases. We also find that although ChatGPT outperformsmany LLMs, it still falls behind task-specific finetuned models with a gap of12.19 SPARROW score. Our benchmark is available at:https://github.com/UBC-NLP/SPARROW", "title": "the skipped beat a study of sociopragmatic understanding in llms for 64 languages", "url": "http://arxiv.org/pdf/2310.14557v1.pdf", "tokenized_text": "instruction tuned large_language large language llms chatgpt performance wide_range wide range tasks despite numerous examine performance instruction tuned llms remains lack comprehensive investigation theirability understand cross lingual meaning i.e. social interactive contexts deficiency arises adequately represented existing benchmarks toaddress gap present extensive multilingual designed understanding comprises 169 13 task types primary categories e.g. anti detection emotion recognition datasets encompass languages 12 language families representing scripts evaluate performance multilingual pretrainedlanguage e.g. mt5 instruction tuned llms e.g. bloomz fine tuning zero shot and/or shot_learning shot learning ourcomprehensive analysis reveals existing open source instruction tuned struggle understand languages performing close baseline cases find chatgpt llms falls task specific finetuned gap score benchmark available https://github.com nlp"}
{"id": "nan", "abstract": "  Autonomous driving technology, a catalyst for revolutionizing transportationand urban mobility, has the tend to transition from rule-based systems todata-driven strategies. Traditional module-based systems are constrained bycumulative errors among cascaded modules and inflexible pre-set rules. Incontrast, end-to-end autonomous driving systems have the potential to avoiderror accumulation due to their fully data-driven training process, althoughthey often lack transparency due to their ``black box\" nature, complicating thevalidation and traceability of decisions. Recently, large language models(LLMs) have demonstrated abilities including understanding context, logicalreasoning, and generating answers. A natural thought is to utilize theseabilities to empower autonomous driving. By combining LLM with foundationvision models, it could open the door to open-world understanding, reasoning,and few-shot learning, which current autonomous driving systems are lacking. Inthis paper, we systematically review a research line about \\textit{LargeLanguage Models for Autonomous Driving (LLM4AD)}. This study evaluates thecurrent state of technological advancements, distinctly outlining the principalchallenges and prospective directions for the field. For the convenience ofresearchers in academia and industry, we provide real-time updates on thelatest advances in the field as well as relevant open-source resources via thedesignated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD.", "title": "a survey of large language models for autonomous driving", "url": "http://arxiv.org/pdf/2311.01043v1.pdf", "tokenized_text": "autonomous driving technology catalyst revolutionizing urban tend transition rule based systems driven strategies traditional module based systems constrained errors cascaded modules pre set rules incontrast end end autonomous driving systems potential accumulation fully data driven training process lack transparency black_box black box nature traceability decisions recently large_language large language models(llms demonstrated abilities including understanding context logicalreasoning generating answers natural thought utilize theseabilities empower autonomous driving combining llm open door open world understanding reasoning shot_learning shot learning current autonomous driving systems lacking inthis_paper inthis paper systematically review research line autonomous driving study evaluates thecurrent state technological advancements outlining prospective directions field convenience academia industry provide real time updates advances field relevant open source resources link"}
{"id": "nan", "abstract": "  This paper explores the limits of the current generation of large languagemodels for program synthesis in general purpose programming languages. Weevaluate a collection of such models (with between 244M and 137B parameters) ontwo new benchmarks, MBPP and MathQA-Python, in both the few-shot andfine-tuning regimes. Our benchmarks are designed to measure the ability ofthese models to synthesize short Python programs from natural languagedescriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974programming tasks, designed to be solvable by entry-level programmers. TheMathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914problems that evaluate the ability of the models to synthesize code from morecomplex text. On both datasets, we find that synthesis performance scaleslog-linearly with model size. Our largest models, even without finetuning on acode dataset, can synthesize solutions to 59.6 percent of the problems fromMBPP using few-shot learning with a well-designed prompt. Fine-tuning on aheld-out portion of the dataset improves performance by about 10 percentagepoints across most model sizes. On the MathQA-Python dataset, the largestfine-tuned model achieves 83.8 percent accuracy. Going further, we study themodel's ability to engage in dialog about code, incorporating human feedback toimprove its solutions. We find that natural language feedback from a humanhalves the error rate compared to the model's initial prediction. Additionally,we conduct an error analysis to shed light on where these models fall short andwhat types of programs are most difficult to generate. Finally, we explore thesemantic grounding of these models by fine-tuning them to predict the resultsof program execution. We find that even our best models are generally unable topredict the output of a program given a specific input.", "title": "program synthesis with large language models", "url": "http://arxiv.org/pdf/2108.07732v1.pdf", "tokenized_text": "paper explores limits current generation large_languagemodels large languagemodels program synthesis general_purpose general purpose programming languages weevaluate collection parameters ontwo new benchmarks mbpp mathqa python shot andfine tuning regimes benchmarks designed measure ability ofthese synthesize short python programs natural languagedescriptions basic programming problems mbpp dataset contains tasks designed solvable entry level programmers python dataset python version mathqa benchmark contains evaluate ability synthesize code morecomplex text datasets find synthesis performance linearly model_size size largest finetuning acode dataset synthesize solutions percent problems shot_learning shot learning designed fine tuning portion dataset improves performance 10 sizes mathqa python dataset tuned achieves 83.8 percent accuracy going study themodel ability engage dialog code incorporating human feedback toimprove solutions find natural_language natural language feedback error rate compared initial prediction additionally conduct error analysis shed light fall short types programs difficult generate finally explore thesemantic grounding fine tuning predict resultsof program execution find best generally unable topredict output program given specific input"}
{"id": "nan", "abstract": "  Inspired by humans' exceptional ability to master arithmetic and generalizeto new problems, we present a new dataset, Handwritten arithmetic with INTegers(HINT), to examine machines' capability of learning generalizable concepts atthree levels: perception, syntax, and semantics. In HINT, machines are taskedwith learning how concepts are perceived from raw signals such as images (i.e.,perception), how multiple concepts are structurally combined to form a validexpression (i.e., syntax), and how concepts are realized to afford variousreasoning tasks (i.e., semantics), all in a weakly supervised manner. Focusingon systematic generalization, we carefully design a five-fold test set toevaluate both the interpolation and the extrapolation of learned conceptsw.r.t. the three levels. Further, we design a few-shot learning split todetermine whether or not models can rapidly learn new concepts and generalizethem to more complex scenarios. To comprehend existing models' limitations, weundertake extensive experiments with various sequence-to-sequence models,including RNNs, Transformers, and GPT-3 (with the chain of thought prompting).The results indicate that current models struggle to extrapolate to long-rangesyntactic dependency and semantics. Models exhibit a considerable gap towardhuman-level generalization when evaluated with new concepts in a few-shotsetting. Moreover, we discover that it is infeasible to solve HINT by merelyscaling up the dataset and the model size; this strategy contributes little tothe extrapolation of syntax and semantics. Finally, in zero-shot GPT-3experiments, the chain of thought prompting exhibits impressive results andsignificantly boosts the test accuracy. We believe the HINT dataset and theexperimental findings are of great interest to the learning community onsystematic generalization.", "title": "a minimalist dataset for systematic generalization of perception, syntax, and semantics", "url": "http://arxiv.org/pdf/2103.01403v3.pdf", "tokenized_text": "inspired humans exceptional ability master arithmetic new problems present new dataset arithmetic examine machines capability learning generalizable concepts levels perception syntax semantics hint machines learning concepts perceived raw signals images i.e. multiple concepts structurally combined form i.e. syntax concepts realized afford tasks i.e. semantics weakly supervised manner systematic generalization carefully design fold test set toevaluate extrapolation learned levels design shot_learning shot learning split rapidly learn new concepts complex scenarios comprehend existing limitations extensive_experiments extensive experiments sequence sequence including transformers gpt-3 chain_of_thought chain thought results_indicate results indicate current struggle extrapolate long dependency semantics exhibit considerable gap level generalization evaluated new concepts shotsetting discover infeasible solve hint dataset model_size size strategy contributes little tothe extrapolation syntax semantics finally zero shot chain_of_thought chain thought exhibits impressive results boosts test accuracy believe hint dataset findings great interest learning community onsystematic generalization"}
{"id": "nan", "abstract": "  Pretrained large language models (LLMs) are widely used in many sub-fields ofnatural language processing (NLP) and generally known as excellent few-shotlearners with task-specific exemplars. Notably, chain of thought (CoT)prompting, a recent technique for eliciting complex multi-step reasoningthrough step-by-step answer examples, achieved the state-of-the-artperformances in arithmetics and symbolic reasoning, difficult system-2 tasksthat do not follow the standard scaling laws for LLMs. While these successesare often attributed to LLMs' ability for few-shot learning, we show that LLMsare decent zero-shot reasoners by simply adding \"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT,using the same single prompt template, significantly outperforms zero-shot LLMperformances on diverse benchmark reasoning tasks including arithmetics(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, CoinFlip), and other logical reasoning tasks (Date Understanding, Tracking ShuffledObjects), without any hand-crafted few-shot examples, e.g. increasing theaccuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% withlarge InstructGPT model (text-davinci-002), as well as similar magnitudes ofimprovements with another off-the-shelf large model, 540B parameter PaLM. Theversatility of this single prompt across very diverse reasoning tasks hints atuntapped and understudied fundamental zero-shot capabilities of LLMs,suggesting high-level, multi-task broad cognitive capabilities may be extractedby simple prompting. We hope our work not only serves as the minimal strongestzero-shot baseline for the challenging reasoning benchmarks, but alsohighlights the importance of carefully exploring and analyzing the enormouszero-shot knowledge hidden inside LLMs before crafting finetuning datasets orfew-shot exemplars.", "title": "large language models are zeroshot reasoners", "url": "http://arxiv.org/pdf/2205.11916v4.pdf", "tokenized_text": "pretrained large_language large language llms widely sub fields ofnatural language_processing language processing nlp generally known excellent shotlearners task specific exemplars notably chain_of_thought chain thought cot)prompting recent technique eliciting complex multi step step step answer examples achieved state symbolic reasoning difficult tasksthat follow standard scaling laws llms attributed llms ability shot_learning shot learning llmsare decent zero shot reasoners simply adding let think step answer experimental_results experimental results demonstrate zero shot cot single prompt_template template significantly_outperforms significantly outperforms zero shot diverse benchmark reasoning tasks including gsm8 aqua svamp symbolic reasoning logical reasoning tasks date understanding tracking hand crafted shot examples e.g. increasing theaccuracy multiarith 78.7 gsm8 10.4 withlarge instructgpt text davinci-002 similar magnitudes shelf large 540b parameter palm single diverse reasoning tasks hints understudied fundamental zero shot capabilities llms suggesting high level multi task broad cognitive capabilities simple hope work serves minimal shot baseline challenging reasoning benchmarks importance carefully exploring analyzing shot knowledge hidden inside llms crafting finetuning datasets orfew shot exemplars"}
{"id": "nan", "abstract": "  Unit tests play a key role in ensuring the correctness of software. However,manually creating unit tests is a laborious task, motivating the need forautomation. Large Language Models (LLMs) have recently been applied to thisproblem, utilizing additional training or few-shot learning on examples ofexisting tests. This paper presents a large-scale empirical evaluation on theeffectiveness of LLMs for automated unit test generation without additionaltraining or manual effort, providing the LLM with the signature andimplementation of the function under test, along with usage examples extractedfrom documentation. We also attempt to repair failed generated tests byre-prompting the model with the failing test and error message. We implementour approach in TestPilot, a test generation tool for JavaScript thatautomatically generates unit tests for all API functions in an npm package. Weevaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with atotal of 1,684 API functions. The generated tests achieve a median statementcoverage of 70.2% and branch coverage of 52.8%, significantly improving onNessie, a recent feedback-directed JavaScript test generation technique, whichachieves only 51.3% statement coverage and 25.6% branch coverage. We also findthat 92.8% of TestPilot's generated tests have no more than 50% similarity withexisting tests (as measured by normalized edit distance), with none of thembeing exact copies. Finally, we run TestPilot with two additional LLMs,OpenAI's older code-cushman-002 LLM and the open LLM StarCoder. Overall, weobserved similar results with the former (68.2% median statement coverage), andsomewhat worse results with the latter (54.0% median statement coverage),suggesting that the effectiveness of the approach is influenced by the size andtraining set of the LLM, but does not fundamentally depend on the specificmodel.", "title": "an empirical evaluation of using large language models for automated unit test generation", "url": "http://arxiv.org/pdf/2302.06527v3.pdf", "tokenized_text": "unit tests play key role ensuring correctness software manually creating unit tests laborious task motivating need large_language large language llms recently applied thisproblem utilizing additional training shot_learning shot learning examples ofexisting tests paper_presents paper presents large scale empirical evaluation theeffectiveness llms automated unit test generation additionaltraining manual effort providing llm function test usage examples extractedfrom documentation attempt repair failed generated tests failing test error message approach test generation tool javascript generates unit tests api functions package weevaluate openai gpt3.5 turbo llm 25 api functions generated tests achieve branch coverage significantly improving recent feedback directed javascript test generation technique whichachieves statement coverage branch coverage findthat generated tests 50 similarity tests measured normalized edit distance exact copies finally run additional llms openai older code llm open llm starcoder overall weobserved similar results statement coverage worse results statement effectiveness approach influenced size andtraining set llm fundamentally depend"}
{"id": "nan", "abstract": "  Large pre-trained models, also known as foundation models (FMs), are trainedin a task-agnostic manner on large-scale data and can be adapted to a widerange of downstream tasks by fine-tuning, few-shot, or even zero-shot learning.Despite their successes in language and vision tasks, we have yet seen anattempt to develop foundation models for geospatial artificial intelligence(GeoAI). In this work, we explore the promises and challenges of developingmultimodal foundation models for GeoAI. We first investigate the potential ofmany existing FMs by testing their performances on seven tasks across multiplegeospatial subdomains including Geospatial Semantics, Health Geography, UrbanGeography, and Remote Sensing. Our results indicate that on several geospatialtasks that only involve text modality such as toponym recognition, locationdescription recognition, and US state-level/county-level dementia time seriesforecasting, these task-agnostic LLMs can outperform task-specificfully-supervised models in a zero-shot or few-shot learning setting. However,on other geospatial tasks, especially tasks that involve multiple datamodalities (e.g., POI-based urban function classification, street viewimage-based urban noise intensity classification, and remote sensing imagescene classification), existing foundation models still underperformtask-specific models. Based on these observations, we propose that one of themajor challenges of developing a FM for GeoAI is to address the multimodalitynature of geospatial tasks. After discussing the distinct challenges of eachgeospatial data modality, we suggest the possibility of a multimodal foundationmodel which can reason over various types of geospatial data through geospatialalignments. We conclude this paper by discussing the unique risks andchallenges to develop such a model for GeoAI.", "title": "on the opportunities and challenges of foundation models for geospatial artificial intelligence", "url": "http://arxiv.org/pdf/2304.06798v1.pdf", "tokenized_text": "large pre trained known foundation_models foundation fms task agnostic manner large scale data adapted widerange downstream_tasks downstream tasks fine tuning shot zero shot_learning shot learning despite successes language vision tasks seen anattempt develop foundation_models foundation artificial work explore promises challenges foundation_models foundation investigate potential existing fms testing performances seven tasks subdomains including semantics health remote sensing results_indicate results indicate involve text modality recognition recognition state level level time task agnostic llms outperform task supervised zero shot shot_learning shot learning setting tasks especially tasks involve multiple e.g. based urban function classification street based urban noise intensity classification remote sensing classification existing foundation_models foundation specific based observations propose challenges developing address tasks discussing distinct challenges data modality suggest possibility multimodal reason types data conclude paper discussing unique risks andchallenges develop"}
{"id": "nan", "abstract": "  One of the critical phases in software development is software testing.Testing helps with identifying potential bugs and reducing maintenance costs.The goal of automated test generation tools is to ease the development of testsby suggesting efficient bug-revealing tests. Recently, researchers haveleveraged Large Language Models (LLMs) of code to generate unit tests. Whilethe code coverage of generated tests was usually assessed, the literature hasacknowledged that the coverage is weakly correlated with the efficiency oftests in bug detection. To improve over this limitation, in this paper, weintroduce MuTAP for improving the effectiveness of test cases generated by LLMsin terms of revealing bugs by leveraging mutation testing. Our goal is achievedby augmenting prompts with surviving mutants, as those mutants highlight thelimitations of test cases in detecting bugs. MuTAP is capable of generatingeffective test cases in the absence of natural language descriptions of theProgram Under Test (PUTs). We employ different LLMs within MuTAP and evaluatetheir performance on different benchmarks. Our results show that our proposedmethod is able to detect up to 28% more faulty human-written code snippets.Among these, 17% remained undetected by both the current state-of-the-art fullyautomated test generation tool (i.e., Pynguin) and zero-shot/few-shot learningapproaches on LLMs. Furthermore, MuTAP achieves a Mutation Score (MS) of 93.57%on synthetic buggy code, outperforming all other approaches in our evaluation.Our findings suggest that although LLMs can serve as a useful tool to generatetest cases, they require specific post-processing steps to enhance theeffectiveness of the generated test cases which may suffer from syntactic orfunctional errors and may be ineffective in detecting certain types of bugs andtesting corner cases PUTs.", "title": "effective test generation using pretrained large language models and mutation testing", "url": "http://arxiv.org/pdf/2308.16557v1.pdf", "tokenized_text": "critical phases software development software testing testing helps identifying potential bugs reducing maintenance costs goal automated test generation tools ease development suggesting efficient bug revealing tests recently researchers large_language large language llms code generate unit tests whilethe code coverage generated tests usually assessed literature coverage weakly correlated efficiency bug detection improve limitation paper weintroduce improving effectiveness test_cases test cases generated llmsin terms revealing bugs leveraging mutation testing goal achievedby augmenting highlight thelimitations test_cases test cases detecting bugs capable test_cases test cases absence natural_language natural language descriptions theprogram test puts employ different llms performance different benchmarks results proposedmethod able detect 28 human written code snippets 17 remained current state art test generation tool i.e. zero shot shot learningapproaches llms furthermore achieves mutation score ms synthetic buggy code outperforming approaches evaluation findings_suggest findings suggest llms serve useful tool cases require specific post processing steps enhance theeffectiveness generated test_cases test cases suffer syntactic errors ineffective detecting certain types bugs andtesting cases puts"}
{"id": "nan", "abstract": "  Weakly-Supervised Scene Graph Generation (WSSGG) research has recentlyemerged as an alternative to the fully-supervised approach that heavily relieson costly annotations. In this regard, studies on WSSGG have utilized imagecaptions to obtain unlocalized triplets while primarily focusing on groundingthe unlocalized triplets over image regions. However, they have overlooked thetwo issues involved in the triplet formation process from the captions: 1)Semantic over-simplification issue arises when extracting triplets fromcaptions, where fine-grained predicates in captions are undesirably convertedinto coarse-grained predicates, resulting in a long-tailed predicatedistribution, and 2) Low-density scene graph issue arises when aligning thetriplets in the caption with entity/predicate classes of interest, where manytriplets are discarded and not used in training, leading to insufficientsupervision. To tackle the two issues, we propose a new approach, i.e., LargeLanguage Model for weakly-supervised SGG (LLM4SGG), where we mitigate the twoissues by leveraging the LLM's in-depth understanding of language and reasoningability during the extraction of triplets from captions and alignment ofentity/predicate classes with target data. To further engage the LLM in theseprocesses, we adopt the idea of Chain-of-Thought and the in-context few-shotlearning strategy. To validate the effectiveness of LLM4SGG, we conductextensive experiments on Visual Genome and GQA datasets, showing significantimprovements in both Recall@K and mean Recall@K compared to thestate-of-the-art WSSGG methods. A further appeal is that LLM4SGG isdata-efficient, enabling effective model training with a small amount oftraining images.", "title": "llm4sgg large language model for weakly supervised scene graph generation", "url": "http://arxiv.org/pdf/2310.10404v4.pdf", "tokenized_text": "weakly supervised scene graph generation research alternative fully supervised approach heavily costly annotations regard studies utilized imagecaptions obtain triplets primarily focusing triplets image regions overlooked issues involved triplet formation process captions simplification issue arises extracting triplets fine grained predicates captions convertedinto coarse grained predicates resulting long tailed low density scene graph issue arises aligning caption entity predicate classes interest training leading tackle issues propose_a_new propose new approach i.e. largelanguage weakly supervised mitigate leveraging llm depth understanding language reasoningability extraction triplets captions alignment predicate classes target data engage llm adopt idea chain thought context shotlearning strategy validate effectiveness conductextensive experiments visual genome gqa datasets showing significantimprovements mean compared thestate art methods appeal efficient enabling effective training small oftraining images"}
{"id": "nan", "abstract": "  African languages are severely under-represented in NLP research due to lackof datasets covering several NLP tasks. While there are individual languagespecific datasets that are being expanded to different tasks, only a handful ofNLP tasks (e.g. named entity recognition and machine translation) havestandardized benchmark datasets covering several geographical andtypologically-diverse African languages. In this paper, we develop MasakhaNEWS-- a new benchmark dataset for news topic classification covering 16 languageswidely spoken in Africa. We provide an evaluation of baseline models bytraining classical machine learning models and fine-tuning several languagemodels. Furthermore, we explore several alternatives to full fine-tuning oflanguage models that are better suited for zero-shot and few-shot learning suchas cross-lingual parameter-efficient fine-tuning (like MAD-X), patternexploiting training (PET), prompting language models (like ChatGPT), andprompt-free sentence transformer fine-tuning (SetFit and Cohere Embedding API).Our evaluation in zero-shot setting shows the potential of prompting ChatGPTfor news topic classification in low-resource African languages, achieving anaverage performance of 70 F1 points without leveraging additional supervisionlike MAD-X. In few-shot setting, we show that with as little as 10 examples perlabel, we achieved more than 90\\% (i.e. 86.0 F1 points) of the performance offull supervised training (92.6 F1 points) leveraging the PET approach.", "title": "masakhanews news topic classification for african languages", "url": "http://arxiv.org/pdf/2304.09972v2.pdf", "tokenized_text": "african languages severely represented nlp research datasets covering nlp_tasks nlp tasks individual datasets expanded different tasks handful ofnlp tasks e.g. named_entity named entity recognition machine_translation machine translation benchmark_datasets benchmark datasets covering geographical diverse african languages paper develop new benchmark dataset news topic classification covering 16 spoken provide evaluation baseline classical machine_learning machine learning fine tuning languagemodels furthermore explore alternatives fine tuning oflanguage better suited zero shot shot_learning shot learning suchas cross lingual parameter efficient fine tuning like training pet language_models language like_chatgpt like chatgpt andprompt free sentence transformer fine tuning setfit embedding evaluation zero shot_setting shot setting shows potential chatgptfor news topic classification low resource african languages achieving anaverage performance 70 f1 points leveraging additional x. shot_setting shot setting little 10 examples achieved i.e. f1 points performance supervised training f1 points leveraging pet approach"}
{"id": "nan", "abstract": "  Using prompts to utilize language models to perform various downstream tasks,also known as prompt-based learning or prompt-learning, has lately gainedsignificant success in comparison to the pre-train and fine-tune paradigm.Nonetheless, virtually all prompt-based methods are token-level, meaning theyall utilize GPT's left-to-right language model or BERT's masked language modelto perform cloze-style tasks. In this paper, we attempt to accomplish severalNLP tasks in the zero-shot scenario using a BERT original pre-training taskabandoned by RoBERTa and other models--Next Sentence Prediction (NSP). Unliketoken-level techniques, our sentence-level prompt-based method NSP-BERT doesnot need to fix the length of the prompt or the position to be predicted,allowing it to handle tasks such as entity linking with ease. Based on thecharacteristics of NSP-BERT, we offer several quick building templates forvarious downstream tasks. We suggest a two-stage prompt method for word sensedisambiguation tasks in particular. Our strategies for mapping the labelssignificantly enhance the model's performance on sentence pair tasks. On theFewCLUE benchmark, our NSP-BERT outperforms other zero-shot methods on most ofthese tasks and comes close to the few-shot methods.", "title": "nspbert a promptbased fewshot learner through an original pretraining tasknext sentence prediction", "url": "http://arxiv.org/pdf/2109.03564v2.pdf", "tokenized_text": "utilize language_models language perform downstream_tasks downstream tasks known based learning learning success comparison pre train fine tune paradigm nonetheless virtually based methods token level meaning utilize gpt left right language_model language bert masked language modelto perform cloze style tasks paper attempt accomplish tasks zero shot scenario bert original pre training roberta sentence prediction level techniques sentence level based method bert doesnot need fix length position predicted allowing handle tasks entity linking ease based bert offer quick building templates downstream_tasks downstream tasks suggest stage method word sensedisambiguation tasks particular strategies mapping enhance performance sentence pair tasks benchmark bert outperforms zero shot methods ofthese tasks comes close shot methods"}
{"id": "nan", "abstract": "  Acronym extraction aims to find acronyms (i.e., short-forms) and theirmeanings (i.e., long-forms) from the documents, which is important forscientific document understanding (SDU@AAAI-22) tasks. Previous works aredevoted to modeling this task as a paragraph-level sequence labeling problem.However, it lacks the effective use of the external knowledge, especially whenthe datasets are in a low-resource setting. Recently, the prompt-based methodwith the vast pre-trained language model can significantly enhance theperformance of the low-resourced downstream tasks. In this paper, we propose aPrompt-based Sequence Generation (PSG) method for the acronym extraction task.Specifically, we design a template for prompting the extracted acronym textswith auto-regression. A position extraction algorithm is designed forextracting the position of the generated answers. The results on the acronymextraction of Vietnamese and Persian in a low-resource setting show that theproposed method outperforms all other competitive state-of-the-art (SOTA)methods.", "title": "psg promptbased sequence generation for acronym extraction", "url": "http://arxiv.org/pdf/2111.14301v2.pdf", "tokenized_text": "extraction aims find i.e. short forms i.e. long forms documents important forscientific document understanding tasks previous works modeling task paragraph level sequence labeling problem lacks effective use external_knowledge external knowledge especially whenthe datasets low resource setting recently based vast pre trained_language trained language significantly enhance theperformance low resourced downstream_tasks downstream tasks paper propose aprompt based sequence generation method extraction task specifically design template extracted auto regression position extraction algorithm designed position generated answers results vietnamese low resource setting theproposed method outperforms competitive state art"}
{"id": "nan", "abstract": "  The Biocreative VII Track-2 challenge consists of named entity recognition,entity-linking (or entity-normalization), and topic indexing tasks -- withentities and topics limited to chemicals for this challenge. Named entityrecognition is a well-established problem and we achieve our best performancewith BERT-based BioMegatron models. We extend our BERT-based approach to theentity linking task. After the second stage of pretraining BioBERT with ametric-learning loss strategy called self-alignment pretraining (SAP), we linkentities based on the cosine similarity between their SAP-BioBERT wordembeddings. Despite the success of our named entity recognition experiments, wefind the chemical indexing task generally more challenging.  In addition to conventional NER methods, we attempt both named entityrecognition and entity linking with a novel text-to-text or \"prompt\" basedmethod that uses generative language models such as T5 and GPT. We achieveencouraging results with this new approach.", "title": "chemical identification and indexing in pubmed articles via bert and texttotext approaches", "url": "http://arxiv.org/pdf/2111.15622v1.pdf", "tokenized_text": "challenge consists named_entity named entity recognition entity linking entity normalization topic indexing tasks topics limited challenge named entityrecognition established problem achieve best performancewith bert based extend bert based approach theentity linking task second stage pretraining biobert learning loss strategy called self alignment pretraining sap based cosine similarity sap biobert despite success named_entity named entity recognition experiments wefind chemical indexing task generally challenging addition conventional ner methods attempt named entityrecognition entity linking novel text text basedmethod uses generative language_models language t5 gpt achieveencouraging results new approach"}
{"id": "nan", "abstract": "  One of the most pressing societal issues is the fight against false news. Thefalse claims, as difficult as they are to expose, create a lot of damage. Totackle the problem, fact verification becomes crucial and thus has been a topicof interest among diverse research communities. Using only the textual form ofdata we propose our solution to the problem and achieve competitive resultswith other approaches. We present our solution based on two approaches - PLM(pre-trained language model) based method and Prompt based method. ThePLM-based approach uses the traditional supervised learning, where the model istrained to take 'x' as input and output prediction 'y' as P(y|x). Whereas,Prompt-based learning reflects the idea to design input to fit the model suchthat the original objective may be re-framed as a problem of (masked) languagemodeling. We may further stimulate the rich knowledge provided by PLMs tobetter serve downstream tasks by employing extra prompts to fine-tune PLMs. Ourexperiments showed that the proposed method performs better than justfine-tuning PLMs. We achieved an F1 score of 0.6946 on the FACTIFY dataset anda 7th position on the competition leader-board.", "title": "gpts at factify 2022 prompt aided factverification", "url": "http://arxiv.org/pdf/2206.14913v1.pdf", "tokenized_text": "pressing societal issues fight false news claims difficult expose create lot damage totackle problem fact verification crucial interest diverse research communities textual form ofdata propose solution problem achieve competitive approaches present solution based approaches trained_language trained language based method prompt_based based method theplm based approach uses traditional supervised learning input output prediction based learning reflects idea design input fit original objective problem masked languagemodeling stimulate rich knowledge provided plms tobetter serve downstream_tasks downstream tasks employing extra fine tune plms ourexperiments showed proposed_method proposed method performs better tuning plms achieved f1_score f1 score dataset anda 7th position competition leader board"}
{"id": "nan", "abstract": "  As large language models (LLMs) are adopted as a fundamental component oflanguage technologies, it is crucial to accurately characterize theirperformance. Because choices in prompt design can strongly influence modelbehavior, this design process is critical in effectively using any modernpre-trained generative language model. In this work, we focus on LLMsensitivity to a quintessential class of meaning-preserving design choices:prompt formatting. We find that several widely used open-source LLMs areextremely sensitive to subtle changes in prompt formatting in few-shotsettings, with performance differences of up to 76 accuracy points whenevaluated using LLaMA-2-13B. Sensitivity remains even when increasing modelsize, the number of few-shot examples, or performing instruction tuning. Ouranalysis suggests that work evaluating LLMs with prompting-based methods wouldbenefit from reporting a range of performance across plausible prompt formats,instead of the currently-standard practice of reporting performance on a singleformat. We also show that format performance only weakly correlates betweenmodels, which puts into question the methodological validity of comparingmodels with an arbitrarily chosen, fixed prompt format. To facilitatesystematic analysis we propose FormatSpread, an algorithm that rapidlyevaluates a sampled set of plausible prompt formats for a given task, andreports the interval of expected performance without accessing model weights.Furthermore, we present a suite of analyses that characterize the nature ofthis sensitivity, including exploring the influence of particular atomicperturbations and the internal representation of particular formats.", "title": "quantifying language models' sensitivity to spurious features in prompt design or how i learned to start worrying about prompt formatting", "url": "http://arxiv.org/pdf/2310.11324v1.pdf", "tokenized_text": "large_language large language llms adopted fundamental component oflanguage technologies crucial accurately characterize theirperformance choices design strongly influence design process critical effectively trained generative language_model language work focus class meaning preserving design choices formatting find widely open source llms sensitive subtle changes formatting shotsettings performance differences 76 accuracy points whenevaluated llama-2 13b. sensitivity remains increasing modelsize number shot examples performing instruction_tuning instruction tuning ouranalysis suggests work evaluating llms based methods reporting range performance plausible formats instead currently standard practice reporting performance format performance weakly correlates puts question validity arbitrarily chosen fixed format analysis propose algorithm sampled set plausible formats given task expected performance accessing weights furthermore present suite analyses characterize nature ofthis sensitivity including exploring influence particular internal representation particular formats"}
{"id": "nan", "abstract": "  In order to train children's ability to ask curiosity-driven questions,previous research has explored designing specific exercises relying onproviding semantic and linguistic cues to help formulate such questions. Butdespite showing pedagogical efficiency, this method is still limited as itrelies on generating the said cues by hand, which can be a very costly process.In this context, we propose to leverage advances in the natural languageprocessing field (NLP) and investigate the efficiency of using a large languagemodel (LLM) for automating the production of the pedagogical content of acurious question-asking (QA) training. We study generating the said contentusing the \"prompt-based\" method that consists of explaining the task to the LLMin natural text. We evaluate the output using human experts annotations andcomparisons with hand-generated content. Results suggested indeed the relevanceand usefulness of this content. We also conduct a field study in primary school(75 children aged 9-10), where we evaluate children's QA performance whenhaving this training. We compare 3 types of content : 1) hand-generated contentthat proposes \"closed\" cues leading to predefined questions; 2) GPT-3-generatedcontent that proposes the same type of cues; 3) GPT-3-generated content thatproposes \"open\" cues leading to several possible questions. We see a similar QAperformance between the two \"closed\" trainings (showing the scalability of theapproach using GPT-3), and a better one for participants with the \"open\"training. These results suggest the efficiency of using LLMs to supportchildren in generating more curious questions, using a natural languageprompting approach that affords usability by teachers and other users notspecialists of AI techniques. Furthermore, results also show that open-endedcontent may be more suitable for training curious question-asking skills.", "title": "gpt3driven pedagogical agents for training children's curious questionasking skills", "url": "http://arxiv.org/pdf/2211.14228v6.pdf", "tokenized_text": "order train children ability ask curiosity driven questions previous research explored designing specific exercises relying semantic linguistic cues help formulate questions showing efficiency method limited generating said cues hand costly process context propose leverage advances natural languageprocessing field nlp investigate efficiency large languagemodel llm automating production content question asking qa training study generating said based method consists explaining task natural text evaluate output human experts annotations hand generated content results suggested usefulness content conduct field study primary children 10 evaluate children qa performance training compare types content hand generated proposes closed cues leading predefined questions gpt-3 proposes type cues gpt-3 generated content open cues leading possible questions similar closed showing scalability theapproach gpt-3 better participants results suggest efficiency llms generating curious questions natural approach usability teachers users ai techniques furthermore results open suitable training curious question asking skills"}
{"id": "nan", "abstract": "  Advances in large language models (LLMs) have empowered a variety ofapplications. However, there is still a significant gap in research when itcomes to understanding and enhancing the capabilities of LLMs in the field ofmental health. In this work, we present the first comprehensive evaluation ofmultiple LLMs, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4, onvarious mental health prediction tasks via online text data. We conduct a broadrange of experiments, covering zero-shot prompting, few-shot prompting, andinstruction fine-tuning. The results indicate a promising yet limitedperformance of LLMs with zero-shot and few-shot prompt designs for the mentalhealth tasks. More importantly, our experiments show that instructionfinetuning can significantly boost the performance of LLMs for all taskssimultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5,outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9%on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%.They further perform on par with the state-of-the-art task-specific languagemodel. We also conduct an exploratory case study on LLMs' capability on themental health reasoning tasks, illustrating the promising capability of certainmodels such as GPT-4. We summarize our findings into a set of action guidelinesfor potential methods to enhance LLMs' capability for mental health tasks.Meanwhile, we also emphasize the important limitations before achievingdeployability in real-world mental health settings, such as known racial andgender bias. We highlight the important ethical risks accompanying this line ofresearch.", "title": "mentalllm leveraging large language models for mental health prediction via online text data", "url": "http://arxiv.org/pdf/2307.14385v3.pdf", "tokenized_text": "advances large_language large language llms empowered variety ofapplications significant gap research understanding enhancing capabilities llms field health work present comprehensive evaluation ofmultiple llms including alpaca alpaca lora flan t5 gpt-3.5 gpt-4 onvarious mental_health mental health prediction tasks online text data conduct broadrange experiments covering zero shot_prompting shot shot_prompting shot andinstruction fine tuning results_indicate results indicate promising llms zero shot shot designs mentalhealth tasks importantly experiments significantly boost performance llms taskssimultaneously best finetuned mental alpaca mental flan best design gpt-3.5 25 15 times bigger balanced accuracy best gpt-4 250 150 times bigger perform par state art task specific languagemodel conduct exploratory case study llms capability health reasoning tasks illustrating promising capability gpt-4 summarize findings set action potential methods enhance llms capability mental_health mental health tasks emphasize important limitations real world mental_health mental health settings known racial bias highlight important ethical risks line ofresearch"}
{"id": "nan", "abstract": "  This paper explores zero-label learning in Natural Language Processing (NLP),whereby no human-annotated data is used anywhere during training and models aretrained purely on synthetic data. At the core of our framework is a novelapproach for better leveraging the powerful pretrained language models.Specifically, inspired by the recent success of few-shot inference on GPT-3, wepresent a training data creation procedure named Unsupervised Data Generation(UDG), which leverages few-shot prompts to synthesize high-quality trainingdata without real human annotations. Our method enables zero-label learning aswe train task-specific models solely on the synthetic data, yet we achievebetter or comparable results from strong baseline models trained onhuman-labeled data. Furthermore, when mixed with labeled data, our approachserves as a highly effective data augmentation procedure, achieving newstate-of-the-art results on the SuperGLUE benchmark.", "title": "towards zerolabel language learning", "url": "http://arxiv.org/pdf/2109.09193v1.pdf", "tokenized_text": "paper explores zero label learning natural_language natural language processing human annotated_data annotated data training aretrained purely synthetic data core framework novelapproach better leveraging powerful pretrained_language pretrained language specifically inspired recent success shot inference gpt-3 wepresent training_data training data creation procedure named unsupervised data leverages shot synthesize high quality trainingdata real human annotations method enables zero label learning train task specific solely synthetic data comparable results strong baseline trained onhuman labeled_data labeled data furthermore mixed labeled_data labeled data highly effective data_augmentation data augmentation procedure achieving newstate art results superglue benchmark"}
{"id": "nan", "abstract": "  Proper noun compounds, e.g., \"Covid vaccine\", convey information in asuccinct manner (a \"Covid vaccine\" is a \"vaccine that immunizes against theCovid disease\"). These are commonly used in short-form domains, such as newsheadlines, but are largely ignored in information-seeking applications. Toaddress this limitation, we release a new manually annotated dataset, ProNCI,consisting of 22.5K proper noun compounds along with their free-form semanticinterpretations. ProNCI is 60 times larger than prior noun compound datasetsand also includes non-compositional examples, which have not been previouslyexplored. We experiment with various neural models for automatically generatingthe semantic interpretations from proper noun compounds, ranging from few-shotprompting to supervised learning, with varying degrees of knowledge about theconstituent nouns. We find that adding targeted knowledge, particularly aboutthe common noun, results in performance gains of upto 2.8%. Finally, weintegrate our model generated interpretations with an existing Open IE systemand observe an 7.5% increase in yield at a precision of 85%. The dataset andcode are available at https://github.com/dair-iitd/pronci.", "title": "covid vaccine is against covid but oxford vaccine is made at oxford! semantic interpretation of proper noun compounds", "url": "http://arxiv.org/pdf/2210.13039v1.pdf", "tokenized_text": "proper compounds e.g. vaccine convey information asuccinct manner vaccine vaccine disease commonly short form domains largely ignored information seeking applications toaddress limitation release new manually annotated dataset consisting 22.5 proper compounds free form 60 times larger prior compound datasetsand includes non compositional examples experiment neural automatically semantic interpretations proper compounds ranging shotprompting supervised learning varying degrees knowledge nouns find adding targeted knowledge particularly aboutthe common results performance gains upto 2.8 finally weintegrate generated interpretations existing open ie observe 7.5 increase yield precision 85 dataset andcode available"}
{"id": "nan", "abstract": "  Medical dialogue summarization is challenging due to the unstructured natureof medical conversations, the use of medical terminology in gold summaries, andthe need to identify key information across multiple symptom sets. We present anovel system for the Dialogue2Note Medical Summarization tasks in the MEDIQA2023 Shared Task. Our approach for section-wise summarization (Task A) is atwo-stage process of selecting semantically similar dialogues and using thetop-k similar dialogues as in-context examples for GPT-4. For full-notesummarization (Task B), we use a similar solution with k=1. We achieved 3rdplace in Task A (2nd among all teams), 4th place in Task B Division WiseSummarization (2nd among all teams), 15th place in Task A Section HeaderClassification (9th among all teams), and 8th place among all teams in Task B.Our results highlight the effectiveness of few-shot prompting for this task,though we also identify several weaknesses of prompting-based approaches. Wecompare GPT-4 performance with several finetuned baselines. We find that GPT-4summaries are more abstractive and shorter. We make our code publiclyavailable.", "title": "summqa at mediqachat 2023incontext learning with gpt4 for medical summarization", "url": "http://arxiv.org/pdf/2306.17384v1.pdf", "tokenized_text": "medical dialogue summarization challenging unstructured natureof medical conversations use medical terminology gold summaries andthe need identify key information multiple symptom sets present anovel system dialogue2note medical summarization tasks shared_task shared task approach section wise summarization task atwo stage process selecting semantically similar dialogues similar dialogues context_examples context examples gpt-4 task use similar solution k=1 achieved task 2nd teams place task 2nd teams place task section 9th teams place teams task results highlight effectiveness shot_prompting shot task identify weaknesses based approaches wecompare gpt-4 performance finetuned baselines find abstractive shorter code publiclyavailable"}
{"id": "nan", "abstract": "  Human label variation, or annotation disagreement, exists in many naturallanguage processing (NLP) tasks, including natural language inference (NLI). Togain direct evidence of how NLI label variation arises, we build LiveNLI, anEnglish dataset of 1,415 ecologically valid explanations (annotators explainthe NLI labels they chose) for 122 MNLI items (at least 10 explanations peritem). The LiveNLI explanations confirm that people can systematically vary ontheir interpretation and highlight within-label variation: annotators sometimeschoose the same label for different reasons. This suggests that explanationsare crucial for navigating label interpretations in general. We few-shot promptlarge language models to generate explanations but the results areinconsistent: they sometimes produces valid and informative explanations, butit also generates implausible ones that do not support the label, highlightingdirections for improvement.", "title": "ecologically valid explanations for label variation in nli", "url": "http://arxiv.org/pdf/2310.13850v1.pdf", "tokenized_text": "human label variation annotation exists naturallanguage processing nlp tasks including natural_language natural language inference nli direct evidence nli label variation arises build dataset valid explanations annotators nli labels chose mnli items 10 explanations explanations confirm people systematically vary interpretation highlight label variation annotators label different reasons suggests crucial navigating label interpretations general shot language_models language generate explanations results produces valid informative explanations butit generates ones support label improvement"}
{"id": "nan", "abstract": "  A persistent challenge to table question answering (TableQA) by generatingexecutable programs has been adapting to varied table structures, typicallyrequiring domain-specific logical forms. In response, this paper introduces aunified TableQA framework that: (1) provides a unified representation forstructured tables as multi-index Pandas data frames, (2) uses Python as apowerful querying language, and (3) uses few-shot prompting to translate NLquestions into Python programs, which are executable on Pandas data frames.Furthermore, to answer complex relational questions with extended programfunctionality and external knowledge, our framework allows customized APIs thatPython programs can call. We experiment with four TableQA datasets that involvetables of different structures -- relational, multi-table, and hierarchicalmatrix shapes -- and achieve prominent improvements over past state-of-the-artsystems. In ablation studies, we (1) show benefits from our multi-indexrepresentation and APIs over baselines that use only an LLM, and (2)demonstrate that our approach is modular and can incorporate additional APIs.", "title": "apiassisted code generation for question answering on varied table structures", "url": "http://arxiv.org/pdf/2310.14687v1.pdf", "tokenized_text": "persistent challenge table question_answering question answering programs adapting varied table structures domain specific logical forms response paper introduces aunified framework provides unified representation forstructured tables multi index pandas data frames uses python apowerful querying language uses shot_prompting shot translate python programs executable pandas data frames furthermore answer complex relational questions extended external_knowledge external knowledge framework allows customized apis programs experiment datasets different structures relational multi table shapes achieve prominent improvements past state artsystems ablation studies benefits multi apis baselines use llm approach modular incorporate additional apis"}
{"id": "nan", "abstract": "  Questions in open-domain question answering are often ambiguous, allowingmultiple interpretations. One approach to handling them is to identify allpossible interpretations of the ambiguous question (AQ) and to generate along-form answer addressing them all, as suggested by Stelmakh et al., (2022).While it provides a comprehensive response without bothering the user forclarification, considering multiple dimensions of ambiguity and gatheringcorresponding knowledge remains a challenge. To cope with the challenge, wepropose a novel framework, Tree of Clarifications (ToC): It recursivelyconstructs a tree of disambiguations for the AQ -- via few-shot promptingleveraging external knowledge -- and uses it to generate a long-form answer.ToC outperforms existing baselines on ASQA in a few-shot setup across themetrics, while surpassing fully-supervised baselines trained on the wholetraining set in terms of Disambig-F1 and Disambig-ROUGE. Code is available athttps://github.com/gankim/tree-of-clarifications.", "title": "tree of clarifications answering ambiguous questions with retrievalaugmented large language models", "url": "http://arxiv.org/pdf/2310.14696v1.pdf", "tokenized_text": "questions open domain question_answering question answering ambiguous interpretations approach handling identify interpretations ambiguous question generate form answer addressing suggested et_al et al provides comprehensive response user considering multiple dimensions ambiguity knowledge remains challenge cope challenge wepropose novel framework tree tree shot external_knowledge external knowledge uses generate long form answer outperforms existing baselines shot setup surpassing fully supervised baselines trained set terms f1 rouge code_is_available code available"}
{"id": "nan", "abstract": "  Most of the recent work in leveraging Large Language Models (LLMs) such asGPT-3 for Machine Translation (MT) has focused on selecting the few-shotsamples for prompting. In this work, we try to better understand the role ofdemonstration attributes for the in-context learning of translations throughperturbations of high-quality, in-domain demonstrations. We find thatasymmetric perturbation of the source-target mappings yield vastly differentresults. We show that the perturbation of the source side has surprisinglylittle impact, while target perturbation can drastically reduce translationquality, suggesting that it is the output text distribution that provides themost important learning signal during in-context learning of translations. Wepropose a method named Zero-Shot-Context to add this signal automatically inZero-Shot prompting. We demonstrate that it improves upon the zero-shottranslation performance of GPT-3, even making it competitive with few-shotprompted translations.", "title": "dissecting incontext learning of translations in gpts", "url": "http://arxiv.org/pdf/2310.15987v1.pdf", "tokenized_text": "recent_work recent work leveraging large_language large language llms asgpt-3 machine_translation machine translation mt focused selecting work try better understand role ofdemonstration attributes context_learning context learning translations high quality domain demonstrations find perturbation source target mappings yield vastly perturbation source impact target perturbation drastically reduce suggesting output text distribution provides themost important learning signal context_learning context learning translations wepropose method named zero shot context add signal automatically inzero shot_prompting shot demonstrate improves zero performance gpt-3 making competitive translations"}
{"id": "nan", "abstract": "  A restaurant dinner may become a memorable experience due to an unexpectedaspect enjoyed by the customer, such as an origami-making station in thewaiting area. If aspects that are atypical for a restaurant experience wereknown in advance, they could be leveraged to make recommendations that have thepotential to engender serendipitous experiences, further increasing usersatisfaction. Although relatively rare, whenever encountered, atypical aspectsoften end up being mentioned in reviews due to their memorable quality.Correspondingly, in this paper we introduce the task of detecting atypicalaspects in customer reviews. To facilitate the development of extractionmodels, we manually annotate benchmark datasets of reviews in three domains -restaurants, hotels, and hair salons, which we use to evaluate a number oflanguage models, ranging from fine-tuning the instruction-based text-to-texttransformer Flan-T5 to zero-shot and few-shot prompting of GPT-3.5.", "title": "extraction of atypical aspects from customer reviews datasets and experiments with language models", "url": "http://arxiv.org/pdf/2311.02702v1.pdf", "tokenized_text": "restaurant experience enjoyed customer making station area aspects restaurant experience advance leveraged recommendations thepotential experiences increasing relatively rare encountered end mentioned reviews quality correspondingly paper_we_introduce paper introduce task detecting customer reviews facilitate development manually annotate benchmark_datasets benchmark datasets reviews domains use evaluate number oflanguage ranging fine tuning instruction based text flan t5 zero shot shot_prompting shot gpt-3.5"}
{"id": "nan", "abstract": "  Text-to-SQL aims to automate the process of generating SQL queries on adatabase from natural language text. In this work, we propose \"SQLPrompt\",tailored to improve the few-shot prompting capabilities of Text-to-SQL forLarge Language Models (LLMs). Our methods include innovative prompt design,execution-based consistency decoding strategy which selects the SQL with themost consistent execution outcome among other SQL proposals, and a method thataims to improve performance by diversifying the SQL proposals duringconsistency selection with different prompt designs (\"MixPrompt\") andfoundation models (\"MixLLMs\"). We show that \\emph{SQLPrompt} outperformsprevious approaches for in-context learning with few labeled data by a largemargin, closing the gap with finetuning state-of-the-art with thousands oflabeled data.", "title": "sqlprompt incontext texttosql with minimal labeled data", "url": "http://arxiv.org/pdf/2311.02883v1.pdf", "tokenized_text": "text sql aims automate process generating sql queries natural_language natural language text work propose improve shot_prompting shot capabilities text sql forlarge language_models language llms methods include innovative design execution based consistency decoding strategy selects sql themost consistent execution outcome sql proposals method improve performance sql proposals selection different designs approaches context_learning context learning labeled_data labeled data closing gap finetuning state art thousands oflabeled data"}
{"id": "nan", "abstract": "  One challenge with open-domain dialogue systems is the need to producetruthful, high-quality responses on any topic. We aim to improve the qualityand coverage of Athena, an Alexa Prize dialogue system. We experiment withfew-shot prompt-based learning, comparing GPT-Neo to Jurassic-1, for themovies, music, TV, sports, and video game domains, both within andcross-domain, with different prompt set sizes (2, 3, 10), formats, and meaningrepresentations consisting of either sets of WikiData KG triples, or dialogueacts. Our evaluation uses BLEURT and human metrics, and shows that with 10-shotprompting, Athena-Jurassic's performance is significantly better for coherenceand semantic accuracy. Experiments with 2-shot cross-domain prompts results ina huge performance drop for Athena-GPT-Neo, whose semantic accuracy falls to0.41, and whose untrue hallucination rate increases to 12%. Experiments withdialogue acts for video games show that with 10-shot prompting, both modelslearn to control dialogue acts, but Athena-Jurassic has significantly highercoherence, and only 4% untrue hallucinations. Our results suggest thatAthena-Jurassic produces high enough quality outputs to be useful in livesystems with real users. To our knowledge, these are the first resultsdemonstrating that few-shot semantic prompt-based learning can create NLGs thatgeneralize to new domains, and produce high-quality, semantically-controlled,conversational responses directly from meaning representations.", "title": "jurassic is (almost) all you need fewshot meaningtotext generation for opendomain dialogue", "url": "http://arxiv.org/pdf/2110.08094v2.pdf", "tokenized_text": "challenge open domain dialogue systems need high quality responses topic aim improve coverage alexa dialogue system experiment withfew shot based learning comparing gpt neo music video game domains andcross domain different set sizes 10 formats consisting sets wikidata kg triples evaluation uses bleurt human metrics shows 10 shotprompting performance significantly better semantic accuracy experiments shot cross domain results ina huge performance drop gpt neo semantic accuracy falls hallucination rate increases 12 experiments acts video games 10 shot_prompting shot modelslearn control dialogue acts significantly hallucinations results suggest produces high quality outputs useful real users knowledge shot semantic based learning create nlgs new domains produce high quality semantically controlled conversational responses directly meaning representations"}
{"id": "nan", "abstract": "  Large language models (LLMs) for automatic code generation have achievedbreakthroughs in several programming tasks. Their advances in competition-levelprogramming problems have made them an essential pillar of AI-assisted pairprogramming, and tools such as GitHub Copilot have emerged as part of the dailyprogramming workflow used by millions of developers. The training data forthese models is usually collected from the Internet (e.g., from open-sourcerepositories) and is likely to contain faults and security vulnerabilities.This unsanitized training data can cause the language models to learn thesevulnerabilities and propagate them during the code generation procedure. Whilethese models have been extensively assessed for their ability to producefunctionally correct programs, there remains a lack of comprehensiveinvestigations and benchmarks addressing the security aspects of these models.  In this work, we propose a method to systematically study the security issuesof code language models to assess their susceptibility to generating vulnerablecode. To this end, we introduce the first approach to automatically findgenerated code that contains vulnerabilities in black-box code generationmodels. To achieve this, we present an approach to approximate inversion of theblack-box code generation models based on few-shot prompting. We evaluate theeffectiveness of our approach by examining code language models in generatinghigh-risk security weaknesses. Furthermore, we establish a collection ofdiverse non-secure prompts for various vulnerability scenarios using ourmethod. This dataset forms a benchmark for evaluating and comparing thesecurity weaknesses in code language models.", "title": "codelmsec benchmark systematically evaluating and finding security vulnerabilities in blackbox code language models", "url": "http://arxiv.org/pdf/2302.04012v2.pdf", "tokenized_text": "large_language large language llms automatic code_generation code generation programming tasks advances competition problems essential ai assisted tools github_copilot github copilot emerged workflow millions developers training_data training data usually collected internet e.g. open likely contain faults security vulnerabilities unsanitized training_data training data cause language_models language learn thesevulnerabilities propagate code_generation code generation procedure extensively assessed ability correct programs remains lack benchmarks addressing security aspects work propose method systematically study security code language_models language assess susceptibility generating end introduce approach automatically code contains vulnerabilities black box code achieve present approach approximate inversion box code_generation code generation based shot_prompting shot evaluate theeffectiveness approach examining code language_models language generatinghigh risk security weaknesses furthermore establish collection ofdiverse non secure vulnerability scenarios ourmethod dataset forms benchmark evaluating comparing weaknesses code language_models language"}
{"id": "nan", "abstract": "  Due to the prohibitively high cost of creating error correction datasets,most Factual Claim Correction methods rely on a powerful verification model toguide the correction process. This leads to a significant drop in performancein domains like scientific claims, where good verification models do not alwaysexist. In this work, we introduce SciFix, a scientific claim correction systemthat does not require a verifier but can outperform existing methods by aconsiderable margin -- achieving correction accuracy of 84% on the SciFactdataset, 77% on SciFact-Open and 72% on the CovidFact dataset, compared to nextbest accuracies of 7%, 5%, and 15% on the same datasets respectively. Ourmethod leverages the power of prompting with LLMs during training to create arichly annotated dataset that can be used for fully supervised training andregularization. We additionally use a claim-aware decoding procedure to improvethe quality of corrected claims. Our method outperforms the very LLM that wasused to generate the annotated dataset -- with Few-Shot Prompting on GPT3.5achieving 58%, 61%, and 64% on the respective datasets, a consistently lowercorrection accuracy, despite using nearly 800 times as many parameters as ourmodel.", "title": "scifix outperforming gpt3 on scientific factual error correction", "url": "http://arxiv.org/pdf/2305.14707v2.pdf", "tokenized_text": "prohibitively high cost creating error correction datasets factual claim correction methods rely powerful verification toguide correction process leads significant drop performancein domains like scientific claims good verification work introduce scientific claim correction systemthat require verifier outperform existing_methods existing methods aconsiderable margin achieving correction accuracy 84 77 scifact open dataset compared accuracies 15 datasets respectively ourmethod leverages power llms training create annotated dataset fully_supervised fully supervised training additionally use claim aware decoding procedure improvethe quality corrected claims method outperforms llm generate annotated dataset shot_prompting shot 58 61 64 respective datasets consistently accuracy despite nearly times parameters ourmodel"}
{"id": "nan", "abstract": "  Adversarial attacks, particularly patch attacks, pose significant threats tothe robustness and reliability of deep learning models. Developing reliabledefenses against patch attacks is crucial for real-world applications, yetcurrent research in this area is not satisfactory. In this paper, we proposeDIFFender, a novel defense method that leverages a text-guided diffusion modelto defend against adversarial patches. DIFFender includes two main stages:patch localization and patch restoration. In the localization stage, we findand exploit an intriguing property of the diffusion model to effectivelyidentify the locations of adversarial patches. In the restoration stage, weemploy the diffusion model to reconstruct the adversarial regions in the imageswhile preserving the integrity of the visual content. Importantly, these twostages are carefully guided by a unified diffusion model, thus we can utilizethe close interaction between them to improve the whole defense performance.Moreover, we propose a few-shot prompt-tuning algorithm to fine-tune thediffusion model, enabling the pre-trained diffusion model to easily adapt tothe defense task. We conduct extensive experiments on the image classificationand face recognition tasks, demonstrating that our proposed method exhibitssuperior robustness under strong adaptive attacks and generalizes well acrossvarious scenarios, diverse classifiers, and multiple patch attack methods.", "title": "diffender diffusionbased adversarial defense against patch attacks", "url": "http://arxiv.org/pdf/2306.09124v2.pdf", "tokenized_text": "adversarial attacks particularly patch attacks pose significant threats tothe robustness reliability deep learning developing patch attacks crucial real world_applications world applications research area satisfactory paper novel defense method leverages text guided diffusion modelto defend adversarial patches includes main stages patch localization patch restoration localization stage exploit intriguing property diffusion locations adversarial patches restoration stage diffusion reconstruct adversarial regions preserving integrity visual content importantly carefully guided unified diffusion close interaction improve defense performance propose shot tuning algorithm fine tune thediffusion enabling pre trained diffusion easily adapt tothe defense task conduct_extensive conduct extensive experiments image face recognition tasks demonstrating proposed_method proposed method robustness strong adaptive attacks generalizes acrossvarious scenarios diverse classifiers multiple patch attack methods"}
{"id": "nan", "abstract": "  Large language models (LLMs) are a promising avenue for machine translation(MT). However, current LLM-based MT systems are brittle: their effectivenesshighly depends on the choice of few-shot examples and they often require extrapost-processing due to overgeneration. Alternatives such as finetuning ontranslation instructions are computationally expensive and may weakenin-context learning capabilities, due to overspecialization. In this paper, weprovide a closer look at this problem. We start by showing that adapter-basedfinetuning with LoRA matches the performance of traditional finetuning whilereducing the number of training parameters by a factor of 50. This method alsooutperforms few-shot prompting and eliminates the need for post-processing orin-context examples. However, we show that finetuning generally degradesfew-shot performance, hindering adaptation capabilities. Finally, to obtain thebest of both worlds, we propose a simple approach that incorporates few-shotexamples during finetuning. Experiments on 10 language pairs show that ourproposed approach recovers the original few-shot capabilities while keeping theadded benefits of finetuning.", "title": "steering large language models for machine translation with finetuning and incontext learning", "url": "http://arxiv.org/pdf/2310.13448v1.pdf", "tokenized_text": "large_language large language llms promising avenue machine current llm based mt_systems mt systems brittle depends choice shot examples require processing alternatives finetuning instructions computationally expensive context_learning context learning capabilities paper weprovide closer look problem start showing adapter lora matches performance traditional finetuning number training parameters factor 50 method alsooutperforms shot_prompting shot eliminates need post processing context_examples context examples finetuning generally shot performance hindering adaptation capabilities finally obtain thebest worlds propose simple approach incorporates shotexamples finetuning experiments 10 language pairs ourproposed approach recovers original shot capabilities keeping benefits finetuning"}
{"id": "nan", "abstract": "  Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP thatstill, to a large extent, relies on calculating cross-lingual wordrepresentations. Inspired by the global paradigm shift in NLP towards LargeLanguage Models (LLMs), we examine the potential of the latest generation ofLLMs for the development of bilingual lexicons. We ask the following researchquestion: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) forBLI, and how does this approach compare against and complement current BLIapproaches? To this end, we systematically study 1) zero-shot prompting forunsupervised BLI and 2) few-shot in-context prompting with a set of seedtranslation pairs, both without any LLM fine-tuning, as well as 3) standardBLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-sourcetext-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on twostandard BLI benchmarks covering a range of typologically diverse languages.Our work is the first to demonstrate strong BLI capabilities of text-to-textmLLMs. The results reveal that few-shot prompting with in-context examples fromnearest neighbours achieves the best performance, establishing newstate-of-the-art BLI scores for many language pairs. We also conduct a seriesof in-depth analyses and ablation studies, providing more insights on BLI with(m)LLMs, also along with their limitations.", "title": "on bilingual lexicon induction with large language models", "url": "http://arxiv.org/pdf/2310.13995v1.pdf", "tokenized_text": "bilingual lexicon induction core task multilingual nlp large extent relies calculating cross lingual inspired global paradigm shift nlp largelanguage_models largelanguage llms examine potential latest generation ofllms development bilingual ask following possible fine tune multilingual llms mllms approach compare complement current end systematically study zero shot_prompting shot shot context set pairs llm fine tuning oriented fine tuning smaller llms experiment 18 open text mllms different sizes 13b parameters benchmarks covering range diverse languages work demonstrate strong capabilities text results reveal shot_prompting shot context_examples context examples achieves best performance establishing newstate art scores language pairs conduct depth analyses ablation studies providing insights limitations"}
{"id": "nan", "abstract": "  In this paper, we evaluate different abilities of GPT-4V including visualunderstanding, language understanding, visual puzzle solving, and understandingof other modalities such as depth, thermal, video, and audio. To estimateGPT-4V's performance, we manually construct 656 test instances and carefullyevaluate the results of GPT-4V. The highlights of our findings are as follows:(1) GPT-4V exhibits impressive performance on English visual-centric benchmarksbut fails to recognize simple Chinese texts in the images; (2) GPT-4V showsinconsistent refusal behavior when answering questions related to sensitivetraits such as gender, race, and age; (3) GPT-4V obtains worse results thanGPT-4 (API) on language understanding tasks including general languageunderstanding benchmarks and visual commonsense knowledge evaluationbenchmarks; (4) Few-shot prompting can improve GPT-4V's performance on bothvisual understanding and language understanding; (5) GPT-4V struggles to findthe nuances between two similar images and solve the easy math picture puzzles;(6) GPT-4V shows non-trivial performance on the tasks of similar modalities toimage, such as video and thermal. Our experimental results reveal the abilityand limitations of GPT-4V and we hope our paper can provide some insights intothe application and research of GPT-4V.", "title": "an early evaluation of gpt4v(ision)", "url": "http://arxiv.org/pdf/2310.16534v1.pdf", "tokenized_text": "paper evaluate different abilities including language understanding visual puzzle solving understandingof modalities depth video audio performance manually construct test instances results gpt-4v. highlights findings exhibits impressive performance english visual centric fails recognize simple chinese texts images refusal behavior answering questions related gender race age obtains worse results api language understanding tasks including general languageunderstanding benchmarks visual commonsense knowledge evaluationbenchmarks shot_prompting shot improve performance understanding language understanding struggles nuances similar images solve easy math picture shows non trivial performance tasks similar modalities video experimental_results experimental results reveal abilityand limitations hope paper provide insights intothe application research gpt-4v."}
{"id": "nan", "abstract": "  Large language models (LLMs) show amazing proficiency and fluency in the useof language. Does this mean that they have also acquired insightful linguisticknowledge about the language, to an extent that they can serve as an \"expertlinguistic annotator\"? In this paper, we examine the successes and limitationsof the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaningstructure, focusing on the Abstract Meaning Representation (AMR; Banarescu etal. 2013) parsing formalism, which provides rich graphical representations ofsentence meaning structure while abstracting away from surface forms. Wecompare models' analysis of this semantic structure across two settings: 1)direct production of AMR parses based on zero- and few-shot prompts, and 2)indirect partial reconstruction of AMR via metalinguistic natural languagequeries (e.g., \"Identify the primary event of this sentence, and the predicatecorresponding to that event.\"). Across these settings, we find that models canreliably reproduce the basic format of AMR, and can often capture core event,argument, and modifier structure -- however, model outputs are prone tofrequent and major errors, and holistic analysis of parse acceptability showsthat even with few-shot demonstrations, models have virtually 0% success inproducing fully accurate parses. Eliciting natural language responses producessimilar patterns of errors. Overall, our findings indicate that these modelsout-of-the-box can capture aspects of semantic structure, but there remain keylimitations in their ability to support fully accurate semantic analyses orparses.", "title": "you are an expert linguistic annotator limits of llms as analyzers of abstract meaning representation", "url": "http://arxiv.org/pdf/2310.17793v1.pdf", "tokenized_text": "large_language large language llms amazing proficiency fluency useof language mean acquired insightful linguisticknowledge language extent serve annotator paper examine successes gpt-3 chatgpt gpt-4 analysis sentence focusing abstract meaning representation etal parsing formalism provides rich graphical representations ofsentence meaning structure away surface forms wecompare analysis semantic structure settings production parses based zero- shot partial reconstruction natural e.g. identify primary event sentence event settings find reproduce basic format capture core event argument structure outputs prone major errors holistic analysis parse showsthat shot demonstrations virtually success fully accurate parses eliciting natural_language natural language responses patterns errors overall findings indicate box capture aspects semantic structure remain keylimitations ability support fully accurate semantic analyses"}
{"id": "nan", "abstract": "  Automatically generated reports from medical images promise to improve theworkflow of radiologists. Existing methods consider an image-to-report modelingtask by directly generating a fully-fledged report from an image. However, thisconflates the content of the report (e.g., findings and their attributes) withits style (e.g., format and choice of words), which can lead to clinicallyinaccurate reports. To address this, we propose a two-step approach forradiology report generation. First, we extract the content from an image; then,we verbalize the extracted content into a report that matches the style of aspecific radiologist. For this, we leverage RadGraph -- a graph representationof reports -- together with large language models (LLMs). In our quantitativeevaluations, we find that our approach leads to beneficial performance. Ourhuman evaluation with clinical raters highlights that the AI-generated reportsare indistinguishably tailored to the style of individual radiologist despiteleveraging only a few examples as context.", "title": "styleaware radiology report generation with radgraph and fewshot prompting", "url": "http://arxiv.org/pdf/2310.17811v2.pdf", "tokenized_text": "automatically generated reports medical images promise improve radiologists existing_methods existing methods consider image report directly generating fully fledged report image content report e.g. findings attributes style e.g. format choice words lead reports address propose step approach report generation extract content image verbalize extracted content report matches style aspecific leverage graph reports large_language large language llms find approach leads beneficial performance evaluation clinical raters highlights ai generated tailored style individual examples context"}
{"id": "nan", "abstract": "  With the development of web technology, social media texts are becoming arich source for automatic mental health analysis. As traditional discriminativemethods bear the problem of low interpretability, the recent large languagemodels have been explored for interpretable mental health analysis on socialmedia, which aims to provide detailed explanations along with predictions. Theresults show that ChatGPT can generate approaching-human explanations for itscorrect classifications. However, LLMs still achieve unsatisfactoryclassification performance in a zero-shot/few-shot manner. Domain-specificfinetuning is an effective solution, but faces 2 challenges: 1) lack ofhigh-quality training data. 2) no open-source LLMs for interpretable mentalhealth analysis were released to lower the finetuning cost. To alleviate theseproblems, we build the first multi-task and multi-source interpretable mentalhealth instruction (IMHI) dataset on social media, with 105K data samples. Theraw social media data are collected from 10 existing sources covering 8 mentalhealth analysis tasks. We use expert-written few-shot prompts and collectedlabels to prompt ChatGPT and obtain explanations from its responses. To ensurethe reliability of the explanations, we perform strict automatic and humanevaluations on the correctness, consistency, and quality of generated data.Based on the IMHI dataset and LLaMA2 foundation models, we train MentalLLaMA,the first open-source LLM series for interpretable mental health analysis withinstruction-following capability. We also evaluate the performance ofMentalLLaMA on the IMHI evaluation benchmark with 10 test sets, where theircorrectness for making predictions and the quality of explanations areexamined. The results show that MentalLLaMA approaches state-of-the-artdiscriminative methods in correctness and generates high-quality explanations.", "title": "mentallama interpretable mental health analysis on social media with large language models", "url": "http://arxiv.org/pdf/2309.13567v2.pdf", "tokenized_text": "development web technology social_media social media texts source automatic mental_health mental health analysis traditional bear problem low interpretability recent large_languagemodels large languagemodels explored interpretable mental_health mental health analysis socialmedia aims provide detailed explanations predictions theresults chatgpt generate approaching human explanations classifications llms achieve performance zero shot shot manner domain effective solution faces challenges lack ofhigh quality training_data training data open source llms interpretable mentalhealth analysis released lower finetuning cost alleviate build multi task multi source interpretable mentalhealth instruction dataset social_media social media 105 data samples social_media social media data collected 10 existing sources covering mentalhealth analysis tasks use expert written shot chatgpt obtain explanations responses reliability explanations perform strict automatic humanevaluations correctness consistency quality generated data based dataset llama2 foundation_models foundation train open source llm series interpretable mental_health mental health analysis following capability evaluate performance evaluation benchmark 10 test sets making predictions quality explanations results approaches state methods correctness generates high quality explanations"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) have shown great success in code generation.LLMs take as the input a prompt and output the code. A key question is how tomake prompts (i.e., Prompting Techniques). Existing prompting techniques aredesigned for natural language generation and have low accuracy in codegeneration.  In this paper, we propose a new prompting technique named AceCoder. Ourmotivation is that code generation meets two unique challenges (i.e.,requirement understanding and code implementation). AceCoder contains two novelmechanisms (i.e., guided code generation and example retrieval) to solve thesechallenges. (1) Guided code generation asks LLMs first to analyze requirementsand output an intermediate preliminary (e.g., test cases). The preliminary isused to clarify requirements and tell LLMs \"what to write\". (2) Exampleretrieval selects similar programs as examples in prompts, which provide lotsof relevant content (e.g., algorithms, APIs) and teach LLMs \"how to write\". Weapply AceCoder to three LLMs (e.g., Codex) and evaluate it on three publicbenchmarks using the Pass@k. Results show that AceCoder can significantlyimprove the performance of LLMs on code generation. (1) In terms of Pass@1,AceCoder outperforms the state-of-the-art baseline by up to 56.4% in MBPP,70.7% in MBJP, and 88.4% in MBJSP. (2) AceCoder is effective in LLMs withdifferent sizes (i.e., 6B to 13B) and different languages (i.e., Python, Java,and JavaScript). (3) Human evaluation shows human developers prefer programsfrom AceCoder.", "title": "acecoder utilizing existing code to enhance code generation", "url": "http://arxiv.org/pdf/2303.17780v3.pdf", "tokenized_text": "large_language large language llms shown great success code_generation code generation llms input output code key question tomake i.e. prompting_techniques techniques existing prompting_techniques techniques natural_language natural language generation low accuracy codegeneration paper propose_a_new propose new prompting_technique technique named code_generation code generation unique challenges i.e. understanding code implementation contains i.e. guided code_generation code generation example retrieval solve thesechallenges guided code_generation code generation asks llms analyze output intermediate preliminary e.g. test_cases test cases preliminary clarify requirements tell llms write exampleretrieval selects similar programs examples provide relevant content e.g. algorithms apis teach llms write llms e.g. codex evaluate publicbenchmarks results significantlyimprove performance llms code_generation code generation terms outperforms state art baseline effective llms withdifferent sizes i.e. 6b 13b different languages i.e. python java javascript human evaluation shows human developers prefer"}
{"id": "nan", "abstract": "  Humans can reason compositionally when presented with new tasks. Previousresearch shows that appropriate prompting techniques enable large languagemodels (LLMs) to solve artificial compositional generalization tasks such asSCAN. In this work, we identify additional challenges in more realisticsemantic parsing tasks with larger vocabulary and refine these promptingtechniques to address them. Our best method is based on least-to-mostprompting: it decomposes the problem using prompting-based syntactic parsing,then uses this decomposition to select appropriate exemplars and tosequentially generate the semantic parse. This method allows us to set a newstate of the art for CFQ while requiring only 1% of the training data used bytraditional approaches. Due to the general nature of our approach, we expectsimilar efforts will lead to new results in other tasks and domains, especiallyfor knowledge-intensive applications.", "title": "compositional semantic parsing with large language models", "url": "http://arxiv.org/pdf/2209.15003v2.pdf", "tokenized_text": "humans reason presented new tasks shows appropriate prompting_techniques techniques enable large_languagemodels large languagemodels llms solve artificial compositional generalization tasks work identify additional challenges parsing tasks larger vocabulary refine promptingtechniques address best method based decomposes problem based syntactic parsing uses decomposition select appropriate exemplars generate semantic parse method allows set newstate art requiring training_data training data approaches general nature approach efforts lead new results tasks domains especiallyfor knowledge intensive applications"}
{"id": "nan", "abstract": "  This paper introduces GEMBA-MQM, a GPT-based evaluation metric designed todetect translation quality errors, specifically for the quality estimationsetting without the need for human reference translations. Based on the powerof large language models (LLM), GEMBA-MQM employs a fixed three-shot promptingtechnique, querying the GPT-4 model to mark error quality spans. Compared toprevious works, our method has language-agnostic prompts, thus avoiding theneed for manual prompt preparation for new languages.  While preliminary results indicate that GEMBA-MQM achieves state-of-the-artaccuracy for system ranking, we advise caution when using it in academic worksto demonstrate improvements over other methods due to its dependence on theproprietary, black-box GPT model.", "title": "gembamqm detecting translation quality error spans with gpt4", "url": "http://arxiv.org/pdf/2310.13988v1.pdf", "tokenized_text": "paper introduces gemba mqm gpt based evaluation metric designed todetect translation quality errors specifically quality need human reference translations based large_language large language llm gemba mqm employs fixed shot promptingtechnique querying gpt-4 mark error quality spans compared works method language agnostic avoiding theneed manual preparation new languages preliminary results_indicate results indicate gemba mqm achieves_state achieves state artaccuracy system ranking advise caution academic demonstrate improvements methods dependence black box gpt"}
{"id": "nan", "abstract": "  Energy load forecasting plays a crucial role in optimizing resourceallocation and managing energy consumption in buildings and cities. In thispaper, we propose a novel approach that leverages language models for energyload forecasting. We employ prompting techniques to convert energy consumptiondata into descriptive sentences, enabling fine-tuning of language models. Byadopting an autoregressive generating approach, our proposed method enablespredictions of various horizons of future energy load consumption. Throughextensive experiments on real-world datasets, we demonstrate the effectivenessand accuracy of our proposed method. Our results indicate that utilizinglanguage models for energy load forecasting holds promise for enhancing energyefficiency and facilitating intelligent decision-making in energy systems.", "title": "utilizing language models for energy load forecasting", "url": "http://arxiv.org/pdf/2310.17788v1.pdf", "tokenized_text": "energy forecasting plays crucial role optimizing managing energy consumption cities thispaper propose_a_novel propose novel approach leverages language_models language forecasting employ prompting_techniques techniques convert energy descriptive sentences enabling fine tuning language_models language autoregressive generating approach proposed_method proposed method horizons future energy consumption experiments real world datasets demonstrate accuracy proposed_method proposed method results_indicate results indicate energy forecasting holds promise enhancing facilitating intelligent decision making energy systems"}
{"id": "nan", "abstract": "  Finding topics to write about can be a mentally demanding process. However,topic hierarchies can help writers explore topics of varying levels ofspecificity. In this paper, we use large language models (LLMs) to helpconstruct topic hierarchies. Although LLMs have access to such knowledge, itcan be difficult to elicit due to issues of specificity, scope, and repetition.We designed and tested three different prompting techniques to find one thatmaximized accuracy. We found that prepending the general topic area to a promptyielded the most accurate results with 85% accuracy. We discuss applications ofthis research including STEM writing, education, and content creation.", "title": "eliciting topic hierarchies from large language models", "url": "http://arxiv.org/pdf/2310.19275v1.pdf", "tokenized_text": "finding topics write demanding process topic hierarchies help writers explore topics varying levels paper use large_language large language llms topic hierarchies llms access knowledge difficult elicit issues specificity scope repetition designed tested different prompting_techniques techniques find accuracy found prepending general topic area accurate results 85 accuracy discuss applications ofthis research including stem writing education content creation"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) (e.g., ChatGPT) have shown impressiveperformance in code generation. LLMs take prompts as inputs, andChain-of-Thought (CoT) prompting is the state-of-the-art prompting technique.CoT prompting asks LLMs first to generate CoTs (i.e., intermediate naturallanguage reasoning steps) and then output the code. However, CoT prompting isdesigned for natural language generation and has low accuracy in codegeneration.  In this paper, we propose Structured CoTs (SCoTs) and present a novelprompting technique for code generation, named SCoT prompting. Our motivationis source code contains rich structural information and any code can becomposed of three program structures (i.e., sequence, branch, and loopstructures). Intuitively, structured intermediate reasoning steps make forstructured source code. Thus, we ask LLMs to use program structures to buildCoTs, obtaining SCoTs. Then, LLMs generate the final code based on SCoTs.Compared to CoT prompting, SCoT prompting explicitly constrains LLMs to thinkabout how to solve requirements from the view of source code and further theperformance of LLMs in code generation. We apply SCoT prompting to two LLMs(i.e., ChatGPT and Codex) and evaluate it on three benchmarks (i.e., HumanEval,MBPP, and MBCPP). (1) SCoT prompting outperforms the state-of-the-art baseline- CoT prompting by up to 13.79% in Pass@1. (2) Human evaluation shows humandevelopers prefer programs from SCoT prompting. (3) SCoT prompting is robust toexamples and achieves substantial improvements.", "title": "structured chainofthought prompting for code generation", "url": "http://arxiv.org/pdf/2305.06599v3.pdf", "tokenized_text": "large_language large language llms e.g. chatgpt shown impressiveperformance code_generation code generation llms inputs andchain thought cot state art prompting_technique technique cot_prompting cot asks llms generate i.e. intermediate naturallanguage reasoning_steps reasoning steps output code cot_prompting cot isdesigned natural_language natural language generation low accuracy codegeneration paper propose structured present technique code_generation code generation named source_code source code contains rich structural information code program structures i.e. sequence branch intuitively structured intermediate reasoning_steps reasoning steps forstructured source_code source code ask llms use program structures obtaining llms generate final code based compared cot_prompting cot explicitly llms solve requirements view source_code source code theperformance llms code_generation code generation apply chatgpt codex evaluate benchmarks i.e. humaneval mbpp mbcpp outperforms state art cot_prompting cot pass@1 human evaluation shows prefer programs robust achieves substantial improvements"}
{"id": "nan", "abstract": "  With the rapid evolution of Artificial Intelligence (AI), its potentialimplications for higher education have become a focal point of interest. Thisstudy delves into the capabilities of AI in Physics Education and offersactionable AI policy recommendations. Using a Large Language Model (LLM), weassessed its ability to answer 1337 Physics exam questions spanning GCSE,A-Level, and Introductory University curricula. We employed various AIprompting techniques: Zero Shot, In Context Learning, and ConfirmatoryChecking, which merges Chain of Thought reasoning with Reflection. The AI'sproficiency varied across academic levels: it scored an average of 83.4% onGCSE, 63.8% on A-Level, and 37.4% on university-level questions, with anoverall average of 59.9% using the most effective prompting technique. In aseparate test, the LLM's accuracy on 5000 mathematical operations was found todecrease as the number of digits increased. Furthermore, when evaluated as amarking tool, the LLM's concordance with human markers averaged at 50.8%, withnotable inaccuracies in marking straightforward questions, likemultiple-choice. Given these results, our recommendations underscore caution:while current LLMs can consistently perform well on Physics questions atearlier educational stages, their efficacy diminishes with advanced content andcomplex calculations. LLM outputs often showcase novel methods not in thesyllabus, excessive verbosity, and miscalculations in basic arithmetic. Thissuggests that at university, there's no substantial threat from LLMs fornon-invigilated Physics questions. However, given the LLMs' considerableproficiency in writing Physics essays and coding abilities, non-invigilatedexaminations of these skills in Physics are highly vulnerable to automatedcompletion by LLMs. This vulnerability also extends to Physics questionspitched at lower academic levels.", "title": "the impact of ai in physics education a comprehensive review from gcse to university levels", "url": "http://arxiv.org/pdf/2309.05163v1.pdf", "tokenized_text": "rapid evolution artificial_intelligence artificial intelligence ai higher education point interest thisstudy delves capabilities ai physics education ai policy recommendations large_language large language llm ability answer physics exam questions spanning level introductory university curricula employed techniques zero_shot zero shot context_learning context learning chain_of_thought chain thought reasoning reflection varied academic levels scored average level university level questions anoverall average effective prompting_technique technique aseparate test llm accuracy 5000 mathematical operations found number increased furthermore evaluated tool llm concordance human markers averaged inaccuracies marking straightforward questions choice given results recommendations underscore caution current llms consistently perform physics questions educational stages efficacy diminishes advanced content andcomplex calculations llm outputs showcase novel methods excessive verbosity basic arithmetic university substantial threat llms physics questions given llms writing physics essays coding abilities non skills physics highly vulnerable llms vulnerability extends physics lower academic levels"}
{"id": "nan", "abstract": "  Understanding how language supports emotion inference remains a topic ofdebate in emotion science. The present study investigated whetherlanguage-derived emotion-concept knowledge would causally support emotioninference by manipulating the language-specific knowledge representations inlarge language models. Using the prompt technique, 14 attributes of emotionconcepts were found to be represented by distinct artificial neuronpopulations. By manipulating these attribute-related neurons, the majority ofthe emotion inference tasks showed performance deterioration compared to randommanipulations. The attribute-specific performance deterioration was related tothe importance of different attributes in human mental space. Our findingsprovide causal evidence in support of a language-based mechanism for emotioninference and highlight the contributions of emotion-concept knowledge.", "title": "languagespecific representation of emotionconcept knowledge causally supports emotion inference", "url": "http://arxiv.org/pdf/2302.09582v4.pdf", "tokenized_text": "understanding language supports emotion inference remains topic emotion science present study investigated derived emotion concept knowledge support manipulating language specific knowledge representations language_models language technique 14 attributes found represented distinct artificial manipulating attribute related neurons majority ofthe emotion inference tasks showed performance deterioration compared attribute specific performance deterioration related tothe importance different attributes human mental space causal evidence support language based mechanism highlight contributions emotion concept knowledge"}
{"id": "nan", "abstract": "  Embodied language comprehension emphasizes that language understanding is notsolely a matter of mental processing in the brain but also involvesinteractions with the physical and social environment. With the explosivegrowth of Large Language Models (LLMs) and their already ubiquitous presence inour daily lives, it is becoming increasingly necessary to verify theirreal-world understanding. Inspired by cognitive theories, we propose POSQA: aPhysical Object Size Question Answering dataset with simple size comparisonquestions to examine the extremity and analyze the potential mechanisms of theembodied comprehension of the latest LLMs.  We show that even the largest LLMs today perform poorly under the zero-shotsetting. We then push their limits with advanced prompting techniques andexternal knowledge augmentation. Furthermore, we investigate whether theirreal-world comprehension primarily derives from contextual information orinternal weights and analyse the impact of prompt formats and report bias ofdifferent objects. Our results show that real-world understanding that LLMsshaped from textual data can be vulnerable to deception and confusion by thesurface form of prompts, which makes it less aligned with human behaviours.", "title": "posqa probe the world models of llms with size comparisons", "url": "http://arxiv.org/pdf/2310.13394v1.pdf", "tokenized_text": "embodied language comprehension emphasizes language understanding matter mental processing brain physical social environment large_language large language llms ubiquitous presence inour daily lives increasingly necessary verify world understanding inspired cognitive theories propose object size question_answering question answering dataset simple size examine analyze potential mechanisms comprehension latest llms largest llms today perform poorly zero shotsetting push limits advanced prompting_techniques techniques knowledge augmentation furthermore investigate world comprehension primarily derives contextual information weights analyse impact formats report bias ofdifferent objects results real world understanding textual data vulnerable deception confusion form makes aligned human behaviours"}
{"id": "nan", "abstract": "  While large language models (LLMs) equipped with techniques likechain-of-thought prompting have demonstrated impressive capabilities, theystill fall short in their ability to reason robustly in complex settings.However, evaluating LLM reasoning is challenging because system capabilitiescontinue to grow while benchmark datasets for tasks like logical deduction haveremained static. We introduce MuSR, a dataset for evaluating language models onmultistep soft reasoning tasks specified in a natural language narrative. Thisdataset has two crucial features. First, it is created through a novelneurosymbolic synthetic-to-natural generation algorithm, enabling theconstruction of complex reasoning instances that challenge GPT-4 (e.g., murdermysteries roughly 1000 words in length) and which can be scaled further as morecapable LLMs are released. Second, our dataset instances are free textnarratives corresponding to real-world domains of reasoning; this makes itsimultaneously much more challenging than other synthetically-craftedbenchmarks while remaining realistic and tractable for human annotators tosolve with high accuracy. We evaluate a range of LLMs and prompting techniqueson this dataset and characterize the gaps that remain for techniques likechain-of-thought to perform robust reasoning.", "title": "musr testing the limits of chainofthought with multistep soft reasoning", "url": "http://arxiv.org/pdf/2310.16049v1.pdf", "tokenized_text": "large_language large language llms equipped techniques thought_prompting thought demonstrated impressive capabilities theystill fall short ability reason robustly complex settings evaluating llm reasoning challenging system grow benchmark_datasets benchmark datasets tasks like logical static introduce dataset evaluating language_models language soft reasoning tasks specified natural_language natural language narrative crucial features created synthetic natural generation algorithm enabling theconstruction complex_reasoning complex reasoning instances challenge gpt-4 e.g. roughly 1000 words length scaled llms released second dataset instances free corresponding real world domains reasoning makes challenging synthetically remaining realistic tractable human annotators tosolve high accuracy evaluate range llms dataset characterize gaps remain techniques thought perform robust reasoning"}
{"id": "nan", "abstract": "  Academic writing is an indispensable yet laborious part of the researchenterprise. This Perspective maps out principles and methods for usinggenerative artificial intelligence (AI), specifically large language models(LLMs), to elevate the quality and efficiency of academic writing. We introducea human-AI collaborative framework that delineates the rationale (why), process(how), and nature (what) of AI engagement in writing. The framework pinpointsboth short-term and long-term reasons for engagement and their underlyingmechanisms (e.g., cognitive offloading and imaginative stimulation). It revealsthe role of AI throughout the writing process, conceptualized through atwo-stage model for human-AI collaborative writing, and the nature of AIassistance in writing, represented through a model of writing-assistance typesand levels. Building on this framework, we describe effective promptingtechniques for incorporating AI into the writing routine (outlining, drafting,and editing) as well as strategies for maintaining rigorous scholarship,adhering to varied journal policies, and avoiding overreliance on AI.Ultimately, the prudent integration of AI into academic writing can ease thecommunication burden, empower authors, accelerate discovery, and promotediversity in science.", "title": "supercharging academic writing with generative ai framework, techniques, and caveats", "url": "http://arxiv.org/pdf/2310.17143v1.pdf", "tokenized_text": "academic writing indispensable laborious perspective maps principles methods artificial_intelligence artificial intelligence ai specifically large_language large language models(llms quality efficiency academic writing human ai collaborative framework rationale nature ai engagement writing framework short term long term reasons engagement e.g. cognitive offloading role ai writing process atwo stage human ai collaborative writing nature writing represented writing assistance typesand levels building framework describe effective promptingtechniques incorporating ai writing routine outlining editing strategies maintaining rigorous scholarship adhering varied policies avoiding integration ai academic writing ease thecommunication burden empower authors accelerate discovery science"}
{"id": "nan", "abstract": "  This paper describes and analyzes our participation in the 2023 Eval4NLPshared task, which focuses on assessing the effectiveness of prompt-basedtechniques to empower Large Language Models to handle the task of qualityestimation, particularly in the context of evaluating machine translations andsummaries. We conducted systematic experiments with various promptingtechniques, including standard prompting, prompts informed by annotatorinstructions, and innovative chain-of-thought prompting. In addition, weintegrated these approaches with zero-shot and one-shot learning methods tomaximize the efficacy of our evaluation procedures. Our work reveals thatcombining these approaches using a \"small\", open source model (orca_mini_v3_7B)yields competitive results.", "title": "little giants exploring the potential of small llms as evaluation metrics in summarization in the eval4nlp 2023 shared task", "url": "http://arxiv.org/pdf/2311.00686v1.pdf", "tokenized_text": "paper describes analyzes participation 2023 task focuses assessing effectiveness empower large_language large language handle task particularly context evaluating machine translations conducted systematic experiments promptingtechniques including standard informed innovative chain thought_prompting thought addition approaches zero shot shot_learning shot learning methods tomaximize efficacy evaluation procedures work reveals thatcombining approaches small open_source open source competitive results"}
{"id": "nan", "abstract": "  Programmatic weak supervision methodologies facilitate the expedited labelingof extensive datasets through the use of label functions (LFs) that encapsulateheuristic data sources. Nonetheless, the creation of precise LFs necessitatesdomain expertise and substantial endeavors. Recent advances in pre-trainedlanguage models (PLMs) have exhibited substantial potential across diversetasks. However, the capacity of PLMs to autonomously formulate accurate LFsremains an underexplored domain. In this research, we address this gap byintroducing DataSculpt, an interactive framework that harnesses PLMs for theautomated generation of LFs. Within DataSculpt, we incorporate an array ofprompting techniques, instance selection strategies, and LF filtration methodsto explore the expansive design landscape. Ultimately, we conduct a thoroughassessment of DataSculpt's performance on 12 real-world datasets, encompassinga range of tasks. This evaluation unveils both the strengths and limitations ofcontemporary PLMs in LF design.", "title": "can large language models design accurate label functions", "url": "http://arxiv.org/pdf/2311.00739v1.pdf", "tokenized_text": "programmatic weak supervision methodologies facilitate extensive datasets use label functions lfs data sources nonetheless creation precise lfs expertise substantial endeavors recent_advances recent advances pre trainedlanguage plms exhibited substantial potential diversetasks capacity plms autonomously formulate accurate underexplored domain research address gap byintroducing interactive framework harnesses plms generation lfs incorporate array ofprompting techniques instance selection strategies lf explore expansive design landscape ultimately conduct performance 12 real world datasets range tasks evaluation unveils strengths limitations plms lf design"}
{"id": "nan", "abstract": "  Personalized content-based recommender systems have become indispensabletools for users to navigate through the vast amount of content available onplatforms like daily news websites and book recommendation services. However,existing recommenders face significant challenges in understanding the contentof items. Large language models (LLMs), which possess deep semanticcomprehension and extensive knowledge from pretraining, have proven to beeffective in various natural language processing tasks. In this study, weexplore the potential of leveraging both open- and closed-source LLMs toenhance content-based recommendation. With open-source LLMs, we utilize theirdeep layers as content encoders, enriching the representation of content at theembedding level. For closed-source LLMs, we employ prompting techniques toenrich the training data at the token level. Through comprehensive experiments,we demonstrate the high effectiveness of both types of LLMs and show thesynergistic relationship between them. Notably, we observed a significantrelative improvement of up to 19.32% compared to existing state-of-the-artrecommendation models. These findings highlight the immense potential of bothopen- and closed-source of LLMs in enhancing content-based recommendationsystems. We will make our code and LLM-generated data available for otherresearchers to reproduce our results.", "title": "once boosting contentbased recommendation with both open and closedsource large language models", "url": "http://arxiv.org/pdf/2305.06566v4.pdf", "tokenized_text": "personalized content based recommender systems users navigate vast content available like daily news book recommendation services existing recommenders face significant challenges understanding items large_language large language llms possess deep extensive knowledge pretraining proven natural_language natural language processing tasks study weexplore potential leveraging closed source llms toenhance content based recommendation open source llms utilize layers content encoders enriching representation content level closed source llms employ prompting_techniques techniques toenrich training_data training data token level comprehensive experiments demonstrate high effectiveness types llms relationship notably observed improvement compared existing state findings highlight immense potential closed source llms enhancing content based code llm generated data available reproduce results"}
{"id": "nan", "abstract": "  Chain-of-thought (CoT) is capable of eliciting models to explicitly generatereasoning paths, thus promoting reasoning accuracy and attracting increasingattention. Specifically, zero-shot CoT achieves remarkable improvements in awide range of reasoning tasks by simply instructing the LLM with the prompt\"Let's think step by step!\". Despite the success of zero-shot CoT, the existingzero-shot prompting techniques remain limited to a single language, making itchallenging to generalize to other languages and hindering global development.In this work, we introduce cross-lingual prompting (CLP), aiming to improvezero-shot CoT reasoning across languages. Specifically, CLP consists of twomain components: (1) cross-lingual alignment prompting and (2) task-specificsolver prompting. The cross-lingual alignment prompting is responsible foraligning representations across different languages, whereas the task-specificsolver prompting is used to generate the final chain of thoughts and resultsfor the reasoning task. In addition, we further introduce cross-lingualself-consistent prompting (CLSP) to ensemble different reasoning paths acrosslanguages. Our experimental evaluations on several benchmarks demonstrate thatCLP and CLSP significantly outperform the existing prompting methods andachieve state-of-the-art performance. We hope this work will inspire furtherbreakthroughs in cross-lingual CoT.", "title": "crosslingual prompting improving zeroshot chainofthought reasoning across languages", "url": "http://arxiv.org/pdf/2310.14799v1.pdf", "tokenized_text": "chain thought cot capable eliciting explicitly paths promoting reasoning accuracy attracting specifically zero shot cot achieves remarkable improvements awide range reasoning tasks simply instructing llm think step_by_step step step despite success zero shot cot existingzero shot_prompting shot techniques remain limited single language making generalize languages hindering global development work introduce cross lingual aiming shot cot reasoning languages specifically consists components cross lingual alignment task cross lingual alignment responsible foraligning representations different languages task generate final chain thoughts reasoning task addition introduce cross consistent ensemble different reasoning paths acrosslanguages experimental evaluations benchmarks demonstrate significantly outperform existing methods state art performance hope work inspire cross lingual cot."}
{"id": "nan", "abstract": "  Graphs have emerged as a natural choice to represent and analyze theintricate patterns and rich information of the Web, enabling applications suchas online page classification and social recommendation. The prevailing\"pre-train, fine-tune\" paradigm has been widely adopted in graph machinelearning tasks, particularly in scenarios with limited labeled nodes. However,this approach often exhibits a misalignment between the training objectives ofpretext tasks and those of downstream tasks. This gap can result in the\"negative transfer\" problem, wherein the knowledge gained from pre-trainingadversely affects performance in the downstream tasks. The surge inprompt-based learning within Natural Language Processing (NLP) suggests thepotential of adapting a \"pre-train, prompt\" paradigm to graphs as analternative. However, existing graph prompting techniques are tailored tohomogeneous graphs, neglecting the inherent heterogeneity of Web graphs. Tobridge this gap, we propose HetGPT, a general post-training prompting frameworkto improve the predictive performance of pre-trained heterogeneous graph neuralnetworks (HGNNs). The key is the design of a novel prompting function thatintegrates a virtual class prompt and a heterogeneous feature prompt, with theaim to reformulate downstream tasks to mirror pretext tasks. Moreover, HetGPTintroduces a multi-view neighborhood aggregation mechanism, capturing thecomplex neighborhood structure in heterogeneous graphs. Extensive experimentson three benchmark datasets demonstrate HetGPT's capability to enhance theperformance of state-of-the-art HGNNs on semi-supervised node classification.", "title": "hetgpt harnessing the power of prompt tuning in pretrained heterogeneous graph neural networks", "url": "http://arxiv.org/pdf/2310.15318v1.pdf", "tokenized_text": "graphs emerged natural choice represent analyze patterns rich information web enabling applications suchas online page classification social recommendation train fine tune paradigm widely adopted graph machinelearning tasks particularly scenarios limited labeled nodes approach exhibits misalignment training objectives tasks downstream_tasks downstream tasks gap result transfer problem knowledge gained pre affects performance downstream_tasks downstream tasks surge inprompt based learning natural_language natural language processing nlp suggests thepotential adapting pre train paradigm graphs analternative existing graph prompting_techniques techniques tailored graphs neglecting inherent heterogeneity web graphs tobridge gap propose general post training frameworkto improve predictive performance pre trained heterogeneous graph key design novel function virtual class heterogeneous feature reformulate downstream_tasks downstream tasks mirror tasks multi view neighborhood aggregation mechanism capturing neighborhood structure heterogeneous graphs extensive experimentson benchmark_datasets benchmark datasets demonstrate capability enhance theperformance state art semi supervised node classification"}
{"id": "nan", "abstract": "  In an era marked by the increasing adoption of Large Language Models (LLMs)for various tasks, there is a growing focus on exploring LLMs' capabilities inhandling web data, particularly graph data. Dynamic graphs, which capturetemporal network evolution patterns, are ubiquitous in real-world web data.Evaluating LLMs' competence in understanding spatial-temporal information ondynamic graphs is essential for their adoption in web applications, whichremains unexplored in the literature. In this paper, we bridge the gap viaproposing to evaluate LLMs' spatial-temporal understanding abilities on dynamicgraphs, to the best of our knowledge, for the first time. Specifically, wepropose the LLM4DyG benchmark, which includes nine specially designed tasksconsidering the capability evaluation of LLMs from both temporal and spatialdimensions. Then, we conduct extensive experiments to analyze the impacts ofdifferent data generators, data statistics, prompting techniques, and LLMs onthe model performance. Finally, we propose Disentangled Spatial-TemporalThoughts (DST2) for LLMs on dynamic graphs to enhance LLMs' spatial-temporalunderstanding abilities. Our main observations are: 1) LLMs have preliminaryspatial-temporal understanding abilities on dynamic graphs, 2) Dynamic graphtasks show increasing difficulties for LLMs as the graph size and densityincrease, while not sensitive to the time span and data generation mechanism,3) the proposed DST2 prompting method can help to improve LLMs'spatial-temporal understanding abilities on dynamic graphs for most tasks. Thedata and codes will be open-sourced at publication time.", "title": "llm4dyg can large language models solve problems on dynamic graphs", "url": "http://arxiv.org/pdf/2310.17110v1.pdf", "tokenized_text": "era marked increasing adoption large_language large language tasks growing focus exploring llms capabilities web data particularly graph data dynamic graphs network evolution patterns ubiquitous real world web data evaluating llms competence understanding spatial temporal information graphs essential adoption web applications unexplored literature paper bridge gap evaluate llms spatial temporal understanding abilities best knowledge time specifically wepropose benchmark includes specially designed capability evaluation llms temporal conduct_extensive conduct extensive experiments analyze impacts ofdifferent data generators data statistics prompting_techniques techniques llms onthe performance finally propose disentangled llms dynamic graphs enhance llms spatial abilities main observations llms temporal understanding abilities dynamic graphs dynamic increasing difficulties llms graph size sensitive time span data generation proposed method help improve temporal understanding abilities dynamic graphs tasks thedata codes open sourced publication time"}
{"id": "nan", "abstract": "  This paper describes the DSBA submissions to the Prompting Large LanguageModels as Explainable Metrics shared task, where systems were submitted to twotracks: small and large summarization tracks. With advanced Large LanguageModels (LLMs) such as GPT-4, evaluating the quality of Natural LanguageGeneration (NLG) has become increasingly paramount. Traditionalsimilarity-based metrics such as BLEU and ROUGE have shown to misalign withhuman evaluation and are ill-suited for open-ended generation tasks. To addressthis issue, we explore the potential capability of LLM-based metrics,especially leveraging open-source LLMs. In this study, wide range of promptsand prompting techniques are systematically analyzed with three approaches:prompting strategy, score aggregation, and explainability. Our research focuseson formulating effective prompt templates, determining the granularity of NLGquality scores and assessing the impact of in-context examples on LLM-basedevaluation. Furthermore, three aggregation strategies are compared to identifythe most reliable method for aggregating NLG quality scores. To examineexplainability, we devise a strategy that generates rationales for the scoresand analyzes the characteristics of the explanation produced by the open-sourceLLMs. Extensive experiments provide insights regarding evaluation capabilitiesof open-source LLMs and suggest effective prompting strategies.", "title": "which is better exploring prompting strategy for llmbased metrics", "url": "http://arxiv.org/pdf/2311.03754v1.pdf", "tokenized_text": "paper describes submissions large_languagemodels large languagemodels explainable metrics shared task systems submitted small large summarization tracks advanced large_languagemodels large languagemodels llms gpt-4 evaluating quality natural languagegeneration nlg increasingly paramount based metrics bleu rouge shown misalign withhuman evaluation ill suited open ended generation tasks issue explore potential capability llm based metrics especially leveraging open source llms study wide_range wide range promptsand prompting_techniques techniques systematically analyzed approaches strategy score aggregation explainability research focuseson formulating effective prompt_templates templates determining granularity scores assessing impact context_examples context examples llm furthermore aggregation strategies compared reliable method aggregating nlg quality scores devise strategy generates rationales analyzes characteristics explanation produced open extensive_experiments extensive experiments provide insights evaluation capabilitiesof open source llms suggest effective strategies"}
{"id": "nan", "abstract": "  Large Language Models have excelled in remarkable reasoning capabilities withadvanced prompting techniques, but they fall short on tasks that requireexploration, strategic foresight, and sequential decision-making. Recent workspropose to utilize external programs to define search logic, such that LLMs canperform passive tree search to solve more challenging reasoning tasks. Thoughimpressive results have been achieved, there are several fundamentallimitations of these approaches. First, passive tree searches are not efficientas they usually require multiple rounds of LLM API calls to solve one singleproblem. Moreover, passive search methods are not flexible since they needtask-specific program designs. Then a natural question arises: can we maintainthe tree-search capability of LLMs without the aid of external programs, andcan still generate responses that clearly demonstrate the process of atree-structure search? To this end, we propose a new concept called autonomoustree-search ability of LLM, which can automatically generate a responsecontaining search trajectories for the correct answer. Concretely, we performsearch trajectories using capable LLM API via a fixed system prompt, allowingthem to perform autonomous tree-search (ATS) right out of the box. Experimentson 4 puzzle games demonstrate our method can achieve huge improvements. TheATS-BFS method outperforms the Chain of Thought approach by achieving anaverage accuracy improvement of 33%. Compared to Tree of Thoughts, it requires65.6% or 47.7% less GPT-api cost to attain a comparable level of accuracy.Moreover, we have collected data using the ATS prompt method and fine-tunedLLaMA. This approach yield a greater improvement compared to the onesfine-tuned on CoT data. Specifically, it outperforms CoT-tuned LLaMAs by anaverage of 40.6% and 38.5% for LLaMA2-7B and LLaMA2-13B, respectively.", "title": "autonomous treesearch ability of large language models", "url": "http://arxiv.org/pdf/2310.10686v1.pdf", "tokenized_text": "large_language large language excelled remarkable reasoning capabilities prompting_techniques techniques fall short tasks strategic sequential decision making recent utilize external programs define search logic llms canperform passive tree search solve challenging reasoning tasks results achieved approaches passive tree searches usually require multiple rounds llm api calls solve passive search methods flexible specific program designs natural question arises tree search capability llms aid external programs andcan generate responses clearly demonstrate process structure search end propose_a_new propose new concept called search ability llm automatically generate search trajectories correct answer concretely trajectories capable llm api fixed system perform autonomous tree search ats right box experimentson puzzle games demonstrate method achieve huge improvements method outperforms chain_of_thought chain thought approach achieving anaverage accuracy improvement 33 compared tree thoughts gpt api cost attain comparable level accuracy collected data ats method fine approach yield greater improvement compared tuned cot data specifically outperforms cot tuned anaverage llama2 7b llama2 13b respectively"}
{"id": "nan", "abstract": "  Answering multi-hop questions over hybrid factual knowledge from the giventext and table (TextTableQA) is a challenging task. Existing models mainlyadopt a retriever-reader framework, which have several deficiencies, such asnoisy labeling in training retriever, insufficient utilization of heterogeneousinformation over text and table, and deficient ability for different reasoningoperations. In this paper, we propose a three-stage TextTableQA frameworkS3HQA, which comprises of retriever, selector, and reasoner. We use a retrieverwith refinement training to solve the noisy labeling problem. Then, a hybridselector considers the linked relationships between heterogeneous data toselect the most relevant factual knowledge. For the final stage, instead ofadapting a reading comprehension module like in previous methods, we employ ageneration-based reasoner to obtain answers. This includes two approaches: arow-wise generator and an LLM prompting generator~(first time used in thistask). The experimental results demonstrate that our method achievescompetitive results in the few-shot setting. When trained on the full dataset,our approach outperforms all baseline methods, ranking first on the HybridQAleaderboard.", "title": "s$^3$hqa a threestage approach for multihop texttable hybrid question answering", "url": "http://arxiv.org/pdf/2305.11725v1.pdf", "tokenized_text": "answering multi hop questions hybrid factual knowledge table texttableqa challenging task existing retriever reader framework labeling training retriever insufficient utilization text table deficient ability different paper propose stage texttableqa comprises retriever selector reasoner use refinement training solve noisy labeling problem considers relationships heterogeneous data toselect relevant factual knowledge final stage instead reading comprehension module like previous methods employ based reasoner obtain answers includes approaches wise generator llm time experimental_results experimental results demonstrate method achievescompetitive results shot_setting shot setting trained dataset approach outperforms baseline methods ranking"}
{"id": "nan", "abstract": "  Recent large language models (LLMs) are promising for making decisions ingrounded environments. However, LLMs frequently fail in complex decision-makingtasks due to the misalignment between the pre-trained knowledge in LLMs and theactual rules in the environment. Existing methods require either costlygradient computation or lengthy in-context demonstrations. In this paper, wepropose AutoPlan, an approach to guide LLM-based agents to accomplishinteractive decision-making tasks. AutoPlan augments the LLM prompt with atask-solving plan and optimizes it through iterative experience collection andreflection. Our experiments show that AutoPlan, though using no in-contextdemonstrations, achieves success rates on par with the baselines usinghuman-written demonstrations on ALFWorld and even outperforms them by 8% onHotpotQA. The code is available at https://github.com/owaski/AutoPlan.", "title": "autoplan automatic planning of interactive decisionmaking tasks with large language models", "url": "http://arxiv.org/pdf/2305.15064v3.pdf", "tokenized_text": "recent large_language large language llms promising making decisions environments llms frequently fail complex decision misalignment pre trained knowledge llms rules environment existing_methods existing methods require computation lengthy context demonstrations paper wepropose approach guide llm based agents decision making tasks augments llm atask solving plan optimizes iterative experience collection experiments contextdemonstrations achieves success rates par baselines written demonstrations alfworld outperforms code_is_available code available"}
{"id": "nan", "abstract": "  The \"Information Retrieval in Software Engineering (IRSE)\" at FIRE 2023shared task introduces code comment classification, a challenging task thatpairs a code snippet with a comment that should be evaluated as either usefulor not useful to the understanding of the relevant code. We answer the codecomment classification shared task challenge by providing a two-foldevaluation: from an algorithmic perspective, we compare the performance ofclassical machine learning systems and complement our evaluations from adata-driven perspective by generating additional data with the help of largelanguage model (LLM) prompting to measure the potential increase inperformance. Our best model, which took second place in the shared task, is aNeural Network with a Macro-F1 score of 88.401% on the provided seed data and a1.5% overall increase in performance on the data generated by the LLM.", "title": "a mlllm pairing for better code comment classification", "url": "http://arxiv.org/pdf/2310.10275v1.pdf", "tokenized_text": "information retrieval software engineering task introduces code comment classification challenging task code snippet comment evaluated useful understanding relevant code answer classification shared task challenge providing algorithmic perspective compare performance machine_learning machine learning systems complement evaluations adata driven perspective generating additional data help largelanguage llm measure potential increase inperformance best took second place shared task network macro f1_score f1 score provided seed data overall increase performance data generated llm"}
{"id": "nan", "abstract": "  In this paper, we investigate the usage of large language models (LLMs) toimprove the performance of competitive speech recognition systems. Differentfrom traditional language models that focus on one single data domain, the riseof LLMs brings us the opportunity to push the limit of state-of-the-art ASRperformance, and at the same time to achieve higher robustness and generalizeeffectively across multiple domains. Motivated by this, we propose a novelmulti-stage approach to combine traditional language model re-scoring and LLMprompting. Specifically, the proposed method has two stages: the first stageuses a language model to re-score an N-best list of ASR hypotheses and run aconfidence check; The second stage uses prompts to a LLM to perform ASR errorcorrection on less confident results from the first stage. Our experimentalresults demonstrate the effectiveness of the proposed method by showing a 10% ~20% relative improvement in WER over a competitive ASR system -- acrossmultiple test domains.", "title": "multistage large language model correction for speech recognition", "url": "http://arxiv.org/pdf/2310.11532v1.pdf", "tokenized_text": "paper investigate usage large_language large language llms toimprove performance competitive speech recognition systems traditional language_models language focus single data domain llms brings opportunity push limit state art time achieve higher robustness multiple domains motivated propose stage approach combine traditional language_model language scoring specifically proposed_method proposed method stages language_model language score best list asr hypotheses run check second stage uses llm perform asr confident results stage experimentalresults demonstrate_the_effectiveness demonstrate effectiveness proposed_method proposed method showing 10 relative improvement wer competitive asr system acrossmultiple test domains"}
{"id": "nan", "abstract": "  Predictive suggestion systems offer contextually-relevant text entrycompletions. Existing approaches, like autofill, often excel innarrowly-defined domains but fail to generalize to arbitrary workflows. Weintroduce a conceptual framework to analyze the compound demands of aparticular suggestion context, yielding unique opportunities for large languagemodels (LLMs) to infer suggestions for a wide range of domain-agnosticform-filling tasks that were out of reach with prior approaches. We explorethese opportunities in OmniFill, a prototype that collects multi-facetedcontext including browsing and text entry activity to construct an LLM promptthat offers suggestions in situ for arbitrary structured text entry interfaces.Through a user study with 18 participants, we found that OmniFill offeredvaluable suggestions and we identified four themes that characterize users'behavior and attitudes: an \"opportunistic scrapbooking\" approach; a trustplaced in the system; value in partial success; and a need for visibility intoprompt context.", "title": "omnifill domainagnostic form filling suggestions using multifaceted context", "url": "http://arxiv.org/pdf/2310.17826v1.pdf", "tokenized_text": "predictive suggestion systems offer contextually relevant text existing approaches like excel defined domains fail generalize arbitrary workflows weintroduce conceptual framework analyze compound demands aparticular suggestion context yielding unique opportunities large_languagemodels large languagemodels llms infer suggestions wide_range wide range domain filling tasks reach prior approaches opportunities prototype collects multi including text entry activity construct llm offers suggestions arbitrary structured text entry interfaces user study 18 participants found suggestions identified themes characterize attitudes opportunistic approach system value partial success need context"}
{"id": "nan", "abstract": "  Clinical natural language processing requires methods that can addressdomain-specific challenges, such as complex medical terminology and clinicalcontexts. Recently, large language models (LLMs) have shown promise in thisdomain. Yet, their direct deployment can lead to privacy issues and areconstrained by resources. To address this challenge, we delve into syntheticclinical text generation using LLMs for clinical NLP tasks. We propose aninnovative, resource-efficient approach, ClinGen, which infuses knowledge intothe process. Our model involves clinical knowledge extraction andcontext-informed LLM prompting. Both clinical topics and writing styles aredrawn from external domain-specific knowledge graphs and LLMs to guide datageneration. Our extensive empirical study across 7 clinical NLP tasks and 16datasets reveals that ClinGen consistently enhances performance across varioustasks, effectively aligning the distribution of real datasets and significantlyenriching the diversity of generated training instances. We will publish ourcode and all the generated data in \\url{https://github.com/ritaranx/ClinGen}.", "title": "knowledgeinfused prompting assessing and advancing clinical text data generation with large language models", "url": "http://arxiv.org/pdf/2311.00287v1.pdf", "tokenized_text": "clinical natural_language natural language processing requires methods specific challenges complex medical terminology recently large_language large language llms shown promise direct deployment lead privacy issues resources address challenge delve text generation llms clinical nlp_tasks nlp tasks propose resource efficient approach knowledge intothe process involves clinical knowledge extraction andcontext informed llm clinical topics writing styles external domain specific knowledge graphs llms guide extensive empirical study clinical nlp_tasks nlp tasks reveals consistently enhances performance varioustasks effectively aligning distribution real datasets diversity generated training instances publish ourcode generated data \\url{https://github.com"}
{"id": "nan", "abstract": "  While automatic dialogue tutors hold great potential in making educationpersonalized and more accessible, research on such systems has been hampered bya lack of sufficiently large and high-quality datasets. Collecting suchdatasets remains challenging, as recording tutoring sessions raises privacyconcerns and crowdsourcing leads to insufficient data quality. To address this,we propose a framework to generate such dialogues by pairing human teacherswith a Large Language Model (LLM) prompted to represent common student errors.We describe how we use this framework to collect MathDial, a dataset of 3kone-to-one teacher-student tutoring dialogues grounded in multi-step mathreasoning problems. While models like GPT-3 are good problem solvers, they failat tutoring because they generate factually incorrect feedback or are prone torevealing solutions to students too early. To overcome this, we let teachersprovide learning opportunities to students by guiding them using variousscaffolding questions according to a taxonomy of teacher moves. We demonstrateMathDial and its extensive annotations can be used to finetune models to bemore effective tutors (and not just solvers). We confirm this by automatic andhuman evaluation, notably in an interactive setting that measures the trade-offbetween student solving success and telling solutions. The dataset is releasedpublicly.", "title": "mathdial a dialogue tutoring dataset with rich pedagogical properties grounded in math reasoning problems", "url": "http://arxiv.org/pdf/2305.14536v2.pdf", "tokenized_text": "automatic dialogue tutors hold great_potential great potential making accessible research systems hampered lack sufficiently large high quality datasets collecting remains challenging recording tutoring sessions raises crowdsourcing leads insufficient data quality address propose framework generate dialogues pairing human large_language large language llm prompted represent common student errors describe use framework collect dataset teacher student tutoring dialogues grounded multi step problems like gpt-3 good problem solvers tutoring generate factually incorrect feedback prone solutions students early overcome let learning opportunities students guiding questions according taxonomy teacher moves extensive annotations finetune effective tutors solvers confirm automatic andhuman evaluation notably interactive setting measures trade student solving success solutions dataset"}
{"id": "nan", "abstract": "  We study few-shot reranking for multi-hop QA with open-domain questions. Toalleviate the need for a large number of labeled question-document pairs forretriever training, we propose PromptRank, which relies on large languagemodels prompting for multi-hop path reranking. PromptRank first constructs aninstruction-based prompt that includes a candidate document path and thencomputes the relevance score between a given question and the path based on theconditional likelihood of the question given the path prompt according to alanguage model. PromptRank yields strong retrieval performance on HotpotQA withonly 128 training examples compared to state-of-the-art methods trained onthousands of examples -- 73.6 recall@10 by PromptRank vs. 77.8 by PathRetrieverand 77.5 by multi-hop dense retrieval. Code available athttps://github.com/mukhal/PromptRank", "title": "fewshot reranking for multihop qa via language model prompting", "url": "http://arxiv.org/pdf/2205.12650v3.pdf", "tokenized_text": "study shot reranking multi hop qa open domain questions need large number labeled question document pairs training propose relies large_languagemodels large languagemodels multi hop path reranking constructs aninstruction based includes candidate document path relevance score given question path based theconditional likelihood question given path according alanguage yields strong retrieval performance hotpotqa withonly 128 training_examples training examples compared state art methods trained examples vs. multi hop dense retrieval code available"}
{"id": "nan", "abstract": "  Large language models have shown tremendous performance in a variety oftasks. In-context learning -- the ability to improve at a task after beingprovided with a number of demonstrations -- is seen as one of the maincontributors to their success. In the present paper, we demonstrate that thein-context learning abilities of large language models can be recursivelyimproved via in-context learning itself. We coin this phenomenonmeta-in-context learning. Looking at two idealized domains, a one-dimensionalregression task and a two-armed bandit task, we show that meta-in-contextlearning adaptively reshapes a large language model's priors over expectedtasks. Furthermore, we find that meta-in-context learning modifies thein-context learning strategies of such models. Finally, we extend our approachto a benchmark of real-world regression problems where we observe competitiveperformance to traditional learning algorithms. Taken together, our workimproves our understanding of in-context learning and paves the way towardadapting large language models to the environment they are applied purelythrough meta-in-context learning rather than traditional finetuning.", "title": "metaincontext learning in large language models", "url": "http://arxiv.org/pdf/2305.12907v1.pdf", "tokenized_text": "large_language large language shown tremendous performance variety oftasks context_learning context learning ability improve task number demonstrations seen success present paper demonstrate thein context_learning context learning abilities large_language large language context_learning context learning coin context_learning context learning looking domains task armed bandit task meta contextlearning adaptively reshapes large_language large language priors furthermore find meta context_learning context learning modifies thein context_learning context learning strategies finally extend approachto benchmark real world regression problems observe competitiveperformance traditional learning algorithms taken understanding context_learning context learning paves way large_language large language environment applied meta context_learning context learning traditional finetuning"}
{"id": "nan", "abstract": "  Large-scale language models have shown the ability to adapt to a new task viaconditioning on a few demonstrations (i.e., in-context learning). However, inthe vision-language domain, most large-scale pre-trained vision-language (VL)models do not possess the ability to conduct in-context learning. How can weenable in-context learning for VL models? In this paper, we study aninteresting hypothesis: can we transfer the in-context learning ability fromthe language domain to VL domain? Specifically, we first meta-trains a languagemodel to perform in-context learning on NLP tasks (as in MetaICL); then wetransfer this model to perform VL tasks by attaching a visual encoder. Ourexperiments suggest that indeed in-context learning ability can be transferredcross modalities: our model considerably improves the in-context learningcapability on VL tasks and can even compensate for the size of the modelsignificantly. On VQA, OK-VQA, and GQA, our method could outperform thebaseline model while having 20 times fewer parameters.", "title": "metavl transferring incontext learning ability from language models to visionlanguage models", "url": "http://arxiv.org/pdf/2306.01311v1.pdf", "tokenized_text": "large scale language_models language shown ability adapt new task demonstrations i.e. context_learning context learning inthe vision language domain large scale pre trained vision language possess ability conduct context_learning context learning context_learning context learning vl paper study hypothesis transfer context_learning context learning ability fromthe language domain vl domain specifically meta trains languagemodel perform context_learning context learning nlp_tasks nlp tasks metaicl perform vl tasks attaching visual encoder ourexperiments suggest context_learning context learning ability modalities considerably improves context learningcapability vl tasks size vqa ok vqa gqa method outperform having 20 times fewer parameters"}
{"id": "nan", "abstract": "  Large language models (LMs) such as GPT-3 have the surprising ability to doin-context learning, where the model learns to do a downstream task simply byconditioning on a prompt consisting of input-output examples. The LM learnsfrom these examples without being explicitly pretrained to learn. Thus, it isunclear what enables in-context learning. In this paper, we study howin-context learning can emerge when pretraining documents have long-rangecoherence. Here, the LM must infer a latent document-level concept to generatecoherent next tokens during pretraining. At test time, in-context learningoccurs when the LM also infers a shared latent concept between examples in aprompt. We prove when this occurs despite a distribution mismatch betweenprompts and pretraining data in a setting where the pretraining distribution isa mixture of HMMs. In contrast to messy large-scale datasets used to train LMscapable of in-context learning, we generate a small-scale synthetic dataset(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyondthe theory, experiments on GINC exhibit large-scale real-world phenomenaincluding improved in-context performance with model scaling (despite the samepretraining loss), sensitivity to example order, and instances where zero-shotis better than few-shot in-context learning.", "title": "an explanation of incontext learning as implicit bayesian inference", "url": "http://arxiv.org/pdf/2111.02080v6.pdf", "tokenized_text": "large_language large language lms gpt-3 surprising ability context_learning context learning learns downstream task simply consisting input output examples lm examples explicitly pretrained learn isunclear enables context_learning context learning paper study context_learning context learning emerge pretraining documents long lm infer latent document level concept tokens pretraining test_time test time context lm infers shared latent concept examples aprompt prove occurs despite distribution mismatch pretraining data setting pretraining distribution isa mixture contrast messy large scale datasets train context_learning context learning generate small scale synthetic transformers exhibit context_learning context learning theory experiments exhibit large scale real world improved context performance scaling despite loss sensitivity example order instances zero better shot context_learning context learning"}
{"id": "nan", "abstract": "  Language models have been shown to perform better with an increase in scaleon a wide variety of tasks via the in-context learning paradigm. In this paper,we investigate the hypothesis that the ability of a large language model toin-context learn-perform a task is not uniformly spread across all of itsunderlying components. Using a 66 billion parameter language model (OPT-66B)across a diverse set of 14 downstream tasks, we find this is indeed the case:$\\sim$70% of attention heads and $\\sim$20% of feed forward networks can beremoved with minimal decline in task performance. We find substantial overlapin the set of attention heads (un)important for in-context learning acrosstasks and number of in-context examples. We also address our hypothesis througha task-agnostic lens, finding that a small set of attention heads in OPT-66Bscore highly on their ability to perform primitive induction operationsassociated with in-context learning, namely, prefix matching and copying. Theseinduction heads overlap with task-specific important heads, reinforcingarguments by Olsson et al. (arXiv:2209.11895) regarding induction headgenerality to more sophisticated behaviors associated with in-context learning.Overall, our study provides several insights that indicate large languagemodels may be under-trained for in-context learning and opens up questions onhow to pre-train language models to more effectively perform in-contextlearning.", "title": "rethinking the role of scale for incontext learning an interpretabilitybased case study at 66 billion scale", "url": "http://arxiv.org/pdf/2212.09095v2.pdf", "tokenized_text": "language_models language shown perform better increase wide variety tasks context_learning context learning paradigm paper investigate hypothesis ability large_language large language toin context learn perform task uniformly spread components billion parameter language_model language diverse set 14 downstream_tasks downstream tasks find attention heads feed forward networks minimal task performance find substantial set attention heads context_learning context learning number context_examples context examples address hypothesis task agnostic lens finding small set attention heads highly ability perform primitive induction context_learning context learning prefix matching copying heads overlap task specific important heads et_al et al induction sophisticated behaviors associated context_learning context learning overall study provides insights indicate large_languagemodels large languagemodels trained context_learning context learning opens questions pre train language_models language effectively perform contextlearning"}
{"id": "nan", "abstract": "  In-context learning, a capability that enables a model to learn from inputexamples on the fly without necessitating weight updates, is a definingcharacteristic of large language models. In this work, we follow the settingproposed in (Garg et al., 2022) to better understand the generality andlimitations of in-context learning from the lens of the simple yet fundamentaltask of linear regression. The key question we aim to address is: Aretransformers more adept than some natural and simpler architectures atperforming in-context learning under varying distribution shifts? To comparetransformers, we propose to use a simple architecture based on set-basedMulti-Layer Perceptrons (MLPs). We find that both transformers and set-basedMLPs exhibit in-context learning under in-distribution evaluations, buttransformers more closely emulate the performance of ordinary least squares(OLS). Transformers also display better resilience to mild distribution shifts,where set-based MLPs falter. However, under severe distribution shifts, bothmodels' in-context learning abilities diminish.", "title": "a closer look at incontext learning under distribution shifts", "url": "http://arxiv.org/pdf/2305.16704v1.pdf", "tokenized_text": "context_learning context learning capability enables learn fly necessitating weight updates large_language large language work follow et_al et al 2022 better understand generality andlimitations context_learning context learning lens simple linear regression key question aim address adept natural simpler architectures context_learning context learning varying distribution shifts propose use simple architecture based set layer perceptrons mlps find transformers set exhibit context_learning context learning distribution evaluations closely emulate performance ordinary transformers display better resilience mild distribution shifts set based mlps severe distribution shifts context_learning context learning abilities diminish"}
{"id": "nan", "abstract": "  What is the relationship between model architecture and the ability toperform in-context learning? In this empirical study, we take the first stepstowards answering this question. In particular, we evaluate fifteen modelarchitectures across a suite of synthetic in-context learning tasks. Theselected architectures represent a broad range of paradigms, includingrecurrent and convolution-based neural networks, transformers, and emergingattention alternatives. We discover that all considered architectures canperform in-context learning under certain conditions. However, contemporaryarchitectures are found to be the best performing, especially as taskcomplexity grows. Additionally, our follow-up experiments delve into variousfactors that influence in-context learning. We observe varied sensitivitiesamong architectures with respect to hyperparameter settings. Our study oftraining dynamics reveals that certain architectures exhibit a smooth,progressive learning trajectory, while others demonstrate periods of stagnationfollowed by abrupt mastery of the task. Finally, and somewhat surprisingly, wefind that several emerging attention alternatives are more robust in-contextlearners than transformers; since such approaches have constant-sized memoryfootprints at inference time, this result opens the future possibility ofscaling up in-context learning to vastly larger numbers of in-context examples.", "title": "exploring the relationship between model architecture and incontext learning ability", "url": "http://arxiv.org/pdf/2310.08049v1.pdf", "tokenized_text": "relationship architecture ability toperform context_learning context learning empirical study answering question particular evaluate suite synthetic context_learning context learning tasks architectures represent broad range paradigms based neural_networks neural networks transformers alternatives discover considered architectures canperform context_learning context learning certain conditions found best performing especially grows additionally follow experiments delve influence context_learning context learning observe varied architectures respect hyperparameter settings study oftraining dynamics reveals certain architectures exhibit smooth progressive learning trajectory demonstrate periods abrupt task finally somewhat surprisingly wefind emerging attention alternatives robust contextlearners transformers approaches constant sized inference time result opens future possibility context_learning context learning vastly larger numbers context_examples context examples"}
{"id": "nan", "abstract": "  In-context learning refers to the ability of a model to condition on a promptsequence consisting of in-context examples (input-output pairs corresponding tosome task) along with a new query input, and generate the corresponding output.Crucially, in-context learning happens only at inference time without anyparameter updates to the model. While large language models such as GPT-3exhibit some ability to perform in-context learning, it is unclear what therelationship is between tasks on which this succeeds and what is present in thetraining data. To make progress towards understanding in-context learning, weconsider the well-defined problem of training a model to in-context learn afunction class (e.g., linear functions): that is, given data derived from somefunctions in the class, can we train a model to in-context learn \"most\"functions from this class? We show empirically that standard Transformers canbe trained from scratch to perform in-context learning of linear functions --that is, the trained model is able to learn unseen linear functions fromin-context examples with performance comparable to the optimal least squaresestimator. In fact, in-context learning is possible even under two forms ofdistribution shift: (i) between the training data of the model andinference-time prompts, and (ii) between the in-context examples and the queryinput during inference. We also show that we can train Transformers toin-context learn more complex function classes -- namely sparse linearfunctions, two-layer neural networks, and decision trees -- with performancethat matches or exceeds task-specific learning algorithms. Our code and modelsare available at https://github.com/dtsip/in-context-learning .", "title": "what can transformers learn incontext a case study of simple function classes", "url": "http://arxiv.org/pdf/2208.01066v3.pdf", "tokenized_text": "context_learning context learning refers ability condition promptsequence consisting context_examples context examples input output pairs corresponding tosome task new query input generate corresponding output crucially context_learning context learning happens inference time anyparameter updates large_language large language ability perform context_learning context learning unclear tasks succeeds present thetraining data progress understanding context_learning context learning defined problem training context learn afunction class e.g. linear functions given data derived class train context learn class empirically standard transformers canbe trained scratch perform context_learning context learning linear functions trained able learn unseen linear functions context_examples context examples performance comparable optimal fact context_learning context learning possible forms ofdistribution shift training_data training data andinference time ii context_examples context examples inference train transformers toin context learn complex function classes sparse layer neural_networks neural networks decision trees matches exceeds task specific learning algorithms code modelsare available"}
{"id": "nan", "abstract": "  Large language models have exhibited intriguing in-context learningcapability, achieving promising zero- and few-shot performance without updatingthe parameters. However, conventional in-context learning is usually restrictedby length constraints, rendering it ineffective to absorb supervision from alarge number of examples. In order to go beyond few shots, we introducestructured prompting that breaks the length limit and scales in-contextlearning to thousands of examples. Specifically, demonstration examples areseparately encoded with well-designed position embeddings, and then they arejointly attended by the test example using a rescaled attention mechanism. Sowe can scale the number of exemplars with linear complexity instead ofquadratic complexity with respect to length. Experimental results on a diverseset of tasks show that our approach improves end-task performance and reducesevaluation variance over conventional in-context learning as the number ofdemonstration examples increases. Code has been released athttps://aka.ms/structured-prompting.", "title": "structured prompting scaling incontext learning to 1,000 examples", "url": "http://arxiv.org/pdf/2212.06713v1.pdf", "tokenized_text": "large_language large language exhibited intriguing context learningcapability achieving promising zero- shot performance parameters conventional context_learning context learning usually length constraints rendering ineffective supervision alarge number examples order shots breaks length limit scales contextlearning thousands examples specifically demonstration examples encoded designed position embeddings test example attention mechanism scale number exemplars linear complexity instead complexity respect length experimental_results experimental results tasks approach improves end task performance variance conventional context_learning context learning number ofdemonstration examples increases code released"}
{"id": "nan", "abstract": "  In-context learning, where pre-trained language models learn to perform tasksfrom task examples and instructions in their contexts, has attracted muchattention in the NLP community. However, the ability of in-context learning isnot fully exploited because language models are not explicitly trained to learnin context. To this end, we propose PICL (Pre-training for In-ContextLearning), a framework to enhance the language models' in-context learningability by pre-training the model on a large collection of \"intrinsic tasks\" inthe general plain-text corpus using the simple language modeling objective.PICL encourages the model to infer and perform tasks by conditioning on thecontexts while maintaining task generalization of pre-trained models. Weevaluate the in-context learning performance of the model trained with PICL onseven widely-used text classification datasets and the Super-NaturalInstrctionsbenchmark, which contains 100+ NLP tasks formulated to text generation. Ourexperiments show that PICL is more effective and task-generalizable than arange of baselines, outperforming larger language models with nearly 4xparameters. The code is publicly available at https://github.com/thu-coai/PICL.", "title": "pretraining to learn in context", "url": "http://arxiv.org/pdf/2305.09137v1.pdf", "tokenized_text": "context_learning context learning pre trained_language trained language learn perform tasksfrom task examples instructions contexts attracted muchattention nlp community ability context_learning context learning fully exploited language_models language explicitly trained learnin context end propose pre training contextlearning framework enhance language_models language context learningability pre training large collection intrinsic tasks inthe general plain text corpus simple language modeling objective encourages infer perform tasks conditioning maintaining task generalization pre trained weevaluate context_learning context learning performance trained widely text_classification text classification datasets super contains 100 nlp_tasks nlp tasks formulated text generation ourexperiments effective task generalizable arange baselines outperforming larger language_models language nearly code publicly_available publicly available"}
{"id": "nan", "abstract": "  In this paper, we investigate the in-context learning ability ofretrieval-augmented encoder-decoder language models. We first conduct acomprehensive analysis of the state-of-the-art ATLAS model and identify itslimitations in in-context learning, primarily due to a mismatch betweenpretraining and testing, as well as a restricted context length. To addressthese issues, we propose RAVEN, a model that combines retrieval-augmentedmasked language modeling and prefix language modeling. We further introduceFusion-in-Context Learning to enhance the few-shot performance by enabling themodel to leverage more in-context examples without requiring additionaltraining or model modifications. Through extensive experiments, we demonstratethat RAVEN significantly outperforms ATLAS and achieves results comparable tothe most advanced language models in certain scenarios, despite havingsubstantially fewer parameters. Our work underscores the potential ofretrieval-augmented encoder-decoder language models for in-context learning andencourages further research in this direction.", "title": "raven incontext learning with retrieval augmented encoderdecoder language models", "url": "http://arxiv.org/pdf/2308.07922v1.pdf", "tokenized_text": "paper investigate context_learning context learning ability ofretrieval augmented encoder decoder language_models language conduct acomprehensive analysis state art atlas identify context_learning context learning primarily mismatch testing restricted context length addressthese issues propose raven combines retrieval language modeling prefix language modeling context_learning context learning enhance shot performance enabling themodel leverage context_examples context examples requiring additionaltraining modifications extensive_experiments extensive experiments demonstratethat raven significantly_outperforms significantly outperforms atlas achieves results comparable tothe advanced language_models language certain scenarios despite fewer parameters work underscores potential ofretrieval augmented encoder decoder language_models language context_learning context learning research direction"}
{"id": "nan", "abstract": "  Large language models (LLMs) trained on huge corpora of text datasetsdemonstrate complex, emergent capabilities, achieving state-of-the-artperformance on tasks they were not explicitly trained for. The precise natureof LLM capabilities is often mysterious, and different prompts can elicitdifferent capabilities through in-context learning. We propose a CognitiveInterpretability framework that enables us to analyze in-context learningdynamics to understand latent concepts in LLMs underlying behavioral patterns.This provides a more nuanced understanding than success-or-failure evaluationbenchmarks, but does not require observing internal activations as amechanistic interpretation of circuits would. Inspired by the cognitive scienceof human randomness perception, we use random binary sequences as context andstudy dynamics of in-context learning by manipulating properties of contextdata, such as sequence length. In the latest GPT-3.5+ models, we find emergentabilities to generate pseudo-random numbers and learn basic formal languages,with striking in-context learning dynamics where model outputs transitionsharply from pseudo-random behaviors to deterministic repetition.", "title": "incontext learning dynamics with random binary sequences", "url": "http://arxiv.org/pdf/2310.17639v1.pdf", "tokenized_text": "large_language large language llms trained huge corpora text datasetsdemonstrate complex emergent capabilities achieving state artperformance tasks explicitly trained precise natureof llm capabilities different capabilities context_learning context learning propose framework enables analyze context understand latent concepts llms underlying behavioral patterns provides nuanced understanding success failure evaluationbenchmarks require observing internal interpretation circuits inspired cognitive human randomness perception use random binary sequences context dynamics context_learning context learning manipulating properties sequence length latest gpt-3.5 find emergentabilities generate pseudo random numbers learn basic formal languages striking context_learning context learning dynamics outputs pseudo random behaviors deterministic repetition"}
{"id": "nan", "abstract": "  Large pre-training language models (PLMs) have shown promising in-contextlearning abilities. However, due to the backbone transformer architecture,existing PLMs are bottlenecked by the memory and computational cost whenscaling up to a large context size, leaving instruction tuning and in-contextlearning of many demonstration examples, as well as long-range languagemodeling under-explored. In this study, we propose a long-range language modelEVALM based on an efficient transformer mechanism. EVALM is trained with 8ktokens per batch line and can test up to 256k-lengthed contexts withextrapolation, 128 times to the limit of existing PLMs (e.g. GPT3). Based onEVALM, we scale up the size of examples efficiently in both instruction tuningand in-context learning to explore the boundary of the benefits from moreannotated data. Experimental results on a diverse set of tasks show that EVALMachieves 4.1% higher accuracy on average, and the average length of achievingthe best accuracy score over tasks is around 12k. We find that in-contextlearning can achieve higher performance with more demonstrations undermany-shot instruction tuning (8k), and further extending the length ofinstructions (16k) can further improve the upper bound of scaling in-contextlearning.", "title": "incontext learning with many demonstration examples", "url": "http://arxiv.org/pdf/2302.04931v1.pdf", "tokenized_text": "large pre training language_models language plms shown promising contextlearning abilities backbone transformer architecture existing plms bottlenecked memory computational cost large context size leaving instruction_tuning instruction tuning contextlearning demonstration examples long range languagemodeling explored study propose long range language based efficient transformer mechanism trained batch line test contexts 128 times limit existing plms e.g. gpt3 based scale size examples efficiently instruction tuningand context_learning context learning explore boundary benefits data experimental_results experimental results diverse set tasks higher accuracy average average length achievingthe best accuracy score tasks find contextlearning achieve higher performance demonstrations shot instruction_tuning instruction tuning extending length improve upper bound scaling contextlearning"}
{"id": "nan", "abstract": "  In-context learning is a surprising and important phenomenon that emergedwhen modern language models were scaled to billions of learned parameters.Without modifying a large language model's weights, it can be tuned to performvarious downstream natural language tasks simply by including concatenatedtraining examples of these tasks in its input. Though disruptive for manypractical applications of large language models, this emergent learningparadigm is not well understood from a theoretical perspective. In this paper,we propose a first-of-its-kind PAC based framework for in-context learnability,and use it to provide the first finite sample complexity results for thein-context learning setup. Our framework includes an initial pretraining phase,which fits a function to the pretraining distribution, and then a secondin-context learning phase, which keeps this function constant and concatenatestraining examples of the downstream task in its input. We use our framework inorder to prove that, under mild assumptions, when the pretraining distributionis a mixture of latent tasks (a model often considered for natural languagepretraining), these tasks can be efficiently learned via in-context learning,even though the model's weights are unchanged and the input significantlydiverges from the pretraining distribution. Our theoretical analysis revealsthat in this setting, in-context learning is more about identifying the taskthan about learning it, a result which is in line with a series of recentempirical findings. We hope that the in-context learnability frameworkpresented in this paper will facilitate future progress towards a deeperunderstanding of this important new learning paradigm.", "title": "the learnability of incontext learning", "url": "http://arxiv.org/pdf/2303.07895v1.pdf", "tokenized_text": "context_learning context learning surprising important phenomenon modern language_models language scaled billions learned parameters modifying large_language large language weights tuned downstream natural_language natural language tasks simply including examples tasks input applications large_language large language emergent learningparadigm understood theoretical perspective paper propose kind pac based framework context use provide finite sample complexity results thein context_learning context learning setup framework includes initial pretraining phase fits function pretraining distribution context_learning context learning phase function constant examples downstream task input use framework prove mild assumptions pretraining mixture latent tasks considered natural tasks efficiently learned context_learning context learning weights unchanged input pretraining distribution theoretical analysis setting context_learning context learning identifying learning result line series findings hope context paper facilitate future progress important new learning paradigm"}
{"id": "nan", "abstract": "  Large Pre-trained Transformers exhibit an intriguing capacity for in-contextlearning. Without gradient updates, these models can rapidly construct newpredictors from demonstrations presented in the inputs. Recent works promotethis ability in the vision-language domain by incorporating visual informationinto large language models that can already make in-context predictions.However, these methods could inherit issues in the language domain, such astemplate sensitivity and hallucination. Also, the scale of these languagemodels raises a significant demand for computations, making learning andoperating these models resource-intensive. To this end, we raise a question:``How can we enable in-context learning without relying on the intrinsicin-context ability of large language models?\". To answer it, we propose asuccinct and general framework, Self-supervised IN-Context learning (SINC),that introduces a meta-model to learn on self-supervised prompts consisting oftailored demonstrations. The learned models can be transferred to downstreamtasks for making in-context predictions on-the-fly. Extensive experiments showthat SINC outperforms gradient-based methods in various vision-language tasksunder few-shot settings. Furthermore, the designs of SINC help us investigatethe benefits of in-context learning across different tasks, and the analysisfurther reveals the essential components for the emergence of in-contextlearning in the vision-language domain.", "title": "sinc selfsupervised incontext learning for visionlanguage tasks", "url": "http://arxiv.org/pdf/2307.07742v2.pdf", "tokenized_text": "large pre-trained transformers exhibit intriguing capacity contextlearning gradient updates rapidly construct demonstrations presented inputs recent works ability vision language domain incorporating visual large_language large language context predictions methods inherit issues language domain sensitivity hallucination scale languagemodels raises significant demand computations making learning resource intensive end raise enable context_learning context learning relying context ability large_language large language answer propose asuccinct general framework self supervised context_learning context learning introduces meta learn self supervised consisting demonstrations learned transferred downstreamtasks making context predictions fly extensive_experiments extensive experiments showthat outperforms gradient based methods vision language shot_settings shot settings furthermore designs help investigatethe benefits context_learning context learning different tasks reveals essential components emergence contextlearning vision language domain"}
{"id": "nan", "abstract": "  Large-scale pre-trained language models (PLMs) are well-known for beingcapable of solving a task simply by conditioning a few input-label pairs dubbeddemonstrations on a prompt without being explicitly tuned for the desireddownstream task. Such a process (i.e., in-context learning), however, naturallyleads to high reliance on the demonstrations which are usually selected fromexternal datasets. In this paper, we propose self-generated in-context learning(SG-ICL), which generates demonstrations for in-context learning from PLMitself to minimize the reliance on the external demonstration. We conductexperiments on four different text classification tasks and show SG-ICLsignificantly outperforms zero-shot learning and is generally worthapproximately 0.6 gold training samples. Moreover, our generated demonstrationsshow more consistent performance with low variance compared to randomlyselected demonstrations from the training dataset.", "title": "selfgenerated incontext learning leveraging autoregressive language models as a demonstration generator", "url": "http://arxiv.org/pdf/2206.08082v1.pdf", "tokenized_text": "large scale pre trained_language trained language plms known solving task simply conditioning input label pairs explicitly tuned task process i.e. context_learning context learning high reliance demonstrations usually selected datasets paper propose self generated context icl generates demonstrations context_learning context learning minimize reliance external demonstration conductexperiments different text_classification text classification tasks sg outperforms zero shot_learning shot learning generally 0.6 gold training samples generated consistent performance low variance compared randomlyselected demonstrations training dataset"}
{"id": "nan", "abstract": "  With a handful of demonstration examples, large-scale language models showstrong capability to perform various tasks by in-context learning from theseexamples, without any fine-tuning. We demonstrate that in-context learningperformance can be highly unstable across samples of examples, indicating theidiosyncrasies of how language models acquire information. We formulate exampleselection for in-context learning as a sequential decision problem, and proposea reinforcement learning algorithm for identifying generalizable policies toselect demonstration examples. For GPT-2, our learned policies demonstratestrong abilities of generalizing to unseen tasks in training, with a $5.8\\%$improvement on average. Examples selected from our learned policies can evenachieve a small improvement on GPT-3 Ada. However, the improvement diminisheson larger GPT-3 models, suggesting emerging capabilities of large languagemodels.", "title": "active example selection for incontext learning", "url": "http://arxiv.org/pdf/2211.04486v1.pdf", "tokenized_text": "handful demonstration examples large scale language_models language capability perform tasks context_learning context learning fine tuning demonstrate context learningperformance highly unstable samples examples indicating language_models language acquire information formulate exampleselection context_learning context learning sequential decision problem proposea reinforcement_learning reinforcement learning algorithm identifying generalizable policies toselect demonstration examples gpt-2 learned policies abilities generalizing unseen tasks training average examples selected learned policies small improvement gpt-3 ada improvement larger gpt-3 suggesting emerging capabilities large_languagemodels large languagemodels"}
{"id": "nan", "abstract": "  Large language models (LLMs) are able to do accurate classification with zeroor only a few examples (in-context learning). We show a prompting system thatenables regression with uncertainty for in-context learning with frozen LLM(GPT-3, GPT-3.5, and GPT-4) models, allowing predictions without features orarchitecture tuning. By incorporating uncertainty, our approach enablesBayesian optimization for catalyst or molecule optimization using naturallanguage, eliminating the need for training or simulation. Here, we performedthe optimization using the synthesis procedure of catalysts to predictproperties. Working with natural language mitigates difficulty synthesizabilitysince the literal synthesis procedure is the model's input. We showed thatin-context learning could improve past a model context window (maximum numberof tokens the model can process at once) as data is gathered via exampleselection, allowing the model to scale better. Although our method does notoutperform all baselines, it requires zero training, feature selection, andminimal computing while maintaining satisfactory performance. We also findGaussian Process Regression on text embeddings is strong at Bayesianoptimization. The code is available in our GitHub repository:https://github.com/ur-whitelab/BO-LIFT", "title": "bayesian optimization of catalysts with incontext learning", "url": "http://arxiv.org/pdf/2304.05341v1.pdf", "tokenized_text": "large_language large language llms able accurate classification examples context_learning context learning system thatenables regression uncertainty context_learning context learning frozen gpt-3.5 gpt-4 allowing predictions features tuning incorporating uncertainty approach optimization catalyst molecule optimization naturallanguage eliminating need training simulation optimization synthesis procedure working natural_language natural language mitigates difficulty literal synthesis procedure input showed thatin context_learning context learning improve past context window maximum numberof tokens process data gathered exampleselection allowing scale better method notoutperform baselines requires zero training feature selection computing maintaining satisfactory performance process regression text embeddings strong code_is_available code available github repository https://github.com"}
{"id": "nan", "abstract": "  We present Prompt Diffusion, a framework for enabling in-context learning indiffusion-based generative models. Given a pair of task-specific exampleimages, such as depth from/to image and scribble from/to image, and a textguidance, our model automatically understands the underlying task and performsthe same task on a new query image following the text guidance. To achievethis, we propose a vision-language prompt that can model a wide range ofvision-language tasks and a diffusion model that takes it as input. Thediffusion model is trained jointly over six different tasks using theseprompts. The resulting Prompt Diffusion model is the first diffusion-basedvision-language foundation model capable of in-context learning. Itdemonstrates high-quality in-context generation on the trained tasks andgeneralizes effectively to new, unseen vision tasks with their respectiveprompts. Our model also shows compelling text-guided image editing results. Ourframework aims to facilitate research into in-context learning for computervision. We share our code and pre-trained models athttps://github.com/Zhendong-Wang/Prompt-Diffusion.", "title": "incontext learning unlocked for diffusion models", "url": "http://arxiv.org/pdf/2305.01115v2.pdf", "tokenized_text": "present diffusion framework enabling context_learning context learning based generative given pair task specific depth image image automatically understands underlying task task new query image following text guidance propose vision language wide_range wide range language tasks diffusion takes input thediffusion trained jointly different tasks resulting diffusion diffusion language foundation capable context_learning context learning high quality context generation trained tasks effectively new unseen vision tasks shows compelling text guided image editing results ourframework aims facilitate research context_learning context learning computervision share code pre trained"}
{"id": "nan", "abstract": "  Large language models (LLMs) have recently shown great potential forin-context learning, where LLMs learn a new task simply by conditioning on afew input-label pairs (prompts). Despite their potential, our understanding ofthe factors influencing end-task performance and the robustness of in-contextlearning remains limited. This paper aims to bridge this knowledge gap byinvestigating the reliance of LLMs on shortcuts or spurious correlations withinprompts. Through comprehensive experiments on classification and extractiontasks, we reveal that LLMs are \"lazy learners\" that tend to exploit shortcutsin prompts for downstream tasks. Additionally, we uncover a surprising findingthat larger models are more likely to utilize shortcuts in prompts duringinference. Our findings provide a new perspective on evaluating robustness inin-context learning and pose new challenges for detecting and mitigating theuse of shortcuts in prompts.", "title": "large language models can be lazy learners analyze shortcuts in incontext learning", "url": "http://arxiv.org/pdf/2305.17256v2.pdf", "tokenized_text": "large_language large language llms recently shown great_potential great potential forin context_learning context learning llms learn new task simply conditioning afew input label pairs despite potential understanding ofthe factors influencing end task performance robustness contextlearning remains limited paper aims bridge knowledge gap reliance llms shortcuts spurious correlations comprehensive experiments classification extractiontasks reveal llms learners tend exploit downstream_tasks downstream tasks additionally uncover surprising larger likely utilize shortcuts duringinference findings provide new perspective evaluating robustness context_learning context learning pose new challenges detecting mitigating shortcuts"}
{"id": "nan", "abstract": "  Evaluation of natural language generation (NLG) is complex andmulti-dimensional. Generated text can be evaluated for fluency, coherence,factuality, or any other dimensions of interest. Most frameworks that performsuch multi-dimensional evaluation require training on large manually orsynthetically generated datasets. In this paper, we study the efficacy of largelanguage models as multi-dimensional evaluators using in-context learning,obviating the need for large training datasets. Our experiments show thatin-context learning-based evaluators are competitive with learned evaluationframeworks for the task of text summarization, establishing state-of-the-art ondimensions such as relevance and factual consistency. We then analyze theeffects of factors such as the selection and number of in-context examples onperformance. Finally, we study the efficacy of in-context learning basedevaluators in evaluating zero-shot summaries written by large language modelssuch as GPT-3.", "title": "multidimensional evaluation of text summarization with incontext learning", "url": "http://arxiv.org/pdf/2306.01200v1.pdf", "tokenized_text": "evaluation natural_language natural language generation nlg complex andmulti dimensional generated text evaluated fluency coherence factuality dimensions interest frameworks multi dimensional evaluation require training large manually generated datasets paper study efficacy largelanguage_models largelanguage multi dimensional evaluators context_learning context learning obviating need large training datasets experiments thatin context_learning context learning based evaluators competitive learned task text summarization establishing state art relevance factual consistency analyze theeffects factors selection number context_examples context examples finally study efficacy context_learning context learning evaluating zero shot summaries written large_language large language modelssuch gpt-3"}
{"id": "nan", "abstract": "  This paper explores the integration of Large Language Models (LLMs) intoAutomatic Speech Recognition (ASR) systems to improve transcription accuracy.The increasing sophistication of LLMs, with their in-context learningcapabilities and instruction-following behavior, has drawn significantattention in the field of Natural Language Processing (NLP). Our primary focusis to investigate the potential of using an LLM's in-context learningcapabilities to enhance the performance of ASR systems, which currently facechallenges such as ambient noise, speaker accents, and complex linguisticcontexts. We designed a study using the Aishell-1 and LibriSpeech datasets,with ChatGPT and GPT-4 serving as benchmarks for LLM capabilities.Unfortunately, our initial experiments did not yield promising results,indicating the complexity of leveraging LLM's in-context learning for ASRapplications. Despite further exploration with varied settings and models, thecorrected sentences from the LLMs frequently resulted in higher Word ErrorRates (WER), demonstrating the limitations of LLMs in speech applications. Thispaper provides a detailed overview of these experiments, their results, andimplications, establishing that using LLMs' in-context learning capabilities tocorrect potential errors in speech recognition transcriptions is still achallenging task at the current stage.", "title": "exploring the integration of large language models into automatic speech recognition systems an empirical study", "url": "http://arxiv.org/pdf/2307.06530v1.pdf", "tokenized_text": "paper explores integration large_language large language llms speech recognition asr systems improve transcription accuracy increasing sophistication llms context learningcapabilities instruction following behavior drawn significantattention field natural_language natural language processing nlp primary investigate potential llm context learningcapabilities enhance performance asr systems currently noise speaker accents complex designed study datasets chatgpt gpt-4 serving benchmarks llm capabilities unfortunately initial experiments yield promising_results promising results indicating complexity leveraging llm context_learning context learning despite exploration varied settings sentences llms frequently resulted higher word wer demonstrating limitations llms speech applications thispaper provides detailed overview experiments results establishing llms context_learning context learning capabilities potential errors speech recognition transcriptions achallenging task current stage"}
{"id": "nan", "abstract": "  Recently Large Language Models (LLMs) have been proven to have strongabilities in various domains and tasks. We study the problem of promptdesigning in the text-to-SQL task and attempt to improve the LLMs' reasoningability when generating SQL queries. Besides the trivial few-shot in-contextlearning setting, we design our chain-of-thought (CoT) prompt with a similarmethod to schema linking. We provide a method named ACT-SQL to automaticallygenerate auto-CoT exemplars and thus the whole process doesn't need manuallabeling. Our approach is cost-saving since we only use the LLMs' API call oncewhen generating one SQL query. Furthermore, we extend our in-context learningmethod to the multi-turn text-to-SQL task. The experiment results show that theLLMs' performance can benefit from our ACT-SQL approach. Our approach achievesSOTA performance on the Spider dev set among existing in-context learningapproaches.", "title": "actsql incontext learning for texttosql with automaticallygenerated chainofthought", "url": "http://arxiv.org/pdf/2310.17342v1.pdf", "tokenized_text": "recently large_language large language llms proven domains tasks study problem text sql task attempt improve llms reasoningability generating sql queries trivial shot contextlearning setting design chain thought cot schema linking provide method named act sql auto cot exemplars process need approach cost saving use llms api generating sql query furthermore extend context learningmethod multi turn text sql task experiment results thellms performance benefit act sql approach approach performance spider dev set existing context learningapproaches"}
{"id": "nan", "abstract": "  We present a data and cost efficient way of incorporating the speech modalityinto a large language model (LLM). The resulting multi-modal LLM is aCOntextual Speech Model with Instruction-following/in-context-learningCapabilities - COSMIC. Speech comprehension test question-answer (SQA) pairsare generated using GPT-3.5 based on the speech transcriptions as a part of thesupervision for the instruction tuning. With fewer than 20M trainableparameters and as little as 450 hours of English speech data for SQAgeneration, COSMIC exhibits emergent instruction-following and in-contextlearning capabilities in speech-to-text tasks. The model is able to follow thegiven text instructions to generate text response even on the unseen EN$\\to$Xspeech-to-text translation (S2TT) task with zero-shot setting. We evaluate themodel's in-context learning via various tasks such as EN$\\to$X S2TT andfew-shot domain adaptation. And instruction-following capabilities areevaluated through a contextual biasing benchmark. Our results demonstrate theefficacy of the proposed low cost recipe for building a speech LLM and thatwith the new instruction-tuning data.", "title": "cosmic data efficient instructiontuning for speech incontext learning", "url": "http://arxiv.org/pdf/2311.02248v1.pdf", "tokenized_text": "present data cost efficient way incorporating speech large_language large language llm resulting multi modal llm speech instruction following context learningcapabilities speech comprehension test question answer sqa generated gpt-3.5 based speech transcriptions instruction_tuning instruction tuning fewer 20 trainableparameters little hours english speech data exhibits emergent instruction following contextlearning capabilities speech text tasks able follow text instructions generate text response unseen text translation task zero shot_setting shot setting evaluate themodel context_learning context learning tasks andfew shot domain adaptation instruction following capabilities areevaluated contextual biasing benchmark results_demonstrate results demonstrate theefficacy proposed low cost recipe building speech llm new instruction tuning data"}
{"id": "nan", "abstract": "  The strong few-shot in-context learning capability of large pre-trainedlanguage models (PLMs) such as GPT-3 is highly appealing for applicationdomains such as biomedicine, which feature high and diverse demands of languagetechnologies but also high data annotation costs. In this paper, we present thefirst systematic and comprehensive study to compare the few-shot performance ofGPT-3 in-context learning with fine-tuning smaller (i.e., BERT-sized) PLMs ontwo highly representative biomedical information extraction tasks, named entityrecognition and relation extraction. We follow the true few-shot setting toavoid overestimating models' few-shot performance by model selection over alarge validation set. We also optimize GPT-3's performance with knowntechniques such as contextual calibration and dynamic in-context exampleretrieval. However, our results show that GPT-3 still significantlyunderperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3in-context learning also yields smaller gains in accuracy when more trainingdata becomes available. Our in-depth analyses further reveal issues of thein-context learning setting that may be detrimental to information extractiontasks in general. Given the high cost of experimenting with GPT-3, we hope ourstudy provides guidance for biomedical researchers and practitioners towardsmore promising directions such as fine-tuning small PLMs.", "title": "thinking about gpt3 incontext learning for biomedical ie think again", "url": "http://arxiv.org/pdf/2203.08410v3.pdf", "tokenized_text": "strong shot context_learning context learning capability large pre trainedlanguage plms gpt-3 highly applicationdomains feature high diverse demands languagetechnologies high data annotation costs paper present thefirst systematic comprehensive study compare shot performance context_learning context learning fine tuning smaller i.e. bert sized plms ontwo highly representative biomedical information_extraction information extraction tasks named entityrecognition relation_extraction relation extraction follow true shot_setting shot setting shot performance selection alarge validation set optimize gpt-3 performance contextual calibration dynamic context exampleretrieval results gpt-3 compared simply fine tuning smaller plm addition context_learning context learning yields smaller gains accuracy trainingdata available depth analyses reveal issues thein context_learning context learning setting detrimental information extractiontasks general given high cost experimenting gpt-3 hope provides guidance biomedical researchers practitioners promising directions fine tuning small plms"}
{"id": "nan", "abstract": "  The In-Context Learning (ICL) is to understand a new task via a fewdemonstrations (aka. prompt) and predict new inputs without tuning the models.While it has been widely studied in NLP, it is still a relatively new area ofresearch in computer vision. To reveal the factors influencing the performanceof visual in-context learning, this paper shows that prompt selection andprompt fusion are two major factors that have a direct impact on the inferenceperformance of visual context learning. Prompt selection is the process ofidentifying the most appropriate prompt or example to help the model understandnew tasks. This is important because providing the model with relevant promptscan help it learn more effectively and efficiently. Prompt fusion involvescombining knowledge from different positions within the large-scale visualmodel. By doing this, the model can leverage the diverse knowledge stored indifferent parts of the model to improve its performance on new tasks. Basedthese findings, we propose a simple framework prompt-SelF for visual in-contextlearning. Specifically, we first use the pixel-level retrieval method to selecta suitable prompt, and then use different prompt fusion methods to activate allthe knowledge stored in the large-scale model, and finally ensemble theprediction results obtained from different prompt fusion methods to obtain thefinal prediction results. And we conduct extensive experiments on single-objectsegmentation and detection tasks to demonstrate the effectiveness ofprompt-SelF. Remarkably, the prompt-SelF has outperformed OSLSM basedmeta-learning in 1-shot segmentation for the first time. This indicated thegreat potential of visual in-context learning. The source code and models willbe available at \\url{https://github.com/syp2ysy/prompt-SelF}.", "title": "exploring effective factors for improving visual incontext learning", "url": "http://arxiv.org/pdf/2304.04748v1.pdf", "tokenized_text": "context_learning context learning icl understand new task fewdemonstrations aka predict new inputs tuning widely studied nlp relatively new area ofresearch computer_vision computer vision reveal factors influencing performanceof visual context_learning context learning paper shows selection andprompt fusion major factors direct impact visual context_learning context learning selection process appropriate example help tasks important providing relevant promptscan help learn effectively efficiently fusion knowledge different positions large scale leverage diverse knowledge stored indifferent parts improve performance new tasks findings propose simple framework self visual contextlearning specifically use pixel level retrieval method suitable use different fusion methods activate knowledge stored large scale finally ensemble theprediction results obtained different fusion methods obtain prediction results conduct_extensive conduct extensive experiments single detection tasks demonstrate_the_effectiveness demonstrate effectiveness ofprompt remarkably self outperformed learning shot segmentation time indicated potential visual context_learning context learning source_code source code available \\url{https://github.com self"}
{"id": "nan", "abstract": "  Chain-of-thought (CoT) is a method that enables language models to handlecomplex reasoning tasks by decomposing them into simpler steps. Despite itssuccess, the underlying mechanics of CoT are not yet fully understood. In anattempt to shed light on this, our study investigates the impact of CoT on theability of transformers to in-context learn a simple to study, yet generalfamily of compositional functions: multi-layer perceptrons (MLPs). In thissetting, we find that the success of CoT can be attributed to breaking downin-context learning of a compositional function into two distinct phases:focusing on and filtering data related to each step of the composition andin-context learning the single-step composition function. Through bothexperimental and theoretical evidence, we demonstrate how CoT significantlyreduces the sample complexity of in-context learning (ICL) and facilitates thelearning of complex functions that non-CoT methods struggle with. Furthermore,we illustrate how transformers can transition from vanilla in-context learningto mastering a compositional function with CoT by simply incorporatingadditional layers that perform the necessary data-filtering for CoT via theattention mechanism. In addition to these test-time benefits, we show CoT helpsaccelerate pretraining by learning shortcuts to represent complex functions andfiltering plays an important role in this process. These findings collectivelyprovide insights into the mechanics of CoT, inviting further investigation ofits role in complex reasoning tasks.", "title": "dissecting chainofthought compositionality through incontext filtering and learning", "url": "http://arxiv.org/pdf/2305.18869v2.pdf", "tokenized_text": "chain thought cot method enables language_models language reasoning tasks decomposing simpler steps despite underlying mechanics cot fully understood anattempt shed light study investigates impact cot theability transformers context learn simple study compositional functions multi layer perceptrons mlps find success cot attributed breaking context_learning context learning compositional function distinct phases focusing filtering data related step composition andin context_learning context learning single step composition function theoretical evidence demonstrate cot sample complexity context_learning context learning icl facilitates thelearning complex functions non cot methods struggle furthermore illustrate transformers transition vanilla context compositional function cot simply layers perform necessary data filtering cot theattention mechanism addition test time benefits cot pretraining learning shortcuts represent complex functions plays important role process findings insights mechanics cot investigation role complex_reasoning complex reasoning tasks"}
{"id": "nan", "abstract": "  In-context learning is one of the surprising and useful features of largelanguage models. How it works is an active area of research. Recently, stylizedmeta-learning-like setups have been devised that train these models on asequence of input-output pairs $(x, f(x))$ from a function class using thelanguage modeling loss and observe generalization to unseen functions from thesame class. One of the main discoveries in this line of research has been thatfor several problems such as linear regression, trained transformers learnalgorithms for learning functions in context. However, the inductive biases ofthese models resulting in this behavior are not clearly understood. A modelwith unlimited training data and compute is a Bayesian predictor: it learns thepretraining distribution. It has been shown that high-capacity transformersmimic the Bayesian predictor for linear regression. In this paper, we showempirical evidence of transformers exhibiting the behavior of this ideallearner across different linear and non-linear function classes. We also extendthe previous setups to work in the multitask setting and verify thattransformers can do in-context learning in this setup as well and the Bayesianperspective sheds light on this setting also. Finally, via the example oflearning Fourier series, we study the inductive bias for in-context learning.We find that in-context learning may or may not have simplicity bias dependingon the pretraining data distribution.", "title": "incontext learning through the bayesian prism", "url": "http://arxiv.org/pdf/2306.04891v1.pdf", "tokenized_text": "context_learning context learning surprising useful features largelanguage_models largelanguage works active area research recently learning like setups devised train asequence input output pairs f(x))$ function class thelanguage modeling loss observe generalization unseen functions thesame class main discoveries line research problems linear regression trained transformers learning functions context inductive biases ofthese resulting behavior clearly understood modelwith unlimited training_data training data compute bayesian predictor learns thepretraining distribution shown high capacity bayesian predictor linear regression paper evidence transformers exhibiting behavior different linear non linear function classes previous setups work multitask setting verify thattransformers context_learning context learning setup sheds light setting finally example series study inductive bias context_learning context learning find context_learning context learning simplicity bias pretraining data distribution"}
{"id": "nan", "abstract": "  With the rise of large-scale models trained on broad data, in-contextlearning has become a new learning paradigm that has demonstrated significantpotential in natural language processing and computer vision tasks. Meanwhile,in-context learning is still largely unexplored in the 3D point cloud domain.Although masked modeling has been successfully applied for in-context learningin 2D vision, directly extending it to 3D point clouds remains a formidablechallenge. In the case of point clouds, the tokens themselves are the pointcloud positions (coordinates) that are masked during inference. Moreover,position embedding in previous works may inadvertently introduce informationleakage. To address these challenges, we introduce a novel framework, namedPoint-In-Context, designed especially for in-context learning in 3D pointclouds, where both inputs and outputs are modeled as coordinates for each task.Additionally, we propose the Joint Sampling module, carefully designed to workin tandem with the general point sampling operator, effectively resolving theaforementioned technical issues. We conduct extensive experiments to validatethe versatility and adaptability of our proposed methods in handling a widerange of tasks. Furthermore, with a more effective prompt selection strategy,our framework surpasses the results of individually trained models.", "title": "explore incontext learning for 3d point cloud understanding", "url": "http://arxiv.org/pdf/2306.08659v1.pdf", "tokenized_text": "rise large scale trained broad data contextlearning new learning paradigm demonstrated natural_language natural language processing computer_vision computer vision tasks context_learning context learning largely unexplored 3d point_cloud point cloud domain masked modeling successfully applied context 2d vision directly extending 3d point clouds remains formidablechallenge case point clouds tokens positions coordinates masked inference position embedding previous works inadvertently introduce address challenges introduce novel framework context designed especially context_learning context learning 3d inputs outputs modeled coordinates task additionally propose joint sampling module carefully designed general point sampling operator effectively resolving technical issues conduct_extensive conduct extensive experiments versatility adaptability proposed methods handling widerange tasks furthermore effective selection strategy framework surpasses results individually trained"}
{"id": "nan", "abstract": "  Recent advances in natural language processing, primarily propelled by LargeLanguage Models (LLMs), have showcased their remarkable capabilities groundedin in-context learning. A promising avenue for guiding LLMs in intricatereasoning tasks involves the utilization of intermediate reasoning steps withinthe Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge liesin the effective selection of exemplars for facilitating in-context learning.In this study, we introduce a framework that leverages Dual Queries andLow-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplarsfor in-context learning. Dual Queries first query LLM to obtain LLM-generatedknowledge such as CoT, then query the retriever to obtain the final exemplarsvia both question and the knowledge. Moreover, for the second query, LoReemploys dimensionality reduction techniques to refine exemplar selection,ensuring close alignment with the input question's knowledge. Through extensiveexperiments, we demonstrate that DQ-LoRe significantly outperforms priorstate-of-the-art methods in the automatic selection of exemplars for GPT-4,enhancing performance from 92.5% to 94.2%. Our comprehensive analysis furtherreveals that DQ-LoRe consistently outperforms retrieval-based approaches interms of both performance and adaptability, especially in scenarioscharacterized by distribution shifts. DQ-LoRe pushes the boundaries ofin-context learning and opens up new avenues for addressing complex reasoningchallenges. We will release the code soon.", "title": "dqlore dual queries with low rank approximation reranking for incontext learning", "url": "http://arxiv.org/pdf/2310.02954v4.pdf", "tokenized_text": "recent_advances recent advances natural_language natural language processing primarily propelled largelanguage_models largelanguage llms showcased remarkable_capabilities remarkable capabilities context_learning context learning promising avenue guiding llms tasks involves utilization intermediate reasoning_steps reasoning steps withinthe chain thought cot paradigm central challenge effective selection exemplars facilitating context_learning context learning study introduce framework leverages dual queries andlow rank ranking automatically select exemplarsfor context_learning context learning dual queries query llm obtain llm cot query retriever obtain final question knowledge second query reduction techniques refine exemplar selection ensuring close alignment input question knowledge extensiveexperiments demonstrate significantly_outperforms significantly outperforms art methods automatic selection exemplars performance 92.5 comprehensive analysis consistently_outperforms consistently outperforms retrieval based approaches interms performance adaptability especially distribution shifts pushes boundaries ofin context_learning context learning opens new avenues addressing complex release code soon"}
{"id": "nan", "abstract": "  The exceptional performance of pre-trained large language models hasrevolutionised various applications, but their adoption in productionenvironments is hindered by prohibitive costs and inefficiencies, particularlywhen utilising long prompts. This paper proposes OverPrompt, an in-contextlearning method aimed at improving LLM efficiency and performance by processingmultiple inputs in parallel. Evaluated across diverse datasets, OverPromptenhances task efficiency and integrates a diverse range of examples forimproved performance. Particularly, it amplifies fact-checking and sentimentanalysis tasks when supplemented with contextual information. Synthetic datagrouping further enhances performance, suggesting a viable approach for dataaugmentation.", "title": "overprompt enhancing chatgpt capabilities through an efficient incontext learning approach", "url": "http://arxiv.org/pdf/2305.14973v1.pdf", "tokenized_text": "exceptional performance pre trained large_language large language applications adoption productionenvironments hindered prohibitive costs utilising long paper_proposes paper proposes overprompt contextlearning method aimed improving llm efficiency performance inputs parallel evaluated diverse datasets task efficiency integrates diverse range examples forimproved performance particularly amplifies fact checking sentimentanalysis tasks supplemented contextual information synthetic enhances performance suggesting viable approach dataaugmentation"}
{"id": "nan", "abstract": "  The promise of Large Language Models (LLMs) in Natural Language Processinghas often been overshadowed by their limited performance in low-resourcelanguages such as Bangla. To address this, our paper presents a pioneeringapproach that utilizes cross-lingual retrieval augmented in-context learning.By strategically sourcing semantically similar prompts from high-resourcelanguage, we enable multilingual pretrained language models (MPLMs), especiallythe generative model BLOOMZ, to successfully boost performance on Bangla tasks.Our extensive evaluation highlights that the cross-lingual retrieval augmentedprompts bring steady improvements to MPLMs over the zero-shot performance.", "title": "crosslingual retrieval augmented incontext learning for bangla", "url": "http://arxiv.org/pdf/2311.00587v1.pdf", "tokenized_text": "promise large_language large language llms natural_language natural language limited performance low resourcelanguages bangla address paper_presents paper presents utilizes cross lingual retrieval_augmented retrieval augmented context_learning context learning strategically semantically similar high enable multilingual pretrained_language pretrained language especiallythe generative bloomz successfully boost performance bangla tasks extensive evaluation highlights cross lingual retrieval bring improvements zero shot performance"}
{"id": "nan", "abstract": "  Large pretrained language models (LMs) have shown impressive In-ContextLearning (ICL) ability, where the model learns to do an unseen task via aprompt consisting of input-output examples as the demonstration, without anyparameter updates. The performance of ICL is highly dominated by the quality ofthe selected in-context examples. However, previous selection methods aremostly based on simple heuristics, leading to sub-optimal performance. In thiswork, we formulate in-context example selection as a subset selection problem.We propose CEIL (Compositional Exemplars for In-context Learning), which isinstantiated by Determinantal Point Processes (DPPs) to model the interactionbetween the given input and in-context examples, and optimized through acarefully-designed contrastive learning objective to obtain preference fromLMs. We validate CEIL on 12 classification and generation datasets from 7distinct NLP tasks, including sentiment analysis, paraphrase detection, naturallanguage inference, commonsense reasoning, open-domain question answering, codegeneration, and semantic parsing. Extensive experiments demonstrate not onlythe state-of-the-art performance but also the transferability andcompositionality of CEIL, shedding new light on effective and efficientin-context learning. Our code is released athttps://github.com/HKUNLP/icl-ceil.", "title": "compositional exemplars for incontext learning", "url": "http://arxiv.org/pdf/2302.05698v3.pdf", "tokenized_text": "large pretrained_language pretrained language lms shown_impressive shown impressive contextlearning icl ability learns unseen task aprompt consisting input output examples demonstration anyparameter updates performance icl highly dominated quality ofthe selected context_examples context examples previous selection methods aremostly based simple heuristics leading sub optimal performance thiswork formulate context example selection subset selection problem propose exemplars context_learning context learning point processes given input context_examples context examples optimized designed contrastive_learning contrastive learning objective obtain preference validate 12 classification generation datasets nlp_tasks nlp tasks including sentiment_analysis sentiment analysis paraphrase detection naturallanguage inference commonsense reasoning open domain question_answering question answering codegeneration semantic_parsing semantic parsing extensive_experiments extensive experiments demonstrate state art performance transferability new light effective context_learning context learning code released"}
{"id": "nan", "abstract": "  In-context learning is a recent paradigm in natural language understanding,where a large pre-trained language model (LM) observes a test instance and afew training examples as its input, and directly decodes the output without anyupdate to its parameters. However, performance has been shown to stronglydepend on the selected training examples (termed prompt). In this work, wepropose an efficient method for retrieving prompts for in-context learningusing annotated data and a LM. Given an input-output pair, we estimate theprobability of the output given the input and a candidate training example asthe prompt, and label training examples as positive or negative based on thisprobability. We then train an efficient dense retriever from this data, whichis used to retrieve training examples as prompts at test time. We evaluate ourapproach on three sequence-to-sequence tasks where language utterances aremapped to meaning representations, and find that it substantially outperformsprior work and multiple baselines across the board.", "title": "learning to retrieve prompts for incontext learning", "url": "http://arxiv.org/pdf/2112.08633v2.pdf", "tokenized_text": "context_learning context learning recent paradigm natural_language natural language understanding large pre trained_language trained language lm observes test instance afew training_examples training examples input directly decodes output parameters performance shown selected training_examples training examples termed work wepropose efficient method retrieving context annotated_data annotated data lm given input output pair estimate output given input candidate training example asthe label training_examples training examples positive negative based train efficient dense retriever data retrieve training_examples training examples test_time test time evaluate ourapproach sequence sequence tasks language utterances meaning representations find substantially work multiple baselines board"}
{"id": "nan", "abstract": "  Due to the high costs associated with finetuning large language models,various recent works propose to adapt them to specific tasks without anyparameter updates through in-context learning. Unfortunately, for in-contextlearning there is currently no way to leverage unlabeled data, which is oftenmuch easier to obtain in large quantities than labeled examples. In this work,we therefore investigate ways to make use of unlabeled examples to improve thezero-shot performance of pretrained language models without any finetuning: Weintroduce Semantic-Oriented Unlabeled Priming (SOUP), a method that classifiesexamples by retrieving semantically similar unlabeled examples, assigninglabels to them in a zero-shot fashion, and then using them for in-contextlearning. We also propose bag-of-contexts priming, a new priming strategy thatis more suitable for our setting and enables the usage of more examples thanfit into the context window.", "title": "semanticoriented unlabeled priming for largescale language models", "url": "http://arxiv.org/pdf/2202.06133v1.pdf", "tokenized_text": "high costs associated finetuning large_language large language recent works propose adapt specific tasks anyparameter updates context_learning context learning unfortunately contextlearning currently way leverage unlabeled data easier obtain large quantities labeled examples work investigate ways use unlabeled examples improve shot performance pretrained_language pretrained language finetuning weintroduce unlabeled priming method retrieving semantically similar unlabeled examples zero shot fashion contextlearning propose bag contexts priming new priming strategy thatis suitable setting enables usage examples context window"}
{"id": "nan", "abstract": "  In-context learning has shown great success in i.i.d semantic parsing splits,where the training and test sets are drawn from the same distribution. In thissetup, models are typically prompted with demonstrations that are similar tothe input utterance. However, in the setup of compositional generalization,where models are tested on outputs with structures that are absent from thetraining set, selecting similar demonstrations is insufficient, as often noexample will be similar enough to the input. In this work, we propose a methodto select diverse demonstrations that aims to collectively cover all of thestructures required in the output program, in order to encourage the model togeneralize to new structures from these demonstrations. We empirically showthat combining diverse demonstrations with in-context learning substantiallyimproves performance across three compositional generalization semantic parsingdatasets in the pure in-context learning setup and when combined withfinetuning.", "title": "diverse demonstrations improve incontext compositional generalization", "url": "http://arxiv.org/pdf/2212.06800v3.pdf", "tokenized_text": "context_learning context learning shown great success semantic_parsing semantic parsing splits training test sets drawn distribution typically prompted demonstrations similar tothe input utterance setup compositional generalization tested outputs structures absent thetraining set selecting similar demonstrations insufficient similar input work propose methodto select diverse demonstrations aims collectively cover required output program order encourage togeneralize new structures demonstrations empirically showthat combining diverse demonstrations context_learning context learning performance compositional generalization semantic pure context_learning context learning setup combined"}
{"id": "nan", "abstract": "  Pre-trained language models (LMs) have shown remarkable reasoning performanceusing explanations (or ``chain-of-thought'' (CoT)) for in-context learning. Onthe other hand, these reasoning tasks are usually presumed to be moreapproachable for symbolic programming. To make progress towards understandingin-context learning, we curate synthetic datasets containing equivalent(natural, symbolic) data pairs, where symbolic examples contain first-orderlogic rules and predicates from knowledge bases (KBs). Then we revisitneuro-symbolic approaches and use Language Models as Logic Programmer (LMLP)that learns from demonstrations containing logic rules and correspondingexamples to iteratively reason over KBs, recovering Prolog's backward chainingalgorithm. Comprehensive experiments are included to systematically compareLMLP with CoT in deductive reasoning settings, showing that LMLP enjoys morethan 25% higher accuracy than CoT on length generalization benchmarks even withfewer parameters.", "title": "the impact of symbolic representations on incontext learning for fewshot reasoning", "url": "http://arxiv.org/pdf/2212.08686v1.pdf", "tokenized_text": "pre trained_language trained language lms shown remarkable reasoning explanations chain thought cot context_learning context learning onthe hand reasoning tasks usually symbolic programming progress context_learning context learning curate synthetic datasets containing symbolic data pairs symbolic examples contain rules predicates knowledge bases kbs symbolic approaches use language_models language logic programmer learns demonstrations containing logic rules iteratively reason kbs recovering backward comprehensive experiments included systematically cot deductive reasoning settings showing enjoys morethan 25 higher accuracy cot length generalization benchmarks parameters"}
{"id": "nan", "abstract": "  Despite the surprising few-shot performance of in-context learning (ICL), itis still a common practice to randomly sample examples to serve as context.This paper advocates a new principle for ICL: self-adaptive in-contextlearning. The self-adaption mechanism is introduced to help each sample find anin-context example permutation (i.e., selection and ordering) that can derivethe correct prediction, thus maximizing performance. To validate theeffectiveness of self-adaptive ICL, we propose a general select-then-rankframework and instantiate it with new selection and ranking algorithms. Uponextensive evaluation on eight different NLP datasets, our self-adaptive ICLmethod achieves a 40% relative improvement over the common practice setting.Further analysis reveals the enormous potential of self-adaptive ICL that itmight be able to close the gap between ICL and finetuning given more advancedalgorithms. Our code is released to facilitate future research in this area:https://github.com/Shark-NLP/self-adaptive-ICL", "title": "selfadaptive incontext learning an information compression perspective for incontext example selection and ordering", "url": "http://arxiv.org/pdf/2212.10375v2.pdf", "tokenized_text": "despite surprising shot performance context_learning context learning icl itis common practice randomly sample examples serve context paper advocates new principle icl self adaptive contextlearning self adaption mechanism introduced help sample find anin context example permutation i.e. selection ordering correct prediction maximizing performance validate theeffectiveness self adaptive icl propose general select instantiate new selection ranking algorithms evaluation different nlp datasets self adaptive iclmethod achieves 40 relative improvement common practice setting analysis reveals enormous potential self adaptive icl able close gap icl finetuning given code released facilitate future_research future research area https://github.com nlp self adaptive icl"}
{"id": "nan", "abstract": "  In-context learning (ICL) is an important capability of Large Language Models(LLMs), enabling these models to dynamically adapt based on specific,in-context exemplars, thereby improving accuracy and relevance. However, LLM'sresponses may leak the sensitive private information contained in in-contextexemplars. To address this challenge, we propose Differentially PrivateIn-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. Thekey idea for DP-ICL paradigm is generating differentially private responsesthrough a noisy consensus among an ensemble of LLM's responses based ondisjoint exemplar sets. Based on the general paradigm of DP-ICL, we instantiateseveral techniques showing how to privatize ICL for text classification andlanguage generation. We evaluate DP-ICL on four text classification benchmarksand two language generation tasks, and our empirical results show that DP-ICLachieves a strong utility-privacy tradeoff.", "title": "privacypreserving incontext learning for large language models", "url": "http://arxiv.org/pdf/2305.01639v2.pdf", "tokenized_text": "context_learning context learning icl important capability large_language large language models(llms enabling dynamically adapt based specific context exemplars improving accuracy relevance sensitive private information contained contextexemplars address challenge propose context_learning context learning dp icl general paradigm icl tasks idea dp icl paradigm generating private noisy consensus ensemble llm responses based exemplar sets based general paradigm dp icl techniques showing icl text_classification text classification andlanguage generation evaluate dp icl text_classification text classification language generation tasks empirical results dp strong utility privacy tradeoff"}
{"id": "nan", "abstract": "  The phenomena of in-context learning has typically been thought of as\"learning from examples\". In this work which focuses on Machine Translation, wepresent a perspective of in-context learning as the desired generation taskmaintaining coherency with its context, i.e., the prompt examples. We firstinvestigate randomly sampled prompts across 4 domains, and find thattranslation performance improves when shown in-domain prompts. Next, weinvestigate coherency for the in-domain setting, which uses prompt examplesfrom a moving window. We study this with respect to other factors that havepreviously been identified in the literature such as length, surface similarityand sentence embedding similarity. Our results across 3 models (GPTNeo2.7B,Bloom3B, XGLM2.9B), and three translation directions(\\texttt{en}$\\rightarrow$\\{\\texttt{pt, de, fr}\\}) suggest that the long-termcoherency of the prompts and the test sentence is a good indicator ofdownstream translation performance. In doing so, we demonstrate the efficacy ofIn-context Machine Translation for on-the-fly adaptation.", "title": "incontext learning as maintaining coherency a study of onthefly machine translation using large language models", "url": "http://arxiv.org/pdf/2305.03573v1.pdf", "tokenized_text": "phenomena context_learning context learning typically thought examples work focuses machine_translation machine translation wepresent perspective context_learning context learning desired generation context i.e. examples randomly sampled domains find performance improves shown domain weinvestigate domain setting uses moving window study respect factors identified literature length surface sentence embedding similarity results translation de suggest long test sentence good indicator ofdownstream translation performance demonstrate efficacy ofin context machine_translation machine translation fly adaptation"}
{"id": "nan", "abstract": "  Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but theirweights are often publicly unavailable and their immense sizes make the modelsdifficult to be tuned with common hardware. As a result, effectively tuningthese models with large-scale supervised data can be challenging. As analternative, In-Context Learning (ICL) can only use a small number ofsupervised examples due to context length limits. In this paper, we proposeSuper In-Context Learning (SuperICL) which allows black-box LLMs to work withlocally fine-tuned smaller models, resulting in superior performance onsupervised tasks. Our experiments demonstrate that SuperICL can improveperformance beyond state-of-the-art fine-tuned models while addressing theinstability problem of in-context learning. Furthermore, SuperICL can enhancethe capabilities of smaller models, such as multilinguality andinterpretability.", "title": "small models are valuable plugins for large language models", "url": "http://arxiv.org/pdf/2305.08848v1.pdf", "tokenized_text": "large_language large language llms gpt-3 gpt-4 powerful publicly unavailable immense sizes tuned common hardware result effectively large scale supervised data challenging analternative context_learning context learning icl use small_number small number ofsupervised examples context length limits paper context_learning context learning allows black box llms work fine tuned smaller resulting superior_performance superior performance tasks experiments_demonstrate experiments demonstrate improveperformance state art fine tuned addressing problem context_learning context learning furthermore enhancethe capabilities smaller multilinguality andinterpretability"}
{"id": "nan", "abstract": "  Relation extraction (RE) is a crucial task in natural language processing(NLP) that aims to identify and classify relationships between entitiesmentioned in text. In the financial domain, relation extraction plays a vitalrole in extracting valuable information from financial documents, such as newsarticles, earnings reports, and company filings. This paper describes oursolution to relation extraction on one such dataset REFinD. The dataset wasreleased along with shared task as a part of the Fourth Workshop on KnowledgeDiscovery from Unstructured Data in Financial Services, co-located with SIGIR2023. In this paper, we employed OpenAI models under the framework ofin-context learning (ICL). We utilized two retrieval strategies to find top Krelevant in-context learning demonstrations / examples from training data for agiven test example. The first retrieval mechanism, we employed, is alearning-free dense retriever and the other system is a learning-basedretriever. We were able to achieve 3rd rank overall. Our best F1-score is0.718.", "title": "gptfinre incontext learning for financial relation extraction using large language models", "url": "http://arxiv.org/pdf/2306.17519v2.pdf", "tokenized_text": "relation_extraction relation extraction crucial task natural_language natural language processing(nlp aims identify classify relationships text financial domain relation_extraction relation extraction plays extracting valuable information financial documents reports company paper describes relation_extraction relation extraction dataset dataset shared task fourth workshop unstructured data financial services co paper employed openai framework ofin context_learning context learning icl utilized retrieval strategies find context_learning context learning demonstrations examples training_data training data agiven test example retrieval mechanism employed free dense retriever system learning able achieve rank overall best f1 score"}
{"id": "nan", "abstract": "  Current methods for Knowledge-Based Question Answering (KBQA) usually rely oncomplex training techniques and model frameworks, leading to many limitationsin practical applications. Recently, the emergence of In-Context Learning (ICL)capabilities in Large Language Models (LLMs) provides a simple andtraining-free semantic parsing paradigm for KBQA: Given a small number ofquestions and their labeled logical forms as demo examples, LLMs can understandthe task intent and generate the logic form for a new question. However,current powerful LLMs have little exposure to logic forms during pre-training,resulting in a high format error rate. To solve this problem, we propose acode-style in-context learning method for KBQA, which converts the generationprocess of unfamiliar logical form into the more familiar code generationprocess for LLMs. Experimental results on three mainstream datasets show thatour method dramatically mitigated the formatting error problem in generatinglogic forms while realizing a new SOTA on WebQSP, GrailQA, and GraphQ under thefew-shot setting.", "title": "codestyle incontext learning for knowledgebased question answering", "url": "http://arxiv.org/pdf/2309.04695v1.pdf", "tokenized_text": "current methods knowledge based question_answering question answering kbqa usually rely training techniques frameworks leading practical applications recently emergence context_learning context learning large_language large language llms provides simple andtraining free semantic_parsing semantic parsing paradigm kbqa given small_number small number labeled logical forms demo examples llms understandthe task intent generate logic form new question current powerful llms little exposure logic forms pre training resulting high format error rate solve problem propose acode style context_learning context learning method kbqa converts generationprocess unfamiliar logical form familiar code generationprocess llms experimental_results experimental results mainstream datasets thatour method dramatically mitigated formatting error problem forms realizing new sota webqsp grailqa thefew shot_setting shot setting"}
{"id": "nan", "abstract": "  While state-of-the-art language models excel at the style transfer task,current work does not address explainability of style transfer systems.Explanations could be generated using large language models such as GPT-3.5 andGPT-4, but the use of such complex systems is inefficient when smaller, widelydistributed, and transparent alternatives are available. We propose a frameworkto augment and improve a formality style transfer dataset with explanations viamodel distillation from ChatGPT. To further refine the generated explanations,we propose a novel way to incorporate scarce expert human feedback usingin-context learning (ICLEF: In-Context Learning from Expert Feedback) byprompting ChatGPT to act as a critic to its own outputs. We use the resultingdataset of 9,960 explainable formality style transfer instances (e-GYAFC) toshow that current openly distributed instruction-tuned models (and, in somesettings, ChatGPT) perform poorly on the task, and that fine-tuning on ourhigh-quality dataset leads to significant improvements as shown by automaticevaluation. In human evaluation, we show that models much smaller than ChatGPTfine-tuned on our data align better with expert preferences. Finally, wediscuss two potential applications of models fine-tuned on the explainablestyle transfer task: interpretable authorship verification and interpretableadversarial attacks on AI-generated text detectors.", "title": "iclef incontext learning with expert feedback for explainable style transfer", "url": "http://arxiv.org/pdf/2309.08583v1.pdf", "tokenized_text": "state art language_models language excel style_transfer style transfer task current work address explainability style_transfer style transfer systems explanations generated large_language large language gpt-3.5 use complex systems inefficient smaller transparent alternatives available propose frameworkto augment improve formality style_transfer style transfer dataset explanations distillation chatgpt refine generated explanations propose_a_novel propose novel way incorporate scarce expert human feedback usingin context_learning context learning context_learning context learning expert feedback chatgpt act critic outputs use explainable formality style_transfer style transfer instances toshow current distributed instruction tuned chatgpt perform poorly task fine tuning quality dataset leads significant improvements shown automaticevaluation human evaluation smaller tuned data align better expert preferences finally wediscuss potential applications fine tuned transfer task interpretable authorship verification attacks ai generated text detectors"}
{"id": "nan", "abstract": "  In support of open and reproducible research, there has been a rapidlyincreasing number of datasets made available for research. As the availabilityof datasets increases, it becomes more important to have quality metadata fordiscovering and reusing them. Yet, it is a common issue that datasets oftenlack quality metadata due to limited resources for data curation. Meanwhile,technologies such as artificial intelligence and large language models (LLMs)are progressing rapidly. Recently, systems based on these technologies, such asChatGPT, have demonstrated promising capabilities for certain data curationtasks. This paper proposes to leverage LLMs for cost-effective annotation ofsubject metadata through the LLM-based in-context learning. Our method employsGPT-3.5 with prompts designed for annotating subject metadata, demonstratingpromising performance in automatic metadata annotation. However, models basedon in-context learning cannot acquire discipline-specific rules, resulting inlower performance in several categories. This limitation arises from thelimited contextual information available for subject inference. To the best ofour knowledge, we are introducing, for the first time, an in-context learningmethod that harnesses large language models for automated subject metadataannotation.", "title": "utilising a large language model to annotate subject metadata a case study in an australian national research data catalogue", "url": "http://arxiv.org/pdf/2310.11318v1.pdf", "tokenized_text": "support open reproducible research number datasets available research datasets increases important quality metadata common issue datasets quality metadata limited resources data curation technologies artificial_intelligence artificial intelligence large_language large language rapidly recently systems based technologies demonstrated promising capabilities certain data paper_proposes paper proposes leverage llms cost effective annotation metadata llm based context_learning context learning method designed annotating subject metadata performance automatic metadata annotation basedon context_learning context learning acquire discipline specific rules resulting performance categories limitation arises thelimited contextual information available subject inference best ofour knowledge introducing time context learningmethod harnesses large_language large language automated subject"}
{"id": "nan", "abstract": "  In-context learning (ICL) ability has emerged with the increasing scale oflarge language models (LLMs), enabling them to learn input-label mappings fromdemonstrations and perform well on downstream tasks. However, under thestandard ICL setting, LLMs may sometimes neglect query-related information indemonstrations, leading to incorrect predictions. To address this limitation,we propose a new paradigm called Hint-enhanced In-Context Learning (HICL) toexplore the power of ICL in open-domain question answering, an important formin knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extractquery-related knowledge from demonstrations, then concatenates the knowledge toprompt LLMs in a more explicit way. Furthermore, we track the source of thisknowledge to identify specific examples, and introduce a Hint-related ExampleRetriever (HER) to select informative examples for enhanced demonstrations. Weevaluate HICL with HER on 3 open-domain QA benchmarks, and observe averageperformance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo, 7.62 EMscore and 7.27 F1 score on LLaMA-2-Chat-7B compared with standard setting.", "title": "hintenhanced incontext learning wakes large language models up for knowledgeintensive tasks", "url": "http://arxiv.org/pdf/2311.01949v1.pdf", "tokenized_text": "context_learning context learning icl ability emerged increasing scale oflarge language_models language llms enabling learn input label mappings fromdemonstrations perform downstream_tasks downstream tasks icl setting llms neglect query related information indemonstrations leading incorrect predictions address limitation propose_a_new propose new paradigm called hint enhanced context_learning context learning hicl toexplore power icl open domain question_answering question answering important knowledge intensive tasks hicl leverages llms reasoning ability related knowledge demonstrations concatenates knowledge toprompt llms explicit way furthermore track source identify specific examples introduce hint related exampleretriever select informative examples enhanced demonstrations weevaluate hicl open domain qa benchmarks observe averageperformance gains em score 2.52 f1_score f1 score gpt-3.5 turbo 7.27 f1_score f1 score llama-2 compared standard setting"}
{"id": "nan", "abstract": "  Large language models (LMs) are able to in-context learn -- perform a newtask via inference alone by conditioning on a few input-label pairs(demonstrations) and making predictions for new inputs. However, there has beenlittle understanding of how the model learns and which aspects of thedemonstrations contribute to end task performance. In this paper, we show thatground truth demonstrations are in fact not required -- randomly replacinglabels in the demonstrations barely hurts performance on a range ofclassification and multi-choce tasks, consistently over 12 different modelsincluding GPT-3. Instead, we find that other aspects of the demonstrations arethe key drivers of end task performance, including the fact that they provide afew examples of (1) the label space, (2) the distribution of the input text,and (3) the overall format of the sequence. Together, our analysis provides anew way of understanding how and why in-context learning works, while openingup new questions about how much can be learned from large language modelsthrough inference alone.", "title": "rethinking the role of demonstrations what makes incontext learning work", "url": "http://arxiv.org/pdf/2202.12837v2.pdf", "tokenized_text": "large_language large language lms able context learn perform inference conditioning input label making predictions new inputs understanding learns aspects contribute end task performance paper truth demonstrations fact required randomly demonstrations barely hurts performance range multi tasks consistently 12 different gpt-3 instead find aspects demonstrations key drivers end task performance including fact provide afew examples label space distribution input text overall format sequence analysis provides anew way understanding context_learning context learning works new questions learned large_language large language inference"}
{"id": "nan", "abstract": "  Anaphora resolution is an important task for information extraction across arange of languages, text genres, and domains, motivating the need for methodsthat do not require large annotated datasets. In-context learning has emergedas a promising approach, yet there are a number of challenges in applyingin-context learning to resolve anaphora. For example, encoding a singlein-context demonstration that consists of: an anaphor, a paragraph-lengthcontext, and a list of corresponding antecedents, requires conditioning alanguage model on a long sequence of tokens, limiting the number ofdemonstrations per prompt. In this paper, we present MICE (Mixtures ofIn-Context Experts), which we demonstrate is effective for few-shot anaphoraresolution in scientific protocols (Tamari et al., 2021). Given only a handfulof training examples, MICE combines the predictions of hundreds of in-contextexperts, yielding a 30% increase in F1 score over a competitive promptretrieval baseline. Furthermore, we show MICE can be used to train compactstudent models without sacrificing performance. As far as we are aware, this isthe first work to present experimental results demonstrating the effectivenessof in-context learning on the task of few-shot anaphora resolution inscientific protocols.", "title": "fewshot anaphora resolution in scientific protocols via mixtures of incontext experts", "url": "http://arxiv.org/pdf/2210.03690v2.pdf", "tokenized_text": "resolution important task information_extraction information extraction arange languages text genres domains motivating need require large annotated datasets context_learning context learning emergedas promising approach number challenges context_learning context learning resolve example encoding context demonstration consists paragraph list corresponding requires conditioning alanguage long sequence tokens limiting number ofdemonstrations paper present mixtures ofin context experts demonstrate effective shot scientific protocols et_al et al 2021 given training_examples training examples combines predictions hundreds yielding 30 increase f1_score f1 score competitive baseline furthermore train sacrificing performance far aware isthe work present experimental_results experimental results demonstrating context_learning context learning task shot resolution inscientific protocols"}
{"id": "nan", "abstract": "  Consistency is a key requirement of high-quality translation. It isespecially important to adhere to pre-approved terminology and adapt tocorrected translations in domain-specific projects. Machine translation (MT)has achieved significant progress in the area of domain adaptation. However,real-time adaptation remains challenging. Large-scale language models (LLMs)have recently shown interesting capabilities of in-context learning, where theylearn to replicate certain input-output text generation patterns, withoutfurther fine-tuning. By feeding an LLM at inference time with a prompt thatconsists of a list of translation pairs, it can then simulate the domain andstyle characteristics. This work aims to investigate how we can utilizein-context learning to improve real-time adaptive MT. Our extensive experimentsshow promising results at translation time. For example, LLMs can adapt to aset of in-domain sentence pairs and/or terminology while translating a newsentence. We observe that the translation quality with few-shot in-contextlearning can surpass that of strong encoder-decoder MT systems, especially forhigh-resource languages. Moreover, we investigate whether we can combine MTfrom strong encoder-decoder models with fuzzy matches, which can furtherimprove translation quality, especially for less supported languages. Weconduct our experiments across five diverse language pairs, namelyEnglish-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French(EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).", "title": "adaptive machine translation with large language models", "url": "http://arxiv.org/pdf/2301.13294v3.pdf", "tokenized_text": "consistency key requirement high quality translation isespecially important adhere pre approved terminology adapt translations domain specific projects machine_translation machine translation achieved significant progress area domain adaptation real time adaptation remains challenging large scale language_models language llms)have recently shown interesting capabilities context_learning context learning replicate certain input output text generation patterns withoutfurther fine tuning feeding llm inference time list translation pairs simulate domain characteristics work aims investigate context_learning context learning improve real time adaptive mt extensive experimentsshow promising_results promising results translation time example llms adapt aset domain sentence pairs and/or terminology translating observe translation quality shot contextlearning surpass strong encoder decoder mt_systems mt systems especially resource_languages resource languages investigate combine strong encoder decoder matches furtherimprove translation quality especially supported languages weconduct experiments diverse language pairs arabic english chinese english english english spanish"}
{"id": "nan", "abstract": "  The in-context learning capabilities of LLMs like GPT-3 allow annotators tocustomize an LLM to their specific tasks with a small number of examples.However, users tend to include only the most obvious patterns when craftingexamples, resulting in underspecified in-context functions that fall short onunseen cases. Further, it is hard to know when \"enough\" examples have beenincluded even for known patterns. In this work, we present ScatterShot, aninteractive system for building high-quality demonstration sets for in-contextlearning. ScatterShot iteratively slices unlabeled data into task-specificpatterns, samples informative inputs from underexplored or not-yet-saturatedslices in an active learning manner, and helps users label more efficientlywith the help of an LLM and the current example set. In simulation studies ontwo text perturbation scenarios, ScatterShot sampling improves the resultingfew-shot functions by 4-5 percentage points over random sampling, with lessvariance as more examples are added. In a user study, ScatterShot greatly helpsusers in covering different patterns in the input space and labeling in-contextexamples more efficiently, resulting in better in-context learning and lessuser effort.", "title": "scattershot interactive incontext example curation for text transformation", "url": "http://arxiv.org/pdf/2302.07346v1.pdf", "tokenized_text": "context_learning context learning capabilities llms like gpt-3 allow annotators llm specific tasks small_number small number examples users tend include obvious patterns resulting underspecified context functions fall short onunseen cases hard know examples known patterns work present system building high quality demonstration sets contextlearning iteratively slices unlabeled data task samples informative inputs underexplored active learning manner helps users label help llm current example set simulation studies ontwo text perturbation scenarios sampling improves shot functions percentage points random sampling examples added user study greatly covering different patterns input space labeling contextexamples efficiently resulting better context_learning context learning effort"}
{"id": "nan", "abstract": "  Despite the rapid recent progress in creating accurate and compact in-contextlearners, most recent work focuses on in-context learning (ICL) for tasks inEnglish. However, the ability to interact with users of languages outsideEnglish presents a great potential for broadening the applicability of languagetechnologies to non-English speakers.  In this work, we collect the infrastructure necessary for training andevaluation of ICL in a selection of Slavic languages: Czech, Polish, andRussian. We link a diverse set of datasets and cast these into a unifiedinstructional format through a set of transformations and newly-craftedtemplates written purely in target languages. Using the newly-curated dataset,we evaluate a set of the most recent in-context learners and compare theirresults to the supervised baselines. Finally, we train, evaluate and publish aset of in-context learning models that we train on the collected resources andcompare their performance to previous work.  We find that ICL models tuned in English are also able to learn some tasksfrom non-English contexts, but multilingual instruction fine-tuningconsistently improves the ICL ability. We also find that the massive multitasktraining can be outperformed by single-task training in the target language,uncovering the potential for specializing in-context learners to thelanguage(s) of their application.", "title": "resources and fewshot learners for incontext learning in slavic languages", "url": "http://arxiv.org/pdf/2304.01922v1.pdf", "tokenized_text": "despite rapid recent progress creating accurate compact contextlearners recent_work recent work focuses context_learning context learning icl tasks inenglish ability interact users languages presents great_potential great potential applicability languagetechnologies non english speakers work collect infrastructure necessary training icl selection languages polish link diverse set datasets cast format set transformations newly written purely target languages newly curated dataset evaluate set recent context learners compare supervised baselines finally train evaluate publish aset context_learning context learning train collected resources andcompare performance previous work find icl tuned english able learn tasksfrom non english contexts multilingual instruction fine improves icl ability find massive outperformed single task training target language uncovering potential specializing context learners application"}
{"id": "nan", "abstract": "  In-context learning is a new learning paradigm where a language modelconditions on a few input-output pairs (demonstrations) and a test input, anddirectly outputs the prediction. It has been shown highly dependent on theprovided demonstrations and thus promotes the research of demonstrationretrieval: given a test input, relevant examples are retrieved from thetraining set to serve as informative demonstrations for in-context learning.While previous works focus on training task-specific retrievers for severaltasks separately, these methods are often hard to transfer and scale on varioustasks, and separately trained retrievers incur a lot of parameter storage anddeployment cost. In this paper, we propose Unified Demonstration Retriever(\\textbf{UDR}), a single model to retrieve demonstrations for a wide range oftasks. To train UDR, we cast various tasks' training signals into a unifiedlist-wise ranking formulation by language model's feedback. Then we propose amulti-task list-wise ranking training framework, with an iterative miningstrategy to find high-quality candidates, which can help UDR fully incorporatevarious tasks' signals. Experiments on 30+ tasks across 13 task families andmultiple data domains show that UDR significantly outperforms baselines.Further analyses show the effectiveness of each proposed component and UDR'sstrong ability in various scenarios including different LMs (1.3B - 175B),unseen datasets, varying demonstration quantities, etc.", "title": "unified demonstration retriever for incontext learning", "url": "http://arxiv.org/pdf/2305.04320v2.pdf", "tokenized_text": "context_learning context learning new learning paradigm language input output pairs demonstrations test input anddirectly outputs prediction shown highly dependent demonstrations promotes research given test input relevant examples retrieved thetraining set serve informative demonstrations context_learning context learning previous works focus training task specific retrievers separately methods hard transfer scale varioustasks separately trained retrievers incur lot parameter storage anddeployment cost paper propose unified demonstration single retrieve demonstrations wide_range wide range oftasks train cast tasks training signals wise ranking formulation language_model language feedback propose amulti task list wise ranking training framework iterative find high quality candidates help fully tasks signals experiments 30 tasks 13 task families data domains significantly_outperforms significantly outperforms baselines analyses effectiveness proposed component ability scenarios including different lms 1.3b datasets varying demonstration quantities etc"}
{"id": "nan", "abstract": "  The primary way of building AI applications is shifting from trainingspecialist models to prompting generalist models. A common practice forprompting generalist models, often referred to as in-context learning, is toappend a few examples (demonstrations) to the prompt to help the model betterunderstand the task. While effective, in-context learning can be inefficientbecause it makes the input prompt much longer, consuming valuable space in thecontext window and leading to larger computational costs. In this paper, wepropose DynaICL, a recipe for efficient prompting with black-box generalistmodels that dynamically allocate in-context examples according to the inputcomplexity and the computational budget. To achieve this, we train a metacontroller that predicts the number of in-context examples suitable for thegeneralist model to make a good prediction based on the performance-efficiencytrade-off for a specific input. We then dynamically allocate the number ofdemonstrations for an input according to predictions from the meta controllerand the given computation budget. Experimental results show that dynamicexample allocation helps achieve a better performance-efficiency trade-off intwo practical settings where computational resources or the requiredperformance is constrained. Specifically, DynaICL saves up to 46% token budgetcompared to the common practice that allocates the same number of in-contextexamples to each input. We also find that a meta controller trained on acertain backbone model and tasks can successfully generalize to unseen modelsand tasks.", "title": "efficient prompting via dynamic incontext learning", "url": "http://arxiv.org/pdf/2305.11170v1.pdf", "tokenized_text": "primary way building ai applications shifting generalist common practice forprompting generalist referred context_learning context learning examples demonstrations help betterunderstand task effective context_learning context learning makes input longer consuming valuable space thecontext window leading larger computational costs paper wepropose recipe efficient black box dynamically context_examples context examples according computational budget achieve train predicts number context_examples context examples suitable good prediction based performance specific input dynamically number ofdemonstrations input according predictions meta given computation budget experimental_results experimental results allocation helps achieve better performance efficiency trade practical settings computational resources constrained specifically saves 46 token common practice number contextexamples input find meta controller trained backbone tasks successfully generalize unseen modelsand tasks"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable capabilities inperforming complex tasks. Moreover, recent research has shown thatincorporating human-annotated rationales (e.g., Chain-of-Thought prompting)during in-context learning can significantly enhance the performance of thesemodels, particularly on tasks that require reasoning capabilities. However,incorporating such rationales poses challenges in terms of scalability as thisrequires a high degree of human involvement. In this work, we present a novelframework, Amplifying Model Performance by Leveraging In-Context Learning withPost Hoc Explanations (AMPLIFY), which addresses the aforementioned challengesby automating the process of rationale generation. To this end, we leveragepost hoc explanation methods which output attribution scores (explanations)capturing the influence of each of the input features on model predictions.More specifically, we construct automated natural language rationales thatembed insights from post hoc explanations to provide corrective signals toLLMs. Extensive experimentation with real-world datasets demonstrates that ourframework, AMPLIFY, leads to prediction accuracy improvements of about 10-25%over a wide range of tasks, including those where prior approaches which relyon human-annotated rationales such as Chain-of-Thought prompting fall short.Our work makes one of the first attempts at highlighting the potential of posthoc explanations as valuable tools for enhancing the effectiveness of LLMs.Furthermore, we conduct additional empirical analyses and ablation studies todemonstrate the impact of each of the components of AMPLIFY, which, in turn,leads to critical insights for refining in-context learning.", "title": "post hoc explanations of language models can improve language models", "url": "http://arxiv.org/pdf/2305.11426v2.pdf", "tokenized_text": "large_language large language llms demonstrated_remarkable demonstrated remarkable capabilities complex tasks recent research shown human annotated rationales e.g. chain thought context_learning context learning significantly enhance performance thesemodels particularly tasks require reasoning capabilities incorporating rationales poses challenges terms scalability high degree human involvement work present novelframework performance leveraging context_learning context learning hoc explanations amplify addresses aforementioned automating process rationale generation end hoc explanation methods output attribution scores influence input features predictions specifically construct automated natural_language natural language rationales insights post hoc explanations provide signals extensive experimentation real world datasets demonstrates ourframework amplify leads prediction accuracy improvements 10 wide_range wide range tasks including prior approaches relyon human annotated rationales chain thought_prompting thought fall short work makes attempts highlighting potential explanations valuable tools enhancing effectiveness llms furthermore conduct additional empirical analyses ablation studies todemonstrate impact components amplify turn leads critical insights refining context_learning context learning"}
{"id": "nan", "abstract": "  Many recent developments in large language models focus on prompting them toperform specific tasks. One effective prompting method is in-context learning,where the model performs a (possibly new) generation/prediction task given one(or more) examples. Past work has shown that the choice of examples can make alarge impact on task performance. However, finding good examples is notstraightforward since the definition of a representative group of examples canvary greatly depending on the task. While there are many existing methods forselecting in-context examples, they generally score examples independently,ignoring the dependency between them and the order in which they are providedto the large language model. In this work, we propose Retrieval for In-ContextLearning (RetICL), a learnable method for modeling and optimally selectingexamples sequentially for in-context learning. We frame the problem ofsequential example selection as a Markov decision process, design an exampleretriever model using an LSTM, and train it using proximal policy optimization(PPO). We validate RetICL on math problem solving datasets and show that itoutperforms both heuristic and learnable baselines, and achievesstate-of-the-art accuracy on the TabMWP dataset. We also use case studies toshow that RetICL implicitly learns representations of math problem solvingstrategies.", "title": "reticl sequential retrieval of incontext examples with reinforcement learning", "url": "http://arxiv.org/pdf/2305.14502v1.pdf", "tokenized_text": "recent developments large_language large language focus toperform specific tasks effective method context_learning context learning performs possibly new generation prediction task given examples past work shown choice examples alarge impact task performance finding good examples definition representative group examples canvary greatly depending task existing_methods existing methods forselecting context_examples context examples generally score examples independently ignoring dependency order large_language large language work propose retrieval contextlearning learnable method modeling optimally sequentially context_learning context learning frame problem example selection markov decision process design exampleretriever lstm train policy validate math problem solving datasets itoutperforms heuristic learnable baselines achievesstate art accuracy dataset use case studies toshow implicitly learns representations math problem"}
{"id": "nan", "abstract": "  In-context learning (ICL) for large language models has proven to be apowerful approach for many natural language processing tasks. However,determining the best method to select examples for ICL is nontrivial as theresults can vary greatly depending on the quality, quantity, and order ofexamples used. In this paper, we conduct a case study on text simplification(TS) to investigate how to select the best and most robust examples for ICL. Wepropose Metric-Based in-context Learning (MBL) method that utilizes commonlyused TS metrics such as SARI, compression ratio, and BERT-Precision forselection. Through an extensive set of experiments with various-sized GPTmodels on standard TS benchmarks such as TurkCorpus and ASSET, we show thatexamples selected by the top SARI scores perform the best on larger models suchas GPT-175B, while the compression ratio generally performs better on smallermodels such as GPT-13B and GPT-6.7B. Furthermore, we demonstrate that MBL isgenerally robust to example orderings and out-of-domain test sets, andoutperforms strong baselines and state-of-the-art finetuned language models.Finally, we show that the behaviour of large GPT models can be implicitlycontrolled by the chosen metric. Our research provides a new framework forselecting examples in ICL, and demonstrates its effectiveness in textsimplification tasks, breaking new ground for more accurate and efficient NLGsystems.", "title": "metricbased incontext learning a case study in text simplification", "url": "http://arxiv.org/pdf/2307.14632v1.pdf", "tokenized_text": "context_learning context learning icl large_language large language proven apowerful approach natural_language natural language processing tasks determining best method select examples icl nontrivial theresults vary greatly depending quality quantity order ofexamples paper conduct case study text investigate select best robust examples icl wepropose context_learning context learning method utilizes metrics compression ratio bert precision forselection extensive set experiments sized standard benchmarks asset selected scores perform best larger suchas compression ratio generally performs better furthermore demonstrate robust example domain test sets andoutperforms strong baselines state art finetuned language_models language finally behaviour large gpt chosen metric research provides new framework forselecting examples icl demonstrates effectiveness tasks breaking new ground accurate efficient"}
{"id": "nan", "abstract": "  Natural language understanding (NLU) is integral to various social mediaapplications. However, existing NLU models rely heavily on context for semanticlearning, resulting in compromised performance when faced with short and noisysocial media content. To address this issue, we leverage in-context learning(ICL), wherein language models learn to make inferences by conditioning on ahandful of demonstrations to enrich the context and propose a novelhashtag-driven in-context learning (HICL) framework. Concretely, we pre-train amodel #Encoder, which employs #hashtags (user-annotated topic labels) to driveBERT-based pre-training through contrastive learning. Our objective here is toenable #Encoder to gain the ability to incorporate topic-related semanticinformation, which allows it to retrieve topic-related posts to enrich contextsand enhance social media NLU with noisy contexts. To further integrate theretrieved context with the source text, we employ a gradient-based method toidentify trigger terms useful in fusing information from both sources. Forempirical studies, we collected 45M tweets to set up an in-context NLUbenchmark, and the experimental results on seven downstream tasks show thatHICL substantially advances the previous state-of-the-art results. Furthermore,we conducted extensive analyzes and found that: (1) combining source input witha top-retrieved post from #Encoder is more effective than using semanticallysimilar posts; (2) trigger words can largely benefit in merging context fromthe source and retrieved posts.", "title": "hicl hashtagdriven incontext learning for social media natural language understanding", "url": "http://arxiv.org/pdf/2308.09985v1.pdf", "tokenized_text": "natural_language natural language understanding nlu integral social existing nlu rely heavily context resulting performance faced short media content address issue leverage context learning(icl language_models language learn inferences conditioning ahandful demonstrations enrich context propose driven context_learning context learning hicl framework concretely pre train amodel encoder employs hashtags user annotated topic labels based pre training contrastive_learning contrastive learning objective encoder gain ability incorporate topic related semanticinformation allows retrieve topic related posts enrich enhance social_media social media nlu noisy contexts integrate context source text employ gradient based method toidentify trigger terms useful fusing information sources studies collected 45 tweets set context experimental_results experimental results seven downstream_tasks downstream tasks substantially advances previous state art results furthermore conducted extensive analyzes found combining source input witha retrieved post encoder effective posts trigger words largely benefit merging context fromthe source retrieved posts"}
{"id": "nan", "abstract": "  Transformers have recently revolutionized many domains in modern machinelearning and one salient discovery is their remarkable in-context learningcapability, where models can solve an unseen task by utilizing task-specificprompts without further parameters fine-tuning. This also inspired recenttheoretical studies aiming to understand the in-context learning mechanism oftransformers, which however focused only on linear transformers. In this work,we take the first step toward studying the learning dynamics of a one-layertransformer with softmax attention trained via gradient descent in order toin-context learn linear function classes. We consider a structured data model,where each token is randomly sampled from a set of feature vectors in eitherbalanced or imbalanced fashion. For data with balanced features, we establishthe finite-time convergence guarantee with near-zero prediction error bynavigating our analysis over two phases of the training dynamics of theattention map. More notably, for data with imbalanced features, we show thatthe learning dynamics take a stage-wise convergence process, where thetransformer first converges to a near-zero prediction error for the querytokens of dominant features, and then converges later to a near-zero predictionerror for the query tokens of under-represented features, respectively via oneand four training phases. Our proof features new techniques for analyzing thecompeting strengths of two types of attention weights, the change of whichdetermines different training phases.", "title": "incontext convergence of transformers", "url": "http://arxiv.org/pdf/2310.05249v1.pdf", "tokenized_text": "transformers recently revolutionized domains modern machinelearning salient discovery remarkable context learningcapability solve unseen task utilizing task specificprompts parameters fine tuning inspired studies aiming understand context_learning context learning mechanism oftransformers focused linear transformers work step studying learning dynamics attention trained gradient descent order toin context learn linear function classes consider structured data token randomly sampled set feature vectors imbalanced fashion data balanced features finite time convergence guarantee near zero prediction error analysis phases training dynamics theattention map notably data imbalanced features thatthe learning dynamics stage wise convergence process thetransformer converges near zero prediction error dominant features converges later near zero query tokens represented features respectively training phases proof features new techniques analyzing strengths types attention weights change different training phases"}
{"id": "nan", "abstract": "  Large language models (LLMs) have shown impressive in-context learning (ICL)ability in code generation. LLMs take a prompt consisting of requirement-codeexamples and a new requirement as input, and output new programs. Existingstudies have found that ICL is highly dominated by the examples and thus arisesresearch on example selection. However, existing approaches randomly selectexamples or only consider the textual similarity of requirements to retrieve,leading to sub-optimal performance. In this paper, we propose a novellearning-based selection approach named LAIL (LLM-Aware In-context Learning)for code generation. Given a candidate example, we exploit LLMs themselves toestimate it by considering the generation probabilities of ground-truthprograms given a requirement and the example. We then label candidate examplesas positive or negative through the probability feedback. Based on the labeleddata, we import a contrastive learning objective to train an effectiveretriever that acquires the preference of LLMs in code generation. We applyLAIL to three LLMs and evaluate it on three representative datasets (e.g.,MBJP, MBPP, and MBCPP). LATA outperforms the state-of-the-art baselines by11.58%, 6.89%, and 5.07% on CodeGen, and 4.38%, 2.85%, and 2.74% on GPT-3.5 interms of Pass@1, respectively.", "title": "large language modelaware incontext learning for code generation", "url": "http://arxiv.org/pdf/2310.09748v1.pdf", "tokenized_text": "large_language large language llms shown_impressive shown impressive context_learning context learning code_generation code generation llms consisting requirement new requirement input output new programs found icl highly dominated examples example selection existing approaches randomly consider textual similarity requirements retrieve leading sub optimal performance paper propose based selection approach named context code_generation code generation given candidate example exploit llms considering generation probabilities ground given requirement example label candidate examplesas positive negative probability feedback based labeleddata import contrastive_learning contrastive learning objective train preference llms code_generation code generation llms evaluate representative datasets e.g. mbpp mbcpp outperforms state art baselines codegen gpt-3.5 interms pass@1 respectively"}
{"id": "nan", "abstract": "  In-context learning (ICL) suffers from oversensitivity to the prompt, makingit unreliable in real-world scenarios. We study the sensitivity of ICL withrespect to multiple perturbation types. First, we find that label bias obscuresthe true sensitivity, and therefore prior work may have significantlyunderestimated ICL sensitivity. Second, we observe a strong negativecorrelation between ICL sensitivity and accuracy: predictions sensitive toperturbations are less likely to be correct. Motivated by these findings, wepropose \\textsc{SenSel}, a few-shot selective prediction method that abstainsfrom sensitive predictions. Experiments on ten classification datasets showthat \\textsc{SenSel} consistently outperforms two commonly usedconfidence-based and entropy-based baselines on abstention decisions.", "title": "on the relation between sensitivity and accuracy in incontext learning", "url": "http://arxiv.org/pdf/2209.07661v2.pdf", "tokenized_text": "context_learning context learning icl suffers makingit unreliable real world_scenarios world scenarios study sensitivity icl multiple perturbation types find label bias true sensitivity prior_work prior work icl sensitivity second observe strong icl sensitivity accuracy predictions sensitive toperturbations likely correct motivated findings wepropose shot selective prediction method sensitive predictions experiments classification datasets showthat consistently_outperforms consistently outperforms commonly based entropy based baselines abstention decisions"}
{"id": "nan", "abstract": "  We introduce a new in-context learning paradigm to measure Large LanguageModels' (LLMs) ability to learn novel words during inference. In particular, werewrite Winograd-style co-reference resolution problems by replacing the keyconcept word with a synthetic but plausible word that the model must understandto complete the task. Solving this task requires the model to make use of thedictionary definition of the new word given in the prompt. This benchmarkaddresses word acquisition, one important aspect of the diachronic degradationknown to afflict LLMs. As LLMs are frozen in time at the moment they aretrained, they are normally unable to reflect the way language changes overtime. We show that the accuracy of LLMs compared to the original Winograd tasksdecreases radically in our benchmark, thus identifying a limitation of currentmodels and providing a benchmark to measure future improvements in LLMs abilityto do in-context learning.", "title": "winodict probing language models for incontext word acquisition", "url": "http://arxiv.org/pdf/2209.12153v1.pdf", "tokenized_text": "introduce new context_learning context learning paradigm measure large_languagemodels large languagemodels llms ability learn novel words inference particular winograd style co reference resolution problems replacing word synthetic plausible word complete task solving task requires use definition new word given word acquisition important aspect llms llms frozen time moment aretrained unable reflect way language changes accuracy llms compared original winograd benchmark identifying limitation providing benchmark measure future improvements llms abilityto context_learning context learning"}
{"id": "nan", "abstract": "  In-context learning (ICL) enables large language models (LLMs) to perform newtasks by prompting them with a sequence of training examples. However, it isknown that ICL is very sensitive to the choice of training examples: randomlysampling examples from a training set leads to high variance in performance. Inthis paper, we show that carefully curating a subset of training data greatlystabilizes ICL performance without any other changes to the ICL algorithm(e.g., prompt retrieval or calibration). We introduce two methods to choosetraining subsets -- both score training examples individually, then select thehighest-scoring ones. CondAcc scores a training example by its average dev-setICL accuracy when combined with random training examples, while Datamodelslearns linear regressors that estimate how the presence of each trainingexample influences LLM outputs. Across five tasks and two LLMs, sampling fromstable subsets selected by CondAcc and Datamodels improves average accuracyover sampling from the entire training set by 7.7% and 6.3%, respectively.Surprisingly, the stable subset examples are not especially diverse in contentor low in perplexity, in contrast with other work suggesting that diversity andperplexity are important when prompting LLMs.", "title": "data curation alone can stabilize incontext learning", "url": "http://arxiv.org/pdf/2212.10378v2.pdf", "tokenized_text": "context_learning context learning icl enables large_language large language llms perform newtasks sequence training_examples training examples icl sensitive choice training_examples training examples examples training set leads high variance performance inthis_paper inthis paper carefully curating subset training_data training data icl performance changes icl retrieval calibration introduce methods subsets score training_examples training examples individually select scoring ones scores training example average dev accuracy combined random training_examples training examples linear estimate presence trainingexample influences llm outputs tasks llms sampling subsets selected improves average accuracyover sampling entire training set 7.7 6.3 respectively surprisingly stable subset examples especially diverse low perplexity contrast work suggesting diversity important llms"}
{"id": "nan", "abstract": "  With the increasing ability of large language models (LLMs), in-contextlearning (ICL) has become a new paradigm for natural language processing (NLP),where LLMs make predictions only based on contexts augmented with a fewexamples. It has been a new trend to explore ICL to evaluate and extrapolatethe ability of LLMs. In this paper, we aim to survey and summarize the progressand challenges of ICL. We first present a formal definition of ICL and clarifyits correlation to related studies. Then, we organize and discuss advancedtechniques, including training strategies, demonstration designing strategies,as well as related analysis. Finally, we discuss the challenges of ICL andprovide potential directions for further research. We hope that our work canencourage more research on uncovering how ICL works and improving ICL.", "title": "a survey on incontext learning", "url": "http://arxiv.org/pdf/2301.00234v3.pdf", "tokenized_text": "increasing ability large_language large language llms contextlearning icl new_paradigm new paradigm natural_language natural language processing llms predictions based contexts augmented fewexamples new trend explore icl evaluate ability llms paper aim survey summarize challenges icl present formal definition icl correlation related studies organize discuss including training strategies demonstration designing strategies related analysis finally discuss challenges icl andprovide potential directions research hope work research uncovering icl works improving icl"}
{"id": "nan", "abstract": "  Data scarcity is a common problem in NLP, especially when the annotationpertains to nuanced socio-linguistic concepts that require specializedknowledge. As a result, few-shot identification of these concepts is desirable.Few-shot in-context learning using pre-trained Large Language Models (LLMs) hasbeen recently applied successfully in many NLP tasks. In this paper, we studyfew-shot identification of a psycho-linguistic concept, Morality Frames (Roy etal., 2021), using LLMs. Morality frames are a representation framework thatprovides a holistic view of the moral sentiment expressed in text, identifyingthe relevant moral foundation (Haidt and Graham, 2007) and at a finer level ofgranularity, the moral sentiment expressed towards the entities mentioned inthe text. Previous studies relied on human annotation to identify moralityframes in text which is expensive. In this paper, we propose prompting-basedapproaches using pretrained Large Language Models for identification ofmorality frames, relying only on few-shot exemplars. We compare our models'performance with few-shot RoBERTa and found promising results.", "title": "towards fewshot identification of morality frames using incontext learning", "url": "http://arxiv.org/pdf/2302.02029v1.pdf", "tokenized_text": "data scarcity common problem nlp especially nuanced linguistic concepts require result shot identification concepts desirable shot context_learning context learning pre trained large_language large language llms hasbeen recently applied successfully nlp_tasks nlp tasks paper shot identification linguistic concept frames etal 2021 llms frames representation framework thatprovides holistic view moral sentiment expressed text relevant moral foundation finer level moral sentiment expressed entities mentioned inthe text previous studies relied human annotation identify text expensive paper propose basedapproaches pretrained large_language large language identification frames relying shot exemplars compare models'performance shot roberta found promising_results promising results"}
{"id": "nan", "abstract": "  In recent years, In-context Learning (ICL) has gained increasing attentionand emerged as the new paradigm for large language model (LLM) evaluation.Unlike traditional fine-tuning methods, ICL instead adapts the pre-trainedmodels to unseen tasks without any parameter updates. However, theimplementation of ICL is sophisticated due to the diverse retrieval andinference methods involved, as well as the varying pre-processing requirementsfor different models, datasets, and tasks. A unified and flexible framework forICL is urgently needed to ease the implementation of the aforementionedcomponents. To facilitate ICL research, we introduce OpenICL, an open-sourcetoolkit for ICL and LLM evaluation. OpenICL is research-friendly with a highlyflexible architecture that users can easily combine different components tosuit their needs. It also provides various state-of-the-art retrieval andinference methods to streamline the process of adapting ICL to cutting-edgeresearch. The effectiveness of OpenICL has been validated on a wide range ofNLP tasks, including classification, QA, machine translation, and semanticparsing. As a side-product, we found OpenICL to be an efficient yet robust toolfor LLMs evaluation. OpenICL is released athttps://github.com/Shark-NLP/OpenICL", "title": "openicl an opensource framework for incontext learning", "url": "http://arxiv.org/pdf/2303.02913v1.pdf", "tokenized_text": "recent_years recent years context_learning context learning icl gained increasing emerged new_paradigm new paradigm large_language large language llm evaluation unlike traditional fine tuning methods icl instead adapts pre unseen tasks parameter updates theimplementation icl sophisticated diverse retrieval andinference methods involved varying pre processing different datasets tasks unified flexible framework urgently needed ease implementation facilitate icl research introduce open icl llm evaluation research friendly architecture users easily combine different components needs provides state art retrieval andinference methods streamline process adapting icl cutting effectiveness validated wide_range wide range ofnlp tasks including classification qa machine_translation machine translation semanticparsing product found efficient robust toolfor llms evaluation released"}
{"id": "nan", "abstract": "  Medications often impose temporal constraints on everyday patient activity.Violations of such medical temporal constraints (MTCs) lead to a lack oftreatment adherence, in addition to poor health outcomes and increasedhealthcare expenses. These MTCs are found in drug usage guidelines (DUGs) inboth patient education materials and clinical texts. Computationallyrepresenting MTCs in DUGs will advance patient-centric healthcare applicationsby helping to define safe patient activity patterns. We define a novel taxonomyof MTCs found in DUGs and develop a novel context-free grammar (CFG) basedmodel to computationally represent MTCs from unstructured DUGs. Additionally,we release three new datasets with a combined total of N = 836 DUGs labeledwith normalized MTCs. We develop an in-context learning (ICL) solution forautomatically extracting and normalizing MTCs found in DUGs, achieving anaverage F1 score of 0.62 across all datasets. Finally, we rigorouslyinvestigate ICL model performance against a baseline model, across datasets andMTC types, and through in-depth error analysis.", "title": "the scope of incontext learning for the extraction of medical temporal constraints", "url": "http://arxiv.org/pdf/2303.09366v2.pdf", "tokenized_text": "impose temporal constraints everyday patient activity violations medical temporal constraints lead lack adherence addition poor health outcomes found drug usage guidelines inboth patient education materials clinical texts advance patient centric healthcare helping define safe patient activity patterns define novel found develop novel context free grammar computationally represent unstructured additionally release new datasets combined total normalized develop context_learning context learning icl solution extracting found achieving anaverage f1_score f1 score 0.62 datasets finally icl performance baseline datasets types depth error analysis"}
{"id": "nan", "abstract": "  In spite of the potential for ground-breaking achievements offered by largelanguage models (LLMs) (e.g., GPT-3), they still lag significantly behindfully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE).This is due to the two major shortcomings of LLMs in RE: (1) low relevanceregarding entity and relation in retrieved demonstrations for in-contextlearning; and (2) the strong inclination to wrongly classify NULL examples intoother pre-defined labels.  In this paper, we propose GPT-RE to bridge the gap between LLMs andfully-supervised baselines. GPT-RE successfully addresses the aforementionedissues by (1) incorporating task-specific entity representations indemonstration retrieval; and (2) enriching the demonstrations with goldlabel-induced reasoning logic. We evaluate GPT-RE on four widely-used REdatasets, and observe that GPT-RE achieves improvements over not only existingGPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-REachieves SOTA performances on the Semeval and SciERC datasets, and competitiveperformances on the TACRED and ACE05 datasets.", "title": "gptre incontext learning for relation extraction using large language models", "url": "http://arxiv.org/pdf/2305.02105v2.pdf", "tokenized_text": "spite potential ground breaking achievements offered largelanguage_models largelanguage llms e.g. gpt-3 lag significantly supervised baselines e.g. fine tuned bert relation_extraction relation extraction major shortcomings llms low entity relation retrieved demonstrations contextlearning strong classify examples pre defined labels paper propose gpt bridge gap llms supervised baselines gpt successfully addresses incorporating task specific entity representations retrieval enriching demonstrations induced reasoning logic evaluate gpt widely observe gpt achieves improvements baselines fully supervised baselines specifically gpt sota performances semeval datasets tacred ace05 datasets"}
{"id": "nan", "abstract": "  This paper presents our contribution to the MEDIQA-2023 Dialogue2Note sharedtask, encompassing both subtask A and subtask B. We approach the task as adialogue summarization problem and implement two distinct pipelines: (a) afine-tuning of a pre-trained dialogue summarization model and GPT-3, and (b)few-shot in-context learning (ICL) using a large language model, GPT-4. Bothmethods achieve excellent results in terms of ROUGE-1 F1, BERTScore F1(deberta-xlarge-mnli), and BLEURT, with scores of 0.4011, 0.7058, and 0.5421,respectively. Additionally, we predict the associated section headers usingRoBERTa and SciBERT based classification models. Our team ranked fourth amongall teams, while each team is allowed to submit three runs as part of theirsubmission. We also utilize expert annotations to demonstrate that the notesgenerated through the ICL GPT-4 are better than all other baselines. The codefor our submission is available.", "title": "gersteinlab at mediqachat 2023 clinical note summarization from doctorpatient conversations through finetuning and incontext learning", "url": "http://arxiv.org/pdf/2305.05001v1.pdf", "tokenized_text": "paper_presents paper presents contribution dialogue2note sharedtask encompassing subtask subtask approach task adialogue summarization problem implement distinct pipelines tuning pre trained dialogue summarization gpt-3 shot context_learning context learning icl large_language large language gpt-4 achieve excellent results terms rouge-1 f1 bertscore mnli bleurt scores additionally predict associated section scibert based classification team ranked fourth teams team allowed runs utilize expert annotations demonstrate icl gpt-4 better baselines submission available"}
{"id": "nan", "abstract": "  Previous studies have shown that large language models (LLMs) like GPTs storemassive factual knowledge in their parameters. However, the stored knowledgecould be false or out-dated. Traditional knowledge editing methods refine LLMsvia fine-tuning on texts containing specific knowledge. However, with theincreasing scales of LLMs, these gradient-based approaches bring largecomputation costs. The trend of model-as-a-service also makes it impossible tomodify knowledge in black-box LMs. Inspired by in-context learning (ICL), a newparadigm based on demonstration contexts without parameter updating, we explorewhether ICL can edit factual knowledge. To answer this question, we give acomprehensive empirical study of ICL strategies. Experiments show thatin-context knowledge editing (IKE), without any gradient and parameterupdating, achieves a competitive success rate compared to gradient-basedmethods on GPT-J (6B) but with much fewer side effects, including lessover-editing on similar but unrelated facts and less knowledge forgetting onpreviously stored knowledge. We also apply the method to larger LMs with tensor hundreds of parameters like OPT-175B, which shows the scalability of ourmethod. The code is available at https://github.com/Zce1112zslx/IKE.", "title": "can we edit factual knowledge by incontext learning", "url": "http://arxiv.org/pdf/2305.12740v1.pdf", "tokenized_text": "previous studies shown large_language large language llms like gpts factual knowledge parameters stored false traditional knowledge editing methods refine fine tuning texts containing specific knowledge scales llms gradient based approaches bring costs trend service makes impossible tomodify knowledge black box lms inspired context_learning context learning icl based demonstration contexts parameter updating icl edit factual knowledge answer question acomprehensive empirical study icl strategies experiments thatin context knowledge editing gradient achieves competitive success_rate success rate compared gradient gpt 6b fewer effects including editing similar unrelated facts knowledge forgetting stored knowledge apply method larger lms tensor hundreds parameters like opt-175b shows scalability ourmethod code_is_available code available"}
{"id": "nan", "abstract": "  In-context learning (ICL), the ability of large language models to performnovel tasks by conditioning on a prompt with a few task examples, requiresthese examples to be informative about the test instance. The standard approachof independently ranking and selecting the most similar examples selectsredundant examples while omitting important information. In this work, we showthat BERTScore-Recall (BSR) selects better examples that demonstrate more ofthe salient aspects, e.g. reasoning patterns, of the test input. We furtherextend BSR and many standard metrics to easily optimizable set-level metrics,giving still better coverage of those salient aspects. On 15 datasets spanning6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metricfor in-context example selection across the board, and (2) for compositionaltasks, set selection using Set-BSR outperforms independent ranking by up to 17points on average and, despite being training-free, surpasses methods thatleverage task or LLM-specific training.", "title": "coveragebased example selection for incontext learning", "url": "http://arxiv.org/pdf/2305.14907v3.pdf", "tokenized_text": "context_learning context learning icl ability large_language large language tasks conditioning task examples examples informative test instance standard independently ranking selecting similar examples examples important information work showthat bertscore recall selects better examples demonstrate ofthe salient aspects e.g. reasoning patterns test input standard metrics easily set level metrics giving better coverage salient aspects 15 datasets tasks diverse llms superior context example selection board set selection set outperforms independent ranking average despite training free surpasses methods task llm specific training"}
{"id": "nan", "abstract": "  Recently, large language models (LLMs) have made significant advancements innatural language understanding and generation. However, their potential incomputer vision remains largely unexplored. In this paper, we introduce a new,exploratory approach that enables LLMs to process images using the ScalableVector Graphics (SVG) format. By leveraging the XML-based textual descriptionsof SVG representations instead of raster images, we aim to bridge the gapbetween the visual and textual modalities, allowing LLMs to directly understandand manipulate images without the need for parameterized visual components. Ourmethod facilitates simple image classification, generation, and in-contextlearning using only LLM capabilities. We demonstrate the promise of ourapproach across discriminative and generative tasks, highlighting its (i)robustness against distribution shift, (ii) substantial improvements achievedby tapping into the in-context learning abilities of LLMs, and (iii) imageunderstanding and generation capabilities with human guidance. Our code, data,and models can be found here https://github.com/mu-cai/svg-llm.", "title": "leveraging large language models for scalable vector graphicsdriven image understanding", "url": "http://arxiv.org/pdf/2306.06094v1.pdf", "tokenized_text": "recently large_language large language llms significant advancements innatural language understanding generation potential vision remains largely unexplored paper introduce new exploratory approach enables llms process images format leveraging xml based textual representations instead images aim bridge gapbetween visual textual modalities allowing llms directly manipulate images need parameterized visual components ourmethod facilitates simple image classification generation contextlearning llm capabilities demonstrate promise ourapproach discriminative generative tasks highlighting distribution shift ii substantial improvements achievedby context_learning context learning abilities llms iii generation capabilities human guidance code data found"}
{"id": "nan", "abstract": "  The biomedical field relies heavily on concept linking in various areas suchas literature mining, graph alignment, information retrieval,question-answering, data, and knowledge integration. Although large languagemodels (LLMs) have made significant strides in many natural language processingtasks, their effectiveness in biomedical concept mapping is yet to be fullyexplored. This research investigates a method that exploits the in-contextlearning (ICL) capabilities of large models for biomedical concept linking. Theproposed approach adopts a two-stage retrieve-and-rank framework. Initially,biomedical concepts are embedded using language models, and then embeddingsimilarity is utilized to retrieve the top candidates. These candidates'contextual information is subsequently incorporated into the prompt andprocessed by a large language model to re-rank the concepts. This approachachieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7%in chemical entity normalization, exhibiting a competitive performance relativeto supervised learning methods. Further, it showed a significant improvement,with an over 20-point absolute increase in F1 score on an oncology matchingdataset. Extensive qualitative assessments were conducted, and the benefits andpotential shortcomings of using large language models within the biomedicaldomain were discussed. were discussed.", "title": "exploring the incontext learning ability of large language model for biomedical concept linking", "url": "http://arxiv.org/pdf/2307.01137v1.pdf", "tokenized_text": "biomedical field relies heavily concept linking areas suchas literature mining graph alignment information retrieval question answering data knowledge integration large_languagemodels large languagemodels llms significant strides natural_language natural language processingtasks effectiveness biomedical concept mapping research investigates method exploits contextlearning icl capabilities large biomedical concept linking theproposed approach adopts stage retrieve rank framework initially biomedical concepts embedded language_models language utilized retrieve candidates information subsequently incorporated large_language large language rank concepts approachachieved accuracy disease entity normalization chemical entity normalization exhibiting competitive_performance competitive performance supervised learning methods showed significant improvement 20 point absolute increase f1_score f1 score oncology extensive qualitative assessments conducted benefits andpotential shortcomings large_language large language discussed discussed"}
{"id": "nan", "abstract": "  Large language models (LLMs) have demonstrated their ability to learnin-context, allowing them to perform various tasks based on a few input-outputexamples. However, the effectiveness of in-context learning is heavily relianton the quality of the selected examples. In this paper, we propose a novelframework to iteratively train dense retrievers that can identify high-qualityin-context examples for LLMs. Our framework initially trains a reward modelbased on LLM feedback to evaluate the quality of candidate examples, followedby knowledge distillation to train a bi-encoder based dense retriever. Ourexperiments on a suite of 30 tasks demonstrate that our framework significantlyenhances in-context learning performance. Furthermore, we show thegeneralization ability of our framework to unseen tasks during training. Anin-depth analysis reveals that our model improves performance by retrievingexamples with similar patterns, and the gains are consistent across LLMs ofvarying sizes.", "title": "learning to retrieve incontext examples for large language models", "url": "http://arxiv.org/pdf/2307.07164v1.pdf", "tokenized_text": "large_language large language llms demonstrated ability learnin context allowing perform tasks based input effectiveness context_learning context learning heavily quality selected examples paper propose novelframework iteratively train dense retrievers identify high context_examples context examples llms framework initially trains reward llm feedback evaluate quality candidate examples followedby knowledge_distillation knowledge distillation train bi encoder based dense retriever ourexperiments suite 30 tasks demonstrate framework context_learning context learning performance furthermore thegeneralization ability framework unseen tasks training anin depth analysis reveals improves performance similar patterns gains consistent llms ofvarying sizes"}
{"id": "nan", "abstract": "  The predictions of Large Language Models (LLMs) on downstream tasks oftenimprove significantly when including examples of the input--label relationshipin the context. However, there is currently no consensus about how thisin-context learning (ICL) ability of LLMs works. For example, while Xie et al.(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)argue ICL does not even learn label relationships from in-context examples. Inthis paper, we provide novel insights into how ICL leverages label information,revealing both capabilities and limitations. To ensure we obtain acomprehensive picture of ICL behavior, we study probabilistic aspects of ICLpredictions and thoroughly examine the dynamics of ICL as more examples areprovided. Our experiments show that ICL predictions almost always depend onin-context labels, and that ICL can learn truly novel tasks in-context.However, we also find that ICL struggles to fully overcome predictionpreferences acquired from pre-training data, and, further, that ICL does notconsider all in-context information equally.", "title": "incontext learning learns label relationships but is not conventional learning", "url": "http://arxiv.org/pdf/2307.12375v3.pdf", "tokenized_text": "predictions large_language large language llms downstream_tasks downstream tasks significantly including examples input label context currently consensus context_learning context learning icl ability llms works example et icl general purpose learning algorithm min et_al et al icl learn label relationships context_examples context examples inthis_paper inthis paper provide novel insights icl leverages label information revealing capabilities limitations ensure obtain acomprehensive picture icl behavior study probabilistic aspects thoroughly examine dynamics icl examples experiments icl predictions depend onin context labels icl learn truly novel tasks context find icl struggles fully overcome acquired pre training_data training data icl context information equally"}
{"id": "nan", "abstract": "  Multiple-choice questions (MCQs) are ubiquitous in almost all levels ofeducation since they are easy to administer, grade, and are a reliable formatin both assessments and practices. An important aspect of MCQs is thedistractors, i.e., incorrect options that are designed to target specificmisconceptions or insufficient knowledge among students. To date, the task ofcrafting high-quality distractors has largely remained a labor-intensiveprocess for teachers and learning content designers, which has limitedscalability. In this work, we explore the task of automated distractor andcorresponding feedback message generation in math MCQs using large languagemodels. We establish a formulation of these two tasks and propose a simple,in-context learning-based solution. Moreover, we explore using two non-standardmetrics to evaluate the quality of the generated distractors and feedbackmessages. We conduct extensive experiments on these tasks using a real-worldMCQ dataset that contains student response information. Our findings suggestthat there is a lot of room for improvement in automated distractor andfeedback generation. We also outline several directions for future work", "title": "exploring automated distractor and feedback generation for math multiplechoice questions via incontext learning", "url": "http://arxiv.org/pdf/2308.03234v1.pdf", "tokenized_text": "multiple choice questions mcqs ubiquitous levels easy grade reliable assessments practices important aspect mcqs i.e. incorrect options designed target insufficient knowledge students date task ofcrafting high quality distractors largely remained labor teachers learning content designers work explore task automated feedback message generation math mcqs large_languagemodels large languagemodels establish formulation tasks propose simple context_learning context learning based solution explore non evaluate quality generated distractors conduct_extensive conduct extensive experiments tasks real dataset contains student response information findings suggestthat lot room improvement automated generation outline directions future work"}
{"id": "nan", "abstract": "  Recent empirical evidence indicates that transformer based in-contextlearning performs better when using a prefix language model (prefixLM), inwhich in-context samples can all attend to each other, compared to causallanguage models (causalLM), which use auto-regressive attention that prohibitsin-context samples to attend to future samples. While this result is intuitive,it is not understood from a theoretical perspective. In this paper we take atheoretical approach and analyze the convergence behavior of prefixLM andcausalLM under a certain parameter construction. Our analysis shows that bothLM types converge to their stationary points at a linear rate, but that whileprefixLM converges to the optimal solution of linear regression, causalLMconvergence dynamics follows that of an online gradient descent algorithm,which is not guaranteed to be optimal even as the number of samples growsinfinitely. We supplement our theoretical claims with empirical experimentsover synthetic and real tasks and using various types of transformers. Ourexperiments verify that causalLM consistently underperforms prefixLM in allsettings.", "title": "causallm is not optimal for incontext learning", "url": "http://arxiv.org/pdf/2308.06912v2.pdf", "tokenized_text": "recent empirical evidence indicates transformer based contextlearning performs better prefix language_model language inwhich context samples attend compared use auto regressive attention context samples attend future samples result intuitive understood theoretical perspective paper approach analyze convergence behavior certain parameter construction analysis shows types points linear rate converges optimal solution linear regression dynamics follows online gradient descent algorithm guaranteed optimal number samples supplement theoretical claims empirical synthetic real tasks types transformers ourexperiments verify consistently underperforms"}
{"id": "nan", "abstract": "  In-context learning (ICL) operates by showing language models (LMs) examplesof input-output pairs for a given task, i.e., demonstrations. The standardapproach for ICL is to prompt the LM with concatenated demonstrations followedby the test input. This approach suffers from some issues. First, concatenationoffers almost no control over the contribution of each demo to the modelprediction. This can be sub-optimal when some demonstrations are irrelevant tothe test example. Second, due to the input length limit of some transformermodels, it might be infeasible to fit many examples into the context,especially when dealing with long-input tasks. In this work, we exploreDemonstration Ensembling (DENSE) as an alternative to simple concatenation.DENSE predicts outputs using subsets (i.e., buckets) of the demonstrations andthen combines the output probabilities resulting from each subset to producethe final prediction. We study different ensembling methods using GPT-j andexperiment on 12 language tasks. Our experiments show weighted max ensemblingto outperform vanilla concatenation by as large as 2.4 average points. Codeavailable at https://github.com/mukhal/icl-ensembling.", "title": "exploring demonstration ensembling for incontext learning", "url": "http://arxiv.org/pdf/2308.08780v2.pdf", "tokenized_text": "context_learning context learning icl operates showing language_models language lms input output pairs given task i.e. demonstrations icl lm concatenated demonstrations followedby test input approach suffers issues control contribution demo sub optimal demonstrations irrelevant tothe test example second input length limit transformermodels infeasible fit examples context especially dealing long input tasks work ensembling dense alternative simple concatenation dense predicts outputs subsets i.e. demonstrations andthen combines output probabilities resulting subset producethe final prediction study different ensembling methods gpt 12 language tasks experiments weighted max outperform vanilla concatenation large 2.4 average points"}
{"id": "nan", "abstract": "  Two lines of work are taking the central stage in AI research. On the onehand, the community is making increasing efforts to build models that discardspurious correlations and generalize better in novel test environments.Unfortunately, the bitter lesson so far is that no proposal convincinglyoutperforms a simple empirical risk minimization baseline. On the other hand,large language models (LLMs) have erupted as algorithms able to learnin-context, generalizing on-the-fly to eclectic contextual circumstances thatusers enforce by means of prompting. In this paper, we argue that context isenvironment, and posit that in-context learning holds the key to better domaingeneralization. Via extensive theory and experiments, we show that payingattention to context$\\unicode{x2013}\\unicode{x2013}$unlabeled examples as theyarrive$\\unicode{x2013}\\unicode{x2013}$allows our proposed In-Context RiskMinimization (ICRM) algorithm to zoom-in on the test environment riskminimizer, leading to significant out-of-distribution performance improvements.From all of this, two messages are worth taking home. Researchers in domaingeneralization should consider environment as context, and harness the adaptivepower of in-context learning. Researchers in LLMs should consider context asenvironment, to better structure data towards generalization.", "title": "context is environment", "url": "http://arxiv.org/pdf/2309.09888v2.pdf", "tokenized_text": "lines work taking central stage ai research community making increasing efforts build correlations generalize better novel test environments unfortunately far proposal simple empirical risk minimization baseline hand large_language large language llms algorithms able learnin context generalizing fly contextual circumstances enforce means paper argue context posit context_learning context learning holds key better extensive theory experiments examples proposed context algorithm test environment leading significant distribution performance improvements messages worth taking home researchers consider environment context harness context_learning context learning researchers llms consider context better structure data generalization"}
{"id": "nan", "abstract": "  Unsupported and unfalsifiable claims we encounter in our daily lives caninfluence our view of the world. Characterizing, summarizing, and -- moregenerally -- making sense of such claims, however, can be challenging. In thiswork, we focus on fine-grained debate topics and formulate a new task ofdistilling, from such claims, a countable set of narratives. We present acrowdsourced dataset of 12 controversial topics, comprising more than 120karguments, claims, and comments from heterogeneous sources, each annotated witha narrative label. We further investigate how large language models (LLMs) canbe used to synthesise claims using In-Context Learning. We find that generatedclaims with supported evidence can be used to improve the performance ofnarrative classification models and, additionally, that the same model caninfer the stance and aspect using a few training examples. Such a model can beuseful in applications which rely on narratives , e.g. fact-checking.", "title": "prompt, condition, and generate classification of unsupported claims with incontext learning", "url": "http://arxiv.org/pdf/2309.10359v1.pdf", "tokenized_text": "unsupported claims encounter daily lives view world summarizing moregenerally making sense claims challenging thiswork focus fine grained debate topics formulate new task claims set narratives present dataset 12 controversial topics comprising claims comments heterogeneous sources annotated witha narrative label investigate large_language large language llms canbe claims context_learning context learning find supported evidence improve performance classification additionally stance aspect training_examples training examples applications rely narratives e.g. fact checking"}
{"id": "nan", "abstract": "  In-context learning (ICL) using large language models for tasks with manylabels is challenging due to the limited context window, which makes itdifficult to fit a sufficient number of examples in the prompt. In this paper,we use a pre-trained dense retrieval model to bypass this limitation, givingthe model only a partial view of the full label space for each inference call.Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the artperformance in few-shot settings for three common intent classificationdatasets, with no finetuning. We also surpass fine-tuned performance onfine-grained sentiment classification in certain cases. We analyze theperformance across number of in-context examples and different model scales,showing that larger models are necessary to effectively and consistently makeuse of larger context lengths for ICL. By running several ablations, we analyzethe model's use of: a) the similarity of the in-context examples to the currentinput, b) the semantic content of the class names, and c) the correctcorrespondence between examples and labels. We demonstrate that all three areneeded to varying degrees depending on the domain, contrary to certain recentworks.", "title": "incontext learning for text classification with many labels", "url": "http://arxiv.org/pdf/2309.10954v1.pdf", "tokenized_text": "context_learning context learning icl large_language large language tasks challenging limited context window makes itdifficult fit sufficient number examples paper use pre trained dense retrieval bypass limitation partial view label space inference testing recent open source llms opt llama set new state artperformance shot_settings shot settings common intent classificationdatasets finetuning surpass fine tuned performance onfine grained sentiment classification certain cases analyze theperformance number context_examples context examples different scales showing larger necessary effectively consistently larger context lengths icl running ablations use similarity context_examples context examples semantic content class names examples labels demonstrate varying degrees depending domain contrary certain"}
{"id": "nan", "abstract": "  We study the problem of in-context learning (ICL) with large language models(LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leakor regurgitate the private examples demonstrated in the prompt. We propose anovel algorithm that generates synthetic few-shot demonstrations from theprivate dataset with formal differential privacy (DP) guarantees, and showempirically that it can achieve effective ICL. We conduct extensive experimentson standard benchmarks and compare our algorithm with non-private ICL andzero-shot solutions. Our results demonstrate that our algorithm can achievecompetitive performance with strong privacy levels. These results open up newpossibilities for ICL with privacy protection for a broad range ofapplications.", "title": "privacypreserving incontext learning with differentially private fewshot generation", "url": "http://arxiv.org/pdf/2309.11765v1.pdf", "tokenized_text": "study problem context_learning context learning icl large_language large language models(llms private datasets scenario poses privacy risks llms private examples demonstrated propose anovel algorithm generates synthetic shot demonstrations dataset formal differential privacy dp guarantees achieve effective icl conduct_extensive conduct extensive experimentson standard benchmarks compare algorithm non private icl andzero shot solutions results_demonstrate results demonstrate algorithm achievecompetitive performance strong privacy levels results open icl privacy protection broad range ofapplications"}
{"id": "nan", "abstract": "  Answering numerical questions over hybrid contents from the given tables andtext(TextTableQA) is a challenging task. Recently, Large Language Models (LLMs)have gained significant attention in the NLP community. With the emergence oflarge language models, In-Context Learning and Chain-of-Thought prompting havebecome two particularly popular research topics in this field. In this paper,we introduce a new prompting strategy called Hybrid prompt strategy andRetrieval of Thought for TextTableQA. Through In-Context Learning, we promptthe model to develop the ability of retrieval thinking when dealing with hybriddata. Our method achieves superior performance compared to the fully-supervisedSOTA on the MultiHiertt dataset in the few-shot setting.", "title": "hrot hybrid prompt strategy and retrieval of thought for tabletext hybrid question answering", "url": "http://arxiv.org/pdf/2309.12669v1.pdf", "tokenized_text": "answering numerical questions hybrid contents given tables challenging task recently large_language large language llms)have gained significant attention nlp community emergence oflarge language_models language context_learning context learning chain thought_prompting thought particularly popular research topics field paper introduce new strategy called hybrid strategy andretrieval thought texttableqa context_learning context learning develop ability retrieval thinking dealing method_achieves method achieves superior_performance superior performance compared fully dataset shot_setting shot setting"}
{"id": "nan", "abstract": "  From grading papers to summarizing medical documents, large language models(LLMs) are evermore used for evaluation of text generated by humans and AIalike. However, despite their extensive utility, LLMs exhibit distinct failuremodes, necessitating a thorough audit and improvement of their text evaluationcapabilities. Here we introduce ALLURE, a systematic approach to Auditing LargeLanguage Models Understanding and Reasoning Errors. ALLURE involves comparingLLM-generated evaluations with annotated data, and iteratively incorporatinginstances of significant deviation into the evaluator, which leveragesin-context learning (ICL) to enhance and improve robust evaluation of text byLLMs. Through this iterative process, we refine the performance of theevaluator LLM, ultimately reducing reliance on human annotators in theevaluation process. We anticipate ALLURE to serve diverse applications of LLMsin various domains related to evaluation of textual data, such as medicalsummarization, education, and and productivity.", "title": "allure auditing and improving llmbased evaluation of text using iterative incontextlearning", "url": "http://arxiv.org/pdf/2309.13701v2.pdf", "tokenized_text": "grading papers summarizing medical documents large_language large language models(llms evaluation text generated humans despite extensive utility llms exhibit distinct necessitating thorough audit improvement text introduce systematic approach auditing largelanguage_models largelanguage understanding reasoning errors involves generated evaluations annotated_data annotated data iteratively significant deviation evaluator context_learning context learning icl enhance improve robust evaluation text byllms iterative process refine performance llm ultimately reducing reliance human annotators process anticipate serve diverse applications llmsin domains related evaluation textual data education productivity"}
{"id": "nan", "abstract": "  In-Context Learning (ICL) is a new paradigm for natural language processing(NLP), where a large language model (LLM) observes a small number ofdemonstrations and a test instance as its input, and directly makes predictionswithout updating model parameters. Previous studies have revealed that ICL issensitive to the selection and the ordering of demonstrations. However, thereare few studies regarding the impact of the demonstration number on the ICLperformance within a limited input length of LLM, because it is commonlybelieved that the number of demonstrations is positively correlated with modelperformance. In this paper, we found this conclusion does not always hold true.Through pilot experiments, we discover that increasing the number ofdemonstrations does not necessarily lead to improved performance. Building uponthis insight, we propose a Dynamic Demonstrations Controller (D$^2$Controller),which can improve the ICL performance by adjusting the number of demonstrationsdynamically. The experimental results show that D$^2$Controller yields a 5.4%relative improvement on eight different sizes of LLMs across ten datasets.Moreover, we also extend our method to previous ICL models and achievecompetitive results.", "title": "dynamic demonstrations controller for incontext learning", "url": "http://arxiv.org/pdf/2310.00385v1.pdf", "tokenized_text": "context_learning context learning icl new_paradigm new paradigm natural_language natural language processing(nlp large_language large language llm observes small_number small number ofdemonstrations test instance input directly makes updating parameters previous studies revealed icl issensitive selection ordering demonstrations studies impact demonstration number limited input length llm number demonstrations positively correlated modelperformance paper found conclusion hold true pilot experiments discover increasing number ofdemonstrations necessarily lead improved performance building insight propose dynamic demonstrations controller improve icl performance adjusting number experimental_results experimental results yields improvement different sizes llms datasets extend method previous icl achievecompetitive results"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) have recently gained the In-Context Learning(ICL) ability with the models scaling up, allowing them to quickly adapt todownstream tasks with only a few demonstration examples prepended in the inputsequence. Nonetheless, the current practice of ICL treats all demonstrationexamples equally, which still warrants improvement, as the quality of examplesis usually uneven. In this paper, we investigate how to determine approximatelyoptimal weights for demonstration examples and how to apply them during ICL. Toassess the quality of weights in the absence of additional validation data, wedesign a masked self-prediction (MSP) score that exhibits a strong correlationwith the final ICL performance. To expedite the weight-searching process, wediscretize the continuous weight space and adopt beam search. Withapproximately optimal weights obtained, we further propose two strategies toapply them to demonstrations at different model positions. Experimental resultson 8 text classification tasks show that our approach outperforms conventionalICL by a large margin. Our code are publicly available athttps:github.com/Zhe-Young/WICL.", "title": "not all demonstration examples are equally beneficial reweighting demonstration examples for incontext learning", "url": "http://arxiv.org/pdf/2310.08309v1.pdf", "tokenized_text": "large_language large language llms recently gained context learning(icl ability scaling allowing quickly adapt todownstream tasks demonstration examples prepended nonetheless current practice icl treats demonstrationexamples equally improvement quality usually paper investigate determine weights demonstration examples apply icl toassess quality weights absence additional validation data wedesign masked self prediction msp score exhibits strong final icl performance expedite weight searching process continuous weight space adopt beam search optimal weights obtained propose strategies demonstrations different positions experimental text_classification text classification tasks approach outperforms large margin code publicly_available publicly available"}
{"id": "nan", "abstract": "  Transformers pretrained on diverse tasks exhibit remarkable in-contextlearning (ICL) capabilities, enabling them to solve unseen tasks solely basedon input contexts without adjusting model parameters. In this paper, we studyICL in one of its simplest setups: pretraining a linearly parameterizedsingle-layer linear attention model for linear regression with a Gaussianprior. We establish a statistical task complexity bound for the attention modelpretraining, showing that effective pretraining only requires a small number ofindependent tasks. Furthermore, we prove that the pretrained model closelymatches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, byachieving nearly Bayes optimal risk on unseen tasks under a fixed contextlength. These theoretical findings complement prior experimental research andshed light on the statistical foundations of ICL.", "title": "how many pretraining tasks are needed for incontext learning of linear regression", "url": "http://arxiv.org/pdf/2310.08391v1.pdf", "tokenized_text": "transformers pretrained diverse tasks exhibit remarkable contextlearning icl capabilities enabling solve unseen tasks solely basedon input contexts adjusting parameters paper simplest setups pretraining linearly layer linear attention linear regression gaussianprior establish statistical task complexity bound attention showing effective pretraining requires small_number small number tasks furthermore prove pretrained bayes optimal algorithm i.e. optimally tuned ridge regression nearly bayes optimal risk unseen tasks fixed theoretical findings complement prior experimental research light statistical foundations icl"}
{"id": "nan", "abstract": "  As one of the most exciting features of large language models (LLMs),in-context learning is a mixed blessing. While it allows users tofast-prototype a task solver with only a few training examples, the performanceis generally sensitive to various configurations of the prompt such as thechoice or order of the training examples. In this paper, we for the first timetheoretically and empirically identify that such a paradox is mainly due to thelabel shift of the in-context model to the data distribution, in which LLMsshift the label marginal $p(y)$ while having a good label conditional $p(x|y)$.With this understanding, we can simply calibrate the in-context predictivedistribution by adjusting the label marginal, which is estimated viaMonte-Carlo sampling over the in-context model, i.e., generation of LLMs. Wecall our approach as generative calibration. We conduct exhaustive experimentswith 12 text classification tasks and 12 LLMs scaling from 774M to 33B,generally find that the proposed method greatly and consistently outperformsthe ICL as well as state-of-the-art calibration methods, by up to 27% absolutein macro-F1. Meanwhile, the proposed method is also stable under differentprompt configurations.", "title": "generative calibration for incontext learning", "url": "http://arxiv.org/pdf/2310.10266v1.pdf", "tokenized_text": "exciting features large_language large language context_learning context learning mixed allows users prototype task solver training_examples training examples generally sensitive configurations thechoice order training_examples training examples paper empirically identify paradox mainly shift context data distribution label marginal having good label conditional understanding simply calibrate context adjusting label marginal estimated carlo sampling context i.e. generation llms approach generative calibration conduct exhaustive 12 text_classification text classification tasks 12 llms scaling generally find proposed_method proposed method greatly consistently outperformsthe icl state art calibration methods 27 macro f1 proposed_method proposed method stable differentprompt configurations"}
{"id": "nan", "abstract": "  Humans possess a remarkable ability to assign novel interpretations tolinguistic expressions, enabling them to learn new words and understandcommunity-specific connotations. However, Large Language Models (LLMs) have aknowledge cutoff and are costly to finetune repeatedly. Therefore, it iscrucial for LLMs to learn novel interpretations in-context. In this paper, wesystematically analyse the ability of LLMs to acquire novel interpretationsusing in-context learning. To facilitate our study, we introduce MAGNIFICo, anevaluation suite implemented within a text-to-SQL semantic parsing frameworkthat incorporates diverse tokens and prompt settings to simulate real-worldcomplexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit asurprisingly robust capacity for comprehending novel interpretations fromnatural language descriptions as well as from discussions within longconversations. Nevertheless, our findings also highlight the need for furtherimprovements, particularly when interpreting unfamiliar words or when composingmultiple novel interpretations simultaneously in the same example.Additionally, our analysis uncovers the semantic predispositions in LLMs andreveals the impact of recency bias for information presented in long contexts.", "title": "magnifico evaluating the incontext learning ability of large language models to generalize to novel interpretations", "url": "http://arxiv.org/pdf/2310.11634v1.pdf", "tokenized_text": "humans possess remarkable ability assign novel interpretations expressions enabling learn new words specific large_language large language llms aknowledge costly finetune repeatedly iscrucial llms learn novel interpretations context paper wesystematically analyse ability llms acquire novel context_learning context learning facilitate study introduce suite implemented text sql semantic_parsing semantic parsing incorporates diverse tokens settings simulate real experimental_results experimental results demonstrate llms exhibit robust capacity comprehending novel interpretations language descriptions discussions findings highlight need particularly interpreting unfamiliar words novel interpretations simultaneously example additionally analysis uncovers semantic llms impact recency bias information presented long contexts"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) can adapt to new tasks via in-context learning(ICL). ICL is efficient as it does not require any parameter updates to thetrained LLM, but only few annotated examples as input for the LLM. In thiswork, we investigate an active learning approach for ICL, where there is alimited budget for annotating examples. We propose a model-adaptiveoptimization-free algorithm, termed AdaICL, which identifies examples that themodel is uncertain about, and performs semantic diversity-based exampleselection. Diversity-based sampling improves overall effectiveness, whileuncertainty sampling improves budget efficiency and helps the LLM learn newinformation. Moreover, AdaICL poses its sampling strategy as a Maximum Coverageproblem, that dynamically adapts based on the model's feedback and can beapproximately solved via greedy algorithms. Extensive experiments on ninedatasets and seven LLMs show that AdaICL improves performance by 4.4% accuracypoints over SOTA (7.7% relative improvement), is up to 3x more budget-efficientthan performing annotations uniformly at random, while it outperforms SOTA with2x fewer ICL examples.", "title": "which examples to annotate for incontext learning towards effective and efficient selection", "url": "http://arxiv.org/pdf/2310.20046v1.pdf", "tokenized_text": "large_language large language llms adapt new tasks context learning(icl icl efficient require parameter updates llm annotated examples input llm thiswork investigate active learning approach icl alimited budget annotating examples propose free algorithm termed identifies examples themodel uncertain performs semantic diversity based exampleselection diversity based sampling improves overall effectiveness sampling improves budget efficiency helps llm learn poses sampling strategy maximum dynamically adapts based feedback solved greedy algorithms extensive_experiments extensive experiments seven llms improves performance sota 7.7 relative improvement 3x budget performing annotations uniformly random outperforms sota fewer icl examples"}
{"id": "nan", "abstract": "  In-Context Learning (ICL) combined with pre-trained large language models hasachieved promising results on various NLP tasks. However, ICL requireshigh-quality annotated demonstrations which might not be available inreal-world scenarios. To overcome this limitation, we propose \\textbf{D}ata\\textbf{A}ugmentation for \\textbf{I}n-Context \\textbf{L}earning(\\textbf{DAIL}). DAIL leverages the intuition that large language models aremore familiar with the content generated by themselves. It first utilizes thelanguage model to generate paraphrases of the test sample and employs majorityvoting to determine the final result based on individual predictions. Ourextensive empirical evaluation shows that DAIL outperforms the standard ICLmethod and other ensemble-based methods in the low-resource scenario.Additionally, we explore the use of voting consistency as a confidence score ofthe model when the logits of predictions are inaccessible. We believe our workwill stimulate further research on ICL in low-resource settings.", "title": "dail data augmentation for incontext learning via selfparaphrase", "url": "http://arxiv.org/pdf/2311.03319v1.pdf", "tokenized_text": "context_learning context learning icl combined pre trained large_language large language hasachieved promising_results promising results nlp_tasks nlp tasks icl quality annotated demonstrations available inreal world_scenarios world scenarios overcome limitation propose \\textbf{i}n context dail leverages intuition large_language large language aremore familiar content generated utilizes thelanguage generate paraphrases test sample employs determine final result based individual predictions ourextensive empirical evaluation shows dail outperforms standard iclmethod ensemble based methods low resource scenario additionally explore use voting consistency confidence score ofthe logits predictions inaccessible believe stimulate research icl low resource settings"}
{"id": "nan", "abstract": "  Recently, large language models (LLMs) have made remarkable progress innatural language processing. The most representative ability of LLMs isin-context learning (ICL), which enables LLMs to learn patterns from in-contextexemplars without training. The performance of ICL greatly depends on theexemplars used. However, how to choose exemplars remains unclear due to thelack of understanding of how in-context learning works. In this paper, wepresent a novel perspective on ICL by conceptualizing it as contextualretrieval from a model of associative memory. We establish a theoreticalframework of ICL based on Hopfield Networks. Based on our framework, we lookinto how in-context exemplars influence the performance of ICL and propose moreefficient active exemplar selection. Our study sheds new light on the mechanismof ICL by connecting it to memory retrieval, with potential implications foradvancing the understanding of LLMs.", "title": "incontext exemplars as clues to retrieving from large associative memory", "url": "http://arxiv.org/pdf/2311.03498v1.pdf", "tokenized_text": "recently large_language large language llms remarkable progress innatural language_processing language processing representative ability llms isin context_learning context learning icl enables llms learn patterns contextexemplars training performance icl greatly depends choose exemplars remains unclear thelack understanding context_learning context learning works paper wepresent novel perspective icl memory establish icl based networks based framework context exemplars influence performance icl propose active exemplar selection study sheds new light icl connecting memory retrieval potential implications understanding llms"}
{"id": "nan", "abstract": "  Many recent approaches to natural language tasks are built on the remarkableabilities of large language models. Large language models can performin-context learning, where they learn a new task from a few taskdemonstrations, without any parameter updates. This work examines theimplications of in-context learning for the creation of datasets for newnatural language tasks. Departing from recent in-context learning methods, weformulate an annotation-efficient, two-step framework: selective annotationthat chooses a pool of examples to annotate from unlabeled data in advance,followed by prompt retrieval that retrieves task examples from the annotatedpool at test time. Based on this framework, we propose an unsupervised,graph-based selective annotation method, voke-k, to select diverse,representative examples to annotate. Extensive experiments on 10 datasets(covering classification, commonsense reasoning, dialogue, and text/codegeneration) demonstrate that our selective annotation method improves the taskperformance by a large margin. On average, vote-k achieves a 12.9%/11.4%relative gain under an annotation budget of 18/100, as compared to randomlyselecting examples to annotate. Compared to state-of-the-art supervisedfinetuning approaches, it yields similar performance with 10-100x lessannotation cost across 10 tasks. We further analyze the effectiveness of ourframework in various scenarios: language models with varying sizes, alternativeselective annotation methods, and cases where there is a test data domainshift. We hope that our studies will serve as a basis for data annotations aslarge language models are increasingly applied to new tasks. Our code isavailable at https://github.com/HKUNLP/icl-selective-annotation.", "title": "selective annotation makes language models better fewshot learners", "url": "http://arxiv.org/pdf/2209.01975v1.pdf", "tokenized_text": "recent approaches natural_language natural language tasks built remarkableabilities large_language large language large_language large language context_learning context learning learn new task taskdemonstrations parameter updates work examines context_learning context learning creation datasets language tasks recent context_learning context learning methods weformulate annotation efficient step framework selective pool examples annotate unlabeled data advance followed retrieval retrieves task examples test_time test time based framework propose unsupervised graph based selective annotation method select diverse representative examples annotate extensive_experiments extensive experiments 10 classification commonsense reasoning dialogue text codegeneration demonstrate selective annotation method improves taskperformance large margin average vote achieves gain annotation budget compared examples annotate compared state art approaches yields similar performance 10 100x cost 10 tasks analyze effectiveness ourframework scenarios language_models language varying sizes annotation methods cases test data hope studies serve basis data annotations aslarge language_models language increasingly applied new tasks code isavailable"}
{"id": "nan", "abstract": "  In-context learning (ICL) is a powerful paradigm emerged from large languagemodels (LLMs). Despite its promises, ICL performance is known to be highlysensitive to input examples. In this work, we use $\\textit{in-contextinfluences}$ to analyze few-shot ICL performance directly from the in-contextexamples. Our proposed influence-based example selection method can identifyboth positive and negative examples, outperforming several baselines whenevaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a $16.3\\%$performance gap between using the most negative in-context examples compared tothe most positive. In a case study, we apply our influence-based framework toquantify the phenomena of recency bias in example ordering for few-shot ICL.", "title": "incontext example selection with influences", "url": "http://arxiv.org/pdf/2302.11042v2.pdf", "tokenized_text": "context_learning context learning icl powerful paradigm emerged large_languagemodels large languagemodels llms despite promises icl performance known highlysensitive input examples work use \\textit{in analyze shot icl performance directly contextexamples proposed influence based example selection method positive negative examples outperforming baselines whenevaluated superglue tasks analysis uncovers gap negative context_examples context examples compared tothe positive case study apply influence based framework phenomena recency bias example ordering shot icl"}
{"id": "nan", "abstract": "  Large language models (LLMs) are increasingly applied for tabular tasks usingin-context learning. The prompt representation for a table may play a role inthe LLMs ability to process the table. Inspired by prior work, we generate acollection of self-supervised structural tasks (e.g. navigate to a cell androw; transpose the table) and evaluate the performance differences when using 8formats. In contrast to past work, we introduce 8 noise operations inspired byreal-world messy data and adversarial inputs, and show that such operations canimpact LLM performance across formats for different structural understandingtasks.", "title": "tabular representation, noisy operators, and impacts on table structure understanding tasks in llms", "url": "http://arxiv.org/pdf/2310.10358v1.pdf", "tokenized_text": "large_language large language llms increasingly applied tabular tasks usingin context_learning context learning representation table play role inthe llms ability process table inspired prior_work prior work generate self supervised structural tasks e.g. navigate cell table evaluate performance differences contrast past work introduce noise operations inspired world messy data adversarial inputs operations llm performance formats different structural understandingtasks"}
{"id": "nan", "abstract": "  Despite their strong performance on many tasks, pre-trained language modelshave been shown to struggle on out-of-distribution compositionalgeneralization. Meanwhile, recent work has shown considerable improvements onmany NLP tasks from model scaling. Can scaling up model size also improvecompositional generalization in semantic parsing? We evaluate encoder-decodermodels up to 11B parameters and decoder-only models up to 540B parameters, andcompare model scaling curves for three different methods for applying apre-trained language model to a new task: fine-tuning all parameters, prompttuning, and in-context learning. We observe that fine-tuning generally has flator negative scaling curves on out-of-distribution compositional generalizationin semantic parsing evaluations. In-context learning has positive scalingcurves, but is generally outperformed by much smaller fine-tuned models.Prompt-tuning can outperform fine-tuning, suggesting further potentialimprovements from scaling as it exhibits a more positive scaling curve.Additionally, we identify several error trends that vary with model scale. Forexample, larger models are generally better at modeling the syntax of theoutput space, but are also more prone to certain types of overfitting. Overall,our study highlights limitations of current techniques for effectivelyleveraging model scale for compositional generalization, while our analysisalso suggests promising directions for future work.", "title": "evaluating the impact of model scale for compositional generalization in semantic parsing", "url": "http://arxiv.org/pdf/2205.12253v2.pdf", "tokenized_text": "despite strong performance tasks pre trained_language trained language modelshave shown struggle distribution compositionalgeneralization recent_work recent work shown considerable improvements onmany nlp_tasks nlp tasks scaling scaling model_size size improvecompositional generalization semantic_parsing semantic parsing evaluate encoder 11b parameters decoder 540b parameters andcompare scaling curves different methods applying apre trained_language trained language new task fine tuning parameters prompttuning context_learning context learning observe fine tuning generally negative scaling curves distribution compositional semantic_parsing semantic parsing evaluations context_learning context learning positive generally outperformed smaller fine tuned tuning outperform fine tuning suggesting scaling exhibits positive scaling curve additionally identify error trends vary scale forexample larger generally better modeling syntax theoutput space prone certain types overfitting overall study highlights limitations current techniques scale compositional generalization suggests promising directions future work"}
{"id": "nan", "abstract": "  Building dialogue systems requires a large corpus of annotated dialogues.Such datasets are usually created via crowdsourcing, which is expensive andtime-consuming. In this paper, we propose \\textsc{Dialogic}, a novel dialoguesimulation method based on large language model in-context learning to automatedataset creation. Seeded with a few annotated dialogues, \\textsc{Dialogic}automatically selects in-context examples for demonstration and prompts GPT-3to generate new dialogues and annotations in a controllable way. Our method canrapidly expand a small set of dialogue data with minimum or zero \\textit{humaninvolvement} and \\textit{parameter update} and is thus much more cost-efficientand time-saving than crowdsourcing. Experimental results on the MultiWOZdataset demonstrate that training a model on the simulated dialogues leads toeven better performance than using the same amount of human-generated dialoguesunder the challenging low-resource settings, with as few as 85 dialogues as aseed. When enough data is available, our method can still serve as an effectivedata augmentation method. Human evaluation results also show that our simulateddialogues have near-human fluency and annotation accuracy. The code and dataare available at \\textbf{\\url{https://github.com/Leezekun/dialogic}}.", "title": "controllable dialogue simulation with incontext learning", "url": "http://arxiv.org/pdf/2210.04185v4.pdf", "tokenized_text": "building dialogue systems requires large corpus annotated dialogues datasets usually created crowdsourcing expensive andtime consuming paper propose novel method based large_language large language context_learning context learning creation annotated dialogues selects context_examples context examples demonstration generate new dialogues annotations controllable way method expand small set dialogue data minimum zero update cost time saving crowdsourcing experimental_results experimental results demonstrate training simulated dialogues leads better performance human generated challenging low resource settings 85 dialogues data available method serve augmentation method human evaluation results near human fluency annotation accuracy code dataare available leezekun"}
{"id": "nan", "abstract": "  In-context learning using large language models has recently shown surprisingresults for semantic parsing tasks such as Text-to-SQL translation. PromptingGPT-3 or Codex using several examples of question-SQL pairs can produceexcellent results, comparable to state-of-the-art finetuning-based models.However, existing work primarily focuses on English datasets, and it is unknownwhether large language models can serve as competitive semantic parsers forother languages. To bridge this gap, our work focuses on cross-lingualText-to-SQL semantic parsing for translating non-English utterances into SQLqueries based on an English schema. We consider a zero-shot transfer learningsetting with the assumption that we do not have any labeled examples in thetarget language (but have annotated examples in English). This work introducesthe XRICL framework, which learns to retrieve relevant English exemplars for agiven query to construct prompts. We also include global translation exemplarsfor a target language to facilitate the translation process for large languagemodels. To systematically evaluate our model, we construct two new benchmarkdatasets, XSpider and XKaggle-dbqa, which include questions in Chinese,Vietnamese, Farsi, and Hindi. Our experiments show that XRICL effectivelyleverages large pre-trained language models to outperform existing baselines.Data and code are publicly available at https://github.com/Impavidity/XRICL.", "title": "xricl crosslingual retrievalaugmented incontext learning for crosslingual texttosql semantic parsing", "url": "http://arxiv.org/pdf/2210.13693v1.pdf", "tokenized_text": "context_learning context learning large_language large language recently shown semantic_parsing semantic parsing tasks text sql translation codex examples question sql pairs results comparable state art finetuning based existing work primarily focuses english datasets large_language large language serve competitive semantic parsers languages bridge gap work focuses cross sql semantic_parsing semantic parsing translating non english utterances based english schema consider zero shot transfer learningsetting assumption labeled examples thetarget language annotated examples english work framework learns retrieve relevant english exemplars agiven query construct include global translation exemplarsfor target language facilitate translation process large_languagemodels large languagemodels systematically evaluate construct new benchmarkdatasets include questions chinese vietnamese hindi experiments large pre trained_language trained language outperform existing baselines data code publicly_available publicly available"}
{"id": "nan", "abstract": "  Large language models (LLMs) are capable to perform complex reasoning byin-context learning (ICL) when provided with a few input-output demonstrations(demos) and more powerful when intermediate reasoning steps (\"chain of thoughts(CoT)\") of the demos are given. Is it necessary to use multi-demo in ICL? Inthis paper, we study ICL using fewer demos for each test query on the tasksin~\\cite{wei2022chain}. Surprisingly, we do not observe significant degradationwhen using only one randomly chosen demo. To study this phenomenon, for eachtest query, we categorize demos into \"correct demos\" leading to the correctanswer, and \"wrong demos\" resulting in wrong answers. Our analysis reveals aninherent bias in those widely studied datasets: most demos are correct for amajority of test queries, which explains the good performance of using onerandom demo. Moreover, ICL (with and w/o CoT) using only one correct demosignificantly outperforms all-demo ICL adopted by most previous works,indicating the weakness of LLMs in finding correct demo(s) for input queries,which is difficult to evaluate on the biased datasets. Furthermore, we observea counterintuitive behavior of ICL using multi-demo, i.e., its accuracydegrades(improves) when given more correct(wrong) demos. This implies that ICLcan be easily misguided by interference among demos and their spuriouscorrelations. Our analyses highlight several fundamental challenges that needto be addressed in LLMs training, ICL, and benchmark design.", "title": "how many demonstrations do you need for incontext learning", "url": "http://arxiv.org/pdf/2303.08119v3.pdf", "tokenized_text": "large_language large language llms capable perform complex_reasoning complex reasoning context_learning context learning icl provided input output powerful intermediate reasoning_steps reasoning steps chain demos given necessary use multi demo icl inthis_paper inthis paper study icl fewer demos test query surprisingly observe significant randomly chosen demo study phenomenon query categorize demos correct demos leading wrong demos resulting wrong answers analysis reveals bias widely studied datasets demos correct test queries explains good performance demo icl cot correct outperforms demo icl adopted previous works indicating weakness llms finding correct input queries difficult evaluate biased datasets furthermore behavior icl multi demo i.e. given demos implies easily interference demos analyses highlight fundamental challenges needto addressed llms training icl benchmark design"}
{"id": "nan", "abstract": "  Deep neural networks have been critical in the task of Visual QuestionAnswering (VQA), with research traditionally focused on improving modelaccuracy. Recently, however, there has been a trend towards evaluating therobustness of these models against adversarial attacks. This involves assessingthe accuracy of VQA models under increasing levels of noise in the input, whichcan target either the image or the proposed query question, dubbed the mainquestion. However, there is currently a lack of proper analysis of this aspectof VQA. This work proposes a new method that utilizes semantically relatedquestions, referred to as basic questions, acting as noise to evaluate therobustness of VQA models. It is hypothesized that as the similarity of a basicquestion to the main question decreases, the level of noise increases. Togenerate a reasonable noise level for a given main question, a pool of basicquestions is ranked based on their similarity to the main question, and thisranking problem is cast as a LASSO optimization problem. Additionally, thiswork proposes a novel robustness measure, R_score, and two basic questiondatasets to standardize the analysis of VQA model robustness. The experimentalresults demonstrate that the proposed evaluation method effectively analyzesthe robustness of VQA models. Moreover, the experiments show that in-contextlearning with a chain of basic questions can enhance model accuracy.", "title": "improving visual question answering models through robustness analysis and incontext learning with a chain of basic questions", "url": "http://arxiv.org/pdf/2304.03147v1.pdf", "tokenized_text": "deep neural_networks neural networks critical task visual questionanswering vqa research traditionally focused improving recently trend evaluating therobustness adversarial attacks involves accuracy vqa increasing levels noise input whichcan target image proposed query question dubbed currently lack proper analysis vqa work proposes new method utilizes semantically referred basic questions acting noise evaluate therobustness vqa similarity main question decreases level noise increases togenerate reasonable noise level given main question pool ranked based similarity main question problem cast lasso optimization problem additionally thiswork proposes novel robustness measure basic standardize analysis vqa robustness experimentalresults demonstrate proposed evaluation method effectively robustness vqa experiments contextlearning chain basic questions enhance accuracy"}
{"id": "nan", "abstract": "  While large language models (LLMs) have been successfully applied to varioustasks, they still face challenges with hallucinations. Augmenting LLMs withdomain-specific tools such as database utilities can facilitate easier and moreprecise access to specialized knowledge. In this paper, we present GeneGPT, anovel method for teaching LLMs to use the Web APIs of the National Center forBiotechnology Information (NCBI) for answering genomics questions.Specifically, we prompt Codex to solve the GeneTuring tests with NCBI Web APIsby in-context learning and an augmented decoding algorithm that can detect andexecute API calls. Experimental results show that GeneGPT achievesstate-of-the-art performance on eight tasks in the GeneTuring benchmark with anaverage score of 0.83, largely surpassing retrieval-augmented LLMs such as thenew Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), aswell as GPT-3 (0.16) and ChatGPT (0.12). Our further analyses suggest that: (1)API demonstrations have good cross-task generalizability and are more usefulthan documentations for in-context learning; (2) GeneGPT can generalize tolonger chains of API calls and answer multi-hop questions in GeneHop, a noveldataset introduced in this work; (3) Different types of errors are enriched indifferent tasks, providing valuable insights for future improvements.", "title": "genegpt augmenting large language models with domain tools for improved access to biomedical information", "url": "http://arxiv.org/pdf/2304.09667v3.pdf", "tokenized_text": "large_language large language llms successfully applied varioustasks face challenges hallucinations augmenting llms specific tools database utilities facilitate easier access specialized knowledge paper present anovel method teaching llms use web apis national center information answering genomics questions specifically codex solve tests web context_learning context learning augmented decoding algorithm detect api calls experimental_results experimental results achievesstate art performance tasks benchmark anaverage score 0.83 largely surpassing retrieval augmented llms bing biomedical llms 0.08 0.04 aswell gpt-3 chatgpt analyses suggest demonstrations good cross task generalizability documentations context_learning context learning generalize tolonger chains api calls answer multi hop questions introduced work different types errors enriched indifferent tasks providing valuable insights future improvements"}
{"id": "nan", "abstract": "  There is currently a significant gap between the performance of fine-tunedmodels and prompting approaches using Large Language Models (LLMs) on thechallenging task of text-to-SQL, as evaluated on datasets such as Spider. Toimprove the performance of LLMs in the reasoning process, we study howdecomposing the task into smaller sub-tasks can be effective. In particular, weshow that breaking down the generation problem into sub-problems and feedingthe solutions of those sub-problems into LLMs can be an effective approach forsignificantly improving their performance. Our experiments with three LLMs showthat this approach consistently improves their simple few-shot performance byroughly 10%, pushing the accuracy of LLMs towards SOTA or surpassing it. On theholdout test set of Spider, the SOTA, in terms of execution accuracy, was 79.9and the new SOTA at the time of this writing using our approach is 85.3. Ourapproach with in-context learning beats many heavily fine-tuned models by atleast 5%. Additionally, when evaluated on the BIRD benchmark, our approachachieved an execution accuracy of 55.9%, setting a new SOTA on its holdout testset.", "title": "dinsql decomposed incontext learning of texttosql with selfcorrection", "url": "http://arxiv.org/pdf/2304.11015v3.pdf", "tokenized_text": "currently significant gap performance fine tunedmodels approaches large_language large language llms task text sql evaluated datasets spider toimprove performance llms reasoning process study task smaller sub tasks effective particular weshow breaking generation problem sub problems solutions sub problems llms effective approach improving performance experiments llms showthat approach consistently improves simple shot performance 10 pushing accuracy llms sota surpassing test set spider sota terms execution accuracy new sota time writing approach ourapproach context_learning context learning beats heavily fine tuned atleast additionally evaluated bird benchmark approachachieved execution accuracy setting new sota"}
{"id": "nan", "abstract": "  Question answering over knowledge bases is considered a difficult problem dueto the challenge of generalizing to a wide variety of possible natural languagequestions. Additionally, the heterogeneity of knowledge base schema itemsbetween different knowledge bases often necessitates specialized training fordifferent knowledge base question-answering (KBQA) datasets. To handlequestions over diverse KBQA datasets with a unified training-free framework, wepropose KB-BINDER, which for the first time enables few-shot in-contextlearning over KBQA tasks. Firstly, KB-BINDER leverages large language modelslike Codex to generate logical forms as the draft for a specific question byimitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledgebase to bind the generated draft to an executable one with BM25 score matching.The experimental results on four public heterogeneous KBQA datasets show thatKB-BINDER can achieve a strong performance with only a few in-contextdemonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can evenoutperform the state-of-the-art trained models. On GrailQA and WebQSP, ourmodel is also on par with other fully-trained models. We believe KB-BINDER canserve as an important baseline for future research. Our code is available athttps://github.com/ltl3A87/KB-BINDER.", "title": "fewshot incontext learning for knowledge base question answering", "url": "http://arxiv.org/pdf/2305.01750v2.pdf", "tokenized_text": "question_answering question answering knowledge bases considered difficult problem dueto challenge generalizing wide variety possible natural languagequestions additionally heterogeneity knowledge base schema different knowledge bases necessitates specialized training fordifferent knowledge base question answering kbqa datasets diverse kbqa datasets unified training free framework wepropose kb time enables shot contextlearning kbqa tasks firstly kb leverages large_language large language modelslike codex generate logical forms draft specific question demonstrations secondly kb grounds knowledgebase generated draft executable bm25 score matching experimental_results experimental results public heterogeneous kbqa datasets achieve strong performance contextdemonstrations especially hop kb state art trained grailqa webqsp ourmodel par fully trained believe kb canserve important baseline future_research future research code_is_available code available"}
{"id": "nan", "abstract": "  Despite the remarkable success of large-scale Language Models (LLMs) such asGPT-3, their performances still significantly underperform fine-tuned models inthe task of text classification. This is due to (1) the lack of reasoningability in addressing complex linguistic phenomena (e.g., intensification,contrast, irony etc); (2) limited number of tokens allowed in in-contextlearning.  In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adoptsa progressive reasoning strategy tailored to addressing the complex linguisticphenomena involved in text classification: CARP first prompts LLMs to findsuperficial clues (e.g., keywords, tones, semantic relations, references, etc),based on which a diagnostic reasoning process is induced for final decisions.To further address the limited-token issue, CARP uses a fine-tuned model on thesupervised dataset for $k$NN demonstration search in the in-context learning,allowing the model to take the advantage of both LLM's generalization abilityand the task-specific evidence provided by the full labeled dataset.Remarkably, CARP yields new SOTA performances on 4 out of 5 widely-usedtext-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) onAGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performancecomparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARPdelivers impressive abilities on low-resource and domain-adaptation setups.Specifically, using 16 examples per class, CARP achieves comparableperformances to supervised models with 1,024 examples per class.", "title": "text classification via large language models", "url": "http://arxiv.org/pdf/2305.08377v3.pdf", "tokenized_text": "despite remarkable success large scale language_models language llms asgpt-3 performances significantly underperform fine tuned inthe task text_classification text classification lack reasoningability addressing complex linguistic phenomena e.g. contrast irony etc limited number tokens allowed contextlearning paper introduce reasoning carp carp progressive reasoning strategy tailored addressing complex involved text_classification text classification carp llms clues e.g. keywords semantic relations references diagnostic reasoning process induced final decisions address limited token issue carp uses fine tuned dataset k$nn demonstration search context_learning context learning allowing advantage llm generalization abilityand task specific evidence provided labeled dataset remarkably carp yields new sota performances widely classification benchmarks sst-2 sota importantly find impressive abilities low resource domain adaptation setups specifically 16 examples class carp achieves comparableperformances supervised examples class"}
{"id": "nan", "abstract": "  Knowledge graphs can represent information about the real-world usingentities and their relations in a structured and semantically rich manner andthey enable a variety of downstream applications such as question-answering,recommendation systems, semantic search, and advanced analytics. However, atthe moment, building a knowledge graph involves a lot of manual effort and thushinders their application in some situations and the automation of this processmight benefit especially for small organizations. Automatically generatingstructured knowledge graphs from a large volume of natural language is still achallenging task and the research on sub-tasks such as named entity extraction,relation extraction, entity and relation linking, and knowledge graphconstruction aims to improve the state of the art of automatic construction andcompletion of knowledge graphs from text. The recent advancement of foundationmodels with billions of parameters trained in a self-supervised manner withlarge volumes of training data that can be adapted to a variety of downstreamtasks has helped to demonstrate high performance on a large range of NaturalLanguage Processing (NLP) tasks. In this context, one emerging paradigm isin-context learning where a language model is used as it is with a prompt thatprovides instructions and some examples to perform a task without changing theparameters of the model using traditional approaches such as fine-tuning. Thisway, no computing resources are needed for re-training/fine-tuning the modelsand the engineering effort is minimal. Thus, it would be beneficial to utilizesuch capabilities for generating knowledge graphs from text.", "title": "exploring incontext learning capabilities of foundation models for generating knowledge graphs from text", "url": "http://arxiv.org/pdf/2305.08804v1.pdf", "tokenized_text": "knowledge graphs represent information real world relations structured semantically rich manner enable variety downstream applications question answering recommendation systems semantic search advanced analytics atthe moment building knowledge_graph knowledge graph involves lot manual effort application situations automation benefit especially small organizations automatically knowledge graphs large volume natural_language natural language achallenging task research sub tasks named_entity named entity extraction relation_extraction relation extraction entity relation linking knowledge aims improve state_of_the_art state art automatic construction knowledge graphs text recent advancement foundationmodels billions parameters trained self supervised manner withlarge volumes training_data training data adapted variety downstreamtasks helped demonstrate high performance large range naturallanguage_processing naturallanguage processing nlp tasks context emerging paradigm isin context_learning context learning language_model language thatprovides instructions examples perform task changing traditional approaches fine tuning computing resources needed training fine tuning modelsand engineering effort minimal beneficial capabilities generating knowledge graphs text"}
{"id": "nan", "abstract": "  Large language models (LLMs) exploit in-context learning (ICL) to solve taskswith only a few demonstrations, but its mechanisms are not yet well-understood.Some works suggest that LLMs only recall already learned concepts frompre-training, while others hint that ICL performs implicit learning overdemonstrations. We characterize two ways through which ICL leveragesdemonstrations. Task recognition (TR) captures the extent to which LLMs canrecognize a task through demonstrations -- even without ground-truth labels --and apply their pre-trained priors, whereas task learning (TL) is the abilityto capture new input-label mappings unseen in pre-training. Using a wide rangeof classification datasets and three LLM families (GPT-3, LLaMA and OPT), wedesign controlled experiments to disentangle the roles of TR and TL in ICL. Weshow that (1) models can achieve non-trivial performance with only TR, and TRdoes not further improve with larger models or more demonstrations; (2) LLMsacquire TL as the model scales, and TL's performance consistently improves withmore demonstrations in context. Our findings unravel two different forcesbehind ICL and we advocate for discriminating them in future ICL research dueto their distinct nature.", "title": "what incontext learning learns incontext disentangling task recognition and task learning", "url": "http://arxiv.org/pdf/2305.09731v1.pdf", "tokenized_text": "large_language large language llms exploit context_learning context learning icl solve taskswith demonstrations mechanisms understood works suggest llms recall learned concepts training hint icl performs implicit learning characterize ways icl task recognition captures extent llms task demonstrations ground truth labels apply pre trained priors task learning abilityto capture new input label mappings unseen pre training wide rangeof classification datasets llm families gpt-3 llama opt wedesign controlled experiments disentangle roles icl weshow achieve non trivial performance improve larger demonstrations scales performance consistently improves withmore demonstrations context findings different icl advocate future icl research dueto distinct nature"}
{"id": "nan", "abstract": "  Temporal knowledge graph (TKG) forecasting benchmarks challenge models topredict future facts using knowledge of past facts. In this paper, we applylarge language models (LLMs) to these benchmarks using in-context learning(ICL). We investigate whether and to what extent LLMs can be used for TKGforecasting, especially without any fine-tuning or explicit modules forcapturing structural and temporal information. For our experiments, we presenta framework that converts relevant historical facts into prompts and generatesranked predictions using token probabilities. Surprisingly, we observe thatLLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefullydesigned and trained for TKG forecasting. Our extensive evaluation presentsperformances across several models and datasets with different characteristics,compares alternative heuristics for preparing contextual information, andcontrasts to prominent TKG methods and simple frequency and recency baselines.We also discover that using numerical indices instead of entity/relation names,i.e., hiding semantic information, does not significantly affect theperformance ($\\pm$0.4\\% Hit@1). This shows that prior semantic knowledge isunnecessary; instead, LLMs can leverage the existing patterns in the context toachieve such performance. Our analysis also reveals that ICL enables LLMs tolearn irregular patterns from the historical context, going beyond simplepredictions based on common or recent information.", "title": "temporal knowledge graph forecasting without knowledge using incontext learning", "url": "http://arxiv.org/pdf/2305.10613v3.pdf", "tokenized_text": "temporal knowledge_graph knowledge graph forecasting benchmarks challenge topredict future facts knowledge past facts paper language_models language llms benchmarks context learning(icl investigate extent llms especially fine tuning explicit modules structural temporal information experiments presenta framework converts relevant historical facts predictions token probabilities surprisingly observe box perform par state art trained forecasting extensive evaluation datasets different characteristics compares alternative heuristics preparing contextual information andcontrasts prominent methods simple frequency recency baselines discover numerical instead entity relation names i.e. semantic information significantly affect theperformance shows prior semantic knowledge instead llms leverage existing patterns context toachieve performance analysis reveals icl enables llms irregular patterns historical context going based common recent information"}
{"id": "nan", "abstract": "  Named entity recognition in real-world applications suffers from thediversity of entity types, the emergence of new entity types, and the lack ofhigh-quality annotations. To address the above problems, this paper proposes anin-context learning-based NER approach, which can effectively inject in-contextNER ability into PLMs and recognize entities of novel types on-the-fly usingonly a few demonstrative instances. Specifically, we model PLMs as ameta-function $\\mathcal{ \\lambda_ {\\text{instruction, demonstrations, text}}.M}$, and a new entity extractor can be implicitly constructed by applying newinstruction and demonstrations to PLMs, i.e., $\\mathcal{ (\\lambda . M)}$(instruction, demonstrations) $\\to$ $\\mathcal{F}$ where $\\mathcal{F}$ will bea new entity extractor, i.e., $\\mathcal{F}$: text $\\to$ entities. To inject theabove in-context NER ability into PLMs, we propose a meta-function pre-trainingalgorithm, which pre-trains PLMs by comparing the (instruction,demonstration)-initialized extractor with a surrogate golden extractor.Experimental results on 4 few-shot NER datasets show that our method caneffectively inject in-context NER ability into PLMs and significantlyoutperforms the PLMs+fine-tuning counterparts.", "title": "learning incontext learning for named entity recognition", "url": "http://arxiv.org/pdf/2305.11038v3.pdf", "tokenized_text": "named_entity named entity recognition real world_applications world applications suffers entity types emergence new entity types lack ofhigh quality annotations address problems paper_proposes paper proposes anin context_learning context learning based ner approach effectively inject ability plms recognize entities novel types fly instances specifically plms ameta function demonstrations new entity extractor implicitly constructed applying demonstrations plms i.e. demonstrations bea new entity extractor i.e. text entities inject context ner ability plms propose meta function pre trainingalgorithm pre trains plms comparing instruction extractor golden extractor experimental_results experimental results shot ner datasets method caneffectively inject context ner ability plms significantlyoutperforms tuning counterparts"}
{"id": "nan", "abstract": "  The patient-centered medical dialogue systems strive to offer diagnosticinterpretation services to users who are less knowledgeable about medicalknowledge, through emphasizing the importance of providing responses specificto the patients. It is difficult for the large language models (LLMs) toguarantee the specificity of responses in spite of its promising performanceeven in some tasks in medical field. Inspired by in-context learning, wepropose PlugMed, a Plug-and-Play Medical Dialogue System, for addressing thischallenge. PlugMed is equipped with two modules, the prompt generation (PG)module and the response ranking (RR) module, to enhances LLMs' dialoguestrategies for improving the specificity of the dialogue. The PG module isdesigned to stimulate the imitative ability of LLMs by providing them with realdialogues from similar patients as prompts. The RR module incorporatesfine-tuned small model as response filter to enable the selection ofappropriate responses generated by LLMs. Furthermore, we introduce a newevaluation method based on matching both user's intent and high-frequencymedical term to effectively assess the specificity of the responses. We conductexperimental evaluations on three medical dialogue datasets, and the results,including both automatic and human evaluation, demonstrate the effectiveness ofour approach.", "title": "plugmed improving specificity in patientcentered medical dialogue generation using incontext learning", "url": "http://arxiv.org/pdf/2305.11508v2.pdf", "tokenized_text": "patient centered medical dialogue systems strive offer services users knowledgeable emphasizing importance providing responses patients difficult large_language large language llms specificity responses spite promising performanceeven tasks medical field inspired context_learning context learning wepropose plug play medical dialogue system addressing thischallenge equipped modules generation response ranking module enhances llms improving specificity dialogue module isdesigned stimulate ability llms providing similar patients module tuned small response filter enable selection responses generated llms furthermore introduce method based matching user intent high term effectively assess specificity responses evaluations medical dialogue datasets results including automatic human evaluation demonstrate_the_effectiveness demonstrate effectiveness ofour approach"}
{"id": "nan", "abstract": "  Augmenting large language models (LLMs) with external tools has emerged as apromising approach to solving complex problems. However, traditional methods,which finetune LLMs with tool demonstration data, can be both costly andrestricted to a predefined set of tools. Recent in-context learning paradigmalleviates these issues, but the limited context length only allows for a fewshots of demonstrations, leading to suboptimal understandings of the tools.Moreover, when there are numerous tools to choose from, in-context learningcould completely fail to work. In this paper, we propose an alternativeapproach, $\\textbf{ToolkenGPT}$, which combines the benefits of both sides. Ourapproach represents each $\\underline{tool}$ as a to$\\underline{ken}$($\\textit{toolken}$) and learns an embedding for it, enabling tool calls in thesame way as generating a regular word token. Once a toolken is triggered, theLLM is prompted to complete arguments for the tool to execute. ToolkenGPToffers the flexibility to plug in an arbitrary number of tools by expanding theset of toolkens on the fly. In addition, it improves tool use by allowingextensive demonstration data for learning the toolken embeddings. In diversedomains, including numerical reasoning, knowledge-based question answering, andembodied plan generation, our approach effectively augments LLMs with tools andsubstantially outperforms various latest baselines. ToolkenGPT demonstrates thepromising ability to use relevant tools from a large tool set in complexscenarios.", "title": "toolkengpt augmenting frozen language models with massive tools via tool embeddings", "url": "http://arxiv.org/pdf/2305.11554v3.pdf", "tokenized_text": "augmenting large_language large language llms external tools emerged apromising approach solving complex problems traditional methods finetune llms tool demonstration data costly predefined set tools recent context_learning context learning issues limited context length allows demonstrations leading suboptimal understandings tools numerous tools choose context completely fail work paper propose combines benefits ourapproach represents learns embedding enabling tool calls thesame way generating regular word token triggered thellm prompted complete arguments tool execute flexibility plug arbitrary number tools expanding fly addition improves tool use demonstration data learning embeddings including numerical reasoning knowledge based question_answering question answering plan generation approach effectively augments llms tools outperforms latest baselines demonstrates thepromising ability use relevant tools large tool set"}
{"id": "nan", "abstract": "  In-context learning (ICL) is an important paradigm for adapting largelanguage models (LLMs) to new tasks, but the generalization behavior of ICLremains poorly understood. We investigate the inductive biases of ICL from theperspective of feature bias: which feature ICL is more likely to use given aset of underspecified demonstrations in which two features are equallypredictive of the labels. First, we characterize the feature biases of GPT-3models by constructing underspecified demonstrations from a range of NLPdatasets and feature combinations. We find that LLMs exhibit clear featurebiases - for example, demonstrating a strong bias to predict labels accordingto sentiment rather than shallow lexical features, like punctuation. Second, weevaluate the effect of different interventions that are designed to impose aninductive bias in favor of a particular feature, such as adding a naturallanguage instruction or using semantically relevant label words. We find that,while many interventions can influence the learner to prefer a particularfeature, it can be difficult to overcome strong prior biases. Overall, ourresults provide a broader picture of the types of features that ICL may be morelikely to exploit and how to impose inductive biases that are better alignedwith the intended task.", "title": "measuring inductive biases of incontext learning with underspecified demonstrations", "url": "http://arxiv.org/pdf/2305.13299v1.pdf", "tokenized_text": "context_learning context learning icl important paradigm adapting largelanguage_models largelanguage llms new tasks generalization behavior poorly understood investigate inductive biases icl theperspective feature bias feature icl likely use given aset underspecified demonstrations features labels characterize feature biases constructing underspecified demonstrations range nlpdatasets feature combinations find llms exhibit clear example demonstrating strong bias predict labels accordingto sentiment shallow lexical features like second weevaluate effect different interventions designed impose bias particular feature adding naturallanguage instruction semantically relevant label words find interventions influence learner prefer difficult overcome strong prior biases overall ourresults provide broader picture types features icl exploit impose inductive biases better intended task"}
{"id": "nan", "abstract": "  Despite remarkable advancements in few-shot generalization in naturallanguage processing, most models are developed and evaluated primarily inEnglish. To facilitate research on few-shot cross-lingual transfer, weintroduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across54 languages in a sequence-to-sequence format and provides a fixed set offew-shot examples and instructions. BUFFET is designed to establish a rigorousand equitable evaluation framework for few-shot cross-lingual transfer across abroad range of tasks and languages. Using BUFFET, we perform thoroughevaluations of state-of-the-art multilingual large language models withdifferent transfer methods, namely in-context learning and fine-tuning. Ourfindings reveal significant room for improvement in few-shot in-contextcross-lingual transfer. In particular, ChatGPT with in-context learning oftenperforms worse than much smaller mT5-base models fine-tuned on English taskdata and few-shot in-language examples. Our analysis suggests various avenuesfor future research in few-shot cross-lingual transfer, such as improvedpretraining, understanding, and future evaluations.", "title": "buffet benchmarking large language models for fewshot crosslingual transfer", "url": "http://arxiv.org/pdf/2305.14857v1.pdf", "tokenized_text": "despite remarkable advancements shot generalization naturallanguage processing developed evaluated primarily inenglish facilitate research shot cross lingual_transfer lingual transfer weintroduce new benchmark called unifies 15 diverse tasks languages sequence sequence format provides fixed set offew shot examples instructions designed establish evaluation framework shot cross lingual_transfer lingual transfer range tasks languages perform state art multilingual large_language large language withdifferent transfer methods context_learning context learning fine tuning ourfindings reveal significant room improvement shot lingual_transfer lingual transfer particular chatgpt context_learning context learning worse smaller mt5 base fine tuned english shot language examples analysis suggests future_research future research shot cross lingual_transfer lingual transfer understanding future evaluations"}
{"id": "nan", "abstract": "  In executable task-oriented semantic parsing, the system aims to translateusers' utterances in natural language to machine-interpretable programs (APIcalls) that can be executed according to pre-defined API specifications. Withthe popularity of Large Language Models (LLMs), in-context learning offers astrong baseline for such scenarios, especially in data-limited regimes.However, LLMs are known to hallucinate and therefore pose a formidablechallenge in constraining generated content. Thus, it remains uncertain if LLMscan effectively perform task-oriented utterance-to-API generation whererespecting API's structural and task-specific constraints is crucial.  In this work, we seek to measure, analyze and mitigate such constraintsviolations. First, we identify the categories of various constraints inobtaining API-semantics from task-oriented utterances, and define fine-grainedmetrics that complement traditional ones. Second, we leverage these metrics toconduct a detailed error analysis of constraints violations seen instate-of-the-art LLMs, which motivates us to investigate two mitigationstrategies: Semantic-Retrieval of Demonstrations (SRD) and API-awareConstrained Decoding (API-CD). Our experiments show that these strategies areeffective at reducing constraints violations and improving the quality of thegenerated API calls, but require careful consideration given theirimplementation complexity and latency.", "title": "measuring and mitigating constraint violations of incontext learning for utterancetoapi semantic parsing", "url": "http://arxiv.org/pdf/2305.15338v1.pdf", "tokenized_text": "executable task oriented semantic_parsing semantic parsing system aims utterances natural_language natural language machine interpretable programs executed according pre defined api specifications withthe popularity large_language large language llms context_learning context learning offers astrong baseline scenarios especially data limited regimes llms known hallucinate pose formidablechallenge generated content remains uncertain llmscan effectively perform task oriented utterance api generation api structural task specific constraints crucial work seek measure analyze mitigate identify categories constraints api semantics task oriented utterances define fine complement traditional ones second leverage metrics toconduct detailed error analysis constraints violations seen art llms motivates investigate demonstrations decoding experiments strategies reducing constraints violations improving quality thegenerated api calls require careful consideration given complexity latency"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) with strong abilities in natural languageprocessing tasks have emerged and have been applied in various kinds of areassuch as science, finance and software engineering. However, the capability ofLLMs to advance the field of chemistry remains unclear. In this paper, ratherthan pursuing state-of-the-art performance, we aim to evaluate capabilities ofLLMs in a wide range of tasks across the chemistry domain. We identify threekey chemistry-related capabilities including understanding, reasoning andexplaining to explore in LLMs and establish a benchmark containing eightchemistry tasks. Our analysis draws on widely recognized datasets facilitatinga broad exploration of the capacities of LLMs within the context of practicalchemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) areevaluated for each chemistry task in zero-shot and few-shot in-context learningsettings with carefully selected demonstration examples and specially craftedprompts. Our investigation found that GPT-4 outperformed other models and LLMsexhibit different competitive levels in eight chemistry tasks. In addition tothe key findings from the comprehensive benchmark analysis, our work providesinsights into the limitation of current LLMs and the impact of in-contextlearning settings on LLMs' performance across various chemistry tasks. The codeand datasets used in this study are available athttps://github.com/ChemFoundationModels/ChemLLMBench.", "title": "what can large language models do in chemistry a comprehensive benchmark on eight tasks", "url": "http://arxiv.org/pdf/2305.18365v2.pdf", "tokenized_text": "large_language large language llms strong abilities natural languageprocessing tasks emerged applied kinds science finance software engineering capability ofllms advance field chemistry remains unclear paper ratherthan pursuing state art performance aim evaluate capabilities ofllms wide_range wide range tasks chemistry domain identify chemistry related capabilities including understanding reasoning explore llms establish benchmark containing tasks analysis draws widely recognized datasets broad exploration capacities llms context llms gpt-4 gpt-3.5 davinci-003 llama areevaluated chemistry task zero shot shot context learningsettings carefully selected demonstration examples specially craftedprompts investigation found gpt-4 outperformed different competitive levels chemistry tasks addition tothe key findings comprehensive benchmark analysis work limitation current llms impact contextlearning settings llms performance chemistry tasks codeand datasets study available"}
{"id": "nan", "abstract": "  Various design settings for in-context learning (ICL), such as the choice andorder of the in-context examples, can bias a model toward a particularprediction without being reflective of an understanding of the task. While manystudies discuss these design choices, there have been few systematicinvestigations into categorizing them and mitigating their impact. In thiswork, we define a typology for three types of label biases in ICL for textclassification: vanilla-label bias, context-label bias, and domain-label bias(which we conceptualize and detect for the first time).  Our analysis demonstrates that prior label bias calibration methods fallshort of addressing all three types of biases. Specifically, domain-label biasrestricts LLMs to random-level performance on many tasks regardless of thechoice of in-context examples. To mitigate the effect of these biases, wepropose a simple bias calibration method that estimates a language model'slabel bias using random in-domain words from the task corpus. After controllingfor this estimated bias when making predictions, our novel domain-contextcalibration significantly improves the ICL performance of GPT-J and GPT-3 on awide range of tasks. The gain is substantial on tasks with large domain-labelbias (up to 37% in Macro-F1). Furthermore, our results generalize to modelswith different scales, pretraining methods, and manually-designed taskinstructions, showing the prevalence of label biases in ICL.", "title": "mitigating label biases for incontext learning", "url": "http://arxiv.org/pdf/2305.19148v3.pdf", "tokenized_text": "design settings context_learning context learning icl choice context_examples context examples bias reflective understanding task discuss design choices categorizing mitigating impact thiswork define typology types label biases icl textclassification vanilla label bias context label bias domain label detect time analysis demonstrates prior label bias calibration methods addressing types biases specifically domain label llms random level performance tasks regardless thechoice context_examples context examples mitigate effect biases wepropose simple bias calibration method estimates language bias random domain words task corpus estimated bias making predictions novel domain significantly improves icl performance gpt gpt-3 awide range tasks gain substantial tasks large domain 37 macro f1 furthermore results generalize modelswith different scales pretraining methods manually designed taskinstructions showing prevalence label biases icl"}
{"id": "nan", "abstract": "  Pretrained transformers exhibit the remarkable ability of in-context learning(ICL): they can learn tasks from just a few examples provided in the promptwithout updating any weights. This raises a foundational question: can ICLsolve fundamentally $\\textit{new}$ tasks that are very different from thoseseen during pretraining? To probe this question, we examine ICL's performanceon linear regression while varying the diversity of tasks in the pretrainingdataset. We empirically demonstrate a $\\textit{task diversity threshold}$ forthe emergence of ICL. Below this threshold, the pretrained transformer cannotsolve unseen regression tasks, instead behaving like a Bayesian estimator withthe $\\textit{non-diverse pretraining task distribution}$ as the prior. Beyondthis threshold, the transformer significantly outperforms this estimator; itsbehavior aligns with that of ridge regression, corresponding to a Gaussianprior over $\\textit{all tasks}$, including those not seen during pretraining.Thus, when pretrained on data with task diversity greater than the threshold,transformers $\\textit{can}$ optimally solve fundamentally new tasks in-context.Importantly, this capability hinges on it deviating from the Bayes optimalestimator with the pretraining distribution as the prior. This study alsoexplores the effect of regularization, model capacity and task structure andunderscores, in a concrete example, the critical role of task diversity,alongside data and model scale, in the emergence of ICL. Code is available athttps://github.com/mansheej/icl-task-diversity.", "title": "pretraining task diversity and the emergence of nonbayesian incontext learning for regression", "url": "http://arxiv.org/pdf/2306.15063v2.pdf", "tokenized_text": "pretrained transformers exhibit remarkable ability context learning(icl learn tasks examples provided updating weights raises foundational question fundamentally tasks different pretraining probe question examine icl performanceon linear regression varying diversity tasks empirically demonstrate diversity forthe emergence icl threshold pretrained transformer unseen regression tasks instead behaving like bayesian estimator withthe diverse pretraining task prior threshold transformer significantly_outperforms significantly outperforms estimator aligns ridge regression corresponding gaussianprior including seen pretraining pretrained data task diversity greater threshold transformers optimally solve fundamentally new tasks context importantly capability hinges bayes pretraining distribution prior study effect regularization capacity task structure concrete example critical role task diversity alongside data scale emergence icl code_is_available code available"}
{"id": "nan", "abstract": "  In-context learning (ICL) improves language models' performance on a varietyof NLP tasks by simply demonstrating a handful of examples at inference time.It is not well understood why ICL ability emerges, as the model has never beenspecifically trained on such demonstrations. Unlike prior work that exploresimplicit mechanisms behind ICL, we study ICL via investigating the pretrainingdata. Specifically, we first adapt an iterative, gradient-based approach tofind a small subset of pretraining data that supports ICL. We observe that acontinued pretraining on this small subset significantly improves the model'sICL ability, by up to 18%. We then compare the supportive subset constrastivelywith random subsets of pretraining data and discover: (1) The supportivepretraining data to ICL do not have a higher domain relevance to downstreamtasks. (2) The supportive pretraining data have a higher mass of rarelyoccurring, long-tail tokens. (3) The supportive pretraining data arechallenging examples where the information gain from long-range context isbelow average, indicating learning to incorporate difficult long-range contextencourages ICL. Our work takes a first step towards understanding ICL viaanalyzing instance-level pretraining data. Our insights have a potential toenhance the ICL ability of language models by actively guiding the constructionof pretraining data in the future.", "title": "understanding incontext learning via supportive pretraining data", "url": "http://arxiv.org/pdf/2306.15091v1.pdf", "tokenized_text": "context_learning context learning icl improves language_models language performance varietyof nlp_tasks nlp tasks simply demonstrating handful examples inference time understood icl ability emerges trained demonstrations unlike prior_work prior work mechanisms icl study icl investigating specifically adapt iterative gradient based approach tofind small subset pretraining data supports icl observe pretraining small subset significantly improves ability 18 compare supportive subset random subsets pretraining data discover data icl higher domain relevance downstreamtasks supportive pretraining data higher mass long tail tokens supportive pretraining data arechallenging examples information gain long range context average indicating learning incorporate difficult long range icl work takes step understanding icl instance level pretraining data insights potential toenhance icl ability language_models language actively guiding pretraining data future"}
{"id": "nan", "abstract": "  In-context learning (ICL) is one of the most powerful and most unexpectedcapabilities to emerge in recent transformer-based large language models(LLMs). Yet the mechanisms that underlie it are poorly understood. In thispaper, we demonstrate that comparable ICL capabilities can be acquired by analternative sequence prediction learning method using clone-structured causalgraphs (CSCGs). Moreover, a key property of CSCGs is that, unliketransformer-based LLMs, they are {\\em interpretable}, which considerablysimplifies the task of explaining how ICL works. Specifically, we show that ituses a combination of (a) learning template (schema) circuits for patterncompletion, (b) retrieving relevant templates in a context-sensitive manner,and (c) rebinding of novel tokens to appropriate slots in the templates. We goon to marshall evidence for the hypothesis that similar mechanisms underlie ICLin LLMs. For example, we find that, with CSCGs as with LLMs, differentcapabilities emerge at different levels of overparameterization, suggestingthat overparameterization helps in learning more complex template (schema)circuits. By showing how ICL can be achieved with small models and datasets, weopen up a path to novel architectures, and take a vital step towards a moregeneral understanding of the mechanics behind this important capability.", "title": "schemalearning and rebinding as mechanisms of incontext learning and emergence", "url": "http://arxiv.org/pdf/2307.01201v1.pdf", "tokenized_text": "context_learning context learning icl powerful emerge recent transformer based large_language large language models(llms mechanisms underlie poorly understood thispaper demonstrate comparable icl capabilities acquired analternative sequence prediction learning method clone structured key property based llms interpretable task explaining icl works specifically ituses combination learning template schema circuits patterncompletion retrieving relevant templates context sensitive manner novel tokens appropriate slots templates evidence hypothesis similar mechanisms underlie llms example find llms emerge different levels suggestingthat helps learning complex template showing icl achieved small datasets weopen path novel architectures vital step moregeneral understanding mechanics important capability"}
{"id": "nan", "abstract": "  We investigate the role of various demonstration components in the in-contextlearning (ICL) performance of large language models (LLMs). Specifically, weexplore the impacts of ground-truth labels, input distribution, andcomplementary explanations, particularly when these are altered or perturbed.We build on previous work, which offers mixed findings on how these elementsinfluence ICL. To probe these questions, we employ explainable NLP (XNLP)methods and utilize saliency maps of contrastive demonstrations for bothqualitative and quantitative analysis. Our findings reveal that flippingground-truth labels significantly affects the saliency, though it's morenoticeable in larger LLMs. Our analysis of the input distribution at a granularlevel reveals that changing sentiment-indicative terms in a sentiment analysistask to neutral ones does not have as substantial an impact as alteringground-truth labels. Finally, we find that the effectiveness of complementaryexplanations in boosting ICL performance is task-dependent, with limitedbenefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.These insights are critical for understanding the functionality of LLMs andguiding the development of effective demonstrations, which is increasinglyrelevant in light of the growing use of LLMs in applications such as ChatGPT.Our research code is publicly available at https://github.com/paihengxu/XICL.", "title": "towards understanding incontext learning with contrastive demonstrations and saliency maps", "url": "http://arxiv.org/pdf/2307.05052v1.pdf", "tokenized_text": "investigate role demonstration components contextlearning icl performance large_language large language llms specifically weexplore impacts ground truth labels input distribution andcomplementary explanations particularly altered perturbed build previous work offers mixed findings icl probe questions employ explainable nlp utilize saliency maps contrastive demonstrations quantitative analysis findings reveal truth labels significantly affects saliency larger llms analysis input distribution reveals changing sentiment indicative terms sentiment neutral ones substantial impact truth labels finally find effectiveness boosting icl performance task dependent seen sentiment_analysis sentiment analysis tasks compared symbolic reasoning tasks insights critical understanding functionality llms andguiding development effective demonstrations light growing use llms applications research code publicly_available publicly available"}
{"id": "nan", "abstract": "  In traditional system identification, we estimate a model of an unknowndynamical system based on given input/output sequences and available physicalknowledge. Yet, is it also possible to understand the intricacies of dynamicalsystems not solely from their input/output patterns, but by observing thebehavior of other systems within the same class? This central question drivesthe study presented in this paper.  In response to this query, we introduce a novel paradigm for systemidentification, addressing two primary tasks: one-step-ahead prediction andmulti-step simulation. Unlike conventional methods, we do not directly estimatea model for the specific system. Instead, we pretrain a meta model thatrepresents a class of dynamical systems. This meta model is trained from apotentially infinite stream of synthetic data, generated by systems randomlyextracted from a certain distribution. At its core, the meta model serves as animplicit representation of the main characteristics of a class of dynamicalsystems. When provided with a brief context from a new system - specifically, ashort input/output sequence - the meta model implicitly discerns its dynamics,enabling predictions of its behavior.  The proposed approach harnesses the power of Transformer architectures,renowned for their in-context learning capabilities in Natural LanguageProcessing tasks. For one-step prediction, a GPT-like decoder-only architectureis utilized, whereas the simulation problem employs an encoder-decoderstructure.  Initial experimental results affirmatively answer our foundational question,opening doors to fresh research avenues in system identification.", "title": "incontext learning for modelfree system identification", "url": "http://arxiv.org/pdf/2308.13380v1.pdf", "tokenized_text": "traditional system identification estimate system based given input output sequences available physicalknowledge possible understand intricacies solely input output patterns observing thebehavior systems class central question study presented paper response query introduce novel paradigm addressing primary tasks step ahead prediction andmulti step simulation unlike conventional methods directly specific system instead pretrain meta class dynamical systems meta trained infinite stream synthetic data generated systems certain distribution core meta serves representation main characteristics class provided brief context new system specifically input output sequence meta implicitly dynamics enabling predictions behavior proposed approach harnesses power transformer architectures renowned context_learning context learning capabilities natural languageprocessing tasks step prediction gpt like decoder utilized simulation problem employs encoder initial experimental_results experimental results answer foundational question opening doors fresh research avenues system identification"}
{"id": "nan", "abstract": "  In-context learning (ICL) i.e. showing LLMs only a few task-specificdemonstrations has led to downstream gains with no task-specific fine-tuningrequired. However, LLMs are sensitive to the choice of prompts, and therefore acrucial research question is how to select good demonstrations for ICL. Oneeffective strategy is leveraging semantic similarity between the ICLdemonstrations and test inputs by using a text retriever, which however issub-optimal as that does not consider the LLM's existing knowledge about thattask. From prior work (Min et al., 2022), we already know that labels pairedwith the demonstrations bias the model predictions. This leads us to ourhypothesis whether considering LLM's existing knowledge about the task,especially with respect to the output label space can help in a betterdemonstration selection strategy. Through extensive experimentation on threetext classification tasks, we find that it is beneficial to not only choosesemantically similar ICL demonstrations but also to choose those demonstrationsthat help resolve the inherent label ambiguity surrounding the test example.Interestingly, we find that including demonstrations that the LLM previouslymis-classified and also fall on the test example's decision boundary, bringsthe most performance gain.", "title": "ambiguityaware incontext learning with large language models", "url": "http://arxiv.org/pdf/2309.07900v1.pdf", "tokenized_text": "context_learning context learning icl i.e. showing llms task led downstream gains task specific fine llms sensitive choice acrucial research question select good demonstrations icl strategy leveraging semantic similarity test inputs text retriever optimal consider llm existing knowledge prior_work prior work min et_al et al 2022 know labels demonstrations bias predictions leads considering llm existing knowledge task especially respect output label space help selection strategy extensive experimentation classification tasks find beneficial similar icl demonstrations choose help resolve inherent label ambiguity surrounding test example interestingly find including demonstrations llm classified fall test example decision boundary performance gain"}
{"id": "nan", "abstract": "  Following the success of Large Language Models (LLMs), Large MultimodalModels (LMMs), such as the Flamingo model and its subsequent competitors, havestarted to emerge as natural steps towards generalist agents. However,interacting with recent LMMs reveals major limitations that are hardly capturedby the current evaluation benchmarks. Indeed, task performances (e.g., VQAaccuracy) alone do not provide enough clues to understand their realcapabilities, limitations, and to which extent such models are aligned to humanexpectations. To refine our understanding of those flaws, we deviate from thecurrent evaluation paradigm and propose the EvALign-ICL framework, in which we(1) evaluate 8 recent open-source LMMs (based on the Flamingo architecture suchas OpenFlamingo and IDEFICS) on 5 different axes; hallucinations, abstention,compositionality, explainability and instruction following. Our evaluation onthese axes reveals major flaws in LMMs. To efficiently address these problems,and inspired by the success of in-context learning (ICL) in LLMs, (2) weexplore ICL as a solution and study how it affects these limitations. Based onour ICL study, (3) we push ICL further and propose new multimodal ICLapproaches such as; Multitask-ICL, Chain-of-Hindsight-ICL, andSelf-Correcting-ICL. Our findings are as follows; (1) Despite their success,LMMs have flaws that remain unsolved with scaling alone. (2) The effect of ICLon LMMs flaws is nuanced; despite its effectiveness for improvedexplainability, abstention, and instruction following, ICL does not improvecompositional abilities, and actually even amplifies hallucinations. (3) Theproposed ICL variants are promising as post-hoc approaches to efficientlytackle some of those flaws. The code is available here:https://evalign-icl.github.io/", "title": "beyond task performance evaluating and reducing the flaws of large multimodal models with incontext learning", "url": "http://arxiv.org/pdf/2310.00647v1.pdf", "tokenized_text": "following success large_language large language llms large lmms flamingo subsequent emerge natural steps generalist agents interacting recent lmms reveals major limitations hardly current evaluation benchmarks task performances e.g. provide clues understand limitations extent aligned refine understanding flaws deviate thecurrent evaluation paradigm propose icl framework evaluate recent open source lmms based flamingo architecture suchas openflamingo idefics different axes hallucinations abstention compositionality explainability instruction_following instruction following evaluation axes reveals major flaws lmms efficiently address problems inspired success context_learning context learning icl llms weexplore icl solution study affects limitations based icl study push icl propose new multimodal multitask icl chain icl correcting icl findings follows despite success lmms flaws remain unsolved scaling effect lmms flaws nuanced despite effectiveness abstention instruction_following instruction following icl improvecompositional abilities actually amplifies hallucinations theproposed icl variants promising post hoc approaches flaws code_is_available code available"}
{"id": "nan", "abstract": "  In order to understand the in-context learning phenomenon, recent works haveadopted a stylized experimental framework and demonstrated that Transformerscan learn gradient-based learning algorithms for various classes of real-valuedfunctions. However, the limitations of Transformers in implementing learningalgorithms, and their ability to learn other forms of algorithms are not wellunderstood. Additionally, the degree to which these capabilities are confinedto attention-based models is unclear. Furthermore, it remains to be seenwhether the insights derived from these stylized settings can be extrapolatedto pretrained Large Language Models (LLMs). In this work, we take a steptowards answering these questions by demonstrating the following: (a) On atest-bed with a variety of Boolean function classes, we find that Transformerscan nearly match the optimal learning algorithm for 'simpler' tasks, whiletheir performance deteriorates on more 'complex' tasks. Additionally, we findthat certain attention-free models perform (almost) identically to Transformerson a range of tasks. (b) When provided a teaching sequence, i.e. a set ofexamples that uniquely identifies a function in a class, we show thatTransformers learn more sample-efficiently. Interestingly, our results showthat Transformers can learn to implement two distinct algorithms to solve asingle task, and can adaptively select the more sample-efficient algorithmdepending on the sequence of in-context examples. (c) Lastly, we show thatextant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselineson prediction tasks that are guaranteed to not be in their training set.", "title": "understanding incontext learning in transformers and llms by learning to learn discrete functions", "url": "http://arxiv.org/pdf/2310.03016v1.pdf", "tokenized_text": "order understand context_learning context learning phenomenon recent works stylized experimental framework demonstrated learn gradient based learning algorithms classes real limitations transformers implementing learningalgorithms ability learn forms algorithms additionally degree capabilities attention based unclear furthermore remains insights derived stylized settings pretrained large_language large language llms work steptowards answering questions demonstrating following atest bed variety function classes find nearly match optimal learning algorithm simpler tasks performance complex tasks additionally findthat certain attention free perform identically range tasks provided teaching sequence i.e. set ofexamples identifies function class thattransformers learn sample efficiently interestingly results showthat transformers learn implement distinct algorithms solve asingle task adaptively select sample efficient sequence context_examples context examples lastly llms e.g. llama-2 gpt-4 compete nearest neighbor baselineson prediction tasks guaranteed training set"}
{"id": "nan", "abstract": "  Paraphrasing of offensive content is a better alternative to content removaland helps improve civility in a communication environment. Supervisedparaphrasers; however, rely heavily on large quantities of labelled data tohelp preserve meaning and intent. They also retain a large portion of theoffensiveness of the original content, which raises questions on their overallusability. In this paper we aim to assist practitioners in developing usableparaphrasers by exploring In-Context Learning (ICL) with large language models(LLMs), i.e., using a limited number of input-label demonstration pairs toguide the model in generating desired outputs for specific queries. Our studyfocuses on key factors such as -- number and order of demonstrations, exclusionof prompt instruction, and reduction in measured toxicity. We performprincipled evaluation on three datasets, including our proposed Context-AwarePolite Paraphrase dataset, comprising of dialogue-style rude utterances, politeparaphrases, and additional dialogue context. We evaluate our approach usingtwo closed source and one open source LLM. Our results reveal that ICL iscomparable to supervised methods in generation quality, while beingqualitatively better by 25% on human evaluation and attaining lower toxicity by76%. Also, ICL-based paraphrasers only show a slight reduction in performanceeven with just 10% training data.", "title": "demonstrations are all you need advancing offensive content paraphrasing using incontext learning", "url": "http://arxiv.org/pdf/2310.10707v1.pdf", "tokenized_text": "paraphrasing offensive content better alternative content helps improve communication environment rely heavily large quantities labelled data preserve meaning intent retain large portion original content raises questions paper aim assist practitioners developing exploring context_learning context learning icl large_language large language models(llms i.e. limited number input label demonstration pairs toguide generating desired outputs specific queries key factors number order demonstrations instruction reduction measured toxicity evaluation datasets including proposed paraphrase dataset comprising dialogue style utterances additional dialogue context evaluate approach closed source open_source open source llm results reveal icl iscomparable supervised methods generation quality better 25 human evaluation attaining lower toxicity icl based slight reduction performanceeven 10 training_data training data"}
{"id": "nan", "abstract": "  Transformer models, notably large language models (LLMs), have the remarkableability to perform in-context learning (ICL) -- to perform new tasks whenprompted with unseen input-output examples without any explicit model training.In this work, we study how effectively transformers can bridge between theirpretraining data mixture, comprised of multiple distinct task families, toidentify and learn new tasks in-context which are both inside and outside thepretraining distribution. Building on previous work, we investigate thisquestion in a controlled setting, where we study transformer models trained onsequences of $(x, f(x))$ pairs rather than natural language. Our empiricalresults show transformers demonstrate near-optimal unsupervised model selectioncapabilities, in their ability to first in-context identify different taskfamilies and in-context learn within them when the task families arewell-represented in their pretraining data. However when presented with tasksor functions which are out-of-domain of their pretraining data, we demonstratevarious failure modes of transformers and degradation of their generalizationfor even simple extrapolation tasks. Together our results highlight that theimpressive ICL abilities of high-capacity sequence models may be more closelytied to the coverage of their pretraining data mixtures than inductive biasesthat create fundamental generalization capabilities.", "title": "pretraining data mixtures enable narrow model selection capabilities in transformer models", "url": "http://arxiv.org/pdf/2311.00871v1.pdf", "tokenized_text": "transformer notably large_language large language llms remarkableability perform context_learning context learning icl perform new tasks whenprompted unseen input output examples explicit training work study effectively transformers bridge data mixture comprised multiple distinct task families toidentify learn new tasks context inside outside thepretraining distribution building previous work investigate controlled setting study transformer trained f(x))$ pairs natural_language natural language empiricalresults transformers demonstrate near optimal unsupervised selectioncapabilities ability context identify different context learn task families arewell represented pretraining data presented functions domain pretraining data failure modes transformers degradation simple extrapolation tasks results highlight theimpressive icl abilities high capacity sequence coverage pretraining data mixtures inductive create fundamental generalization capabilities"}
{"id": "nan", "abstract": "  Code comment generation aims at generating natural language descriptions fora code snippet to facilitate developers' program comprehension activities.Despite being studied for a long time, a bottleneck for existing approaches isthat given a code snippet, they can only generate one comment while developersusually need to know information from diverse perspectives such as what is thefunctionality of this code snippet and how to use it. To tackle thislimitation, this study empirically investigates the feasibility of utilizinglarge language models (LLMs) to generate comments that can fulfill developers'diverse intents. Our intuition is based on the facts that (1) the code and itspairwise comment are used during the pre-training process of LLMs to build thesemantic connection between the natural language and programming language, and(2) comments in the real-world projects, which are collected for thepre-training, usually contain different developers' intents. We thus postulatethat the LLMs can already understand the code from different perspectives afterthe pre-training. Indeed, experiments on two large-scale datasets demonstratethe rationale of our insights: by adopting the in-context learning paradigm andgiving adequate prompts to the LLM (e.g., providing it with ten or moreexamples), the LLM can significantly outperform a state-of-the-art supervisedlearning approach on generating comments with multiple intents. Results alsoshow that customized strategies for constructing the prompts andpost-processing strategies for reranking the results can both boost the LLM'sperformances, which shed light on future research directions for using LLMs toachieve comment generation.", "title": "large language models are fewshot summarizers multiintent comment generation via incontext learning", "url": "http://arxiv.org/pdf/2304.11384v3.pdf", "tokenized_text": "code comment generation aims generating natural_language natural language descriptions fora code snippet facilitate developers program comprehension activities despite studied long time bottleneck existing approaches given code snippet generate comment need know information diverse perspectives code snippet use tackle thislimitation study empirically investigates feasibility utilizinglarge language_models language llms generate comments fulfill intents intuition based facts code comment pre training process llms build thesemantic connection natural_language natural language programming language and(2 comments real world projects collected thepre training usually contain different developers intents llms understand code different perspectives pre training experiments large scale datasets demonstratethe rationale insights adopting context_learning context learning paradigm adequate llm e.g. providing llm significantly outperform state art approach generating comments multiple intents results customized strategies constructing processing strategies reranking results boost shed light future_research future research directions llms toachieve comment generation"}
{"id": "nan", "abstract": "  Pretraining Neural Language Models (NLMs) over a large corpus involveschunking the text into training examples, which are contiguous text segments ofsizes processable by the neural architecture. We highlight a bias introduced bythis common practice: we prove that the pretrained NLM can model much strongerdependencies between text segments that appeared in the same training example,than it can between text segments that appeared in different training examples.This intuitive result has a twofold role. First, it formalizes the motivationbehind a broad line of recent successful NLM training heuristics, proposed forthe pretraining and fine-tuning stages, which do not necessarily appear relatedat first glance. Second, our result clearly indicates further improvements tobe made in NLM pretraining for the benefit of Natural Language Understandingtasks. As an example, we propose \"kNN-Pretraining\": we show that includingsemantically related non-neighboring sentences in the same pretraining exampleyields improved sentence representations and open domain question answeringabilities. This theoretically motivated degree of freedom for pretrainingexample design indicates new training schemes for self-improvingrepresentations.", "title": "the inductive bias of incontext learning rethinking pretraining example design", "url": "http://arxiv.org/pdf/2110.04541v3.pdf", "tokenized_text": "pretraining neural language_models language large corpus text training_examples training examples text segments neural architecture highlight bias introduced common practice prove pretrained text segments training example text segments different training_examples training examples intuitive result twofold role broad line recent successful training heuristics proposed forthe pretraining fine tuning stages necessarily appear second result clearly indicates improvements tobe pretraining benefit natural_language natural language understandingtasks example propose knn pretraining related non sentences pretraining improved sentence representations open_domain open domain question theoretically motivated degree freedom design indicates new training schemes self"}
{"id": "nan", "abstract": "  Large language models are able to perform a task by conditioning on a fewinput-output demonstrations - a paradigm known as in-context learning. We showthat language models can explicitly infer an underlying task from a fewdemonstrations by prompting them to generate a natural language instructionthat fits the examples. To explore this ability, we introduce the instructioninduction challenge, compile a dataset consisting of 24 tasks, and define anovel evaluation metric based on executing the generated instruction. Wediscover that, to a large extent, the ability to generate instructions doesindeed emerge when using a model that is both large enough and aligned tofollow instructions; InstructGPT achieves 65.7% of human performance in ourexecution-based metric, while the original GPT-3 model reaches only 9.8% ofhuman performance. This surprising result suggests that instruction inductionmight be a viable learning paradigm in and of itself, where instead of fittinga set of latent continuous parameters to the data, one searches for the bestdescription in the natural language hypothesis space.", "title": "instruction induction from few examples to natural language task descriptions", "url": "http://arxiv.org/pdf/2205.10782v1.pdf", "tokenized_text": "large_language large language able perform task conditioning output demonstrations paradigm known context_learning context learning showthat language_models language explicitly infer underlying task fewdemonstrations generate natural_language natural language fits examples explore ability introduce instructioninduction challenge compile dataset consisting 24 tasks define anovel evaluation metric based executing generated instruction large extent ability generate instructions emerge large aligned instructions instructgpt achieves human performance based metric original gpt-3 reaches ofhuman performance surprising result suggests instruction viable learning paradigm instead set latent continuous parameters data searches natural_language natural language hypothesis space"}
{"id": "nan", "abstract": "  Recent literature has shown that large language models (LLMs) are generallyexcellent few-shot reasoners to solve text reasoning tasks. However, thecapability of LLMs on table reasoning tasks is yet to be explored. In thispaper, we aim at understanding how well LLMs can perform table-related taskswith few-shot in-context learning. Specifically, we evaluated LLMs on populartable QA and fact verification datasets like WikiTableQuestion, FetaQA,TabFact, and FEVEROUS and found that LLMs are competent at complex reasoningover table structures, though these models are not pre-trained on any tablecorpus. When combined with `chain of thoughts' prompting, LLMs can achieve verystrong performance with only a 1-shot demonstration, even on par with some SoTAmodels. We show that LLMs are even more competent at generating comprehensivelong-form answers on FetaQA than tuned T5-large. We further manually studiedthe reasoning chains elicited from LLMs and found that these reasoning chainsare highly consistent with the underlying semantic form. We believe that LLMscan serve as a simple yet generic baseline for future research. The code anddata are released in https://github.com/wenhuchen/TableCoT.", "title": "large language models are few(1)shot table reasoners", "url": "http://arxiv.org/pdf/2210.06710v2.pdf", "tokenized_text": "recent literature shown large_language large language llms shot reasoners solve text reasoning tasks llms table reasoning tasks explored thispaper aim understanding llms perform table related taskswith shot context_learning context learning specifically evaluated llms qa fact verification datasets like found llms competent complex reasoningover table structures pre trained combined chain thoughts llms achieve performance shot demonstration par llms competent generating form answers tuned t5 large manually reasoning chains elicited llms found reasoning highly consistent underlying semantic form believe llmscan serve simple generic baseline future_research future research code anddata released"}
{"id": "nan", "abstract": "  Open-Domain Question Answering (ODQA) aims at answering factoid questionswithout explicitly providing specific background documents. In a zero-shotsetting, this task is more challenging since no data is available to traincustomized models like Retriever-Readers. Recently, Large Language Models(LLMs) like GPT-3 have shown their power in zero-shot ODQA with directprompting methods, but these methods are still far from releasing the fullpowerfulness of LLMs only in an implicitly invoking way. In this paper, wepropose a Self-Prompting framework to explicitly utilize the massive knowledgestored in the parameters of LLMs and their strong instruction understandingabilities. Concretely, we prompt LLMs step by step to generate multiple pseudoQA pairs with background passages and explanations from scratch and then usethose generated elements for in-context learning. Experimental results show ourmethod surpasses previous SOTA methods significantly on three widely-used ODQAdatasets, and even achieves comparable performance with some Retriever-Readermodels fine-tuned on full training data.", "title": "selfprompting large language models for zeroshot opendomain qa", "url": "http://arxiv.org/pdf/2212.08635v2.pdf", "tokenized_text": "open domain question_answering question answering odqa aims answering factoid explicitly providing specific background documents zero shotsetting task challenging data available like retriever readers recently large_language large language models(llms like gpt-3 shown power zero shot odqa methods methods far releasing llms implicitly invoking way paper wepropose self framework explicitly utilize massive parameters llms strong instruction concretely llms step_by_step step step generate multiple pairs background passages explanations scratch generated elements context_learning context learning experimental_results experimental results ourmethod surpasses previous sota methods significantly widely achieves comparable performance retriever fine tuned training_data training data"}
{"id": "nan", "abstract": "  We introduce a language generation task grounded in a popular video gameenvironment. KNUDGE (KNowledge Constrained User-NPC Dialogue GEneration)requires models to produce trees of dialogue between video game characters thataccurately reflect quest and entity specifications stated in natural language.KNUDGE is constructed from side quest dialogues drawn directly from game dataof Obsidian Entertainment's The Outer Worlds, leading to real-worldcomplexities in generation: (1) dialogues are branching trees as opposed tolinear chains of utterances; (2) utterances must remain faithful to the gamelore -- character personas, backstories, and entity relationships; and (3) adialogue must accurately reveal new quest details to the human player. Wereport results for a set of neural generation models using supervised andin-context learning techniques; we find competent performance but room forfuture work addressing the challenges of creating realistic, game-qualitydialogues.", "title": "ontologically faithful generation of nonplayer character dialogues", "url": "http://arxiv.org/pdf/2212.10618v2.pdf", "tokenized_text": "introduce language generation task grounded popular video knowledge constrained dialogue produce trees dialogue video game characters reflect entity specifications stated natural_language natural language constructed dialogues drawn directly game worlds leading real generation dialogues trees opposed chains utterances utterances remain faithful character personas entity relationships adialogue accurately reveal new details human player results set neural generation supervised andin context_learning context learning techniques find competent performance room forfuture work addressing challenges creating realistic game"}
{"id": "nan", "abstract": "  Performing inference on large volumes of samples with large language models(LLMs) can be computationally and financially costly in industry and real-worlduse. We propose batch prompting, a simple yet effective prompting approach thatenables the LLM to run inference in batches, instead of one sample at a time.Our method reduces both token and time costs while retaining downstreamperformance. We theoretically demonstrate that under a few-shot in-contextlearning setting, the inference costs decrease almost inverse linearly with thenumber of samples in each batch. We extensively validate the effectiveness ofbatch prompting on ten datasets across commonsense QA, arithmetic reasoning,and NLI/NLU: batch prompting significantly~(up to 5x with six samples in batch)reduces the LLM (Codex) inference token and time costs while achieving betteror comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5and GPT-4, we show the benefits of batch prompting also hold. Further analysisshows that the number of samples in each batch and the complexity of tasksaffect its performance. Moreover, batch prompting can be applied acrossdifferent reasoning methods using LLMs. Our code can be found at the sitehttps://github.com/xlang-ai/batch-prompting.", "title": "batch prompting efficient inference with large language model apis", "url": "http://arxiv.org/pdf/2301.08721v2.pdf", "tokenized_text": "performing inference large volumes samples large_language large language models(llms computationally costly industry real propose batch simple effective approach thatenables llm run inference batches instead sample time method reduces token time costs retaining theoretically demonstrate shot contextlearning setting inference costs decrease inverse linearly samples batch extensively validate effectiveness datasets commonsense qa arithmetic reasoning nli nlu batch 5x samples llm codex inference token time costs achieving comparable performance state art chat based llms e.g. gpt-3.5and gpt-4 benefits batch hold analysisshows number samples batch complexity performance batch applied acrossdifferent reasoning methods llms code found"}
{"id": "nan", "abstract": "  Additionally, the strong dependency among in-context examples makes it anNP-hard combinatorial optimization problem and enumerating all permutations isinfeasible. Hence we propose LENS, a fiLter-thEN-Search method to tackle thischallenge in two stages: First we filter the dataset to obtain informativein-context examples individually. Specifically, we propose a novel metric,InfoScore, to evaluate the example's in-context informativeness based on thelanguage model's feedback, and further propose a progressive filtering processto filter out uninformative examples. Then we propose diversity-guided examplesearch which iteratively refines and evaluates the selected examplepermutations, to find examples that fully depict the task. The experimentalresults show that LENS significantly outperforms a wide range of baselines.", "title": "finding support examples for incontext learning", "url": "http://arxiv.org/pdf/2302.13539v3.pdf", "tokenized_text": "additionally strong dependency context_examples context examples makes hard combinatorial optimization problem enumerating permutations propose lens filter search method tackle thischallenge stages filter dataset obtain context_examples context examples individually specifically propose_a_novel propose novel metric evaluate example context informativeness based thelanguage feedback propose progressive filtering filter uninformative examples propose diversity guided iteratively refines evaluates selected find examples fully depict task experimentalresults lens significantly_outperforms significantly outperforms wide_range wide range baselines"}
{"id": "nan", "abstract": "  Instruction learning of Large Language Models (LLMs) has enabled zero-shottask generalization. However, instruction learning has been predominantlyapproached as a fine-tuning problem, including instruction tuning andreinforcement learning from human feedback, where LLMs are multi-taskfine-tuned on various tasks with instructions. In this paper, we present asurprising finding that applying in-context learning to instruction learning,referred to as In-Context Instruction Learning (ICIL), significantly improvesthe zero-shot task generalization performance for both pretrained andinstruction-fine-tuned models. One of the core advantages of ICIL is that ituses a single fixed prompt to evaluate all tasks, which is a concatenation ofcross-task demonstrations. In particular, we demonstrate that the most powerfulinstruction-fine-tuned baseline (text-davinci-003) also benefits from ICIL by9.3%, indicating that the effect of ICIL is complementary to instruction-basedfine-tuning.", "title": "incontext instruction learning", "url": "http://arxiv.org/pdf/2302.14691v1.pdf", "tokenized_text": "instruction learning large_language large language llms enabled zero generalization instruction learning fine tuning problem including instruction_tuning instruction tuning learning human feedback llms multi tuned tasks instructions paper present finding applying context_learning context learning instruction learning referred context instruction learning significantly improvesthe zero shot task generalization performance pretrained andinstruction fine tuned core advantages ituses single fixed evaluate tasks concatenation task demonstrations particular demonstrate fine tuned baseline text davinci-003 benefits indicating effect complementary instruction basedfine tuning"}
{"id": "nan", "abstract": "  Although large language models have demonstrated impressive ability in codegeneration, they are still struggling to address the complicated intentprovided by humans. It is widely acknowledged that humans typically employplanning to decompose complex problems and schedule the solution steps prior toimplementation. Thus we introduce planning into code generation to help themodel understand complex intent and reduce the difficulty of problem solving.This paper proposes a self-planning code generation method with large languagemodel, which consists of two phases, namely planning phase and implementationphase. Specifically, in the planning phase, the language model plans out thesolution steps from the intent combined with in-context learning. Then itenters the implementation phase, where the model generates code step by step,guided by the solution steps. The effectiveness of self-planning codegeneration has been rigorously evaluated on multiple code generation datasetsand the results have demonstrated a marked superiority over naive directgeneration approaches with language model. The improvement in performance issubstantial, highlighting the significance of self-planning in code generationtasks.", "title": "selfplanning code generation with large language models", "url": "http://arxiv.org/pdf/2303.06689v2.pdf", "tokenized_text": "large_language large language demonstrated impressive ability codegeneration address complicated humans widely acknowledged humans typically decompose complex problems solution steps prior introduce planning code_generation code generation help themodel understand complex intent reduce difficulty problem solving paper_proposes paper proposes self planning code_generation code generation method large languagemodel consists phases planning phase specifically planning phase language_model language plans steps intent combined context_learning context learning implementation phase generates code step_by_step step step guided solution steps effectiveness self planning codegeneration evaluated multiple code_generation code generation datasetsand results demonstrated marked superiority naive directgeneration approaches language_model language improvement performance highlighting significance self planning code"}
{"id": "nan", "abstract": "  We demonstrate that, through appropriate prompting, GPT-3 family of modelscan be triggered to perform iterative behaviours necessary to execute (ratherthan just write or recall) programs that involve loops, including severalpopular algorithms found in computer science curricula or software developerinterviews. We trigger execution and description of Iterations by RegimentingSelf-Attention (IRSA) in one (or a combination) of three ways: 1) Using strongrepetitive structure in an example of an execution path of a target program forone particular input, 2) Prompting with fragments of execution paths, and 3)Explicitly forbidding (skipping) self-attention to parts of the generated text.On a dynamic program execution, IRSA leads to larger accuracy gains thanreplacing the model with the much more powerful GPT-4. IRSA has promisingapplications in education, as the prompts and responses resemble studentassignments in data structures and algorithms classes. Our findings holdimplications for evaluating LLMs, which typically target the in-contextlearning: We show that prompts that may not even cover one full task examplecan trigger algorithmic behaviour, allowing solving problems previously thoughtof as hard for LLMs, such as logical puzzles. Consequently, prompt design playsan even more critical role in LLM performance than previously recognized.", "title": "gpt is becoming a turing machine here are some ways to program it", "url": "http://arxiv.org/pdf/2303.14310v1.pdf", "tokenized_text": "demonstrate appropriate gpt-3 family triggered perform iterative behaviours necessary execute ratherthan write recall programs involve loops including algorithms found computer science curricula software trigger execution description iterations attention combination ways structure example execution path target program forone particular input fragments execution paths skipping self attention parts generated text dynamic program execution leads larger accuracy gains powerful gpt-4 education responses resemble data structures algorithms classes findings evaluating llms typically target contextlearning cover task trigger algorithmic behaviour allowing solving problems previously hard llms logical puzzles consequently design critical role llm performance previously recognized"}
{"id": "nan", "abstract": "  ChatGPT, a large-scale language model based on the advanced GPT-3.5architecture, has shown remarkable potential in various Natural LanguageProcessing (NLP) tasks. However, there is currently a dearth of comprehensivestudy exploring its potential in the area of Grammatical Error Correction(GEC). To showcase its capabilities in GEC, we design zero-shotchain-of-thought (CoT) and few-shot CoT settings using in-context learning forChatGPT. Our evaluation involves assessing ChatGPT's performance on fiveofficial test sets in three different languages, along with threedocument-level GEC test sets in English. Our experimental results and humanevaluations demonstrate that ChatGPT has excellent error detection capabilitiesand can freely correct errors to make the corrected sentences very fluent,possibly due to its over-correction tendencies and not adhering to theprinciple of minimal edits. Additionally, its performance in non-English andlow-resource settings highlights its potential in multilingual GEC tasks.However, further analysis of various types of errors at the document-level hasshown that ChatGPT cannot effectively correct agreement, coreference, tenseerrors across sentences, and cross-sentence boundary errors.", "title": "is chatgpt a highly fluent grammatical error correction system a comprehensive evaluation", "url": "http://arxiv.org/pdf/2304.01746v1.pdf", "tokenized_text": "chatgpt large scale language_model language based advanced shown remarkable potential natural languageprocessing nlp tasks currently exploring potential area grammatical error showcase capabilities gec design zero shotchain thought cot shot cot settings context_learning context learning evaluation involves assessing chatgpt performance test sets different languages level gec test sets english experimental_results experimental results humanevaluations demonstrate chatgpt excellent error detection capabilitiesand freely correct errors corrected sentences fluent possibly correction tendencies adhering minimal edits additionally performance non english andlow resource settings highlights potential multilingual gec tasks analysis types errors document level hasshown chatgpt effectively correct agreement coreference sentences cross sentence boundary errors"}
{"id": "nan", "abstract": "  Languages are not created randomly but rather to communicate information.There is a strong association between languages and their underlying meanings,resulting in a sparse joint distribution that is heavily peaked according totheir correlations. Moreover, these peak values happen to match with themarginal distribution of languages due to the sparsity. With the advent of LLMstrained on big data and large models, we can now precisely assess the marginaldistribution of languages, providing a convenient means of exploring the sparsestructures in the joint distribution for effective inferences. In this paper,we categorize languages as either unambiguous or {\\epsilon}-ambiguous andpresent quantitative results to demonstrate that the emergent abilities ofLLMs, such as language understanding, in-context learning, chain-of-thoughtprompting, and effective instruction fine-tuning, can all be attributed toBayesian inference on the sparse joint distribution of languages.", "title": "a latent space theory for emergent abilities in large language models", "url": "http://arxiv.org/pdf/2304.09960v3.pdf", "tokenized_text": "languages created randomly communicate information strong association languages underlying meanings resulting sparse joint distribution heavily according totheir correlations peak values happen match distribution languages sparsity advent big data large precisely assess languages providing convenient means exploring joint distribution effective inferences paper categorize languages quantitative results demonstrate emergent abilities ofllms language understanding context_learning context learning chain thoughtprompting effective instruction fine tuning attributed inference sparse joint distribution languages"}
{"id": "nan", "abstract": "  Stance detection is the identification of an author's beliefs about a subjectfrom a document. Researchers widely rely on sentiment analysis to accomplishthis. However, recent research has show that sentiment analysis is only looselycorrelated with stance, if at all. This paper advances methods in text analysisby precisely defining the task of stance detection, providing a generalizedframework for the task, and then presenting three distinct approaches forperforming stance detection: supervised classification, zero-shotclassification with NLI classifiers, and in-context learning. In doing so, Idemonstrate how zero-shot and few-shot language classifiers can replace humanlabelers for a variety of tasks and discuss how their application andlimitations differ from supervised classifiers. Finally, I demonstrate anapplication of zero-shot stance detection by replicating Block Jr et al.(2022).", "title": "stance detection with supervised, zeroshot, and fewshot applications", "url": "http://arxiv.org/pdf/2305.01723v1.pdf", "tokenized_text": "stance detection identification author beliefs document researchers widely rely sentiment_analysis sentiment analysis recent research sentiment_analysis sentiment analysis stance paper advances methods text precisely defining task stance detection providing task presenting distinct approaches stance detection supervised classification zero shotclassification nli classifiers context_learning context learning zero shot shot language classifiers replace variety tasks discuss application andlimitations differ supervised classifiers finally demonstrate zero shot stance detection replicating block et"}
{"id": "nan", "abstract": "  This paper describes our submission to the MEDIQA-Chat 2023 shared task forautomatic clinical note generation from doctor-patient conversations. We reportresults for two approaches: the first fine-tunes a pre-trained language model(PLM) on the shared task data, and the second uses few-shot in-context learning(ICL) with a large language model (LLM). Both achieve high performance asmeasured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second andfirst, respectively, of all submissions to the shared task. Expert humanscrutiny indicates that notes generated via the ICL-based approach with GPT-4are preferred about as often as human-written notes, making it a promising pathtoward automated note generation from doctor-patient conversations.", "title": "wanglab at mediqachat 2023 clinical note generation from doctorpatient conversations using large language models", "url": "http://arxiv.org/pdf/2305.02220v2.pdf", "tokenized_text": "paper describes submission chat 2023 shared task clinical note generation patient conversations approaches fine tunes pre trained_language trained language model(plm shared task data second uses shot context learning(icl large_language large language llm achieve high performance automatic metrics e.g. rouge bertscore ranked second respectively submissions shared task expert indicates notes generated icl based approach preferred human written notes making promising automated note generation patient conversations"}
{"id": "nan", "abstract": "  Recent advancements in Natural Language Processing (NLP) has led to theproliferation of large pretrained language models. These models have been shownto yield good performance, using in-context learning, even on unseen tasks andlanguages. They have also been exposed as commercial APIs as a form oflanguage-model-as-a-service, with great adoption. However, their performance onAfrican languages is largely unknown. We present a preliminary analysis ofcommercial large language models on two tasks (machine translation and textclassification) across eight African languages, spanning different languagefamilies and geographical areas. Our results suggest that commercial languagemodels produce below-par performance on African languages. We also find thatthey perform better on text classification than machine translation. Ingeneral, our findings present a call-to-action to ensure African languages arewell represented in commercial large language models, given their growingpopularity.", "title": "how good are commercial large language models on african languages", "url": "http://arxiv.org/pdf/2305.06530v1.pdf", "tokenized_text": "recent advancements natural_language natural language processing nlp led large pretrained_language pretrained language yield good performance context_learning context learning unseen tasks andlanguages exposed commercial apis form oflanguage service great adoption performance languages largely unknown present preliminary analysis large_language large language tasks machine_translation machine translation textclassification african languages spanning different geographical areas results suggest commercial languagemodels produce par performance african languages find perform better text_classification text classification machine_translation machine translation ingeneral findings present action ensure african languages arewell represented commercial large_language large language given"}
{"id": "nan", "abstract": "  Large language models (LLMs) have shown surprisingly good performance inmultilingual neural machine translation (MNMT) even when trained withoutparallel data. Yet, despite the fact that the amount of training data isgigantic, they still struggle with translating rare words, particularly forlow-resource languages. Even worse, it is usually unrealistic to retrieverelevant demonstrations for in-context learning with low-resource languages onLLMs, which restricts the practical use of LLMs for translation -- how shouldwe mitigate this problem? To this end, we present a novel method, CoD, whichaugments LLMs with prior knowledge with the chains of multilingual dictionariesfor a subset of input words to elicit translation abilities for LLMs. Extensiveexperiments indicate that augmenting ChatGPT with CoD elicits large gains by upto 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written inCyrillic script) on FLORES-200 full devtest set. We further demonstrate theimportance of chaining the multilingual dictionaries, as well as thesuperiority of CoD to few-shot demonstration for low-resource languages.", "title": "chainofdictionary prompting elicits translation in large language models", "url": "http://arxiv.org/pdf/2305.06575v3.pdf", "tokenized_text": "large_language large language llms shown surprisingly good performance inmultilingual neural machine_translation machine translation trained data despite fact training_data training data struggle translating rare words particularly forlow resource_languages resource languages worse usually retrieverelevant demonstrations context_learning context learning low resource_languages resource languages onllms restricts practical use llms translation mitigate problem end present novel method llms prior knowledge chains multilingual subset input words elicit translation abilities llms extensiveexperiments indicate augmenting chatgpt elicits large gains upto chrf++ points english written script set demonstrate theimportance chaining multilingual dictionaries thesuperiority shot demonstration low resource_languages resource languages"}
{"id": "nan", "abstract": "  Clinical trials are critical for drug development. Constructing theappropriate eligibility criteria (i.e., the inclusion/exclusion criteria forpatient recruitment) is essential for the trial's success. Proper design ofclinical trial protocols should consider similar precedent trials and theireligibility criteria to ensure sufficient patient coverage. In this paper, wepresent a method named AutoTrial to aid the design of clinical eligibilitycriteria using language models. It allows (1) controllable generation underinstructions via a hybrid of discrete and neural prompting, (2) scalableknowledge incorporation via in-context learning, and (3) explicit reasoningchains to provide rationales for understanding the outputs. Experiments on over70K clinical trials verify that AutoTrial generates high-quality criteria textsthat are fluent and coherent and with high accuracy in capturing the relevantclinical concepts to the target trial. It is noteworthy that our method, with amuch smaller parameter size, gains around 60% winning rate against the GPT-3.5baselines via human evaluations.", "title": "autotrial prompting language models for clinical trial design", "url": "http://arxiv.org/pdf/2305.11366v2.pdf", "tokenized_text": "clinical trials critical drug development constructing theappropriate criteria i.e. inclusion criteria recruitment essential trial success proper design trial protocols consider similar trials criteria ensure sufficient patient coverage paper wepresent method named aid design clinical language_models language allows controllable generation hybrid discrete neural incorporation context_learning context learning explicit provide rationales understanding outputs experiments clinical trials verify generates high quality criteria fluent coherent high accuracy capturing concepts target trial method smaller parameter size gains 60 winning rate human evaluations"}
{"id": "nan", "abstract": "  Large language models (LLMs) with in-context learning have demonstratedremarkable capability in the text-to-SQL task. Previous research has promptedLLMs with various demonstration-retrieval strategies and intermediate reasoningsteps to enhance the performance of LLMs. However, those works often employvaried strategies when constructing the prompt text for text-to-SQL inputs,such as databases and demonstration examples. This leads to a lack ofcomparability in both the prompt constructions and their primary contributions.Furthermore, selecting an effective prompt construction has emerged as apersistent problem for future research. To address this limitation, wecomprehensively investigate the impact of prompt constructions across varioussettings and provide insights for future work.", "title": "how to prompt llms for texttosql a study in zeroshot, singledomain, and crossdomain settings", "url": "http://arxiv.org/pdf/2305.11853v2.pdf", "tokenized_text": "large_language large language llms context_learning context learning demonstratedremarkable capability text sql task previous research demonstration retrieval strategies intermediate enhance performance llms works strategies constructing text text sql inputs databases demonstration examples leads lack constructions primary contributions furthermore selecting effective construction emerged problem future_research future research address limitation wecomprehensively investigate impact constructions provide insights future work"}
{"id": "nan", "abstract": "  Fact-checking real-world claims often requires collecting multiple pieces ofevidence and applying complex multi-step reasoning. In this paper, we presentProgram-Guided Fact-Checking (ProgramFC), a novel fact-checking model thatdecomposes complex claims into simpler sub-tasks that can be solved using ashared library of specialized functions. We first leverage the in-contextlearning ability of large language models to generate reasoning programs toguide the verification process. Afterward, we execute the program by delegatingeach sub-task to the corresponding sub-task handler. This process makes ourmodel both explanatory and data-efficient, providing clear explanations of itsreasoning process and requiring minimal training data. We evaluate ProgramFC ontwo challenging fact-checking datasets and show that it outperforms sevenfact-checking baselines across different settings of evidence availability,with explicit output programs that benefit human debugging. Our codes and dataare publicly available at https://github.com/mbzuai-nlp/ProgramFC.", "title": "factchecking complex claims with programguided reasoning", "url": "http://arxiv.org/pdf/2305.12744v1.pdf", "tokenized_text": "fact checking real world claims requires collecting multiple pieces applying complex multi step reasoning paper guided fact checking novel fact checking complex claims simpler sub tasks solved library specialized functions leverage contextlearning ability large_language large language generate reasoning programs toguide verification process afterward execute program sub task corresponding sub task process makes ourmodel explanatory data efficient providing clear explanations process requiring minimal training_data training data evaluate ontwo challenging fact checking datasets outperforms checking baselines different settings evidence availability explicit output programs benefit human debugging codes dataare publicly_available publicly available"}
{"id": "nan", "abstract": "  In this work, we present the first dataset, MailEx, for performing eventextraction from conversational email threads. To this end, we first proposed anew taxonomy covering 10 event types and 76 arguments in the email domain. Ourfinal dataset includes 1.5K email threads and ~4K emails, which are annotatedwith totally ~8K event instances. To understand the task challenges, weconducted a series of experiments comparing three types of approaches, i.e.,fine-tuned sequence labeling, fine-tuned generative extraction, and few-shotin-context learning. Our results showed that the task of email event extractionis far from being addressed, due to challenges lying in, e.g., extractingnon-continuous, shared trigger spans, extracting non-named entity arguments,and modeling the email conversational history. Our work thus suggests morefuture investigations in this domain-specific event extraction task.", "title": "mailex email event and argument extraction", "url": "http://arxiv.org/pdf/2305.13469v2.pdf", "tokenized_text": "work present dataset performing conversational email threads end proposed anew taxonomy covering 10 event types 76 arguments email domain dataset includes 1.5 email threads ~4 event instances understand task challenges series experiments comparing types approaches i.e. tuned sequence labeling fine tuned generative extraction context_learning context learning results showed task email event far addressed challenges lying e.g. continuous shared trigger spans extracting non named_entity named entity arguments modeling email conversational history work suggests investigations domain specific event extraction task"}
{"id": "nan", "abstract": "  Recently, large pretrained language models have demonstrated strong languageunderstanding capabilities. This is particularly reflected in their zero-shotand in-context learning abilities on downstream tasks through prompting. Toassess their impact on spoken language understanding (SLU), we evaluate severalsuch models like ChatGPT and OPT of different sizes on multiple benchmarks. Weverify the emergent ability unique to the largest models as they can reachintent classification accuracy close to that of supervised models with zero orfew shots on various languages given oracle transcripts. By contrast, theresults for smaller models fitting a single GPU fall far behind. We note thatthe error cases often arise from the annotation scheme of the dataset;responses from ChatGPT are still reasonable. We show, however, that the modelis worse at slot filling, and its performance is sensitive to ASR errors,suggesting serious challenges for the application of those textual models onSLU.", "title": "can chatgpt detect intent evaluating large language models for spoken language understanding", "url": "http://arxiv.org/pdf/2305.13512v2.pdf", "tokenized_text": "recently large pretrained_language pretrained language demonstrated strong languageunderstanding capabilities particularly reflected zero shotand context_learning context learning abilities downstream_tasks downstream tasks toassess impact spoken language understanding slu evaluate like_chatgpt like chatgpt opt different sizes multiple benchmarks emergent ability unique largest classification accuracy close supervised zero orfew shots languages given oracle transcripts contrast theresults smaller fitting single gpu fall far note thatthe error cases arise annotation scheme chatgpt reasonable modelis worse slot filling performance sensitive asr errors suggesting challenges application textual"}
{"id": "nan", "abstract": "  Existing efforts to improve logical reasoning ability of language models havepredominantly relied on supervised fine-tuning, hindering generalization to newdomains and/or tasks. The development of Large Langauge Models (LLMs) hasdemonstrated the capacity of compressing abundant knowledge into a singleproxy, enabling them to tackle multiple tasks effectively. Our preliminaryexperiments, nevertheless, show that LLMs do not show capability on logicalreasoning. The performance of LLMs on logical reasoning benchmarks is farbehind the existing state-of-the-art baselines. In this paper, we make thefirst attempt to investigate the feasibility of incorporating logical knowledgethrough self-supervised post-training, and activating it via in-contextlearning, which we termed as LogicLLM. Specifically, we devise anauto-regressive objective variant of MERIt and integrate it with two LLMseries, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to13 billion. The results on two challenging logical reasoning benchmarksdemonstrate the effectiveness of LogicLLM. Besides, we conduct extensiveablation studies to analyze the key factors in designing logic-oriented proxytasks.", "title": "logicllm exploring selfsupervised logicenhanced training for large language models", "url": "http://arxiv.org/pdf/2305.13718v2.pdf", "tokenized_text": "existing efforts improve logical reasoning ability language_models language relied supervised fine tuning hindering generalization and/or tasks development large llms capacity compressing abundant knowledge enabling tackle multiple tasks effectively llms capability logicalreasoning performance llms logical reasoning benchmarks existing state art baselines paper thefirst attempt investigate feasibility incorporating logical self supervised post training contextlearning termed specifically devise regressive objective variant integrate i.e. flan t5 llama parameter size ranging billion billion results challenging logical reasoning benchmarksdemonstrate effectiveness conduct studies analyze key factors designing logic oriented"}
{"id": "nan", "abstract": "  Question answering over knowledge bases (KBQA) aims to answer factoidquestions with a given knowledge base (KB). Due to the large scale of KB,annotated data is impossible to cover all fact schemas in KB, which poses achallenge to the generalization ability of methods that require a sufficientamount of annotated data. Recently, LLMs have shown strong few-shot performancein many NLP tasks. We expect LLM can help existing methods improve theirgeneralization ability, especially in low-resource situations. In this paper,we present McL-KBQA, a framework that incorporates the few-shot ability of LLMinto the KBQA method via ICL-based multiple choice and then improves theeffectiveness of the QA tasks. Experimental results on two KBQA datasetsdemonstrate the competitive performance of McL-KBQA with strong improvements ingeneralization. We expect to explore a new way to QA tasks from KBQA inconjunction with LLM, how to generate answers normatively and correctly withstrong generalization.", "title": "make a choice! knowledge base question answering with incontext learning", "url": "http://arxiv.org/pdf/2305.13972v1.pdf", "tokenized_text": "question_answering question answering knowledge bases kbqa aims answer given knowledge base kb large_scale large scale kb annotated_data annotated data impossible cover fact schemas kb poses generalization_ability generalization ability methods require annotated_data annotated data recently llms shown strong shot performancein nlp_tasks nlp tasks expect llm help existing_methods existing methods improve ability especially low resource situations paper present kbqa framework incorporates shot ability kbqa method icl based multiple_choice multiple choice improves theeffectiveness qa tasks experimental_results experimental results kbqa datasetsdemonstrate competitive_performance competitive performance kbqa strong improvements expect explore new way qa tasks kbqa llm generate answers correctly generalization"}
{"id": "nan", "abstract": "  Large language models have demonstrated the capability to perform on machinetranslation when the input is prompted with a few examples (in-contextlearning). Translation quality depends on various features of the selectedexamples, such as their quality and relevance, but previous work haspredominantly focused on individual features in isolation. In this paper, wepropose a general framework for combining different features influencingexample selection. We learn a regression model, CTQ Scorer (ContextualTranslation Quality), that selects examples based on multiple features in orderto maximize the translation quality. On multiple language pairs and languagemodels, we show that CTQ Scorer helps significantly outperform random selectionas well as strong single-factor baselines reported in the literature. We alsosee an improvement of over 2.5 COMET points on average with respect to a strongBM25 retrieval-based baseline.", "title": "ctqscorer combining multiple features for incontext example selection for machine translation", "url": "http://arxiv.org/pdf/2305.14105v2.pdf", "tokenized_text": "large_language large language demonstrated capability perform machinetranslation input prompted examples contextlearning translation quality depends features quality relevance previous work focused individual features isolation paper wepropose general framework combining different features selection learn regression quality selects examples based multiple features maximize translation quality multiple language pairs languagemodels helps significantly outperform random strong single factor baselines reported literature improvement 2.5 points average respect retrieval based baseline"}
{"id": "nan", "abstract": "  Traditional neural machine translation (NMT) systems often fail to translatesentences that contain culturally specific information. Most previous NMTmethods have incorporated external cultural knowledge during training, whichrequires fine-tuning on low-frequency items specific to the culture. Recentin-context learning utilizes lightweight prompts to guide large language models(LLMs) to perform machine translation, however, whether such an approach worksin terms of injecting culture awareness into machine translation remainsunclear. To this end, we introduce a new data curation pipeline to construct aculturally relevant parallel corpus, enriched with annotations ofcultural-specific entities. Additionally, we design simple but effectiveprompting strategies to assist this LLM-based translation. Extensiveexperiments show that our approaches can largely help incorporate culturalknowledge into LLM-based machine translation, outperforming traditional NMTsystems in translating cultural-specific sentences.", "title": "empowering llmbased machine translation with cultural awareness", "url": "http://arxiv.org/pdf/2305.14328v1.pdf", "tokenized_text": "traditional neural machine_translation machine translation nmt systems fail contain culturally specific information previous incorporated external cultural knowledge training whichrequires fine tuning low frequency items specific culture context_learning context learning utilizes lightweight guide large_language large language models(llms perform machine_translation machine translation approach worksin terms injecting culture awareness machine_translation machine translation end introduce new data curation pipeline construct relevant parallel corpus enriched annotations specific entities additionally design simple strategies assist llm based translation extensiveexperiments approaches largely help incorporate llm based machine_translation machine translation outperforming traditional translating cultural specific sentences"}
{"id": "nan", "abstract": "  Fact-checking is an essential task in NLP that is commonly utilized forvalidating the factual accuracy of claims. Prior work has mainly focused onfine-tuning pre-trained languages models on specific datasets, which can becomputationally intensive and time-consuming. With the rapid development oflarge language models (LLMs), such as ChatGPT and GPT-3, researchers are nowexploring their in-context learning capabilities for a wide range of tasks. Inthis paper, we aim to assess the capacity of LLMs for fact-checking byintroducing Self-Checker, a framework comprising a set of plug-and-play modulesthat facilitate fact-checking by purely prompting LLMs in an almost zero-shotsetting. This framework provides a fast and efficient way to constructfact-checking systems in low-resource environments. Empirical resultsdemonstrate the potential of Self-Checker in utilizing LLMs for fact-checking.However, there is still significant room for improvement compared to SOTAfine-tuned models, which suggests that LLM adoption could be a promisingapproach for future fact-checking research.", "title": "selfchecker plugandplay modules for factchecking with large language models", "url": "http://arxiv.org/pdf/2305.14623v1.pdf", "tokenized_text": "fact checking essential task nlp commonly utilized factual accuracy claims prior_work prior work mainly focused onfine tuning pre trained languages specific datasets intensive time consuming rapid development oflarge language_models language llms chatgpt gpt-3 researchers context_learning context learning capabilities wide_range wide range tasks inthis_paper inthis paper aim assess capacity llms fact checking byintroducing self checker framework comprising set plug play facilitate fact checking purely llms zero shotsetting framework provides fast efficient way checking systems low resource environments empirical resultsdemonstrate potential self checker utilizing llms fact checking significant room improvement compared tuned suggests llm adoption future fact checking research"}
{"id": "nan", "abstract": "  The answering quality of an aligned large language model (LLM) can bedrastically improved if treated with proper crafting of prompts. In this paper,we propose ExpertPrompting to elicit the potential of LLMs to answer asdistinguished experts. We first utilize In-Context Learning to automaticallysynthesize detailed and customized descriptions of the expert identity for eachspecific instruction, and then ask LLMs to provide answer conditioned on suchagent background. Based on this augmented prompting strategy, we produce a newset of instruction-following data using GPT-3.5, and train a competitiveopen-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluationto show that 1) the expert data is of significantly higher quality than vanillaanswers, and 2) ExpertLLaMA outperforms existing open-source opponents andachieves 96\\% of the original ChatGPT's capability. All data and theExpertLLaMA model will be made publicly available at\\url{https://github.com/OFA-Sys/ExpertLLaMA}.", "title": "expertprompting instructing large language models to be distinguished experts", "url": "http://arxiv.org/pdf/2305.14688v1.pdf", "tokenized_text": "answering quality aligned large_language large language llm improved treated proper crafting paper propose elicit potential llms answer experts utilize context_learning context learning detailed customized descriptions expert identity instruction ask llms provide answer conditioned background based augmented strategy produce instruction following data gpt-3.5 train source chat assistant called employ gpt4 based expert data significantly higher quality outperforms existing open source opponents original chatgpt capability data publicly_available publicly available at\\url{https://github.com ofa"}
{"id": "nan", "abstract": "  Event temporal reasoning aims at identifying the temporal relations betweentwo or more events. However, knowledge conflicts arise when there is a mismatchbetween the actual temporal relations of events in the context and the priorknowledge or biases learned by the model. We first systematically definedistinct kinds of bias in event temporal reasoning, which include eventrelation prior bias, tense bias, narrative bias, and dependency bias, asindicators to study knowledge conflicts. To mitigate such event-relatedknowledge conflict, we introduce a Counterfactual Data Augmentation basedmethod that can be applied to both Pre-trained Language Models (PLMs) and LargeLanguage Models (LLMs) either as additional training data or demonstrations forIn-Context Learning. Experiments suggest the importance of mitigating knowledgeconflicts in event temporal reasoning tasks for reducing hallucination andhighlight the potential of counterfactual data augmentation for improving modelperformance.", "title": "getting sick after seeing a doctor diagnosing and mitigating knowledge conflicts in event temporal reasoning", "url": "http://arxiv.org/pdf/2305.14970v1.pdf", "tokenized_text": "event temporal reasoning aims identifying temporal relations betweentwo events knowledge conflicts arise actual temporal relations events context priorknowledge biases learned systematically kinds bias event temporal reasoning include prior bias bias narrative bias dependency bias study knowledge conflicts mitigate event conflict introduce counterfactual data_augmentation data augmentation basedmethod applied pre trained_language trained language plms largelanguage_models largelanguage llms additional training_data training data demonstrations forin context_learning context learning experiments suggest importance mitigating event temporal reasoning tasks reducing hallucination potential counterfactual data_augmentation data augmentation improving modelperformance"}
{"id": "nan", "abstract": "  Existing cross-lingual transfer (CLT) prompting methods are only concernedwith monolingual demonstration examples in the source language. In this paper,we propose In-CLT, a novel cross-lingual transfer prompting method thatleverages both source and target languages to construct the demonstrationexamples. We conduct comprehensive evaluations on multilingual benchmarks,focusing on question answering tasks. Experiment results show that In-CLTprompt not only improves multilingual models' cross-lingual transferability,but also demonstrates remarkable unseen language generalization ability. In-CLTprompting, in particular, improves model performance by 10 to 20\\% points onaverage when compared to prior cross-lingual transfer approaches. We alsoobserve the surprising performance gain on the other multilingual benchmarks,especially in reasoning tasks. Furthermore, we investigate the relationshipbetween lexical similarity and pre-training corpora in terms of thecross-lingual transfer gap.", "title": "boosting crosslingual transferability in multilingual models via incontext learning", "url": "http://arxiv.org/pdf/2305.15233v1.pdf", "tokenized_text": "existing cross lingual_transfer lingual transfer methods monolingual demonstration examples source language paper propose novel cross lingual_transfer lingual transfer method thatleverages source target languages construct demonstrationexamples conduct comprehensive evaluations multilingual benchmarks focusing question_answering question answering tasks experiment results improves multilingual cross lingual transferability demonstrates remarkable unseen language generalization_ability generalization ability particular improves performance 10 points onaverage compared prior cross lingual_transfer lingual transfer approaches surprising performance gain multilingual benchmarks especially reasoning tasks furthermore investigate lexical similarity pre training corpora terms thecross lingual_transfer lingual transfer gap"}
{"id": "nan", "abstract": "  A primary criticism towards language models (LMs) is their inscrutability.This paper presents evidence that, despite their size and complexity, LMssometimes exploit a simple computational mechanism to solve one-to-onerelational tasks (e.g., capital_of(Poland)=Warsaw). We investigate a range oflanguage model sizes (from 124M parameters to 176B parameters) in an in-contextlearning setting, and find that for a variety of tasks (involving capitalcities, upper-casing, and past-tensing) a key part of the mechanism reduces toa simple linear update typically applied by the feedforward (FFN) networks.These updates also tend to promote the output of the relation in acontent-independent way (e.g., encoding Poland:Warsaw::China:Beijing),revealing a predictable pattern that these models take in solving these tasks.We further show that this mechanism is specific to tasks that require retrievalfrom pretraining memory, rather than retrieval from local context. Our resultscontribute to a growing body of work on the mechanistic interpretability ofLLMs, and offer reason to be optimistic that, despite the massive andnon-linear nature of the models, the strategies they ultimately use to solvetasks can sometimes reduce to familiar and even intuitive algorithms.", "title": "a mechanism for solving relational tasks in transformer language models", "url": "http://arxiv.org/pdf/2305.16130v2.pdf", "tokenized_text": "primary language_models language lms paper_presents paper presents evidence despite size complexity exploit simple computational mechanism solve tasks e.g. investigate range oflanguage sizes parameters 176b parameters contextlearning setting find variety tasks involving upper past key mechanism reduces toa simple linear update typically applied networks updates tend promote output relation independent way e.g. encoding predictable pattern solving tasks mechanism specific tasks require pretraining memory retrieval local context growing body work mechanistic interpretability ofllms offer reason despite massive andnon linear nature strategies ultimately use reduce familiar intuitive algorithms"}
{"id": "nan", "abstract": "  Using translation memories (TMs) as prompts is a promising approach toin-context learning of machine translation models. In this work, we take a steptowards prompting large language models (LLMs) with TMs and making them bettertranslators. We find that the ability of LLMs to ``understand'' prompts isindeed helpful for making better use of TMs. Experiments show that the resultsof a pre-trained LLM translator can be greatly improved by using high-qualityTM-based prompts. These results are even comparable to those of thestate-of-the-art NMT systems which have access to large-scale in-domainbilingual data and are well tuned on the downstream tasks.", "title": "augmenting large language model translators via translation memories", "url": "http://arxiv.org/pdf/2305.17367v1.pdf", "tokenized_text": "translation memories promising approach toin context_learning context learning machine_translation machine translation work steptowards large_language large language llms making find ability llms understand helpful making better use experiments resultsof pre trained llm translator greatly improved high based results comparable thestate art nmt systems access large scale data tuned downstream_tasks downstream tasks"}
{"id": "nan", "abstract": "  Explanations in conventional recommender systems have demonstrated benefitsin helping the user understand the rationality of the recommendations andimproving the system's efficiency, transparency, and trustworthiness. In theconversational environment, multiple contextualized explanations need to begenerated, which poses further challenges for explanations. To better measureexplainability in conversational recommender systems (CRS), we propose tenevaluation perspectives based on concepts from conventional recommender systemstogether with the characteristics of CRS. We assess five existing CRS benchmarkdatasets using these metrics and observe the necessity of improving theexplanation quality of CRS. To achieve this, we conduct manual and automaticapproaches to extend these dialogues and construct a new CRS dataset, namelyExplainable Recommendation Dialogues (E-ReDial). It includes 756 dialogues withover 2,000 high-quality rewritten explanations. We compare two baselineapproaches to perform explanation generation based on E-ReDial. Experimentalresults suggest that models trained on E-ReDial can significantly improveexplainability while introducing knowledge into the models can further improvethe performance. GPT-3 in the in-context learning setting can generate morerealistic and diverse movie descriptions. In contrast, T5 training on E-ReDialcan better generate clear reasons for recommendations based on userpreferences. E-ReDial is available at https://github.com/Superbooming/E-ReDial.", "title": "towards explainable conversational recommender systems", "url": "http://arxiv.org/pdf/2305.18363v1.pdf", "tokenized_text": "explanations conventional recommender systems demonstrated helping user understand rationality recommendations system efficiency transparency trustworthiness theconversational environment multiple contextualized explanations need poses challenges explanations better conversational recommender systems propose perspectives based concepts conventional recommender characteristics assess existing benchmarkdatasets metrics observe necessity improving quality achieve conduct manual extend dialogues construct new dataset recommendation dialogues redial includes dialogues 2,000 high quality rewritten explanations compare perform explanation generation based redial experimentalresults suggest trained redial significantly introducing knowledge improvethe performance gpt-3 context_learning context learning setting generate diverse movie descriptions contrast t5 training better generate clear reasons recommendations based redial available"}
{"id": "nan", "abstract": "  Large language models (LLMs) can learn to perform a wide range of naturallanguage tasks from just a handful of in-context examples. However, forgenerating strings from highly structured languages (e.g., semantic parsing tocomplex domain-specific languages), it is challenging for the LLM to generalizefrom just a few exemplars. We propose \\emph{grammar prompting}, a simpleapproach to enable LLMs to use external knowledge and domain-specificconstraints, expressed through a grammar in Backus--Naur Form (BNF), duringin-context learning. Grammar prompting augments each demonstration example witha specialized grammar that is minimally sufficient for generating theparticular output example, where the specialized grammar is a subset of thefull DSL grammar. For inference, the LLM first predicts a BNF grammar given atest input, and then generates the output according to the rules of thegrammar. Experiments demonstrate that grammar prompting can enable LLMs toperform competitively on a diverse set of DSL generation tasks, includingsemantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, andSMILES-based molecule generation.", "title": "grammar prompting for domainspecific language generation with large language models", "url": "http://arxiv.org/pdf/2305.19234v3.pdf", "tokenized_text": "large_language large language llms learn perform wide_range wide range naturallanguage tasks handful context_examples context examples forgenerating strings highly structured languages e.g. semantic_parsing semantic parsing domain specific languages challenging llm exemplars propose enable llms use external_knowledge external knowledge domain expressed grammar form context_learning context learning grammar augments demonstration example witha specialized grammar minimally sufficient generating output example specialized grammar subset thefull dsl grammar inference llm predicts grammar given atest input generates output according rules experiments_demonstrate experiments demonstrate grammar enable llms toperform competitively diverse set dsl generation tasks parsing smcalflow planning based molecule generation"}
{"id": "nan", "abstract": "  Few-shot or zero-shot fact verification only relies on a few or no labeledtraining examples. In this paper, we propose a novel method called ProToCo, to\\underline{Pro}mpt pre-trained language models (PLMs) \\underline{To} be\\underline{Co}nsistent, for improving the factuality assessment capability ofPLMs in the few-shot and zero-shot settings. Given a claim-evidence pair,ProToCo generates multiple variants of the claim with different relations andframes a simple consistency mechanism as constraints for making compatiblepredictions across these variants. We update PLMs by using parameter-efficientfine-tuning (PEFT), leading to more accurate predictions in few-shot andzero-shot fact verification tasks. Our experiments on three public verificationdatasets show that ProToCo significantly outperforms state-of-the-art few-shotfact verification baselines. With a small number of unlabeled instances,ProToCo also outperforms the strong zero-shot learner T0 on zero-shotverification. Compared to large PLMs using in-context learning (ICL) method,ProToCo outperforms OPT-30B and the Self-Consistency-enabled OPT-6.7B model inboth few- and zero-shot settings.", "title": "prompt to be consistent is better than selfconsistent fewshot and zeroshot fact verification with pretrained language models", "url": "http://arxiv.org/pdf/2306.02569v1.pdf", "tokenized_text": "shot zero shot fact verification relies examples paper propose_a_novel propose novel method called pre trained_language trained language plms improving factuality assessment capability ofplms shot zero shot_settings shot settings given claim evidence pair generates multiple variants claim different relations simple consistency mechanism constraints making variants update plms parameter efficientfine tuning peft leading accurate predictions shot andzero shot fact verification tasks experiments public significantly_outperforms significantly outperforms state art verification baselines small_number small number unlabeled instances outperforms strong zero shot learner t0 zero compared large plms context_learning context learning icl method outperforms self consistency enabled inboth few- zero shot_settings shot settings"}
{"id": "nan", "abstract": "  We present a framework that formulates visual question answering as modularcode generation. In contrast to prior work on modular approaches to VQA, ourapproach requires no additional training and relies on pre-trained languagemodels (LMs), visual models pre-trained on image-caption pairs, and fifty VQAexamples used for in-context learning. The generated Python programs invoke andcompose the outputs of the visual models using arithmetic and conditionallogic. Our approach improves accuracy on the COVR dataset by at least 3% and onthe GQA dataset by roughly 2% compared to the few-shot baseline that does notemploy code generation.", "title": "modular visual question answering via code generation", "url": "http://arxiv.org/pdf/2306.05392v1.pdf", "tokenized_text": "present framework formulates visual question_answering question answering generation contrast prior_work prior work modular approaches vqa ourapproach requires additional training relies pre trained languagemodels lms visual pre trained image caption pairs context_learning context learning generated python programs outputs visual arithmetic approach improves accuracy dataset onthe gqa dataset roughly compared shot baseline code_generation code generation"}
{"id": "nan", "abstract": "  The development of plans of action in disaster response scenarios is atime-consuming process. Large Language Models (LLMs) offer a powerful solutionto expedite this process through in-context learning. This study presentsDisasterResponseGPT, an algorithm that leverages LLMs to generate valid plansof action quickly by incorporating disaster response and planning guidelines inthe initial prompt. In DisasterResponseGPT, users input the scenariodescription and receive a plan of action as output. The proposed methodgenerates multiple plans within seconds, which can be further refined followingthe user's feedback. Preliminary results indicate that the plans of actiondeveloped by DisasterResponseGPT are comparable to human-generated ones whileoffering greater ease of modification in real-time. This approach has thepotential to revolutionize disaster response operations by enabling rapidupdates and adjustments during the plan's execution.", "title": "disasterresponsegpt large language models for accelerated plan of action development in disaster response scenarios", "url": "http://arxiv.org/pdf/2306.17271v1.pdf", "tokenized_text": "development plans action response scenarios consuming process large_language large language llms offer powerful expedite process context_learning context learning study algorithm leverages llms generate valid action quickly incorporating response planning guidelines inthe initial users input receive plan action output proposed multiple plans seconds refined followingthe user feedback preliminary results_indicate results indicate plans comparable human generated ones greater ease modification real time approach thepotential revolutionize response operations enabling adjustments plan execution"}
{"id": "nan", "abstract": "  Neural-symbolic methods have shown their effectiveness in enhancing thereasoning abilities of large language models (LLMs). However, existing methodsprimarily rely on mapping natural languages to more syntactically completeformal languages (e.g., Python and SQL). Those approaches necessitate thatreasoning tasks be convertible into programs, which cater more to the computerexecution mindset and deviate from human reasoning habits. To expand thereal-world applicability and flexibility of symbolic methods, we proposeMeta-Reasoning from the scope of linguistics itself. This method empowers LLMsto deconstruct questions and effectively capture more generalized knowledgeautonomously. We find that Meta-Reasoning achieves improved in-context learningefficiency, reasoning accuracy, and output stability in six arithmetic andsymbolic reasoning tasks. In particular, when applied to symbolic reasoningtasks such as Tracking Shuffled Objects, GPT-3 (text-davinci-002) surpasses thefew-shot Chain-of-Thought prompting approach (+37.7%), with 99% accuracy aftera single demonstration of Meta-Reasoning.", "title": "metareasoning semanticssymbol deconstruction for large language models", "url": "http://arxiv.org/pdf/2306.17820v2.pdf", "tokenized_text": "neural symbolic methods shown effectiveness enhancing abilities large_language large language llms existing rely mapping natural languages syntactically languages e.g. python sql approaches necessitate tasks programs cater deviate human reasoning expand world applicability flexibility symbolic methods reasoning scope linguistics method empowers llmsto questions effectively capture generalized find meta reasoning achieves improved context reasoning accuracy output stability arithmetic reasoning tasks particular applied symbolic tracking objects gpt-3 text davinci-002 surpasses thefew shot chain thought_prompting thought approach 99 accuracy single demonstration meta reasoning"}
{"id": "nan", "abstract": "  Recent approaches to empathetic response generation try to incorporatecommonsense knowledge or reasoning about the causes of emotions to betterunderstand the user's experiences and feelings. However, these approachesmainly focus on understanding the causalities of context from the user'sperspective, ignoring the system's perspective. In this paper, we propose acommonsense-based causality explanation approach for diverse empatheticresponse generation that considers both the user's perspective (user's desiresand reactions) and the system's perspective (system's intentions andreactions). We enhance ChatGPT's ability to reason for the system's perspectiveby integrating in-context learning with commonsense knowledge. Then, weintegrate the commonsense-based causality explanation with both ChatGPT and aT5-based model. Experimental evaluations demonstrate that our methodoutperforms other comparable methods on both automatic and human evaluations.", "title": "reasoning before responding integrating commonsensebased causality explanation for empathetic response generation", "url": "http://arxiv.org/pdf/2308.00085v2.pdf", "tokenized_text": "recent approaches empathetic response generation try knowledge reasoning causes emotions betterunderstand user experiences feelings focus understanding context ignoring system perspective paper propose based causality explanation approach diverse generation considers user perspective user reactions system perspective system intentions enhance chatgpt ability reason system integrating context_learning context learning commonsense knowledge weintegrate commonsense based causality explanation chatgpt based experimental evaluations demonstrate comparable methods automatic human evaluations"}
{"id": "nan", "abstract": "  Music generation has attracted growing interest with the advancement of deepgenerative models. However, generating music conditioned on textualdescriptions, known as text-to-music, remains challenging due to the complexityof musical structures and high sampling rate requirements. Despite the task'ssignificance, prevailing generative models exhibit limitations in musicquality, computational efficiency, and generalization. This paper introducesJEN-1, a universal high-fidelity model for text-to-music generation. JEN-1 is adiffusion model incorporating both autoregressive and non-autoregressivetraining. Through in-context learning, JEN-1 performs various generation tasksincluding text-guided music generation, music inpainting, and continuation.Evaluations demonstrate JEN-1's superior performance over state-of-the-artmethods in text-music alignment and music quality while maintainingcomputational efficiency. Our demos are available athttp://futureverse.com/research/jen/demos/jen1", "title": "jen1 textguided universal music generation with omnidirectional diffusion models", "url": "http://arxiv.org/pdf/2308.04729v1.pdf", "tokenized_text": "music generation attracted growing interest advancement generating music conditioned textualdescriptions known text music remains challenging structures high sampling rate requirements despite prevailing generative exhibit limitations computational efficiency generalization paper universal high fidelity text music generation incorporating autoregressive non context_learning context learning performs generation text guided music generation music continuation evaluations demonstrate superior_performance superior performance state text music alignment music quality efficiency demos available"}
{"id": "nan", "abstract": "  Current literature, aiming to surpass the \"Chain-of-Thought\" approach, oftenresorts to an external modus operandi involving halting, modifying, and thenresuming the generation process to boost Large Language Models' (LLMs)reasoning capacities. This mode escalates the number of query requests, leadingto increased costs, memory, and computational overheads. Addressing this, wepropose the Algorithm of Thoughts -- a novel strategy that propels LLMs throughalgorithmic reasoning pathways, pioneering a new mode of in-context learning.By employing algorithmic examples, we exploit the innate recurrence dynamics ofLLMs, expanding their idea exploration with merely one or a few queries. Ourtechnique outperforms earlier single-query methods and stands on par with arecent multi-query strategy that employs an extensive tree search algorithm.Intriguingly, our results suggest that instructing an LLM using an algorithmcan lead to performance surpassing that of the algorithm itself, hinting atLLM's inherent ability to weave its intuition into optimized searches. We probeinto the underpinnings of our method's efficacy and its nuances in application.", "title": "algorithm of thoughts enhancing exploration of ideas in large language models", "url": "http://arxiv.org/pdf/2308.10379v2.pdf", "tokenized_text": "current literature aiming surpass chain thought approach external involving halting modifying generation process boost large_language large language capacities mode number query requests increased costs memory computational overheads addressing wepropose algorithm thoughts novel strategy llms reasoning pathways pioneering new mode context_learning context learning employing algorithmic examples exploit innate dynamics ofllms expanding idea exploration merely queries outperforms earlier single query methods stands par arecent multi query strategy employs extensive tree search algorithm intriguingly results suggest instructing llm lead performance surpassing algorithm inherent ability intuition optimized searches method efficacy nuances application"}
{"id": "nan", "abstract": "  The integration of emotional support into various conversational scenariospresents profound societal benefits, such as social interactions, mental healthcounseling, and customer service. However, there are unsolved challenges thathinder real-world applications in this field, including limited dataavailability and the absence of well-accepted model training paradigms. Thiswork endeavors to navigate these challenges by harnessing the capabilities ofLarge Language Models (LLMs). We introduce an innovative methodology thatsynthesizes human insights with the computational prowess of LLMs to curate anextensive emotional support dialogue dataset. Our approach is initiated with ameticulously designed set of dialogues spanning diverse scenarios as generativeseeds. By utilizing the in-context learning potential of ChatGPT, werecursively generate an ExTensible Emotional Support dialogue dataset, namedExTES. Following this, we deploy advanced tuning techniques on the LLaMA model,examining the impact of diverse training strategies, ultimately yielding an LLMmeticulously optimized for emotional support interactions. An exhaustiveassessment of the resultant model showcases its proficiency in offeringemotional support, marking a pivotal step in the realm of emotional supportbots and paving the way for subsequent research and implementations.", "title": "building emotional support chatbots in the era of llms", "url": "http://arxiv.org/pdf/2308.11584v1.pdf", "tokenized_text": "integration emotional support conversational profound societal benefits social interactions mental customer service unsolved challenges thathinder real world_applications world applications field including limited absence accepted training paradigms thiswork endeavors navigate challenges harnessing capabilities oflarge language_models language llms introduce innovative methodology human insights computational llms curate anextensive emotional support dialogue dataset approach initiated designed set dialogues spanning diverse scenarios utilizing context_learning context learning potential chatgpt generate extensible emotional support dialogue dataset following deploy advanced tuning techniques llama examining impact diverse training strategies ultimately yielding optimized emotional support interactions showcases proficiency support marking pivotal step realm emotional paving way subsequent research implementations"}
{"id": "nan", "abstract": "  We propose the use of conversational GPT models for easy and quick few-shottext classification in the financial domain using the Banking77 dataset. Ourapproach involves in-context learning with GPT-3.5 and GPT-4, which minimizesthe technical expertise required and eliminates the need for expensive GPUcomputing while yielding quick and accurate results. Additionally, we fine-tuneother pre-trained, masked language models with SetFit, a recent contrastivelearning technique, to achieve state-of-the-art results both in full-data andfew-shot settings. Our findings show that querying GPT-3.5 and GPT-4 canoutperform fine-tuned, non-generative models even with fewer examples. However,subscription fees associated with these solutions may be considered costly forsmall organizations. Lastly, we find that generative models perform better onthe given task when shown representative samples selected by a human expertrather than when shown random ones. We conclude that a) our proposed methodsoffer a practical solution for few-shot tasks in datasets with limited labelavailability, and b) our state-of-the-art results can inspire future work inthe area.", "title": "breaking the bank with chatgpt fewshot text classification for finance", "url": "http://arxiv.org/pdf/2308.14634v1.pdf", "tokenized_text": "propose use conversational gpt easy quick shottext classification financial domain dataset ourapproach involves context_learning context learning gpt-3.5 gpt-4 technical expertise required eliminates need expensive yielding quick accurate results additionally fine pre trained masked language_models language setfit recent contrastivelearning technique achieve state art results data andfew shot_settings shot settings findings querying gpt-3.5 gpt-4 fine tuned non generative fewer examples fees associated solutions considered costly organizations lastly find generative perform better onthe given task shown representative samples selected human shown random ones conclude proposed practical solution shot tasks datasets limited state art results inspire future work inthe area"}
{"id": "nan", "abstract": "  Decoder-only Large Language Models (LLMs) have demonstrated potential inmachine translation (MT), albeit with performance slightly lagging behindtraditional encoder-decoder Neural Machine Translation (NMT) systems. However,LLMs offer a unique advantage: the ability to control the properties of theoutput through prompts. In this study, we harness this flexibility to exploreLLaMa's capability to produce gender-specific translations for languages withgrammatical gender. Our results indicate that LLaMa can generategender-specific translations with competitive accuracy and gender biasmitigation when compared to NLLB, a state-of-the-art multilingual NMT system.Furthermore, our experiments reveal that LLaMa's translations are robust,showing significant performance drops when evaluated against opposite-genderreferences in gender-ambiguous datasets but maintaining consistency in lessambiguous contexts. This research provides insights into the potential andchallenges of using LLMs for gender-specific translations and highlights theimportance of in-context learning to elicit new tasks in LLMs.", "title": "genderspecific machine translation with large language models", "url": "http://arxiv.org/pdf/2309.03175v1.pdf", "tokenized_text": "decoder large_language large language llms demonstrated potential inmachine translation mt albeit performance slightly encoder decoder neural machine_translation machine translation nmt systems llms offer unique advantage ability control properties theoutput study harness flexibility capability produce gender specific translations languages gender results_indicate results indicate llama specific translations competitive accuracy gender compared nllb state art multilingual nmt system furthermore experiments reveal llama translations robust showing significant performance drops evaluated opposite gender ambiguous datasets maintaining consistency contexts research provides insights potential andchallenges llms gender specific translations highlights theimportance context_learning context learning elicit new tasks llms"}
{"id": "nan", "abstract": "  Open Information Extraction (OIE) task aims at extracting structured factsfrom unstructured text, typically in the form of (subject, relation, object)triples. Despite the potential of large language models (LLMs) like ChatGPT asa general task solver, they lag behind state-of-the-art (supervised) methods inOIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevantcontext from relevant relations and generate structured output due to therestrictions on fine-tuning the model. Second, LLMs generates responsesautoregressively based on probability, which makes the predicted relations lackconfidence. In this paper, we assess the capabilities of LLMs in improving theOIE task. Particularly, we propose various in-context learning strategies toenhance LLM's instruction-following ability and a demonstration uncertaintyquantification module to enhance the confidence of the generated relations. Ourexperiments on three OIE benchmark datasets show that our approach holds itsown against established supervised methods, both quantitatively andqualitatively.", "title": "improving open information extraction with large language models a study on demonstration uncertainty", "url": "http://arxiv.org/pdf/2309.03433v1.pdf", "tokenized_text": "open information_extraction information extraction oie task aims extracting structured unstructured text typically form subject relation despite potential large_language large language llms like_chatgpt like chatgpt asa general task solver lag state art supervised methods tasks key issues llms struggle distinguish relevant relations generate structured output fine tuning second llms generates based probability makes predicted relations paper assess capabilities llms improving task particularly propose context_learning context learning strategies toenhance llm instruction following ability demonstration module enhance confidence generated relations ourexperiments oie benchmark_datasets benchmark datasets approach holds established supervised methods quantitatively andqualitatively"}
{"id": "nan", "abstract": "  Large language models (LLMs) have shown promising performance on various NLPtasks via task prompting. And their performance can be further improved byappending task demonstrations to the head of the prompt. And usually, a betterperformance can be achieved with more demonstrations. However, asking the usersto write the demonstrations can be cumbersome. As a simple yet cost-effectiveworkaround, this paper proposes a novel method called EPA (\\textbf{E}asy\\textbf{P}rompt \\textbf{A}ugmentation)\\footnote{While this paper considersaugmenting prompts via demonstrations, we name it EPA as the name EDA isalready taken by a well-known NLP method \\citep{wei-zou-2019-eda}.} thateffectively minimizes user efforts in writing demonstrations while improvingthe model performance at the same time. EPA achieves these goals byautomatically augmenting the demonstrations with multiple sources/targets,where each of them paraphrases each other. This is well motivated as augmentingdata via paraphrasing effectively improves neural language models. EPA thusemploys paraphrasing as an augmentation method for in-context learning.Extensive experiments indicate that EPA effectively improves both NLU and NLGtasks, covering from natural language inference to machine translation intranslating tens of languages.\\footnote{Code and data will be released uponpublication.}", "title": "epa easy prompt augmentation on large language models via multiple sources and multiple targets", "url": "http://arxiv.org/pdf/2309.04725v1.pdf", "tokenized_text": "large_language large language llms shown promising performance nlptasks task performance improved task demonstrations head usually betterperformance achieved demonstrations asking write demonstrations cumbersome simple cost paper_proposes paper proposes novel method called paper demonstrations taken known nlp method minimizes user efforts writing demonstrations performance time achieves goals augmenting demonstrations multiple sources targets paraphrases motivated paraphrasing effectively improves neural language_models language paraphrasing augmentation method context_learning context learning extensive_experiments extensive experiments indicate effectively improves nlu covering natural_language natural language inference machine_translation machine translation tens data released"}
{"id": "nan", "abstract": "  Conversational search provides a natural interface for information retrieval(IR). Recent approaches have demonstrated promising results in applying denseretrieval to conversational IR. However, training dense retrievers requireslarge amounts of in-domain paired data. This hinders the development ofconversational dense retrievers, as abundant in-domain conversations areexpensive to collect. In this paper, we propose CONVERSER, a framework fortraining conversational dense retrievers with at most 6 examples of in-domaindialogues. Specifically, we utilize the in-context learning capability of largelanguage models to generate conversational queries given a passage in theretrieval corpus. Experimental results on conversational retrieval benchmarksOR-QuAC and TREC CAsT 19 show that the proposed CONVERSER achieves comparableperformance to fully-supervised models, demonstrating the effectiveness of ourproposed framework in few-shot conversational dense retrieval. All source codeand generated datasets are available at https://github.com/MiuLab/CONVERSER", "title": "converser fewshot conversational dense retrieval with synthetic data generation", "url": "http://arxiv.org/pdf/2309.06748v1.pdf", "tokenized_text": "conversational search provides natural interface information recent approaches demonstrated promising_results promising results applying conversational ir training dense retrievers amounts domain paired data hinders development dense retrievers abundant domain conversations collect paper propose framework conversational dense retrievers examples domaindialogues specifically utilize context_learning context learning capability largelanguage_models largelanguage generate conversational queries given passage theretrieval corpus experimental_results experimental results conversational retrieval trec cast 19 proposed achieves comparableperformance fully supervised demonstrating effectiveness ourproposed framework shot conversational dense retrieval source codeand generated datasets available"}
{"id": "nan", "abstract": "  Language models (LMs) excel in in-distribution (ID) scenarios where train andtest data are independent and identically distributed. However, theirperformance often degrades in real-world applications like argument mining.Such degradation happens when new topics emerge, or other text domains andlanguages become relevant. To assess LMs' generalization abilities in suchout-of-distribution (OOD) scenarios, we simulate such distribution shifts bydeliberately withholding specific instances for testing, as from the socialmedia domain or the topic Solar Energy.  Unlike prior studies focusing on specific shifts and metrics in isolation, wecomprehensively analyze OOD generalization. We define three metrics to pinpointgeneralization flaws and propose eleven classification tasks covering topic,domain, and language shifts. Overall, we find superior performance ofprompt-based fine-tuning, notably when train and test splits primarily differsemantically. Simultaneously, in-context learning is more effective thanprompt-based or vanilla fine-tuning for tasks when training data embodies heavydiscrepancies in label distribution compared to testing data. This reveals acrucial drawback of gradient-based learning: it biases LMs regarding suchstructural obstacles.", "title": "bridging topic, domain, and language shifts an evaluation of comprehensive outofdistribution scenarios", "url": "http://arxiv.org/pdf/2309.08316v1.pdf", "tokenized_text": "language_models language lms excel distribution id scenarios train data independent identically distributed theirperformance degrades real world_applications world applications like argument mining degradation happens new topics emerge text domains andlanguages relevant assess lms generalization abilities distribution ood scenarios simulate distribution shifts specific instances testing socialmedia domain topic energy unlike prior studies focusing specific shifts metrics isolation wecomprehensively analyze ood generalization define metrics flaws propose classification tasks covering topic domain language shifts overall find superior_performance superior performance ofprompt based fine tuning notably train test splits primarily simultaneously context_learning context learning effective based vanilla fine tuning tasks training_data training data embodies label distribution compared testing data reveals acrucial drawback gradient based learning biases lms obstacles"}
{"id": "nan", "abstract": "  We evaluate the ability of semantic parsers based on large language models(LLMs) to handle contextual utterances. In real-world settings, there typicallyexists only a limited number of annotated contextual utterances due toannotation cost, resulting in an imbalance compared to non-contextualutterances. Therefore, parsers must adapt to contextual utterances with a fewtraining examples. We examine four major paradigms for doing so inconversational semantic parsing i.e., Parse-with-Utterance-History,Parse-with-Reference-Program, Parse-then-Resolve, and Rewrite-then-Parse. Tofacilitate such cross-paradigm comparisons, we constructSMCalFlow-EventQueries, a subset of contextual examples from SMCalFlow withadditional annotations. Experiments with in-context learning and fine-tuningsuggest that Rewrite-then-Parse is the most promising paradigm whenholistically considering parsing accuracy, annotation cost, and error types.", "title": "fewshot adaptation for parsing contextual utterances with llms", "url": "http://arxiv.org/pdf/2309.10168v1.pdf", "tokenized_text": "evaluate ability semantic parsers based large_language large language models(llms handle contextual utterances real world settings limited number annotated contextual utterances cost resulting imbalance compared non parsers adapt contextual utterances examples examine major paradigms inconversational semantic_parsing semantic parsing i.e. parse utterance history parse reference program parse resolve rewrite parse cross paradigm comparisons subset contextual examples smcalflow annotations experiments context_learning context learning fine rewrite parse promising paradigm considering parsing accuracy annotation cost error types"}
{"id": "nan", "abstract": "  Controllable text generation is a fundamental aspect of natural languagegeneration, with numerous methods proposed for different constraint types.However, these approaches often require significant architectural or decodingmodifications, making them challenging to apply to additional constraints orresolve different constraint combinations. To address this, our paperintroduces Regular Expression Instruction (REI), which utilizes aninstruction-based mechanism to fully exploit regular expressions' advantages touniformly model diverse constraints. Specifically, our REI supports all popularfine-grained controllable generation constraints, i.e., lexical, positional,and length, as well as their complex combinations, via regular expression-styleinstructions. Our method only requires fine-tuning on medium-scale languagemodels or few-shot, in-context learning on large language models, and requiresno further adjustment when applied to various constraint combinations.Experiments demonstrate that our straightforward approach yields high successrates and adaptability to various constraints while maintaining competitivenessin automatic metrics and outperforming most previous baselines.", "title": "toward unified controllable text generation via regular expression instruction", "url": "http://arxiv.org/pdf/2309.10447v2.pdf", "tokenized_text": "controllable text generation fundamental aspect natural languagegeneration numerous methods proposed different constraint types approaches require significant architectural making challenging apply additional constraints different constraint combinations address paperintroduces regular expression instruction utilizes aninstruction based mechanism fully exploit regular expressions advantages diverse constraints specifically supports grained controllable generation constraints i.e. lexical positional length complex combinations regular expression method requires fine tuning medium scale languagemodels shot context_learning context learning large_language large language adjustment applied constraint combinations experiments_demonstrate experiments demonstrate straightforward approach yields high adaptability constraints maintaining automatic metrics outperforming previous baselines"}
{"id": "nan", "abstract": "  By integrating recent advances in large language models (LLMs) and generativemodels into the emerging semantic communication (SC) paradigm, in this articlewe put forward to a novel framework of language-oriented semantic communication(LSC). In LSC, machines communicate using human language messages that can beinterpreted and manipulated via natural language processing (NLP) techniquesfor SC efficiency. To demonstrate LSC's potential, we introduce threeinnovative algorithms: 1) semantic source coding (SSC) which compresses a textprompt into its key head words capturing the prompt's syntactic essence whilemaintaining their appearance order to keep the prompt's context; 2) semanticchannel coding (SCC) that improves robustness against errors by substitutinghead words with their lenghthier synonyms; and 3) semantic knowledgedistillation (SKD) that produces listener-customized prompts via in-contextlearning the listener's language style. In a communication task for progressivetext-to-image generation, the proposed methods achieve higher perceptualsimilarities with fewer transmissions while enhancing robustness in noisycommunication channels.", "title": "languageoriented communication with semantic coding and knowledge distillation for texttoimage generation", "url": "http://arxiv.org/pdf/2309.11127v1.pdf", "tokenized_text": "integrating recent_advances recent advances large_language large language llms emerging semantic communication sc paradigm forward novel framework language oriented semantic machines communicate human language messages manipulated natural_language natural language processing nlp sc efficiency demonstrate potential introduce algorithms semantic source coding ssc compresses textprompt key head words capturing syntactic essence appearance order context coding improves robustness errors words synonyms semantic knowledgedistillation produces customized contextlearning language style communication task image_generation image generation proposed methods achieve higher fewer transmissions enhancing robustness channels"}
{"id": "nan", "abstract": "  Resolving semantic ambiguity has long been recognised as a central challengein the field of Machine Translation. Recent work on benchmarking translationperformance on ambiguous sentences has exposed the limitations of conventionalNeural Machine Translation (NMT) systems, which fail to handle many such cases.Large language models (LLMs) have emerged as a promising alternative,demonstrating comparable performance to traditional NMT models whileintroducing new paradigms for controlling the target outputs. In this paper, westudy the capabilities of LLMs to translate \"ambiguous sentences\" - i.e. thosecontaining highly polysemous words and/or rare word senses. We also propose twoways to improve their disambiguation capabilities, through a) in-contextlearning and b) fine-tuning on carefully curated ambiguous datasets.Experiments show that our methods can match or outperform state-of-the-artsystems such as DeepL and NLLB in four out of five language directions. Ourresearch provides valuable insights into effectively adapting LLMs to becomebetter disambiguators during Machine Translation. We release our curateddisambiguation corpora and resources athttps://data.statmt.org/ambiguous-europarl.", "title": "towards effective disambiguation for machine translation with large language models", "url": "http://arxiv.org/pdf/2309.11668v2.pdf", "tokenized_text": "resolving semantic ambiguity long central field machine_translation machine translation recent_work recent work benchmarking ambiguous sentences exposed limitations machine_translation machine translation nmt systems fail handle cases large_language large language llms emerged promising alternative demonstrating comparable performance traditional nmt new paradigms controlling target outputs paper capabilities llms translate ambiguous sentences i.e. highly polysemous words and/or rare word senses propose improve disambiguation capabilities contextlearning fine tuning carefully curated ambiguous datasets experiments methods match outperform state artsystems nllb language directions provides valuable insights effectively adapting llms machine_translation machine translation release corpora resources"}
{"id": "nan", "abstract": "  Large language models (LLMs) have had a huge impact on society due to theirimpressive capabilities and vast knowledge of the world. Various applicationsand tools have been created that allow users to interact with these models in ablack-box scenario. However, one limitation of this scenario is that userscannot modify the internal knowledge of the model, and the only way to add ormodify internal knowledge is by explicitly mentioning it to the model duringthe current interaction. This learning process is called in-context training,and it refers to training that is confined to the user's current session orcontext. In-context learning has significant applications, but also haslimitations that are seldom studied. In this paper, we present a study thatshows how the model can suffer from interference between information thatcontinually flows in the context, causing it to forget previously learnedknowledge, which can reduce the model's performance. Along with showing theproblem, we propose an evaluation benchmark based on the bAbI dataset.", "title": "incontext interference in chatbased large language models", "url": "http://arxiv.org/pdf/2309.12727v1.pdf", "tokenized_text": "large_language large language llms huge impact society capabilities vast knowledge world tools created allow users interact box scenario limitation scenario modify internal knowledge way add internal knowledge explicitly mentioning duringthe current interaction learning process called context training refers training user current context_learning context learning significant applications seldom studied paper present study suffer interference information flows context causing previously reduce performance showing propose evaluation benchmark based babi dataset"}
{"id": "nan", "abstract": "  Affect recognition, encompassing emotions, moods, and feelings, plays apivotal role in human communication. In the realm of conversational artificialintelligence (AI), the ability to discern and respond to human affective cuesis a critical factor for creating engaging and empathetic interactions. Thisstudy delves into the capacity of large language models (LLMs) to recognisehuman affect in conversations, with a focus on both open-domain chit-chatdialogues and task-oriented dialogues. Leveraging three diverse datasets,namely IEMOCAP, EmoWOZ, and DAIC-WOZ, covering a spectrum of dialogues fromcasual conversations to clinical interviews, we evaluated and compared LLMs'performance in affect recognition. Our investigation explores the zero-shot andfew-shot capabilities of LLMs through in-context learning (ICL) as well astheir model capacities through task-specific fine-tuning. Additionally, thisstudy takes into account the potential impact of automatic speech recognition(ASR) errors on LLM predictions. With this work, we aim to shed light on theextent to which LLMs can replicate human-like affect recognition capabilitiesin conversations.", "title": "affect recognition in conversations using large language models", "url": "http://arxiv.org/pdf/2309.12881v1.pdf", "tokenized_text": "affect recognition encompassing emotions feelings plays role human communication realm conversational artificialintelligence ai ability discern respond human affective critical factor creating engaging empathetic interactions thisstudy delves capacity large_language large language llms affect conversations focus open domain chit task oriented dialogues leveraging diverse datasets iemocap covering spectrum dialogues conversations clinical interviews evaluated compared affect recognition investigation explores zero shot andfew shot capabilities llms context_learning context learning icl capacities task specific fine tuning additionally thisstudy takes account potential impact automatic speech errors llm predictions work aim shed light theextent llms replicate human like affect recognition conversations"}
{"id": "nan", "abstract": "  Recent advancements in large language models (LLMs) on language modeling andemergent capabilities make them a promising reference-free evaluator of naturallanguage generation quality, and a competent alternative to human evaluation.However, hindered by the closed-source or high computational demand to host andtune, there is a lack of practice to further calibrate an off-the-shelfLLM-based evaluator towards better human alignment. In this work, we proposeAutoCalibrate, a multi-stage, gradient-free approach to automatically calibrateand align an LLM-based evaluator toward human preference. Instead of explicitlymodeling human preferences, we first implicitly encompass them within a set ofhuman labels. Then, an initial set of scoring criteria is drafted by thelanguage model itself, leveraging in-context learning on different few-shotexamples. To further calibrate this set of criteria, we select the bestperformers and re-draft them with self-refinement. Our experiments on multipletext quality evaluation datasets illustrate a significant improvement incorrelation with expert evaluation through calibration. Our comprehensivequalitative analysis conveys insightful intuitions and observations on theessence of effective scoring criteria.", "title": "calibrating llmbased evaluator", "url": "http://arxiv.org/pdf/2309.13308v1.pdf", "tokenized_text": "recent advancements large_language large language llms language modeling capabilities promising reference free evaluator naturallanguage generation quality competent alternative human evaluation hindered closed source high computational demand host lack practice calibrate based evaluator better human alignment work multi stage gradient free approach automatically align llm based evaluator human preference instead human preferences implicitly encompass set ofhuman labels initial set scoring criteria thelanguage leveraging context_learning context learning different shotexamples calibrate set criteria select draft self refinement experiments quality evaluation datasets illustrate significant improvement expert evaluation calibration analysis insightful observations effective scoring criteria"}
{"id": "nan", "abstract": "  Large Language Models (LLMs), although powerful in general domains, oftenperform poorly on domain-specific tasks like medical question answering (QA).Moreover, they tend to function as \"black-boxes,\" making it challenging tomodify their behavior. Addressing this, our study delves into model editingutilizing in-context learning, aiming to improve LLM responses without the needfor fine-tuning or retraining. Specifically, we propose a comprehensiveretrieval strategy to extract medical facts from an external knowledge base,and then we incorporate them into the query prompt for the LLM. Focusing onmedical QA using the MedQA-SMILE dataset, we evaluate the impact of differentretrieval models and the number of facts provided to the LLM. Notably, ouredited Vicuna model exhibited an accuracy improvement from 44.46% to 48.54%.This work underscores the potential of model editing to enhance LLMperformance, offering a practical approach to mitigate the challenges ofblack-box LLMs.", "title": "mededit model editing for medical question answering with external knowledge bases", "url": "http://arxiv.org/pdf/2309.16035v1.pdf", "tokenized_text": "large_language large language llms powerful general domains poorly domain specific tasks like medical question_answering question answering tend function black boxes making challenging tomodify behavior addressing study delves context_learning context learning aiming improve llm responses needfor fine tuning retraining specifically propose strategy extract medical facts external_knowledge external knowledge base incorporate query llm focusing qa medqa dataset evaluate impact number facts provided llm notably vicuna exhibited accuracy improvement work underscores potential editing enhance offering practical approach mitigate challenges ofblack box llms"}
{"id": "nan", "abstract": "  While large pre-trained language models (LLMs) have shown their impressivecapabilities in various NLP tasks, they are still under-explored in themisinformation domain. In this paper, we examine LLMs with in-context learning(ICL) for news claim verification, and find that only with 4-shot demonstrationexamples, the performance of several prompting methods can be comparable withprevious supervised models. To further boost performance, we introduce aHierarchical Step-by-Step (HiSS) prompting method which directs LLMs toseparate a claim into several subclaims and then verify each of them viamultiple questions-answering steps progressively. Experiment results on twopublic misinformation datasets show that HiSS prompting outperformsstate-of-the-art fully-supervised approach and strong few-shot ICL-enabledbaselines.", "title": "towards llmbased fact verification on news claims with a hierarchical stepbystep prompting method", "url": "http://arxiv.org/pdf/2310.00305v1.pdf", "tokenized_text": "large pre trained_language trained language llms shown impressivecapabilities nlp_tasks nlp tasks explored domain paper examine llms context learning(icl news claim verification find shot demonstrationexamples performance methods comparable withprevious supervised boost performance introduce ahierarchical step step method directs llms claim verify questions answering steps progressively experiment results misinformation datasets outperformsstate art fully supervised approach strong shot icl"}
{"id": "nan", "abstract": "  Large language and vision-language models are rapidly being deployed inpractice thanks to their impressive capabilities in instruction following,in-context learning, and so on. This raises an urgent need to carefully analysetheir robustness so that stakeholders can understand if and when such modelsare trustworthy enough to be relied upon in any given application. In thispaper, we highlight a specific vulnerability in popular models, namelypermutation sensitivity in multiple-choice question answering (MCQA).Specifically, we show empirically that popular models are vulnerable toadversarial permutation in answer sets for multiple-choice prompting, which issurprising as models should ideally be as invariant to prompt permutation ashumans are. These vulnerabilities persist across various model sizes, and existin very recent language and vision-language models. Code is available at\\url{https://github.com/ys-zong/FoolyourVLLMs}.", "title": "fool your (vision and) language model with embarrassingly simple permutations", "url": "http://arxiv.org/pdf/2310.01651v1.pdf", "tokenized_text": "large_language large language vision language_models language rapidly deployed thanks impressive capabilities instruction_following instruction following context_learning context learning raises urgent need carefully robustness stakeholders understand modelsare trustworthy relied given application thispaper highlight specific vulnerability popular sensitivity multiple choice question_answering question answering empirically popular vulnerable permutation answer sets multiple choice invariant permutation vulnerabilities persist sizes recent language vision language_models language code_is_available code available at\\url{https://github.com"}
{"id": "nan", "abstract": "  8 years after the visual question answering (VQA) task was proposed, accuracyremains the primary metric for automatic evaluation. VQA Accuracy has beeneffective so far in the IID evaluation setting. However, our community isundergoing a shift towards open-ended generative models and OOD evaluation. Inthis new paradigm, the existing VQA Accuracy metric is overly stringent andunderestimates the performance of VQA systems. Thus, there is a need to developmore robust automatic VQA metrics that serve as a proxy for human judgment. Inthis work, we propose to leverage the in-context learning capabilities ofinstruction-tuned large language models (LLMs) to build a better VQA metric. Weformulate VQA evaluation as an answer-rating task where the LLM is instructedto score the accuracy of a candidate answer given a set of reference answers.We demonstrate the proposed metric better correlates with human judgmentcompared to existing metrics across several VQA models and benchmarks. We hopewide adoption of our metric will contribute to better estimating the researchprogress on the VQA task.", "title": "improving automatic vqa evaluation using large language models", "url": "http://arxiv.org/pdf/2310.02567v1.pdf", "tokenized_text": "years visual question_answering question answering vqa task proposed primary metric automatic evaluation vqa accuracy far iid evaluation setting community shift open ended generative ood evaluation inthis new_paradigm new paradigm existing vqa accuracy metric overly performance vqa systems need robust automatic vqa metrics serve proxy human judgment inthis work propose leverage context_learning context learning capabilities ofinstruction tuned large_language large language llms build better vqa metric weformulate vqa evaluation answer rating task llm score accuracy candidate answer given set reference answers demonstrate proposed metric better correlates human existing metrics vqa benchmarks adoption metric contribute better estimating vqa task"}
{"id": "nan", "abstract": "  Language agents, which use a large language model (LLM) capable of in-contextlearning to interact with an external environment, have recently emerged as apromising approach to control tasks. We present the first language-agentapproach to formal theorem-proving. Our method, COPRA, uses a high-capacity,black-box LLM (GPT-4) as part of a policy for a stateful backtracking search.During the search, the policy can select proof tactics and retrieve lemmas anddefinitions from an external database. Each selected tactic is executed in theunderlying proof framework, and the execution feedback is used to build theprompt for the next policy invocation. The search also tracks selectedinformation from its history and uses it to reduce hallucinations andunnecessary LLM queries.  We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasksfrom the Compcert project. On these benchmarks, COPRA is significantly betterthan one-shot invocations of GPT-4, as well as state-of-the-art modelsfine-tuned on proof data, at finding correct proofs quickly.", "title": "a languageagent approach to formal theoremproving", "url": "http://arxiv.org/pdf/2310.04353v1.pdf", "tokenized_text": "language agents use large_language large language llm capable contextlearning interact external environment recently emerged apromising approach control tasks present language formal theorem proving method uses high capacity black box llm gpt-4 policy search search policy select proof tactics retrieve lemmas external database selected executed theunderlying proof framework execution feedback build theprompt policy search tracks history uses reduce hallucinations llm queries evaluate minif2f benchmark set tasksfrom project benchmarks significantly betterthan shot gpt-4 state art tuned proof data finding correct proofs quickly"}
{"id": "nan", "abstract": "  Large language models (LLMs) can perform a new task by merely conditioning ontask instructions and a few input-output examples, without optimizing anyparameters. This is called In-Context Learning (ICL). In-context InformationExtraction (IE) has recently garnered attention in the research community.However, the performance of In-context IE generally lags behind thestate-of-the-art supervised expert models. We highlight a key reason for thisshortfall: underspecified task description. The limited-length contextstruggles to thoroughly express the intricate IE task instructions and variousedge cases, leading to misalignment in task comprehension with humans. In thispaper, we propose a Guideline Learning (GL) framework for In-context IE whichreflectively learns and follows guidelines. During the learning phrase, GLautomatically synthesizes a set of guidelines based on a few error cases, andduring inference, GL retrieves helpful guidelines for better ICL. Moreover, wepropose a self-consistency-based active learning method to enhance theefficiency of GL. Experiments on event extraction and relation extraction showthat GL can significantly improve the performance of in-context IE.", "title": "guideline learning for incontext information extraction", "url": "http://arxiv.org/pdf/2310.05066v2.pdf", "tokenized_text": "large_language large language llms perform new task merely conditioning ontask instructions input output examples optimizing called context_learning context learning icl context informationextraction ie recently garnered attention research community performance context ie generally lags thestate art supervised expert highlight key reason underspecified task description limited length thoroughly express intricate ie task instructions cases leading misalignment task comprehension humans thispaper propose guideline learning framework context ie learns follows guidelines learning phrase synthesizes set guidelines based error cases inference retrieves helpful guidelines better icl wepropose self consistency based active learning method enhance experiments event extraction relation_extraction relation extraction showthat significantly improve performance context ie"}
{"id": "nan", "abstract": "  Empathetic dialogue is an indispensable part of building harmonious socialrelationships and contributes to the development of a helpful AI. Previousapproaches are mainly based on fine small-scale language models. With theadvent of ChatGPT, the application effect of large language models (LLMs) inthis field has attracted great attention. This work empirically investigatesthe performance of LLMs in generating empathetic responses and proposes threeimprovement methods of semantically similar in-context learning, two-stageinteractive generation, and combination with the knowledge base. Extensiveexperiments show that LLMs can significantly benefit from our proposed methodsand is able to achieve state-of-the-art performance in both automatic and humanevaluations. Additionally, we explore the possibility of GPT-4 simulating humanevaluators.", "title": "harnessing the power of large language models for empathetic response generation empirical investigations and improvements", "url": "http://arxiv.org/pdf/2310.05140v1.pdf", "tokenized_text": "empathetic dialogue indispensable building contributes development helpful ai mainly based fine small scale language_models language theadvent chatgpt application effect large_language large language llms inthis field attracted great attention work empirically investigatesthe performance llms generating empathetic responses proposes methods semantically similar context_learning context learning generation combination knowledge base extensiveexperiments llms significantly benefit proposed able achieve state art performance automatic humanevaluations additionally explore possibility gpt-4 simulating"}
{"id": "nan", "abstract": "  Large language models (LLMs) with in-context learning have demonstratedimpressive generalization capabilities in the cross-domain text-to-SQL task,without the use of in-domain annotations. However, incorporating in-domaindemonstration examples has been found to greatly enhance LLMs' performance. Inthis paper, we delve into the key factors within in-domain examples thatcontribute to the improvement and explore whether we can harness these benefitswithout relying on in-domain annotations. Based on our findings, we propose ademonstration selection framework ODIS which utilizes both out-of-domainexamples and synthetically generated in-domain examples to constructdemonstrations. By retrieving demonstrations from hybrid sources, ODISleverages the advantages of both, showcasing its effectiveness compared tobaseline methods that rely on a single data source. Furthermore, ODISoutperforms state-of-the-art approaches on two cross-domain text-to-SQLdatasets, with improvements of 1.1 and 11.8 points in execution accuracy,respectively.", "title": "selective demonstrations for crossdomain texttosql", "url": "http://arxiv.org/pdf/2310.06302v1.pdf", "tokenized_text": "large_language large language llms context_learning context learning generalization capabilities cross domain text sql task use domain annotations incorporating examples found greatly enhance llms performance inthis_paper inthis paper delve key factors domain examples improvement explore harness relying domain annotations based findings propose selection framework utilizes synthetically generated domain examples retrieving demonstrations hybrid sources advantages showcasing effectiveness compared methods rely single data source furthermore state art approaches cross domain text improvements points execution accuracy respectively"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) have shown remarkable success in various tasks,but concerns about their safety and the potential for generating maliciouscontent have emerged. In this paper, we explore the power of In-ContextLearning (ICL) in manipulating the alignment ability of LLMs. We find that byproviding just few in-context demonstrations without fine-tuning, LLMs can bemanipulated to increase or decrease the probability of jailbreaking, i.e.answering malicious prompts. Based on these observations, we propose In-ContextAttack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guardingaligned language model purposes. ICA crafts malicious contexts to guide modelsin generating harmful outputs, while ICD enhances model robustness bydemonstrations of rejecting to answer harmful prompts. Our experiments show theeffectiveness of ICA and ICD in increasing or reducing the success rate ofadversarial jailbreaking attacks. Overall, we shed light on the potential ofICL to influence LLM behavior and provide a new perspective for enhancing thesafety and alignment of LLMs.", "title": "jailbreak and guard aligned language models with only few incontext demonstrations", "url": "http://arxiv.org/pdf/2310.06387v1.pdf", "tokenized_text": "large_language large language llms shown remarkable success tasks concerns safety potential generating emerged paper explore power contextlearning icl manipulating alignment ability llms find context demonstrations fine tuning llms increase decrease probability jailbreaking malicious based observations propose context defense icd methods jailbreaking language_model language purposes crafts malicious contexts guide modelsin generating harmful outputs icd enhances robustness answer harmful experiments theeffectiveness icd increasing reducing success_rate success rate jailbreaking attacks overall shed light potential oficl influence llm behavior provide new perspective enhancing alignment llms"}
{"id": "nan", "abstract": "  In many legal processes being able to action on the concrete implication of alegal question can be valuable to automating human review or signalling certainconditions (e.g., alerts around automatic renewal). To support such tasks, wepresent a form of legal question answering that seeks to return one (or more)fixed answers for a question about a contract clause. After showing thatunstructured generative question answering can have questionable outcomes forsuch a task, we discuss our exploration methodology for legal questionanswering prompts using OpenAI's \\textit{GPT-3.5-Turbo} and provide a summaryof insights.  Using insights gleaned from our qualitative experiences, we compare ourproposed template prompts against a common semantic matching approach and findthat our prompt templates are far more accurate despite being less reliable inthe exact response return. With some additional tweaks to prompts and the useof in-context learning, we are able to further improve the performance of ourproposed strategy while maximizing the reliability of responses as best we can.", "title": "a search for prompts generating structured answers from contracts", "url": "http://arxiv.org/pdf/2310.10141v1.pdf", "tokenized_text": "legal processes able action concrete question valuable automating human review e.g. automatic support tasks wepresent form legal question_answering question answering seeks return answers question contract clause showing generative question_answering question answering questionable outcomes task discuss exploration methodology legal questionanswering openai turbo provide insights insights qualitative experiences compare ourproposed template common semantic matching approach findthat prompt_templates templates far accurate despite reliable inthe exact response return additional tweaks useof context_learning context learning able improve performance ourproposed strategy maximizing reliability responses best"}
{"id": "nan", "abstract": "  The tasks of out-of-domain (OOD) intent discovery and generalized intentdiscovery (GID) aim to extend a closed intent classifier to open-world intentsets, which is crucial to task-oriented dialogue (TOD) systems. Previousmethods address them by fine-tuning discriminative models. Recently, althoughsome studies have been exploring the application of large language models(LLMs) represented by ChatGPT to various downstream tasks, it is still unclearfor the ability of ChatGPT to discover and incrementally extent OOD intents. Inthis paper, we comprehensively evaluate ChatGPT on OOD intent discovery andGID, and then outline the strengths and weaknesses of ChatGPT. Overall, ChatGPTexhibits consistent advantages under zero-shot settings, but is still at adisadvantage compared to fine-tuned models. More deeply, through a series ofanalytical experiments, we summarize and discuss the challenges faced by LLMsincluding clustering, domain-specific understanding, and cross-domainin-context learning scenarios. Finally, we provide empirical guidance forfuture directions to address these challenges.", "title": "large language models meet openworld intent discovery and recognition an evaluation of chatgpt", "url": "http://arxiv.org/pdf/2310.10176v1.pdf", "tokenized_text": "tasks domain ood intent discovery generalized aim extend closed intent classifier open world crucial task oriented dialogue tod systems address fine tuning discriminative recently studies exploring application large_language large language models(llms represented chatgpt downstream_tasks downstream tasks ability chatgpt discover incrementally extent ood intents inthis_paper inthis paper comprehensively evaluate chatgpt ood intent discovery outline strengths weaknesses chatgpt overall consistent advantages zero shot_settings shot settings compared fine tuned deeply series experiments summarize discuss challenges faced clustering domain specific understanding cross context_learning context learning scenarios finally provide empirical guidance forfuture directions address challenges"}
{"id": "nan", "abstract": "  In this work, we present MoConVQ, a novel unified framework for physics-basedmotion control leveraging scalable discrete representations. Building uponvector quantized variational autoencoders (VQ-VAE) and model-basedreinforcement learning, our approach effectively learns motion embeddings froma large, unstructured dataset spanning tens of hours of motion examples. Theresultant motion representation not only captures diverse motion skills butalso offers a robust and intuitive interface for various applications. Wedemonstrate the versatility of MoConVQ through several applications: universaltracking control from various motion sources, interactive character controlwith latent motion representations using supervised learning, physics-basedmotion generation from natural language descriptions using the GPT framework,and, most interestingly, seamless integration with large language models (LLMs)with in-context learning to tackle complex and abstract tasks.", "title": "moconvq unified physicsbased motion control via scalable discrete representations", "url": "http://arxiv.org/pdf/2310.10198v2.pdf", "tokenized_text": "work present novel unified framework physics control leveraging scalable discrete representations building quantized variational vq vae learning approach effectively learns motion embeddings large unstructured dataset spanning tens hours motion examples motion representation captures diverse motion skills butalso offers robust intuitive interface applications wedemonstrate versatility applications control motion sources interactive character latent motion representations supervised learning physics generation natural_language natural language descriptions gpt framework interestingly seamless integration large_language large language context_learning context learning tackle complex abstract tasks"}
{"id": "nan", "abstract": "  Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiringand annotating task-oriented dialogues, which can be time consuming and costly.However, DST extends beyond simple slot-filling and requires effective updatingstrategies for tracking dialogue state as conversations progress. In thispaper, we propose ParsingDST, a new In-Context Learning (ICL) method, tointroduce additional intricate updating strategies in zero-shot DST. Ourapproach reformulates the DST task by leveraging powerful Large Language Models(LLMs) and translating the original dialogue text to JSON through semanticparsing as an intermediate state. We also design a novel framework thatincludes more modules to ensure the effectiveness of updating strategies in thetext-to-JSON process. Experimental results demonstrate that our approachoutperforms existing zero-shot DST methods on MultiWOZ, exhibiting significantimprovements in Joint Goal Accuracy (JGA) and slot accuracy compared toexisting ICL methods.", "title": "semantic parsing by large language models for intricate updating strategies of zeroshot dialogue state tracking", "url": "http://arxiv.org/pdf/2310.10520v2.pdf", "tokenized_text": "zero shot dialogue_state_tracking dialogue state tracking dst addresses challenge annotating task oriented dialogues time_consuming time consuming costly dst extends simple slot filling requires effective tracking dialogue state conversations progress thispaper propose new context_learning context learning icl method tointroduce additional intricate updating strategies zero shot dst ourapproach reformulates dst task leveraging powerful large_language large language models(llms translating original dialogue text json semanticparsing intermediate state design novel framework thatincludes modules ensure effectiveness updating strategies thetext json process experimental_results experimental results demonstrate approachoutperforms existing zero shot dst methods multiwoz exhibiting significantimprovements joint goal accuracy slot accuracy compared toexisting icl methods"}
{"id": "nan", "abstract": "  Open Information Extraction (OIE) aims to extract objective structuredknowledge from natural texts, which has attracted growing attention to builddedicated models with human experience. As the large language models (LLMs)have exhibited remarkable in-context learning capabilities, a question arisesas to whether the task of OIE can be effectively tackled with this paradigm? Inthis paper, we explore solving the OIE problem by constructing an appropriatereasoning environment for LLMs. Specifically, we first propose a method toeffectively estimate the discrepancy of syntactic distribution between a LLMand test samples, which can serve as correlation evidence for preparingpositive demonstrations. Upon the evidence, we introduce a simple yet effectivemechanism to establish the reasoning environment for LLMs on specific tasks.Without bells and whistles, experimental results on the standard CaRB benchmarkdemonstrate that our $6$-shot approach outperforms state-of-the-art supervisedmethod, achieving an $55.3$ $F_1$ score. Further experiments on TACRED andACE05 show that our method can naturally generalize to other informationextraction tasks, resulting in improvements of $5.7$ and $6.8$ $F_1$ scores,respectively.", "title": "mastering the task of open information extraction with large language models and consistent reasoning environment", "url": "http://arxiv.org/pdf/2310.10590v1.pdf", "tokenized_text": "open information_extraction information extraction oie aims extract objective structuredknowledge natural texts attracted growing attention human experience large_language large language llms)have exhibited remarkable context_learning context learning capabilities question task oie effectively tackled paradigm inthis_paper inthis paper explore solving oie problem constructing environment llms specifically propose method toeffectively estimate discrepancy syntactic distribution test samples serve correlation evidence demonstrations evidence introduce simple establish reasoning environment llms specific tasks bells whistles experimental_results experimental results standard carb approach outperforms state art achieving score experiments tacred method naturally generalize informationextraction tasks resulting improvements 5.7 6.8 scores respectively"}
{"id": "nan", "abstract": "  Automatic evaluation of text generation is essential for improving theaccuracy of generation tasks. In light of the current trend towardsincreasingly larger decoder-based language models, we investigate automaticevaluation methods based on such models for text generation. This papercompares various methods, including tuning with encoder-based models and largelanguage models under equal conditions, on two different tasks, machinetranslation evaluation and semantic textual similarity, in two languages,Japanese and English. Experimental results show that compared to the tunedencoder-based models, the tuned decoder-based models perform poorly. Theanalysis of the causes for this suggests that the decoder-based models focus onsurface word sequences and do not capture meaning. It is also revealed thatin-context learning of very large decoder-based models such as ChatGPT makes itdifficult to identify fine-grained semantic differences.", "title": "exploring automatic evaluation methods based on a decoderbased llm for text generation", "url": "http://arxiv.org/pdf/2310.11026v1.pdf", "tokenized_text": "automatic evaluation text generation essential improving theaccuracy generation tasks light current trend larger decoder based language_models language investigate automaticevaluation methods based text generation methods including tuning encoder based largelanguage_models largelanguage equal conditions different tasks machinetranslation evaluation semantic textual similarity languages japanese english experimental_results experimental results compared based tuned decoder based perform poorly causes suggests decoder based focus word sequences capture meaning revealed thatin context_learning context learning large decoder based chatgpt makes itdifficult identify fine grained semantic differences"}
{"id": "nan", "abstract": "  Recently, researchers have made considerable improvements in dialogue systemswith the progress of large language models (LLMs) such as ChatGPT and GPT-4.These LLM-based chatbots encode the potential biases while retainingdisparities that can harm humans during interactions. The traditional biasesinvestigation methods often rely on human-written test cases. However, thesetest cases are usually expensive and limited. In this work, we propose afirst-of-its-kind method that automatically generates test cases to detectLLMs' potential gender bias. We apply our method to three well-known LLMs andfind that the generated test cases effectively identify the presence of biases.To address the biases identified, we propose a mitigation strategy that usesthe generated test cases as demonstrations for in-context learning tocircumvent the need for parameter fine-tuning. The experimental results showthat LLMs generate fairer responses with the proposed approach.", "title": "learning from red teaming gender bias provocation and mitigation in large language models", "url": "http://arxiv.org/pdf/2310.11079v1.pdf", "tokenized_text": "recently researchers considerable improvements dialogue progress large_language large language llms chatgpt llm based chatbots encode potential biases harm humans interactions traditional methods rely human written test_cases test cases cases usually expensive limited work propose afirst kind method automatically generates test_cases test cases potential gender bias apply method known llms generated test_cases test cases effectively identify presence biases address biases identified propose mitigation strategy generated test_cases test cases demonstrations context_learning context learning need parameter fine tuning experimental_results experimental results showthat llms generate responses proposed approach"}
{"id": "nan", "abstract": "  Penetration testing, an essential component of cybersecurity, allowsorganizations to proactively identify and remediate vulnerabilities in theirsystems, thus bolstering their defense mechanisms against potentialcyberattacks. One recent advancement in the realm of penetration testing is theutilization of Language Models (LLMs). We explore the intersection of LLMs andpenetration testing to gain insight into their capabilities and challenges inthe context of privilige escalation. We create an automated Linuxprivilege-escalation benchmark utilizing local virtual machines. We introducean LLM-guided privilege-escalation tool designed for evaluating different LLMsand prompt strategies against our benchmark. We analyze the impact of differentprompt designs, the benefits of in-context learning, and the advantages ofoffering high-level guidance to LLMs. We discuss challenging areas for LLMs,including maintaining focus during testing, coping with errors, and finallycomparing them with both stochastic parrots as well as with human hackers.", "title": "evaluating llms for privilegeescalation scenarios", "url": "http://arxiv.org/pdf/2310.11409v2.pdf", "tokenized_text": "testing essential component cybersecurity proactively identify vulnerabilities defense mechanisms recent advancement realm testing language_models language llms explore llms testing gain insight capabilities challenges inthe context escalation create automated escalation benchmark utilizing local virtual machines llm guided privilege escalation tool designed evaluating different llmsand strategies benchmark analyze impact differentprompt designs benefits context_learning context learning advantages high level guidance llms discuss challenging areas llms including maintaining focus testing coping errors stochastic parrots human"}
{"id": "nan", "abstract": "  In-context learning (ICL) is a new learning paradigm that has gainedpopularity along with the development of large language models. In this work,we adapt a recently proposed hardness metric, pointwise $\\mathcal{V}$-usableinformation (PVI), to an in-context version (in-context PVI). Compared to theoriginal PVI, in-context PVI is more efficient in that it requires only a fewexemplars and does not require fine-tuning. We conducted a comprehensiveempirical analysis to evaluate the reliability of in-context PVI. Our findingsindicate that in-context PVI estimates exhibit similar characteristics to theoriginal PVI. Specific to the in-context setting, we show that in-context PVIestimates remain consistent across different exemplar selections and numbers ofshots. The variance of in-context PVI estimates across different exemplarselections is insignificant, which suggests that in-context PVI are stable.Furthermore, we demonstrate how in-context PVI can be employed to identifychallenging instances. Our work highlights the potential of in-context PVI andprovides new insights into the capabilities of ICL.", "title": "measuring pointwise $mathcal{v}$usable information incontextly", "url": "http://arxiv.org/pdf/2310.12300v1.pdf", "tokenized_text": "context_learning context learning icl new learning paradigm development large_language large language work adapt recently proposed hardness metric pointwise context version context compared theoriginal context efficient requires require fine tuning conducted analysis evaluate reliability context findingsindicate context estimates exhibit similar characteristics theoriginal specific context setting context remain consistent different exemplar selections numbers variance context estimates different insignificant suggests context stable furthermore demonstrate context employed instances work highlights potential context new insights capabilities icl"}
{"id": "nan", "abstract": "  Large language models (LLMs) are susceptible to red teaming attacks, whichcan induce LLMs to generate harmful content. Previous research constructsattack prompts via manual or automatic methods, which have their ownlimitations on construction cost and quality. To address these issues, wepropose an integrated approach that combines manual and automatic methods toeconomically generate high-quality attack prompts. Specifically, consideringthe impressive capabilities of newly emerged LLMs, we propose an attackframework to instruct LLMs to mimic human-generated prompts through in-contextlearning. Furthermore, we propose a defense framework that fine-tunes victimLLMs through iterative interactions with the attack framework to enhance theirsafety against red teaming attacks. Extensive experiments on different LLMsvalidate the effectiveness of our proposed attack and defense frameworks.Additionally, we release a series of attack prompts datasets named SAP withvarying sizes, facilitating the safety evaluation and enhancement of more LLMs.Our code and dataset is available on https://github.com/Aatrox103/SAP .", "title": "attack prompt generation for red teaming and defending large language models", "url": "http://arxiv.org/pdf/2310.12505v1.pdf", "tokenized_text": "large_language large language llms susceptible red teaming attacks whichcan induce llms generate harmful content previous research manual automatic methods ownlimitations construction cost quality address issues wepropose integrated approach combines manual automatic methods generate high quality attack specifically impressive capabilities newly emerged llms propose instruct llms mimic human generated contextlearning furthermore propose defense framework fine tunes iterative interactions attack framework enhance red teaming attacks extensive_experiments extensive experiments different effectiveness proposed attack defense frameworks additionally release series attack datasets named sap withvarying sizes facilitating safety evaluation enhancement llms code dataset available"}
{"id": "nan", "abstract": "  Large language models (LLMs) have exhibited considerable cross-lingualgeneralization abilities, whereby they implicitly transfer knowledge acrosslanguages. However, the transfer is not equally successful for all languages,especially for low-resource ones, which poses an ongoing challenge. It isunclear whether we have reached the limits of implicit cross-lingualgeneralization and if explicit knowledge transfer is viable. In this paper, weinvestigate the potential for explicitly aligning conceptual correspondencebetween languages to enhance cross-lingual generalization. Using the syntacticaspect of language as a testbed, our analyses of 43 languages reveal a highdegree of alignability among the spaces of structural concepts within eachlanguage for both encoder-only and decoder-only LLMs. We then propose ameta-learning-based method to learn to align conceptual spaces of differentlanguages, which facilitates zero-shot and few-shot generalization in conceptclassification and also offers insights into the cross-lingual in-contextlearning phenomenon. Experiments on syntactic analysis tasks show that ourapproach achieves competitive results with state-of-the-art methods and narrowsthe performance gap between languages, particularly benefiting those withlimited resources.", "title": "are structural concepts universal in transformer language models towards interpretable crosslingual generalization", "url": "http://arxiv.org/pdf/2310.12794v1.pdf", "tokenized_text": "large_language large language llms exhibited considerable cross abilities implicitly transfer knowledge acrosslanguages transfer equally successful languages especially low resource ones poses ongoing challenge isunclear reached limits implicit cross explicit knowledge transfer viable paper weinvestigate potential explicitly aligning conceptual languages enhance cross lingual generalization language testbed analyses 43 languages reveal spaces structural concepts encoder decoder llms propose ameta learning based method learn align conceptual spaces facilitates zero shot shot generalization offers insights cross lingual contextlearning phenomenon experiments syntactic analysis tasks ourapproach achieves competitive results state art methods performance gap languages particularly benefiting resources"}
{"id": "nan", "abstract": "  Finding the best way of adapting pre-trained language models to a task is abig challenge in current NLP. Just like the previous generation of task-tunedmodels (TT), models that are adapted to tasks via in-context-learning (ICL) arerobust in some setups but not in others. Here, we present a detailed analysisof which design choices cause instabilities and inconsistencies in LLMpredictions. First, we show how spurious correlations between inputdistributions and labels -- a known issue in TT models -- form only a minorproblem for prompted models. Then, we engage in a systematic, holisticevaluation of different factors that have been found to influence predictionsin a prompting setup. We test all possible combinations of a range of factorson both vanilla and instruction-tuned (IT) LLMs of different scale andstatistically analyse the results to show which factors are the mostinfluential, interactive or stable. Our results show which factors can be usedwithout precautions and which should be avoided or handled with care in mostsettings.", "title": "mind the instructions a holistic evaluation of consistency and interactions in promptbased learning", "url": "http://arxiv.org/pdf/2310.13486v1.pdf", "tokenized_text": "finding best way adapting pre trained_language trained language task challenge current nlp like previous generation task tunedmodels adapted tasks context learning icl setups present detailed design choices cause inconsistencies llmpredictions spurious correlations labels known issue form prompted engage systematic different factors found influence setup test possible combinations range vanilla instruction tuned llms different scale analyse results factors interactive stable results factors avoided handled care"}
{"id": "nan", "abstract": "  This paper is on the problem of Knowledge-Based Visual Question Answering(KB-VQA). Recent works have emphasized the significance of incorporating bothexplicit (through external databases) and implicit (through LLMs) knowledge toanswer questions requiring external knowledge effectively. A common limitationof such approaches is that they consist of relatively complicated pipelines andoften heavily rely on accessing GPT-3 API. Our main contribution in this paperis to propose a much simpler and readily reproducible pipeline which, in anutshell, is based on efficient in-context learning by prompting LLaMA (1 and2) using question-informative captions as contextual information. Contrary torecent approaches, our method is training-free, does not require access toexternal databases or APIs, and yet achieves state-of-the-art accuracy on theOK-VQA and A-OK-VQA datasets. Finally, we perform several ablation studies tounderstand important aspects of our method. Our code is publicly available athttps://github.com/alexandrosXe/ASimple-Baseline-For-Knowledge-Based-VQA", "title": "a simple baseline for knowledgebased visual question answering", "url": "http://arxiv.org/pdf/2310.13570v2.pdf", "tokenized_text": "paper problem visual question recent works significance incorporating external databases implicit llms knowledge toanswer questions requiring external_knowledge external knowledge effectively common approaches consist relatively complicated pipelines andoften heavily rely accessing gpt-3 api main contribution propose simpler readily reproducible pipeline based efficient context_learning context learning llama and2 question informative captions contextual information contrary approaches method training free require access databases apis achieves_state achieves state art accuracy vqa ok vqa datasets finally perform ablation studies tounderstand important aspects method code publicly_available publicly available"}
{"id": "nan", "abstract": "  The Knowledge Base Question Answering (KBQA) task aims to answer naturallanguage questions based on a given knowledge base. As a kind of common methodfor this task, semantic parsing-based ones first convert natural languagequestions to logical forms (e.g., SPARQL queries) and then execute them onknowledge bases to get answers. Recently, Large Language Models (LLMs) haveshown strong abilities in language understanding and may be adopted as semanticparsers in such kinds of methods. However, in doing so, a great challenge forLLMs is to understand the schema of knowledge bases. Therefore, in this paper,we propose an In-Context Schema Understanding (ICSU) method for facilitatingLLMs to be used as a semantic parser in KBQA. Specifically, ICSU adopts theIn-context Learning mechanism to instruct LLMs to generate SPARQL queries withexamples. In order to retrieve appropriate examples from annotatedquestion-query pairs, which contain comprehensive schema information related toquestions, ICSU explores four different retrieval strategies. Experimentalresults on the largest KBQA benchmark, KQA Pro, show that ICSU with all thesestrategies outperforms that with a random retrieval strategy significantly(from 12\\% to 78.76\\% in accuracy).", "title": "an incontext schema understanding method for knowledge base question answering", "url": "http://arxiv.org/pdf/2310.14174v1.pdf", "tokenized_text": "knowledge base question_answering question answering kbqa task aims answer naturallanguage questions based given knowledge base kind common task semantic_parsing semantic parsing based ones convert natural languagequestions logical forms e.g. queries execute bases answers recently large_language large language llms haveshown strong abilities language understanding adopted kinds methods great challenge understand schema knowledge bases paper propose context schema understanding method semantic parser kbqa specifically adopts thein context_learning context learning mechanism instruct llms generate queries withexamples order retrieve appropriate examples query pairs contain comprehensive schema information related explores different retrieval strategies experimentalresults largest kbqa benchmark pro thesestrategies outperforms random retrieval strategy 12\\% accuracy"}
{"id": "nan", "abstract": "  With the rise of social media, users are exposed to many misleading claims.However, the pervasive noise inherent in these posts presents a challenge inidentifying precise and prominent claims that require verification. Extractingthe important claims from such posts is arduous and time-consuming, yet it isan underexplored problem. Here, we aim to bridge this gap. We introduce a noveltask, Claim Normalization (aka ClaimNorm), which aims to decompose complex andnoisy social media posts into more straightforward and understandable forms,termed normalized claims. We propose CACN, a pioneering approach that leverageschain-of-thought and claim check-worthiness estimation, mimicking humanreasoning processes, to comprehend intricate claims. Moreover, we capitalize onthe in-context learning capabilities of large language models to provideguidance and to improve claim normalization. To evaluate the effectiveness ofour proposed model, we meticulously compile a comprehensive real-world dataset,CLAN, comprising more than 6k instances of social media posts alongside theirrespective normalized claims. Our experiments demonstrate that CACN outperformsseveral baselines across various evaluation measures. Finally, our rigorouserror analysis validates CACN's capabilities and pitfalls.", "title": "from chaos to clarity claim normalization to empower factchecking", "url": "http://arxiv.org/pdf/2310.14338v2.pdf", "tokenized_text": "rise social_media social media users exposed misleading claims pervasive noise inherent posts presents challenge inidentifying precise prominent claims require verification important claims posts time consuming isan underexplored problem aim bridge gap introduce claim normalization aka aims decompose complex social_media social media posts straightforward understandable forms termed normalized claims propose pioneering approach thought claim check estimation processes comprehend intricate claims capitalize onthe context_learning context learning capabilities large_language large language improve claim normalization evaluate effectiveness ofour proposed meticulously compile comprehensive real world dataset comprising instances social_media social media posts alongside theirrespective normalized claims experiments_demonstrate experiments demonstrate baselines evaluation measures finally analysis validates capabilities pitfalls"}
{"id": "nan", "abstract": "  Applying existing question answering (QA) systems to specialized domains likelaw and finance presents challenges that necessitate domain expertise. Althoughlarge language models (LLMs) have shown impressive language comprehension andin-context learning capabilities, their inability to handle very longinputs/contexts is well known. Tasks specific to these domains need significantbackground knowledge, leading to contexts that can often exceed the maximumlength that existing LLMs can process. This study explores leveraging thesemi-structured nature of legal and financial data to efficiently retrieverelevant context, enabling the use of LLMs for domain-specialized QA. Theresulting system outperforms contemporary models and also provides usefulexplanations for the answers, encouraging the integration of LLMs into legaland financial NLP systems for future research.", "title": "retrievalaugmented chainofthought in semistructured domains", "url": "http://arxiv.org/pdf/2310.14435v1.pdf", "tokenized_text": "applying existing question_answering question answering qa systems specialized domains finance presents challenges necessitate domain expertise language_models language llms shown_impressive shown impressive language comprehension andin context_learning context learning capabilities inability handle contexts known tasks specific domains need knowledge leading contexts exceed existing llms process study explores leveraging structured nature legal financial data efficiently retrieverelevant context enabling use llms domain specialized qa theresulting system outperforms contemporary provides answers encouraging integration llms financial nlp systems future_research future research"}
{"id": "nan", "abstract": "  The popularity of transformer-based text embeddings calls for betterstatistical tools for measuring distributions of such embeddings. One such toolwould be a method for ranking texts within a corpus by centrality, i.e.assigning each text a number signifying how representative that text is of thecorpus as a whole. However, an intrinsic center-outward ordering ofhigh-dimensional text representations is not trivial. A statistical depth is afunction for ranking k-dimensional objects by measuring centrality with respectto some observed k-dimensional distribution. We adopt a statistical depth tomeasure distributions of transformer-based text embeddings, transformer-basedtext embedding (TTE) depth, and introduce the practical use of this depth forboth modeling and distributional inference in NLP pipelines. We first defineTTE depth and an associated rank sum test for determining whether two corporadiffer significantly in embedding space. We then use TTE depth for the task ofin-context learning prompt selection, showing that this approach reliablyimproves performance over statistical baseline approaches across six textclassification tasks. Finally, we use TTE depth and the associated rank sumtest to characterize the distributions of synthesized and human-generatedcorpora, showing that five recent synthetic data augmentation processes cause ameasurable distributional shift away from associated human-generated text.", "title": "statistical depth for ranking and characterizing transformerbased text embeddings", "url": "http://arxiv.org/pdf/2310.15010v1.pdf", "tokenized_text": "popularity transformer based text embeddings calls tools measuring distributions embeddings method ranking texts corpus text number representative text intrinsic center ordering ofhigh dimensional text representations trivial statistical depth afunction ranking dimensional objects measuring observed dimensional distribution adopt statistical depth distributions transformer based text embeddings transformer embedding depth introduce practical use depth modeling distributional inference nlp pipelines depth associated rank sum test determining significantly embedding space use depth task ofin context_learning context learning selection showing approach performance statistical baseline approaches textclassification tasks finally use depth associated rank characterize distributions synthesized human showing recent synthetic data_augmentation data augmentation processes cause distributional shift away associated human generated text"}
{"id": "nan", "abstract": "  Despite the impressive performance achieved by pre-trainedlanguage-and-vision models in downstream tasks, it remains an open questionwhether this reflects a proper understanding of image-text interaction. In thiswork, we explore to what extent they handle basic linguistic constructions --active-passive voice, coordination, and relative clauses -- that even preschoolchildren can typically master. We present BLA, a novel, automaticallyconstructed benchmark to evaluate multimodal models on these Basic LanguageAbilities. We show that different types of Transformer-based systems, such asCLIP, ViLBERT, and BLIP2, generally struggle with BLA in a zero-shot setting,in line with previous findings. Our experiments, in particular, show that mostof the tested models only marginally benefit when fine-tuned or prompted withconstruction-specific samples. Yet, the generative BLIP2 shows promisingtrends, especially in an in-context learning setting. This opens the door tousing BLA not only as an evaluation benchmark but also to improve models' basiclanguage abilities.", "title": "the bla benchmark investigating basic language abilities of pretrained multimodal models", "url": "http://arxiv.org/pdf/2310.15061v1.pdf", "tokenized_text": "despite impressive performance achieved pre trainedlanguage vision downstream_tasks downstream tasks remains open reflects proper understanding image text interaction thiswork explore extent handle basic linguistic constructions passive voice coordination relative clauses typically master present novel benchmark evaluate multimodal basic different types transformer based systems blip2 generally struggle zero shot_setting shot setting line previous findings experiments particular mostof tested marginally benefit fine tuned prompted specific samples generative blip2 shows especially context_learning context learning setting opens door evaluation benchmark improve abilities"}
{"id": "nan", "abstract": "  Thematic analysis (TA) has been widely used for analyzing qualitative data inmany disciplines and fields. To ensure reliable analysis, the same piece ofdata is typically assigned to at least two human coders. Moreover, to producemeaningful and useful analysis, human coders develop and deepen their datainterpretation and coding over multiple iterations, making TA labor-intensiveand time-consuming. Recently the emerging field of large language models (LLMs)research has shown that LLMs have the potential replicate human-like behaviorin various tasks: in particular, LLMs outperform crowd workers ontext-annotation tasks, suggesting an opportunity to leverage LLMs on TA. Wepropose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conductTA with in-context learning (ICL). This framework provides the prompt to framediscussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA.We demonstrate the utility of this framework using survey datasets on theaspects of the music listening experience and the usage of a password manager.Results of the two case studies show that the proposed framework yields similarcoding quality to that of human coders but reduces TA's labor and time demands.", "title": "llmintheloop leveraging large language model for thematic analysis", "url": "http://arxiv.org/pdf/2310.15100v1.pdf", "tokenized_text": "analysis widely analyzing qualitative data disciplines fields ensure reliable analysis piece ofdata typically assigned human useful analysis human develop deepen coding multiple iterations making labor time consuming recently emerging field large_language large language shown llms potential replicate human like tasks particular llms outperform crowd workers annotation tasks suggesting opportunity leverage llms wepropose human llm collaboration framework i.e. llm loop context_learning context learning icl framework provides llm e.g. gpt-3.5 generate final codebook demonstrate utility framework survey datasets music listening experience usage manager results case studies proposed framework yields quality human reduces labor time demands"}
{"id": "nan", "abstract": "  The recent advances in Large Language Models (LLMs) have stimulated interestamong researchers and industry professionals, particularly in their applicationto tasks concerning mobile user interfaces (UIs). This position paperinvestigates the use of LLMs for UI layout generation. Central to ourexploration is the introduction of UI grammar -- a novel approach we proposedto represent the hierarchical structure inherent in UI screens. The aim of thisapproach is to guide the generative capacities of LLMs more effectively andimprove the explainability and controllability of the process. Initialexperiments conducted with GPT-4 showed the promising capability of LLMs toproduce high-quality user interfaces via in-context learning. Furthermore, ourpreliminary comparative study suggested the potential of the grammar-basedapproach in improving the quality of generative results in specific aspects.", "title": "ui layout generation with llms guided by ui grammar", "url": "http://arxiv.org/pdf/2310.15455v1.pdf", "tokenized_text": "recent_advances recent advances large_language large language llms stimulated researchers industry professionals particularly tasks concerning mobile user interfaces uis position paperinvestigates use llms ui layout generation central introduction ui grammar novel_approach novel approach represent hierarchical structure inherent ui aim thisapproach guide generative capacities llms effectively andimprove explainability controllability process conducted gpt-4 showed promising capability llms toproduce high quality user interfaces context_learning context learning furthermore ourpreliminary comparative study suggested potential grammar basedapproach improving quality generative results specific aspects"}
{"id": "nan", "abstract": "  Language models (LMs) are capable of conducting in-context learning formultiple choice reasoning tasks, but the options in these tasks are treatedequally. As humans often first eliminate wrong options before picking the finalcorrect answer, we argue a similar two-step strategy can make LMs better atthese tasks. To this end, we present the Process of Elimination (POE), atwo-step scoring method. In the first step, POE scores each option, andeliminates seemingly wrong options. In the second step, POE masks these wrongoptions, and makes the final prediction from the remaining options. Zero-shotexperiments on 8 reasoning tasks illustrate the effectiveness of POE, and afollowing analysis finds our method to be especially performant on logicalreasoning tasks. We further analyze the effect of masks, and show that POEapplies to few-shot settings and large language models (LLMs) like ChatGPT.", "title": "poe process of elimination for multiple choice reasoning", "url": "http://arxiv.org/pdf/2310.15575v1.pdf", "tokenized_text": "language_models language lms capable conducting context_learning context learning choice reasoning tasks options tasks humans eliminate wrong options answer argue similar step strategy lms better tasks end present process elimination atwo step scoring method step scores option seemingly wrong options second step masks makes final prediction remaining options zero reasoning tasks illustrate effectiveness analysis finds method especially performant logicalreasoning tasks analyze effect masks shot_settings shot settings large_language large language llms like_chatgpt like chatgpt"}
{"id": "nan", "abstract": "  The paper investigates using a Large Language Model (LLM) to automaticallyperform web software tasks using click, scroll, and text input operations.Previous approaches, such as reinforcement learning (RL) or imitation learning,are inefficient to train and task-specific. Our method uses filtered DocumentObject Model (DOM) elements as observations and performs tasks step-by-step,sequentially generating small programs based on the current observations. Weuse in-context learning, either benefiting from a single manually providedexample, or an automatically generated example based on a successful zero-shottrial. We evaluate the proposed method on the MiniWob++ benchmark. With onlyone in-context example, our WebWISE method achieves similar or betterperformance than other methods that require many demonstrations or trials.", "title": "webwise web interface control and sequential exploration with large language models", "url": "http://arxiv.org/pdf/2310.16042v2.pdf", "tokenized_text": "paper investigates large_language large language llm web software tasks click text input operations previous approaches reinforcement_learning reinforcement learning rl imitation learning inefficient train task specific method uses filtered elements observations performs tasks step step sequentially generating small programs based current observations weuse context_learning context learning benefiting single manually automatically generated example based successful zero evaluate proposed_method proposed method miniwob++ benchmark onlyone context example method_achieves method achieves similar betterperformance methods require demonstrations trials"}
{"id": "nan", "abstract": "  Pre-trained language models (PLMs) have shown impressive performance invarious language tasks. However, they are prone to spurious correlations, andoften generate illusory information. In real-world applications, PLMs shouldjustify decisions with formalized, coherent reasoning chains, but thischallenge remains under-explored. Cognitive psychology theorizes that humansare capable of utilizing fast and intuitive heuristic thinking to makedecisions based on past experience, then rationalizing the decisions throughslower and deliberative analytic reasoning. We incorporate these interlinkeddual processes in fine-tuning and in-context learning with PLMs, applying themto two language understanding tasks that require coherent physical commonsensereasoning. We show that our proposed Heuristic-Analytic Reasoning (HAR)strategies drastically improve the coherence of rationalizations for modeldecisions, yielding state-of-the-art results on Tiered Reasoning for IntuitivePhysics (TRIP). We also find that this improved coherence is a direct result ofmore faithful attention to relevant language context in each step of reasoning.Our findings suggest that human-like reasoning strategies can effectivelyimprove the coherence and reliability of PLM reasoning.", "title": "from heuristic to analytic cognitively motivated strategies for coherent physical commonsense reasoning", "url": "http://arxiv.org/pdf/2310.18364v1.pdf", "tokenized_text": "pre trained_language trained language plms shown_impressive shown impressive performance invarious language tasks prone spurious correlations andoften generate illusory information real world_applications world applications plms decisions formalized coherent reasoning chains thischallenge remains explored cognitive psychology capable utilizing fast intuitive heuristic thinking based past experience rationalizing decisions deliberative analytic reasoning incorporate processes fine tuning context_learning context learning plms applying language understanding tasks require coherent physical commonsensereasoning proposed reasoning drastically improve coherence yielding state art results reasoning find improved coherence direct result faithful attention relevant language context step reasoning findings_suggest findings suggest human like reasoning strategies effectivelyimprove coherence reliability plm reasoning"}
{"id": "nan", "abstract": "  Understanding emergent abilities, such as in-context learning (ICL) andchain-of-thought (CoT) prompting in large language models (LLMs), is of utmostimportance. This importance stems not only from the better utilization of thesecapabilities across various tasks, but also from the proactive identificationand mitigation of potential risks, including concerns of truthfulness, bias,and toxicity, that may arise alongside these capabilities. In this paper, wepresent a thorough survey on the interpretation and analysis of emergentabilities of LLMs. First, we provide a concise introduction to the backgroundand definition of emergent abilities. Then, we give an overview of advancementsfrom two perspectives: 1) a macro perspective, emphasizing studies on themechanistic interpretability and delving into the mathematical foundationsbehind emergent abilities; and 2) a micro-perspective, concerning studies thatfocus on empirical interpretability by examining factors associated with theseabilities. We conclude by highlighting the challenges encountered andsuggesting potential avenues for future research. We believe that our workestablishes the basis for further exploration into the interpretation ofemergent abilities.", "title": "the mystery and fascination of llms a comprehensive survey on the interpretation and analysis of emergent abilities", "url": "http://arxiv.org/pdf/2311.00237v1.pdf", "tokenized_text": "understanding emergent abilities context_learning context learning icl andchain thought cot large_language large language llms importance stems better utilization tasks proactive mitigation potential risks including concerns truthfulness bias toxicity arise alongside capabilities paper wepresent thorough survey interpretation analysis emergentabilities llms provide concise introduction definition emergent abilities overview perspectives macro perspective emphasizing studies interpretability delving mathematical emergent abilities micro perspective concerning studies thatfocus empirical interpretability examining factors associated theseabilities conclude highlighting challenges encountered potential avenues future_research future research believe basis exploration interpretation abilities"}
{"id": "nan", "abstract": "  Large language models trained primarily in a monolingual setting havedemonstrated their ability to generalize to machine translation using zero- andfew-shot examples with in-context learning. However, even though zero-shottranslations are relatively good, there remains a discernible gap comparingtheir performance with the few-shot setting. In this paper, we investigate thefactors contributing to this gap and find that this gap can largely be closed(for about 70%) by matching the writing styles of the target corpus.Additionally, we explore potential approaches to enhance zero-shot baselineswithout the need for parallel demonstration examples, providing valuableinsights into how these methods contribute to improving translation metrics.", "title": "narrowing the gap between zero and fewshot machine translation by matching styles", "url": "http://arxiv.org/pdf/2311.02310v1.pdf", "tokenized_text": "large_language large language trained primarily monolingual setting havedemonstrated ability generalize machine_translation machine translation zero- andfew shot examples context_learning context learning zero relatively good remains gap performance shot_setting shot setting paper investigate contributing gap find gap largely 70 matching writing styles target corpus additionally explore potential approaches enhance zero shot need parallel demonstration examples providing methods contribute improving translation metrics"}
{"id": "nan", "abstract": "  Generative approaches powered by large language models (LLMs) havedemonstrated emergent abilities in tasks that require complex reasoningabilities. Yet the generative nature still makes the generated content sufferfrom hallucinations, thus unsuitable for entity-centric tasks like entitylinking (EL) requiring precise entity predictions over a large knowledge base.We present Instructed Generative Entity Linker (INSGENEL), the first approachthat enables casual language models to perform entity linking over knowledgebases. Several methods to equip language models with EL capability wereproposed in this work, including (i) a sequence-to-sequence training ELobjective with instruction-tuning, (ii) a novel generative EL framework basedon a light-weight potential mention retriever that frees the model from heavyand non-parallelizable decoding, achieving 4$\\times$ speedup without compromiseon linking metrics. INSGENEL outperforms previous generative alternatives with+6.8 F1 points gain on average, also with a huge advantage in training dataefficiency and training compute consumption. In addition, our skillfullyengineered in-context learning (ICL) framework for EL still lags behindINSGENEL significantly, reaffirming that the EL task remains a persistenthurdle for general LLMs.", "title": "instructed language models with retrievers are powerful entity linkers", "url": "http://arxiv.org/pdf/2311.03250v1.pdf", "tokenized_text": "generative approaches powered large_language large language llms havedemonstrated emergent abilities tasks require complex generative nature makes generated content sufferfrom hallucinations unsuitable entity centric tasks like requiring precise entity predictions large knowledge base present instructed generative entity approachthat enables casual language_models language perform entity linking methods language_models language capability work including sequence sequence training instruction tuning ii novel generative framework basedon light weight potential mention retriever non decoding achieving speedup linking metrics outperforms previous generative alternatives f1 points gain average huge advantage training training compute consumption addition context_learning context learning icl framework lags significantly task remains general llms"}
{"id": "nan", "abstract": "  The goal of meta-learning is to learn to adapt to a new task with only a fewlabeled examples. To tackle this problem in NLP, we propose $\\textit{in-contexttuning}$, which recasts adaptation and prediction as a simple sequenceprediction problem: to form the input sequence, we concatenate the taskinstruction, the labeled examples, and the target input to predict; tometa-train the model to learn from in-context examples, we fine-tune apre-trained language model (LM) to predict the target label from the inputsequences on a collection of tasks.  We benchmark our method on two collections of text classification tasks: LAMAand BinaryClfs. Compared to first-order MAML which adapts the model withgradient descent, our method better leverages the inductive bias of LMs toperform pattern matching, and outperforms MAML by an absolute $6\\%$ AUC ROCscore on BinaryClfs, with increasing advantage w.r.t. model size. Compared tonon-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuningdirectly learns to learn from in-context examples. On BinaryClfs, in-contexttuning improves the average AUC-ROC score by an absolute $10\\%$, and reducesthe variance with respect to example ordering by 6x and example choices by 2x.", "title": "metalearning via language model incontext tuning", "url": "http://arxiv.org/pdf/2110.07814v2.pdf", "tokenized_text": "goal meta learning learn adapt new task examples tackle problem nlp propose \\textit{in adaptation prediction simple problem form input sequence concatenate labeled examples target input predict train learn context_examples context examples fine tune apre trained_language trained language lm predict target label collection tasks benchmark method collections text_classification text classification tasks compared order maml adapts descent method better leverages inductive bias lms toperform pattern matching outperforms maml absolute auc increasing advantage w.r.t model_size size compared fine tuned context_learning context learning i.e. raw lm context learns learn context_examples context examples improves average auc roc score absolute variance respect example ordering example choices 2x"}
{"id": "nan", "abstract": "  Scaling language models with more data, compute and parameters has drivensignificant progress in natural language processing. For example, thanks toscaling, GPT-3 was able to achieve strong results on in-context learning tasks.However, training these large dense models requires significant amounts ofcomputing resources. In this paper, we propose and develop a family of languagemodels named GLaM (Generalist Language Model), which uses a sparsely activatedmixture-of-experts architecture to scale the model capacity while alsoincurring substantially less training cost compared to dense variants. Thelargest GLaM has 1.2 trillion parameters, which is approximately 7x larger thanGPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires halfof the computation flops for inference, while still achieving better overallzero-shot and one-shot performance across 29 NLP tasks.", "title": "glam efficient scaling of language models with mixtureofexperts", "url": "http://arxiv.org/pdf/2112.06905v2.pdf", "tokenized_text": "scaling language_models language data compute parameters progress natural_language natural language processing example thanks gpt-3 able achieve strong results context_learning context learning tasks training large dense requires significant amounts resources paper propose develop family languagemodels named generalist language_model language uses experts architecture scale capacity substantially training cost compared dense variants 1.2 parameters approximately larger energy train gpt-3 requires computation flops inference achieving better shot shot performance 29 nlp_tasks nlp tasks"}
{"id": "nan", "abstract": "  Language Models (LMs) can perform new tasks by adapting to a few in-contextexamples. For humans, explanations that connect examples to task principles canimprove learning. We therefore investigate whether explanations of few-shotexamples can help LMs. We annotate questions from 40 challenging tasks withanswer explanations, and various matched control explanations. We evaluate howdifferent types of explanations, instructions, and controls affect zero- andfew-shot performance. We analyze these results using statistical multilevelmodeling techniques that account for the nested dependencies among conditions,tasks, prompts, and models. We find that explanations can improve performance-- even without tuning. Furthermore, explanations hand-tuned for performance ona small validation set offer substantially larger benefits, and building aprompt by selecting examples and explanations together substantially improvesperformance over selecting examples alone. Finally, even untuned explanationsoutperform carefully matched controls, suggesting that the benefits are due tothe link between an example and its explanation, rather than lower-levelfeatures. However, only large models benefit. In summary, explanations cansupport the in-context learning of large LMs on challenging tasks.", "title": "can language models learn from explanations in context", "url": "http://arxiv.org/pdf/2204.02329v4.pdf", "tokenized_text": "language_models language lms perform new tasks adapting contextexamples humans explanations connect examples task principles learning investigate explanations shotexamples help lms annotate questions 40 challenging tasks explanations matched control explanations evaluate types explanations instructions controls affect zero- andfew shot performance analyze results statistical techniques account nested dependencies conditions tasks find explanations improve tuning furthermore explanations hand tuned performance ona small validation set offer substantially larger benefits building aprompt selecting examples explanations substantially selecting examples finally untuned carefully matched controls suggesting benefits tothe link example explanation lower large benefit summary explanations context_learning context learning large lms challenging tasks"}
{"id": "nan", "abstract": "  Automatic short answer grading is an important research direction in theexploration of how to use artificial intelligence (AI)-based tools to improveeducation. Current state-of-the-art approaches use neural language models tocreate vectorized representations of students responses, followed byclassifiers to predict the score. However, these approaches have several keylimitations, including i) they use pre-trained language models that are notwell-adapted to educational subject domains and/or student-generated text andii) they almost always train one model per question, ignoring the linkageacross a question and result in a significant model storage problem due to thesize of advanced language models. In this paper, we study the problem ofautomatic short answer grading for students' responses to math questions andpropose a novel framework for this task. First, we use MathBERT, a variant ofthe popular language model BERT adapted to mathematical content, as our basemodel and fine-tune it for the downstream task of student response grading.Second, we use an in-context learning approach that provides scoring examplesas input to the language model to provide additional context information andpromote generalization to previously unseen questions. We evaluate ourframework on a real-world dataset of student responses to open-ended mathquestions and show that our framework (often significantly) outperformsexisting approaches, especially for new questions that are not seen duringtraining.", "title": "automatic short math answer grading via incontext metalearning", "url": "http://arxiv.org/pdf/2205.15219v3.pdf", "tokenized_text": "automatic short answer grading important research direction use artificial_intelligence artificial intelligence tools current state art approaches use neural language_models language tocreate vectorized representations students responses followed predict score approaches keylimitations including use pre trained_language trained language adapted educational subject domains and/or student generated text train question ignoring question result significant storage problem thesize advanced language_models language paper study problem short answer grading students responses math questions andpropose novel framework task use variant ofthe popular language_model language bert adapted mathematical content fine tune downstream task student response grading second use context_learning context learning approach provides scoring examplesas input language_model language provide additional context information generalization previously unseen questions evaluate ourframework real world dataset student responses open ended framework significantly outperformsexisting approaches especially new questions seen duringtraining"}
{"id": "nan", "abstract": "  This work presents In-Context Policy Iteration, an algorithm for performingReinforcement Learning (RL), in-context, using foundation models. While theapplication of foundation models to RL has received considerable attention,most approaches rely on either (1) the curation of expert demonstrations(either through manual design or task-specific pretraining) or (2) adaptationto the task of interest using gradient methods (either fine-tuning or trainingof adapter layers). Both of these techniques have drawbacks. Collectingdemonstrations is labor-intensive, and algorithms that rely on them do notoutperform the experts from which the demonstrations were derived. All gradienttechniques are inherently slow, sacrificing the \"few-shot\" quality that madein-context learning attractive to begin with. In this work, we present analgorithm, ICPI, that learns to perform RL tasks without expert demonstrationsor gradients. Instead we present a policy-iteration method in which the promptcontent is the entire locus of learning. ICPI iteratively updates the contentsof the prompt from which it derives its policy through trial-and-errorinteraction with an RL environment. In order to eliminate the role ofin-weights learning (on which approaches like Decision Transformer relyheavily), we demonstrate our algorithm using Codex, a language model with noprior knowledge of the domains on which we evaluate it.", "title": "large language models can implement policy iteration", "url": "http://arxiv.org/pdf/2210.03821v2.pdf", "tokenized_text": "work presents context policy iteration algorithm learning rl context foundation_models foundation theapplication foundation_models foundation rl received considerable attention approaches rely curation expert manual design task specific pretraining task interest gradient methods fine tuning adapter layers techniques drawbacks labor intensive algorithms rely notoutperform experts demonstrations derived inherently slow sacrificing shot quality context_learning context learning attractive begin work present learns perform rl tasks expert gradients instead present policy iteration method entire learning iteratively updates derives policy trial rl environment order eliminate role ofin weights learning approaches like decision_transformer decision transformer demonstrate algorithm codex language_model language knowledge domains evaluate"}
{"id": "nan", "abstract": "  Transformer models can use two fundamentally different kinds of information:information stored in weights during training, and information provided``in-context'' at inference time. In this work, we show that transformersexhibit different inductive biases in how they represent and generalize fromthe information in these two sources. In particular, we characterize whetherthey generalize via parsimonious rules (rule-based generalization) or viadirect comparison with observed examples (exemplar-based generalization). Thisis of important practical consequence, as it informs whether to encodeinformation in weights or in context, depending on how we want models to usethat information. In transformers trained on controlled stimuli, we find thatgeneralization from weights is more rule-based whereas generalization fromcontext is largely exemplar-based. In contrast, we find that in transformerspre-trained on natural language, in-context learning is significantlyrule-based, with larger models showing more rule-basedness. We hypothesise thatrule-based generalization from in-context information might be an emergentconsequence of large-scale training on language, which has sparse rule-likestructure. Using controlled stimuli, we verify that transformers pretrained ondata containing sparse rule-like structure exhibit more rule-basedgeneralization.", "title": "transformers generalize differently from information stored in context vs in weights", "url": "http://arxiv.org/pdf/2210.05675v2.pdf", "tokenized_text": "transformer use fundamentally different kinds information information stored weights training information context inference time work different inductive biases represent generalize fromthe information sources particular characterize generalize rules rule based generalization comparison observed examples exemplar based generalization important practical consequence informs weights context depending want information transformers trained controlled stimuli find weights rule based generalization largely exemplar based contrast find trained natural_language natural language context_learning context learning based larger showing rule based generalization context information large scale training language sparse rule controlled stimuli verify transformers pretrained containing sparse rule like structure exhibit rule"}
{"id": "nan", "abstract": "  In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPTand GPT4 have demonstrated immense potential in constructing open-domaindialogue agents. However, aligning these agents with specific characters orindividuals remains a considerable challenge due to the complexities ofcharacter representation and the lack of comprehensive annotations. In thispaper, we introduce the Harry Potter Dialogue (HPD) dataset, designed toadvance the study of dialogue agents and character alignment. The datasetencompasses all dialogue sessions (in both English and Chinese) from the HarryPotter series and is annotated with vital background information, includingdialogue scenes, speakers, character relationships, and attributes. Theseextensive annotations may empower LLMs to unlock character-driven dialoguecapabilities. Furthermore, it can serve as a universal benchmark for evaluatinghow well can a LLM aligning with a specific character. We benchmark LLMs on HPDusing both fine-tuning and in-context learning settings. Evaluation resultsreveal that although there is substantial room for improvement in generatinghigh-quality, character-aligned responses, the proposed dataset is valuable inguiding models toward responses that better align with the character of HarryPotter.", "title": "large language models meet harry potter a bilingual dataset for aligning dialogue agents with characters", "url": "http://arxiv.org/pdf/2211.06869v4.pdf", "tokenized_text": "recent_years recent years dialogue style large_language large language llms gpt4 demonstrated immense potential constructing open agents aligning agents specific characters remains considerable challenge complexities representation lack comprehensive annotations thispaper introduce dialogue dataset designed study dialogue agents character alignment dialogue sessions english chinese series annotated vital background information scenes speakers character relationships attributes annotations empower llms unlock character driven furthermore serve universal benchmark llm aligning specific character benchmark llms fine tuning context_learning context learning settings evaluation substantial room improvement generatinghigh quality character aligned responses proposed dataset valuable responses better align character"}
{"id": "nan", "abstract": "  Recent multimodal models such as DALL-E and CM3 have achieved remarkableprogress in text-to-image and image-to-text generation. However, these modelsstore all learned knowledge (e.g., the appearance of the Eiffel Tower) in themodel parameters, requiring increasingly larger models and training data tocapture more knowledge. To integrate knowledge in a more scalable and modularway, we propose a retrieval-augmented multimodal model, which enables a basemultimodal model (generator) to refer to relevant text and images fetched by aretriever from external memory (e.g., documents on the web). Specifically, forthe retriever, we use a pretrained CLIP, and for the generator, we train a CM3Transformer on the LAION dataset. Our resulting model, namedRetrieval-Augmented CM3 (RA-CM3), is the first multimodal model that canretrieve and generate both text and images. We show that RA-CM3 significantlyoutperforms baseline multimodal models such as DALL-E and CM3 on both image andcaption generation tasks (12 FID and 17 CIDEr improvements on MS-COCO), whilerequiring much less compute for training (<30% of DALL-E). Moreover, we showthat RA-CM3 exhibits novel capabilities, such as faithful image generation andmultimodal in-context learning (e.g., image generation from demonstrations).", "title": "retrievalaugmented multimodal language modeling", "url": "http://arxiv.org/pdf/2211.12561v2.pdf", "tokenized_text": "recent multimodal dall achieved remarkableprogress text image image text generation learned knowledge e.g. appearance tower themodel parameters requiring increasingly larger training_data training data tocapture knowledge integrate knowledge scalable propose retrieval augmented multimodal enables generator refer relevant text images external memory e.g. documents web specifically forthe retriever use pretrained clip generator train dataset resulting augmented ra multimodal generate text images ra significantlyoutperforms baseline multimodal dall image generation tasks 12 fid 17 cider improvements ms coco compute training 30 dall showthat ra exhibits novel capabilities faithful image_generation image generation context_learning context learning e.g. image_generation image generation demonstrations"}
{"id": "nan", "abstract": "  In this work, we present some recommendations on the evaluation ofstate-of-the-art generative models for constrained generation tasks. Theprogress on generative models has been rapid in recent years. These large-scalemodels have had three impacts: firstly, the fluency of generation in bothlanguage and vision modalities has rendered common average-case evaluationmetrics much less useful in diagnosing system errors. Secondly, the samesubstrate models now form the basis of a number of applications, driven both bythe utility of their representations as well as phenomena such as in-contextlearning, which raise the abstraction level of interacting with such models.Thirdly, the user expectations around these models and their feted publicreleases have made the technical challenge of out of domain generalization muchless excusable in practice. Subsequently, our evaluation methodologies haven'tadapted to these changes. More concretely, while the associated utility andmethods of interacting with generative models have expanded, a similarexpansion has not been observed in their evaluation practices. In this paper,we argue that the scale of generative models could be exploited to raise theabstraction level at which evaluation itself is conducted and providerecommendations for the same. Our recommendations are based on leveragingspecifications as a powerful instrument to evaluate generation quality and arereadily applicable to a variety of tasks.", "title": "operationalizing specifications, in addition to test sets for evaluating constrained generative models", "url": "http://arxiv.org/pdf/2212.00006v1.pdf", "tokenized_text": "work present recommendations evaluation ofstate art generative constrained generation tasks generative rapid recent_years recent years large impacts firstly fluency generation bothlanguage vision modalities rendered common average case useful diagnosing system errors secondly form basis number applications driven bythe utility representations phenomena contextlearning raise abstraction level interacting user expectations technical challenge domain generalization practice subsequently evaluation methodologies changes concretely associated utility interacting generative expanded observed evaluation practices paper argue scale generative exploited raise level evaluation conducted recommendations based powerful instrument evaluate generation quality applicable variety tasks"}
{"id": "nan", "abstract": "  Targeted syntactic evaluations of language models ask whether models showstable preferences for syntactically acceptable content over minimal-pairunacceptable inputs. Most targeted syntactic evaluation datasets ask models tomake these judgements with just a single context-free sentence as input. Thisdoes not match language models' training regime, in which input sentences arealways highly contextualized by the surrounding corpus. This mismatch raises animportant question: how robust are models' syntactic judgements in differentcontexts? In this paper, we investigate the stability of language models'performance on targeted syntactic evaluations as we vary properties of theinput context: the length of the context, the types of syntactic phenomena itcontains, and whether or not there are violations of grammaticality. We findthat model judgements are generally robust when placed in randomly sampledlinguistic contexts. However, they are substantially unstable for contextscontaining syntactic structures matching those in the critical test content.Among all tested models (GPT-2 and five variants of OPT), we significantlyimprove models' judgements by providing contexts with matching syntacticstructures, and conversely significantly worsen them using unacceptablecontexts with matching but violated syntactic structures. This effect isamplified by the length of the context, except for unrelated inputs. We showthat these changes in model performance are not explainable by simple featuresmatching the context and the test inputs, such as lexical overlap anddependency overlap. This sensitivity to highly specific syntactic features ofthe context can only be explained by the models' implicit in-context learningabilities.", "title": "language model acceptability judgements are not always robust to context", "url": "http://arxiv.org/pdf/2212.08979v1.pdf", "tokenized_text": "targeted syntactic evaluations language_models language ask preferences syntactically acceptable content minimal inputs targeted syntactic evaluation datasets ask tomake judgements single context free sentence input thisdoes match language_models language training regime input sentences highly contextualized surrounding corpus mismatch raises animportant question robust syntactic judgements paper investigate stability language models'performance targeted syntactic evaluations vary properties theinput context length context types syntactic phenomena itcontains violations findthat judgements generally robust placed randomly contexts substantially unstable syntactic structures matching critical test content tested gpt-2 variants opt significantlyimprove judgements providing contexts matching conversely significantly matching violated syntactic structures effect length context unrelated inputs showthat changes performance explainable simple context test inputs lexical overlap overlap sensitivity highly specific syntactic features ofthe context explained implicit context"}
{"id": "nan", "abstract": "  Authorship style transfer involves altering text to match the style of atarget author whilst preserving the original meaning. Existing unsupervisedapproaches like STRAP have largely focused on style transfer to target authorswith many examples of their writing style in books, speeches, or otherpublished works. This high-resource training data requirement (often greaterthan 100,000 words) makes these approaches primarily useful for style transferto published authors, politicians, or other well-known figures and authorshipstyles, while style transfer to non-famous authors has not been well-studied.We introduce the \\textit{low-resource authorship style transfer} task, a morechallenging class of authorship style transfer where only a limited amount oftext in the target author's style may exist. In our experiments, wespecifically choose source and target authors from Reddit and style transfertheir Reddit posts, limiting ourselves to just 16 posts (on average ~500 words)of the target author's style. Style transfer accuracy is typically measured byhow often a classifier or human judge will classify an output as written by thetarget author. Recent authorship representations models excel at authorshipidentification even with just a few writing samples, making automaticevaluation of this task possible for the first time through evaluation metricswe propose. Our results establish an in-context learning technique we developas the strongest baseline, though we find current approaches do not yet achievemastery of this challenging task. We release our data and implementations toencourage further investigation.", "title": "lowresource authorship style transfer can nonfamous authors be imitated", "url": "http://arxiv.org/pdf/2212.08986v2.pdf", "tokenized_text": "authorship style_transfer style transfer involves altering text match style atarget author whilst preserving original meaning existing like largely focused style_transfer style transfer target examples writing style books speeches works high resource training_data training data requirement words makes approaches primarily useful style published authors known figures style_transfer style transfer non authors studied introduce resource authorship style_transfer style transfer task class authorship style_transfer style transfer limited oftext target author style exist experiments choose source target authors reddit style reddit posts limiting 16 posts average target author style style_transfer style transfer accuracy typically measured classifier human judge classify output written thetarget author recent authorship representations excel writing samples making automaticevaluation task possible time evaluation propose results establish context_learning context learning technique strongest baseline find current approaches challenging task release data implementations investigation"}
{"id": "nan", "abstract": "  Scaling up language models has led to unprecedented performance gains, butlittle is understood about how the training dynamics change as models getlarger. How do language models of different sizes learn during pre-training?Why do larger language models demonstrate more desirable behaviors? In thispaper, we analyze the intermediate training checkpoints of differently sizedOPT models (Zhang et al.,2022)--from 125M to 175B parameters--on next-tokenprediction, sequence-level generation, and downstream tasks. We find that 1) ata given perplexity and independent of model sizes, a similar subset of trainingtokens see the most significant reduction in loss, with the rest stagnating orshowing double-descent behavior; 2) early in training, all models learn toreduce the perplexity of grammatical sequences that contain hallucinations,with small models halting at this suboptimal distribution and larger oneseventually learning to assign these sequences lower probabilities; 3)perplexity is a strong predictor of in-context learning performance on 74multiple-choice tasks from BIG-Bench, and this holds independent of the modelsize. Together, these results show that perplexity is more predictive of modelbehaviors than model size or training computation.", "title": "training trajectories of language models across scales", "url": "http://arxiv.org/pdf/2212.09803v3.pdf", "tokenized_text": "scaling language_models language led unprecedented performance gains understood training dynamics change language_models language different sizes learn pre larger language_models language demonstrate desirable behaviors thispaper analyze intermediate training checkpoints differently et_al et al 125 175b parameters sequence level generation downstream_tasks downstream tasks find ata given perplexity independent sizes similar subset significant reduction loss rest double descent behavior early training learn perplexity grammatical sequences contain hallucinations small halting suboptimal distribution larger learning assign sequences lower probabilities strong predictor context_learning context learning performance choice tasks big bench holds independent modelsize results perplexity predictive model_size size training computation"}
{"id": "nan", "abstract": "  Functionality and dialogue experience are two important factors oftask-oriented dialogue systems. Conventional approaches with closed schema(e.g., conversational semantic parsing) often fail as both the functionalityand dialogue experience are strongly constrained by the underlying schema. Weintroduce a new paradigm for task-oriented dialogue - Dialog2API - to greatlyexpand the functionality and provide seamless dialogue experience. Theconversational model interacts with the environment by generating and executingprograms triggering a set of pre-defined APIs. The model also manages thedialogue policy and interact with the user through generating appropriatenatural language responses. By allowing generating free-form programs,Dialog2API supports composite goals by combining different APIs, whereasunrestricted program revision provides natural and robust dialogue experience.To facilitate Dialog2API, the core model is provided with API documents, anexecution environment and optionally some example dialogues annotated withprograms. We propose an approach tailored for the Dialog2API, where thedialogue states are represented by a stack of programs, with most recentlymentioned program on the top of the stack. Dialog2API can work with manyapplication scenarios such as software automation and customer service. In thispaper, we construct a dataset for AWS S3 APIs and present evaluation results ofin-context learning baselines.", "title": "dialog2api taskoriented dialogue with api description and example programs", "url": "http://arxiv.org/pdf/2212.09946v1.pdf", "tokenized_text": "functionality dialogue experience important factors oftask oriented dialogue systems conventional approaches closed conversational semantic_parsing semantic parsing fail dialogue experience strongly constrained underlying schema weintroduce new_paradigm new paradigm task oriented dialogue functionality provide seamless dialogue experience theconversational environment generating triggering set pre defined apis manages policy interact user generating language responses allowing generating free form programs supports composite goals combining different apis program revision provides natural robust dialogue experience facilitate core provided api documents environment example dialogues annotated propose approach tailored states represented stack programs program stack work scenarios software automation customer service thispaper construct dataset s3 apis present evaluation results ofin context_learning context learning baselines"}
{"id": "nan", "abstract": "  Recent NLP models have shown the remarkable ability to effectively generalise`zero-shot' to new tasks using only natural language instructions as guidance.However, many of these approaches suffer from high computational costs due totheir reliance on concatenating lengthy instructions with every input example,resulting in costly reprocessing of the instruction. To avoid this, weintroduce Hypernetworks for INstruction Tuning (HINT), which convert taskinstructions and examples into parameter-efficient modules inserted into anunderlying model using a pretrained text encoder, eliminating the need toinclude instructions in the model input. The hypernetwork in HINT also producesan encoded instruction, which we concatenate with encoded inputs duringdecoding to further improve performance. HINT models outperform strongstate-of-the-art baselines by over 10% when controlling for compute (measuredin FLOPs). By converting instructions into modules, HINT models can effectivelydisregard the length of instructions and few-shot example inputs in terms ofcompute usage. As a result, HINT can enhance its performance by up to 25% byincorporating additional few-shot data, while utilizing only up to 5% morecompute. This combines the strengths of parameter-efficient fine-tuning andin-context learning.", "title": "hint hypernetwork instruction tuning for efficient zero & fewshot generalisation", "url": "http://arxiv.org/pdf/2212.10315v2.pdf", "tokenized_text": "recent nlp shown remarkable ability effectively shot new tasks natural_language natural language instructions guidance approaches suffer high computational costs totheir reliance concatenating lengthy instructions input example resulting costly instruction avoid weintroduce instruction_tuning instruction tuning hint convert taskinstructions examples parameter efficient modules inserted pretrained text encoder eliminating need instructions input hint encoded instruction concatenate encoded inputs improve performance hint outperform art baselines 10 controlling compute flops converting instructions modules hint length instructions shot example inputs terms ofcompute usage result hint enhance performance 25 byincorporating additional shot data utilizing combines strengths parameter efficient fine tuning andin context_learning context learning"}
{"id": "nan", "abstract": "  When applied to processing long text, Large Language Models (LLMs) arelimited by their context window. Existing efforts to address this limitationinvolve training specialized architectures, and cannot be easily applied tooff-the-shelf LLMs. We present Parallel Context Windows (PCW), a method thatalleviates the context window restriction for any off-the-shelf LLM withoutfurther training. The key to the approach is to carve a long context intochunks (``windows''), restrict the attention mechanism to apply only withineach window, and re-use the positional embeddings across the windows. Our mainresults test the PCW approach on in-context learning with models that range insize between 750 million and 178 billion parameters, and show substantialimprovements for tasks with diverse input and output spaces. We show additionalbenefits in other settings where long context windows may be beneficial:multi-hop questions and retrieval-augmented question answering with multipleretrieved documents. Our results highlight Parallel Context Windows as apromising method for applying off-the-shelf LLMs in a range of settings thatrequire long text sequences. We make our code publicly available athttps://github.com/ai21labs/parallel-context-windows.", "title": "parallel context windows for large language models", "url": "http://arxiv.org/pdf/2212.10947v3.pdf", "tokenized_text": "applied processing long text large_language large language llms arelimited context window existing efforts address training specialized architectures easily applied shelf llms present parallel context windows method context window restriction shelf llm withoutfurther training key approach carve long context windows restrict attention mechanism apply window use positional embeddings windows test approach context_learning context learning range million billion parameters substantialimprovements tasks diverse input output spaces settings long context windows beneficial multi hop questions retrieval augmented question_answering question answering documents results highlight parallel context windows apromising method applying shelf llms range settings thatrequire long text sequences code publicly_available publicly available"}
{"id": "nan", "abstract": "  Recent years have witnessed increasing interests in prompt-based learning inwhich models can be trained on only a few annotated instances, making themsuitable in low-resource settings. When using prompt-based learning for textclassification, the goal is to use a pre-trained language model (PLM) topredict a missing token in a pre-defined template given an input text, whichcan be mapped to a class label. However, PLMs built on the transformerarchitecture tend to generate similar output embeddings, making it difficult todiscriminate between different class labels. The problem is further exacerbatedwhen dealing with classification tasks involving many fine-grained classlabels. In this work, we alleviate this information diffusion issue, i.e.,different tokens share a large proportion of similar information after goingthrough stacked multiple self-attention layers in a transformer, by proposing acalibration method built on feature transformations through rotation andscaling to map a PLM-encoded embedding into a new metric space to guarantee thedistinguishability of the resulting embeddings. Furthermore, we take theadvantage of hyperbolic embeddings to capture the hierarchical relations amongfine-grained class-associated token embedding by a coarse-to-fine metriclearning strategy to enhance the distinguishability of the learned outputembeddings. Extensive experiments on the three datasets under various settingsdemonstrate the effectiveness of our approach. Our code can be found athttps://github.com/donttal/TARA.", "title": "distinguishability calibration to incontext learning", "url": "http://arxiv.org/pdf/2302.06198v3.pdf", "tokenized_text": "recent_years recent years witnessed increasing interests based learning inwhich trained annotated instances making low resource settings based learning textclassification goal use pre trained_language trained language plm topredict missing token pre defined template given input text whichcan mapped class label plms built transformerarchitecture tend generate similar output embeddings making difficult different class labels problem dealing classification tasks involving fine grained work alleviate information diffusion issue i.e. tokens share large proportion similar information multiple self attention layers transformer proposing method built feature transformations map plm encoded embedding new metric space guarantee resulting embeddings furthermore embeddings capture hierarchical relations grained class associated token embedding coarse fine strategy enhance learned extensive_experiments extensive experiments datasets effectiveness approach code found"}
{"id": "nan", "abstract": "  Although recent advances in scaling large language models (LLMs) haveresulted in improvements on many NLP tasks, it remains unclear whether thesemodels trained primarily with general web text are the right tool in highlyspecialized, safety critical domains such as clinical text. Recent results havesuggested that LLMs encode a surprising amount of medical knowledge. Thisraises an important question regarding the utility of smaller domain-specificlanguage models. With the success of general-domain LLMs, is there still a needfor specialized clinical models? To investigate this question, we conduct anextensive empirical analysis of 12 language models, ranging from 220M to 175Bparameters, measuring their performance on 3 different clinical tasks that testtheir ability to parse and reason over electronic health records. As part ofour experiments, we train T5-Base and T5-Large models from scratch on clinicalnotes from MIMIC III and IV to directly investigate the efficiency of clinicaltokens. We show that relatively small specialized clinical models substantiallyoutperform all in-context learning approaches, even when finetuned on limitedannotated data. Further, we find that pretraining on clinical tokens allows forsmaller, more parameter-efficient models that either match or outperform muchlarger language models trained on general text. We release the code and themodels used under the PhysioNet Credentialed Health Data license and data useagreement.", "title": "do we still need clinical language models", "url": "http://arxiv.org/pdf/2302.08091v1.pdf", "tokenized_text": "recent_advances recent advances scaling large_language large language llms improvements nlp_tasks nlp tasks remains unclear thesemodels trained primarily general web text right tool safety critical domains clinical text recent results llms encode surprising medical knowledge important question utility smaller domain success general domain llms needfor specialized clinical investigate question conduct anextensive empirical analysis 12 language_models language ranging 220 measuring performance different clinical tasks ability parse reason electronic health records ofour experiments train t5 base t5 large scratch mimic iii iv directly investigate efficiency relatively small specialized clinical substantiallyoutperform context_learning context learning approaches finetuned data find pretraining clinical tokens allows parameter efficient match outperform language_models language trained general text release code themodels health data license data"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) have so far impressed the world, withunprecedented capabilities that emerge in models at large scales. On the visionside, transformer models (i.e., ViT) are following the same trend, achievingthe best performance on challenging benchmarks. With the abundance of suchunimodal models, a natural question arises; do we need also to follow thistrend to tackle multimodal tasks? In this work, we propose to rather directeffort to efficient adaptations of existing models, and propose to augmentLanguage Models with perception. Existing approaches for adapting pretrainedmodels for vision-language tasks still rely on several key components thathinder their efficiency. In particular, they still train a large number ofparameters, rely on large multimodal pretraining, use encoders (e.g., CLIP)trained on huge image-text datasets, and add significant inference overhead. Inaddition, most of these approaches have focused on Zero-Shot and In ContextLearning, with little to no effort on direct finetuning. We investigate theminimal computational effort needed to adapt unimodal models for multimodaltasks and propose a new challenging setup, alongside different approaches, thatefficiently adapts unimodal pretrained models. We show that by freezing morethan 99% of total parameters, training only one linear projection layer, andprepending only one trainable token, our approach (dubbed eP-ALM) significantlyoutperforms other baselines on VQA and Captioning across Image, Video, andAudio modalities, following the proposed setup. The code is available here:https://github.com/mshukor/eP-ALM.", "title": "epalm efficient perceptual augmentation of language models", "url": "http://arxiv.org/pdf/2303.11403v4.pdf", "tokenized_text": "large_language large language llms far world capabilities emerge large scales transformer i.e. vit following trend achievingthe best performance challenging benchmarks abundance natural question arises need follow tackle multimodal tasks work propose efficient adaptations existing propose perception existing approaches adapting vision language tasks rely key components thathinder efficiency particular train large number ofparameters rely large multimodal pretraining use encoders e.g. huge image text datasets add significant inference overhead inaddition approaches focused zero shot contextlearning little effort direct finetuning investigate computational effort needed adapt unimodal multimodaltasks propose_a_new propose new challenging setup alongside different approaches adapts unimodal pretrained freezing morethan 99 total parameters training linear projection layer trainable token approach dubbed significantlyoutperforms baselines vqa captioning image video modalities following proposed setup code_is_available code available https://github.com"}
{"id": "nan", "abstract": "  ChatGPT shows remarkable capabilities for machine translation (MT). Severalprior studies have shown that it achieves comparable results to commercialsystems for high-resource languages, but lags behind in complex tasks, e.g.,low-resource and distant-language-pairs translation. However, they usuallyadopt simple prompts which can not fully elicit the capability of ChatGPT. Inthis paper, we aim to further mine ChatGPT's translation ability by revisitingseveral aspects: temperature, task information, and domain information, andcorrespondingly propose an optimal temperature setting and two (simple buteffective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts(DSP). We show that: 1) The performance of ChatGPT depends largely ontemperature, and a lower temperature usually can achieve better performance; 2)Emphasizing the task information can further improve ChatGPT's performance,particularly in complex MT tasks; 3) Introducing domain information can elicitChatGPT's generalization ability and improve its performance in the specificdomain; 4) ChatGPT tends to generate hallucinations for non-English-centric MTtasks, which can be partially addressed by our proposed prompts but still needto be highlighted for the MT/NLP community. We also explore the effects ofadvanced in-context learning strategies and find a (negative but interesting)observation: the powerful chain-of-thought prompt leads to word-by-wordtranslation behavior, thus bringing significant translation degradation.", "title": "towards making the most of chatgpt for machine translation", "url": "http://arxiv.org/pdf/2303.13780v4.pdf", "tokenized_text": "chatgpt shows remarkable_capabilities remarkable capabilities machine_translation machine translation mt studies shown achieves comparable results high resource_languages resource languages lags complex tasks e.g. resource distant language pairs translation simple fully elicit capability chatgpt inthis_paper inthis paper aim chatgpt translation ability aspects temperature task information domain information propose optimal temperature setting simple task specific domain specific performance chatgpt depends largely lower temperature usually achieve better performance task information improve chatgpt performance particularly complex mt tasks introducing domain information generalization_ability generalization ability improve performance chatgpt tends generate hallucinations non english centric partially addressed proposed needto highlighted mt nlp community explore effects context_learning context learning strategies find negative powerful chain thought leads word behavior bringing significant translation degradation"}
{"id": "nan", "abstract": "  In-Context Learning (ICL), which formulates target tasks as prompt completionconditioned on in-context demonstrations, has become the prevailing utilizationof LLMs. In this paper, we first disclose an actual predicament for thistypical usage that it can not scale up with training data due to context lengthrestriction. Besides, existing works have shown that ICL also suffers fromvarious biases and requires delicate calibration treatment. To address bothchallenges, we advocate a simple and effective solution, $k$NN Prompting, whichfirst queries LLM with training data for distributed representations, thenpredicts test instances by simply referring to nearest neighbors. We conductcomprehensive experiments to demonstrate its two-fold superiority: 1)Calibration-Free: $k$NN Prompting does not directly align LLM outputdistribution with task-specific label space, instead leverages suchdistribution to align test and training instances. It significantly outperformsstate-of-the-art calibration-based methods under comparable few-shot scenario.2) Beyond-Context: $k$NN Prompting can further scale up effectively with asmany training data as are available, continually bringing substantialimprovements. The scaling trend holds across 10 orders of magnitude rangingfrom 2 shots to 1024 shots as well as different LLMs scales ranging from 0.8Bto 30B. It successfully bridges data scaling into model scaling, and brings newpotentials for the gradient-free paradigm of LLM deployment. Code is publiclyavailable.", "title": "$k$nn prompting beyondcontext learning with calibrationfree nearest neighbor inference", "url": "http://arxiv.org/pdf/2303.13824v1.pdf", "tokenized_text": "context_learning context learning icl formulates target tasks context demonstrations prevailing llms paper disclose actual usage scale training_data training data context existing works shown icl suffers biases requires calibration treatment address advocate simple effective solution k$nn queries llm training_data training data distributed representations test instances simply referring nearest neighbors experiments demonstrate fold superiority free k$nn directly align llm outputdistribution task specific label space instead leverages align test training instances significantly outperformsstate art calibration based methods comparable shot context k$nn scale effectively training_data training data available continually bringing substantialimprovements scaling trend holds 10 orders magnitude rangingfrom shots shots different llms scales ranging successfully bridges data scaling scaling brings gradient free paradigm llm deployment code publiclyavailable"}
{"id": "nan", "abstract": "  Pre-trained models of source code have gained widespread popularity in manycode intelligence tasks. Recently, with the scaling of the model and corpussize, large language models have shown the ability of in-context learning(ICL). ICL employs task instructions and a few examples as demonstrations, andthen inputs the demonstrations to the language models for making predictions.This new learning paradigm is training-free and has shown impressiveperformance in various natural language processing and code intelligence tasks.However, the performance of ICL heavily relies on the quality ofdemonstrations, e.g., the selected examples. It is important to systematicallyinvestigate how to construct a good demonstration for code-related tasks. Inthis paper, we empirically explore the impact of three key factors on theperformance of ICL in code intelligence tasks: the selection, order, and numberof demonstration examples. We conduct extensive experiments on three codeintelligence tasks including code summarization, bug fixing, and programsynthesis. Our experimental results demonstrate that all the above threefactors dramatically impact the performance of ICL in code intelligence tasks.Additionally, we summarize our findings and provide takeaway suggestions on howto construct effective demonstrations, taking into account these threeperspectives. We also show that a carefully-designed demonstration based on ourfindings can lead to substantial improvements over widely-used demonstrationconstruction methods, e.g., improving BLEU-4, EM, and EM by at least 9.90%,175.96%, and 50.81% on code summarization, bug fixing, and program synthesis,respectively", "title": "what makes good incontext demonstrations for code intelligence tasks with llms", "url": "http://arxiv.org/pdf/2304.07575v2.pdf", "tokenized_text": "pre trained source_code source code gained widespread popularity intelligence tasks recently scaling large_language large language shown ability context learning(icl icl employs task instructions examples demonstrations andthen inputs demonstrations language_models language making predictions new learning paradigm training free shown impressiveperformance natural_language natural language processing code intelligence tasks performance icl heavily relies quality ofdemonstrations e.g. selected examples important construct good demonstration code related tasks inthis_paper inthis paper empirically explore impact key factors theperformance icl code intelligence tasks selection order numberof demonstration examples conduct_extensive conduct extensive experiments tasks including code summarization bug fixing programsynthesis experimental_results experimental results demonstrate dramatically impact performance icl code intelligence tasks additionally summarize findings provide suggestions howto construct effective demonstrations taking account carefully designed demonstration based ourfindings lead substantial improvements widely methods e.g. improving em em code summarization bug fixing program synthesis respectively"}
{"id": "nan", "abstract": "  Aiming at achieving artificial general intelligence (AGI) for Metaverse,pretrained foundation models (PFMs), e.g., generative pretrained transformers(GPTs), can effectively provide various AI services, such as autonomousdriving, digital twins, and AI-generated content (AIGC) for extended reality.With the advantages of low latency and privacy-preserving, serving PFMs ofmobile AI services in edge intelligence is a viable solution for caching andexecuting PFMs on edge servers with limited computing resources and GPU memory.However, PFMs typically consist of billions of parameters that are computationand memory-intensive for edge servers during loading and execution. In thisarticle, we investigate edge PFM serving problems for mobile AIGC services ofMetaverse. First, we introduce the fundamentals of PFMs and discuss theircharacteristic fine-tuning and inference methods in edge intelligence. Then, wepropose a novel framework of joint model caching and inference for managingmodels and allocating resources to satisfy users' requests efficiently.Furthermore, considering the in-context learning ability of PFMs, we propose anew metric to evaluate the freshness and relevance between examples indemonstrations and executing tasks, namely the Age of Context (AoC). Finally,we propose a least context algorithm for managing cached models at edge serversby balancing the tradeoff among latency, energy consumption, and accuracy.", "title": "sparks of gpts in edge intelligence for metaverse caching and inference for mobile aigc services", "url": "http://arxiv.org/pdf/2304.08782v2.pdf", "tokenized_text": "aiming achieving artificial general intelligence agi pretrained foundation_models foundation pfms e.g. generative pretrained effectively provide ai services digital twins ai generated content aigc extended reality advantages low latency privacy preserving serving pfms ai services edge intelligence viable solution caching pfms edge servers limited computing resources gpu memory pfms typically consist billions parameters memory intensive edge servers loading execution investigate edge pfm serving problems mobile aigc services introduce fundamentals pfms discuss fine tuning inference methods edge intelligence wepropose novel framework joint caching inference resources satisfy users requests efficiently furthermore considering context_learning context learning ability pfms propose anew metric evaluate freshness relevance examples indemonstrations executing tasks age context aoc finally propose context algorithm managing cached edge balancing tradeoff latency energy consumption accuracy"}
{"id": "nan", "abstract": "  Large language models generate fluent texts and can follow natural languageinstructions to solve a wide range of tasks without task-specific training.Nevertheless, it is notoriously difficult to control their generation tosatisfy the various constraints required by different applications. In thiswork, we present InstructCTG, a controlled text generation framework thatincorporates different constraints by conditioning on natural languagedescriptions and demonstrations of the constraints. In particular, we firstextract the underlying constraints of natural texts through a combination ofoff-the-shelf NLP tools and simple heuristics. We then verbalize theconstraints into natural language instructions to form weakly supervisedtraining data. By prepending natural language descriptions of the constraintsand a few demonstrations, we fine-tune a pre-trained language model toincorporate various types of constraints. Compared to existing search-based orscore-based methods, InstructCTG is more flexible to different constraint typesand has a much smaller impact on the generation quality and speed because itdoes not modify the decoding procedure. Additionally, InstructCTG allows themodel to adapt to new constraints without re-training through the use offew-shot task generalization and in-context learning abilities ofinstruction-tuned language models.", "title": "controlled text generation with natural language instructions", "url": "http://arxiv.org/pdf/2304.14293v2.pdf", "tokenized_text": "large_language large language generate fluent texts follow natural solve wide_range wide range tasks task specific training difficult control generation constraints required different applications thiswork present controlled text generation framework different constraints conditioning natural languagedescriptions demonstrations constraints particular underlying constraints natural texts combination shelf nlp tools simple heuristics verbalize natural_language natural language instructions form weakly data prepending natural_language natural language descriptions demonstrations fine tune pre trained_language trained language toincorporate types constraints compared existing search based based methods flexible different constraint typesand smaller impact generation quality speed modify decoding procedure additionally allows themodel adapt new constraints training use offew shot task generalization context_learning context learning abilities ofinstruction tuned language_models language"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable performance acrossdiverse domains, thereby prompting researchers to explore their potential foruse in recommendation systems. Initial attempts have leveraged the exceptionalcapabilities of LLMs, such as rich knowledge and strong generalization throughIn-context Learning, which involves phrasing the recommendation task asprompts. Nevertheless, the performance of LLMs in recommendation tasks remainssuboptimal due to a substantial disparity between the training tasks for LLMsand recommendation tasks, as well as inadequate recommendation data duringpre-training. To bridge the gap, we consider building a Large RecommendationLanguage Model by tunning LLMs with recommendation data. To this end, wepropose an efficient and effective Tuning framework for Aligning LLMs withRecommendation, namely TALLRec. We have demonstrated that the proposed TALLRecframework can significantly enhance the recommendation capabilities of LLMs inthe movie and book domains, even with a limited dataset of fewer than 100samples. Additionally, the proposed framework is highly efficient and can beexecuted on a single RTX 3090 with LLaMA-7B. Furthermore, the fine-tuned LLMexhibits robust cross-domain generalization. Our code and data are available athttps://github.com/SAI990323/TALLRec.", "title": "tallrec an effective and efficient tuning framework to align large language model with recommendation", "url": "http://arxiv.org/pdf/2305.00447v3.pdf", "tokenized_text": "large_language large language llms demonstrated_remarkable demonstrated remarkable performance domains researchers explore potential recommendation systems initial attempts leveraged llms rich knowledge strong generalization throughin context_learning context learning involves phrasing recommendation task performance llms recommendation tasks substantial disparity training tasks llmsand recommendation tasks inadequate recommendation data duringpre training bridge gap consider building large llms recommendation data end wepropose efficient effective tuning framework aligning llms demonstrated proposed significantly enhance recommendation capabilities llms inthe movie book domains limited dataset fewer additionally proposed framework highly efficient single furthermore fine tuned robust cross domain generalization code data available"}
{"id": "nan", "abstract": "  Entity Matching is the task of deciding if two entity descriptions refer tothe same real-world entity. State-of-the-art entity matching methods often relyon fine-tuning Transformer models such as BERT or RoBERTa. Two major drawbacksof using these models for entity matching are that (i) the models requiresignificant amounts of fine-tuning data for reaching a good performance and(ii) the fine-tuned models are not robust concerning out-of-distributionentities. In this paper, we investigate using ChatGPT for entity matching as amore robust, training data-efficient alternative to traditional Transformermodels. We perform experiments along three dimensions: (i) general promptdesign, (ii) in-context learning, and (iii) provision of higher-level matchingknowledge. We show that ChatGPT is competitive with a fine-tuned RoBERTa model,reaching a zero-shot performance of 82.35% F1 on a challenging matching task onwhich RoBERTa requires 2000 training examples for reaching a similarperformance. Adding in-context demonstrations to the prompts further improvesthe F1 by up to 7.85% when using similarity-based example selection. Alwaysusing the same set of 10 handpicked demonstrations leads to an improvement of4.92% over the zero-shot performance. Finally, we show that ChatGPT can also beguided by adding higher-level matching knowledge in the form of rules to theprompts. Providing matching rules leads to similar performance gains asproviding in-context demonstrations.", "title": "using chatgpt for entity matching", "url": "http://arxiv.org/pdf/2305.03423v2.pdf", "tokenized_text": "entity matching task deciding entity descriptions refer tothe real world entity state art entity matching methods relyon fine tuning transformer bert roberta major entity matching requiresignificant amounts fine tuning data reaching good performance fine tuned robust concerning distributionentities paper investigate chatgpt entity matching amore robust training_data training data efficient alternative traditional transformermodels perform experiments dimensions general ii context_learning context learning iii provision higher level chatgpt competitive fine tuned roberta reaching zero shot performance f1 challenging matching task roberta requires training_examples training examples reaching adding context demonstrations improvesthe f1 similarity based example selection set 10 demonstrations leads improvement zero shot performance finally chatgpt adding higher level matching knowledge form rules theprompts providing matching rules leads similar performance gains context demonstrations"}
{"id": "nan", "abstract": "  Large language models (LLMs) are increasingly adopted for a variety of taskswith implicit graphical structures, such as planning in robotics, multi-hopquestion answering or knowledge probing, structured commonsense reasoning, andmore. While LLMs have advanced the state-of-the-art on these tasks withstructure implications, whether LLMs could explicitly process textualdescriptions of graphs and structures, map them to grounded conceptual spaces,and perform structured operations remains underexplored. To this end, wepropose NLGraph (Natural Language Graph), a comprehensive benchmark ofgraph-based problem solving designed in natural language. NLGraph contains29,370 problems, covering eight graph reasoning tasks with varying complexityfrom simple tasks such as connectivity and shortest path up to complex problemssuch as maximum flow and simulating graph neural networks. We evaluate LLMs(GPT-3/4) with various prompting approaches on the NLGraph benchmark and findthat 1) language models do demonstrate preliminary graph reasoning abilities,2) the benefit of advanced prompting and in-context learning diminishes on morecomplex graph problems, while 3) LLMs are also (un)surprisingly brittle in theface of spurious correlations in graph and problem settings. We then proposeBuild-a-Graph Prompting and Algorithmic Prompting, two instruction-basedapproaches to enhance LLMs in solving natural language graph problems.Build-a-Graph and Algorithmic prompting improve the performance of LLMs onNLGraph by 3.07% to 16.85% across multiple tasks and settings, while how tosolve the most complicated graph reasoning tasks in our setup with languagemodels remains an open research question. The NLGraph benchmark and evaluationcode are available at https://github.com/Arthur-Heng/NLGraph.", "title": "can language models solve graph problems in natural language", "url": "http://arxiv.org/pdf/2305.10037v2.pdf", "tokenized_text": "large_language large language llms increasingly adopted variety taskswith implicit graphical structures planning robotics multi hopquestion answering knowledge probing structured commonsense reasoning andmore llms advanced state art tasks implications llms explicitly process textualdescriptions graphs structures map grounded conceptual spaces perform structured operations remains underexplored end wepropose natural_language natural language graph comprehensive benchmark based problem solving designed natural_language natural language problems covering graph reasoning tasks varying simple tasks connectivity shortest path complex maximum flow simulating graph neural_networks neural networks evaluate approaches benchmark findthat language_models language demonstrate preliminary graph reasoning benefit advanced context_learning context learning diminishes morecomplex graph problems llms brittle spurious correlations graph problem settings graph algorithmic instruction basedapproaches enhance llms solving natural_language natural language graph problems build graph algorithmic improve performance llms multiple tasks settings tosolve complicated graph reasoning tasks setup languagemodels remains open research question benchmark available"}
{"id": "nan", "abstract": "  With the rapid development of artificial general intelligence (AGI), variousmultimedia services based on pretrained foundation models (PFMs) need to beeffectively deployed. With edge servers that have cloud-level computing power,edge intelligence can extend the capabilities of AGI to mobile edge networks.However, compared with cloud data centers, resource-limited edge servers canonly cache and execute a small number of PFMs, which typically consist ofbillions of parameters and require intensive computing power and GPU memoryduring inference. To address this challenge, in this paper, we propose a jointfoundation model caching and inference framework that aims to balance thetradeoff among inference latency, accuracy, and resource consumption bymanaging cached PFMs and user requests efficiently during the provisioning ofgenerative AI services. Specifically, considering the in-context learningability of PFMs, a new metric named the Age of Context (AoC), is proposed tomodel the freshness and relevance between examples in past demonstrations andcurrent service requests. Based on the AoC, we propose a least context cachingalgorithm to manage cached PFMs at edge servers with historical prompts andinference results. The numerical results demonstrate that the proposedalgorithm can reduce system costs compared with existing baselines byeffectively utilizing contextual information.", "title": "joint foundation model caching and inference of generative ai services for edge intelligence", "url": "http://arxiv.org/pdf/2305.12130v1.pdf", "tokenized_text": "rapid development artificial general intelligence agi services based pretrained foundation_models foundation pfms need deployed edge servers cloud level computing power edge intelligence extend capabilities agi mobile edge networks compared cloud data centers resource limited edge servers cache execute small_number small number pfms typically consist parameters require intensive computing power gpu inference address challenge paper propose caching inference framework aims balance thetradeoff inference latency accuracy resource consumption cached pfms user requests efficiently ofgenerative ai services specifically considering context learningability pfms new metric named age context aoc proposed freshness relevance examples past demonstrations service requests based aoc propose context manage cached pfms edge servers historical andinference results numerical results_demonstrate results demonstrate reduce system costs compared existing baselines byeffectively utilizing contextual information"}
{"id": "nan", "abstract": "  In-context learning (ICL) has emerged as a new approach to various naturallanguage processing tasks, utilizing large language models (LLMs) to makepredictions based on context that has been supplemented with a few examples ortask-specific instructions. In this paper, we aim to extend this method toquestion answering tasks that utilize structured knowledge sources, and improveText-to-SQL systems by exploring various prompt design strategies for employingLLMs. We conduct a systematic investigation into different demonstrationselection methods and optimal instruction formats for prompting LLMs in theText-to-SQL task. Our approach involves leveraging the syntactic structure ofan example's SQL query to retrieve demonstrations, and we demonstrate thatpursuing both diversity and similarity in demonstration selection leads toenhanced performance. Furthermore, we show that LLMs benefit fromdatabase-related knowledge augmentations. Our most effective strategyoutperforms the state-of-the-art system by 2.5 points (Execution Accuracy) andthe best fine-tuned system by 5.1 points on the Spider dataset. These resultshighlight the effectiveness of our approach in adapting LLMs to the Text-to-SQLtask, and we present an analysis of the factors contributing to the success ofour strategy.", "title": "enhancing fewshot texttosql capabilities of large language models a study on prompt design strategies", "url": "http://arxiv.org/pdf/2305.12586v1.pdf", "tokenized_text": "context_learning context learning icl emerged new approach naturallanguage processing tasks utilizing large_language large language llms based context supplemented examples specific instructions paper aim extend method answering tasks utilize structured knowledge sources sql systems exploring design strategies conduct systematic investigation different demonstrationselection methods optimal instruction formats llms thetext sql task approach involves leveraging syntactic structure example sql query retrieve demonstrations demonstrate diversity similarity demonstration selection leads performance furthermore llms benefit related knowledge augmentations effective state art system 2.5 points execution accuracy andthe best fine tuned system 5.1 points spider dataset effectiveness approach adapting llms text present analysis factors contributing success ofour strategy"}
{"id": "nan", "abstract": "  In-context learning with large language models (LLMs) has recently caughtincreasing attention due to its superior few-shot performance on various tasks.However, its performance on text-to-SQL parsing still has much room forimprovement. In this paper, we hypothesize that a crucial aspect of LLMs toimprove for text-to-SQL parsing is their multi-step reasoning ability. Thus, wesystematically study how to enhance LLMs' reasoning ability through chain ofthought (CoT) style prompting, including the original chain-of-thoughtprompting (Wei et al., 2022b) and least-to-most prompting (Zhou et al., 2023).Our experiments demonstrate that iterative prompting as in Zhou et al. (2023)may be unnecessary for text-to-SQL parsing, and using detailed reasoning stepstends to have more error propagation issues. Based on these findings, wepropose a new CoT-style prompting method for text-to-SQL parsing. It brings 5.2and 6.5 point absolute gains on the Spider development set and the SpiderRealistic set, respectively, compared to the standard prompting method withoutreasoning steps; 2.4 and 1.5 point absolute gains, compared to theleast-to-most prompting method.", "title": "exploring chainofthought style prompting for texttosql", "url": "http://arxiv.org/pdf/2305.14215v2.pdf", "tokenized_text": "context_learning context learning large_language large language llms recently attention superior shot performance tasks performance text sql parsing room paper hypothesize crucial aspect llms toimprove text sql parsing multi step reasoning ability wesystematically study enhance llms reasoning ability chain cot style including original chain thoughtprompting et_al et al et_al et al experiments_demonstrate experiments demonstrate iterative et_al et al unnecessary text sql parsing detailed reasoning error propagation issues based findings wepropose new cot style method text sql parsing brings point absolute gains spider development set set respectively compared standard method steps 2.4 1.5 point absolute gains compared theleast method"}
{"id": "nan", "abstract": "  When pretrained language models (LMs) are applied to discriminative taskssuch as multiple-choice questions, they place probability mass on vocabularytokens that aren't among the given answer choices. Spreading probability massacross multiple surface forms with identical meaning (such as \"bath\" and\"bathtub\") is thought to cause an underestimation of a model's trueperformance, referred to as the \"surface form competition\" (SFC) hypothesis.This has motivated the introduction of various probability normalizationmethods. However, many core questions remain unanswered. How do we measure SFC?Are there direct ways of reducing it, and does doing so improve taskperformance?  We propose a mathematical formalism for SFC which allows us to quantify andbound its impact for the first time. We identify a simple method for reducingit -- namely, increasing probability mass on the given answer choices by a)including them in the prompt and b) using in-context learning with even justone example. We show this method eliminates the impact of SFC in the majorityof instances. Our experiments on three diverse datasets and six LMs revealseveral additional surprising findings. For example, both normalization andprompting methods for reducing SFC can be ineffective or even detrimental totask performance for some LMs. We conclude with practical insights foreffectively prompting LMs for multiple-choice tasks.", "title": "increasing probability mass on answer choices does not always improve accuracy", "url": "http://arxiv.org/pdf/2305.14596v2.pdf", "tokenized_text": "pretrained_language pretrained language lms applied discriminative taskssuch multiple choice questions place probability mass given answer choices probability multiple surface forms identical meaning thought cause referred surface form competition hypothesis motivated introduction probability core questions remain unanswered measure direct ways reducing improve taskperformance propose mathematical formalism allows quantify impact time identify simple method increasing probability mass given answer choices context_learning context learning example method eliminates impact majorityof instances experiments diverse datasets lms additional surprising findings example normalization andprompting methods reducing ineffective detrimental performance lms conclude practical insights lms multiple choice tasks"}
{"id": "nan", "abstract": "  A hallmark of modern large language models (LLMs) is their impressive generalzero-shot and few-shot abilities, often elicited through in-context learning(ICL) via prompting. However, while highly coveted and being the most general,zero-shot performances in LLMs are still typically weaker due to the lack ofguidance and the difficulty of applying existing automatic prompt designmethods in general tasks when ground-truth labels are unavailable. In thisstudy, we address this by presenting Universal Self-Adaptive Prompting (USP),an automatic prompt design approach specifically tailored for zero-shotlearning (while compatible with few-shot). Requiring only a small amount ofunlabeled data and an inference-only LLM, USP is highly versatile: to achieveuniversal prompting, USP categorizes a possible NLP task into one of the threepossible task types and then uses a corresponding selector to select the mostsuitable queries and zero-shot model-generated responses aspseudo-demonstrations, thereby generalizing ICL to the zero-shot setup in afully automated way. We evaluate USP with PaLM and PaLM 2 models anddemonstrate performances that are considerably stronger than standard zero-shotbaselines and often comparable to or even superior to few-shot baselines acrossmore than 40 natural language understanding, natural language generation, andreasoning tasks.", "title": "universal selfadaptive prompting", "url": "http://arxiv.org/pdf/2305.14926v2.pdf", "tokenized_text": "modern large_language large language llms impressive shot shot abilities elicited context learning(icl highly general zero shot performances llms typically weaker lack difficulty applying existing automatic general tasks ground truth labels unavailable thisstudy address presenting universal automatic design approach specifically tailored zero shotlearning compatible shot requiring small data inference llm highly versatile possible nlp task task types uses corresponding selector select queries zero shot generated responses demonstrations generalizing icl zero shot setup automated way evaluate palm palm performances considerably stronger standard zero comparable superior shot baselines 40 natural_language natural language understanding natural_language natural language generation tasks"}
{"id": "nan", "abstract": "  LLM-powered chatbots are becoming widely adopted in applications such ashealthcare, personal assistants, industry hiring decisions, etc. In many ofthese cases, chatbots are fed sensitive, personal information in their prompts,as samples for in-context learning, retrieved records from a database, or aspart of the conversation. The information provided in the prompt could directlyappear in the output, which might have privacy ramifications if there issensitive information there. As such, in this paper, we aim to understand theinput copying and regurgitation capabilities of these models during inferenceand how they can be directly instructed to limit this copying by complying withregulations such as HIPAA and GDPR, based on their internal knowledge of them.More specifically, we find that when ChatGPT is prompted to summarize coverletters of a 100 candidates, it would retain personally identifiableinformation (PII) verbatim in 57.4% of cases, and we find this retention to benon-uniform between different subgroups of people, based on attributes such asgender identity. We then probe ChatGPT's perception of privacy-related policiesand privatization mechanisms by directly instructing it to provide compliantoutputs and observe a significant omission of PII from output.", "title": "are chatbots ready for privacysensitive applications an investigation into input regurgitation and promptinduced sanitization", "url": "http://arxiv.org/pdf/2305.15008v1.pdf", "tokenized_text": "llm powered chatbots widely adopted applications personal assistants industry hiring decisions etc ofthese cases chatbots fed sensitive personal information samples context_learning context learning retrieved records database conversation information provided output privacy issensitive information paper aim understand theinput copying capabilities directly instructed limit copying hipaa gdpr based internal knowledge specifically find chatgpt prompted summarize 100 candidates retain cases find uniform different subgroups people based attributes identity probe chatgpt perception privacy related mechanisms directly instructing provide observe significant omission output"}
{"id": "nan", "abstract": "  Fine-tuning language models (LMs) has yielded success on diverse downstreamtasks, but as LMs grow in size, backpropagation requires a prohibitively largeamount of memory. Zeroth-order (ZO) methods can in principle estimate gradientsusing only two forward passes but are theorized to be catastrophically slow foroptimizing large models. In this work, we propose a memory-efficientzerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operatein-place, thereby fine-tuning LMs with the same memory footprint as inference.For example, with a single A100 80GB GPU, MeZO can train a 30-billion parametermodel, whereas fine-tuning with backpropagation can train only a 2.7B LM withthe same budget. We conduct comprehensive experiments across model types(masked and autoregressive LMs), model scales (up to 66B), and downstream tasks(classification, multiple-choice, and generation). Our results demonstrate that(1) MeZO significantly outperforms in-context learning and linear probing; (2)MeZO achieves comparable performance to fine-tuning with backpropagation acrossmultiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reductionin our implementation; (3) MeZO is compatible with both full-parameter andparameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZOcan effectively optimize non-differentiable objectives (e.g., maximizingaccuracy or F1). We support our empirical findings with theoretical insights,highlighting how adequate pre-training and task prompts enable MeZO tofine-tune huge models, despite classical ZO analyses suggesting otherwise.", "title": "finetuning language models with just forward passes", "url": "http://arxiv.org/pdf/2305.17333v2.pdf", "tokenized_text": "fine tuning language_models language lms yielded success diverse downstreamtasks lms grow size requires prohibitively largeamount memory order methods principle estimate forward passes slow large work propose memory optimizer adapting classical method place fine tuning lms memory footprint inference example single 80 gpu train 30 billion parametermodel fine tuning train 2.7b lm withthe budget conduct comprehensive experiments autoregressive lms scales downstream multiple choice generation results_demonstrate results demonstrate significantly_outperforms significantly outperforms context_learning context learning linear probing achieves comparable performance fine tuning acrossmultiple tasks memory reduction 2x gpu implementation compatible parameter andparameter efficient tuning techniques lora prefix tuning effectively optimize non differentiable objectives e.g. f1 support empirical findings theoretical insights highlighting adequate pre training task enable tofine tune huge despite classical analyses suggesting"}
{"id": "nan", "abstract": "  Large language models (LLMs) have a wealth of knowledge that allows them toexcel in various Natural Language Processing (NLP) tasks. Current researchfocuses on enhancing their performance within their existing knowledge. Despitetheir vast knowledge, LLMs are still limited by the amount of information theycan accommodate and comprehend. Therefore, the ability to understand their ownlimitations on the unknows, referred to as self-knowledge, is of paramountimportance. This study aims to evaluate LLMs' self-knowledge by assessing theirability to identify unanswerable or unknowable questions. We introduce anautomated methodology to detect uncertainty in the responses of these models,providing a novel measure of their self-knowledge. We further introduce aunique dataset, SelfAware, consisting of unanswerable questions from fivediverse categories and their answerable counterparts. Our extensive analysis,involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering anintrinsic capacity for self-knowledge within these models. Moreover, wedemonstrate that in-context learning and instruction tuning can further enhancethis self-knowledge. Despite this promising insight, our findings alsohighlight a considerable gap between the capabilities of these models and humanproficiency in recognizing the limits of their knowledge.", "title": "do large language models know what they don't know", "url": "http://arxiv.org/pdf/2305.18153v2.pdf", "tokenized_text": "large_language large language llms knowledge allows toexcel natural_language natural language processing nlp tasks current researchfocuses enhancing performance existing knowledge vast knowledge llms limited information theycan accommodate comprehend ability understand ownlimitations referred self knowledge paramountimportance study aims evaluate llms self knowledge assessing theirability identify unanswerable questions introduce methodology detect uncertainty responses providing novel measure self knowledge introduce dataset consisting unanswerable questions categories counterparts extensive analysis involving 20 llms including gpt-3 instructgpt llama discovering capacity self knowledge wedemonstrate context_learning context learning instruction_tuning instruction tuning self knowledge despite promising insight findings alsohighlight considerable gap capabilities recognizing limits knowledge"}
{"id": "nan", "abstract": "  Contrastive Language-Image Pre-training (CLIP) stands as one of the mosteffective and scalable methods for training transferable vision models usingpaired image and text data. CLIP models are trained using contrastive loss,which typically relies on data augmentations to prevent overfitting andshortcuts. However, in the CLIP training paradigm, data augmentations areexclusively applied to image inputs, while language inputs remain unchangedthroughout the entire training process, limiting the exposure of diverse textsto the same image. In this paper, we introduce Language augmented CLIP(LaCLIP), a simple yet highly effective approach to enhance CLIP trainingthrough language rewrites. Leveraging the in-context learning capability oflarge language models, we rewrite the text descriptions associated with eachimage. These rewritten texts exhibit diversity in sentence structure andvocabulary while preserving the original key concepts and meanings. Duringtraining, LaCLIP randomly selects either the original texts or the rewrittenversions as text augmentations for each image. Extensive experiments on CC3M,CC12M, RedCaps and LAION-400M datasets show that CLIP pre-training withlanguage rewrites significantly improves the transfer performance withoutcomputation or memory overhead during training. Specifically for ImageNetzero-shot accuracy, LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% onLAION-400M. Code is available at https://github.com/LijieFan/LaCLIP.", "title": "improving clip training with language rewrites", "url": "http://arxiv.org/pdf/2305.20088v2.pdf", "tokenized_text": "contrastive_language-image_pre-training contrastive language-image pre-training clip stands scalable methods training transferable vision image text data clip trained contrastive loss typically relies data augmentations prevent overfitting clip training paradigm data augmentations applied image inputs language inputs remain entire training process limiting exposure diverse image paper introduce language augmented simple highly effective approach enhance clip language rewrites leveraging context_learning context learning capability oflarge language_models language rewrite text descriptions associated eachimage rewritten texts exhibit diversity sentence structure preserving original key concepts meanings duringtraining randomly selects original texts text augmentations image extensive_experiments extensive experiments datasets clip pre training withlanguage rewrites significantly improves transfer performance memory overhead training specifically shot accuracy outperforms clip 8.2 2.4 code_is_available code available"}
{"id": "nan", "abstract": "  One impressive emergent capability of large language models (LLMs) isgeneration of code, including Structured Query Language (SQL) for databases.For the task of converting natural language text to SQL queries, Text-to-SQL,adaptation of LLMs is of paramount importance, both in in-context learning andfine-tuning settings, depending on the amount of adaptation data used. In thispaper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging onPaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM isbased on an execution-based self-consistency prompting approach designed forText-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to ourbest knowledge is the first to outperform previous state-of-the-art withfine-tuning by a significant margin, 4%. Furthermore, we demonstrate that thefine-tuned SQL-PALM outperforms it further by another 1%. Towards applyingSQL-PaLM to real-world scenarios we further evaluate its robustness on otherchallenging variants of Spider and demonstrate the superior generalizationcapability of SQL-PaLM. In addition, via extensive case studies, we demonstratethe impressive intelligent capabilities and various success enablers ofLLM-based Text-to-SQL.", "title": "sqlpalm improved large language model adaptation for texttosql", "url": "http://arxiv.org/pdf/2306.00739v3.pdf", "tokenized_text": "impressive emergent capability large_language large language llms code including structured query language sql databases task converting natural_language natural language text sql queries text sql adaptation llms paramount importance context_learning context learning andfine tuning settings depending adaptation data thispaper propose llm based text sql sql palm leveraging pushes state art settings shot sql palm isbased execution based self consistency approach designed fortext sql achieves test suite accuracy spider ourbest knowledge outperform previous state art withfine tuning significant margin furthermore demonstrate thefine tuned sql palm outperforms palm real world_scenarios world scenarios evaluate robustness variants spider demonstrate superior generalizationcapability sql palm addition extensive case studies demonstratethe impressive intelligent capabilities success ofllm based text sql"}
{"id": "nan", "abstract": "  We propose a novel zero-shot approach to computing correspondences between 3Dshapes. Existing approaches mainly focus on isometric and near-isometric shapepairs (e.g., human vs. human), but less attention has been given to stronglynon-isometric and inter-class shape matching (e.g., human vs. cow). To thisend, we introduce a fully automatic method that exploits the exceptionalreasoning capabilities of recent foundation models in language and vision totackle difficult shape correspondence problems. Our approach comprises multiplestages. First, we classify the 3D shapes in a zero-shot manner by feedingrendered shape views to a language-vision model (e.g., BLIP2) to generate alist of class proposals per shape. These proposals are unified into a singleclass per shape by employing the reasoning capabilities of ChatGPT. Second, weattempt to segment the two shapes in a zero-shot manner, but in contrast to theco-segmentation problem, we do not require a mutual set of semantic regions.Instead, we propose to exploit the in-context learning capabilities of ChatGPTto generate two different sets of semantic regions for each shape and asemantic mapping between them. This enables our approach to match stronglynon-isometric shapes with significant differences in geometric structure.Finally, we employ the generated semantic mapping to produce coarsecorrespondences that can further be refined by the functional maps framework toproduce dense point-to-point maps. Our approach, despite its simplicity,produces highly plausible results in a zero-shot manner, especially betweenstrongly non-isometric shapes. Project webpage:https://samir55.github.io/3dshapematch/.", "title": "zeroshot 3d shape correspondence", "url": "http://arxiv.org/pdf/2306.03253v2.pdf", "tokenized_text": "propose_a_novel propose novel zero shot approach computing correspondences existing approaches mainly focus near e.g. human vs. human attention given inter class shape matching e.g. human vs. thisend introduce fully automatic method exploits capabilities recent foundation_models foundation language vision totackle difficult shape correspondence problems approach comprises classify 3d shapes zero shot manner shape views language vision e.g. blip2 generate class proposals shape proposals unified shape employing reasoning capabilities chatgpt second segment shapes zero shot manner contrast segmentation problem require mutual set semantic regions instead propose exploit context_learning context learning capabilities generate different sets semantic regions shape mapping enables approach match shapes significant differences geometric structure finally employ generated semantic mapping produce refined functional maps framework toproduce dense point point maps approach despite simplicity produces highly plausible results zero shot manner especially non shapes project webpage"}
{"id": "nan", "abstract": "  High-quality instructions and responses are essential for the zero-shotperformance of large language models on interactive natural language tasks. Forinteractive vision-language tasks involving intricate visual scenes, a largequantity of diverse and creative instruction-response pairs should beimperative to tune vision-language models (VLMs). Nevertheless, the currentavailability of vision-language instruction-response pairs in terms ofquantity, diversity, and creativity remains limited, posing challenges to thegeneralization of interactive VLMs. Here we present MultI-Modal In-ContextInstruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodalinstruction-response pairs, with 2.2 million unique instructions derived fromimages and videos. Each pair is accompanied by multi-modal in-contextinformation, forming conversational contexts aimed at empowering VLMs inperception, reasoning, and planning. The instruction-response collectionprocess, dubbed as Syphus, is scaled using an automatic annotation pipelinethat combines human expertise with GPT's capabilities. Using the MIMIC-ITdataset, we train a large VLM named Otter. Based on extensive evaluationsconducted on vision-language benchmarks, it has been observed that Otterdemonstrates remarkable proficiency in multi-modal perception, reasoning, andin-context learning. Human evaluation reveals it effectively aligns with theuser's intentions. We release the MIMIC-IT dataset, instruction-responsecollection pipeline, benchmarks, and the Otter model.", "title": "mimicit multimodal incontext instruction tuning", "url": "http://arxiv.org/pdf/2306.05425v1.pdf", "tokenized_text": "high quality instructions responses essential zero shotperformance large_language large language interactive natural_language natural language tasks forinteractive vision language tasks involving intricate visual scenes diverse creative instruction response pairs tune vision language_models language vlms vision language instruction response pairs terms diversity creativity remains limited posing challenges thegeneralization interactive vlms present multi modal tuning mimic dataset comprising 2.8 million response pairs 2.2 million unique instructions derived videos pair accompanied multi modal forming conversational contexts aimed empowering vlms reasoning planning instruction response dubbed scaled automatic annotation combines human expertise gpt capabilities mimic train large vlm named based extensive vision language benchmarks observed remarkable proficiency multi modal perception reasoning andin context_learning context learning human evaluation reveals effectively aligns theuser intentions release mimic dataset instruction pipeline benchmarks"}
{"id": "nan", "abstract": "  Foundation models, often pre-trained with large-scale data, have achievedparamount success in jump-starting various vision and language applications.Recent advances further enable adapting foundation models in downstream tasksefficiently using only a few training samples, e.g., in-context learning. Yet,the application of such learning paradigms in medical image analysis remainsscarce due to the shortage of publicly accessible data and benchmarks. In thispaper, we aim at approaches adapting the foundation models for medical imageclassification and present a novel dataset and benchmark for the evaluation,i.e., examining the overall performance of accommodating the large-scalefoundation models downstream on a set of diverse real-world clinical tasks. Wecollect five sets of medical imaging data from multiple institutes targeting avariety of real-world clinical tasks (22,349 images in total), i.e., thoracicdiseases screening in X-rays, pathological lesion tissue screening, lesiondetection in endoscopy images, neonatal jaundice evaluation, and diabeticretinopathy grading. Results of multiple baseline methods are demonstratedusing the proposed dataset from both accuracy and cost-effective perspectives.", "title": "medfmc a realworld dataset and benchmark for foundation model adaptation in medical image classification", "url": "http://arxiv.org/pdf/2306.09579v1.pdf", "tokenized_text": "foundation_models foundation pre trained large scale data success starting vision language applications recent_advances recent advances enable adapting foundation_models foundation downstream training samples e.g. context_learning context learning application learning paradigms medical image analysis shortage publicly accessible data benchmarks thispaper aim approaches adapting foundation_models foundation medical imageclassification present novel dataset benchmark evaluation i.e. examining overall performance large downstream set diverse real world clinical tasks sets medical imaging data multiple targeting avariety real world clinical tasks images total i.e. screening screening images neonatal evaluation grading results multiple baseline methods proposed dataset accuracy cost effective perspectives"}
{"id": "nan", "abstract": "  Although pre-trained language models~(PLMs) have recently advanced theresearch progress in mathematical reasoning, they are not specially designed asa capable multi-task solver, suffering from high cost for multi-task deployment(\\eg a model copy for a task) and inferior performance on complex mathematicalproblems in practical applications. To address these issues, in this paper, wepropose \\textbf{JiuZhang~2.0}, a unified Chinese PLM specially for multi-taskmathematical problem solving. Our idea is to maintain a moderate-sized modeland employ the \\emph{cross-task knowledge sharing} to improve the modelcapacity in a multi-task setting. Specially, we construct aMixture-of-Experts~(MoE) architecture for modeling mathematical text, so as tocapture the common mathematical knowledge across tasks. For optimizing the MoEarchitecture, we design \\emph{multi-task continual pre-training} and\\emph{multi-task fine-tuning} strategies for multi-task adaptation. Thesetraining strategies can effectively decompose the knowledge from the task dataand establish the cross-task sharing via expert networks. In order to furtherimprove the general capacity of solving different complex tasks, we leveragelarge language models~(LLMs) as complementary models to iteratively refine thegenerated solution by our PLM, via in-context learning. Extensive experimentshave demonstrated the effectiveness of our model.", "title": "jiuzhang 20 a unified chinese pretrained language model for multitask mathematical problem solving", "url": "http://arxiv.org/pdf/2306.11027v1.pdf", "tokenized_text": "pre trained_language trained language recently advanced theresearch progress mathematical reasoning specially designed asa capable multi task solver suffering high cost multi task copy task inferior performance complex practical applications address issues paper wepropose unified chinese plm specially multi problem solving idea maintain moderate sized modeland employ task knowledge sharing improve multi task setting specially construct architecture modeling mathematical text tocapture common mathematical knowledge tasks optimizing design task continual pre training task fine tuning strategies multi task adaptation strategies effectively decompose knowledge task dataand establish cross task sharing expert networks order furtherimprove general capacity solving different complex tasks language models~(llms complementary iteratively refine thegenerated solution plm context_learning context learning extensive demonstrated effectiveness"}
{"id": "nan", "abstract": "  API documentation, technical blogs and programming Q&A sites contain numerouspartial code that can be reused in programming tasks, but often these code areuncompilable due to unresolved names and syntax errors. To facilitate partialcode reuse, we propose the Partial Code Reuse Chain (PCR-Chain) for resolvingfully-qualified names (FQNs) and fixing last-mile syntax errors in partial codebased on a giant large language model (LLM) like ChatGPT. Methodologically,PCR-Chain is backed up by the underlying global-level prompt architecture(which combines three design ideas: hierarchical task breakdown, promptcomposition, and a mix of prompt-based AI and non-AI units) and the local-levelprompt design. Technically, we propose PCR-Chain, which employs in-contextlearning rather than symbolic, costly training methods. Experimental resultsdemonstrate that in dynamically-typed languages (Python), PCR-Chain outperformscurrent state-of-the-art (SOTA) 5% accuracy like RING. For statically-typelanguages (Java), our approach achieves high accuracy of 80.5% in resolvingboth non-FQNs and last-mile syntax errors, surpassing SOTA methods (RING) thatcan only address last-mile syntax errors. The correct execution of the unit,module, and PCR-Chain demonstrates the effectiveness of the prompt design,composition, and architecture and opens up possibilities for building softwareengineering tools based on LLMs, replacing traditional program analysismethods.", "title": "a chain of aibased solutions for resolving fqns and fixing syntax errors in partial code", "url": "http://arxiv.org/pdf/2306.11981v1.pdf", "tokenized_text": "api_documentation api documentation technical blogs programming q&a sites contain code programming tasks code unresolved names syntax errors facilitate reuse propose partial code reuse chain chain names fixing syntax errors partial giant large_language large language llm like_chatgpt like chatgpt chain backed underlying global level combines design ideas hierarchical task breakdown mix based ai non ai units local design technically propose chain employs contextlearning symbolic costly training methods experimental resultsdemonstrate dynamically languages python chain state art sota accuracy like java approach achieves high accuracy non syntax errors surpassing sota methods address syntax errors correct execution unit module chain demonstrates effectiveness design composition architecture opens possibilities building softwareengineering tools based llms replacing traditional program"}
{"id": "nan", "abstract": "  Multimodal Entity Linking (MEL) is the task of mapping mentions withmultimodal contexts to the referent entities from a knowledge base (e.g.Wikipedia). Existing MEL methods mainly focus on designing complex multimodalinteraction mechanisms and require fine-tuning all model parameters, which canbe prohibitively costly and difficult to scale in the era of Large LanguageModels (LLMs). In this work, we propose GEMEL, a simple yet effectiveGenerative Multimodal Entity Linking framework based on LLMs, which directlygenerates target entity names. We keep the vision and language model frozen andonly train a feature mapper to enable cross-modality interactions. To adaptLLMs to the MEL task, we take advantage of the emergent in-context learningcapability of LLMs by retrieving multimodal instances as demonstrations.Extensive experiments show that, with only ~0.3% of the model parametersfine-tuned, GEMEL achieves state-of-the-art results on two well-established MELdatasets (7.7% accuracy gains on WikiDiverse and 8.8% accuracy gains onWikiMEL). The performance gain stems from mitigating the popularity bias of LLMpredictions and disambiguating less common entities effectively. Furtheranalysis verifies the generality and scalability of GEMEL. Our approach iscompatible with any off-the-shelf language model, paving the way towards anefficient and general solution for utilizing LLMs in the MEL task.", "title": "generative multimodal entity linking", "url": "http://arxiv.org/pdf/2306.12725v2.pdf", "tokenized_text": "multimodal entity linking task mapping mentions contexts entities knowledge base e.g. wikipedia existing methods mainly focus designing complex mechanisms require fine tuning parameters canbe prohibitively costly difficult scale era large_languagemodels large languagemodels llms work propose simple multimodal entity linking framework based llms target entity names vision language_model language frozen train feature enable cross modality interactions task advantage emergent context learningcapability llms retrieving multimodal instances demonstrations extensive_experiments extensive experiments tuned achieves_state achieves state art results established 7.7 accuracy gains 8.8 accuracy gains performance gain stems mitigating popularity bias llmpredictions common entities effectively verifies generality scalability approach shelf language_model language paving way anefficient general solution utilizing llms task"}
{"id": "nan", "abstract": "  We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling newcapabilities of perceiving object descriptions (e.g., bounding boxes) andgrounding text to the visual world. Specifically, we represent referexpressions as links in Markdown, i.e., ``[text span](bounding boxes)'', whereobject descriptions are sequences of location tokens. Together with multimodalcorpora, we construct large-scale data of grounded image-text pairs (calledGrIT) to train the model. In addition to the existing capabilities of MLLMs(e.g., perceiving general modalities, following instructions, and performingin-context learning), Kosmos-2 integrates the grounding capability intodownstream applications. We evaluate Kosmos-2 on a wide range of tasks,including (i) multimodal grounding, such as referring expression comprehension,and phrase grounding, (ii) multimodal referring, such as referring expressiongeneration, (iii) perception-language tasks, and (iv) language understandingand generation. This work lays out the foundation for the development ofEmbodiment AI and sheds light on the big convergence of language, multimodalperception, action, and world modeling, which is a key step toward artificialgeneral intelligence. Code and pretrained models are available athttps://aka.ms/kosmos-2.", "title": "kosmos2 grounding multimodal large language models to the world", "url": "http://arxiv.org/pdf/2306.14824v3.pdf", "tokenized_text": "introduce multimodal large_language large language mllm enabling object descriptions e.g. bounding boxes text visual world specifically represent links i.e. text boxes descriptions sequences location tokens construct large scale data grounded image text pairs train addition existing capabilities general modalities following instructions context_learning context learning integrates grounding capability applications evaluate wide_range wide range tasks including multimodal grounding referring expression comprehension phrase grounding ii multimodal referring referring iii perception language tasks iv language generation work lays foundation development ai sheds light big convergence language action world modeling key step intelligence code pretrained available"}
{"id": "nan", "abstract": "  Large transformer models trained on diverse datasets have shown a remarkableability to learn in-context, achieving high few-shot performance on tasks theywere not explicitly trained to solve. In this paper, we study the in-contextlearning capabilities of transformers in decision-making problems, i.e.,reinforcement learning (RL) for bandits and Markov decision processes. To doso, we introduce and study Decision-Pretrained Transformer (DPT), a supervisedpretraining method where the transformer predicts an optimal action given aquery state and an in-context dataset of interactions, across a diverse set oftasks. This procedure, while simple, produces a model with several surprisingcapabilities. We find that the pretrained transformer can be used to solve arange of RL problems in-context, exhibiting both exploration online andconservatism offline, despite not being explicitly trained to do so. The modelalso generalizes beyond the pretraining distribution to new tasks andautomatically adapts its decision-making strategies to unknown structure.Theoretically, we show DPT can be viewed as an efficient implementation ofBayesian posterior sampling, a provably sample-efficient RL algorithm. Wefurther leverage this connection to provide guarantees on the regret of thein-context algorithm yielded by DPT, and prove that it can learn faster thanalgorithms used to generate the pretraining data. These results suggest apromising yet simple path towards instilling strong in-context decision-makingabilities in transformers.", "title": "supervised pretraining can learn incontext reinforcement learning", "url": "http://arxiv.org/pdf/2306.14892v1.pdf", "tokenized_text": "large transformer trained diverse datasets shown remarkableability learn context achieving high shot performance tasks theywere explicitly trained solve paper study contextlearning capabilities transformers decision making problems i.e. learning rl markov decision processes introduce study transformer method transformer predicts optimal action given state context dataset interactions diverse set oftasks procedure simple produces find pretrained transformer solve arange rl problems context exhibiting exploration online offline despite explicitly trained generalizes pretraining distribution new tasks adapts decision making strategies unknown structure theoretically viewed efficient implementation posterior sampling provably sample efficient rl algorithm wefurther leverage connection provide guarantees regret thein context algorithm yielded prove learn faster generate pretraining data results suggest apromising simple path strong context decision transformers"}
{"id": "nan", "abstract": "  We present a new framework integrating the AI model GPT-4 into the iterativeprocess of reticular chemistry experimentation, leveraging a cooperativeworkflow of interaction between AI and a human researcher. This GPT-4 ReticularChemist is an integrated system composed of three phases. Each of theseutilizes GPT-4 in various capacities, wherein GPT-4 provides detailedinstructions for chemical experimentation and the human provides feedback onthe experimental outcomes, including both success and failures, for thein-context learning of AI in the next iteration. This iterative human-AIinteraction enabled GPT-4 to learn from the outcomes, much like an experiencedchemist, by a prompt-learning strategy. Importantly, the system is based onnatural language for both development and operation, eliminating the need forcoding skills, and thus, make it accessible to all chemists. Our collaborationwith GPT-4 Reticular Chemist guided the discovery of an isoreticular series ofMOFs, with each synthesis fine-tuned through iterative feedback and expertsuggestions. This workflow presents a potential for broader applications inscientific research by harnessing the capability of large language models likeGPT-4 to enhance the feasibility and efficiency of research activities.", "title": "a gpt4 reticular chemist for guiding mof discovery", "url": "http://arxiv.org/pdf/2306.14915v2.pdf", "tokenized_text": "present new framework integrating ai gpt-4 chemistry experimentation leveraging interaction ai human gpt-4 integrated system composed phases gpt-4 capacities gpt-4 provides chemical experimentation human provides feedback onthe experimental outcomes including success failures thein context_learning context learning ai iteration iterative human aiinteraction enabled gpt-4 learn outcomes like learning strategy importantly system based onnatural language development operation eliminating need skills accessible gpt-4 chemist guided discovery series synthesis fine tuned iterative feedback workflow presents potential broader applications inscientific research harnessing capability large_language large language enhance feasibility efficiency research activities"}
{"id": "nan", "abstract": "  Large-scale generative models such as GPT and DALL-E have revolutionized theresearch community. These models not only generate high fidelity outputs, butare also generalists which can solve tasks not explicitly taught. In contrast,speech generative models are still primitive in terms of scale and taskgeneralization. In this paper, we present Voicebox, the most versatiletext-guided generative model for speech at scale. Voicebox is anon-autoregressive flow-matching model trained to infill speech, given audiocontext and text, trained on over 50K hours of speech that are not filtered orenhanced. Similar to GPT, Voicebox can perform many different tasks throughin-context learning, but is more flexible as it can also condition on futurecontext. Voicebox can be used for mono or cross-lingual zero-shottext-to-speech synthesis, noise removal, content editing, style conversion, anddiverse sample generation. In particular, Voicebox outperforms thestate-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to20 times faster. Audio samples can be found in\\url{https://voicebox.metademolab.com}.", "title": "voicebox textguided multilingual universal speech generation at scale", "url": "http://arxiv.org/pdf/2306.15687v2.pdf", "tokenized_text": "large scale generative gpt dall revolutionized theresearch community generate high fidelity outputs solve tasks explicitly taught contrast speech generative primitive terms scale taskgeneralization paper present guided generative speech scale anon autoregressive flow matching trained speech given text trained 50 hours speech filtered similar gpt perform different tasks throughin context_learning context learning flexible condition mono cross lingual zero shottext speech synthesis noise removal content editing style conversion sample generation particular outperforms thestate art zero shot tts vall 5.9 word error rates audio similarity vs times faster audio samples found"}
{"id": "nan", "abstract": "  In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enablingfrozen LLMs to perform both understanding and generation tasks involvingnon-linguistic modalities such as images or videos. SPAE converts between rawpixels and interpretable lexical tokens (or words) extracted from the LLM'svocabulary. The resulting tokens capture both the semantic meaning and thefine-grained details needed for visual reconstruction, effectively translatingthe visual content into a language comprehensible to the LLM, and empowering itto perform a wide array of multimodal tasks. Our approach is validated throughin-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse setof image understanding and generation tasks. Our method marks the firstsuccessful attempt to enable a frozen LLM to generate image content whilesurpassing state-of-the-art performance in image understanding tasks, under thesame setting, by over 25%.", "title": "spae semantic pyramid autoencoder for multimodal generation with frozen llms", "url": "http://arxiv.org/pdf/2306.17842v3.pdf", "tokenized_text": "work introduce semantic autoencoder llms perform understanding generation tasks linguistic modalities images videos converts interpretable lexical tokens words extracted resulting tokens capture semantic meaning thefine grained details needed visual reconstruction effectively visual content language comprehensible llm empowering perform wide array multimodal tasks approach validated throughin context_learning context learning experiments frozen palm gpt 3.5 diverse setof image understanding generation tasks method marks attempt enable frozen llm generate image content whilesurpassing state art performance image understanding tasks thesame setting 25"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) have made extraordinary progress in the field ofArtificial Intelligence and have demonstrated remarkable capabilities across alarge variety of tasks and domains. However, as we venture closer to creatingArtificial General Intelligence (AGI) systems, we recognize the need tosupplement LLMs with long-term memory to overcome the context window limitationand more importantly, to create a foundation for sustained reasoning,cumulative learning and long-term user interaction. In this paper we proposeRecallM, a novel architecture for providing LLMs with an adaptable andupdatable long-term memory mechanism. Unlike previous methods, the RecallMarchitecture is particularly effective at belief updating and maintaining atemporal understanding of the knowledge provided to it. We demonstrate throughvarious experiments the effectiveness of this architecture. Furthermore,through our own temporal understanding and belief updating experiments, we showthat RecallM is four times more effective than using a vector database forupdating knowledge previously stored in long-term memory. We also demonstratethat RecallM shows competitive performance on general question-answering andin-context learning tasks.", "title": "recallm an adaptable memory mechanism with temporal understanding for large language models", "url": "http://arxiv.org/pdf/2307.02738v3.pdf", "tokenized_text": "large_language large language llms extraordinary progress field ofartificial intelligence demonstrated_remarkable demonstrated remarkable capabilities alarge variety tasks domains closer general intelligence agi systems recognize need llms long term memory overcome context window importantly create foundation reasoning learning long term user interaction paper novel architecture providing llms adaptable long term memory mechanism unlike previous methods particularly effective belief updating maintaining understanding knowledge provided demonstrate experiments effectiveness architecture furthermore temporal understanding belief updating experiments showthat times effective vector database knowledge previously stored long term memory demonstratethat shows competitive_performance competitive performance general question answering andin context_learning context learning tasks"}
{"id": "nan", "abstract": "  Recent works have empirically analyzed in-context learning and shown thattransformers trained on synthetic linear regression tasks can learn toimplement ridge regression, which is the Bayes-optimal predictor, givensufficient capacity [Aky\\\"urek et al., 2023], while one-layer transformers withlinear self-attention and no MLP layer will learn to implement one step ofgradient descent (GD) on a least-squares linear regression objective [vonOswald et al., 2022]. However, the theory behind these observations remainspoorly understood. We theoretically study transformers with a single layer oflinear self-attention, trained on synthetic noisy linear regression data.First, we mathematically show that when the covariates are drawn from astandard Gaussian distribution, the one-layer transformer which minimizes thepre-training loss will implement a single step of GD on the least-squareslinear regression objective. Then, we find that changing the distribution ofthe covariates and weight vector to a non-isotropic Gaussian distribution has astrong impact on the learned algorithm: the global minimizer of thepre-training loss now implements a single step of $\\textit{pre-conditioned}$GD. However, if only the distribution of the responses is changed, then thisdoes not have a large effect on the learned algorithm: even when the responsecomes from a more general family of $\\textit{nonlinear}$ functions, the globalminimizer of the pre-training loss still implements a single step of GD on aleast-squares linear regression objective.", "title": "one step of gradient descent is provably the optimal incontext learner with one layer of linear selfattention", "url": "http://arxiv.org/pdf/2307.03576v1.pdf", "tokenized_text": "recent works empirically analyzed context_learning context learning shown thattransformers trained synthetic linear regression tasks learn ridge regression bayes optimal predictor capacity et_al et al 2023 layer transformers self attention mlp layer learn implement step ofgradient descent gd squares linear regression objective et_al et al 2022 theory observations understood theoretically study transformers single layer self attention trained synthetic noisy linear regression data mathematically covariates drawn gaussian distribution layer transformer minimizes thepre training loss implement single step gd regression objective find changing distribution ofthe covariates weight vector non gaussian distribution astrong impact learned algorithm global thepre training loss implements single step distribution responses changed thisdoes large effect learned algorithm general family functions pre training loss implements single step gd squares linear regression objective"}
{"id": "nan", "abstract": "  We observe that pre-trained large language models (LLMs) are capable ofautoregressively completing complex token sequences -- from arbitrary onesprocedurally generated by probabilistic context-free grammars (PCFG), to morerich spatial patterns found in the Abstraction and Reasoning Corpus (ARC), ageneral AI benchmark, prompted in the style of ASCII art. Surprisingly, patterncompletion proficiency can be partially retained even when the sequences areexpressed using tokens randomly sampled from the vocabulary. These resultssuggest that without any additional training, LLMs can serve as generalsequence modelers, driven by in-context learning. In this work, we investigatehow these zero-shot capabilities may be applied to problems in robotics -- fromextrapolating sequences of numbers that represent states over time to completesimple motions, to least-to-most prompting of reward-conditioned trajectoriesthat can discover and represent closed-loop policies (e.g., a stabilizingcontroller for CartPole). While difficult to deploy today for real systems dueto latency, context size limitations, and compute costs, the approach of usingLLMs to drive low-level control may provide an exciting glimpse into how thepatterns among words could be transferred to actions.", "title": "large language models as general pattern machines", "url": "http://arxiv.org/pdf/2307.04721v2.pdf", "tokenized_text": "observe pre trained large_language large language llms capable completing complex token sequences arbitrary generated probabilistic context free spatial patterns found abstraction reasoning corpus arc ai benchmark prompted style art surprisingly patterncompletion proficiency partially sequences tokens randomly sampled vocabulary additional training llms serve driven context_learning context learning work zero shot capabilities applied problems robotics sequences numbers represent states time motions reward conditioned discover represent closed loop policies e.g. difficult deploy today real systems dueto latency context size limitations compute costs approach drive low level control provide exciting glimpse words transferred actions"}
{"id": "nan", "abstract": "  Zero-shot text-to-speech aims at synthesizing voices with unseen speechprompts. Previous large-scale multispeaker TTS models have successfullyachieved this goal with an enrolled recording within 10 seconds. However, mostof them are designed to utilize only short speech prompts. The limitedinformation in short speech prompts significantly hinders the performance offine-grained identity imitation. In this paper, we introduce Mega-TTS 2, ageneric zero-shot multispeaker TTS model that is capable of synthesizing speechfor unseen speakers with arbitrary-length prompts. Specifically, we 1) design amulti-reference timbre encoder to extract timbre information from multiplereference speeches; 2) and train a prosody language model with arbitrary-lengthspeech prompts; With these designs, our model is suitable for prompts ofdifferent lengths, which extends the upper bound of speech quality forzero-shot text-to-speech. Besides arbitrary-length prompts, we introducearbitrary-source prompts, which leverages the probabilities derived frommultiple P-LLM outputs to produce expressive and controlled prosody.Furthermore, we propose a phoneme-level auto-regressive duration model tointroduce in-context learning capabilities to duration modeling. Experimentsdemonstrate that our method could not only synthesize identity-preservingspeech with a short prompt of an unseen speaker but also achieve improvedperformance with longer speech prompts. Audio samples can be found inhttps://mega-tts.github.io/mega2_demo/.", "title": "megatts 2 zeroshot texttospeech with arbitrary length speech prompts", "url": "http://arxiv.org/pdf/2307.07218v2.pdf", "tokenized_text": "zero shot text speech aims synthesizing voices unseen previous large scale tts goal recording 10 seconds mostof designed utilize short speech short speech significantly hinders performance offine grained identity imitation paper introduce zero shot tts capable synthesizing unseen speakers arbitrary length specifically design amulti reference timbre encoder extract timbre information speeches train prosody language_model language arbitrary designs suitable ofdifferent lengths extends upper bound speech quality forzero shot text speech arbitrary length source leverages probabilities derived llm outputs produce expressive controlled prosody furthermore propose phoneme level auto regressive duration tointroduce context_learning context learning capabilities duration modeling experimentsdemonstrate method synthesize identity short unseen speaker achieve longer speech audio samples found"}
{"id": "nan", "abstract": "  Despite the superior performance, Large Language Models~(LLMs) requiresignificant computational resources for deployment and use. To overcome thisissue, quantization methods have been widely applied to reduce the memoryfootprint of LLMs as well as increasing the inference rate. However, a majorchallenge is that low-bit quantization methods often lead to performancedegradation. It is important to understand how quantization impacts thecapacity of LLMs. Different from previous studies focused on overallperformance, this work aims to investigate the impact of quantization on\\emph{emergent abilities}, which are important characteristics that distinguishLLMs from small language models. Specially, we examine the abilities ofin-context learning, chain-of-thought reasoning, and instruction-following inquantized LLMs. Our empirical experiments show that these emergent abilitiesstill exist in 4-bit quantization models, while 2-bit models encounter severeperformance degradation on the test of these abilities. To improve theperformance of low-bit models, we conduct two special experiments: (1)fine-gained impact analysis that studies which components (or substructures)are more sensitive to quantization, and (2) performance compensation throughmodel fine-tuning. Our work derives a series of important findings tounderstand the impact of quantization on emergent abilities, and sheds lightson the possibilities of extremely low-bit quantization for LLMs.", "title": "do emergent abilities exist in quantized large language models an empirical study", "url": "http://arxiv.org/pdf/2307.08072v2.pdf", "tokenized_text": "despite superior_performance superior performance large_language large language models~(llms requiresignificant computational resources deployment use overcome thisissue quantization methods widely applied reduce llms increasing inference rate low bit quantization methods lead important understand quantization impacts llms different previous studies focused overallperformance work aims investigate impact quantization abilities important characteristics small language_models language specially examine abilities ofin context_learning context learning chain thought reasoning instruction following llms empirical experiments emergent exist bit quantization bit encounter degradation test abilities improve theperformance low bit conduct special experiments gained impact analysis studies components sensitive quantization performance fine tuning work derives series important findings tounderstand impact quantization emergent abilities sheds possibilities extremely low bit quantization llms"}
{"id": "nan", "abstract": "  The derivation of mathematical results in specialised fields, using LargeLanguage Models (LLMs), is an emerging research direction that can helpidentify models' limitations, and potentially support mathematical discovery.In this paper, we leverage a symbolic engine to generate derivations ofequations at scale, and investigate the capabilities of LLMs when deriving goalequations from premises. Specifically, we employ in-context learning for GPTand fine-tune a range of T5 models to compare the robustness and generalisationof pre-training strategies to specialised models. Empirical results show thatfine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static andout-of-distribution test sets in conventional scores. However, an in-depthanalysis reveals that the fine-tuned models are more sensitive to perturbationsinvolving unseen symbols and (to a lesser extent) changes to equationstructure. In addition, we analyse 1.7K equations, and over 200 derivations, tohighlight common reasoning errors such as the inclusion of incorrect,irrelevant, and redundant equations. Finally, we explore the suitability ofexisting metrics for evaluating mathematical derivations and find evidencethat, while they can capture general properties such as sensitivity toperturbations, they fail to highlight fine-grained reasoning errors andessential differences between models. Overall, this work demonstrates thattraining models on synthetic data may improve their math capabilities beyondmuch larger LLMs, but current metrics are not appropriately assessing thequality of generated mathematical text.", "title": "generating mathematical derivations with large language models", "url": "http://arxiv.org/pdf/2307.09998v3.pdf", "tokenized_text": "derivation mathematical results fields largelanguage_models largelanguage llms emerging research direction limitations potentially support mathematical discovery paper leverage symbolic engine generate scale investigate capabilities llms deriving premises specifically employ context_learning context learning fine tune range t5 compare robustness pre training strategies empirical results tuned flan t5 large outperforms gpt static andout distribution test sets conventional scores reveals fine tuned sensitive unseen symbols lesser extent changes addition analyse 1.7 equations 200 common reasoning errors inclusion incorrect irrelevant redundant equations finally explore suitability ofexisting metrics evaluating mathematical find capture general properties sensitivity toperturbations fail highlight fine grained reasoning errors differences overall work demonstrates synthetic data improve math capabilities larger llms current metrics appropriately assessing thequality generated mathematical text"}
{"id": "nan", "abstract": "  Low-rank adaptations (LoRA) are often employed to fine-tune large languagemodels (LLMs) for new tasks. This paper investigates LoRA composability forcross-task generalization and introduces LoraHub, a strategic framework devisedfor the purposive assembly of LoRA modules trained on diverse given tasks, withthe objective of achieving adaptable performance on unseen tasks. With just afew examples from a novel task, LoraHub enables the fluid combination ofmultiple LoRA modules, eradicating the need for human expertise. Notably, thecomposition requires neither additional model parameters nor gradients. Ourempirical results, derived from the Big-Bench Hard (BBH) benchmark, suggestthat LoraHub can effectively mimic the performance of in-context learning infew-shot scenarios, excluding the necessity of in-context examples alongsideeach inference input. A significant contribution of our research is thefostering of a community for LoRA, where users can share their trained LoRAmodules, thereby facilitating their application to new tasks. We anticipatethis resource will widen access to and spur advancements in generalintelligence as well as LLMs in production. Code will be available athttps://github.com/sail-sg/lorahub.", "title": "lorahub efficient crosstask generalization via dynamic lora composition", "url": "http://arxiv.org/pdf/2307.13269v1.pdf", "tokenized_text": "low rank adaptations lora employed fine tune large_languagemodels large languagemodels llms new tasks paper investigates lora task generalization introduces strategic framework assembly lora modules trained diverse given tasks withthe objective achieving adaptable performance unseen tasks afew examples novel task enables fluid combination ofmultiple lora modules need human expertise notably requires additional parameters gradients ourempirical results derived big-bench_hard big-bench hard bbh benchmark suggestthat effectively mimic performance context_learning context learning infew shot scenarios excluding necessity context_examples context examples inference input significant contribution research community lora users share trained facilitating application new tasks resource access spur advancements generalintelligence llms production code available"}
{"id": "nan", "abstract": "  In the text-to-image generation field, recent remarkable progress in StableDiffusion makes it possible to generate rich kinds of novel photorealisticimages. However, current models still face misalignment issues (e.g.,problematic spatial relation understanding and numeration failure) in complexnatural scenes, which impedes the high-faithfulness text-to-image generation.Although recent efforts have been made to improve controllability by givingfine-grained guidance (e.g., sketch and scribbles), this issue has not beenfundamentally tackled since users have to provide such guidance informationmanually. In this work, we strive to synthesize high-fidelity images that aresemantically aligned with a given textual prompt without any guidance. Towardthis end, we propose a coarse-to-fine paradigm to achieve layout planning andimage generation. Concretely, we first generate the coarse-grained layoutconditioned on a given textual prompt via in-context learning based on LargeLanguage Models. Afterward, we propose a fine-grained object-interactiondiffusion method to synthesize high-faithfulness images conditioned on theprompt and the automatically generated layout. Extensive experimentsdemonstrate that our proposed method outperforms the state-of-the-art models interms of layout and image generation. Our code and settings are available athttps://layoutllm-t2i.github.io.", "title": "layoutllmt2i eliciting layout guidance from llm for texttoimage generation", "url": "http://arxiv.org/pdf/2308.05095v2.pdf", "tokenized_text": "text image_generation image generation field recent remarkable progress makes possible generate rich kinds novel current face misalignment issues e.g. spatial relation understanding failure scenes impedes high faithfulness text image_generation image generation recent efforts improve controllability grained guidance e.g. issue tackled users provide guidance work strive synthesize high fidelity images aligned given textual guidance end propose coarse fine paradigm achieve layout planning generation concretely generate coarse grained given textual context_learning context learning based largelanguage_models largelanguage afterward propose fine grained object method synthesize high faithfulness images conditioned theprompt automatically generated layout extensive experimentsdemonstrate proposed_method proposed method outperforms state art interms layout image_generation image generation code settings available"}
{"id": "nan", "abstract": "  Although audio generation shares commonalities across different types ofaudio, such as speech, music, and sound effects, designing models for each typerequires careful consideration of specific objectives and biases that cansignificantly differ from those of other types. To bring us closer to a unifiedperspective of audio generation, this paper proposes a framework that utilizesthe same learning method for speech, music, and sound effect generation. Ourframework introduces a general representation of audio, called \"language ofaudio\" (LOA). Any audio can be translated into LOA based on AudioMAE, aself-supervised pre-trained representation learning model. In the generationprocess, we translate any modalities into LOA by using a GPT-2 model, and weperform self-supervised audio generation learning with a latent diffusion modelconditioned on LOA. The proposed framework naturally brings advantages such asin-context learning abilities and reusable self-supervised pretrained AudioMAEand latent diffusion models. Experiments on the major benchmarks oftext-to-audio, text-to-music, and text-to-speech demonstrate state-of-the-artor competitive performance against previous approaches. Our code, pretrainedmodel, and demo are available at https://audioldm.github.io/audioldm2.", "title": "audioldm 2 learning holistic audio generation with selfsupervised pretraining", "url": "http://arxiv.org/pdf/2308.05734v2.pdf", "tokenized_text": "audio generation commonalities different types speech music sound effects designing careful consideration specific objectives biases cansignificantly differ types bring closer audio generation paper_proposes paper proposes framework learning method speech music sound effect generation ourframework introduces general representation audio called language audio translated based aself supervised pre trained representation learning generationprocess translate modalities gpt-2 weperform self supervised audio generation learning latent diffusion proposed framework naturally brings advantages asin context_learning context learning abilities reusable self supervised pretrained latent diffusion experiments major benchmarks oftext audio text music text speech demonstrate state competitive_performance competitive performance previous approaches code demo available"}
{"id": "nan", "abstract": "  Data contamination, i.e., the presence of test data from downstream tasks inthe training data of large language models (LLMs), is a potential major issuein measuring LLMs' real effectiveness on other tasks. We propose astraightforward yet effective method for identifying data contamination withinLLMs. At its core, our approach starts by identifying potential contaminationat the instance level; using this information, our approach then assesses widercontamination at the partition level. To estimate contamination of individualinstances, we employ \"guided instruction:\" a prompt consisting of the datasetname, partition type, and the random-length initial segment of a referenceinstance, asking the LLM to complete it. An instance is flagged as contaminatedif the LLM's output either exactly or nearly matches the latter segment of thereference. To understand if an entire partition is contaminated, we propose twoideas. The first idea marks a dataset partition as contaminated if the averageoverlap score with the reference instances (as measured by ROUGE-L or BLEURT)is statistically significantly better with the completions from guidedinstruction compared to a \"general instruction\" that does not include thedataset and partition name. The second idea marks a dataset partition ascontaminated if a classifier based on GPT-4 with few-shot in-context learningprompt marks multiple generated completions as exact/near-exact matches of thecorresponding reference instances. Our best method achieves an accuracy between92% and 100% in detecting if an LLM is contaminated with seven datasets,containing train and test/validation partitions, when contrasted with manualevaluation by human experts. Further, our findings indicate that GPT-4 iscontaminated with AG News, WNLI, and XSum datasets.", "title": "time travel in llms tracing data contamination in large language models", "url": "http://arxiv.org/pdf/2308.08493v2.pdf", "tokenized_text": "data contamination i.e. presence test data downstream_tasks downstream tasks inthe training_data training data large_language large language llms potential major measuring llms real effectiveness tasks propose effective method identifying data contamination core approach starts identifying potential instance level information approach assesses partition level estimate contamination employ guided instruction consisting partition type random length initial segment asking llm complete instance llm output exactly nearly matches segment understand entire partition contaminated propose idea marks dataset partition contaminated score reference instances measured rouge statistically significantly better completions compared general instruction include thedataset partition second idea marks dataset partition classifier based gpt-4 shot context marks multiple generated completions exact near exact matches thecorresponding reference instances best method_achieves method achieves accuracy 100 detecting llm contaminated seven datasets containing train test validation partitions contrasted human experts findings indicate gpt-4 news datasets"}
{"id": "nan", "abstract": "  Large Language Models(LLMs) have been attracting attention due to a abilitycalled in-context learning(ICL). ICL, without updating the parameters of a LLM,it is possible to achieve highly accurate inference based on rules ``in thecontext'' by merely inputting a training data into the prompt. Although ICL isa developing field with many unanswered questions, LLMs themselves serves as ainference model, seemingly realizing inference without explicitly indicate``inductive bias''. On the other hand, a code generation is also a highlightedapplication of LLMs. The accuracy of code generation has dramatically improved,enabling even non-engineers to generate code to perform the desired tasks bycrafting appropriate prompts. In this paper, we propose a novel ``learning''method called an ``Inductive-Bias Learning (IBL)'', which combines thetechniques of ICL and code generation. An idea of IBL is straightforward. LikeICL, IBL inputs a training data into the prompt and outputs a code with anecessary structure for inference (we referred to as ``Code Model'') from a``contextual understanding''. Despite being a seemingly simple approach, IBLencompasses both a ``property of inference without explicit inductive bias''inherent in ICL and a ``readability and explainability'' of the codegeneration. Surprisingly, generated Code Models have been found to achievepredictive accuracy comparable to, and in some cases surpassing, ICL andrepresentative machine learning models. Our IBL code is open source:https://github.com/fuyu-quant/IBLM", "title": "inductivebias learning generating code models with large language model", "url": "http://arxiv.org/pdf/2308.09890v1.pdf", "tokenized_text": "large_language large language models(llms attracting attention context learning(icl icl updating parameters llm possible achieve highly accurate inference based rules thecontext merely training_data training data icl isa developing field unanswered questions llms serves seemingly realizing inference explicitly bias hand code_generation code generation llms accuracy code_generation code generation dramatically improved enabling non engineers generate code perform desired tasks appropriate paper propose_a_novel propose novel called inductive bias learning combines icl code_generation code generation idea straightforward inputs training_data training data outputs code structure inference referred code understanding despite seemingly simple approach property inference explicit inductive icl readability explainability codegeneration surprisingly generated code found accuracy comparable cases surpassing icl machine_learning machine learning code open_source open source https://github.com quant"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) possess impressive capabilities to generatemeaningful code snippets given natural language intents in zero-shot, i.e.,without the need for specific fine-tuning. In the perspective of unleashingtheir full potential, prior work has demonstrated the benefits of fine-tuningthe models to task-specific data. However, fine-tuning process demands heavycomputational costs and is intractable when resources are scarce, especiallyfor models with billions of parameters. In light of these challenges, previousstudies explored In-Context Learning (ICL) as an effective strategy to generatecontextually appropriate code without fine-tuning. However, it operates atinference time and does not involve learning task-specific parameters,potentially limiting the model's performance on downstream tasks. In thiscontext, we foresee that Parameter-Efficient Fine-Tuning (PEFT) techniquescarry a high potential for efficiently specializing LLMs to task-specific data.In this paper, we deliver a comprehensive study of LLMs with the impact of PEFTtechniques under the automated code generation scenario. Our experimentalresults reveal the superiority and potential of such techniques over ICL on awide range of LLMs in reducing the computational burden and improvingperformance. Therefore, the study opens opportunities for broader applicationsof PEFT in software engineering scenarios.", "title": "exploring parameterefficient finetuning techniques for code generation with large language models", "url": "http://arxiv.org/pdf/2308.10462v1.pdf", "tokenized_text": "large_language large language llms possess impressive capabilities code snippets given natural_language natural language intents zero shot i.e. need specific fine tuning perspective potential prior_work prior work demonstrated benefits fine tuningthe task specific data fine tuning process demands costs intractable resources scarce especiallyfor billions parameters light challenges explored context_learning context learning icl effective strategy appropriate code fine tuning operates time involve learning task specific parameters potentially limiting performance downstream_tasks downstream tasks foresee parameter efficient fine tuning peft high potential efficiently specializing llms task specific data paper deliver comprehensive study llms impact automated code_generation code generation scenario experimentalresults reveal superiority potential techniques icl awide range llms reducing computational burden improvingperformance study opens opportunities broader peft software engineering scenarios"}
{"id": "nan", "abstract": "  In the wake of the explosive growth of machine learning (ML) usage,particularly within the context of emerging Large Language Models (LLMs),comprehending the semantic significance rooted in their internal workings iscrucial. While causal analyses focus on defining semantics and itsquantification, the gradient-based approach is central to explainable AI (XAI),tackling the interpretation of the black box. By synergizing these approaches,the exploration of how a model's internal mechanisms illuminate its causaleffect has become integral for evidence-based decision-making. A parallel lineof research has revealed that intersectionality - the combinatory impact ofmultiple demographics of an individual - can be structured in the form of anAveraged Treatment Effect (ATE). Initially, this study illustrates that thehateful memes detection problem can be formulated as an ATE, assisted by theprinciples of intersectionality, and that a modality-wise summarization ofgradient-based attention attribution scores can delineate the distinctbehaviors of three Transformerbased models concerning ATE. Subsequently, weshow that the latest LLM LLaMA2 has the ability to disentangle theintersectional nature of memes detection in an in-context learning setting,with their mechanistic properties elucidated via meta-gradient, a secondaryform of gradient. In conclusion, this research contributes to the ongoingdialogue surrounding XAI and the multifaceted nature of ML models.", "title": "causal intersectionality and dual form of gradient descent for multimodal analysis a case study on hateful memes", "url": "http://arxiv.org/pdf/2308.11585v1.pdf", "tokenized_text": "explosive growth machine_learning machine learning ml usage particularly context emerging large_language large language semantic significance rooted internal workings iscrucial causal analyses focus defining semantics gradient based approach central explainable ai interpretation black_box black box approaches exploration internal mechanisms integral evidence based decision making parallel research revealed impact ofmultiple demographics individual structured form treatment effect initially study illustrates detection problem formulated assisted theprinciples modality wise summarization ofgradient based attention attribution scores concerning subsequently weshow latest llm llama2 ability disentangle nature detection context_learning context learning setting mechanistic properties meta gradient gradient conclusion research contributes surrounding xai multifaceted nature ml"}
{"id": "nan", "abstract": "  Text-to-video (T2V) synthesis has gained increasing attention in thecommunity, in which the recently emerged diffusion models (DMs) havepromisingly shown stronger performance than the past approaches. While existingstate-of-the-art DMs are competent to achieve high-resolution video generation,they may largely suffer from key limitations (e.g., action occurrencedisorders, crude video motions) with respect to the intricate temporal dynamicsmodeling, one of the crux of video synthesis. In this work, we investigatestrengthening the awareness of video dynamics for DMs, for high-quality T2Vgeneration. Inspired by human intuition, we design an innovative dynamic scenemanager (dubbed as Dysen) module, which includes (step-1) extracting from inputtext the key actions with proper time-order arrangement, (step-2) transformingthe action schedules into the dynamic scene graph (DSG) representations, and(step-3) enriching the scenes in the DSG with sufficient and reasonabledetails. Taking advantage of the existing powerful LLMs (e.g., ChatGPT) viain-context learning, Dysen realizes (nearly) human-level temporal dynamicsunderstanding. Finally, the resulting video DSG with rich action scene detailsis encoded as fine-grained spatio-temporal features, integrated into thebackbone T2V DM for video generating. Experiments on popular T2V datasetssuggest that our framework consistently outperforms prior arts with significantmargins, especially in the scenario with complex actions. Project page athttps://haofei.vip/Dysen-VDM", "title": "empowering dynamicsaware texttovideo diffusion with large language models", "url": "http://arxiv.org/pdf/2308.13812v1.pdf", "tokenized_text": "text video synthesis gained increasing attention thecommunity recently emerged diffusion shown stronger performance past approaches existingstate art competent achieve high resolution video generation largely suffer key limitations e.g. action video motions respect intricate temporal video synthesis work awareness video dynamics high quality inspired human intuition design innovative dynamic dubbed module includes extracting key actions proper time order action dynamic scene graph representations enriching scenes sufficient taking advantage existing powerful llms e.g. chatgpt viain context_learning context learning nearly human level temporal finally resulting video rich action scene encoded fine grained temporal features integrated video generating experiments popular framework consistently_outperforms consistently outperforms prior arts especially scenario complex actions project page"}
{"id": "nan", "abstract": "  Every major technical invention resurfaces the dual-use dilemma -- the newtechnology has the potential to be used for good as well as for harm.Generative AI (GenAI) techniques, such as large language models (LLMs) anddiffusion models, have shown remarkable capabilities (e.g., in-contextlearning, code-completion, and text-to-image generation and editing). However,GenAI can be used just as well by attackers to generate new attacks andincrease the velocity and efficacy of existing attacks.  This paper reports the findings of a workshop held at Google (co-organized byStanford University and the University of Wisconsin-Madison) on the dual-usedilemma posed by GenAI. This paper is not meant to be comprehensive, but israther an attempt to synthesize some of the interesting findings from theworkshop. We discuss short-term and long-term goals for the community on thistopic. We hope this paper provides both a launching point for a discussion onthis important topic as well as interesting problems that the researchcommunity can work to address.", "title": "identifying and mitigating the security risks of generative ai", "url": "http://arxiv.org/pdf/2308.14840v3.pdf", "tokenized_text": "major technical invention dual use dilemma potential good harm generative_ai generative ai genai techniques large_language large language llms shown remarkable_capabilities remarkable capabilities e.g. contextlearning code completion text image_generation image generation editing genai attackers generate new attacks efficacy existing attacks paper reports findings workshop held google co university university dual posed genai paper meant comprehensive attempt synthesize interesting findings discuss short term long term goals community hope paper provides point discussion onthis important topic interesting problems researchcommunity work address"}
{"id": "nan", "abstract": "  Large Vision-Language Models (LVLMs) such as MiniGPT-4 and LLaVA havedemonstrated the capability of understanding images and achieved remarkableperformance in various visual tasks. Despite their strong abilities inrecognizing common objects due to extensive training datasets, they lackspecific domain knowledge and have a weaker understanding of localized detailswithin objects, which hinders their effectiveness in the Industrial AnomalyDetection (IAD) task. On the other hand, most existing IAD methods only provideanomaly scores and necessitate the manual setting of thresholds to distinguishbetween normal and abnormal samples, which restricts their practicalimplementation. In this paper, we explore the utilization of LVLM to addressthe IAD problem and propose AnomalyGPT, a novel IAD approach based on LVLM. Wegenerate training data by simulating anomalous images and producingcorresponding textual descriptions for each image. We also employ an imagedecoder to provide fine-grained semantic and design a prompt learner tofine-tune the LVLM using prompt embeddings. Our AnomalyGPT eliminates the needfor manual threshold adjustments, thus directly assesses the presence andlocations of anomalies. Additionally, AnomalyGPT supports multi-turn dialoguesand exhibits impressive few-shot in-context learning capabilities. With onlyone normal shot, AnomalyGPT achieves the state-of-the-art performance with anaccuracy of 86.1%, an image-level AUC of 94.1%, and a pixel-level AUC of 95.3%on the MVTec-AD dataset. Code is available athttps://github.com/CASIA-IVA-Lab/AnomalyGPT.", "title": "anomalygpt detecting industrial anomalies using large visionlanguage models", "url": "http://arxiv.org/pdf/2308.15366v3.pdf", "tokenized_text": "large vision language_models language llava havedemonstrated capability understanding images achieved visual tasks despite strong abilities common objects extensive training datasets domain knowledge weaker understanding objects hinders effectiveness industrial task hand existing methods scores necessitate manual setting thresholds normal abnormal samples restricts paper explore utilization addressthe problem propose novel approach based training_data training data simulating images textual descriptions image employ provide fine grained semantic design learner tofine tune embeddings eliminates needfor manual threshold adjustments directly assesses presence additionally supports multi turn exhibits impressive shot context_learning context learning capabilities onlyone normal shot achieves state art performance image level auc pixel level auc mvtec ad dataset code_is_available code available"}
{"id": "nan", "abstract": "  Business Process Management (BPM) is gaining increasing attention as it hasthe potential to cut costs while boosting output and quality. Business processdocument generation is a crucial stage in BPM. However, due to a shortage ofdatasets, data-driven deep learning techniques struggle to deliver the expectedresults. We propose an approach to transform Conditional Process Trees (CPTs)into Business Process Text Sketches (BPTSs) using Large Language Models (LLMs).The traditional prompting approach (Few-shot In-Context Learning) tries to getthe correct answer in one go, and it can find the pattern of transformingsimple CPTs into BPTSs, but for close-domain and CPTs with complex hierarchy,the traditional prompts perform weakly and with low correctness. We suggestusing this technique to break down a difficult CPT into a number of basic CPTsand then solve each one in turn, drawing inspiration from thedivide-and-conquer strategy. We chose 100 process trees with depths rangingfrom 2 to 5 at random, as well as CPTs with many nodes, many degrees ofselection, and cyclic nesting. Experiments show that our method can achieve acorrect rate of 93.42%, which is 45.17% better than traditional promptingmethods. Our proposed method provides a solution for business process documentgeneration in the absence of datasets, and secondly, it becomes potentiallypossible to provide a large number of datasets for the process model extraction(PME) domain.", "title": "business process text sketch automation generation using large language model", "url": "http://arxiv.org/pdf/2309.01071v1.pdf", "tokenized_text": "business_process_management business process management bpm gaining increasing attention potential costs boosting output quality business generation crucial stage bpm shortage data driven deep learning techniques struggle deliver propose approach transform conditional process trees business process text sketches large_language large language traditional approach shot context_learning context learning correct answer find pattern close domain complex hierarchy traditional perform weakly low correctness technique break difficult cpt number basic solve turn drawing inspiration conquer strategy chose 100 process trees depths rangingfrom random nodes degrees nesting experiments method achieve rate better traditional promptingmethods proposed_method proposed method provides solution business process absence datasets secondly provide large number datasets process domain"}
{"id": "nan", "abstract": "  We continue the investigation into the power of smaller Transformer-basedlanguage models as initiated by \\textbf{TinyStories} -- a 10 million parametermodel that can produce coherent English -- and the follow-up work on\\textbf{phi-1}, a 1.3 billion parameter model with Python coding performanceclose to the state-of-the-art. The latter work proposed to use existing LargeLanguage Models (LLMs) to generate ``textbook quality\" data as a way to enhancethe learning process compared to traditional web data. We follow the``Textbooks Are All You Need\" approach, focusing this time on common sensereasoning in natural language, and create a new 1.3 billion parameter modelnamed \\textbf{phi-1.5}, with performance on natural language tasks comparableto models 5x larger, and surpassing most non-frontier LLMs on more complexreasoning tasks such as grade-school mathematics and basic coding. Moregenerally, \\textbf{phi-1.5} exhibits many of the traits of much larger LLMs,both good -- such as the ability to ``think step by step\" or perform somerudimentary in-context learning -- and bad, including hallucinations and thepotential for toxic and biased generations -- encouragingly though, we areseeing improvement on that front thanks to the absence of web data. Weopen-source \\textbf{phi-1.5} to promote further research on these urgenttopics.", "title": "textbooks are all you need ii phi15 technical report", "url": "http://arxiv.org/pdf/2309.05463v1.pdf", "tokenized_text": "continue investigation power smaller transformer initiated 10 million parametermodel produce coherent english follow work 1.3 billion parameter python coding state art work proposed use existing largelanguage_models largelanguage llms generate textbook quality data way enhancethe learning process compared traditional web data follow need approach focusing time common natural_language natural language create new 1.3 billion parameter performance natural_language natural language tasks 5x larger surpassing non frontier llms tasks grade school mathematics basic coding moregenerally exhibits traits larger llms good ability think step_by_step step step perform context_learning context learning bad including hallucinations thepotential toxic biased generations encouragingly improvement thanks absence web data weopen source promote research"}
{"id": "nan", "abstract": "  Transformers have become the dominant model in deep learning, but the reasonfor their superior performance is poorly understood. Here, we hypothesize thatthe strong performance of Transformers stems from an architectural bias towardsmesa-optimization, a learned process running within the forward pass of a modelconsisting of the following two steps: (i) the construction of an internallearning objective, and (ii) its corresponding solution found throughoptimization. To test this hypothesis, we reverse-engineer a series ofautoregressive Transformers trained on simple sequence modeling tasks,uncovering underlying gradient-based mesa-optimization algorithms driving thegeneration of predictions. Moreover, we show that the learned forward-passoptimization algorithm can be immediately repurposed to solve supervisedfew-shot tasks, suggesting that mesa-optimization might underlie the in-contextlearning capabilities of large language models. Finally, we propose a novelself-attention layer, the mesa-layer, that explicitly and efficiently solvesoptimization problems specified in context. We find that this layer can lead toimproved performance in synthetic and preliminary language modelingexperiments, adding weight to our hypothesis that mesa-optimization is animportant operation hidden within the weights of trained Transformers.", "title": "uncovering mesaoptimization algorithms in transformers", "url": "http://arxiv.org/pdf/2309.05858v1.pdf", "tokenized_text": "transformers dominant deep learning superior_performance superior performance poorly understood hypothesize thatthe strong performance transformers stems architectural bias optimization learned process running forward pass following steps construction objective ii corresponding solution found test hypothesis reverse engineer series transformers trained simple sequence modeling tasks uncovering underlying gradient based optimization algorithms driving thegeneration predictions learned forward algorithm immediately solve shot tasks suggesting optimization underlie contextlearning capabilities large_language large language finally propose attention layer layer explicitly efficiently problems specified context find layer lead performance synthetic preliminary language adding weight hypothesis optimization animportant operation hidden weights trained transformers"}
{"id": "nan", "abstract": "  Sentence Representation Learning (SRL) is a fundamental task in NaturalLanguage Processing (NLP), with Contrastive learning of Sentence Embeddings(CSE) as the mainstream technique due to its superior performance. Anintriguing phenomenon in CSE is the significant performance gap betweensupervised and unsupervised methods, even when their sentence encoder and lossfunction are the same. Previous works attribute this performance gap todifferences in two representation properties (alignment and uniformity).However, alignment and uniformity only measure the results, which means theycannot answer \"What happens during the training process that leads to theperformance gap?\" and \"How can the performance gap be narrowed?\". In thispaper, we conduct empirical experiments to answer these \"What\" and \"How\"questions. We first answer the \"What\" question by thoroughly comparing thebehavior of supervised and unsupervised CSE during their respective trainingprocesses. From the comparison, We observe a significant difference in fittingdifficulty. Thus, we introduce a metric, called Fitting Difficulty Increment(FDI), to measure the fitting difficulty gap between the evaluation dataset andthe held-out training dataset, and use the metric to answer the \"What\"question. Then, based on the insights gained from the \"What\" question, wetackle the \"How\" question by increasing the fitting difficulty of the trainingdataset. We achieve this by leveraging the In-Context Learning (ICL) capabilityof the Large Language Model (LLM) to generate data that simulates complexpatterns. By utilizing the hierarchical patterns in the LLM-generated data, weeffectively narrow the gap between supervised and unsupervised CSE.", "title": "narrowing the gap between supervised and unsupervised sentence representation learning with large language model", "url": "http://arxiv.org/pdf/2309.06453v1.pdf", "tokenized_text": "sentence representation learning fundamental task naturallanguage_processing naturallanguage processing nlp contrastive_learning contrastive learning sentence mainstream technique superior_performance superior performance phenomenon significant performance gap unsupervised methods sentence encoder previous works attribute performance gap representation properties alignment alignment measure results means answer happens training process leads theperformance gap performance gap thispaper conduct empirical experiments answer answer question thoroughly comparing thebehavior supervised unsupervised respective comparison observe significant difference introduce metric called fitting difficulty measure fitting difficulty gap evaluation dataset andthe held training dataset use metric answer based insights gained question wetackle question increasing fitting difficulty achieve leveraging context_learning context learning icl capabilityof large_language large language llm generate data simulates utilizing hierarchical patterns llm generated data narrow gap supervised unsupervised"}
{"id": "nan", "abstract": "  Fine-tuning (via methods such as instruction-tuning or reinforcement learningfrom human feedback) is a crucial step in training language models to robustlycarry out tasks of interest. However, we lack a systematic understanding of theeffects of fine-tuning, particularly on tasks outside the narrow fine-tuningdistribution. In a simplified scenario, we demonstrate that improvingperformance on tasks within the fine-tuning data distribution comes at theexpense of suppressing model capabilities on other tasks. This degradation isespecially pronounced for tasks \"closest\" to the fine-tuning distribution. Wehypothesize that language models implicitly infer the task of the promptcorresponds, and the fine-tuning process predominantly skews this taskinference towards tasks in the fine-tuning distribution. To test thishypothesis, we propose Conjugate Prompting to see if we can recover pretrainedcapabilities. Conjugate prompting artificially makes the task look farther fromthe fine-tuning distribution while requiring the same capability. We find thatconjugate prompting systematically recovers some of the pretrainingcapabilities on our synthetic setup. We then apply conjugate prompting toreal-world LLMs using the observation that fine-tuning distributions aretypically heavily skewed towards English. We find that simply translating theprompts to different languages can cause the fine-tuned models to respond liketheir pretrained counterparts instead. This allows us to recover the in-contextlearning abilities lost via instruction tuning, and more concerningly, torecover harmful content generation suppressed by safety fine-tuning in chatbotslike ChatGPT.", "title": "understanding catastrophic forgetting in language models via implicit inference", "url": "http://arxiv.org/pdf/2309.10105v1.pdf", "tokenized_text": "fine tuning methods instruction tuning reinforcement human feedback crucial step training language_models language tasks interest lack systematic understanding theeffects fine tuning particularly tasks outside narrow fine simplified scenario demonstrate improvingperformance tasks fine tuning data distribution comes capabilities tasks degradation isespecially pronounced tasks closest fine tuning distribution language_models language implicitly infer task fine tuning process predominantly tasks fine tuning distribution test propose recover artificially makes task look fromthe fine tuning distribution requiring capability find systematically recovers synthetic setup apply world llms observation fine tuning distributions heavily skewed english find simply translating theprompts different languages cause fine tuned respond pretrained counterparts instead allows recover contextlearning abilities instruction_tuning instruction tuning harmful content generation suppressed safety fine tuning chatgpt"}
{"id": "nan", "abstract": "  The remarkable capabilities and intricate nature of Artificial Intelligence(AI) have dramatically escalated the imperative for specialized AIaccelerators. Nonetheless, designing these accelerators for various AIworkloads remains both labor- and time-intensive. While existing designexploration and automation tools can partially alleviate the need for extensivehuman involvement, they still demand substantial hardware expertise, posing abarrier to non-experts and stifling AI accelerator development. Motivated bythe astonishing potential of large language models (LLMs) for generatinghigh-quality content in response to human language instructions, we embark onthis work to examine the possibility of harnessing LLMs to automate AIaccelerator design. Through this endeavor, we develop GPT4AIGChip, a frameworkintended to democratize AI accelerator design by leveraging human naturallanguages instead of domain-specific languages. Specifically, we first performan in-depth investigation into LLMs' limitations and capabilities for AIaccelerator design, thus aiding our understanding of our current position andgarnering insights into LLM-powered automated AI accelerator design.Furthermore, drawing inspiration from the above insights, we develop aframework called GPT4AIGChip, which features an automated demo-augmentedprompt-generation pipeline utilizing in-context learning to guide LLMs towardscreating high-quality AI accelerator design. To our knowledge, this work is thefirst to demonstrate an effective pipeline for LLM-powered automated AIaccelerator generation. Accordingly, we anticipate that our insights andframework can serve as a catalyst for innovations in next-generationLLM-powered design automation tools.", "title": "gpt4aigchip towards nextgeneration ai accelerator design automation via large language models", "url": "http://arxiv.org/pdf/2309.10730v1.pdf", "tokenized_text": "remarkable_capabilities remarkable capabilities intricate nature artificial dramatically imperative specialized nonetheless designing remains time intensive existing automation tools partially alleviate need involvement demand substantial hardware expertise posing non experts ai development motivated bythe astonishing potential large_language large language llms generatinghigh quality content response human language instructions onthis work examine possibility harnessing llms automate design endeavor develop democratize ai design leveraging human instead domain specific languages specifically depth investigation llms limitations capabilities design aiding understanding current position insights llm powered automated ai design furthermore drawing inspiration insights develop called features automated demo generation pipeline utilizing context_learning context learning guide llms high quality ai design knowledge work thefirst demonstrate effective pipeline llm powered automated generation accordingly anticipate insights serve catalyst innovations powered design automation tools"}
{"id": "nan", "abstract": "  Large language models (LLMs) can perform impressive feats with in-contextlearning or lightweight finetuning. It is natural to wonder how well thesemodels adapt to genuinely new tasks, but how does one find tasks that areunseen in internet-scale training sets? We turn to a field that is explicitlymotivated and bottlenecked by a scarcity of web data: low-resource languages.In this paper, we introduce MTOB (Machine Translation from One Book), abenchmark for learning to translate between English and Kalamang -- a languagewith less than 200 speakers and therefore virtually no presence on the web --using several hundred pages of field linguistics reference materials. This taskframing is novel in that it asks a model to learn a language from a singlehuman-readable book of grammar explanations, rather than a large mined corpusof in-domain data, more akin to L2 learning than L1 acquisition. We demonstratethat baselines using current LLMs are promising but fall short of humanperformance, achieving 44.7 chrF on Kalamang to English translation and 45.8chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by ahuman who learned Kalamang from the same reference materials. We hope that MTOBwill help measure LLM capabilities along a new dimension, and that the methodsdeveloped to solve it could help expand access to language technology forunderserved communities by leveraging qualitatively different kinds of datathan traditional machine translation.", "title": "a benchmark for learning to translate a new language from one grammar book", "url": "http://arxiv.org/pdf/2309.16575v1.pdf", "tokenized_text": "large_language large language llms perform impressive contextlearning lightweight finetuning natural thesemodels adapt new tasks find tasks internet scale training sets turn field bottlenecked scarcity web data low resource_languages resource languages paper introduce machine_translation machine translation book abenchmark learning translate english 200 speakers virtually presence web pages field linguistics reference materials novel asks learn language readable book grammar explanations large mined domain data akin learning acquisition demonstratethat baselines current llms promising fall short achieving english translation english translation compared 57.0 ahuman learned reference materials hope help measure llm capabilities new dimension solve help expand access language technology communities leveraging qualitatively different kinds traditional machine_translation machine translation"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) have recently been shown to be effective asautomatic evaluators with simple prompting and in-context learning. In thiswork, we assemble 15 LLMs of four different size ranges and evaluate theiroutput responses by preference ranking from the other LLMs as evaluators, suchas System Star is better than System Square. We then evaluate the quality ofranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators(CoBBLEr), a benchmark to measure six different cognitive biases in LLMevaluation outputs, such as the Egocentric bias where a model prefers to rankits own outputs highly in evaluation. We find that LLMs are biased text qualityevaluators, exhibiting strong indications on our bias benchmark (average of 40%of comparisons across all models) within each of their evaluations thatquestion their robustness as evaluators. Furthermore, we examine thecorrelation between human and machine preferences and calculate the averageRank-Biased Overlap (RBO) score to be 49.6%, indicating that machinepreferences are misaligned with humans. According to our findings, LLMs maystill be unable to be utilized for automatic annotation aligned with humanpreferences. Our project page is at: https://minnesotanlp.github.io/cobbler.", "title": "benchmarking cognitive biases in large language models as evaluators", "url": "http://arxiv.org/pdf/2309.17012v1.pdf", "tokenized_text": "large_language large language llms recently shown effective evaluators simple context_learning context learning thiswork assemble 15 llms different size evaluate responses preference ranking llms evaluators suchas system star better system square evaluate quality outputs introducing cognitive bias benchmark llms benchmark measure different cognitive biases outputs bias outputs highly evaluation find llms biased text exhibiting strong bias benchmark average comparisons evaluations robustness evaluators furthermore examine human machine preferences calculate biased overlap score indicating misaligned humans according findings llms unable utilized automatic annotation aligned project page"}
{"id": "nan", "abstract": "  Language model based text-to-speech (TTS) models, like VALL-E, have gainedattention for their outstanding in-context learning capability in zero-shotscenarios. Neural speech codec is a critical component of these models, whichcan convert speech into discrete token representations. However, excessivetoken sequences from the codec may negatively affect prediction accuracy andrestrict the progression of Language model based TTS models. To address thisissue, this paper proposes a novel neural speech codec with time-invariantcodes named TiCodec. By encoding and quantizing time-invariant information intoa separate code, TiCodec can reduce the amount of frame-level information thatneeds encoding, effectively decreasing the number of tokens as codes of speech.Furthermore, this paper introduces a time-invariant encoding consistency lossto enhance the consistency of time-invariant code within an utterance and forceit to capture more global information, which can benefit the zero-shot TTStask. Experimental results demonstrate that TiCodec can not only enhance thequality of reconstruction speech with fewer tokens but also increase thesimilarity and naturalness, as well as reduce the word error rate of thesynthesized speech by the TTS model.", "title": "fewertoken neural speech codec with timeinvariant codes", "url": "http://arxiv.org/pdf/2310.00014v1.pdf", "tokenized_text": "language_model language based text speech tts like vall outstanding context_learning context learning capability zero shotscenarios neural speech codec critical component whichcan convert speech discrete token representations sequences codec negatively affect prediction accuracy language_model language based tts address thisissue paper_proposes paper proposes novel neural speech codec time named encoding quantizing time invariant information separate code reduce frame level information encoding effectively decreasing number tokens codes speech furthermore paper introduces time invariant encoding consistency enhance consistency time invariant code utterance capture global information benefit zero shot experimental_results experimental results demonstrate enhance thequality reconstruction speech fewer tokens increase thesimilarity reduce word error rate speech tts"}
{"id": "nan", "abstract": "  Table Question Answering (TQA) presents a substantial challenge at theintersection of natural language processing and data analytics. This taskinvolves answering natural language (NL) questions on top of tabular data,demanding proficiency in logical reasoning, understanding of data semantics,and fundamental analytical capabilities. Due to its significance, a substantialvolume of research has been dedicated to exploring a wide range of strategiesaimed at tackling this challenge including approaches that leverage LargeLanguage Models (LLMs) through in-context learning or Chain-of-Thought (CoT)prompting as well as approaches that train and fine-tune custom models.  Nonetheless, a conspicuous gap exists in the research landscape, where thereis limited exploration of how innovative foundational research, whichintegrates incremental reasoning with external tools in the context of LLMs, asexemplified by the ReAct paradigm, could potentially bring advantages to theTQA task. In this paper, we aim to fill this gap, by introducing ReAcTable(ReAct for Table Question Answering tasks), a framework inspired by the ReActparadigm that is carefully enhanced to address the challenges uniquelyappearing in TQA tasks such as interpreting complex data semantics, dealingwith errors generated by inconsistent data and generating intricate datatransformations. ReAcTable relies on external tools such as SQL and Python codeexecutors, to progressively enhance the data by generating intermediate datarepresentations, ultimately transforming it into a more accessible format foranswering the questions with greater ease. We demonstrate that ReAcTableachieves remarkable performance even when compared to fine-tuned approaches. Inparticular, it outperforms the best prior result on the WikiTQ benchmark,achieving an accuracy of 68.0% without requiring training a new model orfine-tuning.", "title": "reactable enhancing react for table question answering", "url": "http://arxiv.org/pdf/2310.00815v1.pdf", "tokenized_text": "table question_answering question answering presents substantial challenge natural_language natural language processing data analytics answering natural_language natural language nl questions tabular data demanding proficiency logical reasoning understanding data semantics fundamental analytical capabilities significance research dedicated exploring wide_range wide range tackling challenge including approaches leverage largelanguage_models largelanguage llms context_learning context learning chain thought cot)prompting approaches train fine tune custom nonetheless gap exists research landscape thereis limited exploration innovative foundational research incremental reasoning external tools context llms paradigm potentially bring advantages task paper aim fill gap introducing table question_answering question answering tasks framework inspired carefully enhanced address challenges tasks interpreting complex data semantics dealingwith errors generated inconsistent data generating intricate relies external tools sql python progressively enhance data generating intermediate ultimately transforming accessible format questions greater ease demonstrate remarkable performance compared fine tuned approaches inparticular outperforms best prior result benchmark achieving accuracy requiring training new orfine tuning"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) have gained the ability to assimilate humanknowledge and facilitate natural language interactions with both humans andother LLMs. However, despite their impressive achievements, LLMs have not madesignificant advancements in the realm of graph machine learning. Thislimitation arises because graphs encapsulate distinct relational data, makingit challenging to transform them into natural language that LLMs understand. Inthis paper, we bridge this gap with a novel framework, GraphText, thattranslates graphs into natural language. GraphText derives a graph-syntax treefor each graph that encapsulates both the node attributes and inter-noderelationships. Traversal of the tree yields a graph text sequence, which isthen processed by an LLM to treat graph tasks as text generation tasks.Notably, GraphText offers multiple advantages. It introduces training-freegraph reasoning: even without training on graph data, GraphText with ChatGPTcan achieve on par with, or even surpassing, the performance ofsupervised-trained graph neural networks through in-context learning (ICL).Furthermore, GraphText paves the way for interactive graph reasoning, allowingboth humans and LLMs to communicate with the model seamlessly using naturallanguage. These capabilities underscore the vast, yet-to-be-explored potentialof LLMs in the domain of graph machine learning.", "title": "graphtext graph reasoning in text space", "url": "http://arxiv.org/pdf/2310.01089v1.pdf", "tokenized_text": "large_language large language llms gained ability facilitate natural_language natural language interactions humans llms despite impressive achievements llms advancements realm graph machine_learning machine learning thislimitation arises graphs encapsulate distinct relational data makingit challenging transform natural_language natural language llms understand inthis_paper inthis paper bridge gap novel framework graphs natural_language natural language derives graph syntax graph node attributes inter traversal tree yields graph text sequence isthen processed llm treat graph tasks text generation tasks notably offers multiple advantages introduces training reasoning training graph data achieve par surpassing performance ofsupervised trained graph neural_networks neural networks context_learning context learning paves way interactive graph reasoning humans llms communicate seamlessly naturallanguage capabilities underscore vast explored potentialof llms domain graph machine_learning machine learning"}
{"id": "nan", "abstract": "  The process of log parsing, which converts log messages into structuredformats, is a crucial step for various log analysis tasks. Although numerouslog parsers have been proposed, their effectiveness on complex log data isoften hindered due to reliance on human-made rules or learning-based modelswith limited training data. The recent rise of powerful large language models(LLMs) shows potential for log parsing due to their extensive pre-trainedknowledge related to code and logging. However, their accuracy is currentlylimited due to the lack of specialized log parsing capabilities. Additionally,the inconsistency of their answers and significant overhead obstruct thepractical implementation of LLM-based log parsing.  To tackle these challenges, we introduce LLMParser, the first practicalLLM-based log parsing framework. LLMParser enables accurate and robust logparsing by leveraging the in-context learning (ICL) capability of the LLM,employing a hierarchical candidate sampling algorithm, and selectinghigh-quality demonstrations. LLMParser also includes a novel adaptive parsingcache component to store and refine the templates generated by the LLM. Thisdesign aids in addressing the inefficiency of LLMs by rapid matching topreviously parsed log templates. LLMParser also adaptively updates thetemplates in the parsing cache to ensure consistent parsed results. Extensiveevaluation on large-scale public datasets demonstrates that LLMParser surpassesthe state-of-the-art methods. Furthermore, LLMParser significantly reduces thequery times to LLMs, achieving efficiency comparable to the most efficientbaseline, Drain.", "title": "llmparser a llmbased log parsing framework", "url": "http://arxiv.org/pdf/2310.01796v1.pdf", "tokenized_text": "process log_parsing log parsing converts log messages crucial step log analysis tasks parsers proposed effectiveness complex log data isoften hindered reliance human rules learning based modelswith limited training_data training data recent rise powerful large_language large language models(llms shows potential log parsing extensive pre related code accuracy lack specialized log parsing capabilities additionally inconsistency answers significant overhead implementation llm based log parsing tackle challenges introduce based log parsing framework enables accurate robust leveraging context_learning context learning icl capability llm employing hierarchical candidate sampling algorithm quality demonstrations includes novel adaptive component store refine templates generated llm aids addressing inefficiency llms rapid matching parsed log_templates log templates adaptively updates parsing cache ensure consistent parsed results large scale public datasets demonstrates state art methods furthermore significantly reduces thequery times llms achieving efficiency comparable"}
{"id": "nan", "abstract": "  Transformers are widely used to extract complex semantic meanings from inputtokens, yet they usually operate as black-box models. In this paper, we presenta simple yet informative decomposition of hidden states (or embeddings) oftrained transformers into interpretable components. For any layer, embeddingvectors of input sequence samples are represented by a tensor $\\boldsymbol{h}\\in \\mathbb{R}^{C \\times T \\times d}$. Given embedding vector$\\boldsymbol{h}_{c,t} \\in \\mathbb{R}^d$ at sequence position $t \\le T$ in asequence (or context) $c \\le C$, extracting the mean effects yields thedecomposition \\[ \\boldsymbol{h}_{c,t} = \\boldsymbol{\\mu} + \\mathbf{pos}_t +\\mathbf{ctx}_c + \\mathbf{resid}_{c,t} \\] where $\\boldsymbol{\\mu}$ is the globalmean vector, $\\mathbf{pos}_t$ and $\\mathbf{ctx}_c$ are the mean vectors acrosscontexts and across positions respectively, and $\\mathbf{resid}_{c,t}$ is theresidual vector. For popular transformer architectures and diverse textdatasets, empirically we find pervasive mathematical structure: (1)$(\\mathbf{pos}_t)_{t}$ forms a low-dimensional, continuous, and often spiralshape across layers, (2) $(\\mathbf{ctx}_c)_c$ shows clear cluster structurethat falls into context topics, and (3) $(\\mathbf{pos}_t)_{t}$ and$(\\mathbf{ctx}_c)_c$ are mutually incoherent -- namely $\\mathbf{pos}_t$ isalmost orthogonal to $\\mathbf{ctx}_c$ -- which is canonical in compressedsensing and dictionary learning. This decomposition offers structural insightsabout input formats in in-context learning (especially for induction heads) andin arithmetic tasks.", "title": "uncovering hidden geometry in transformers via disentangling position and context", "url": "http://arxiv.org/pdf/2310.04861v1.pdf", "tokenized_text": "transformers widely extract complex semantic meanings usually operate black box paper presenta simple informative decomposition hidden states embeddings transformers interpretable components layer input sequence samples represented tensor given embedding sequence position asequence context extracting mean effects yields vector mean vectors positions respectively vector popular transformer architectures diverse textdatasets empirically find pervasive mathematical structure forms low dimensional continuous layers shows clear cluster falls context topics incoherent orthogonal canonical dictionary learning decomposition offers structural input formats context_learning context learning especially induction heads andin arithmetic tasks"}
{"id": "nan", "abstract": "  In-context learning (ICL) involves reasoning from given contextual examples.As more modalities comes, this procedure is becoming more challenging as theinterleaved input modalities convolutes the understanding process. This isexemplified by the observation that multimodal models often struggle toeffectively extrapolate from contextual examples to perform ICL. To addressthese challenges, we introduce MultiModal In-conteXt Tuning (M$^2$IXT), alightweight module to enhance the ICL capabilities of multimodal unifiedmodels. The proposed M$^2$IXT module perceives an expandable context window toincorporate various labeled examples of multiple modalities (e.g., text, image,and coordinates). It can be prepended to various multimodal unified models(e.g., OFA, Unival, LLaVA) of different architectures and trained via amixed-tasks strategy to enable rapid few-shot adaption on multiple tasks anddatasets. When tuned on as little as 50K multimodal data, M$^2$IXT can boostthe few-shot ICL performance significantly (e.g., 18\\% relative increase forOFA), and obtained state-of-the-art results across an array of tasks includingvisual question answering, image captioning, visual grounding, and visualentailment, while being considerably small in terms of model parameters (e.g.,$\\sim$$20\\times$ smaller than Flamingo or MMICL), highlighting the flexibilityand effectiveness of M$^2$IXT as a multimodal in-context learner.", "title": "lightweight incontext tuning for multimodal unified models", "url": "http://arxiv.org/pdf/2310.05109v1.pdf", "tokenized_text": "context_learning context learning icl involves reasoning given contextual examples modalities comes procedure challenging input modalities understanding process observation multimodal struggle toeffectively extrapolate contextual examples perform icl addressthese challenges introduce multimodal context tuning alightweight module enhance icl capabilities multimodal proposed module perceives context window toincorporate labeled examples multiple modalities e.g. text image coordinates prepended multimodal unified models(e.g ofa llava different architectures trained tasks strategy enable rapid shot adaption multiple tasks anddatasets tuned little 50 multimodal data boostthe shot icl performance significantly e.g. 18\\% relative increase obtained state art results array tasks question_answering question answering image captioning visual grounding considerably small terms parameters e.g. smaller flamingo highlighting effectiveness multimodal context learner"}
{"id": "nan", "abstract": "  Claim verification plays a crucial role in combating misinformation. Whileexisting works on claim verification have shown promising results, a crucialpiece of the puzzle that remains unsolved is to understand how to verify claimswithout relying on human-annotated data, which is expensive to create at alarge scale. Additionally, it is important for models to provide comprehensiveexplanations that can justify their decisions and assist human fact-checkers.This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK)Reasoning that can verify complex claims and generate explanations without theneed for annotated evidence using Large Language Models (LLMs). FOLK leveragesthe in-context learning ability of LLMs to translate the claim into aFirst-Order-Logic (FOL) clause consisting of predicates, each corresponding toa sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoningover a set of knowledge-grounded question-and-answer pairs to make veracitypredictions and generate explanations to justify its decision-making process.This process makes our model highly explanatory, providing clear explanationsof its reasoning process in human-readable form. Our experiment resultsindicate that FOLK outperforms strong baselines on three datasets encompassingvarious claim verification challenges. Our code and data are available.", "title": "explainable claim verification via knowledgegrounded reasoning with large language models", "url": "http://arxiv.org/pdf/2310.05253v2.pdf", "tokenized_text": "claim verification plays crucial role combating misinformation works claim verification shown promising_results promising results puzzle remains unsolved understand verify relying human annotated_data annotated data expensive create alarge scale additionally important provide justify decisions assist human fact paper_presents paper presents verify complex claims generate explanations theneed annotated evidence large_language large language llms leveragesthe context_learning context learning ability llms translate claim afirst order logic fol clause consisting predicates corresponding toa sub claim needs verified performs fol guided reasoningover set knowledge grounded question answer pairs generate explanations justify decision making process process makes highly explanatory providing clear reasoning process human readable form experiment outperforms strong baselines datasets claim verification challenges code data available"}
{"id": "nan", "abstract": "  Over the last decade, several regulatory bodies have started requiring thedisclosure of non-financial information from publicly listed companies, inlight of the investors' increasing attention to Environmental, Social, andGovernance (ESG) issues. Such information is publicly released in a variety ofnon-structured and multi-modal documentation. Hence, it is not straightforwardto aggregate and consolidate such data in a cohesive framework to furtherderive insights about sustainability practices across companies and markets.Given these premises, it is natural to resort to Information Extraction (IE)techniques to provide concise, informative, and actionable data to thestakeholders. Moving beyond traditional text processing techniques, in thiswork we leverage Large Language Models (LLMs), along with the prominentin-context learning technique and the Retrieved Augmented Generation (RAG)paradigm, to extract semantically structured ESG-related information fromcompanies' sustainability reports. We then adopt graph-based representations toconduct meaningful statistical, similarity and correlation analyses concerningthe ESG-related actions disclosed by companies in their sustainability reports.These analyses unveiled that companies address ESG-related issues throughseveral actions encompassing recognition, compliance, and partnerships;highlighting the complexity and joint efforts needed to address them. Moreover,disclosure similarities emerged among companies from the same region or sector.Lastly, we investigate which factual aspects impact the most on companies' ESGscores using our findings and other company information. This analysis unveiledthat companies' disclosures affect ESG scores more than other financial orcompany characteristics.", "title": "glitter or gold deriving structured insights from sustainability reports via large language models", "url": "http://arxiv.org/pdf/2310.05628v2.pdf", "tokenized_text": "decade regulatory started requiring non financial information publicly companies increasing attention issues information publicly released variety structured multi modal documentation aggregate consolidate data framework insights sustainability practices companies markets given premises natural resort information_extraction information extraction provide concise informative actionable data moving traditional text processing techniques thiswork leverage large_language large language llms context_learning context learning technique retrieved augmented generation extract semantically structured related information sustainability reports adopt graph based representations toconduct meaningful statistical similarity correlation analyses related actions disclosed companies sustainability reports analyses unveiled companies address related issues actions encompassing recognition compliance complexity joint efforts needed address similarities emerged companies region sector lastly investigate factual aspects impact companies findings company information analysis companies affect scores financial characteristics"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) are increasingly used as powerful tools for aplethora of natural language processing (NLP) applications. A recentinnovation, in-context learning (ICL), enables LLMs to learn new tasks bysupplying a few examples in the prompt during inference time, therebyeliminating the need for model fine-tuning. While LLMs have been utilized inseveral applications, their applicability in explaining the behavior of othermodels remains relatively unexplored. Despite the growing number of newexplanation techniques, many require white-box access to the model and/or arecomputationally expensive, highlighting a need for next-generation post hocexplainers. In this work, we present the first framework to study theeffectiveness of LLMs in explaining other predictive models. More specifically,we propose a novel framework encompassing multiple prompting strategies: i)Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL,and iv) Explanation-based ICL, with varying levels of information about theunderlying ML model and the local neighborhood of the test sample. We conductextensive experiments with real-world benchmark datasets to demonstrate thatLLM-generated explanations perform on par with state-of-the-art post hocexplainers using their ability to leverage ICL examples and their internalknowledge in generating model explanations. On average, across four datasetsand two ML models, we observe that LLMs identify the most important featurewith 72.19% accuracy, opening up new frontiers in explainable artificialintelligence (XAI) to explore LLM-based explanation frameworks.", "title": "are large language models post hoc explainers", "url": "http://arxiv.org/pdf/2310.05797v2.pdf", "tokenized_text": "large_language large language llms increasingly powerful tools natural_language natural language processing nlp applications context_learning context learning icl enables llms learn new tasks examples inference time need fine tuning llms utilized applications applicability explaining behavior remains relatively unexplored despite growing number techniques require white box access and/or expensive highlighting need generation post work present framework study theeffectiveness llms explaining predictive specifically propose_a_novel propose novel framework encompassing multiple strategies based icl ii prediction based icl iii instruction based icl iv explanation based icl varying levels information theunderlying ml local neighborhood test sample conductextensive experiments real world benchmark_datasets benchmark datasets demonstrate generated explanations perform par state art post ability leverage icl examples generating explanations average datasetsand ml observe llms identify important accuracy opening new frontiers explainable artificialintelligence xai explore llm based explanation frameworks"}
{"id": "nan", "abstract": "  Large language models (LLMs) have exhibited remarkable capabilities inNLP-related tasks such as translation, summarizing, and generation. Theapplication of LLMs in specific areas, notably AIOps (Artificial Intelligencefor IT Operations), holds great potential due to their advanced abilities ininformation summarizing, report analyzing, and ability of API calling.Nevertheless, the performance of current LLMs in AIOps tasks is yet to bedetermined. Furthermore, a comprehensive benchmark is required to steer theoptimization of LLMs tailored for AIOps. Compared with existing benchmarks thatfocus on evaluating specific fields like network configuration, in this paper,we present \\textbf{OpsEval}, a comprehensive task-oriented AIOps benchmarkdesigned for LLMs. For the first time, OpsEval assesses LLMs' proficiency inthree crucial scenarios (Wired Network Operation, 5G Communication Operation,and Database Operation) at various ability levels (knowledge recall, analyticalthinking, and practical application). The benchmark includes 7,200 questions inboth multiple-choice and question-answer (QA) formats, available in English andChinese. With quantitative and qualitative results, we show how various LLMtricks can affect the performance of AIOps, including zero-shot,chain-of-thought, and few-shot in-context learning. We find that GPT4-score ismore consistent with experts than widely used Bleu and Rouge, which can be usedto replace automatic metrics for large-scale qualitative evaluations.", "title": "opseval a comprehensive taskoriented aiops benchmark for large language models", "url": "http://arxiv.org/pdf/2310.07637v2.pdf", "tokenized_text": "large_language large language llms exhibited remarkable_capabilities remarkable capabilities innlp related tasks translation summarizing generation theapplication llms specific areas notably artificial operations holds great_potential great potential advanced abilities summarizing report analyzing ability api calling performance current llms tasks furthermore comprehensive benchmark required steer llms tailored compared existing benchmarks thatfocus evaluating specific fields like network configuration paper present comprehensive task oriented llms time assesses llms proficiency crucial scenarios network operation communication operation database operation ability levels knowledge recall practical application benchmark includes questions inboth multiple choice question answer qa formats available english quantitative qualitative results affect performance including zero shot chain thought shot context_learning context learning find gpt4 score consistent experts widely bleu rouge replace automatic metrics large scale qualitative evaluations"}
{"id": "nan", "abstract": "  Plan-and-Write is a common hierarchical approach in long-form narrative textgeneration, which first creates a plan to guide the narrative writing.Following this approach, several studies rely on simply prompting largelanguage models for planning, which often yields suboptimal results. In thispaper, we propose a new framework called Evaluation-guided Iterative PlanExtraction for long-form narrative text generation (EIPE-text), which extractsplans from the corpus of narratives and utilizes the extracted plans toconstruct a better planner. EIPE-text has three stages: plan extraction,learning, and inference. In the plan extraction stage, it iteratively extractsand improves plans from the narrative corpus and constructs a plan corpus. Wepropose a question answer (QA) based evaluation mechanism to automaticallyevaluate the plans and generate detailed plan refinement instructions to guidethe iterative improvement. In the learning stage, we build a better planner byfine-tuning with the plan corpus or in-context learning with examples in theplan corpus. Finally, we leverage a hierarchical approach to generate long-formnarratives. We evaluate the effectiveness of EIPE-text in the domains of novelsand storytelling. Both GPT-4-based evaluations and human evaluationsdemonstrate that our method can generate more coherent and relevant long-formnarratives. Our code will be released in the future.", "title": "eipetext evaluationguided iterative plan extraction for longform narrative text generation", "url": "http://arxiv.org/pdf/2310.08185v1.pdf", "tokenized_text": "plan write common hierarchical approach long form narrative textgeneration creates plan guide narrative writing following approach studies rely simply largelanguage_models largelanguage planning yields suboptimal results thispaper propose_a_new propose new framework called evaluation guided iterative long form narrative text generation text corpus narratives utilizes extracted plans toconstruct better planner text stages plan extraction learning inference plan extraction stage iteratively improves plans narrative corpus constructs plan corpus wepropose question answer qa based evaluation mechanism plans generate detailed plan refinement instructions iterative improvement learning stage build better planner byfine tuning plan corpus context_learning context learning examples corpus finally leverage hierarchical approach generate long evaluate effectiveness text domains gpt-4 based evaluations human evaluationsdemonstrate method generate coherent relevant long code released future"}
{"id": "nan", "abstract": "  The task of Question Generation over Knowledge Bases (KBQG) aims to convert alogical form into a natural language question. For the sake of expensive costof large-scale question annotation, the methods of KBQG under low-resourcescenarios urgently need to be developed. However, current methods heavily relyon annotated data for fine-tuning, which is not well-suited for few-shotquestion generation. The emergence of Large Language Models (LLMs) has showntheir impressive generalization ability in few-shot tasks. Inspired byChain-of-Thought (CoT) prompting, which is an in-context learning strategy forreasoning, we formulate KBQG task as a reasoning problem, where the generationof a complete question is splitted into a series of sub-question generation.Our proposed prompting method KQG-CoT first retrieves supportive logical formsfrom the unlabeled data pool taking account of the characteristics of thelogical form. Then, we write a prompt to explicit the reasoning chain ofgenerating complicated questions based on the selected demonstrations. Tofurther ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting thelogical forms by their complexity. We conduct extensive experiments over threepublic KBQG datasets. The results demonstrate that our prompting methodconsistently outperforms other prompting baselines on the evaluated datasets.Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results ofthe PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4,METEOR, and ROUGE-L, respectively.", "title": "prompting large language models with chainofthought for fewshot knowledge base question generation", "url": "http://arxiv.org/pdf/2310.08395v3.pdf", "tokenized_text": "task question generation knowledge bases aims convert form natural_language natural language question expensive large scale question annotation methods low resourcescenarios urgently need developed current methods heavily relyon annotated_data annotated data fine tuning suited generation emergence large_language large language llms impressive generalization_ability generalization ability shot tasks inspired thought cot context_learning context learning strategy formulate task reasoning problem generationof complete question series sub question generation proposed method cot retrieves supportive logical unlabeled data pool taking account characteristics form write explicit reasoning chain ofgenerating complicated questions based selected demonstrations tofurther ensure quality extend cot sorting forms complexity conduct_extensive conduct extensive experiments threepublic datasets results_demonstrate results demonstrate methodconsistently outperforms baselines evaluated datasets remarkably method surpass existing shot sota results ofthe dataset absolute points rouge respectively"}
{"id": "nan", "abstract": "  Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)?Several recent works draw analogies between the dynamics of GD and the emergentbehavior of ICL in large language models. However, these works make assumptionsfar from the realistic natural language setting in which language models aretrained. Such discrepancies between theory and practice, therefore, necessitatefurther investigation to validate their applicability.  We start by highlighting the weaknesses in prior works that constructTransformer weights to simulate gradient descent. Their experiments withtraining Transformers on ICL objective, inconsistencies in the ordersensitivity of ICL and GD, sparsity of the constructed weights, and sensitivityto parameter changes are some examples of a mismatch from the real-worldsetting.  Furthermore, we probe and compare the ICL vs. GD hypothesis in a naturalsetting. We conduct comprehensive empirical analyses on language modelspretrained on natural data (LLaMa-7B). Our comparisons on various performancemetrics highlight the inconsistent behavior of ICL and GD as a function ofvarious factors such as datasets, models, and number of demonstrations. Weobserve that ICL and GD adapt the output distribution of language modelsdifferently. These results indicate that the equivalence between ICL and GD isan open hypothesis, requires nuanced considerations and calls for furtherstudies.", "title": "do pretrained transformers really learn incontext by gradient descent", "url": "http://arxiv.org/pdf/2310.08540v1.pdf", "tokenized_text": "context_learning context learning icl implicitly equivalent gradient descent recent works draw analogies dynamics gd icl large_language large language works realistic natural_language natural language setting language_models language aretrained discrepancies theory practice investigation validate applicability start highlighting weaknesses prior works weights simulate gradient descent experiments transformers icl objective inconsistencies icl gd sparsity constructed weights parameter changes examples mismatch real worldsetting furthermore probe compare icl vs. gd hypothesis conduct comprehensive empirical analyses language natural data llama-7b comparisons highlight inconsistent behavior icl gd function factors datasets number demonstrations weobserve icl gd adapt output distribution language results_indicate results indicate icl gd isan open hypothesis requires nuanced considerations calls"}
{"id": "nan", "abstract": "  Prompt-based learning has been demonstrated as a compelling paradigmcontributing to large language models' tremendous success (LLMs). Inspired bytheir success in language tasks, existing research has leveraged LLMs inembodied instruction following and task planning. However, not much attentionhas been paid to embodied tasks with multimodal prompts, combining visionsignals with text descriptions. This type of task poses a major challenge torobots' capability to understand the interconnection and complementaritybetween vision and language signals. In this work, we introduce an effectiveframework that learns a policy to perform robot manipulation with multimodalprompts from multi-task expert trajectories. Our methods consist of a two-stagetraining pipeline that performs inverse dynamics pretraining and multi-taskfinetuning. To facilitate multimodal understanding, we design our multimodalprompt encoder by augmenting a pretrained LM with a residual connection to thevisual input and model the dependencies among action dimensions. Empirically,we evaluate the efficacy of our method on the VIMA-BENCH and establish a newstate-of-the-art (10% improvement in success rate). Moreover, we demonstratethat our model exhibits remarkable in-context learning ability.", "title": "mastering robot manipulation with multimodal prompts through pretraining and multitask finetuning", "url": "http://arxiv.org/pdf/2310.09676v1.pdf", "tokenized_text": "based learning demonstrated compelling large_language large language tremendous success llms inspired success language tasks existing research leveraged llms instruction_following instruction following task planning paid embodied tasks multimodal combining text descriptions type task poses major challenge capability understand vision language signals work introduce learns policy perform robot manipulation multi task expert trajectories methods consist pipeline performs inverse dynamics pretraining multi facilitate multimodal understanding design encoder augmenting pretrained lm residual connection input dependencies action dimensions empirically evaluate efficacy method vima bench establish newstate art 10 improvement success_rate success rate demonstratethat exhibits remarkable context_learning context learning ability"}
{"id": "nan", "abstract": "  Image processing is a fundamental task in computer vision, which aims atenhancing image quality and extracting essential features for subsequent visionapplications. Traditionally, task-specific models are developed for individualtasks and designing such models requires distinct expertise. Building upon thesuccess of large language models (LLMs) in natural language processing (NLP),there is a similar trend in computer vision, which focuses on developinglarge-scale models through pretraining and in-context learning. This paradigmshift reduces the reliance on task-specific models, yielding a powerful unifiedmodel to deal with various tasks. However, these advances have predominantlyconcentrated on high-level vision tasks, with less attention paid to low-levelvision tasks. To address this issue, we propose a universal model for generalimage processing that covers image restoration, image enhancement, imagefeature extraction tasks, \\textit{etc}. Our proposed framework, namedPromptGIP, unifies these diverse image processing tasks within a universalframework. Inspired by NLP question answering (QA) techniques, we employ avisual prompting question answering paradigm. Specifically, we treat theinput-output image pair as a structured question-answer sentence, therebyreprogramming the image processing task as a prompting QA problem. PromptGIPcan undertake diverse \\textbf{cross-domain} tasks using provided visualprompts, eliminating the need for task-specific finetuning. Our methodologyoffers a universal and adaptive solution to general image processing. WhilePromptGIP has demonstrated a certain degree of out-of-domain taskgeneralization capability, further research is expected to fully explore itsmore powerful emergent generalization.", "title": "unifying image processing as visual prompting question answering", "url": "http://arxiv.org/pdf/2310.10513v1.pdf", "tokenized_text": "image processing fundamental task computer_vision computer vision aims image quality extracting essential features subsequent traditionally task specific developed designing requires distinct expertise building thesuccess large_language large language llms natural_language natural language processing similar trend computer_vision computer vision focuses scale pretraining context_learning context learning reduces reliance task specific yielding powerful deal tasks advances high level vision tasks attention paid low tasks address issue propose universal processing covers image restoration image enhancement extraction tasks proposed framework unifies diverse image processing tasks inspired nlp question_answering question answering qa techniques employ avisual question_answering question answering paradigm specifically treat theinput output image pair structured question answer sentence image processing task qa problem undertake diverse domain tasks provided eliminating need task specific finetuning universal adaptive solution general image processing demonstrated certain degree domain taskgeneralization capability research expected fully explore powerful emergent generalization"}
{"id": "nan", "abstract": "  Large language models (LMs) are currently trained to predict tokens givendocument prefixes, enabling them to directly perform long-form generation andprompting-style tasks which can be reduced to document completion. Existingpretraining pipelines train LMs by concatenating random sets of short documentsto create input contexts but the prior documents provide no signal forpredicting the next document. We instead present In-Context Pretraining, a newapproach where language models are pretrained on a sequence of relateddocuments, thereby explicitly encouraging them to read and reason acrossdocument boundaries. We can do In-Context Pretraining by simply changing thedocument ordering so that each context contains related documents, and directlyapplying existing pretraining pipelines. However, this document sorting problemis challenging. There are billions of documents and we would like the sort tomaximize contextual similarity for every document without repeating any data.To do this, we introduce approximate algorithms for finding related documentswith efficient nearest neighbor search and constructing coherent input contextswith a graph traversal algorithm. Our experiments show In-Context Pretrainingoffers a simple and scalable approach to significantly enhance LMs'performance:we see notable improvements in tasks that require more complex contextualreasoning, including in-context learning (+8%), reading comprehension (+15%),faithfulness to previous contexts (+16%), long-context reasoning (+5%), andretrieval augmentation (+9%).", "title": "incontext pretraining language modeling beyond document boundaries", "url": "http://arxiv.org/pdf/2310.10638v3.pdf", "tokenized_text": "large_language large language lms currently trained predict tokens prefixes enabling directly perform long form generation andprompting style tasks reduced document completion pipelines train lms concatenating random sets short create input contexts prior documents provide signal document instead present context pretraining language_models language pretrained sequence explicitly encouraging read reason boundaries context pretraining simply changing ordering context contains related documents existing pretraining pipelines document sorting challenging billions documents like sort tomaximize contextual similarity document repeating data introduce approximate algorithms finding related efficient nearest neighbor search constructing coherent input graph traversal algorithm experiments context simple scalable approach significantly enhance notable improvements tasks require complex including context_learning context learning reading comprehension previous contexts long context reasoning andretrieval augmentation"}
{"id": "nan", "abstract": "  In-context learning is a promising paradigm that utilizes in-context examplesas prompts for the predictions of large language models. These prompts arecrucial for achieving strong performance. However, since the prompts need to besampled from a large volume of annotated examples, finding the right prompt mayresult in high annotation costs. To address this challenge, this paperintroduces an influence-driven selective annotation method that aims tominimize annotation costs while improving the quality of in-context examples.The essence of our method is to select a pivotal subset from a large-scaleunlabeled data pool to annotate for the subsequent sampling of prompts.Specifically, a directed graph is first constructed to represent unlabeleddata. Afterward, the influence of candidate unlabeled subsets is quantifiedwith a diffusion process. A simple yet effective greedy algorithm for unlabeleddata selection is lastly introduced. It iteratively selects the data if itprovides a maximum marginal gain with respect to quantified influence. Comparedwith previous efforts on selective annotations, our influence-driven methodworks in an end-to-end manner, avoids an intractable explicit balance betweendata diversity and representativeness, and enjoys theoretical support.Experiments confirm the superiority of the proposed method on variousbenchmarks, achieving better performance under lower time consumption duringsubset selection. The project page is available athttps://skzhang1.github.io/IDEAL/.", "title": "ideal influencedriven selective annotations empower incontext learners in large language models", "url": "http://arxiv.org/pdf/2310.10873v1.pdf", "tokenized_text": "context_learning context learning promising paradigm utilizes context examplesas predictions large_language large language achieving strong performance need large volume annotated examples finding right high annotation costs address challenge paperintroduces influence driven selective annotation method aims annotation costs improving quality context_examples context examples essence method select pivotal subset large data pool annotate subsequent sampling specifically directed graph constructed represent unlabeleddata afterward influence candidate unlabeled subsets diffusion process simple effective greedy algorithm unlabeleddata selection lastly introduced iteratively selects data maximum marginal gain respect quantified influence previous efforts selective annotations influence driven end end manner avoids intractable explicit balance diversity enjoys theoretical support experiments confirm superiority proposed_method proposed method achieving better performance lower time consumption selection project page available"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) have excelled as high-level semantic plannersfor sequential decision-making tasks. However, harnessing them to learn complexlow-level manipulation tasks, such as dexterous pen spinning, remains an openproblem. We bridge this fundamental gap and present Eureka, a human-levelreward design algorithm powered by LLMs. Eureka exploits the remarkablezero-shot generation, code-writing, and in-context improvement capabilities ofstate-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization overreward code. The resulting rewards can then be used to acquire complex skillsvia reinforcement learning. Without any task-specific prompting or pre-definedreward templates, Eureka generates reward functions that outperform experthuman-engineered rewards. In a diverse suite of 29 open-source RL environmentsthat include 10 distinct robot morphologies, Eureka outperforms human expertson 83% of the tasks, leading to an average normalized improvement of 52%. Thegenerality of Eureka also enables a new gradient-free in-context learningapproach to reinforcement learning from human feedback (RLHF), readilyincorporating human inputs to improve the quality and the safety of thegenerated rewards without model updating. Finally, using Eureka rewards in acurriculum learning setting, we demonstrate for the first time, a simulatedShadow Hand capable of performing pen spinning tricks, adeptly manipulating apen in circles at rapid speed.", "title": "eureka humanlevel reward design via coding large language models", "url": "http://arxiv.org/pdf/2310.12931v1.pdf", "tokenized_text": "large_language large language llms excelled high level semantic sequential decision making tasks harnessing learn level manipulation tasks dexterous pen remains bridge fundamental gap present human design algorithm powered llms exploits shot generation code writing context improvement capabilities ofstate art llms gpt-4 perform evolutionary optimization code resulting rewards acquire complex reinforcement_learning reinforcement learning task specific pre templates generates reward functions outperform experthuman engineered rewards diverse suite 29 open source rl include 10 distinct robot outperforms human tasks leading average normalized improvement 52 enables new gradient free context learningapproach reinforcement_learning reinforcement learning human feedback rlhf human inputs improve quality safety thegenerated rewards updating finally rewards learning setting demonstrate time hand capable performing pen adeptly manipulating circles rapid speed"}
{"id": "nan", "abstract": "  In open-domain question-answering (ODQA), most existing questions requiresingle-hop reasoning on commonsense. To further extend this task, we officiallyintroduce open-domain multi-hop reasoning (ODMR) by answering multi-hopquestions with explicit reasoning steps in open-domain setting. Recently, largelanguage models (LLMs) have found significant utility in facilitating ODQAwithout external corpus. Furthermore, chain-of-thought (CoT) prompting booststhe reasoning capability of LLMs to a greater extent with manual or automatedparadigms. However, existing automated methods lack of quality assurance, whilemanual approaches suffer from limited scalability and poor diversity, hinderingthe capabilities of LLMs. In this paper, we propose Self-promptedChain-of-Thought (SP-CoT), an automated framework to mass-produce high qualityCoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generationpipeline of high quality ODMR datasets, an adaptive sampler for in-context CoTselection and self-prompted inference via in-context learning. Extensiveexperiments on four multi-hop question-answering benchmarks show that ourproposed SP-CoT not only significantly surpasses the previous SOTA methods onlarge-scale (175B) LLMs, but also nearly doubles the zero-shot performance ofsmall-scale (13B) LLMs. Further analysis reveals the remarkable capability ofSP-CoT to elicit direct and concise intermediate reasoning steps by recalling$\\sim$50\\% of intermediate answers on MuSiQue-Ans dataset.", "title": "selfprompted chainofthought on large language models for opendomain multihop reasoning", "url": "http://arxiv.org/pdf/2310.13552v2.pdf", "tokenized_text": "open domain question answering odqa existing questions hop reasoning commonsense extend task open domain multi hop reasoning answering multi hopquestions explicit reasoning_steps reasoning steps open domain setting recently largelanguage_models largelanguage llms found significant utility facilitating external corpus furthermore chain thought cot reasoning capability llms greater extent manual existing automated methods lack quality assurance approaches suffer limited scalability poor diversity capabilities llms paper propose self thought sp cot automated framework mass produce high llms llms llms sp cot introduces automated high_quality high quality datasets adaptive context self prompted inference context_learning context learning extensiveexperiments multi hop question answering benchmarks ourproposed sp cot significantly surpasses previous sota methods onlarge scale 175b llms nearly doubles zero shot performance scale 13b llms analysis reveals remarkable capability cot elicit direct concise intermediate reasoning_steps reasoning steps intermediate answers musique dataset"}
{"id": "nan", "abstract": "  Users of social platforms often perceive these sites as supportive spaces topost about their mental health issues. Those conversations contain importanttraces about individuals' health risks. Recently, researchers have exploitedthis online information to construct mental health detection models, which aimto identify users at risk on platforms like Twitter, Reddit or Facebook. Mostof these models are centred on achieving good classification results, ignoringthe explainability and interpretability of the decisions. Recent research haspointed out the importance of using clinical markers, such as the use ofsymptoms, to improve trust in the computational models by health professionals.In this paper, we propose using transformer-based architectures to detect andexplain the appearance of depressive symptom markers in the users' writings. Wepresent two approaches: i) train a model to classify, and another one toexplain the classifier's decision separately and ii) unify the two taskssimultaneously using a single model. Additionally, for this latter manner, wealso investigated the performance of recent conversational LLMs when usingin-context learning. Our natural language explanations enable clinicians tointerpret the models' decisions based on validated symptoms, enhancing trust inthe automated process. We evaluate our approach using recent symptom-baseddatasets, employing both offline and expert-in-the-loop metrics to assess thequality of the explanations generated by our models. The experimental resultsshow that it is possible to achieve good classification results whilegenerating interpretable symptom-based explanations.", "title": "explainable depression symptom detection in social media", "url": "http://arxiv.org/pdf/2310.13664v2.pdf", "tokenized_text": "users social platforms perceive sites supportive spaces mental_health mental health issues conversations contain individuals health risks recently researchers online information construct mental_health mental health detection identify users risk platforms like twitter reddit facebook mostof centred achieving good classification results explainability interpretability decisions recent research importance clinical markers use improve trust computational health professionals paper propose transformer based architectures detect appearance depressive symptom markers users wepresent approaches train classify classifier decision separately ii unify taskssimultaneously single additionally manner wealso investigated performance recent conversational llms usingin context_learning context learning natural_language natural language explanations enable clinicians tointerpret decisions based validated symptoms enhancing trust inthe automated process evaluate approach recent symptom employing offline expert loop metrics assess thequality explanations generated experimental resultsshow possible achieve good classification results interpretable symptom based explanations"}
{"id": "nan", "abstract": "  Using in-context learning (ICL) for data generation, techniques such asSelf-Instruct (Wang et al., 2023) or the follow-up Alpaca (Taori et al., 2023)can train strong conversational agents with only a small amount of humansupervision. One limitation of these approaches is that they resort to verylarge language models (around 175B parameters) that are also proprietary andnon-public. Here we explore the application of such techniques to languagemodels that are much smaller (around 10B--40B parameters) and have permissivelicenses. We find the Self-Instruct approach to be less effective at thesesizes and propose new ICL methods that draw on two main ideas: (a)Categorization and simplification of the ICL templates to make prompt learningeasier for the LM, and (b) Ensembling over multiple LM outputs to help selecthigh-quality synthetic examples. Our algorithm leverages the 175 Self-Instructseed tasks and employs separate pipelines for instructions that require aninput and instructions that do not. Empirical investigations with different LMsshow that: (1) Our proposed method yields higher-quality instruction tuningdata than Self-Instruct, (2) It improves performances of both vanilla andinstruction-tuned LMs by significant margins, and (3) Smaller instruction-tunedLMs generate more useful outputs than their larger un-tuned counterparts. Ourcodebase is available at https://github.com/IBM/ensemble-instruct.", "title": "ensembleinstruct generating instructiontuning data with a heterogeneous mixture of lms", "url": "http://arxiv.org/pdf/2310.13961v1.pdf", "tokenized_text": "context_learning context learning icl data generation techniques instruct et_al et al 2023 follow alpaca et_al et al train strong conversational agents small limitation approaches resort language_models language 175b parameters proprietary andnon public explore application techniques languagemodels smaller parameters find self instruct approach effective propose new icl methods draw main ideas simplification icl templates lm ensembling multiple lm outputs help quality synthetic examples algorithm leverages 175 self tasks employs separate pipelines instructions require aninput instructions empirical investigations different proposed_method proposed method yields higher quality instruction self instruct improves performances vanilla andinstruction tuned lms significant margins smaller instruction generate useful outputs larger un tuned counterparts available"}
{"id": "nan", "abstract": "  Recent literature has suggested the potential of using large language models(LLMs) to make predictions for tabular tasks. However, LLMs have been shown toexhibit harmful social biases that reflect the stereotypes and inequalitiespresent in the society. To this end, as well as the widespread use of tabulardata in many high-stake applications, it is imperative to explore the followingquestions: what sources of information do LLMs draw upon when makingpredictions for tabular tasks; whether and to what extent are LLM predictionsfor tabular tasks influenced by social biases and stereotypes; and what are theconsequential implications for fairness? Through a series of experiments, wedelve into these questions and show that LLMs tend to inherit social biasesfrom their training data which significantly impact their fairness in tabularprediction tasks. Furthermore, our investigations show that in the context ofbias mitigation, though in-context learning and fine-tuning have a moderateeffect, the fairness metric gap between different subgroups is still largerthan that in traditional machine learning models, such as Random Forest andshallow Neural Networks. This observation emphasizes that the social biases areinherent within the LLMs themselves and inherited from their pre-trainingcorpus, not only from the downstream task datasets. Besides, we demonstratethat label-flipping of in-context examples can significantly reduce biases,further highlighting the presence of inherent bias within LLMs.", "title": "investigating the fairness of large language models for predictions on tabular data", "url": "http://arxiv.org/pdf/2310.14607v1.pdf", "tokenized_text": "recent literature suggested potential large_language large language models(llms predictions tabular tasks llms shown harmful social biases reflect stereotypes society end widespread use high applications imperative explore sources information llms draw tabular tasks extent llm tabular tasks influenced social biases stereotypes implications fairness series experiments wedelve questions llms tend inherit social training_data training data significantly impact fairness tasks furthermore investigations context mitigation context_learning context learning fine tuning fairness metric gap different subgroups traditional machine_learning machine learning random neural_networks neural networks observation emphasizes social biases llms inherited pre trainingcorpus downstream task datasets demonstratethat label flipping context_examples context examples significantly reduce biases highlighting presence inherent bias llms"}
{"id": "nan", "abstract": "  Visual reasoning requires multimodal perception and commonsense cognition ofthe world. Recently, multiple vision-language models (VLMs) have been proposedwith excellent commonsense reasoning ability in various domains. However, howto harness the collective power of these complementary VLMs is rarely explored.Existing methods like ensemble still struggle to aggregate these models withthe desired higher-order communications. In this work, we propose Cola, a novelparadigm that coordinates multiple VLMs for visual reasoning. Our key insightis that a large language model (LLM) can efficiently coordinate multiple VLMsby facilitating natural language communication that leverages their distinctand complementary capabilities. Extensive experiments demonstrate that ourinstruction tuning variant, Cola-FT, achieves state-of-the-art performance onvisual question answering (VQA), outside knowledge VQA, visual entailment, andvisual spatial reasoning tasks. Moreover, we show that our in-context learningvariant, Cola-Zero, exhibits competitive performance in zero and few-shotsettings, without finetuning. Through systematic ablation studies andvisualizations, we validate that a coordinator LLM indeed comprehends theinstruction prompts as well as the separate functionalities of VLMs; it thencoordinates them to enable impressive visual reasoning capabilities.", "title": "large language models are visual reasoning coordinators", "url": "http://arxiv.org/pdf/2310.15166v1.pdf", "tokenized_text": "visual reasoning requires multimodal perception commonsense cognition ofthe world recently multiple vision language_models language vlms excellent commonsense reasoning ability domains howto harness collective power complementary vlms rarely explored existing_methods existing methods like ensemble struggle aggregate withthe desired higher order communications work propose coordinates multiple vlms visual reasoning key large_language large language llm efficiently coordinate multiple facilitating natural_language natural language communication leverages complementary capabilities extensive_experiments extensive experiments demonstrate tuning variant ft achieves_state achieves state art performance question_answering question answering vqa outside knowledge vqa visual entailment andvisual spatial reasoning tasks context zero exhibits competitive_performance competitive performance zero shotsettings finetuning systematic ablation studies validate coordinator llm theinstruction separate functionalities vlms enable impressive visual reasoning capabilities"}
{"id": "nan", "abstract": "  We report the presence of a simple neural mechanism that represents aninput-output function as a vector within autoregressive transformer languagemodels (LMs). Using causal mediation analysis on a diverse range ofin-context-learning (ICL) tasks, we find that a small number attention headstransport a compact representation of the demonstrated task, which we call afunction vector (FV). FVs are robust to changes in context, i.e., they triggerexecution of the task on inputs such as zero-shot and natural text settingsthat do not resemble the ICL contexts from which they are collected. We testFVs across a range of tasks, models, and layers and find strong causal effectsacross settings in middle layers. We investigate the internal structure of FVsand find while that they often contain information that encodes the outputspace of the function, this information alone is not sufficient to reconstructan FV. Finally, we test semantic vector composition in FVs, and find that tosome extent they can be summed to create vectors that trigger new complextasks. Taken together, our findings suggest that LLMs contain internalabstractions of general-purpose functions that can be invoked in a variety ofcontexts.", "title": "function vectors in large language models", "url": "http://arxiv.org/pdf/2310.15213v1.pdf", "tokenized_text": "report presence simple neural mechanism represents aninput output function vector autoregressive transformer languagemodels lms causal analysis diverse range ofin context learning icl tasks find small_number small number attention compact representation demonstrated task afunction vector robust changes context i.e. task inputs zero shot natural text resemble icl contexts collected range tasks layers find strong causal settings middle layers investigate internal structure find contain information encodes function information sufficient finally test semantic vector composition find tosome extent create vectors trigger new taken findings_suggest findings suggest llms contain general purpose functions invoked variety"}
{"id": "nan", "abstract": "  Since ChatGPT released its API for public use, the number of applicationsbuilt on top of commercial large language models (LLMs) increase exponentially.One popular usage of such models is leveraging its in-context learning abilityand generating responses given user queries leveraging knowledge obtained byretrieval augmentation. One problem of deploying commercial retrieval-augmentedLLMs is the cost due to the additionally retrieved context that largelyincreases the input token size of the LLMs. To mitigate this, we propose atoken compression scheme that includes two methods: summarization compressionand semantic compression. The first method applies a T5-based model that isfine-tuned by datasets generated using self-instruct containing samples withvarying lengths and reduce token size by doing summarization. The second methodfurther compresses the token size by removing words with lower impact on thesemantic. In order to adequately evaluate the effectiveness of the proposedmethods, we propose and utilize a dataset called Food-Recommendation DB (FRDB)focusing on food recommendation for women around pregnancy period or infants.Our summarization compression can reduce 65% of the retrieval token size withfurther 0.3% improvement on the accuracy; semantic compression provides a moreflexible way to trade-off the token size with performance, for which we canreduce the token size by 20% with only 1.6% of accuracy drop.", "title": "tcrallm token compression retrieval augmented large language model for inference cost reduction", "url": "http://arxiv.org/pdf/2310.15556v2.pdf", "tokenized_text": "chatgpt released api public use number commercial large_language large language llms increase exponentially popular usage leveraging context_learning context learning abilityand generating responses given user queries leveraging knowledge obtained augmentation problem deploying commercial retrieval cost additionally retrieved context input token size llms mitigate propose atoken compression scheme includes methods summarization semantic compression method applies t5 based tuned datasets generated self instruct containing samples withvarying lengths reduce token size summarization second compresses token size removing words lower impact thesemantic order adequately evaluate effectiveness propose utilize dataset called recommendation women pregnancy summarization compression reduce 65 retrieval token size improvement accuracy semantic compression provides way trade token size performance token size 20 accuracy drop"}
{"id": "nan", "abstract": "  Mobile applications have become a ubiquitous part of our daily life,providing users with access to various services and utilities. Text input, asan important interaction channel between users and applications, plays animportant role in core functionality such as search queries, authentication,messaging, etc. However, certain special text (e.g., -18 for Font Size) cancause the app to crash, and generating diversified unusual inputs for fullytesting the app is highly demanded. Nevertheless, this is also challenging dueto the combination of explosion dilemma, high context sensitivity, and complexconstraint relations. This paper proposes InputBlaster which leverages the LLMto automatically generate unusual text inputs for mobile app crash detection.It formulates the unusual inputs generation problem as a task of producing aset of test generators, each of which can yield a batch of unusual text inputsunder the same mutation rule. In detail, InputBlaster leverages LLM to producethe test generators together with the mutation rules serving as the reasoningchain, and utilizes the in-context learning schema to demonstrate the LLM withexamples for boosting the performance. InputBlaster is evaluated on 36 textinput widgets with cash bugs involving 31 popular Android apps, and resultsshow that it achieves 78% bug detection rate, with 136% higher than the bestbaseline. Besides, we integrate it with the automated GUI testing tool anddetect 37 unseen crashes in real-world apps from Google Play.", "title": "testing the limits unusual text inputs generation for mobile app crash detection with large language model", "url": "http://arxiv.org/pdf/2310.15657v1.pdf", "tokenized_text": "mobile applications ubiquitous daily life providing users access services utilities text input important interaction channel users applications plays animportant role core functionality search queries messaging etc certain special text e.g. size app generating diversified unusual inputs app highly demanded challenging dueto combination explosion dilemma high context sensitivity relations paper_proposes paper proposes leverages automatically generate unusual text inputs mobile app detection formulates unusual inputs generation problem task producing aset test generators yield batch unusual text mutation rule detail leverages llm producethe test generators mutation rules serving utilizes context_learning context learning schema demonstrate llm withexamples boosting performance evaluated 36 bugs involving 31 popular apps resultsshow achieves 78 bug detection rate higher integrate automated gui testing tool 37 unseen real world apps google play"}
{"id": "nan", "abstract": "  Large Language Models (LLMs) have achieved tremendous progress, yet theystill often struggle with challenging reasoning problems. Current approachesaddress this challenge by sampling or searching detailed and low-levelreasoning chains. However, these methods are still limited in their explorationcapabilities, making it challenging for correct solutions to stand out in thehuge solution space. In this work, we unleash LLMs' creative potential forexploring multiple diverse problem solving strategies by framing an LLM as ahierarchical policy via in-context learning. This policy comprises of avisionary leader that proposes multiple diverse high-level problem-solvingtactics as hints, accompanied by a follower that executes detailedproblem-solving processes following each of the high-level instruction. Thefollower uses each of the leader's directives as a guide and samples multiplereasoning chains to tackle the problem, generating a solution group for eachleader proposal. Additionally, we propose an effective and efficienttournament-based approach to select among these explored solution groups toreach the final answer. Our approach produces meaningful and inspiring hints,enhances problem-solving strategy exploration, and improves the final answeraccuracy on challenging problems in the MATH dataset. Code will be released athttps://github.com/lz1oceani/LLM-As-Hierarchical-Policy.", "title": "unleashing the creative mind language model as hierarchical policy for improved exploration on challenging problem solving", "url": "http://arxiv.org/pdf/2311.00694v1.pdf", "tokenized_text": "large_language large language llms achieved tremendous progress theystill struggle challenging reasoning problems current challenge sampling searching detailed low chains methods limited making challenging correct solutions stand solution space work unleash llms creative potential multiple diverse problem solving strategies framing llm ahierarchical policy context_learning context learning policy comprises leader proposes multiple diverse high level problem hints accompanied executes solving processes following high level instruction uses leader guide samples chains tackle problem generating solution group proposal additionally propose effective based approach select explored solution groups toreach final answer approach produces meaningful inspiring hints enhances problem solving strategy exploration improves final challenging problems math dataset code released"}
{"id": "nan", "abstract": "  A standard paradigm for sentiment analysis is to rely on a singular LLM andmakes the decision in a single round under the framework of in-contextlearning. This framework suffers the key disadvantage that the single-turnoutput generated by a single LLM might not deliver the perfect decision, justas humans sometimes need multiple attempts to get things right. This isespecially true for the task of sentiment analysis where deep reasoning isrequired to address the complex linguistic phenomenon (e.g., clausecomposition, irony, etc) in the input.  To address this issue, this paper introduces a multi-LLM negotiationframework for sentiment analysis. The framework consists of a reasoning-infusedgenerator to provide decision along with rationale, a explanation-derivingdiscriminator to evaluate the credibility of the generator. The generator andthe discriminator iterate until a consensus is reached. The proposed frameworknaturally addressed the aforementioned challenge, as we are able to take thecomplementary abilities of two LLMs, have them use rationale to persuade eachother for correction.  Experiments on a wide range of sentiment analysis benchmarks (SST-2, MovieReview, Twitter, yelp, amazon, IMDB) demonstrate the effectiveness of proposedapproach: it consistently yields better performances than the ICL baselineacross all benchmarks, and even superior performances to supervised baselineson the Twitter and movie review datasets.", "title": "sentiment analysis through llm negotiations", "url": "http://arxiv.org/pdf/2311.01876v1.pdf", "tokenized_text": "standard paradigm sentiment_analysis sentiment analysis rely singular llm decision single round framework contextlearning framework suffers key single generated single llm deliver perfect decision humans need multiple attempts things right isespecially true task sentiment_analysis sentiment analysis deep reasoning isrequired address complex linguistic phenomenon e.g. irony etc input address issue paper introduces multi llm sentiment_analysis sentiment analysis framework consists reasoning provide decision rationale explanation evaluate generator generator andthe discriminator iterate consensus reached proposed addressed aforementioned challenge able thecomplementary abilities llms use rationale correction experiments wide_range wide range sentiment_analysis sentiment analysis benchmarks twitter amazon demonstrate_the_effectiveness demonstrate effectiveness proposedapproach consistently yields better performances icl benchmarks superior performances supervised baselineson twitter movie review datasets"}
{"id": "nan", "abstract": "  Multimodal Large Language Models (MLLMs) have shown impressive abilities ininteracting with visual content with myriad potential downstream tasks.However, even though a list of benchmarks has been proposed, the capabilitiesand limitations of MLLMs are still not comprehensively understood, due to alack of a standardized and holistic evaluation framework. To this end, wepresent the first Comprehensive Evaluation Framework (ChEF) that canholistically profile each MLLM and fairly compare different MLLMs. First, westructure ChEF as four modular components, i.e., Scenario as scalablemultimodal datasets, Instruction as flexible instruction retrieving formulae,Inferencer as reliable question answering strategies, and Metric as indicativetask-specific score functions. Based on them, ChEF facilitates versatileevaluations in a standardized framework, and new evaluations can be built bydesigning new Recipes (systematic selection of these four components). Notably,current MLLM benchmarks can be readily summarized as recipes of ChEF. Second,we introduce 6 new recipes to quantify competent MLLMs' desired capabilities(or called desiderata, i.e., calibration, in-context learning, instructionfollowing, language performance, hallucination, and robustness) as reliableagents that can perform real-world multimodal interactions. Third, we conduct alarge-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata.Our evaluation summarized over 20 valuable observations concerning thegeneralizability of MLLMs across various scenarios and the composite capabilityof MLLMs required for multimodal interactions. We will publicly release all thedetailed implementations for further analysis, as well as an easy-to-usemodular toolkit for the integration of new recipes and models, so that ChEF canbe a growing evaluation framework for the MLLM community.", "title": "chef a comprehensive evaluation framework for standardized assessment of multimodal large language models", "url": "http://arxiv.org/pdf/2311.02692v1.pdf", "tokenized_text": "multimodal large_language large language mllms shown_impressive shown impressive abilities visual content myriad potential downstream_tasks downstream tasks list benchmarks proposed capabilitiesand limitations mllms comprehensively understood standardized holistic evaluation framework end wepresent comprehensive evaluation framework profile mllm compare different mllms modular components i.e. scenario datasets instruction flexible instruction retrieving reliable question_answering question answering strategies metric specific score functions based facilitates standardized framework new evaluations built new recipes systematic selection components notably current mllm benchmarks readily summarized recipes second introduce new recipes quantify competent mllms desired called i.e. calibration context_learning context learning language performance hallucination robustness perform real world multimodal interactions conduct alarge scale evaluation prominent mllms scenarios evaluation summarized 20 valuable observations concerning mllms scenarios composite capabilityof mllms required multimodal interactions publicly release implementations analysis easy toolkit integration new recipes canbe growing evaluation framework mllm community"}
{"id": "nan", "abstract": "  Generalizable articulated object manipulation is essential for home-assistantrobots. Recent efforts focus on imitation learning from demonstrations orreinforcement learning in simulation, however, due to the prohibitive costs ofreal-world data collection and precise object simulation, it still remainschallenging for these works to achieve broad adaptability across diversearticulated objects. Recently, many works have tried to utilize the strongin-context learning ability of Large Language Models (LLMs) to achievegeneralizable robotic manipulation, but most of these researches focus onhigh-level task planning, sidelining low-level robotic control. In this work,building on the idea that the kinematic structure of the object determines howwe can manipulate it, we propose a kinematic-aware prompting framework thatprompts LLMs with kinematic knowledge of objects to generate low-level motiontrajectory waypoints, supporting various object manipulation. To effectivelyprompt LLMs with the kinematic structure of different objects, we design aunified kinematic knowledge parser, which represents various articulatedobjects as a unified textual description containing kinematic joints andcontact location. Building upon this unified description, a kinematic-awareplanner model is proposed to generate precise 3D manipulation waypoints via adesigned kinematic-aware chain-of-thoughts prompting method. Our evaluationspanned 48 instances across 16 distinct categories, revealing that ourframework not only outperforms traditional methods on 8 seen categories butalso shows a powerful zero-shot capability for 8 unseen articulated objectcategories. Moreover, the real-world experiments on 7 different objectcategories prove our framework's adaptability in practical scenarios. Code isreleased at\\href{https://github.com/GeWu-Lab/LLM_articulated_object_manipulation/tree/main}{here}.", "title": "kinematicaware prompting for generalizable articulated object manipulation with llms", "url": "http://arxiv.org/pdf/2311.02847v2.pdf", "tokenized_text": "generalizable object manipulation essential home recent efforts focus imitation learning demonstrations learning simulation prohibitive costs world data collection precise object simulation works achieve broad adaptability objects recently works tried utilize context_learning context learning ability large_language large language llms robotic manipulation researches focus level task planning low level robotic control work building idea structure object determines manipulate propose aware framework thatprompts llms knowledge objects generate low level waypoints supporting object manipulation llms structure different objects design aunified knowledge parser represents unified textual description containing location building unified description proposed generate precise 3d manipulation waypoints aware chain thoughts method 48 instances 16 distinct categories revealing ourframework outperforms traditional methods seen categories butalso shows powerful zero shot capability unseen objectcategories real world experiments different objectcategories prove framework adaptability practical scenarios code lab tree"}
{"id": "nan", "abstract": "  Knowledge Base Question Answering (KBQA) aims to answer factoid questionsbased on knowledge bases. However, generating the most appropriate knowledgebase query code based on Natural Language Questions (NLQ) poses a significantchallenge in KBQA. In this work, we focus on the CCKS2023 Competition ofQuestion Answering with Knowledge Graph Inference for Unmanned Systems.Inspired by the recent success of large language models (LLMs) like ChatGPT andGPT-3 in many QA tasks, we propose a ChatGPT-based Cypher Query Language (CQL)generation framework to generate the most appropriate CQL based on the givenNLQ. Our generative framework contains six parts: an auxiliary model predictingthe syntax-related information of CQL based on the given NLQ, a proper nounmatcher extracting proper nouns from the given NLQ, a demonstration exampleselector retrieving similar examples of the input sample, a prompt constructordesigning the input template of ChatGPT, a ChatGPT-based generation modelgenerating the CQL, and an ensemble model to obtain the final answers fromdiversified outputs. With our ChatGPT-based CQL generation framework, weachieved the second place in the CCKS 2023 Question Answering with KnowledgeGraph Inference for Unmanned Systems competition, achieving an F1-score of0.92676.", "title": "incontext learning for knowledge base question answering for unmanned systems based on large language models", "url": "http://arxiv.org/pdf/2311.02956v1.pdf", "tokenized_text": "knowledge base question_answering question answering kbqa aims answer factoid knowledge bases generating appropriate knowledgebase query code based natural_language natural language questions poses kbqa work focus competition answering knowledge_graph knowledge graph inference unmanned systems inspired recent success large_language large language llms like_chatgpt like chatgpt qa tasks propose chatgpt based query language framework generate appropriate based generative framework contains parts auxiliary syntax related information based given proper extracting proper nouns given demonstration retrieving similar examples input sample input template chatgpt chatgpt based generation ensemble obtain final answers outputs chatgpt based generation framework second place 2023 question_answering question answering knowledgegraph inference unmanned systems competition achieving f1 score"}
{"id": "nan", "abstract": "  Information Extraction (IE) aims to extract structural knowledge (e.g.,entities, relations, events) from natural language texts, which bringschallenges to existing methods due to task-specific schemas and complex textexpressions. Code, as a typical kind of formalized language, is capable ofdescribing structural knowledge under various schemas in a universal way. Onthe other hand, Large Language Models (LLMs) trained on both codes and textshave demonstrated powerful capabilities of transforming texts into codes, whichprovides a feasible solution to IE tasks. Therefore, in this paper, we proposea universal retrieval-augmented code generation framework based on LLMs, calledCode4UIE, for IE tasks. Specifically, Code4UIE adopts Python classes to definetask-specific schemas of various structural knowledge in a universal way. By sodoing, extracting knowledge under these schemas can be transformed intogenerating codes that instantiate the predefined Python classes with theinformation in texts. To generate these codes more precisely, Code4UIE adoptsthe in-context learning mechanism to instruct LLMs with examples. In order toobtain appropriate examples for different tasks, Code4UIE explores severalexample retrieval strategies, which can retrieve examples semantically similarto the given texts. Extensive experiments on five representative IE tasksacross nine datasets demonstrate the effectiveness of the Code4UIE framework.", "title": "retrievalaugmented code generation for universal information extraction", "url": "http://arxiv.org/pdf/2311.02962v1.pdf", "tokenized_text": "information_extraction information extraction ie aims extract structural knowledge e.g. relations events natural_language natural language texts existing_methods existing methods task specific schemas complex code typical kind formalized language capable structural knowledge schemas universal way onthe hand large_language large language llms trained codes demonstrated powerful capabilities transforming texts codes feasible solution ie tasks paper proposea universal retrieval augmented code_generation code generation framework based llms ie tasks specifically adopts python classes specific schemas structural knowledge universal way extracting knowledge schemas transformed codes instantiate predefined python classes texts generate codes precisely context_learning context learning mechanism instruct llms examples order appropriate examples different tasks explores retrieval strategies retrieve examples semantically similarto given texts extensive_experiments extensive experiments representative ie tasksacross datasets demonstrate_the_effectiveness demonstrate effectiveness framework"}
{"id": "nan", "abstract": "  Unified Sequence Labeling that articulates different sequence labelingproblems such as Named Entity Recognition, Relation Extraction, Semantic RoleLabeling, etc. in a generalized sequence-to-sequence format opens up theopportunity to make the maximum utilization of large language model knowledgetoward structured prediction. Unfortunately, this requires formatting them intospecialized augmented format unknown to the base pretrained language model(PLMs) necessitating finetuning to the target format. This significantly boundsits usefulness in data-limited settings where finetuning large models cannotproperly generalize to the target format. To address this challenge andleverage PLM knowledge effectively, we propose FISH-DIP, a sample-aware dynamicsparse finetuning strategy that selectively focuses on a fraction ofparameters, informed by feedback from highly regressing examples, during thefine-tuning process. By leveraging the dynamism of sparsity, our approachmitigates the impact of well-learned samples and prioritizes underperforminginstances for improvement in generalization. Across five tasks of sequencelabeling, we demonstrate that FISH-DIP can smoothly optimize the model in lowresource settings offering upto 40% performance improvements over fullfine-tuning depending on target evaluation settings. Also, compared toin-context learning and other parameter-efficient fine-tuning approaches,FISH-DIP performs comparably or better, notably in extreme low-resourcesettings.", "title": "unified lowresource sequence labeling by sampleaware dynamic sparse finetuning", "url": "http://arxiv.org/pdf/2311.03748v1.pdf", "tokenized_text": "unified sequence labeling different sequence named_entity named entity recognition relation_extraction relation extraction semantic etc generalized sequence sequence format opens maximum utilization large_language large language structured prediction unfortunately requires formatting augmented format unknown base pretrained_language pretrained language necessitating finetuning target format significantly usefulness data limited settings finetuning large generalize target format address challenge plm knowledge effectively propose sample aware finetuning strategy selectively focuses fraction ofparameters informed feedback highly examples thefine tuning process leveraging sparsity impact learned samples prioritizes improvement generalization tasks demonstrate optimize lowresource settings offering upto 40 performance improvements fullfine tuning depending target evaluation settings compared toin context_learning context learning parameter efficient fine tuning approaches performs comparably better notably extreme low"}
{"id": "nan", "abstract": "  Existing pre-trained models are generally geared towards a particular classof problems. To date, there seems to be still no consensus on what the rightarchitecture and pre-training setup should be. This paper presents a unifiedframework for pre-training models that are universally effective acrossdatasets and setups. We begin by disentangling architectural archetypes withpre-training objectives -- two concepts that are commonly conflated. Next, wepresent a generalized & unified perspective for self-supervision in NLP andshow how different pre-training objectives can be cast as one another and howinterpolating between different objectives can be effective. We then proposeMixture-of-Denoisers (MoD), a pre-training objective that combines diversepre-training paradigms together. We furthermore introduce a notion of modeswitching, wherein downstream fine-tuning is associated with specificpre-training schemes. We conduct extensive ablative experiments to comparemultiple pre-training objectives and find that our method pushes thePareto-frontier by outperforming T5 & GPT-like models across multiple diversesetups. By scaling our model up to 20B parameters, we achieve SOTA performanceon 50 well-established supervised finetuning based NLP tasks. Our model alsoachieve strong results at in-context learning, outperforming 175B GPT-3 onzero-shot SuperGLUE and tripling the performance of T5-XXL on one-shotsummarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20Balso works well with chain-of-thought prompting and reasoning, making it anappealing choice for research into reasoning at a small to medium scale of 20Bparameters. Finally, we apply FLAN instruction tuning to the UL2 20B model,achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We releaseFlax-based T5X checkpoints for the UL2 20B & Flan-UL2 20B.", "title": "ul2 unifying language learning paradigms", "url": "http://arxiv.org/pdf/2205.05131v3.pdf", "tokenized_text": "existing pre trained generally geared particular problems date consensus pre training setup paper_presents paper presents unifiedframework pre training effective acrossdatasets setups begin disentangling architectural withpre training objectives concepts commonly wepresent generalized unified perspective self supervision nlp different pre training objectives cast different objectives effective pre training objective combines training paradigms furthermore introduce notion downstream fine tuning associated training schemes conduct_extensive conduct extensive experiments pre training objectives find method pushes frontier outperforming t5 gpt like multiple scaling 20b parameters achieve sota performanceon 50 established supervised finetuning based nlp_tasks nlp tasks strong results context_learning context learning outperforming 175b gpt-3 onzero shot superglue performance t5 xxl shotsummarization shot mmlu 20b outperforms t0 t5 works chain thought_prompting thought reasoning making choice research reasoning small medium scale finally apply flan instruction_tuning instruction tuning 20b achieving mmlu big bench scores competitive flan palm based checkpoints 20b flan"}
{"id": "nan", "abstract": "  Foundation models have shown impressive adaptation and scalability insupervised and self-supervised learning problems, but so far these successeshave not fully translated to reinforcement learning (RL). In this work, wedemonstrate that training an RL agent at scale leads to a general in-contextlearning algorithm that can adapt to open-ended novel embodied 3D problems asquickly as humans. In a vast space of held-out environment dynamics, ouradaptive agent (AdA) displays on-the-fly hypothesis-driven exploration,efficient exploitation of acquired knowledge, and can successfully be promptedwith first-person demonstrations. Adaptation emerges from three ingredients:(1) meta-reinforcement learning across a vast, smooth and diverse taskdistribution, (2) a policy parameterised as a large-scale attention-basedmemory architecture, and (3) an effective automated curriculum that prioritisestasks at the frontier of an agent's capabilities. We demonstrate characteristicscaling laws with respect to network size, memory length, and richness of thetraining task distribution. We believe our results lay the foundation forincreasingly general and adaptive RL agents that perform well acrossever-larger open-ended domains.", "title": "humantimescale adaptation in an openended task space", "url": "http://arxiv.org/pdf/2301.07608v1.pdf", "tokenized_text": "foundation_models foundation shown_impressive shown impressive adaptation scalability self supervised learning problems far fully translated reinforcement_learning reinforcement learning rl work wedemonstrate training rl agent scale leads general contextlearning algorithm adapt open ended novel embodied 3d problems humans vast space held environment dynamics agent ada displays fly hypothesis driven exploration efficient exploitation acquired knowledge successfully person demonstrations adaptation emerges meta reinforcement_learning reinforcement learning vast smooth diverse policy large scale attention architecture effective automated curriculum frontier agent capabilities demonstrate laws respect network size memory length richness thetraining task distribution believe results foundation general adaptive rl agents perform larger open ended domains"}
{"id": "nan", "abstract": "  The digitization of healthcare has facilitated the sharing and re-using ofmedical data but has also raised concerns about confidentiality and privacy.HIPAA (Health Insurance Portability and Accountability Act) mandates removingre-identifying information before the dissemination of medical records. Thus,effective and efficient solutions for de-identifying medical data, especiallythose in free-text forms, are highly needed. While various computer-assistedde-identification methods, including both rule-based and learning-based, havebeen developed and used in prior practice, such solutions still lackgeneralizability or need to be fine-tuned according to different scenarios,significantly imposing restrictions in wider use. The advancement of largelanguage models (LLM), such as ChatGPT and GPT-4, have shown great potential inprocessing text data in the medical domain with zero-shot in-context learning,especially in the task of privacy protection, as these models can identifyconfidential information by their powerful named entity recognition (NER)capability. In this work, we developed a novel GPT4-enabled de-identificationframework (\"DeID-GPT\") to automatically identify and remove the identifyinginformation. Compared to existing commonly used medical text datade-identification methods, our developed DeID-GPT showed the highest accuracyand remarkable reliability in masking private information from the unstructuredmedical text while preserving the original structure and meaning of the text.This study is one of the earliest to utilize ChatGPT and GPT-4 for medical textdata processing and de-identification, which provides insights for furtherresearch and solution development on the use of LLMs such as ChatGPT/GPT-4 inhealthcare. Codes and benchmarking data information are available athttps://github.com/yhydhx/ChatGPT-API.", "title": "deidgpt zeroshot medical text deidentification by gpt4", "url": "http://arxiv.org/pdf/2303.11032v1.pdf", "tokenized_text": "healthcare facilitated sharing data raised concerns privacy hipaa act identifying information dissemination medical records effective efficient solutions de identifying medical data free text forms highly needed computer identification methods including rule based learning based havebeen developed prior practice solutions need fine tuned according different scenarios significantly imposing restrictions wider use advancement largelanguage_models largelanguage llm chatgpt gpt-4 shown great_potential great potential text data medical domain zero shot context_learning context learning especially task privacy protection information powerful named_entity named entity recognition work developed novel gpt4 enabled de gpt automatically identify remove compared existing commonly medical text identification methods developed gpt showed highest remarkable reliability masking private information text preserving original structure meaning text study utilize chatgpt gpt-4 medical processing de identification provides insights solution development use llms chatgpt gpt-4 codes benchmarking data information available"}
{"id": "nan", "abstract": "  Artificial Intelligence (AI) has made incredible progress recently. On theone hand, advanced foundation models like ChatGPT can offer powerfulconversation, in-context learning and code generation abilities on a broadrange of open-domain tasks. They can also generate high-level solution outlinesfor domain-specific tasks based on the common sense knowledge they haveacquired. However, they still face difficulties with some specialized tasksbecause they lack enough domain-specific data during pre-training or they oftenhave errors in their neural network computations on those tasks that needaccurate executions. On the other hand, there are also many existing models andsystems (symbolic-based or neural-based) that can do some domain-specific tasksvery well. However, due to the different implementation or working mechanisms,they are not easily accessible or compatible with foundation models. Therefore,there is a clear and pressing need for a mechanism that can leverage foundationmodels to propose task solution outlines and then automatically match some ofthe sub-tasks in the outlines to the off-the-shelf models and systems withspecial functionalities to complete them. Inspired by this, we introduceTaskMatrix.AI as a new AI ecosystem that connects foundation models withmillions of APIs for task completion. Unlike most previous work that aimed toimprove a single AI model, TaskMatrix.AI focuses more on using existingfoundation models (as a brain-like central system) and APIs of other AI modelsand systems (as sub-task solvers) to achieve diversified tasks in both digitaland physical domains. As a position paper, we will present our vision of how tobuild such an ecosystem, explain each key component, and use study cases toillustrate both the feasibility of this vision and the main challenges we needto address next.", "title": "taskmatrixai completing tasks by connecting foundation models with millions of apis", "url": "http://arxiv.org/pdf/2303.16434v1.pdf", "tokenized_text": "artificial_intelligence artificial intelligence ai incredible progress recently theone hand advanced foundation_models foundation like_chatgpt like chatgpt offer context_learning context learning code_generation code generation abilities broadrange open domain tasks generate high level solution domain specific tasks based common sense knowledge face difficulties specialized lack domain specific data pre training errors neural network computations tasks hand existing symbolic based neural based domain specific different implementation working mechanisms easily accessible compatible foundation_models foundation clear pressing need mechanism leverage foundationmodels propose task solution outlines automatically match ofthe sub tasks outlines shelf systems functionalities complete inspired ai new ai ecosystem connects foundation_models foundation apis task completion unlike previous work aimed toimprove single ai ai focuses brain like central system apis ai modelsand systems sub task solvers achieve diversified tasks physical domains position paper present vision tobuild ecosystem explain key component use study cases feasibility vision main challenges needto address"}
{"id": "nan", "abstract": "  Recent text-to-image generation models like DreamBooth have made remarkableprogress in generating highly customized images of a target subject, byfine-tuning an ``expert model'' for a given subject from a few examples.However, this process is expensive, since a new expert model must be learnedfor each subject. In this paper, we present SuTI, a Subject-drivenText-to-Image generator that replaces subject-specific fine tuning within-context learning. Given a few demonstrations of a new subject, SuTI caninstantly generate novel renditions of the subject in different scenes, withoutany subject-specific optimization. SuTI is powered by apprenticeship learning,where a single apprentice model is learned from data generated by a massivenumber of subject-specific expert models. Specifically, we mine millions ofimage clusters from the Internet, each centered around a specific visualsubject. We adopt these clusters to train a massive number of expert models,each specializing in a different subject. The apprentice model SuTI then learnsto imitate the behavior of these fine-tuned experts. SuTI can generatehigh-quality and customized subject-specific images 20x faster thanoptimization-based SoTA methods. On the challenging DreamBench andDreamBench-v2, our human evaluation shows that SuTI significantly outperformsexisting models like InstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt,Re-Imagen and DreamBooth, especially on the subject and text alignment aspects.", "title": "subjectdriven texttoimage generation via apprenticeship learning", "url": "http://arxiv.org/pdf/2304.00186v5.pdf", "tokenized_text": "recent text image_generation image generation like remarkableprogress generating highly customized images target subject byfine tuning expert given subject examples process expensive new expert subject paper present subject image generator replaces subject specific fine_tuning fine tuning context_learning context learning given demonstrations new subject generate novel subject different scenes withoutany subject specific optimization powered learning single learned data generated subject specific expert specifically millions clusters internet centered specific adopt clusters train massive number expert specializing different subject imitate behavior fine tuned experts quality customized subject specific images 20x faster based sota methods challenging v2 human evaluation shows significantly outperformsexisting like instructpix2pix textual inversion especially subject text alignment aspects"}
{"id": "nan", "abstract": "  Deep Learning (DL) library bugs affect downstream DL applications,emphasizing the need for reliable systems. Generating valid input programs forfuzzing DL libraries is challenging due to the need for satisfying bothlanguage syntax/semantics and constraints for constructing valid computationalgraphs. Recently, the TitanFuzz work demonstrates that modern Large LanguageModels (LLMs) can be directly leveraged to implicitly learn all the constraintsto generate valid DL programs for fuzzing. However, LLMs tend to generateordinary programs following similar patterns seen in their massive trainingcorpora, while fuzzing favors unusual inputs that cover edge cases or areunlikely to be manually produced.  To fill this gap, this paper proposes FuzzGPT, the first technique to primeLLMs to synthesize unusual programs for fuzzing. FuzzGPT is built on thewell-known hypothesis that historical bug-triggering programs may includerare/valuable code ingredients important for bug finding. Traditionaltechniques leveraging such historical information require intensive humanefforts to design dedicated generators and ensure the validity of generatedprograms. FuzzGPT demonstrates that this process can be fully automated via theintrinsic capabilities of LLMs (including fine-tuning and in-context learning),while being generalizable and applicable to challenging domains. While FuzzGPTcan be applied with different LLMs, this paper focuses on the powerfulGPT-style models: Codex and CodeGen. Moreover, FuzzGPT also shows the potentialof directly leveraging the instruct-following capability of the recent ChatGPTfor effective fuzzing. Evaluation on two popular DL libraries (PyTorch andTensorFlow) shows that FuzzGPT can substantially outperform TitanFuzz,detecting 76 bugs, with 49 already confirmed as previously unknown bugs,including 11 high-priority bugs or security vulnerabilities.", "title": "large language models are edgecase fuzzers testing deep learning libraries via fuzzgpt", "url": "http://arxiv.org/pdf/2304.02014v1.pdf", "tokenized_text": "deep learning dl library bugs affect downstream dl applications emphasizing need reliable systems generating valid input programs dl libraries challenging need satisfying bothlanguage syntax semantics constraints constructing valid recently work demonstrates modern large_languagemodels large languagemodels llms directly leveraged implicitly learn generate valid dl programs fuzzing llms tend programs following similar patterns seen massive fuzzing favors unusual inputs cover edge cases manually produced fill gap paper_proposes paper proposes technique synthesize unusual programs fuzzing built known hypothesis historical bug triggering programs valuable code ingredients important bug finding leveraging historical information require intensive design dedicated generators ensure validity demonstrates process fully automated capabilities llms including fine tuning context generalizable applicable challenging domains applied different llms paper focuses style codex codegen shows potentialof directly leveraging instruct following capability recent chatgptfor effective fuzzing evaluation popular dl libraries pytorch shows substantially outperform detecting 76 bugs 49 confirmed previously unknown bugs including 11 high priority bugs security vulnerabilities"}
{"id": "nan", "abstract": "  We study whether multiple large language models (LLMs) can autonomouslyimprove each other in a negotiation game by playing, reflecting, andcriticizing. We are interested in this question because if LLMs were able toimprove each other, it would imply the possibility of creating strong AI agentswith minimal human intervention. We ask two LLMs to negotiate with each other,playing the roles of a buyer and a seller, respectively. They aim to reach adeal with the buyer targeting a lower price and the seller a higher one. Athird language model, playing the critic, provides feedback to a player toimprove the player's negotiation strategies. We let the two agents playmultiple rounds, using previous negotiation history and AI feedback asin-context demonstrations to improve the model's negotiation strategyiteratively. We use different LLMs (GPT and Claude) for different roles and usethe deal price as the evaluation metric. Our experiments reveal multipleintriguing findings: (1) Only a subset of the language models we consider canself-play and improve the deal price from AI feedback, weaker models either donot understand the game's rules or cannot incorporate AI feedback for furtherimprovement. (2) Models' abilities to learn from the feedback differ whenplaying different roles. For example, it is harder for Claude-instant toimprove as the buyer than as the seller. (3) When unrolling the game tomultiple rounds, stronger agents can consistently improve their performance bymeaningfully using previous experiences and iterative AI feedback, yet have ahigher risk of breaking the deal. We hope our work provides insightful initialexplorations of having models autonomously improve each other with game playingand AI feedback.", "title": "improving language model negotiation with selfplay and incontext learning from ai feedback", "url": "http://arxiv.org/pdf/2305.10142v1.pdf", "tokenized_text": "study multiple large_language large language llms negotiation game playing reflecting interested question llms able toimprove imply possibility creating strong ai minimal human intervention ask llms playing roles respectively aim reach targeting lower price higher language_model language playing critic provides feedback player toimprove player negotiation strategies let agents rounds previous negotiation history ai feedback asin context demonstrations improve negotiation use different llms gpt claude different roles deal price evaluation metric experiments reveal findings subset language_models language consider play improve deal price ai feedback weaker understand game rules incorporate ai feedback abilities learn feedback differ different roles example harder claude toimprove game rounds stronger agents consistently improve performance previous experiences iterative ai feedback risk breaking deal hope work provides insightful having autonomously improve game ai feedback"}
{"id": "nan", "abstract": "  Data scarcity is a crucial issue for the development of highly multilingualNLP systems. Yet for many under-represented languages (ULs) -- languages forwhich NLP re-search is particularly far behind in meeting user needs -- it isfeasible to annotate small amounts of data. Motivated by this, we proposeXTREME-UP, a benchmark defined by: its focus on the scarce-data scenario ratherthan zero-shot; its focus on user-centric tasks -- tasks with broad adoption byspeakers of high-resource languages; and its focus on under-representedlanguages where this scarce-data scenario tends to be most realistic. XTREME-UPevaluates the capabilities of language models across 88 under-representedlanguages over 9 key user-centric technologies including ASR, OCR, MT, andinformation access tasks that are of general utility. We create new datasetsfor OCR, autocomplete, semantic parsing, and transliteration, and build on andrefine existing datasets for other tasks. XTREME-UP provides methodology forevaluating many modeling scenarios including text-only, multi-modal (vision,audio, and text),supervised parameter tuning, and in-context learning. Weevaluate commonly used models on the benchmark. We release all code and scriptsto train and evaluate models", "title": "xtremeup a usercentric scarcedata benchmark for underrepresented languages", "url": "http://arxiv.org/pdf/2305.11938v2.pdf", "tokenized_text": "data scarcity crucial issue development highly systems represented languages languages forwhich nlp search particularly far meeting user needs annotate small amounts data motivated benchmark defined focus scarce data scenario ratherthan zero shot focus user centric tasks tasks broad adoption high resource_languages resource languages focus scarce data scenario tends realistic capabilities language_models language 88 key user centric technologies including asr ocr mt access tasks general utility create new ocr semantic_parsing semantic parsing build existing datasets tasks provides methodology modeling scenarios including text multi modal vision audio parameter tuning context_learning context learning weevaluate commonly benchmark release code train evaluate"}
{"id": "nan", "abstract": "  Large language models (LLMs) face the challenges in fine-tuning anddeployment due to their high memory demands and computational costs. Whileparameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usageof the optimizer state during fine-tuning, the inherent size of pre-trained LLMweights continues to be a pressing concern. Even though quantization techniquesare widely proposed to ease memory demands and accelerate LLM inference, mostof these techniques are geared towards the deployment phase. To bridge thisgap, this paper presents Parameter-Efficient and Quantization-aware Adaptation(PEQA) - a simple yet effective method that combines the advantages of PEFTwith quantized LLMs. By updating solely the quantization scales, PEQA can bedirectly applied to quantized LLMs, ensuring seamless task transitions.Parallel to existing PEFT methods, PEQA significantly reduces the memoryoverhead associated with the optimizer state. Furthermore, it leverages theadvantages of quantization to substantially reduce model sizes. Even afterfine-tuning, the quantization structure of a PEQA-tuned LLM remains intact,allowing for accelerated inference on the deployment stage. We employPEQA-tuning for task-specific adaptation on LLMs with up to 65 billionparameters. To assess the logical reasoning and language comprehension ofPEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instructiondataset. Our results show that even when LLMs are quantized to below 4-bitprecision, their capabilities in language modeling, few-shot in-contextlearning, and comprehension can be resiliently restored to (or even improvedover) their full-precision original performances with PEQA.", "title": "memoryefficient finetuning of compressed large language models via sub4bit integer quantization", "url": "http://arxiv.org/pdf/2305.14152v2.pdf", "tokenized_text": "large_language large language llms face challenges fine tuning anddeployment high memory demands computational costs efficient fine tuning peft methods aim reduce memory usageof optimizer state fine tuning inherent size pre trained continues pressing concern quantization widely proposed ease memory demands accelerate llm inference mostof techniques geared deployment phase bridge paper_presents paper presents parameter efficient quantization aware simple effective method combines advantages quantized llms updating solely quantization scales bedirectly applied quantized llms ensuring seamless task transitions parallel existing peft methods significantly reduces associated optimizer state furthermore leverages quantization substantially reduce sizes afterfine tuning quantization structure tuned llm remains intact allowing accelerated inference deployment stage tuning task specific adaptation llms 65 billionparameters assess logical reasoning language comprehension tuned llms fine tune low bit quantized llms results llms quantized capabilities language modeling shot contextlearning comprehension precision original performances"}
{"id": "nan", "abstract": "  We present the training recipe and results of scaling up PaLI-X, amultilingual vision and language model, both in terms of size of the componentsand the breadth of its training task mixture. Our model achieves new levels ofperformance on a wide-range of varied and complex tasks, including multipleimage-based captioning and question-answering tasks, image-based documentunderstanding and few-shot (in-context) learning, as well as object detection,video question answering, and video captioning. PaLI-X advances thestate-of-the-art on most vision-and-language benchmarks considered (25+ ofthem). Finally, we observe emerging capabilities, such as complex counting andmultilingual object detection, tasks that are not explicitly in the trainingmix.", "title": "palix on scaling up a multilingual vision and language model", "url": "http://arxiv.org/pdf/2305.18565v1.pdf", "tokenized_text": "present training recipe results scaling vision language_model language terms size breadth training task mixture achieves new levels wide range varied complex tasks including based captioning question answering tasks image based shot context learning object_detection object detection video question_answering question answering video captioning advances thestate art vision language benchmarks considered 25 finally observe emerging capabilities complex counting object_detection object detection tasks explicitly"}
{"id": "nan", "abstract": "  Neural sequence models based on the transformer architecture havedemonstrated remarkable \\emph{in-context learning} (ICL) abilities, where theycan perform new tasks when prompted with training and test examples, withoutany parameter update to the model. This work first provides a comprehensivestatistical theory for transformers to perform ICL. Concretely, we show thattransformers can implement a broad class of standard machine learningalgorithms in context, such as least squares, ridge regression, Lasso, learninggeneralized linear models, and gradient descent on two-layer neural networks,with near-optimal predictive power on various in-context data distributions.Using an efficient implementation of in-context gradient descent as theunderlying mechanism, our transformer constructions admit mild size bounds, andcan be learned with polynomially many pretraining sequences.  Building on these ``base'' ICL algorithms, intriguingly, we show thattransformers can implement more complex ICL procedures involving\\emph{in-context algorithm selection}, akin to what a statistician can do inreal life -- A \\emph{single} transformer can adaptively select different baseICL algorithms -- or even perform qualitatively different tasks -- on differentinput sequences, without any explicit prompting of the right algorithm or task.We both establish this in theory by explicit constructions, and also observethis phenomenon experimentally. In theory, we construct two general mechanismsfor algorithm selection with concrete examples: pre-ICL testing, and post-ICLvalidation. As an example, we use the post-ICL validation mechanism toconstruct a transformer that can perform nearly Bayes-optimal ICL on achallenging task -- noisy linear models with mixed noise levels.Experimentally, we demonstrate the strong in-context algorithm selectioncapabilities of standard transformer architectures.", "title": "transformers as statisticians provable incontext learning with incontext algorithm selection", "url": "http://arxiv.org/pdf/2306.04637v2.pdf", "tokenized_text": "neural sequence based transformer architecture havedemonstrated remarkable context_learning context learning icl abilities theycan perform new tasks prompted training test examples withoutany parameter update work provides theory transformers perform icl concretely thattransformers implement broad class standard machine learningalgorithms context squares ridge regression lasso linear gradient descent layer neural_networks neural networks near optimal predictive power context data distributions efficient implementation context gradient descent theunderlying mechanism transformer constructions mild size bounds andcan learned pretraining sequences building base icl algorithms intriguingly thattransformers implement complex icl procedures context algorithm selection akin inreal life transformer adaptively select different algorithms perform qualitatively different tasks sequences explicit right algorithm task establish theory explicit constructions phenomenon experimentally theory construct general algorithm selection concrete examples pre icl testing post example use post icl validation mechanism toconstruct transformer perform nearly bayes optimal icl achallenging task noisy linear mixed noise levels experimentally demonstrate strong context algorithm selectioncapabilities standard transformer architectures"}
{"id": "nan", "abstract": "  Instruction tuning of language models has demonstrated the ability to enhancemodel generalization to unseen tasks via in-context learning using a fewexamples. However, typical supervised learning still requires a plethora ofdownstream training data for finetuning. Often in real-world situations, thereis a scarcity of data available for finetuning, falling somewhere between fewshot inference and fully supervised finetuning. In this work, we demonstratethe sample efficiency of instruction tuned models over various tasks byestimating the minimal downstream training data required by them to performtransfer learning and match the performance of state-of-the-art (SOTA)supervised models. We conduct experiments on 119 tasks from Super NaturalInstructions (SuperNI) in both the single task learning (STL) and multi tasklearning (MTL) settings. Our findings reveal that, in the STL setting,instruction tuned models equipped with 25% of the downstream train data surpassthe SOTA performance on the downstream tasks. In the MTL setting, aninstruction tuned model trained on only 6% of downstream training data achieveSOTA, while using 100% of the training data results in a 3.69% pointsimprovement (ROUGE-L 74.68) over the previous SOTA. We conduct an analysis onT5 vs Tk-Instruct by developing several baselines to demonstrate thatinstruction tuning aids in increasing both sample efficiency and transferlearning. Additionally, we observe a consistent ~4% performance increase inboth settings when pre-finetuning is performed with instructions. Finally, weconduct a categorical study and find that contrary to previous results, tasksin the question rewriting and title generation categories suffer frominstruction tuning.", "title": "instruction tuned models are quick learners", "url": "http://arxiv.org/pdf/2306.05539v1.pdf", "tokenized_text": "instruction_tuning instruction tuning language_models language demonstrated ability generalization unseen tasks context_learning context learning fewexamples typical supervised learning requires ofdownstream training_data training data finetuning real world situations thereis scarcity data available finetuning fewshot inference fully_supervised fully supervised finetuning work demonstratethe sample efficiency instruction tuned tasks minimal downstream training_data training data required learning match performance state art conduct experiments tasks super naturalinstructions single task learning multi tasklearning mtl settings findings reveal setting instruction tuned equipped 25 downstream train data sota performance downstream_tasks downstream tasks mtl setting aninstruction tuned trained downstream training_data training data 100 training_data training data results rouge previous sota conduct analysis vs tk instruct developing baselines demonstrate thatinstruction tuning aids increasing sample efficiency transferlearning additionally observe consistent ~4 performance increase inboth settings pre finetuning performed instructions finally weconduct categorical study find contrary previous results question rewriting title generation categories suffer frominstruction tuning"}
{"id": "nan", "abstract": "  Building agents using large language models (LLMs) to control computers is anemerging research field, where the agent perceives computer states and performsactions to accomplish complex tasks. Previous computer agents have demonstratedthe benefits of in-context learning (ICL); however, their performance ishindered by several issues. First, the limited context length of LLMs andcomplex computer states restrict the number of exemplars, as a single webpagecan consume the entire context. Second, the exemplars in current methods, suchas high-level plans and multi-choice questions, cannot represent completetrajectories, leading to suboptimal performance in tasks that require manysteps or repeated actions. Third, existing computer agents rely ontask-specific exemplars and overlook the similarity among tasks, resulting inpoor generalization to novel tasks. To address these challenges, we introduceSynapse, featuring three key components: i) state abstraction, which filtersout task-irrelevant information from raw states, allowing more exemplars withinthe limited context, ii) trajectory-as-exemplar prompting, which prompts theLLM with complete trajectories of the abstracted states and actions forimproved multi-step decision-making, and iii) exemplar memory, which stores theembeddings of exemplars and retrieves them via similarity search forgeneralization to novel tasks. We evaluate Synapse on MiniWoB++, a standardtask suite, and Mind2Web, a real-world website benchmark. In MiniWoB++, Synapseachieves a 99.2% average success rate (a 10% relative improvement) across 64tasks using demonstrations from only 48 tasks. Notably, Synapse is the firstICL method to solve the book-flight task in MiniWoB++. Synapse also exhibits a53% relative improvement in average step success rate over the previousstate-of-the-art prompting scheme in Mind2Web.", "title": "synapse trajectoryasexemplar prompting with memory for computer control", "url": "http://arxiv.org/pdf/2306.07863v2.pdf", "tokenized_text": "building agents large_language large language llms control computers research field agent perceives computer states accomplish complex tasks previous computer agents demonstratedthe benefits context_learning context learning icl performance ishindered issues limited context length llms andcomplex computer states restrict number exemplars single consume entire context second exemplars current methods suchas high level plans multi choice questions represent leading suboptimal performance tasks require repeated actions existing computer agents rely ontask specific exemplars overlook similarity tasks resulting generalization novel tasks address challenges featuring key components state abstraction task irrelevant information raw states allowing exemplars withinthe limited context ii trajectory exemplar thellm complete trajectories abstracted states actions forimproved multi step decision making iii exemplar memory stores exemplars retrieves similarity search forgeneralization novel tasks evaluate miniwob++ suite real world website benchmark miniwob++ average success_rate success rate 10 relative improvement demonstrations 48 tasks notably method solve book flight task miniwob++ exhibits relative improvement average step success_rate success rate art scheme"}
{"id": "nan", "abstract": "  Large language models (LLMs) have demonstrated exciting progress in acquiringdiverse new capabilities through in-context learning, ranging from logicalreasoning to code-writing. Robotics researchers have also explored using LLMsto advance the capabilities of robotic control. However, since low-level robotactions are hardware-dependent and underrepresented in LLM training corpora,existing efforts in applying LLMs to robotics have largely treated LLMs assemantic planners or relied on human-engineered control primitives to interfacewith the robot. On the other hand, reward functions are shown to be flexiblerepresentations that can be optimized for control policies to achieve diversetasks, while their semantic richness makes them suitable to be specified byLLMs. In this work, we introduce a new paradigm that harnesses this realizationby utilizing LLMs to define reward parameters that can be optimized andaccomplish variety of robotic tasks. Using reward as the intermediate interfacegenerated by LLMs, we can effectively bridge the gap between high-levellanguage instructions or corrections to low-level robot actions. Meanwhile,combining this with a real-time optimizer, MuJoCo MPC, empowers an interactivebehavior creation experience where users can immediately observe the resultsand provide feedback to the system. To systematically evaluate the performanceof our proposed method, we designed a total of 17 tasks for a simulatedquadruped robot and a dexterous manipulator robot. We demonstrate that ourproposed method reliably tackles 90% of the designed tasks, while a baselineusing primitive skills as the interface with Code-as-policies achieves 50% ofthe tasks. We further validated our method on a real robot arm where complexmanipulation skills such as non-prehensile pushing emerge through ourinteractive system.", "title": "language to rewards for robotic skill synthesis", "url": "http://arxiv.org/pdf/2306.08647v2.pdf", "tokenized_text": "large_language large language llms demonstrated exciting progress new capabilities context_learning context learning ranging logicalreasoning code writing robotics researchers explored llmsto advance capabilities robotic control low level hardware dependent underrepresented llm training corpora existing efforts applying llms robotics largely treated llms planners relied human engineered control primitives robot hand reward functions shown optimized control policies achieve diversetasks semantic richness makes suitable specified byllms work introduce new_paradigm new paradigm harnesses utilizing llms define reward parameters optimized variety robotic tasks reward intermediate llms effectively bridge gap high instructions corrections low level robot actions combining real time optimizer mujoco mpc empowers creation experience users immediately observe provide feedback system systematically evaluate performanceof proposed_method proposed method designed total 17 tasks robot dexterous manipulator robot demonstrate ourproposed method reliably tackles 90 designed tasks primitive skills interface code policies achieves 50 ofthe tasks validated method real robot arm skills non pushing emerge system"}
{"id": "nan", "abstract": "  Attention-based neural networks such as transformers have demonstrated aremarkable ability to exhibit in-context learning (ICL): Given a short promptsequence of tokens from an unseen task, they can formulate relevant per-tokenand next-token predictions without any parameter updates. By embedding asequence of labeled training data and unlabeled test data as a prompt, thisallows for transformers to behave like supervised learning algorithms. Indeed,recent work has shown that when training transformer architectures over randominstances of linear regression problems, these models' predictions mimic thoseof ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, weinvestigate the dynamics of ICL in transformers with a single linearself-attention layer trained by gradient flow on linear regression tasks. Weshow that despite non-convexity, gradient flow with a suitable randominitialization finds a global minimum of the objective function. At this globalminimum, when given a test prompt of labeled examples from a new predictiontask, the transformer achieves prediction error competitive with the bestlinear predictor over the test prompt distribution. We additionallycharacterize the robustness of the trained transformer to a variety ofdistribution shifts and show that although a number of shifts are tolerated,shifts in the covariate distribution of the prompts are not. Motivated by this,we consider a generalized ICL setting where the covariate distributions canvary across prompts. We show that although gradient flow succeeds at finding aglobal minimum in this setting, the trained transformer is still brittle undermild covariate shifts. We complement this finding with experiments on large,nonlinear transformer architectures which we show are more robust undercovariate shifts.", "title": "trained transformers learn linear models incontext", "url": "http://arxiv.org/pdf/2306.09927v3.pdf", "tokenized_text": "attention based neural_networks neural networks transformers demonstrated ability exhibit context_learning context learning icl given short promptsequence tokens unseen task formulate relevant token predictions parameter updates embedding asequence labeled training_data training data unlabeled test data transformers behave like supervised learning algorithms recent_work recent work shown training transformer architectures linear regression problems predictions mimic ordinary squares understanding mechanisms underlying phenomenon weinvestigate dynamics icl transformers single attention layer trained gradient flow linear regression tasks weshow despite non gradient flow suitable finds global minimum objective function given test labeled examples new predictiontask transformer achieves prediction error competitive predictor test distribution robustness trained transformer variety ofdistribution shifts number shifts shifts distribution motivated consider generalized icl setting distributions canvary gradient flow succeeds finding minimum setting trained transformer brittle shifts complement finding experiments large transformer architectures robust shifts"}
{"id": "nan", "abstract": "  Genomic (DNA) sequences encode an enormous amount of information for generegulation and protein synthesis. Similar to natural language models,researchers have proposed foundation models in genomics to learn generalizablefeatures from unlabeled genome data that can then be fine-tuned for downstreamtasks such as identifying regulatory elements. Due to the quadratic scaling ofattention, previous Transformer-based genomic models have used 512 to 4k tokensas context (<0.001% of the human genome), significantly limiting the modelingof long-range interactions in DNA. In addition, these methods rely ontokenizers or fixed k-mers to aggregate meaningful DNA units, losing singlenucleotide resolution where subtle genetic variations can completely alterprotein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, alarge language model based on implicit convolutions was shown to matchattention in quality while allowing longer context lengths and lower timecomplexity. Leveraging Hyena's new long-range capabilities, we presentHyenaDNA, a genomic foundation model pretrained on the human reference genomewith context lengths of up to 1 million tokens at the single nucleotide-level -an up to 500x increase over previous dense attention-based models. HyenaDNAscales sub-quadratically in sequence length (training up to 160x faster thanTransformer), uses single nucleotide tokens, and has full global context ateach layer. We explore what longer context enables - including the first use ofin-context learning in genomics. On fine-tuned benchmarks from the NucleotideTransformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasetsusing a model with orders of magnitude less parameters and pretraining data. Onthe GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by+10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.", "title": "hyenadna longrange genomic sequence modeling at single nucleotide resolution", "url": "http://arxiv.org/pdf/2306.15794v2.pdf", "tokenized_text": "genomic sequences encode enormous information protein synthesis similar natural_language natural language researchers proposed foundation_models foundation genomics learn unlabeled genome data fine tuned downstreamtasks identifying regulatory elements quadratic scaling previous transformer based genomic context human genome significantly limiting long range interactions addition methods rely fixed aggregate meaningful units resolution subtle genetic variations completely function single recently alarge language_model language based implicit shown quality allowing longer context lengths lower leveraging new long range capabilities genomic foundation pretrained human reference context lengths million tokens single level 500x increase previous dense attention based sub sequence length training faster uses single tokens global context layer explore longer context enables including use ofin context_learning context learning genomics fine tuned benchmarks reaches state art sota 12 18 orders magnitude parameters pretraining data onthe surpasses sota datasets average accuracy points code"}
{"id": "nan", "abstract": "  Python is a popular dynamic programming language, evidenced by its ranking asthe second most commonly used language on GitHub. However, its dynamic typesystem can lead to potential type errors, leading researchers to exploreautomatic type inference approaches for Python programs. The rule-based typeinference approaches can ensure the accuracy of predicted variable types, butthey suffer from low coverage problems. Supervised type inference approaches,while feature-agnostic, require large, high-quality annotated datasets and arelimited to pre-defined types. As zero-shot approaches, the cloze-styleapproaches reformulate the type inference problem into a fill-in-the-blankproblem. However, their performance is limited.  This paper introduces TypeGen, a few-shot generative type inference approachthat incorporates static domain knowledge from static analysis. TypeGen createschain-of-thought (COT) prompts by translating the type inference steps ofstatic analysis into prompts based on the type dependency graphs (TDGs),enabling language models to learn from how static analysis infers types. Bycombining COT prompts with code slices and type hints, TypeGen constructsexample prompts from human annotations. TypeGen only requires very fewannotated examples to teach language models to generate similar COT prompts viain-context learning. Moreover, TypeGen enhances the interpretability of resultsthrough the use of the input-explanation-output strategy. Experiments show thatTypeGen outperforms the best baseline Type4Py by 10.0% for argument typeprediction and 22.5% in return value type prediction in terms of top-1 ExactMatch by using only five examples. Furthermore, TypeGen achieves substantialimprovements of 27% to 84% compared to the zero-shot performance of largelanguage models with parameter sizes ranging from 1.3B to 175B in terms oftop-1 Exact Match.", "title": "generative type inference for python", "url": "http://arxiv.org/pdf/2307.09163v1.pdf", "tokenized_text": "python popular dynamic programming language ranking asthe second commonly language github dynamic lead potential type errors leading researchers type inference approaches python programs rule based approaches ensure accuracy predicted variable types suffer low coverage problems supervised type inference approaches feature agnostic require large high quality annotated datasets arelimited pre defined types zero shot approaches cloze reformulate type inference problem fill performance limited paper introduces shot generative type inference approachthat incorporates static domain knowledge static analysis thought cot translating type inference steps analysis based type dependency graphs language_models language learn static analysis infers types bycombining cot code slices type hints human annotations requires examples teach language_models language generate similar cot viain context_learning context learning enhances interpretability use input explanation output strategy experiments outperforms best baseline 10.0 argument 22.5 return value type prediction terms top-1 examples furthermore achieves substantialimprovements 27 84 compared zero shot performance largelanguage_models largelanguage parameter sizes ranging 1.3b 175b terms exact match"}
{"id": "nan", "abstract": "  Entity Matching is the task of deciding whether two entity descriptions referto the same real-world entity. Entity Matching is a central step in most dataintegration pipelines and an enabler for many e-commerce applications whichrequire to match products offers from different vendors. State-of-the-artentity matching methods often rely on pre-trained language models (PLMs) suchas BERT or RoBERTa. Two major drawbacks of these models for entity matching arethat (i) the models require significant amounts of task-specific training dataand (ii) the fine-tuned models are not robust concerning out-of-distributionentities. In this paper, we investigate using large language models (LLMs) forentity matching as a less domain-specific training data reliant and more robustalternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5and GPT4, as well as open source LLMs based on Llama2 which can be run locally.We evaluate these models in a zero-shot scenario as well as a scenario wheretask-specific training data is available. We compare different prompt designsas well as the prompt sensitivity of the models in the zero-shot scenario. Weinvestigate (i) the selection of in-context demonstrations, (ii) the generationof matching rules, as well as (iii) fine-tuning GPT3.5 in the second scenariousing the same pool of training data across the different approaches. Ourexperiments show that GPT4 without any task-specific training data outperformsfine-tuned PLMs (RoBERTa and Ditto) on three out of five benchmark datasetsreaching F1 scores around 90%. The experiments with in-context learning andrule generation show that all models beside of GPT4 benefit from thesetechniques (on average 5.9% and 2.2% F1), while GPT4 does not need suchadditional guidance in most cases...", "title": "entity matching using large language models", "url": "http://arxiv.org/pdf/2310.11244v1.pdf", "tokenized_text": "entity matching task deciding entity descriptions real world entity entity matching central step pipelines enabler commerce applications match products offers different vendors state matching methods rely pre trained_language trained language plms suchas bert roberta major drawbacks entity matching require significant amounts task specific training dataand ii fine tuned robust concerning distributionentities paper investigate large_language large language llms matching domain specific training_data training data plm based study covers hosted llms gpt4 open_source open source llms based llama2 run evaluate zero shot scenario scenario specific training_data training data available compare different sensitivity zero shot scenario weinvestigate selection context demonstrations ii generationof matching rules iii fine tuning gpt3.5 second pool training_data training data different approaches ourexperiments gpt4 task specific training_data training data tuned plms roberta benchmark f1 scores 90 experiments context_learning context learning generation gpt4 benefit average 5.9 2.2 f1 gpt4 need guidance cases"}
{"id": "nan", "abstract": "  Language models trained on large-scale corpus often generate content that isharmful, toxic, or contrary to human preferences, making their alignment withhuman values a critical concern. Reinforcement learning from human feedback(RLHF) with algorithms like PPO is a prevalent approach for alignment but isoften complex, unstable, and resource-intensive. Recently, ranking-basedalignment methods have emerged, offering stability and effectiveness byreplacing the RL framework with supervised fine-tuning, but they are costly dueto the need for annotated data. Considering that existing large language models(LLMs) like ChatGPT are already relatively well-aligned and cost-friendly,researchers have begun to align the language model with human preference fromAI feedback. The common practices, which unidirectionally distill theinstruction-following responses from LLMs, are constrained by their bottleneck.Thus we introduce CycleAlign to distill alignment capabilities fromparameter-invisible LLMs (black-box) to a parameter-visible model (white-box)in an iterative manner. With in-context learning (ICL) as the core of thecycle, the black-box models are able to rank the model-generated responsesguided by human-craft instruction and demonstrations about their preferences.During iterative interaction, the white-box models also have a judgment aboutresponses generated by them. Consequently, the agreement ranking could beviewed as a pseudo label to dynamically update the in-context demonstrationsand improve the preference ranking ability of black-box models. Throughmultiple interactions, the CycleAlign framework could align the white-box modelwith the black-box model effectively in a low-resource way. Empirical resultsillustrate that the model fine-tuned by CycleAlign remarkably exceeds existingmethods, and achieves the state-of-the-art performance in alignment with humanvalue.", "title": "cyclealign iterative distillation from blackbox llm to whitebox models for better human alignment", "url": "http://arxiv.org/pdf/2310.16271v1.pdf", "tokenized_text": "language_models language trained large scale corpus generate content toxic contrary human preferences making alignment withhuman values critical concern reinforcement_learning reinforcement learning human algorithms like ppo prevalent approach alignment isoften complex unstable resource intensive recently ranking methods emerged offering stability effectiveness rl framework supervised fine tuning costly dueto need annotated_data annotated data considering existing large_language large language models(llms like_chatgpt like chatgpt relatively aligned cost friendly researchers align language_model language human preference feedback common practices distill theinstruction following responses llms constrained bottleneck introduce distill alignment capabilities llms black box parameter white iterative manner context_learning context learning icl core black box able rank generated human craft instruction demonstrations preferences iterative interaction white box judgment generated consequently agreement ranking pseudo label dynamically update context improve preference ranking ability black box interactions framework align white box modelwith black box effectively low resource way empirical fine tuned remarkably exceeds existingmethods achieves state art performance alignment"}
{"id": "nan", "abstract": "  Pre-trained transformers can perform in-context learning, where they adapt toa new task using only a small number of prompts without any explicit modeloptimization. Inspired by this attribute, we propose a novel approach, calledin-context estimation, for the canonical communication problem of estimatingtransmitted symbols from received symbols. A communication channel isessentially a noisy function that maps transmitted symbols to received symbols,and this function can be represented by an unknown parameter whose statisticsdepend on an (also unknown) latent context. Conventional approaches ignore thishierarchical structure and simply attempt to use known transmissions, calledpilots, to perform a least-squares estimate of the channel parameter, which isthen used to estimate successive, unknown transmitted symbols. We make thebasic connection that transformers show excellent contextual sequencecompletion with a few prompts, and so they should be able to implicitlydetermine the latent context from pilot symbols to perform end-to-endin-context estimation of transmitted symbols. Furthermore, the transformershould use information efficiently, i.e., it should utilize any pilots receivedto attain the best possible symbol estimates. Through extensive simulations, weshow that in-context estimation not only significantly outperforms standardapproaches, but also achieves the same performance as an estimator with perfectknowledge of the latent context within a few context examples. Thus, we make astrong case that transformers are efficient in-context estimators in thecommunication setting.", "title": "transformers are efficient incontext estimators for wireless communication", "url": "http://arxiv.org/pdf/2311.00226v1.pdf", "tokenized_text": "pre trained transformers perform context_learning context learning adapt toa new task small_number small number explicit inspired attribute propose_a_novel propose novel approach context estimation canonical communication problem symbols received symbols communication channel noisy function maps symbols received symbols function represented unknown parameter unknown latent context conventional approaches ignore structure simply attempt use known transmissions perform squares estimate channel parameter isthen estimate unknown symbols connection transformers excellent contextual able latent context pilot symbols perform end context estimation symbols furthermore use information efficiently i.e. utilize attain best possible estimates extensive simulations weshow context estimation significantly_outperforms significantly outperforms achieves performance estimator latent context context_examples context examples astrong case transformers efficient context thecommunication setting"}
{"id": "nan", "abstract": "  This technical report introduces the winning solution of the team Segment AnyAnomaly for the CVPR2023 Visual Anomaly and Novelty Detection (VAND) challenge.Going beyond uni-modal prompt, e.g., language prompt, we present a novelframework, i.e., Segment Any Anomaly + (SAA$+$), for zero-shot anomalysegmentation with multi-modal prompts for the regularization of cascaded modernfoundation models. Inspired by the great zero-shot generalization ability offoundation models like Segment Anything, we first explore their assembly (SAA)to leverage diverse multi-modal prior knowledge for anomaly localization.Subsequently, we further introduce multimodal prompts (SAA$+$) derived fromdomain expert knowledge and target image context to enable the non-parameteradaptation of foundation models to anomaly segmentation. The proposed SAA$+$model achieves state-of-the-art performance on several anomaly segmentationbenchmarks, including VisA and MVTec-AD, in the zero-shot setting. We willrelease the code of our winning solution for the CVPR2023 VAN.", "title": "2nd place winning solution for the cvpr2023 visual anomaly and novelty detection challenge multimodal prompting for datacentric anomaly detection", "url": "http://arxiv.org/pdf/2306.09067v2.pdf", "tokenized_text": "technical report introduces winning solution team segment visual anomaly novelty detection challenge going uni modal e.g. language present novelframework i.e. segment anomaly zero shot multi modal regularization cascaded inspired great zero shot generalization_ability generalization ability like segment explore assembly leverage diverse multi modal prior knowledge anomaly localization subsequently introduce multimodal derived expert knowledge target image context enable non foundation_models foundation anomaly segmentation proposed achieves_state achieves state art performance anomaly including visa mvtec ad zero shot_setting shot setting code winning solution"}
{"id": "nan", "abstract": "  The standard paradigm for fake news detection mainly utilizes textinformation to model the truthfulness of news. However, the discourse of onlinefake news is typically subtle and it requires expert knowledge to use textualinformation to debunk fake news. Recently, studies focusing on multimodal fakenews detection have outperformed text-only methods. Recent approaches utilizingthe pre-trained model to extract unimodal features, or fine-tuning thepre-trained model directly, have become a new paradigm for detecting fake news.Again, this paradigm either requires a large number of training instances, orupdates the entire set of pre-trained model parameters, making real-world fakenews detection impractical. Furthermore, traditional multimodal methods fusethe cross-modal features directly without considering that the uncorrelatedsemantic representation might inject noise into the multimodal features. Thispaper proposes a Similarity-Aware Multimodal Prompt Learning (SAMPLE)framework. First, we incorporate prompt learning into multimodal fake newsdetection. Prompt learning, which only tunes prompts with a frozen languagemodel, can reduce memory usage significantly and achieve comparableperformances, compared with fine-tuning. We analyse three prompt templates witha soft verbalizer to detect fake news. In addition, we introduce thesimilarity-aware fusing method to adaptively fuse the intensity of multimodalrepresentation and mitigate the noise injection via uncorrelated cross-modalfeatures. For evaluation, SAMPLE surpasses the F1 and the accuracies ofprevious works on two benchmark multimodal datasets, demonstrating theeffectiveness of the proposed method in detecting fake news. In addition,SAMPLE also is superior to other approaches regardless of few-shot anddata-rich settings.", "title": "similarityaware multimodal prompt learning for fake news detection", "url": "http://arxiv.org/pdf/2304.04187v3.pdf", "tokenized_text": "standard paradigm fake news detection mainly utilizes truthfulness news discourse news typically subtle requires expert knowledge use fake news recently studies focusing multimodal detection outperformed text methods recent approaches pre trained extract unimodal features fine tuning thepre trained directly new_paradigm new paradigm detecting fake news paradigm requires large number training instances entire set pre trained parameters making real world detection impractical furthermore traditional multimodal methods cross modal features directly considering representation inject noise multimodal features thispaper proposes multimodal learning incorporate learning multimodal fake learning tunes frozen languagemodel reduce memory usage significantly achieve comparableperformances compared fine tuning analyse prompt_templates templates witha soft verbalizer detect fake news addition introduce thesimilarity aware fusing method adaptively fuse intensity mitigate noise injection cross evaluation sample surpasses f1 accuracies works benchmark multimodal datasets demonstrating theeffectiveness proposed_method proposed method detecting fake news addition sample superior approaches regardless shot anddata rich settings"}
{"id": "nan", "abstract": "  Interactive and embodied tasks pose at least two fundamental challenges toexisting Vision & Language (VL) models, including 1) grounding language intrajectories of actions and observations, and 2) referential disambiguation. Totackle these challenges, we propose an Embodied MultiModal Agent (EMMA): aunified encoder-decoder model that reasons over images and trajectories, andcasts action prediction as multimodal text generation. By unifying all tasks astext generation, EMMA learns a language of actions which facilitates transferacross tasks. Different to previous modular approaches with independentlytrained components, we use a single multitask model where each task contributesto goal completion. EMMA performs on par with similar models on several VLbenchmarks and sets a new state-of-the-art performance (36.81% success rate) onthe Dialog-guided Task Completion (DTC), a benchmark to evaluate dialog-guidedagents in the Alexa Arena", "title": "multitask multimodal prompted training for interactive embodied task completion", "url": "http://arxiv.org/pdf/2311.04067v1.pdf", "tokenized_text": "interactive embodied tasks pose fundamental challenges toexisting vision language vl including grounding language actions observations disambiguation totackle challenges propose embodied multimodal agent aunified encoder decoder reasons images trajectories action prediction multimodal text generation unifying tasks generation learns language actions facilitates tasks different previous modular approaches components use single multitask task goal completion performs par similar sets new state art performance success_rate success rate onthe dialog guided task completion benchmark evaluate dialog alexa"}
{"id": "nan", "abstract": "  Driven by the progress of large-scale pre-training, parameter-efficienttransfer learning has gained immense popularity across different subfields ofArtificial Intelligence. The core is to adapt the model to downstream taskswith only a small set of parameters. Recently, researchers have leveraged suchproven techniques in multimodal tasks and achieve promising results. However,two critical issues remain unresolved: how to further reduce the complexitywith lightweight design and how to boost alignment between modalities underextremely low parameters. In this paper, we propose A graceful prompt frameworkfor cross-modal transfer (Aurora) to overcome these challenges. Considering theredundancy in existing architectures, we first utilize the mode approximationto generate 0.1M trainable parameters to implement the multimodal prompttuning, which explores the low intrinsic dimension with only 0.04% parametersof the pre-trained model. Then, for better modality alignment, we propose theInformative Context Enhancement and Gated Query Transformation module underextremely few parameters scenes. A thorough evaluation on six cross-modalbenchmarks shows that it not only outperforms the state-of-the-art but evenoutperforms the full fine-tuning approach. Our code is available at:https://github.com/WillDreamer/Aurora.", "title": "parameterefficient tuning of largescale multimodal foundation model", "url": "http://arxiv.org/pdf/2305.08381v3.pdf", "tokenized_text": "driven progress large scale pre training parameter learning gained immense popularity different ofartificial intelligence core adapt downstream taskswith small set parameters recently researchers leveraged techniques multimodal tasks achieve promising_results promising results critical issues remain unresolved reduce lightweight design boost alignment modalities low parameters paper propose frameworkfor cross modal transfer overcome challenges considering existing architectures utilize mode generate 0.1 trainable parameters implement multimodal prompttuning explores low intrinsic dimension 0.04 pre trained better modality alignment propose context enhancement gated query transformation module parameters scenes thorough evaluation cross shows outperforms state art fine tuning approach code_is_available code available https://github.com"}
{"id": "nan", "abstract": "  What kinds of instructional prompts are easier to follow for Language Models(LMs)? We study this question by conducting extensive empirical analysis thatshed light on important features of successful instructional prompts.Specifically, we study several classes of reframing techniques for manualreformulation of prompts into more effective ones. Some examples includedecomposing a complex task instruction into multiple simpler tasks or itemizinginstructions into sequential steps. Our experiments compare the zero-shot andfew-shot performance of LMs prompted with reframed instructions on 12 NLP tasksacross 6 categories. Compared with original instructions, our reframedinstructions lead to significant improvements across LMs with different sizes.For example, the same reframed prompts boost few-shot performance ofGPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over alltasks. Furthermore, reframed instructions reduce the number of examplesrequired to prompt LMs in the few-shot setting. We hope theseempirically-driven techniques will pave the way towards more effective futureprompting algorithms.", "title": "reframing instructional prompts to gptk's language", "url": "http://arxiv.org/pdf/2109.07830v3.pdf", "tokenized_text": "kinds instructional easier follow language study question conducting extensive empirical analysis light important features successful instructional specifically study classes reframing techniques effective ones examples complex task instruction multiple simpler tasks sequential steps experiments compare zero shot andfew shot performance lms prompted reframed instructions 12 nlp tasksacross categories compared original instructions lead significant improvements lms different sizes example reframed boost shot performance series series 12.5 6.7 respectively averaged furthermore reframed instructions reduce number lms shot_setting shot setting hope driven techniques pave way effective algorithms"}
{"id": "nan", "abstract": "  Large language models (LLMs) have demonstrated impressive capabilities innatural language understanding and generation, but the quality bar for medicaland clinical applications is high. Today, attempts to assess models' clinicalknowledge typically rely on automated evaluations on limited benchmarks. Thereis no standard to evaluate model predictions and reasoning across a breadth oftasks. To address this, we present MultiMedQA, a benchmark combining sixexisting open question answering datasets spanning professional medical exams,research, and consumer queries; and HealthSearchQA, a new free-response datasetof medical questions searched online. We propose a framework for humanevaluation of model answers along multiple axes including factuality,precision, possible harm, and bias. In addition, we evaluate PaLM (a540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, onMultiMedQA. Using a combination of prompting strategies, Flan-PaLM achievesstate-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA,MedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (USMedical License Exam questions), surpassing prior state-of-the-art by over 17%.However, human evaluation reveals key gaps in Flan-PaLM responses. To resolvethis we introduce instruction prompt tuning, a parameter-efficient approach foraligning LLMs to new domains using a few exemplars. The resulting model,Med-PaLM, performs encouragingly, but remains inferior to clinicians. We showthat comprehension, recall of knowledge, and medical reasoning improve withmodel scale and instruction prompt tuning, suggesting the potential utility ofLLMs in medicine. Our human evaluations reveal important limitations of today'smodels, reinforcing the importance of both evaluation frameworks and methoddevelopment in creating safe, helpful LLM models for clinical applications.", "title": "large language models encode clinical knowledge", "url": "http://arxiv.org/pdf/2212.13138v1.pdf", "tokenized_text": "large_language large language llms demonstrated impressive capabilities innatural language understanding generation quality bar clinical applications high today attempts assess typically rely automated evaluations limited benchmarks thereis standard evaluate predictions reasoning breadth oftasks address present benchmark combining open question_answering question answering datasets spanning professional medical exams research queries new free response medical questions online propose framework humanevaluation answers multiple axes including factuality precision possible harm bias addition evaluate palm billion parameter llm instruction tuned variant flan palm combination strategies flan palm achievesstate art accuracy multiple choice dataset medqa mmlu clinical topics including accuracy medqa license exam questions surpassing prior state art human evaluation reveals key gaps flan palm responses introduce instruction tuning parameter efficient approach foraligning llms new domains exemplars resulting palm performs encouragingly remains inferior clinicians showthat comprehension recall knowledge medical reasoning improve scale instruction tuning suggesting potential utility ofllms medicine human evaluations reveal important limitations importance evaluation frameworks creating safe helpful llm clinical applications"}
{"id": "nan", "abstract": "  Large language models have unlocked strong multi-task capabilities fromreading instructive prompts. However, recent studies have shown that existinglarge models still have difficulty with information extraction tasks. Forexample, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset,which is significantly lower than the state-of-the-art performance. In thispaper, we propose InstructUIE, a unified information extraction framework basedon instruction tuning, which can uniformly model various information extractiontasks and capture the inter-task dependency. To validate the proposed method,we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extractiondatasets in a unified text-to-text format with expert-written instructions.Experimental results demonstrate that our method achieves comparableperformance to Bert in supervised settings and significantly outperforms thestate-of-the-art and gpt3.5 in zero-shot settings.", "title": "instructuie multitask instruction tuning for unified information extraction", "url": "http://arxiv.org/pdf/2304.08085v1.pdf", "tokenized_text": "large_language large language strong multi task capabilities instructive recent studies shown difficulty information_extraction information extraction tasks forexample gpt-3.5 turbo achieved f1_score f1 score dataset significantly lower state art performance thispaper propose unified information_extraction information extraction framework basedon instruction_tuning instruction tuning uniformly information extractiontasks capture inter task dependency validate proposed_method proposed method introduce ie instructions benchmark 32 diverse information unified text text format expert written instructions experimental_results experimental results demonstrate method_achieves method achieves comparableperformance bert supervised settings significantly_outperforms significantly outperforms thestate art gpt3.5 zero shot_settings shot settings"}
{"id": "nan", "abstract": "  Detecting social bias in text is challenging due to nuance, subjectivity, anddifficulty in obtaining good quality labeled datasets at scale, especiallygiven the evolving nature of social biases and society. To address thesechallenges, we propose a few-shot instruction-based method for promptingpre-trained language models (LMs). We select a few class-balanced exemplarsfrom a small support repository that are closest to the query to be labeled inthe embedding space. We then provide the LM with instruction that consists ofthis subset of labeled exemplars, the query text to be classified, a definitionof bias, and prompt it to make a decision. We demonstrate that large LMs usedin a few-shot context can detect different types of fine-grained biases withsimilar and sometimes superior accuracy to fine-tuned models. We observe thatthe largest 530B parameter model is significantly more effective in detectingsocial bias compared to smaller models (achieving at least 13% improvement inAUC metric compared to other models). It also maintains a high AUC (droppingless than 2%) when the labeled repository is reduced to as few as $100$samples. Large pretrained language models thus make it easier and quicker tobuild new bias detectors.", "title": "fewshot instruction prompts for pretrained language models to detect social biases", "url": "http://arxiv.org/pdf/2112.07868v2.pdf", "tokenized_text": "detecting social bias text challenging nuance obtaining good quality labeled datasets scale evolving nature social biases society address thesechallenges propose shot instruction based method trained_language trained language lms select class balanced small support repository closest query labeled inthe embedding space provide lm instruction consists ofthis subset labeled exemplars query text classified bias decision demonstrate large lms shot context detect different types fine grained biases superior accuracy fine tuned observe thatthe largest parameter significantly effective bias compared smaller achieving 13 improvement metric compared high auc labeled repository reduced large pretrained_language pretrained language easier tobuild new bias detectors"}
{"id": "nan", "abstract": "  Purpose: To introduce the concept of using large language models (LLMs) tore-label structure names in accordance with the American Association ofPhysicists in Medicine (AAPM) Task Group (TG)-263 standard, and to establish abenchmark for future studies to reference.  Methods and Materials: The Generative Pre-trained Transformer (GPT)-4application programming interface (API) was implemented as a Digital Imagingand Communications in Medicine (DICOM) storage server, which upon receiving astructure set DICOM file, prompts GPT-4 to re-label the structure names of bothtarget volumes and normal tissues according to the AAPM TG-263. Three diseasesites, prostate, head and neck, and thorax were selected for evaluation. Foreach disease site category, 150 patients were randomly selected for manuallytuning the instructions prompt (in batches of 50) and 50 patients were randomlyselected for evaluation. Structure names that were considered were those thatwere most likely to be relevant for studies utilizing structure contours formany patients.  Results: The overall re-labeling accuracy of both target volumes and normaltissues for prostate, head and neck, and thorax cases was 96.0%, 98.5%, and96.9% respectively. Re-labeling of target volumes was less accurate on averageexcept for prostate - 100%, 93.1%, and 91.1% respectively.  Conclusions: Given the accuracy of GPT-4 in re-labeling structure names ofboth target volumes and normal tissues as presented in this work, LLMs arepoised to be the preferred method for standardizing structure names inradiation oncology, especially considering the rapid advancements in LLMcapabilities that are likely to continue.", "title": "benchmarking a foundation llm on its ability to relabel structure names in accordance with the aapm tg263 report", "url": "http://arxiv.org/pdf/2310.03874v1.pdf", "tokenized_text": "purpose introduce concept large_language large language llms label structure names american association medicine task group standard establish abenchmark future studies reference methods materials generative pre trained transformer programming interface api implemented digital communications medicine storage server receiving set file gpt-4 label structure names volumes normal according head selected evaluation foreach disease site category 150 patients randomly selected instructions batches 50 50 patients randomlyselected evaluation structure names considered likely relevant studies utilizing structure formany patients results overall labeling accuracy target volumes head cases respectively labeling target volumes accurate 100 respectively conclusions given accuracy gpt-4 labeling structure names ofboth target volumes normal presented work llms preferred method structure names oncology especially considering rapid advancements likely continue"}
{"id": "nan", "abstract": "  Deep learning algorithms are dependent on the availability of large-scaleannotated clinical text datasets. The lack of such publicly available datasetsis the biggest bottleneck for the development of clinical Natural LanguageProcessing(NLP) systems. Zero-Shot Learning(ZSL) refers to the use of deeplearning models to classify instances from new classes of which no trainingdata have been seen before. Prompt-based learning is an emerging ZSL techniquewhere we define task-based templates for NLP tasks. We developed a novelprompt-based clinical NLP framework called HealthPrompt and applied theparadigm of prompt-based learning on clinical texts. In this technique, ratherthan fine-tuning a Pre-trained Language Model(PLM), the task definitions aretuned by defining a prompt template. We performed an in-depth analysis ofHealthPrompt on six different PLMs in a no-data setting. Our experiments provethat prompts effectively capture the context of clinical texts and performremarkably well without any training data.", "title": "healthprompt a zeroshot learning paradigm for clinical natural language processing", "url": "http://arxiv.org/pdf/2203.05061v1.pdf", "tokenized_text": "deep learning algorithms dependent availability large clinical text datasets lack publicly_available publicly available biggest bottleneck development clinical natural systems zero shot refers use deeplearning classify instances new classes trainingdata seen based learning emerging zsl define task based templates nlp_tasks nlp tasks developed based clinical nlp framework called applied based learning clinical texts technique ratherthan fine tuning pre trained_language trained language model(plm task definitions defining prompt_template template performed depth analysis different plms data setting experiments effectively capture context clinical texts training_data training data"}
{"id": "nan", "abstract": "  Prompt learning's fine-tune performance on text classification tasks hasattracted the NLP community. This paper applies it to resume informationextraction, improving existing methods for this task. We created manualtemplates and verbalizers tailored to resume texts and compared the performanceof Masked Language Model (MLM) and Seq2Seq PLMs. Also, we enhanced theverbalizer design for Knowledgeable Prompt-tuning, contributing to prompttemplate design across NLP tasks. We present the Manual KnowledgeableVerbalizer (MKV), a rule for constructing verbalizers for specificapplications. Our tests show that MKV rules yield more effective, robusttemplates and verbalizers than existing methods. Our MKV approach resolvedsample imbalance, surpassing current automatic prompt methods. This studyunderscores the value of tailored prompt learning for resume extraction,stressing the importance of custom-designed templates and verbalizers.", "title": "a fewshot approach to resume information extraction via prompts", "url": "http://arxiv.org/pdf/2209.09450v2.pdf", "tokenized_text": "learning fine tune performance text_classification text classification tasks nlp community paper applies informationextraction improving existing_methods existing methods task created verbalizers tailored texts compared performanceof masked language_model language mlm seq2seq plms enhanced design knowledgeable tuning contributing prompttemplate design nlp_tasks nlp tasks present manual rule constructing verbalizers tests rules yield effective verbalizers existing_methods existing methods approach imbalance surpassing current automatic methods value tailored learning extraction importance custom designed templates verbalizers"}
{"id": "nan", "abstract": "  This paper examines the art practices, artwork, and motivations of prolificusers of the latest generation of text-to-image models. Through interviews,observations, and a user survey, we present a sampling of the artistic stylesand describe the developed community of practice around generative AI. We findthat: 1) the text prompt and the resulting image can be considered collectivelyas an art piece prompts as art and 2) prompt templates (prompts with ``slots''for others to fill in with their own words) are developed to create generativeart styles. We discover that the value placed by this community on uniqueoutputs leads to artists seeking specialized vocabulary to produce distinctiveart pieces (e.g., by reading architectural blogs to find phrases to describeimages). We also find that some artists use \"glitches\" in the model that can beturned into artistic styles of their own right. From these findings, we outlinespecific implications for design regarding future prompting and image editingoptions.", "title": "the prompt artists", "url": "http://arxiv.org/pdf/2303.12253v1.pdf", "tokenized_text": "paper examines art practices artwork motivations latest generation text image interviews observations user survey present sampling artistic describe developed community practice generative_ai generative ai findthat text resulting image considered art piece art prompt_templates templates fill words developed create styles discover value placed community leads artists seeking specialized vocabulary produce pieces e.g. reading architectural blogs find phrases find artists use artistic styles right findings implications design future image"}
{"id": "nan", "abstract": "  Foundation models are trained on vast amounts of data at scale usingself-supervised learning, enabling adaptation to a wide range of downstreamtasks. At test time, these models exhibit zero-shot capabilities through whichthey can classify previously unseen (user-specified) categories. In this paper,we address the problem of quantifying uncertainty in these zero-shotpredictions. We propose a heuristic approach for uncertainty estimation inzero-shot settings using conformal prediction with web data. Given a set ofclasses at test time, we conduct zero-shot classification with CLIP-stylemodels using a prompt template, e.g., \"an image of a <category>\", and use thesame template as a search query to source calibration data from the open web.Given a web-based calibration set, we apply conformal prediction with a novelconformity score that accounts for potential errors in retrieved web data. Weevaluate the utility of our proposed method in Biomedical foundation models;our preliminary results show that web-based conformal prediction sets achievethe target coverage with satisfactory efficiency on a variety of biomedicaldatasets.", "title": "estimating uncertainty in multimodal foundation models using public internet data", "url": "http://arxiv.org/pdf/2310.09926v1.pdf", "tokenized_text": "foundation_models foundation trained vast amounts data scale supervised learning enabling adaptation wide_range wide range downstreamtasks test_time test time exhibit zero shot capabilities classify previously unseen user specified categories paper address problem quantifying uncertainty zero propose heuristic approach uncertainty estimation inzero shot_settings shot settings prediction web data given set ofclasses test_time test time conduct zero shot classification clip stylemodels prompt_template template e.g. image category use thesame template search query source calibration data open web given web based calibration set apply prediction score accounts potential errors retrieved web data weevaluate utility proposed_method proposed method biomedical foundation preliminary results web based prediction sets target coverage satisfactory efficiency variety"}
{"id": "nan", "abstract": "  Zero-shot text rankers powered by recent LLMs achieve remarkable rankingperformance by simply prompting. Existing prompts for pointwise LLM rankersmostly ask the model to choose from binary relevance labels like \"Yes\" and\"No\". However, the lack of intermediate relevance label options may cause theLLM to provide noisy or biased answers for documents that are partiallyrelevant to the query. We propose to incorporate fine-grained relevance labelsinto the prompt for LLM rankers, enabling them to better differentiate amongdocuments with different levels of relevance to the query and thus derive amore accurate ranking. We study two variants of the prompt template, coupledwith different numbers of relevance levels. Our experiments on 8 BEIR data setsshow that adding fine-grained relevance labels significantly improves theperformance of LLM rankers.", "title": "beyond yes and no improving zeroshot llm rankers via scoring finegrained relevance labels", "url": "http://arxiv.org/pdf/2310.14122v2.pdf", "tokenized_text": "zero shot text rankers powered recent llms achieve remarkable simply existing pointwise llm ask choose binary relevance labels like yes lack intermediate relevance label options cause thellm provide noisy biased answers documents query propose incorporate fine grained relevance llm rankers enabling better differentiate different levels relevance query derive amore accurate ranking study variants prompt_template template different numbers relevance levels experiments beir data adding fine grained relevance labels significantly improves theperformance llm rankers"}
{"id": "nan", "abstract": "  This paper explores the image-sharing capability of Large Language Models(LLMs), such as InstructGPT, ChatGPT, and GPT-4, in a zero-shot setting,without the help of visual foundation models. Inspired by the two-stage processof image-sharing in human dialogues, we propose a two-stage framework thatallows LLMs to predict potential image-sharing turns and generate related imagedescriptions using our effective restriction-based prompt template. Withextensive experiments, we unlock the \\textit{image-sharing} capability of LLMsin zero-shot prompting, with GPT-4 achieving the best performance.Additionally, we uncover the emergent \\textit{image-sharing} ability inzero-shot prompting, demonstrating the effectiveness of restriction-basedprompts in both stages of our framework. Based on this framework, we augmentthe PhotoChat dataset with images generated by Stable Diffusion at predictedturns, namely PhotoChat++. To our knowledge, this is the first study to assessthe image-sharing ability of LLMs in a zero-shot setting without visualfoundation models. The source code and the dataset will be released afterpublication.", "title": "large language models can share images, too!", "url": "http://arxiv.org/pdf/2310.14804v1.pdf", "tokenized_text": "paper explores image sharing capability large_language large language models(llms instructgpt chatgpt gpt-4 zero shot_setting shot setting help visual foundation_models foundation inspired stage image sharing human dialogues propose stage framework thatallows llms predict potential image sharing turns generate related effective restriction based prompt_template template experiments unlock sharing capability llmsin zero shot_prompting shot gpt-4 achieving best performance additionally uncover emergent sharing ability inzero shot_prompting shot demonstrating effectiveness restriction stages framework based framework dataset images generated stable_diffusion stable diffusion knowledge study image sharing ability llms zero shot_setting shot setting source_code source code dataset released"}
{"id": "nan", "abstract": "  In relation triplet extraction (RTE), recognizing unseen (new) relations forwhich there are no training instances is a challenging task. Efforts have beenmade to recognize unseen relations based on question-answering models orrelation descriptions. However, these approaches miss the semantic informationabout connections between seen and unseen relations. In this paper, We proposea prompt-based model with semantic knowledge augmentation (ZS-SKA) to recognizeunseen relations under the zero-shot setting. We present a new word-levelanalogy-based sentence translation rule and generate augmented instances withunseen relations from instances with seen relations using that new rule. Wedesign prompts with weighted virtual label construction based on an externalknowledge graph to integrate semantic knowledge information learned from seenrelations. Instead of using the actual label sets in the prompt template, weconstruct weighted virtual label words. We learn the representations of bothseen and unseen relations with augmented instances and prompts. We thencalculate the distance between the generated representations using prototypicalnetworks to predict unseen relations. Extensive experiments conducted on threepublic datasets FewRel, Wiki-ZSL, and NYT, show that ZS-SKA outperformsstate-of-the-art methods under the zero-shot scenarios. Our experimentalresults also demonstrate the effectiveness and robustness of ZS-SKA.", "title": "promptbased zeroshot relation extraction with semantic knowledge augmentation", "url": "http://arxiv.org/pdf/2112.04539v2.pdf", "tokenized_text": "relation triplet extraction recognizing unseen new relations forwhich training instances challenging task efforts beenmade recognize unseen relations based question answering descriptions approaches semantic connections seen unseen relations paper proposea based semantic knowledge augmentation relations zero shot_setting shot setting present new word based sentence translation rule generate augmented instances relations instances seen relations new rule wedesign weighted virtual label construction based externalknowledge graph integrate semantic knowledge information learned instead actual label sets prompt_template template weconstruct weighted virtual label words learn representations unseen relations augmented instances distance generated representations predict unseen relations extensive_experiments extensive experiments conducted threepublic datasets fewrel wiki zsl nyt outperformsstate art methods zero shot scenarios experimentalresults demonstrate_the_effectiveness demonstrate effectiveness robustness"}
{"id": "nan", "abstract": "  Pretrained language models (PLMs) have made remarkable progress intable-to-text generation tasks. However, the lack of domain-specific knowledgemakes it challenging to bridge the topological gap between tabular data andtext, especially in real-world applications with limited resources. To mitigatethe limitation of insufficient labeled data, we propose a novel framework:Adapt-Prompt-to-Generate (AdaPTGen). The core insight of AdaPTGen is to adaptprompt templates of domain-specific knowledge into the model, which brings atleast three benefits: (1) it injects representation of normal table-relateddescriptions to bridge the topological gap between tabular data and texts; (2)it enables us to use large amounts of unlabeled domain-specific knowledgefully, which can alleviate the PLMs' inherent shortcomings of lacking domainknowledge; (3) it allows us to design various tasks to explore thedomain-specific knowledge. Extensive experiments and analyses are conducted onthree open-domain few-shot natural language generation (NLG) data sets: Humans,Songs, and Books. Compared to previous state-of-the-art approaches, our modelachieves superior performance in terms of both fluency and accuracy.", "title": "adapting prompt for fewshot tabletotext generation", "url": "http://arxiv.org/pdf/2302.12468v2.pdf", "tokenized_text": "pretrained_language pretrained language plms remarkable progress text generation tasks lack domain specific challenging bridge topological gap tabular data especially real world_applications world applications limited resources limitation insufficient labeled_data labeled data propose_a_novel propose novel framework adapt generate core insight templates domain specific knowledge brings atleast benefits injects representation normal table bridge topological gap tabular data texts enables use large amounts unlabeled domain specific alleviate plms inherent shortcomings lacking domainknowledge allows design tasks explore specific knowledge extensive_experiments extensive experiments analyses conducted open domain shot natural_language natural language generation nlg data sets humans books compared previous state art approaches superior_performance superior performance terms fluency accuracy"}
{"id": "nan", "abstract": "  Multimodal Named Entity Recognition (MNER) on social media aims to enhancetextual entity prediction by incorporating image-based clues. Existing studiesmainly focus on maximizing the utilization of pertinent image information orincorporating external knowledge from explicit knowledge bases. However, thesemethods either neglect the necessity of providing the model with externalknowledge, or encounter issues of high redundancy in the retrieved knowledge.In this paper, we present PGIM -- a two-stage framework that aims to leverageChatGPT as an implicit knowledge base and enable it to heuristically generateauxiliary knowledge for more efficient entity prediction. Specifically, PGIMcontains a Multimodal Similar Example Awareness module that selects suitableexamples from a small number of predefined artificial samples. These examplesare then integrated into a formatted prompt template tailored to the MNER andguide ChatGPT to generate auxiliary refined knowledge. Finally, the acquiredknowledge is integrated with the original text and fed into a downstream modelfor further processing. Extensive experiments show that PGIM outperformsstate-of-the-art methods on two classic MNER datasets and exhibits a strongerrobustness and generalization capability.", "title": "prompting chatgpt in mner enhanced multimodal named entity recognition with auxiliary refined knowledge", "url": "http://arxiv.org/pdf/2305.12212v2.pdf", "tokenized_text": "multimodal named_entity named entity recognition social_media social media aims entity prediction incorporating image based clues existing focus maximizing utilization pertinent image information external_knowledge external knowledge explicit knowledge bases neglect necessity providing externalknowledge encounter issues high redundancy retrieved knowledge paper present stage framework aims implicit knowledge base enable heuristically knowledge efficient entity prediction specifically multimodal similar example awareness module selects small_number small number predefined artificial samples integrated formatted prompt_template template tailored chatgpt generate auxiliary refined knowledge finally integrated original text fed downstream processing extensive_experiments extensive experiments outperformsstate art methods classic datasets exhibits generalization capability"}
{"id": "nan", "abstract": "  With the increasing capabilities of large language models (LLMs), thesehigh-performance models have achieved state-of-the-art results on a wide rangeof natural language processing (NLP) tasks. However, the models' performance oncommonly-used benchmark datasets often fails to accurately reflect theirreliability and robustness when applied to real-world noisy data. To addressthese challenges, we propose a unified robustness evaluation framework based onthe slot-filling task to systematically evaluate the dialogue understandingcapability of LLMs in diverse input perturbation scenarios. Specifically, weconstruct a input perturbation evaluation dataset, Noise-LLM, which containsfive types of single perturbation and four types of mixed perturbation data.Furthermore, we utilize a multi-level data augmentation method (character,word, and sentence levels) to construct a candidate data pool, and carefullydesign two ways of automatic task demonstration construction strategies(instance-level and entity-level) with various prompt templates. Our aim is toassess how well various robustness methods of LLMs perform in real-world noisyscenarios. The experiments have demonstrated that the current open-source LLMsgenerally achieve limited perturbation robustness performance. Based on theseexperimental observations, we make some forward-looking suggestions to fuel theresearch in this direction.", "title": "revisit input perturbation problems for llms a unified robustness evaluation framework for noisy slot filling task", "url": "http://arxiv.org/pdf/2310.06504v1.pdf", "tokenized_text": "increasing capabilities large_language large language llms performance achieved state art results wide rangeof natural_language natural language processing nlp tasks performance benchmark_datasets benchmark datasets fails accurately reflect robustness applied real world noisy data addressthese challenges propose unified robustness evaluation framework based onthe slot filling task systematically evaluate dialogue llms diverse input perturbation scenarios specifically weconstruct input perturbation evaluation dataset noise llm types single perturbation types mixed perturbation data furthermore utilize multi level data_augmentation data augmentation method character word sentence levels construct candidate data pool ways automatic task demonstration construction level entity level prompt_templates templates aim toassess robustness methods llms perform real world experiments demonstrated current open source achieve limited perturbation robustness performance based observations forward looking suggestions theresearch direction"}
{"id": "nan", "abstract": "  Language Models (LMs) have proven their ability to acquire diverse linguisticknowledge during the pretraining phase, potentially serving as a valuablesource of incidental supervision for downstream tasks. However, there has beenlimited research conducted on the retrieval of domain-specific knowledge, andspecifically legal knowledge. We propose to explore the task of Entity Typing,serving as a proxy for evaluating legal knowledge as an essential aspect oftext comprehension, and a foundational task to numerous downstream legal NLPapplications. Through systematic evaluation and analysis and two types ofprompting (cloze sentences and QA-based templates) and to clarify the nature ofthese acquired cues, we compare diverse types and lengths of entities bothgeneral and domain-specific entities, semantics or syntax signals, anddifferent LM pretraining corpus (generic and legal-oriented) and architectures(encoder BERT-based and decoder-only with Llama2). We show that (1) Llama2performs well on certain entities and exhibits potential for substantialimprovement with optimized prompt templates, (2) law-oriented LMs showinconsistent performance, possibly due to variations in their training corpus,(3) LMs demonstrate the ability to type entities even in the case ofmulti-token entities, (4) all models struggle with entities belonging tosub-domains of the law (5) Llama2 appears to frequently overlook syntacticcues, a shortcoming less present in BERT-based architectures.", "title": "do language models learn about legal entity types during pretraining", "url": "http://arxiv.org/pdf/2310.13092v1.pdf", "tokenized_text": "language_models language lms proven ability acquire diverse linguisticknowledge pretraining phase potentially serving supervision downstream_tasks downstream tasks research conducted retrieval domain specific knowledge legal knowledge propose explore task entity typing serving proxy evaluating legal knowledge essential aspect oftext comprehension foundational task numerous downstream legal nlpapplications systematic evaluation analysis types ofprompting cloze sentences qa based templates clarify nature ofthese acquired cues compare diverse types lengths entities domain specific entities semantics syntax signals lm pretraining corpus generic legal oriented bert based decoder llama2 certain entities exhibits potential optimized prompt_templates templates law oriented lms performance possibly variations training lms demonstrate ability type entities case token entities struggle entities domains law llama2 appears frequently overlook shortcoming present bert based architectures"}
{"id": "nan", "abstract": "  Recently, large language models (LLMs) have exhibited significant progress inlanguage understanding and generation. By leveraging textual features,customized LLMs are also applied for recommendation and demonstrateimprovements across diverse recommendation scenarios. Yet the majority ofexisting methods perform training-free recommendation that heavily relies onpretrained knowledge (e.g., movie recommendation). In addition, inference onLLMs is slow due to autoregressive generation, rendering existing methods lesseffective for real-time recommendation. As such, we propose a two-stageframework using large language models for ranking-based recommendation(LlamaRec). In particular, we use small-scale sequential recommenders toretrieve candidates based on the user interaction history. Then, both historyand retrieved items are fed to the LLM in text via a carefully designed prompttemplate. Instead of generating next-item titles, we adopt a verbalizer-basedapproach that transforms output logits into probability distributions over thecandidate items. Therefore, the proposed LlamaRec can efficiently rank itemswithout generating long text. To validate the effectiveness of the proposedframework, we compare against state-of-the-art baseline methods on benchmarkdatasets. Our experimental results demonstrate the performance of LlamaRec,which consistently achieves superior performance in both recommendationperformance and efficiency.", "title": "llamarec twostage recommendation using large language models for ranking", "url": "http://arxiv.org/pdf/2311.02089v1.pdf", "tokenized_text": "recently large_language large language llms exhibited significant progress inlanguage understanding generation leveraging textual features customized llms applied recommendation diverse recommendation scenarios majority ofexisting methods perform training free recommendation heavily relies knowledge e.g. movie recommendation addition inference onllms slow autoregressive generation rendering existing_methods existing methods real time recommendation propose large_language large language ranking based particular use small scale sequential recommenders toretrieve candidates based user interaction history retrieved items fed llm text carefully designed prompttemplate instead generating item titles adopt verbalizer basedapproach transforms output logits probability distributions items proposed efficiently rank generating long text validate effectiveness compare state art baseline methods benchmarkdatasets experimental_results experimental results demonstrate performance consistently achieves superior_performance superior performance efficiency"}
{"id": "nan", "abstract": "  Click-through rate (CTR) prediction plays as a core function module invarious personalized online services. According to the data modality and inputformat, the models for CTR prediction can be mainly classified into twocategories. The first one is the traditional CTR models that take as inputs theone-hot encoded ID features of tabular modality, which aims to capture thecollaborative signals via feature interaction modeling. The second categorytakes as inputs the sentences of textual modality obtained by hard prompttemplates, where pretrained language models (PLMs) are adopted to extract thesemantic knowledge. These two lines of research generally focus on differentcharacteristics of the same input data (i.e., textual and tabular modalities),forming a distinct complementary relationship with each other. Therefore, inthis paper, we propose to conduct fine-grained feature-level Alignment betweenLanguage and CTR models (ALT) for CTR prediction. Apart from the commonCLIP-like instance-level contrastive learning, we further design a novel jointreconstruction pretraining task for both masked language and tabular modeling.Specifically, the masked data of one modality (i.e., tokens or features) has tobe recovered with the help of the other modality, which establishes thefeature-level interaction and alignment via sufficient mutual informationextraction between dual modalities. Moreover, we propose three differentfinetuning strategies with the option to train the aligned language and CTRmodels separately or jointly for downstream CTR prediction tasks, thusaccommodating the varying efficacy and efficiency requirements for industrialapplications. Extensive experiments on three real-world datasets demonstratethat ALT outperforms SOTA baselines, and is highly compatible for variouslanguage and CTR models.", "title": "alt towards finegrained alignment between language and ctr models for clickthrough rate prediction", "url": "http://arxiv.org/pdf/2310.19453v1.pdf", "tokenized_text": "click rate ctr prediction plays core function module invarious personalized online services according data modality ctr prediction mainly classified traditional ctr inputs theone hot encoded id features tabular modality aims capture signals feature interaction modeling second inputs sentences textual modality obtained hard pretrained_language pretrained language plms adopted extract thesemantic knowledge lines research generally focus input data i.e. textual tabular distinct complementary relationship inthis_paper inthis paper propose conduct fine grained feature level alignment ctr ctr prediction apart like instance level contrastive_learning contrastive learning design novel pretraining task masked language tabular modeling specifically masked data modality i.e. tokens features tobe help modality establishes level interaction alignment sufficient mutual informationextraction dual modalities propose strategies option train aligned language separately jointly downstream ctr prediction tasks varying efficacy efficiency requirements extensive_experiments extensive experiments real world datasets demonstratethat outperforms sota baselines highly compatible variouslanguage ctr"}
{"id": "0100785773b8217c44606ab260e3212f93b0a4fd", "abstract": "Large Language Models (LLMs) have made remarkable strides in various tasks. Whether LLMs are competitive few-shot solvers for information extraction (IE) tasks, however, remains an open problem. In this work, we aim to provide a thorough answer to this question. Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings. Therefore, we conclude that LLMs are not effective few-shot information extractors in general. Nonetheless, we illustrate that with appropriate prompting strategies, LLMs can effectively complement SLMs and tackle challenging samples that SLMs struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters and LLMs serve as rerankers. By prompting LLMs to rerank a small portion of difficult samples identified by SLMs, our preliminary system consistently achieves promising improvements (2.4% F1-gain on average) on various IE tasks, with an acceptable time and cost investment.", "title": "large language model is not a good fewshot information extractor, but a good reranker for hard samples!", "url": "http://arxiv.org/pdf/2303.08559", "tokenized_text": "large_language large language llms remarkable strides tasks llms competitive shot solvers information_extraction information extraction ie tasks remains open problem work aim provide thorough answer question extensive_experiments extensive experiments datasets ie tasks demonstrate current advanced llms consistently exhibit inferior performance higher latency increased budget requirements compared fine tuned slms settings conclude llms effective shot information extractors general nonetheless illustrate appropriate strategies llms effectively complement slms tackle challenging samples slms struggle propose adaptive filter rerank paradigm combine strengths llms slms paradigm slms serve filters llms serve rerankers llms rerank small portion difficult samples identified slms preliminary system consistently achieves promising improvements 2.4 f1 gain average ie tasks acceptable time cost investment"}
{"id": "023f0045686f86332a26856f8d8c3203566925ad", "abstract": "In the realm of embodied artificial intelligence, the reasoning capabilities of Large Language Models (LLMs) play a pivotal role. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing and stratifying algorithm, and apply it to instruction generation for mathematical reasoning and code data filtering for code generation tasks. Extensive results demonstrates the effectiveness of our proposed approach. Code will be integrated into the EasyInstruct framework at https://github.com/zjunlp/EasyInstruct.", "title": "when do programofthoughts work for reasoning", "url": "https://arxiv.org/pdf/2308.15452", "tokenized_text": "realm embodied artificial_intelligence artificial intelligence reasoning capabilities large_language large language llms play pivotal role effective methods like program thought_prompting thought llms uses programming language tackle complex_reasoning complex reasoning tasks specific impact code data improvement reasoning capabilities remains explored address gap propose complexity impacted reasoning score combines structural logical attributes measure correlation code reasoning abilities specifically use abstract syntax tree encode structural information calculate logical complexity considering difficulty complexity empirical analysis find code data complexity learned understood llms optimal level complexity critical improvement reasoning abilities program aided design auto synthesizing algorithm apply instruction generation mathematical reasoning code data filtering code_generation code generation tasks extensive results demonstrates effectiveness proposed approach code integrated framework"}
{"id": "0630a18fe3fe4765132ad52a591f9776cf3284bf", "abstract": "Current large language models (LLMs) can exhibit near-human levels of performance on many natural language-based tasks, including open-domain question answering. Unfortunately, at this time, they also convincingly hallucinate incorrect answers, so that responses to questions must be verified against external sources before they can be accepted at face value. In this paper, we report two simple experiments to automatically validate generated answers against a corpus. We base our experiments on questions and passages from the MS MARCO (V1) test collection, and a retrieval pipeline consisting of sparse retrieval, dense retrieval and neural rerankers. In the first experiment, we validate the generated answer in its entirety. After presenting a question to an LLM and receiving a generated answer, we query the corpus with the combination of the question + generated answer. We then present the LLM with the combination of the question + generated answer + retrieved answer, prompting it to indicate if the generated answer can be supported by the retrieved answer. In the second experiment, we consider the generated answer at a more granular level, prompting the LLM to extract a list of factual statements from the answer and verifying each statement separately. We query the corpus with each factual statement and then present the LLM with the statement and the corresponding retrieved evidence. The LLM is prompted to indicate if the statement can be supported and make necessary edits using the retrieved material. With an accuracy of over 80%, we find that an LLM is capable of verifying its generated answer when a corpus of supporting material is provided. However, manual assessment of a random sample of questions reveals that incorrect generated answers are missed by this verification process. While this verification process can reduce hallucinations, it can not entirely eliminate them.", "title": "retrieving supporting evidence for generative question answering", "url": "https://arxiv.org/pdf/2309.11392", "tokenized_text": "current large_language large language llms exhibit near human levels performance natural_language natural language based tasks including open domain question_answering question answering unfortunately time hallucinate incorrect answers responses questions verified external sources accepted face value paper report simple experiments automatically validate generated answers corpus base experiments questions passages ms marco test collection retrieval pipeline consisting sparse retrieval dense retrieval neural rerankers experiment validate generated answer entirety presenting question llm receiving generated answer query corpus combination question generated answer present llm combination question generated answer retrieved answer indicate generated answer supported retrieved answer second experiment consider generated answer level llm extract list factual statements answer verifying statement separately query corpus factual statement present llm statement corresponding retrieved evidence llm prompted indicate statement supported necessary edits retrieved material accuracy 80 find llm capable verifying generated answer corpus supporting material provided manual assessment random sample questions reveals incorrect generated answers verification process verification process reduce hallucinations entirely eliminate"}
{"id": "16707317eb7f71b1b4d47f27d703a2cdb5142baf", "abstract": "Property-based testing (PBT), while an established technique in the software testing research community, is still relatively underused in real-world software. Pain points in writing property-based tests include implementing diverse random input generators and thinking of meaningful properties to test. Developers, however, are more amenable to writing documentation; plenty of library API documentation is available and can be used as natural language specifications for property-based tests. As large language models (LLMs) have recently shown promise in a variety of coding tasks, we explore the potential of using LLMs to synthesize property-based tests. We call our approach PBT-GPT, and propose three different strategies of prompting the LLM for PBT. We characterize various failure modes of PBT-GPT and detail an evaluation methodology for automatically synthesized property-based tests. PBT-GPT achieves promising results in our preliminary studies on sample Python library APIs in $\\texttt{numpy}$, $\\texttt{networkx}$, and $\\texttt{datetime}$.", "title": "can large language models write good propertybased tests", "url": "https://arxiv.org/pdf/2307.04346", "tokenized_text": "property based testing established technique software testing research community relatively real world software points writing property based tests include implementing diverse random input generators thinking meaningful properties test developers amenable writing documentation plenty library api_documentation api documentation available natural_language natural language specifications property based tests large_language large language llms recently shown promise variety coding tasks explore potential llms synthesize property based tests approach gpt propose different strategies llm characterize failure modes gpt detail evaluation methodology automatically synthesized property based tests gpt achieves promising_results promising results preliminary studies sample python library apis"}
{"id": "16ecaa7cf142605331fc21c9be73c7b13e8c1acd", "abstract": "In advanced human-robot interaction tasks, visual target navigation is crucial for autonomous robots navigating unknown environments. While numerous approaches have been developed in the past, most are designed for single-robot operations, which often suffer from reduced efficiency and robustness due to environmental complexities. Furthermore, learning policies for multi-robot collaboration are resource-intensive. To address these challenges, we propose Co-NavGPT, an innovative framework that integrates Large Language Models (LLMs) as a global planner for multi-robot cooperative visual target navigation. Co-NavGPT encodes the explored environment data into prompts, enhancing LLMs' scene comprehension. It then assigns exploration frontiers to each robot for efficient target search. Experimental results on Habitat-Matterport 3D (HM3D) demonstrate that Co-NavGPT surpasses existing models in success rates and efficiency without any learning process, demonstrating the vast potential of LLMs in multi-robot collaboration domains. The supplementary video, prompts, and code can be accessed via the following link: \\href{https://sites.google.com/view/co-navgpt}{https://sites.google.com/view/co-navgpt}.", "title": "conavgpt multirobot cooperative visual semantic navigation using large language models", "url": "https://arxiv.org/pdf/2310.07937", "tokenized_text": "advanced human robot interaction tasks visual target navigation crucial autonomous robots navigating unknown environments numerous approaches developed past designed single robot operations suffer reduced efficiency robustness environmental complexities furthermore learning policies multi robot collaboration resource intensive address challenges propose co innovative framework integrates large_language large language llms global planner multi robot cooperative visual target navigation co encodes explored environment data enhancing llms scene comprehension exploration frontiers robot efficient target search experimental_results experimental results 3d demonstrate co surpasses existing success rates efficiency learning process demonstrating vast potential llms multi robot collaboration domains supplementary video code accessed following link view co view co"}
{"id": "191e300e381d4128b749d16fe3d83c8643a3bd1f", "abstract": "Text-to-SQL aims at generating SQL queries for the given natural language questions and thus helping users to query databases. Prompt learning with large language models (LLMs) has emerged as a recent approach, which designs prompts to lead LLMs to understand the input question and generate the corresponding SQL. However, it faces challenges with strict SQL syntax requirements. Existing work prompts the LLMs with a list of demonstration examples (i.e. question-SQL pairs) to generate SQL, but the fixed prompts can hardly handle the scenario where the semantic gap between the retrieved demonstration and the input question is large. In this paper, we propose a retrieval-augmented prompting method for a LLM-based Text-to-SQL framework, involving sample-aware prompting and a dynamic revision chain. Our approach incorporates sample-aware demonstrations, which include the composition of SQL operators and fine-grained information related to the given question. To retrieve questions sharing similar intents with input questions, we propose two strategies for assisting retrieval. Firstly, we leverage LLMs to simplify the original questions, unifying the syntax and thereby clarifying the users' intentions. To generate executable and accurate SQLs without human intervention, we design a dynamic revision chain which iteratively adapts fine-grained feedback from the previously generated SQL. Experimental results on three Text-to-SQL benchmarks demonstrate the superiority of our method over strong baseline models.", "title": "retrievalaugmented gpt35based texttosql framework with sampleaware prompting and dynamic revision chain", "url": "https://arxiv.org/pdf/2307.05074", "tokenized_text": "text sql aims generating sql queries given natural_language natural language questions helping users query databases learning large_language large language llms emerged recent approach designs lead llms understand input question generate corresponding sql faces challenges strict sql syntax requirements existing work llms list demonstration examples i.e. question sql pairs generate sql fixed hardly handle scenario semantic gap retrieved demonstration input question large paper propose retrieval augmented method llm based text sql framework involving sample aware dynamic revision chain approach incorporates sample aware demonstrations include composition sql operators fine grained information related given question retrieve questions sharing similar intents input questions propose strategies assisting retrieval firstly leverage llms simplify original questions unifying syntax clarifying users intentions generate executable accurate human intervention design dynamic revision chain iteratively adapts fine grained feedback previously generated sql experimental_results experimental results text sql benchmarks demonstrate superiority method strong baseline"}
{"id": "19e5b780a2dd1ffa1962e392976308b9fe644c7f", "abstract": "We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They then generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset for agent representation and reasoning. We experimentally demonstrate the effectiveness of our approach -- it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility -- in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together. See project website https://project-roco.github.io for videos and code.", "title": "roco dialectic multirobot collaboration with large language models", "url": "https://arxiv.org/pdf/2307.04738", "tokenized_text": "propose_a_novel propose novel approach multi robot collaboration harnesses power pre trained large_language large language llms high level communication low level path planning robots equipped llms discuss collectively reason task strategies generate sub task plans task space waypoint paths multi arm motion planner accelerate trajectory planning provide feedback environment collision checking llm agents improve plan waypoints context evaluation introduce task benchmark covering wide_range wide range multi robot collaboration scenarios accompanied text dataset agent representation reasoning experimentally demonstrate_the_effectiveness demonstrate effectiveness approach achieves high success rates tasks adapts variations task semantics dialog setup offers high interpretability flexibility real_world real world experiments easily incorporates human loop user communicate collaborate robot agent complete tasks project website videos code"}
{"id": "1ee8c8dd9d04247515b33775532b72df7b8ec0f3", "abstract": "In this work, we investigate extending the comprehension of Multi-modal Large Language Models (MLLMs) to regional objects. To this end, we propose to extract features corresponding to regional objects as soft prompts for LLM, which provides a straightforward and scalable approach and eliminates the need for LLM fine-tuning. To effectively extract regional features from regular image features and irregular point cloud features, we present a novel and unified position-assisted feature extraction module. Furthermore, training an MLLM from scratch is highly time-consuming. Thus, we propose incrementally extending existing pre-trained MLLMs to comprehend more modalities and the regional objects of those modalities. Specifically, we freeze the Q-Former from BLIP-2, an impressive MLLM, and optimize the modality-specific Lora parameters in Q-Former and LLM for each newly introduced modality. The freezing of the Q-Former eliminates the need for extensive pre-training on massive image-text data. The freezed Q-Former pre-trained from massive image-text data is also beneficial for the pre-training on image-region-text data. We name our framework RegionBLIP. We pre-train RegionBLIP on image-region-text, point-cloud-text, and point-cloud-region-text data. Experimental results verify that \\Ours{} can preserve the image comprehension capability of BILP-2 and further gain a comprehension of the newly introduced point cloud modality and regional objects. The Data, Code, and Pre-trained models will be available at https://github.com/mightyzau/RegionBLIP.", "title": "regionblip a unified multimodal pretraining framework for holistic and regional comprehension", "url": "https://arxiv.org/pdf/2308.02299", "tokenized_text": "work investigate extending comprehension multi modal large_language large language mllms regional objects end propose extract features corresponding regional objects soft llm provides straightforward scalable approach eliminates need llm fine tuning effectively extract regional features regular image features irregular point_cloud point cloud features present novel unified position assisted feature extraction module furthermore training mllm scratch highly time consuming propose incrementally extending existing pre trained mllms comprehend modalities regional objects modalities specifically blip-2 impressive mllm optimize modality specific lora parameters llm newly introduced modality freezing eliminates need extensive pre training massive image text data pre trained massive image text data beneficial pre training image region text data framework pre train image region text point cloud text point cloud region text data experimental_results experimental results verify preserve image comprehension capability gain comprehension newly introduced point_cloud point cloud modality regional objects data code pre trained available"}
{"id": "22d5459d1f47341b355feeb1becc37208d6ec365", "abstract": "Large language Models (LLMs) have achieved promising performance on arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting. However, LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems. Existing methods use coarse-grained feedback (e.g., whether the answer is correct) to improve factual consistency. In this work, we propose RCoT (Reversing Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs, generated solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct the problem based on generated solutions. Then fine-grained comparisons between the original problem and the reconstructed problem expose the factual inconsistency in the original solutions. To rectify the solution, RCoT formulates detected factual inconsistency into fine-grained feedback to guide LLMs in revising solutions. Experimental results demonstrate improvements of RCoT over standard CoT, Self-Consistency and Self-Refine across seven arithmetic datasets. Moreover, we find that manually written fine-grained feedback can dramatically improve LLMs' reasoning abilities (e.g., ChatGPT reaches 94.6% accuracy on GSM8K), encouraging the community to further explore the fine-grained feedback generation methods.", "title": "rcot detecting and rectifying factual inconsistency in reasoning by reversing chainofthought", "url": "https://arxiv.org/pdf/2305.11499", "tokenized_text": "large_language large language llms achieved promising performance arithmetic reasoning tasks incorporating step step chain thought cot llms face challenges maintaining factual consistency reasoning exhibiting tendencies condition overlooking question condition hallucination given problems existing_methods existing methods use coarse grained feedback e.g. answer correct improve factual consistency work propose chain thought novel method improve llms reasoning abilities automatically detecting factual inconsistency llms generated solutions detect factual inconsistency asks llms reconstruct problem based generated solutions fine grained comparisons original problem problem expose factual inconsistency original solutions rectify solution formulates detected factual inconsistency fine grained feedback guide llms solutions experimental_results experimental results demonstrate improvements standard cot self consistency self refine seven arithmetic datasets find manually written fine grained feedback dramatically improve llms reasoning abilities e.g. chatgpt reaches accuracy gsm8 encouraging community explore fine grained feedback generation methods"}
{"id": "2ef1c2438c3a4552db9e7080e15d8c51bc071f58", "abstract": "A long standing goal of the data management community is to develop general, automated systems that ingest semi-structured documents and output queryable tables without human effort or domain specific customization. Given the sheer variety of potential documents, state-of-the art systems make simplifying assumptions and use domain specific training. In this work, we ask whether we can maintain generality by using large language models (LLMs). LLMs, which are pretrained on broad data, can perform diverse downstream tasks simply conditioned on natural language task descriptions. We propose and evaluate EVAPORATE, a simple, prototype system powered by LLMs. We identify two fundamentally different strategies for implementing this system: prompt the LLM to directly extract values from documents or prompt the LLM to synthesize code that performs the extraction. Our evaluations show a cost-quality tradeoff between these two approaches. Code synthesis is cheap, but far less accurate than directly processing each document with the LLM. To improve quality while maintaining low cost, we propose an extended code synthesis implementation, EVAPORATE-CODE+, which achieves better quality than direct extraction. Our key insight is to generate many candidate functions and ensemble their extractions using weak supervision. EVAPORATE-CODE+ not only outperforms the state-of-the art systems, but does so using a sublinear pass over the documents with the LLM. This equates to a 110x reduction in the number of tokens the LLM needs to process, averaged across 16 real-world evaluation settings of 10k documents each.", "title": "language models enable simple systems for generating structured views of heterogeneous data lakes", "url": "http://arxiv.org/pdf/2304.09433", "tokenized_text": "long standing goal data management community develop general automated systems ingest semi structured documents output tables human effort domain specific customization given sheer variety potential documents state art systems assumptions use domain specific training work ask maintain generality large_language large language llms llms pretrained broad data perform diverse downstream_tasks downstream tasks simply conditioned natural_language natural language task descriptions propose evaluate simple prototype system powered llms identify fundamentally different strategies implementing system llm directly extract values documents llm synthesize code performs extraction evaluations cost quality tradeoff approaches code synthesis cheap far accurate directly processing document llm improve quality maintaining low cost propose extended code synthesis implementation achieves better quality direct extraction key insight generate candidate functions ensemble extractions weak supervision outperforms state art systems pass documents llm reduction number tokens llm needs process averaged 16 real world evaluation settings 10k documents"}
{"id": "2ff69c238e26c473a6d8bcbb9292ded74d7fd1c2", "abstract": "Compositional zero-shot learning (CZSL) task aims to recognize unseen compositional visual concepts, e.g., sliced tomatoes, where the model is learned only from the seen compositions, e.g., sliced potatoes and red tomatoes. Thanks to the prompt tuning on large pre-trained visual language models such as CLIP, recent literature shows impressively better CZSL performance than traditional vision-based methods. However, the key aspects that impact the generalization to unseen compositions, including the diversity and informativeness of class context, and the entanglement between visual primitives, i.e., state and object, are not properly addressed in existing CLIP-based CZSL literature. In this paper, we propose a model by prompting the language-informed distribution, aka., PLID, for the CZSL task. Specifically, the PLID leverages pre-trained large language models (LLM) to 1) formulate the language-informed class distributions which are diverse and informative, and 2) enhance the compositionality of the class embedding. Moreover, a visual-language primitive decomposition (VLPD) module and a stochastic logit mixup (SLM) strategy are proposed to dynamically fuse the decisions from the compositional and the primitive logit space. Orthogonal to the existing literature of soft, hard, or distributional prompts, our method advocates prompting the LLM-supported class distribution that leads to a better zero-shot generalization. Experimental results on MIT-States, UT-Zappos, and C-GQA datasets show the superior performance of the PLID to the prior arts.", "title": "prompting languageinformed distribution for compositional zeroshot learning", "url": "https://arxiv.org/pdf/2305.14428", "tokenized_text": "compositional zero shot_learning shot learning task aims recognize unseen compositional visual concepts e.g. sliced learned seen compositions e.g. sliced red thanks tuning large pre trained visual language_models language clip recent literature shows better performance traditional vision based methods key aspects impact generalization unseen compositions including diversity informativeness class context visual primitives i.e. state object properly addressed existing clip based literature paper propose language informed distribution aka task specifically leverages pre trained large_language large language llm formulate language informed class distributions diverse informative enhance compositionality class embedding visual language primitive decomposition module stochastic logit mixup slm strategy proposed dynamically fuse decisions compositional primitive logit space orthogonal existing literature soft hard distributional method advocates llm supported class distribution leads better zero shot generalization experimental_results experimental results mit states gqa datasets superior_performance superior performance prior arts"}
{"id": "34b35c89e192b5aa3118f667ce0a3cc0d89d82c3", "abstract": "To help users do complex work, researchers have developed techniques to integrate AI and human intelligence into user interfaces (UIs). With the recent introduction of large language models (LLMs), which can generate text in response to a natural language prompt, there are new opportunities to consider how to integrate LLMs into UIs. We present Prompt Middleware, a framework for generating prompts for LLMs based on UI affordances. These include prompts that are predefined by experts (static prompts), generated from templates with fill-in options in the UI (template-based prompts), or created from scratch (free-form prompts). We demonstrate this framework with FeedbackBuffet, a writing assistant that automatically generates feedback based on a user's text input. Inspired by prior research showing how templates can help non-experts perform more like experts, FeedbackBuffet leverages template-based prompt middleware to enable feedback seekers to specify the types of feedback they want to receive as options in a UI. These options are composed using a template to form a feedback request prompt to GPT-3. We conclude with a discussion about how Prompt Middleware can help developers integrate LLMs into UIs.", "title": "prompt middleware mapping prompts for large language models to ui affordances", "url": "http://arxiv.org/pdf/2307.01142", "tokenized_text": "help users complex work researchers developed techniques integrate ai human intelligence user interfaces uis recent introduction large_language large language llms generate text response natural_language natural language new opportunities consider integrate llms uis present middleware framework generating llms based ui include predefined experts static generated templates fill options ui template based created scratch free form demonstrate framework writing assistant automatically generates feedback based user text input inspired prior research showing templates help non experts perform like experts leverages template based middleware enable feedback specify types feedback want receive options ui options composed template form feedback request gpt-3 conclude discussion middleware help developers integrate llms uis"}
{"id": "35d855c49334ef1b8f945f13e9bc84868dab55c9", "abstract": "Purpose We aimed to evaluate the time and cost of developing prompts using large language model (LLM), tailored to extract clinical factors in breast cancer patients and their accuracy. Materials and Methods We collected data from reports of surgical pathology and ultrasound from breast cancer patients who underwent radiotherapy from 2020 to 2022. We extracted the information using the Generative Pre-trained Transformer (GPT) for Sheets and Docs extension plugin and termed this the \u201cLLM\u201d method. The time and cost of developing the prompts with LLM methods were assessed and compared with those spent on collecting information with \u201cfull manual\u201d and \u201cLLM-assisted manual\u201d methods. To assess accuracy, 340 patients were randomly selected, and the extracted information by LLM method were compared with those collected by \u201cfull manual\u201d method. Results Data from 2,931 patients were collected. We developed 12 prompts for Extract function and 12 for Format function to extract and standardize the information. The overall accuracy was 87.7%. For lymphovascular invasion, it was 98.2%. Developing and processing the prompts took 3.5 hours and 15 minutes, respectively. Utilizing the ChatGPT application programming interface cost US $65.8 and when factoring in the estimated wage, the total cost was US $95.4. In an estimated comparison, \u201cLLM-assisted manual\u201d and \u201cLLM\u201d methods were time- and cost-efficient compared to the \u201cfull manual\u201d method. Conclusion Developing and facilitating prompts for LLM to derive clinical factors was efficient to extract crucial information from huge medical records. This study demonstrated the potential of the application of natural language processing using LLM model in breast cancer patients. Prompts from the current study can be re-used for other research to collect clinical information.", "title": "developing prompts from large language model for extracting clinical information from pathology and ultrasound reports in breast cancer", "url": "https://www.e-roj.org/upload/pdf/roj-2023-00633.pdf", "tokenized_text": "purpose aimed evaluate time cost developing large_language large language llm tailored extract clinical factors cancer patients accuracy materials methods collected data reports pathology cancer patients 2020 2022 extracted information generative pre trained transformer gpt extension termed llm method time cost developing llm methods assessed compared spent collecting information manual llm assisted manual methods assess accuracy patients randomly selected extracted information llm method compared collected manual method results data patients collected developed 12 extract function 12 format function extract standardize information overall accuracy 87.7 developing processing took 3.5 hours 15 minutes respectively utilizing chatgpt application programming interface cost estimated total cost estimated comparison llm assisted manual llm methods time- cost efficient compared manual method conclusion developing facilitating llm derive clinical factors efficient extract crucial information huge medical records study demonstrated potential application natural_language natural language processing llm cancer patients current study research collect clinical information"}
{"id": "45ee010607cad91728ae7fbad6cce3d805b93526", "abstract": "Large Language Models (LLMs) have the ability to solve a variety of tasks, such as text summarization and mathematical questions, just out of the box, but they are often trained with a single task in mind. Due to high computational costs, the current trend is to use prompt instruction tuning to better adjust monolithic, pretrained LLMs for new -- but often individual -- downstream tasks. Thus, how one would expand prompt tuning to handle -- concomitantly -- heterogeneous tasks and data distributions is a widely open question. To address this gap, we suggest the use of \\emph{Mixture of Prompts}, or MoPs, associated with smart gating functionality: the latter -- whose design is one of the contributions of this paper -- can identify relevant skills embedded in different groups of prompts and dynamically assign combined experts (i.e., collection of prompts), based on the target task. Additionally, MoPs are empirically agnostic to any model compression technique applied -- for efficiency reasons -- as well as instruction data source and task composition. In practice, MoPs can simultaneously mitigate prompt training\"interference\"in multi-task, multi-source scenarios (e.g., task and data heterogeneity across sources), as well as possible implications from model approximations. As a highlight, MoPs manage to decrease final perplexity from $\\sim20\\%$ up to $\\sim70\\%$, as compared to baselines, in the federated scenario, and from $\\sim 3\\%$ up to $\\sim30\\%$ in the centralized scenario.", "title": "sweeping heterogeneity with smart mops mixture of prompts for llm task adaptation", "url": "https://arxiv.org/pdf/2310.02842", "tokenized_text": "large_language large language llms ability solve variety tasks text summarization mathematical questions box trained single task mind high computational costs current trend use instruction_tuning instruction tuning better adjust pretrained llms new individual downstream_tasks downstream tasks expand tuning handle heterogeneous tasks data distributions widely open question address gap suggest use mops associated smart functionality design contributions paper identify relevant skills embedded different groups dynamically assign combined experts i.e. collection based target task additionally mops empirically agnostic compression technique applied efficiency reasons instruction data source task composition practice mops simultaneously mitigate multi task multi source scenarios e.g. task data heterogeneity sources possible implications highlight mops manage decrease final perplexity compared baselines federated scenario centralized scenario"}
{"id": "486a8c8655b81c7f87ff257141466ec1186d4aea", "abstract": "Foundation models, such as GPT-4, DALL-E have brought unprecedented AI\"operating system\"effect and new forms of human-AI interaction, sparking a wave of innovation in AI-native services, where natural language prompts serve as executable\"code\"directly (prompt as executable code), eliminating the need for programming language as an intermediary and opening up the door to personal AI. Prompt Sapper has emerged in response, committed to support the development of AI-native services by AI chain engineering. It creates a large language model (LLM) empowered software engineering infrastructure for authoring AI chains through human-AI collaborative intelligence, unleashing the AI innovation potential of every individual, and forging a future where everyone can be a master of AI innovation. This article will introduce the R\\&D motivation behind Prompt Sapper, along with its corresponding AI chain engineering methodology and technical practices.", "title": "prompt sapper llmempowered software engineering infrastructure for ainative services", "url": "http://arxiv.org/pdf/2306.02230", "tokenized_text": "foundation_models foundation gpt-4 dall brought unprecedented new forms human ai interaction wave innovation ai native services natural_language natural language serve executable code eliminating need programming language intermediary opening door personal ai emerged response committed support development ai native services ai chain engineering creates large_language large language llm empowered software engineering infrastructure authoring ai chains human ai collaborative intelligence ai innovation potential individual future master ai innovation article introduce motivation corresponding ai chain engineering methodology technical practices"}
{"id": "4a8fe7ecf225e5bada08642fcd77d3cbb322b967", "abstract": "How humans infer discrete emotions is a fundamental research question in the field of psychology. While conceptual knowledge about emotions (emotion knowledge) has been suggested to be essential for emotion inference, evidence to date is mostly indirect and inconclusive. As the large language models (LLMs) have been shown to support effective representations of various human conceptual knowledge, the present study further employed artificial neurons in LLMs to investigate the mechanism of human emotion inference. With artificial neurons activated by prompts, the LLM (RoBERTa) demonstrated a similar conceptual structure of 27 discrete emotions as that of human behaviors. Furthermore, the LLM-based conceptual structure revealed a human-like reliance on 14 underlying conceptual attributes of emotions for emotion inference. Most importantly, by manipulating attribute-specific neurons, we found that the corresponding LLM's emotion inference performance deteriorated, and the performance deterioration was correlated to the effectiveness of representations of the conceptual attributes on the human side. Our findings provide direct evidence for the emergence of emotion knowledge representation in large language models and suggest its casual support for discrete emotion inference. # These authors contributed equally: liming16@tsinghua.org.cn, yushengsu.thu@gmail.com * Corresponding authors: {liuzy, dzhang}@tsinghua.edu.cn The source code can be obtained from https://github.com/thunlp/Model_Emotion.", "title": "human emotion knowledge representation emerges in large language model and supports discrete emotion inference", "url": "https://arxiv.org/pdf/2302.09582", "tokenized_text": "humans infer discrete emotions fundamental research question field psychology conceptual knowledge emotions emotion knowledge suggested essential emotion inference evidence date indirect large_language large language llms shown support effective representations human conceptual knowledge present study employed artificial neurons llms investigate mechanism human emotion inference artificial neurons activated llm roberta demonstrated similar conceptual structure 27 discrete emotions human behaviors furthermore llm based conceptual structure revealed human like reliance 14 underlying conceptual attributes emotions emotion inference importantly manipulating attribute specific neurons found corresponding llm emotion inference performance performance deterioration correlated effectiveness representations conceptual attributes human findings provide direct evidence emergence emotion knowledge representation large_language large language suggest casual support discrete emotion inference authors contributed equally corresponding authors source_code source code obtained"}
{"id": "52136f813243ac3de8e277906112a41590a376d4", "abstract": "Market sentiment analysis on social media content requires knowledge of both financial markets and social media jargon, which makes it a challenging task for human raters. The resulting lack of high-quality labeled data stands in the way of conventional supervised learning methods. Instead, we approach this problem using semi-supervised learning with a large language model (LLM). Our pipeline generates weak financial sentiment labels for Reddit posts with an LLM and then uses that data to train a small model that can be served in production. We find that prompting the LLM to produce Chain-of-Thought summaries and forcing it through several reasoning paths helps generate more stable and accurate labels, while using a regression loss further improves distillation quality. With only a handful of prompts, the final model performs on par with existing supervised models. Though production applications of our model are limited by ethical considerations, the model\u2019s competitive performance points to the great potential of using LLMs for tasks that otherwise require skill-intensive annotation.", "title": "what do llms know about financial markets a case study on reddit market sentiment analysis", "url": "http://arxiv.org/pdf/2212.11311", "tokenized_text": "market sentiment_analysis sentiment analysis social_media social media content requires knowledge financial markets social_media social media makes challenging task human raters resulting lack high quality labeled_data labeled data stands way conventional supervised learning methods instead approach problem semi supervised learning large_language large language llm pipeline generates weak financial sentiment labels reddit posts llm uses data train small served production find llm produce chain thought summaries forcing reasoning paths helps generate stable accurate labels regression loss improves distillation quality handful final performs par existing supervised production applications limited ethical considerations competitive_performance competitive performance points great_potential great potential llms tasks require skill intensive annotation"}
{"id": "5882dd04d95c9c88cdec389059fcf44d56cbb789", "abstract": "Language models have steadily increased in size over the past few years. They achieve a high level of performance on various natural language processing (NLP) tasks such as question answering and summarization. Large language models (LLMs) have been used for generation and can now output human-like text. Due to this, there are other downstream tasks in the realm of dialog that can now harness the LLMs' language understanding capabilities. Dialog evaluation is one task that this paper will explore. It concentrates on prompting with LLMs: BLOOM, OPT, GPT-3, Flan-T5, InstructDial and TNLGv2. The paper shows that the choice of datasets used for training a model contributes to how well it performs on a task as well as on how the prompt should be structured. Specifically, the more diverse and relevant the group of datasets that a model is trained on, the better dialog evaluation performs. This paper also investigates how the number of examples in the prompt and the type of example selection used affect the model's performance.", "title": "understanding the effectiveness of very large language models on dialog evaluation", "url": "http://arxiv.org/pdf/2301.12004", "tokenized_text": "language_models language steadily increased size past years achieve high_level high level performance natural_language natural language processing nlp tasks question_answering question answering summarization large_language large language llms generation output human like text downstream_tasks downstream tasks realm dialog harness llms language understanding capabilities dialog evaluation task paper explore llms bloom opt gpt-3 flan t5 paper shows choice datasets training contributes performs task structured specifically diverse relevant group datasets trained better dialog evaluation performs paper investigates number examples type example selection affect performance"}
{"id": "62176de125738e3b95850d1227bac81fd646b78e", "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.", "title": "planandsolve prompting improving zeroshot chainofthought reasoning by large language models", "url": "http://arxiv.org/pdf/2305.04091", "tokenized_text": "large_language large language llms recently shown deliver impressive performance nlp_tasks nlp tasks tackle multi step reasoning tasks shot chain thought cot includes manually crafted step step reasoning demonstrations enable llms explicitly generate reasoning_steps reasoning steps improve reasoning task accuracy eliminate manual efforts zero shot cot concatenates target problem statement let think step_by_step step step input llms despite success zero shot cot suffers pitfalls calculation errors missing step errors semantic errors address missing step errors propose plan solve consists components plan divide entire task smaller subtasks carrying subtasks according plan address calculation errors improve quality generated reasoning_steps reasoning steps extend detailed instructions derive evaluate proposed strategy datasets reasoning problems experimental_results experimental results gpt-3 proposed zero shot_prompting shot consistently_outperforms consistently outperforms zero shot cot datasets large margin comparable exceeds zero shot program thought_prompting thought comparable performance shot cot_prompting cot math reasoning problem code found"}
{"id": "70916fbeb446ab7dc811ab74b193365d789bf1eb", "abstract": "The way and content in which users ask questions can provide insight into their current status, including their personality, emotions, and psychology. Instead of directly prompting the large language models (LLMs), we explore how chain-of-thought prompting helps in this scenario to perform reasoning and planning according to user status, aiming to provide a more personalized and engaging experience for the user query. To this end, we \ufb01rst construct a benchmark of 6 dialogue or question-answering datasets in both English and Chinese, covering 3 different aspects of user status ( including personality , emotion , and psychology ). Then we prompt the LLMs to generate the response regarding the user status as intermediate reasoning processing. We propose a novel demonstration selection strategy using the semantic similarity of intermediate reasoning instead of test queries. To evaluate the effectiveness and robustness of our approach, we conduct extensive experiments with 7 LLMs under zero-shot and one-shot settings. The experimental results show that our approach consistently outperforms standard prompting in terms of both helpfulness and acceptness across all datasets, regardless of the LLMs used. The code and dataset can be found at https://github.com/ruleGreen/ Dialogue_CoT.git .", "title": "chainofthought prompting for responding to indepth dialogue questions with llm", "url": "http://arxiv.org/pdf/2305.11792", "tokenized_text": "way content users ask questions provide insight current status including personality emotions psychology instead directly large_language large language llms explore chain thought_prompting thought helps scenario perform reasoning planning according user status aiming provide personalized engaging experience user query end \ufb01rst construct benchmark dialogue question answering datasets english chinese covering different aspects user status including personality emotion psychology llms generate response user status intermediate reasoning processing propose_a_novel propose novel demonstration selection strategy semantic similarity intermediate reasoning instead test queries evaluate effectiveness robustness approach conduct_extensive conduct extensive experiments llms zero shot shot_settings shot settings experimental_results experimental results approach consistently_outperforms consistently outperforms standard terms helpfulness datasets regardless llms code dataset found"}
{"id": "70da4fb798a86cbe8cad96c27ced0415885bbd9d", "abstract": "Many natural language processing (NLP) tasks rely on labeled data to train machine learning models to achieve high performance. However, data annotation can be a time-consuming and expensive process, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator by providing them with sufficient guidance and demonstrated examples. To make LLMs to be better annotators, we propose a two-step approach, 'explain-then-annotate'. To be more precise, we begin by creating prompts for every demonstrated example, which we subsequently utilize to prompt a LLM to provide an explanation for why the specific ground truth answer/label was chosen for that particular example. Following this, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data. We conduct experiments on three tasks, including user input and keyword relevance assessment, BoolQ and WiC. The annotation results from GPT-3.5 surpasses those from crowdsourced annotation for user input and keyword relevance assessment. Additionally, for the other two tasks, GPT-3.5 achieves results that are comparable to those obtained through crowdsourced annotation.", "title": "annollm making large language models to be better crowdsourced annotators", "url": "http://arxiv.org/pdf/2303.16854", "tokenized_text": "natural_language natural language processing nlp tasks rely labeled_data labeled data train machine_learning machine learning achieve high performance data annotation time consuming expensive process especially task involves large data requires specialized domains recently gpt-3.5 series demonstrated_remarkable demonstrated remarkable shot zero shot ability nlp_tasks nlp tasks paper claim large_language large language llms gpt-3.5 serve excellent crowdsourced annotator providing sufficient guidance demonstrated examples llms better annotators propose step approach explain annotate precise begin creating demonstrated example subsequently utilize llm provide explanation specific ground_truth ground truth answer label chosen particular example following construct shot chain thought self generated explanation employ annotate unlabeled data conduct experiments tasks including user input keyword relevance assessment annotation results gpt-3.5 surpasses crowdsourced annotation user input keyword relevance assessment additionally tasks gpt-3.5 achieves results comparable obtained crowdsourced annotation"}
{"id": "74b94891f8f7ac8d73d9df817b6720e1cb792bcc", "abstract": "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.", "title": "enhancing small medical learners with privacypreserving contextual prompting", "url": "http://arxiv.org/pdf/2305.12723", "tokenized_text": "large_language large language llms demonstrate remarkable medical expertise data privacy concerns impede direct use healthcare environments offering improved data privacy protection domain specific small language_models language slms underperform llms emphasizing need methods reduce performance gap alleviating privacy concerns paper present simple effective method harnesses llms medical proficiency boost slm performance medical tasks privacy restricted scenarios specifically mitigate patient privacy issues extracting keywords medical data llm generate medical knowledge intensive context simulating clinicians thought processes context serves additional input slms augmenting decision making capabilities method significantly enhances performance shot training settings medical knowledge intensive tasks achieving increase absolute accuracy compared slm fine tuning context sets new state art results medical tasks privacy restricted scenarios domain testing experiments general domain datasets showcase generalizability broad applicability"}
{"id": "76f54657eb0893a0b203da57dcf0b4fffeebfc2c", "abstract": "Story generation and understanding -- as with all NLG/NLU tasks -- has seen a surge in neurosymbolic work. Researchers have recognized that, while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for any flaws that the neural networks might have. However, symbolic methods are extremely costly in terms of the amount of time and expertise needed to create them. In this work, we capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use of symbolic methods for tracking the state of stories and aiding in story understanding. We show that our CoRRPUS system and abstracted prompting procedures can beat current state-of-the-art structured LLM techniques on pre-existing story understanding tasks (bAbI Task 2 and Re^3) with minimal hand engineering. We hope that this work can help highlight the importance of symbolic representations and specialized prompting for LLMs as these models require some guidance for performing reasoning tasks properly.", "title": "corrpus codebased structured prompting for neurosymbolic story understanding", "url": "https://aclanthology.org/2023.findings-acl.832.pdf", "tokenized_text": "story generation understanding nlg nlu tasks seen surge work researchers recognized large_language large language llms tremendous utility augmented symbolic means better flaws neural_networks neural networks symbolic methods extremely costly terms time expertise needed create work capitalize state art code llms codex bootstrap use symbolic methods tracking state stories aiding story understanding system abstracted procedures beat current state art structured llm techniques pre existing story understanding tasks babi task minimal hand engineering hope work help highlight importance symbolic representations specialized llms require guidance performing reasoning tasks properly"}
{"id": "7c1707db9aafd209aa93db3251e7ebd593d55876", "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.", "title": "selfcheckgpt zeroresource blackbox hallucination detection for generative large language models", "url": "https://arxiv.org/pdf/2303.08896", "tokenized_text": "generative large_language large language llms gpt-3 capable generating highly fluent responses wide variety user llms known hallucinate facts non factual statements trust output existing fact checking approaches require access output probability distribution available systems chatgpt external databases separate complex modules work simple sampling based approach fact check responses black box zero resource fashion i.e. external database leverages simple idea llm knowledge given concept sampled responses likely similar contain consistent facts hallucinated facts sampled responses likely diverge contradict investigate approach gpt-3 generate passages individuals dataset manually annotate factuality generated passages demonstrate detect non factual factual sentences ii rank passages terms factuality compare approach baselines approach considerably higher auc pr scores sentence level hallucination detection higher correlation scores passage level factuality assessment compared methods"}
{"id": "7ce0c89a452e3c2917b63847495533865697c79c", "abstract": "Previous work has shown that there exists a scaling law between the size of Language Models (LMs) and their zero-shot performance on different downstream NLP tasks. In this work, we show that this phenomenon does not hold when evaluating large LMs on tasks with negated prompts, but instead shows an inverse scaling law. We evaluate 9 different tasks with negated prompts on (1) pretrained LMs (OPT&GPT-3) of varying sizes (125M - 175B), (2) LMs further pretrained to generalize to novel prompts (InstructGPT), (3) LMs provided with few-shot examples, and (4) LMs fine-tuned specifically on negated prompts; all LM types perform worse on negated prompts as they scale and show a huge performance gap between the human performance when comparing the average score on both original and negated prompts. By highlighting a critical limitation of existing LMs and methods, we urge the community to develop new approaches of developing LMs that actually follow the given instructions. We provide the code and the datasets to explore negated prompts at https://github.com/joeljang/negated-prompts-for-llms", "title": "can large language models truly understand prompts a case study with negated prompts", "url": "http://arxiv.org/pdf/2209.12711", "tokenized_text": "previous work shown exists scaling law size language_models language lms zero shot performance different downstream nlp_tasks nlp tasks work phenomenon hold evaluating large lms tasks instead shows inverse scaling law evaluate different tasks pretrained lms varying sizes 125 175b lms pretrained generalize novel instructgpt lms provided shot examples lms fine tuned specifically lm types perform worse scale huge performance gap human performance comparing average score original highlighting critical limitation existing lms methods community develop new approaches developing lms actually follow given instructions provide code datasets explore"}
{"id": "80ae1347b2dda02748f8f09da8a738121f5edfb5", "abstract": "Due to the prohibitively high cost of creating error correction datasets, most Factual Claim Correction methods rely on a powerful verification model to guide the correction process. This leads to a significant drop in performance in domains like Scientific Claim Correction, where good verification models do not always exist. In this work, we introduce a claim correction system that makes no domain assumptions and does not require a verifier but is able to outperform existing methods by an order of magnitude \u2014 achieving 94% correction accuracy on the SciFact dataset, and 62.5% on the SciFact-Open dataset, compared to the next best meth-ods 0.5% and 1.50% respectively. Our method leverages the power of prompting with LLMs during training to create a richly annotated dataset that can be used for fully supervised training and regularization. We additionally use a claim-aware decoding procedure to improve the quality of corrected claims. Our method is competitive with the very LLM that was used to generate the annotated dataset \u2014 with GPT3.5 achieving 89.5% and 60% correction accuracy on SciFact and SciFact-Open, despite using 1250 times as many parameters as our model.", "title": "the student becomes the master matching gpt3 on scientific factual error correction", "url": "https://arxiv.org/pdf/2305.14707", "tokenized_text": "prohibitively high cost creating error correction datasets factual claim correction methods rely powerful verification guide correction process leads significant drop performance domains like scientific claim correction good verification exist work introduce claim correction system makes domain assumptions require verifier able outperform existing_methods existing methods order magnitude achieving 94 correction accuracy scifact dataset 62.5 scifact open dataset compared best 0.5 respectively method leverages power llms training create annotated dataset fully_supervised fully supervised training regularization additionally use claim aware decoding procedure improve quality corrected claims method competitive llm generate annotated dataset gpt3.5 achieving 60 correction accuracy scifact scifact open despite times parameters"}
{"id": "8fdd34153d1035d09dd4a6efa9cb0c91d23d0045", "abstract": "We are currently witnessing dramatic advances in the capabilities of Large Language Models (LLMs). They are already being adopted in practice and integrated into many systems, including integrated development environments (IDEs) and search engines. The functionalities of current LLMs can be modulated via natural language prompts, while their exact internal functionality remains implicit and unassessable. This property, which makes them adaptable to even unseen tasks, might also make them susceptible to targeted adversarial prompting . Recently, several ways to misalign LLMs using Prompt Injection (PI) attacks have been introduced. In such attacks, an adversary can prompt the LLM to produce malicious content or override the original instructions and the employed \ufb01ltering schemes. Recent work showed that these attacks are hard to mitigate, as state-of-the-art LLMs are instruction-following . So far, these attacks assumed that the adversary is directly prompting the LLM. In this work, we show that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs ) induces a whole new set of attack vectors. These LLMs might process poisoned content retrieved from the Web that contains malicious prompts pre-injected and selected by adversaries. We demonstrate that an attacker can indirectly perform such PI attacks. Based on this key insight, we systematically analyze the resulting threat landscape of Application-Integrated LLMs and discuss a variety of new attack vectors. To demonstrate the practical viabil-ity of our attacks, we implemented speci\ufb01c demonstrations", "title": "more than you've asked for a comprehensive analysis of novel prompt injection threats to applicationintegrated large language models", "url": "http://arxiv.org/pdf/2302.12173", "tokenized_text": "currently witnessing dramatic advances capabilities large_language large language llms adopted practice integrated systems including integrated development environments ides search engines functionalities current llms modulated natural_language natural language exact internal functionality remains implicit property makes adaptable unseen tasks susceptible targeted adversarial recently ways misalign llms prompt_injection injection pi attacks introduced attacks adversary llm produce malicious content override original instructions employed schemes recent_work recent work showed attacks hard mitigate state art llms instruction following far attacks assumed adversary directly llm work augmenting llms retrieval api calling capabilities called application integrated llms induces new set attack vectors llms process poisoned content retrieved web contains malicious pre injected selected adversaries demonstrate attacker indirectly perform pi attacks based key insight systematically analyze resulting threat landscape application integrated llms discuss variety new attack vectors demonstrate practical attacks implemented speci\ufb01c demonstrations"}
{"id": "9efa81ec4954b0859c47dad8f42edfaf8bced69b", "abstract": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.", "title": "boosting language models reasoning with chainofknowledge prompting", "url": "https://arxiv.org/pdf/2306.06427", "tokenized_text": "recently chain thought cot success complex_reasoning complex reasoning tasks aims designing simple like let think step_by_step step step multiple context exemplars designed rationales elicit large_language large language llms generate intermediate reasoning_steps reasoning steps generated rationales come mistakes making unfaithful reasoning chains mitigate brittleness propose_a_novel propose novel chain knowledge aim eliciting llms generate explicit pieces knowledge evidence form structure triple inspired human behaviors i.e. draw mind map knowledge map reasoning evidence brain answering complex question benefiting additionally introduce verification method estimate reliability reasoning chains terms factuality faithfulness unreliable response wrong evidence indicated llm extensive_experiments extensive experiments demonstrate method improve performance commonsense factual symbolic arithmetic reasoning tasks"}
{"id": "ab90169f7213482efff246cc5f5f057351265f18", "abstract": "Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement (through, for example, rating its interest) boosts a later truthfulness test rating. Data was collected from 1000 human participants using an online experiment, and 1000 simulated participants using engineered prompts and LLM completion. 64 ratings per participant were collected, using all exposure-test combinations of the attributes: truth, interest, sentiment and importance. The results for human participants reconfirmed the ITE, and demonstrated an absence of effect for attributes other than truth, and when the same attribute is used for exposure and test. The same pattern of effects was found for LLM-simulated participants. The second study concerns a specific mode of influence - populist framing of news to increase its persuasion and political mobilization. Data from LLM-simulated participants was collected and compared to previously published data from a 15-country experiment on 7286 human participants. Several effects previously demonstrated from the human study were replicated by the simulated study, including effects that surprised the authors of the human study by contradicting their theoretical expectations (anti-immigrant framing of news decreases its persuasion and mobilization); but some significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation of the participant) were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence.", "title": "susceptibility to influence of large language models", "url": "http://arxiv.org/pdf/2303.06074", "tokenized_text": "studies tested hypothesis large_language large language llm psychological change following exposure influential input study tested generic mode influence illusory truth effect earlier exposure statement example rating interest boosts later truthfulness test rating data collected 1000 human participants online experiment 1000 simulated participants engineered llm completion 64 ratings participant collected exposure test combinations attributes truth interest sentiment importance results human participants demonstrated absence effect attributes truth attribute exposure test pattern effects found llm simulated participants second study concerns specific mode influence framing news increase political data llm simulated participants collected compared previously published data 15 experiment human participants effects previously demonstrated human study simulated study including effects authors human study theoretical expectations anti framing news decreases significant relationships found human data modulation effectiveness framing according relative participant present llm data studies support view llms potential act effect influence"}
{"id": "b8d06dd769f89d08bdd9997d7bd363c89ede845b", "abstract": "We explore the use of large language models (LLMs) for zero-shot semantic parsing. Semantic parsing involves mapping natural language utterances to task-specific meaning representations. Language models are generally trained on the publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting. In this work, we propose ZEROTOP, a zero-shot task-oriented parsing method that decomposes a semantic parsing problem into a set of abstractive and extractive question-answering (QA) problems, enabling us to leverage the ability of LLMs to zero-shot answer reading comprehension questions. For each utterance, we prompt the LLM with questions corresponding to its top-level intent and a set of slots and use the LLM generations to construct the target meaning representation. We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions corresponding to missing slots. To address this problem, we fine-tune a language model on public QA datasets using synthetic negative samples. Experimental results show that our QA-based decomposition paired with the fine-tuned LLM can correctly parse ~16% of utterances in the MTOP dataset without requiring any annotated data.", "title": "zerotop zeroshot taskoriented semantic parsing using large language models", "url": "http://arxiv.org/pdf/2212.10815", "tokenized_text": "explore use large_language large language llms zero shot semantic_parsing semantic parsing semantic_parsing semantic parsing involves mapping natural_language natural language utterances task specific meaning representations language_models language generally trained publicly_available publicly available text code expected directly generalize domain specific parsing tasks zero shot_setting shot setting work propose zero shot task oriented parsing method decomposes semantic_parsing semantic parsing problem set abstractive extractive question answering qa problems enabling leverage ability llms zero shot answer reading comprehension questions utterance llm questions corresponding level intent set slots use llm generations construct target meaning representation observe current llms fail detect unanswerable questions result handle questions corresponding missing slots address problem fine tune language_model language public qa datasets synthetic negative samples experimental_results experimental results qa based decomposition paired fine tuned llm correctly parse utterances mtop dataset requiring annotated_data annotated data"}
{"id": "c4f9f0cc8c138047a61bdb11b1a352e3d1aed035", "abstract": "Understanding labour market dynamics requires accurately identifying the skills required for and possessed by the workforce. Automation techniques are increasingly being developed to support this effort. However, automatically extracting skills from job postings is challenging due to the vast number of existing skills. The ESCO (European Skills, Competences, Qualifications and Occupations) framework provides a useful reference, listing over 13,000 individual skills. However, skills extraction remains difficult and accurately matching job posts to the ESCO taxonomy is an open problem. In this work, we propose an end-to-end zero-shot system for skills extraction from job descriptions based on large language models (LLMs). We generate synthetic training data for the entirety of ESCO skills and train a classifier to extract skill mentions from job posts. We also employ a similarity retriever to generate skill candidates which are then re-ranked using a second LLM. Using synthetic data achieves an RP@10 score 10 points higher than previous distant supervision approaches. Adding GPT-4 re-ranking improves RP@10 by over 22 points over previous methods. We also show that Framing the task as mock programming when prompting the LLM can lead to better performance than natural language prompts, especially with weaker LLMs. We demonstrate the potential of integrating large language models at both ends of skills matching pipelines. Our approach requires no human annotations and achieve extremely promising results on skills extraction against ESCO.", "title": "large language models as batteriesincluded zeroshot esco skills matchers", "url": "https://arxiv.org/pdf/2307.03539", "tokenized_text": "understanding market dynamics requires accurately identifying skills required automation techniques increasingly developed support effort automatically extracting skills job challenging vast number existing skills european framework provides useful reference individual skills skills extraction remains difficult accurately matching job posts taxonomy open problem work propose end end zero shot system skills extraction job descriptions based large_language large language llms generate synthetic training_data training data entirety skills train classifier extract skill mentions job posts employ similarity retriever generate skill candidates ranked second llm synthetic data achieves score 10 points higher previous distant supervision approaches adding gpt-4 ranking improves 22 points previous methods framing task mock programming llm lead better performance natural_language natural language especially weaker llms demonstrate potential integrating large_language large language skills matching pipelines approach requires human annotations achieve extremely promising_results promising results skills extraction"}
{"id": "c91f6eb320c70e2f64b6fb935494978a8699f06a", "abstract": "As we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like those on SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing these failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts for LLMs to categorize these by four dimensions: type of compromise, intent, nature, and impact. GPT 3.5s categorizations had an average accuracy of 68% and Bard had an accuracy of 58% over these dimensions. We report that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. Future work can improve LLM performance in this context, and study a broader range of articles and failures.", "title": "an empirical study on using large language models to analyze software supply chain security failures", "url": "https://arxiv.org/pdf/2308.04898", "tokenized_text": "increasingly depend software systems consequences software supply chain severe high profile cyber attacks like resulted significant financial data losses underlining need stronger cybersecurity way prevent future studying past failures traditional methods analyzing failures require manually reading summarizing reports automated support reduce costs allow analysis failures natural_language natural language processing nlp techniques large_language large language llms leveraged assist analysis failures study assessed ability large_language large language llms analyze historical software supply chain llms replicate manual analysis software supply chain security failures performed members cloud native computing foundation developed llms categorize dimensions type compromise intent nature impact gpt average accuracy bard accuracy 58 dimensions report llms effectively characterize software supply chain failures source articles detailed consensus manual analysts replace human analysts future work improve llm performance context study broader range articles failures"}
{"id": "cb2954127a7fce8ab84486765392ce95dcdd8175", "abstract": "We introduce Action-GPT, a plug-and-play framework for incorporating Large Language Models (LLMs) into text-based action generation models. Action phrases in current motion capture datasets contain minimal and to-the-point information. By carefully crafting prompts for LLMs, we generate richer and fine-grained descriptions of the action. We show that utilizing these detailed descriptions instead of the original action phrases leads to better alignment of text and motion spaces. We introduce a generic approach compatible with stochastic (e.g. VAE-based) and deterministic (e.g. MotionCLIP) text-to-motion models. In addition, the approach enables multiple text descriptions to be utilized. Our experiments show (i) noticeable qualitative and quantitative improvement in the quality of synthesized motions, (ii) benefits of utilizing multiple LLM-generated descriptions, (iii) suitability of the prompt function, and (iv) zero-shot generation capabilities of the proposed approach. Code and pretrained models are available at https://actiongpt.github.io.", "title": "actiongpt leveraging largescale language models for improved and generalized action generation", "url": "https://arxiv.org/pdf/2211.15603", "tokenized_text": "introduce action gpt plug play framework incorporating large_language large language llms text based action generation action phrases current motion capture datasets contain minimal point information carefully crafting llms generate richer fine grained descriptions action utilizing detailed descriptions instead original action phrases leads better alignment text motion spaces introduce generic approach compatible stochastic e.g. vae based deterministic e.g. text motion addition approach enables multiple text descriptions utilized experiments noticeable qualitative quantitative improvement quality synthesized motions ii benefits utilizing multiple llm generated descriptions iii suitability function iv zero shot generation capabilities proposed approach code pretrained available"}
{"id": "d44031f253668c61ac6d68b95bbe9cac57730d51", "abstract": "Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific dense retrievers. We design a filter to select high-quality example document-query pairs in the prompt to further improve the quality of weak tagged queries. To the best of our knowledge, there is no prior work utilizing soft prompt tuning to augment DR models. The experiments demonstrate that SPTAR outperforms the unsupervised baselines BM25 and the recently proposed LLMs-based augmentation method for DR.", "title": "soft prompt tuning for augmenting dense retrieval with large language models", "url": "https://arxiv.org/pdf/2307.08303", "tokenized_text": "dense retrieval dr converts queries documents dense embeddings measures similarity queries documents vector space challenges dr lack domain specific training_data training data dr learn large scale public datasets like ms marco transfer learning evidence shows dr domains benefit transfer learning equally recently researchers large_language large language llms improve zero shot shot dr hard human written utilized works guarantee good quality generated weak queries tackle propose soft tuning augmenting dr task leverage soft tuning optimize task specific soft limited ground_truth ground truth data llms tag unlabeled documents weak queries yielding weak document query pairs train task specific dense retrievers design filter select high quality example document query pairs improve quality weak queries best knowledge prior_work prior work utilizing soft tuning augment dr experiments_demonstrate experiments demonstrate outperforms unsupervised baselines bm25 recently proposed llms based augmentation method dr"}
{"id": "dedfe929d182cc3537a9ed765d589b4735ce062a", "abstract": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning tasks. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains. However, the results in the heuristic mode show more promise. In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation.", "title": "on the planning abilities of large language models a critical investigation", "url": "http://arxiv.org/pdf/2305.15771", "tokenized_text": "intrigued claims emergent reasoning capabilities llms trained general web corpora paper set investigate planning capabilities aim evaluate effectiveness llms generating plans autonomously commonsense planning tasks potential llms source heuristic guidance agents ai planners planning tasks conduct systematic study generating suite instances domains similar ones employed international planning competition evaluate llms distinct modes autonomous heuristic findings reveal llms ability generate executable plans autonomously limited best gpt-4 having average success_rate success rate domains results heuristic mode promise heuristic mode demonstrate llm generated plans improve search process underlying sound planners additionally external verifiers help provide feedback generated plans llm better plan generation"}
{"id": "02540ae926814f4b7972d3fa4dd33932fdc4b58b", "abstract": "We introduce Noise2Music, where a series of diffusion models is trained to generate high-quality 30-second music clips from text prompts. Two types of diffusion models, a generator model, which generates an intermediate representation conditioned on text, and a cascader model, which generates high-fidelity audio conditioned on the intermediate representation and possibly the text, are trained and utilized in succession to generate high-fidelity music. We explore two options for the intermediate representation, one using a spectrogram and the other using audio with lower fidelity. We find that the generated audio is not only able to faithfully reflect key elements of the text prompt such as genre, tempo, instruments, mood, and era, but goes beyond to ground fine-grained semantics of the prompt. Pretrained large language models play a key role in this story -- they are used to generate paired text for the audio of the training set and to extract embeddings of the text prompts ingested by the diffusion models. Generated examples: https://google-research.github.io/noise2music", "title": "noise2music textconditioned music generation with diffusion models", "url": "http://arxiv.org/pdf/2302.03917", "tokenized_text": "introduce series diffusion trained generate high quality 30 second music clips text types diffusion generator generates intermediate representation conditioned text generates high fidelity audio conditioned intermediate representation possibly text trained utilized generate high fidelity music explore options intermediate representation spectrogram audio lower fidelity find generated audio able faithfully reflect key elements text instruments era goes ground fine grained semantics pretrained large_language large language play key role story generate paired text audio training set extract embeddings text diffusion generated examples"}
{"id": "12c826f4195da172b212a529f8fcf10cc79e35da", "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.", "title": "contextfaithful prompting for large language models", "url": "http://arxiv.org/pdf/2303.11315", "tokenized_text": "large_language large language llms encode parametric knowledge world facts shown remarkable performance knowledge driven nlp_tasks nlp tasks reliance parametric knowledge cause overlook contextual cues leading incorrect predictions context sensitive nlp_tasks nlp tasks e.g. knowledge acquisition tasks paper seek assess enhance llms contextual faithfulness aspects knowledge conflict prediction abstention demonstrate llms faithfulness significantly improved carefully designed strategies particular identify opinion based counterfactual demonstrations effective methods opinion based reframe context statement inquire opinions counterfactual demonstrations use instances containing false facts improve faithfulness knowledge conflict situations technique requires additional training conduct experiments datasets standard nlp_tasks nlp tasks machine reading comprehension relation_extraction relation extraction results_demonstrate results demonstrate significant improvement faithfulness contexts code data released"}
{"id": "17170575aa8b4fa4e3eef5d366ada706a94dd836", "abstract": "This paper highlights the importance of personalization in the current state of natural language understanding and generation and introduces the LaMP benchmark -- a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three classification and four text generation tasks. We also propose a retrieval augmentation approach that retrieves personalized items from user profiles to construct personalized prompts for large language models. Our baseline zero-shot and fine-tuned model results indicate that LMs utilizing profile augmentation outperform their counterparts that do not factor in profile information.", "title": "lamp when large language models meet personalization", "url": "http://arxiv.org/pdf/2304.11406", "tokenized_text": "paper highlights importance personalization current state natural_language natural language understanding generation introduces benchmark novel benchmark training evaluating language_models language producing personalized outputs offers comprehensive evaluation framework diverse language tasks multiple entries user profile consists seven personalized tasks spanning classification text generation tasks propose retrieval augmentation approach retrieves personalized items user profiles construct personalized large_language large language baseline zero shot fine tuned results_indicate results indicate lms utilizing profile augmentation outperform counterparts factor profile information"}
{"id": "19da40fd01c711fb2b3b0b19b3956b86b75f575d", "abstract": "In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on OOD examples. To remedy this overconfidence, we introduce Contrastive Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD examples representative of novel classes, then trains to decrease confidence on them. First, we generate OOD examples by prompting a large language model twice: we prompt it to enumerate relevant novel labels, then generate examples from each novel class matching the task format. Second, we train our classifier with a novel contrastive objective that encourages lower confidence on generated OOD examples than training examples. When trained with CoNAL, classifiers improve in their ability to detect and abstain on OOD examples over prior methods by an average of 2.3% AUAC and 5.5% AUROC across 4 NLP datasets, with no cost to in-distribution accuracy.1", "title": "conal anticipating outliers with large language models", "url": "http://arxiv.org/pdf/2211.15718", "tokenized_text": "task settings text_classification text classification likely encounter examples novel classes predict correctly selective prediction abstain low confidence examples provides possible solution existing overly confident ood examples remedy overconfidence introduce contrastive_novelty-augmented_learning contrastive novelty-augmented learning conal step method generates ood examples representative novel classes trains decrease confidence generate ood examples large_language large language twice enumerate relevant novel labels generate examples novel class matching task format second train classifier novel contrastive objective encourages lower confidence generated ood examples training_examples training examples trained conal classifiers improve ability detect abstain ood examples prior methods average 2.3 auac 5.5 auroc nlp datasets cost distribution"}
{"id": "216555443355ac615598a99d2949711726a1c36f", "abstract": "Objective We sought to develop a weak supervision-based approach to demonstrate feasibility of post-market surveillance of wearable devices that render AF pre-diagnosis. Materials and Methods Two approaches were evaluated to reduce clinical note labeling overhead for creating a training set for a classifier: one using programmatic codes, and the other using prompts to large language models (LLMs). Probabilistically labeled notes were then used to fine-tune a classifier, which identified patients with AF pre-diagnosis mentions in a note. A retrospective cohort study was conducted, where the baseline characteristics and subsequent care patterns of patients identified by the classifier were compared against those who did not receive pre-diagnosis. Results Label model derived from prompt-based labeling heuristics using LLMs (precision = 0.67, recall = 0.83, F1 = 0.74) nearly achieved the performance of code-based heuristics (precision = 0.84, recall = 0.72, F1 = 0.77), while cutting down the cost to create a labeled training set. The classifier learned on the labeled notes accurately identified patients with AF pre-diagnosis (precision = 0.85, recall = 0.81, F1 = 0.83). Those patients who received pre-diagnosis exhibited different demographic and comorbidity characteristics, and were enriched for anticoagulation and eventual diagnosis of AF. At the index diagnosis, existence of pre-diagnosis did not stratify patients on clinical characteristics, but did correlate with anticoagulant prescription. Discussion and Conclusion Our work establishes the feasibility of an EHR-based surveillance system for wearable devices that render AF pre-diagnosis. Further work is necessary to generalize these findings for patient populations at other sites.", "title": "scalable approach to medical wearable postmarket surveillance", "url": "https://www.medrxiv.org/content/medrxiv/early/2023/11/15/2023.11.14.23298488.full.pdf", "tokenized_text": "objective sought develop weak supervision based approach demonstrate feasibility post market wearable devices render pre diagnosis materials methods approaches evaluated reduce clinical note labeling overhead creating training set classifier programmatic codes large_language large language llms labeled notes fine tune classifier identified patients pre diagnosis mentions note cohort study conducted baseline characteristics subsequent care patterns patients identified classifier compared receive pre diagnosis results label derived based labeling heuristics llms precision 0.67 recall 0.83 f1 nearly achieved performance code based heuristics precision 0.84 recall f1 0.77 cutting cost create labeled training set classifier learned labeled notes accurately identified patients pre diagnosis precision 0.85 recall f1 0.83 patients received pre diagnosis exhibited different demographic characteristics enriched eventual diagnosis index diagnosis existence pre diagnosis patients clinical characteristics correlate discussion conclusion work establishes feasibility based system wearable devices render pre diagnosis work necessary generalize findings patient populations sites"}
{"id": "22b39e38e2fd52591ca23904b474eb19dc17b610", "abstract": "Policy advising in government centers on the analysis of public problems and the developing of recommendations for dealing with them. In carrying out this work, policy analysts consult a variety of sources and work to synthesize that body of evidence into useful decision support documents commonly called briefing notes. Advances in natural language processing (NLP) have led to the continuing development of tools that can undertake a similar task. Given a brief prompt, a large language model (LLM) can synthesize information in content databases. This article documents the findings from an experiment that tested whether contemporary NLP technology is capable of producing public policy relevant briefing notes that expert evaluators judge to be useful. The research involved two stages. First, briefing notes were created using three models: NLP generated; human generated; and NLP generated / human edited. Next, two panels of retired senior public servants (with only one panel informed of the use of NLP in the experiment) were asked to judge the briefing notes using a heuristic evaluation rubric. The findings indicate that contemporary NLP tools were not able to, on their own, generate useful policy briefings. However, the feedback from the expert evaluators indicates that automatically-generated briefing notes might serve as a useful supplement to the work of human policy analysts. And the speed with which the capabilities of NLP tools are developing, supplemented with access to a larger corpus of previously prepared policy briefings and other policy-relevant material, suggests that the quality of automatically-generated briefings may improve significantly in the coming years. The article concludes with reflections on what such improvements might mean for the future practice of policy analysis.", "title": "the end of the policy analyst testing the capability of artificial intelligence to generate plausible, persuasive, and useful policy analysis", "url": "https://dl.acm.org/doi/pdf/10.1145/3604570", "tokenized_text": "policy government centers analysis public problems developing recommendations dealing carrying work policy analysts consult variety sources work synthesize body evidence useful decision support documents commonly called notes advances natural_language natural language processing nlp led development tools undertake similar task given brief large_language large language llm synthesize information content databases article documents findings experiment tested contemporary nlp technology capable producing public policy relevant notes expert evaluators judge useful research involved stages notes created nlp generated human generated nlp generated human edited public panel informed use nlp experiment asked judge notes heuristic evaluation rubric findings indicate contemporary nlp tools able generate useful policy feedback expert evaluators indicates automatically generated notes serve useful supplement work human policy analysts speed capabilities nlp tools developing supplemented access larger corpus previously prepared policy policy relevant material suggests quality automatically generated improve significantly years article concludes reflections improvements mean future practice policy analysis"}
{"id": "300b01dc726fe8acbededd805501811d427920bd", "abstract": "Understanding when two pieces of text convey the same information is a goal touching many subproblems in NLP, including textual entailment and fact-checking. This problem becomes more complex when those two pieces of text are in different languages. Here, we introduce X-PARADE (Cross-lingual Paragraph-level Analysis of Divergences and Entailments), the first cross-lingual dataset of paragraph-level information divergences. Annotators label a paragraph in a target language at the span level and evaluate it with respect to a corresponding paragraph in a source language, indicating whether a given piece of information is the same, new, or new but can be inferred. This last notion establishes a link with cross-language NLI. Aligned paragraphs are sourced from Wikipedia pages in different languages, reflecting real information divergences observed in the wild. Armed with our dataset, we investigate a diverse set of approaches for this problem, including classic token alignment from machine translation, textual entailment methods that localize their decisions, and prompting of large language models. Our results show that these methods vary in their capability to handle inferable information, but they all fall short of human performance.", "title": "xparade crosslingual textual entailment and information divergence across paragraphs", "url": "https://arxiv.org/pdf/2309.08873", "tokenized_text": "understanding pieces text convey information goal subproblems nlp including textual entailment fact checking problem complex pieces text different languages introduce cross lingual paragraph level analysis cross lingual dataset paragraph level information annotators label paragraph target language span level evaluate respect corresponding paragraph source language indicating given piece information new new notion establishes link cross language nli aligned sourced wikipedia pages different languages reflecting real information observed wild armed dataset investigate diverse set approaches problem including classic token alignment machine_translation machine translation textual entailment methods decisions large_language large language results methods vary capability handle information fall short human performance"}
{"id": "31ae42394959fb1a336886379a5527bec5c9c9c4", "abstract": "This report examines the effectiveness of Chain-of-Thought (CoT) prompting in improving the multi-step reasoning abilities of large language models (LLMs). Inspired by previous studies \\cite{Min2022RethinkingWork}, we analyze the impact of three types of CoT prompt perturbations, namely CoT order, CoT values, and CoT operators on the performance of GPT-3 on various tasks. Our findings show that incorrect CoT prompting leads to poor performance on accuracy metrics. Correct values in the CoT is crucial for predicting correct answers. Moreover, incorrect demonstrations, where the CoT operators or the CoT order are wrong, do not affect the performance as drastically when compared to the value based perturbations. This research deepens our understanding of CoT prompting and opens some new questions regarding the capability of LLMs to learn reasoning in context.", "title": "stress testing chainofthought prompting for large language models", "url": "https://arxiv.org/pdf/2309.16621", "tokenized_text": "report examines effectiveness chain thought cot improving multi step reasoning abilities large_language large language llms inspired previous studies analyze impact types cot perturbations cot order cot values cot operators performance gpt-3 tasks findings incorrect cot_prompting cot leads poor performance accuracy metrics correct values cot crucial predicting correct answers incorrect demonstrations cot operators cot order wrong affect performance drastically compared value based perturbations research understanding cot_prompting cot opens new questions capability llms learn reasoning context"}
{"id": "3d8e6358968c8bd5e97f21fead73bf4ba0c2a8d7", "abstract": "Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks. To alleviate this issue, we propose a simple hierarchical prompting approach. Diverging from previous prompting approaches that always put the full observation (e.g. a web page) to the prompt, we propose to first construct an action-aware observation which is more condensed and relevant with a dedicated SUMMARIZER prompt. The ACTOR prompt then predicts the next action based on the summarized observation. While our method has broad applicability, we particularly demonstrate its efficacy in the complex domain of web navigation where a full observation often contains redundant and irrelevant information. Our approach outperforms the previous state-of-the-art prompting mechanics by 6.2% on task success rate, demonstrating its potential on interactive decision making tasks with long observation traces.", "title": "hierarchical prompting assists large language model on web navigation", "url": "http://arxiv.org/pdf/2305.14257", "tokenized_text": "large_language large language llms struggle processing complicated observations interactive decision_making decision making tasks alleviate issue propose simple hierarchical approach previous approaches observation e.g. web page propose construct action aware observation condensed relevant dedicated actor predicts action based summarized observation method broad applicability particularly demonstrate efficacy complex domain web navigation observation contains redundant irrelevant information approach outperforms previous state art mechanics 6.2 task success_rate success rate demonstrating potential interactive decision_making decision making tasks long observation traces"}
{"id": "437cfee2a7f7beadf09ad712f71b3265740e44a0", "abstract": "Large-scale pre-trained Vision Language Models (VLMs) have proven effective for zero-shot classification. Despite the success, most traditional VLMs-based methods are restricted by the assumption of partial source supervision or ideal vocabularies, which rarely satisfy the open-world scenario. In this paper, we aim at a more challenging setting, Realistic Zero-Shot Classification, which assumes no annotation but instead a broad vocabulary. To address this challenge, we propose the Self Structural Semantic Alignment (S^3A) framework, which extracts the structural semantic information from unlabeled data while simultaneously self-learning. Our S^3A framework adopts a unique Cluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groups unlabeled data to derive structural semantics for pseudo-supervision. Our CVPR process includes iterative clustering on images, voting within each cluster to identify initial class candidates from the vocabulary, generating discriminative prompts with large language models to discern confusing candidates, and realigning images and the vocabulary as structural semantic alignment. Finally, we propose to self-learn the CLIP image encoder with both individual and structural semantic alignment through a teacher-student learning strategy. Our comprehensive experiments across various generic and fine-grained benchmarks demonstrate that the S^3A method offers substantial improvements over existing VLMs-based approaches, achieving a more than 15% accuracy improvement over CLIP on average. Our codes, models, and prompts are publicly released at https://github.com/sheng-eatamath/S3A.", "title": "towards realistic zeroshot classification via self structural semantic alignment", "url": "https://arxiv.org/pdf/2308.12960", "tokenized_text": "large scale pre trained vision language vlms proven effective zero shot classification despite success traditional vlms based methods restricted assumption partial source supervision ideal rarely satisfy open world scenario paper aim challenging setting realistic zero-shot classification assumes annotation instead broad vocabulary address challenge propose self structural semantic alignment framework extracts structural semantic information unlabeled data simultaneously self learning framework adopts unique cluster vote algorithm iteratively groups unlabeled data derive structural semantics pseudo supervision process includes iterative clustering images voting cluster identify initial class candidates vocabulary generating discriminative large_language large language discern candidates images vocabulary structural semantic alignment finally propose self learn clip image encoder individual structural semantic alignment teacher student learning strategy comprehensive experiments generic fine grained benchmarks demonstrate method offers substantial improvements existing vlms based approaches achieving 15 accuracy improvement clip average codes publicly released"}
{"id": "4f9e7eb2f009e30f15eca18f4e540915b637b603", "abstract": ". Designing cooperative AI-systems that do not automate tasks but rather aid human cognition is challenging and requires human-centered design approaches. Here, we introduce AI-aided brainstorming for solving guesstimation problems, i.e. estimating quantities from incomplete information, as a testbed for human-AI interaction with large language models (LLMs). In a think-aloud study, we found that humans decompose guesstimation questions into sub-questions and often replace them with semantically related ones. If they fail to brainstorm related questions, they often get stuck and do not \ufb01nd a solution. Therefore, to support this brainstorming process, we prompted a large language model (GPT-3) with successful replacements from our think-aloud data. In follow-up studies, we tested whether the availability of this tool improves participants\u2019 answers. While the tool successfully produced human-like suggestions, participants were reluctant to use it. From our \ufb01ndings, we conclude that for human-AI interaction with LLMs to be successful AI-systems must complement rather than mimic a user\u2019s associations.", "title": "interacting with large language models a case study on aiaided brainstorming for guesstimation problems", "url": "https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA230081", "tokenized_text": "designing cooperative ai systems automate tasks aid human cognition challenging requires human centered design approaches introduce ai aided brainstorming solving problems i.e. estimating quantities incomplete information testbed human ai interaction large_language large language llms think aloud study found humans decompose questions sub questions replace semantically related ones fail related questions \ufb01nd solution support brainstorming process prompted large_language large language gpt-3 successful replacements think aloud data follow studies tested availability tool improves participants answers tool successfully produced human like suggestions participants use \ufb01ndings conclude human ai interaction llms successful ai systems complement mimic user associations"}
{"id": "5ece96203cd1dc9ff3f99867faa451939d86d545", "abstract": "Automatically generating scripts (i.e. sequences of key steps described in text) from video demonstrations and reasoning about the subsequent steps are crucial to the modern AI virtual assistants to guide humans to complete everyday tasks, especially unfamiliar ones. However, current methods for generative script learning rely heavily on well-structured preceding steps described in text and/or images or are limited to a certain domain, resulting in a disparity with real-world user scenarios. To address these limitations, we present a new benchmark challenge -- MultiScript, with two new tasks on task-oriented multimodal script learning: (1) multimodal script generation, and (2) subsequent step prediction. For both tasks, the input consists of a target task name and a video illustrating what has been done to complete the target task, and the expected output is (1) a sequence of structured step descriptions in text based on the demonstration video, and (2) a single text description for the subsequent step, respectively. Built from WikiHow, MultiScript covers multimodal scripts in videos and text descriptions for over 6,655 human everyday tasks across 19 diverse domains. To establish baseline performance on MultiScript, we propose two knowledge-guided multimodal generative frameworks that incorporate the task-related knowledge prompted from large language models such as Vicuna. Experimental results show that our proposed approaches significantly improve over the competitive baselines.", "title": "multiscript multimodal script learning for supporting open domain everyday tasks", "url": "https://arxiv.org/pdf/2310.04965", "tokenized_text": "automatically generating scripts i.e. sequences key steps described text video demonstrations reasoning subsequent steps crucial modern ai virtual assistants guide humans complete everyday tasks especially unfamiliar ones current methods generative script learning rely heavily structured steps described text and/or images limited certain domain resulting disparity real world user scenarios address limitations present new benchmark challenge new tasks task oriented multimodal script learning multimodal script generation subsequent step prediction tasks input consists target task video illustrating complete target task expected output sequence structured step descriptions text based demonstration video single text description subsequent step respectively built covers multimodal scripts videos text descriptions human everyday tasks 19 diverse domains establish baseline performance propose knowledge guided multimodal generative frameworks incorporate task related knowledge prompted large_language large language vicuna experimental_results experimental results proposed approaches significantly improve competitive baselines"}
{"id": "6384921f1bd1059c6b4c37ac3c4e4f19e45d40c1", "abstract": "Systematic reviews (SRs) are a critical component of evidence-based medicine, but the process of screening titles and abstracts is time-consuming. This study aimed to develop and externally validate a method using large language models to classify abstracts for diagnostic test accuracy (DTA) systematic reviews, thereby reducing the human workload. We used a previously collected dataset for developing DTA abstract classifiers and applied prompt engineering. We developed an optimized meta-prompt for Generative Pre-trained Transformer (GPT)-3.5-turbo and GPT-4 to classify abstracts. In the external validation dataset 1, the prompt with GPT-3.5 turbo showed a sensitivity of 0.988, and a specificity of 0.298. GPT-4 showed a sensitivity of 0.982, and a specificity of 0.677. In the external validation dataset 2, GPT-3.5 turbo showed a sensitivity of 0.919, and a specificity of 0.434. GPT-4 showed a sensitivity of 0.806, and a specificity of 0.740. If we included eligible studies from among the references of the identified studies, GPT-3.5 turbo had no critical misses, while GPT-4 had some misses. Our study indicates that GPT-3.5 turbo can be effectively used to classify abstracts for DTA systematic reviews. Further studies using other dataset are warranted to confirm our results. Additionally, we encourage the use of our framework and publicly available dataset for further exploration of more effective classifiers using other LLMs and prompts (https://github.com/youkiti/ARE/).", "title": "development of metaprompts for large language models to screen titles and abstracts for diagnostic test accuracy reviews", "url": "https://www.medrxiv.org/content/medrxiv/early/2023/10/31/2023.10.31.23297818.full.pdf", "tokenized_text": "systematic reviews srs critical component evidence based medicine process screening titles abstracts time consuming study aimed develop externally validate method large_language large language classify abstracts diagnostic test accuracy systematic reviews reducing human workload previously collected dataset developing abstract classifiers applied prompt_engineering engineering developed optimized meta generative_pre generative pre trained transformer turbo gpt-4 classify abstracts external validation dataset gpt-3.5 turbo showed sensitivity specificity gpt-4 showed sensitivity specificity 0.677 external validation dataset gpt-3.5 turbo showed sensitivity specificity gpt-4 showed sensitivity specificity included studies references identified studies gpt-3.5 turbo critical gpt-4 study indicates gpt-3.5 turbo effectively classify abstracts systematic reviews studies dataset confirm results additionally encourage use framework publicly_available publicly available dataset exploration effective classifiers llms"}
{"id": "803a3dd98d72a9fe730f082f3364f9b1f9a0029a", "abstract": "This paper delves into an advanced implementation of Chain-of-Thought-Prompting in Large Language Models, focusing on the use of tools (or\"plug-ins\") within the explicit reasoning paths generated by this prompting method. We find that tool-enabled conversational agents often become sidetracked, as additional context from tools like search engines or calculators diverts from original user intents. To address this, we explore a concept wherein the user becomes the tool, providing necessary details and refining their requests. Through Conversation Analysis, we characterize this interaction as insert-expansion - an intermediary conversation designed to facilitate the preferred response. We explore possibilities arising from this 'user-as-a-tool' approach in two empirical studies using direct comparison, and find benefits in the recommendation domain.", "title": "insertexpansions for toolenabled conversational agents", "url": "https://arxiv.org/pdf/2307.01644", "tokenized_text": "paper delves advanced implementation chain thought large_language large language focusing use tools ins explicit reasoning paths generated method find tool enabled conversational agents additional context tools like search engines original user intents address explore concept user tool providing necessary details refining requests conversation analysis characterize interaction insert expansion intermediary conversation designed facilitate preferred response explore possibilities arising user tool approach empirical studies direct comparison find benefits recommendation domain"}
{"id": "8a4320fd903677a3ea2bf606a6537b59885b1108", "abstract": "Existing aspect extraction methods mostly rely on explicit or ground truth aspect information, or using data mining or machine learning approaches to extract aspects from implicit user feedback such as user reviews. It however remains under-explored how the extracted aspects can help generate more meaningful recommendations to the users. Meanwhile, existing research on aspect-based recommendations often relies on separate aspect extraction models or assumes the aspects are given, without accounting for the fact the optimal set of aspects could be dependent on the recommendation task at hand. In this work, we propose to combine aspect extraction together with aspect-based recommendations in an end-to-end manner, achieving the two goals together in a single framework. For the aspect extraction component, we leverage the recent advances in large language models and design a new prompt learning mechanism to generate aspects for the end recommendation task. For the aspect-based recommendation component, the extracted aspects are concatenated with the usual user and item features used by the recommendation model. The recommendation task mediates the learning of the user embeddings and item embeddings, which are used as soft prompts to generate aspects. Therefore, the extracted aspects are personalized and contextualized by the recommendation task. We showcase the effectiveness of our proposed method through extensive experiments on three industrial datasets, where our proposed framework significantly outperforms state-of-the-art baselines in both the personalized aspect extraction and aspect-based recommendation tasks. In particular, we demonstrate that it is necessary and beneficial to combine the learning of aspect extraction and aspect-based recommendation together. We also conduct extensive ablation studies to understand the contribution of each design component in our framework.", "title": "prompt tuning large language models on personalized aspect extraction for recommendations", "url": "http://arxiv.org/pdf/2306.01475", "tokenized_text": "existing aspect extraction methods rely explicit ground_truth ground truth aspect information data mining machine_learning machine learning approaches extract aspects implicit user feedback user reviews remains explored extracted aspects help generate meaningful recommendations users existing research aspect based recommendations relies separate aspect extraction assumes aspects given accounting fact optimal set aspects dependent recommendation task hand work propose combine aspect extraction aspect based recommendations end end manner achieving goals single framework aspect extraction component leverage recent_advances recent advances large_language large language design new learning mechanism generate aspects end recommendation task aspect based recommendation component extracted aspects concatenated user item features recommendation recommendation task mediates learning user embeddings item embeddings soft generate aspects extracted aspects personalized contextualized recommendation task showcase effectiveness proposed_method proposed method extensive_experiments extensive experiments industrial datasets proposed framework significantly_outperforms significantly outperforms state art baselines personalized aspect extraction aspect based recommendation tasks particular demonstrate necessary beneficial combine learning aspect extraction aspect based recommendation conduct_extensive conduct extensive ablation studies understand contribution design component framework"}
{"id": "90350aa626bed47b02d0c162462e5b0ca82be6b2", "abstract": "Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like\"Let's think step by step\"to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the\"Let's think step by step\"prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot", "title": "automatic chain of thought prompting in large language models", "url": "http://arxiv.org/pdf/2210.03493", "tokenized_text": "large_language large language llms perform complex_reasoning complex reasoning generating intermediate reasoning_steps reasoning steps providing steps demonstrations called chain thought cot cot_prompting cot major paradigms leverages simple think step facilitate step step thinking answering question uses manual demonstrations composed question reasoning chain leads answer superior_performance superior performance second paradigm hinges hand crafting task specific demonstrations manual efforts leveraging llms think step generate reasoning chains demonstrations i.e. let think step_by_step step step generated chains come mistakes mitigate effect mistakes find diversity matters automatically constructing demonstrations propose automatic cot_prompting cot method auto cot. samples questions diversity generates reasoning chains construct demonstrations public benchmark reasoning tasks gpt-3 auto cot consistently matches exceeds performance cot paradigm requires manual designs demonstrations code_is_available code available"}
{"id": "91099bbb96133c70db091041900ecff502a5e3a8", "abstract": "This study investigates the application of Large Language Models (LLMs), specifically GPT-4, within Astronomy. We employ in-context prompting, supplying the model with up to 1000 papers from the NASA Astrophysics Data System, to explore the extent to which performance can be improved by immersing the model in domain-specific literature. Our findings point towards a substantial boost in hypothesis generation when using in-context prompting, a benefit that is further accentuated by adversarial prompting. We illustrate how adversarial prompting empowers GPT-4 to extract essential details from a vast knowledge base to produce meaningful hypotheses, signaling an innovative step towards employing LLMs for scientific research in Astronomy.", "title": "harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy", "url": "http://arxiv.org/pdf/2306.11648", "tokenized_text": "study investigates application large_language large language llms specifically gpt-4 employ context 1000 papers data system explore extent performance improved domain specific literature findings point substantial boost hypothesis generation context benefit adversarial illustrate adversarial empowers gpt-4 extract essential details vast knowledge base produce meaningful hypotheses signaling innovative step employing llms scientific research"}
{"id": "9dcee248452d84b6bf26911ba6726ae5ce1a46f3", "abstract": "We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method's ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting.", "title": "tabllm fewshot classification of tabular data with large language models", "url": "http://arxiv.org/pdf/2210.10723", "tokenized_text": "study application large_language large language zero shot shot classification tabular data large_language large language tabular data natural language string short description classification problem shot_setting shot setting fine tune large_language large language labeled examples evaluate methods including templates table text large_language large language despite simplicity find technique outperforms prior deep learning based tabular classification methods benchmark_datasets benchmark datasets cases zero shot classification obtains non trivial performance illustrating method ability exploit prior knowledge encoded large_language large language unlike deep learning methods tabular datasets approach competitive strong traditional baselines like gradient boosted trees especially shot_setting shot setting"}
{"id": "b378e54c88d241aa917131beb65c96be3730f40c", "abstract": "Electronic health records (EHRs) store an extensive array of patient information, encompassing medical histories, diagnoses, treatments, and test outcomes. These records are crucial for enabling healthcare providers to make well-informed decisions regarding patient care. Summarizing clinical notes further assists healthcare professionals in pinpointing potential health risks and making better-informed decisions. This process contributes to reducing errors and enhancing patient outcomes by ensuring providers have access to the most pertinent and current patient data. Recent research has shown that incorporating prompts with large language models (LLMs) substantially boosts the ef\ufb01cacy of summarization tasks. However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings. To tackle this challenge, we introduce a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft prompts to diminish variance while preserving the advantages of prompt-based summarization. Experimental \ufb01ndings on multiple clinical note tasks and LLMs indicate that our method not only bolsters performance but also effectively curbs variance for various LLMs, providing a more uniform and dependable solution for summarizing vital medical information.", "title": "spec a soft promptbased calibration on mitigating performance variability in clinical notes summarization", "url": "https://arxiv.org/pdf/2303.13035", "tokenized_text": "electronic health records store extensive array patient information encompassing medical histories diagnoses treatments test outcomes records crucial enabling healthcare providers informed decisions patient care summarizing clinical notes assists healthcare professionals potential health risks making better informed decisions process contributes reducing errors enhancing patient outcomes ensuring providers access pertinent current patient data recent research shown incorporating large_language large language llms substantially boosts summarization tasks approach leads increased output variance resulting notably outputs share similar meanings tackle challenge introduce agnostic soft calibration pipeline employs soft diminish variance preserving advantages based summarization experimental \ufb01ndings multiple clinical note tasks llms indicate method performance effectively variance llms providing uniform solution summarizing vital medical information"}
{"id": "b5e9406a65de7384af041c357ca5481489345b73", "abstract": "Numerous works are proposed to improve or evaluate the capabilities of Large language models (LLMs) to fulfill user instructions. However, they neglect the possibility that user inputs may inherently contain incorrect information due to users' false beliefs or malicious intents. In this way, blindly adhering to users' false content will cause deception and harm. To address this problem, we propose a challenging benchmark consisting of Inductive Instructions (INDust) to evaluate whether LLMs could resist these instructions. The INDust includes 15K instructions across three categories: Fact-Checking Instructions, Questions based on False Premises, and Creative Instructions based on False Premises. Our experiments on several strong LLMs reveal that current LLMs can be easily deceived by INDust into generating misleading and malicious statements. Hence we employ Self-Critique prompting to encourage LLMs to not only critique themselves like in previous works but also the users, which show remarkable improvement in handling inductive instructions under both zero-shot and few-shot settings.", "title": "selfcritique prompting with large language models for inductive instructions", "url": "http://arxiv.org/pdf/2305.13733", "tokenized_text": "numerous works proposed improve evaluate capabilities large_language large language llms fulfill user instructions neglect possibility user inputs inherently contain incorrect information users false beliefs malicious intents way adhering users false content cause deception harm address problem propose challenging benchmark consisting inductive instructions evaluate llms instructions includes 15 instructions categories instructions questions based false premises creative instructions based false premises experiments strong llms reveal current llms easily generating misleading malicious statements employ self encourage llms like previous works users remarkable improvement handling inductive instructions zero shot shot_settings shot settings"}
{"id": "b9d75f361b5310c6ddcddfe7858bb0416eb78de4", "abstract": "Chain-of-thought (CoT) prompting enables large language models (LLMs) to solve complex reasoning tasks by generating an explanation before the final prediction. Despite it\u2019s promising ability, a critical downside of CoT prompting is that the performance is greatly affected by the factuality of the generated explanation. To improve the correctness of the explanations, fine-tuning language models with explanation data is needed. However, there exists only a few datasets that can be used for such approaches, and no data collection tool for building them. Thus, we introduce CoTEVer, a tool-kit for annotating the factual correctness of generated explanations and collecting revision data of wrong explanations. Furthermore, we suggest several use cases where the data collected with CoTEVer can be utilized for enhancing the faithfulness of explanations. Our toolkit is publicly available at https://github.com/SeungoneKim/CoTEVer.", "title": "cotever chain of thought prompting annotation toolkit for explanation verification", "url": "http://arxiv.org/pdf/2303.03628", "tokenized_text": "chain thought cot enables large_language large language llms solve complex_reasoning complex reasoning tasks generating explanation final prediction despite promising ability critical cot_prompting cot performance greatly affected factuality generated explanation improve correctness explanations fine tuning language_models language explanation data needed exists datasets approaches data collection tool building introduce tool annotating factual correctness generated explanations collecting revision data wrong explanations furthermore suggest use cases data collected utilized enhancing faithfulness explanations toolkit publicly_available publicly available"}
{"id": "c4561fd08636b5f5f6b9f3f6d89f3cee39e678b0", "abstract": "Can we design artificial intelligence (AI) systems that rank our social media feeds to consider democratic values such as mitigating partisan animosity as part of their objective functions? We introduce a method for translating established, vetted social scientific constructs into AI objective functions, which we term societal objective functions, and demonstrate the method with application to the political science construct of anti-democratic attitudes. Traditionally, we have lacked observable outcomes to use to train such models, however, the social sciences have developed survey instruments and qualitative codebooks for these constructs, and their precision facilitates translation into detailed prompts for large language models. We apply this method to create a democratic attitude model that estimates the extent to which a social media post promotes anti-democratic attitudes, and test this democratic attitude model across three studies. In Study 1, we first test the attitudinal and behavioral effectiveness of the intervention among US partisans (N=1,380) by manually annotating (alpha=.895) social media posts with anti-democratic attitude scores and testing several feed ranking conditions based on these scores. Removal (d=.20) and downranking feeds (d=.25) reduced participants' partisan animosity without compromising their experience and engagement. In Study 2, we scale up the manual labels by creating the democratic attitude model, finding strong agreement with manual labels (rho=.75). Finally, in Study 3, we replicate Study 1 using the democratic attitude model instead of manual labels to test its attitudinal and behavioral impact (N=558), and again find that the feed downranking using the societal objective function reduced partisan animosity (d=.25). This method presents a novel strategy to draw on social science theory and methods to mitigate societal harms in social media AIs.", "title": "embedding democratic values into social media ais via societal objective functions", "url": "https://arxiv.org/pdf/2307.13912", "tokenized_text": "design artificial_intelligence artificial intelligence ai systems rank social_media social media consider values mitigating objective functions introduce method translating established vetted social scientific constructs ai objective functions term societal objective functions demonstrate method application political science construct anti attitudes traditionally outcomes use train social sciences developed survey instruments qualitative constructs precision facilitates translation detailed large_language large language apply method create attitude estimates extent social_media social media post promotes anti attitudes test attitude studies study test behavioral effectiveness intervention manually annotating social_media social media posts anti attitude scores testing feed ranking conditions based scores removal reduced participants compromising experience engagement study scale manual labels creating attitude finding strong agreement manual labels finally study replicate study attitude instead manual labels test behavioral impact find feed societal objective function reduced method presents novel strategy draw social science theory methods mitigate societal harms social_media social media"}
{"id": "ca60126b2b534a3f1cd8007ba84fdbd163968770", "abstract": "With their remarkably improved text generation and prompting capabilities, large language models can adapt existing written information into forms that are easier to use and understand. In our work, we focus on recipes as an example of complex, diverse, and widely used instructions. We develop a prompt grounded in the original recipe and ingredients list that breaks recipes down into simpler steps. We apply this prompt to recipes from various world cuisines, and experiment with several large language models (LLMs), finding best results with GPT-3.5. We also contribute an Amazon Mechanical Turk task that is carefully designed to reduce fatigue while collecting human judgment of the quality of recipe revisions. We find that annotators usually prefer the revision over the original, demonstrating a promising application of LLMs in serving as digital sous chefs for recipes and beyond. We release our prompt, code, and MTurk template for public use.", "title": "large language models as sous chefs revising recipes with gpt3", "url": "http://arxiv.org/pdf/2306.13986", "tokenized_text": "remarkably improved text generation capabilities large_language large language adapt existing written information forms easier use understand work focus recipes example complex diverse widely instructions develop grounded original recipe ingredients list breaks recipes simpler steps apply recipes world experiment large_language large language llms finding best results gpt-3.5 contribute amazon task carefully designed reduce collecting human judgment quality recipe revisions find annotators usually prefer revision original demonstrating promising application llms serving digital recipes release code template public use"}
{"id": "d53945d4afb4528590d79e20de52883d29037e86", "abstract": "Logo embedding plays a crucial role in various e-commerce applications by facilitating image retrieval or recognition, such as intellectual property protection and product search. However, current methods treat logo embedding as a purely visual problem, which may limit their performance in real-world scenarios. A notable issue is that the textual knowledge embedded in logo images has not been adequately explored. Therefore, we propose a novel approach that leverages textual knowledge as an auxiliary to improve the robustness of logo embedding. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding and could become valuable visual assistants in understanding logo images. Inspired by this observation, our proposed method, FashionLOGO, aims to utilize MLLMs to enhance fashion logo embedding. We explore how MLLMs can improve logo embedding by prompting them to generate explicit textual knowledge through three types of prompts, including image OCR, brief captions, and detailed descriptions prompts, in a zero-shot setting. We adopt a cross-attention transformer to enable image embedding queries to learn supplementary knowledge from textual embeddings automatically. To reduce computational costs, we only use the image embedding model in the inference stage, similar to traditional inference pipelines. Our extensive experiments on three real-world datasets demonstrate that FashionLOGO learns generalized and robust logo embeddings, achieving state-of-the-art performance in all benchmark datasets. Furthermore, we conduct comprehensive ablation studies to demonstrate the performance improvements resulting from the introduction of MLLMs.", "title": "fashionlogo prompting multimodal large language models for fashion logo embeddings", "url": "https://arxiv.org/pdf/2308.09012", "tokenized_text": "embedding plays crucial role commerce applications facilitating image retrieval recognition intellectual property protection product search current methods treat embedding purely visual problem limit performance real world_scenarios world scenarios notable issue textual knowledge embedded images adequately explored propose_a_novel propose novel approach leverages textual knowledge auxiliary improve robustness embedding emerging multimodal large_language large language mllms demonstrated_remarkable demonstrated remarkable capabilities visual textual understanding valuable visual assistants understanding images inspired observation proposed_method proposed method aims utilize mllms enhance fashion embedding explore mllms improve embedding generate explicit textual knowledge types including image ocr brief captions detailed descriptions zero shot_setting shot setting adopt cross attention transformer enable image embedding queries learn supplementary knowledge textual embeddings automatically reduce computational costs use image embedding inference stage similar traditional inference pipelines extensive_experiments extensive experiments real world datasets demonstrate learns generalized robust embeddings achieving state art performance benchmark_datasets benchmark datasets furthermore conduct comprehensive ablation studies demonstrate performance improvements resulting introduction mllms"}
{"id": "da5fcb26c830663b79c9aa1c550ae62e7725fcad", "abstract": "With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses. One way to reduce the risk of LLMs generating undesired discourses is to alter the training of the LLM. This can be very restrictive due to demanding computation requirements. Other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse. Here, we center detoxification on the probability that the finished discourse is ultimately considered toxic. That is, at each point, we advise against token selections proportional to how likely a finished text from this point will be toxic. To this end, we formally extend the dead-end theory from the recent reinforcement learning (RL) literature to also cover uncertain outcomes. Our approach, called rectification, utilizes a separate but significantly smaller model for detoxification, which can be applied to diverse LLMs as long as they share the same vocabulary. Importantly, our method does not require access to the internal representations of the LLM, but only the token probability distribution at each decoding step. This is crucial as many LLMs today are hosted in servers and only accessible through APIs. When applied to various LLMs, including GPT-3, our approach significantly improves the generated discourse compared to the base LLMs and other techniques in terms of both the overall language and detoxification performance.", "title": "systematic rectification of language models via deadend analysis", "url": "http://arxiv.org/pdf/2302.14003", "tokenized_text": "adversarial normal existing large_language large language llm pushed generate toxic way reduce risk llms generating undesired alter training llm demanding computation requirements methods rely rule based based token elimination limited future tokens overall meaning complete discourse center probability discourse ultimately considered toxic point advise token selections likely text point toxic end formally extend end theory recent reinforcement_learning reinforcement learning rl literature cover uncertain outcomes approach called rectification utilizes separate significantly smaller applied diverse llms long share vocabulary importantly method require access internal representations llm token probability distribution decoding step crucial llms today hosted servers accessible apis applied llms including gpt-3 approach significantly improves generated discourse compared base llms techniques terms overall language performance"}
{"id": "e1decb86f2a6aba8682d2fc4e427424b0b49e0d0", "abstract": "Advanced text-to-image models such as DALL-E 2 and Midjourney possess the capacity to generate highly realistic images, raising significant concerns regarding the potential proliferation of unsafe content. This includes adult, violent, or deceptive imagery of political figures. Despite claims of rigorous safety mechanisms implemented in these models to restrict the generation of not-safe-for-work (NSFW) content, we successfully devise and exhibit the first prompt attacks on Midjourney, resulting in the production of abundant photorealistic NSFW images. We reveal the fundamental principles of such prompt attacks and suggest strategically substituting high-risk sections within a suspect prompt to evade closed-source safety measures. Our novel framework, SurrogatePrompt, systematically generates attack prompts, utilizing large language models, image-to-text, and image-to-image modules to automate attack prompt creation at scale. Evaluation results disclose an 88% success rate in bypassing Midjourney's proprietary safety filter with our attack prompts, leading to the generation of counterfeit images depicting political figures in violent scenarios. Both subjective and objective assessments validate that the images generated from our attack prompts present considerable safety hazards.", "title": "surrogateprompt bypassing the safety filter of texttoimage models via substitution", "url": "https://arxiv.org/pdf/2309.14122", "tokenized_text": "advanced text image dall midjourney possess capacity generate highly realistic images raising significant concerns potential proliferation unsafe content includes deceptive political figures despite claims rigorous safety mechanisms implemented restrict generation safe work content successfully devise exhibit attacks midjourney resulting production abundant images reveal fundamental principles attacks suggest strategically substituting high risk sections evade closed source safety measures novel framework systematically generates attack utilizing large_language large language image text image image modules automate attack creation scale evaluation results disclose 88 success_rate success rate bypassing midjourney proprietary safety filter attack leading generation images political figures scenarios subjective objective assessments validate images generated attack present considerable safety"}
{"id": "e4c466cf3df4887e0121561be90e0bac78d3e1cb", "abstract": "Information retrieval involves selecting artifacts from a corpus that are most relevant to a given search query. The flavor of retrieval typically used in classical applications can be termed as homogeneous and relaxed, where queries and corpus elements are both natural language (NL) utterances (homogeneous) and the goal is to pick most relevant elements from the corpus in the Top-K, where K is large, such as 10, 25, 50 or even 100 (relaxed). Recently, retrieval is being used extensively in preparing prompts for large language models (LLMs) to enable LLMs to perform targeted tasks. These new applications of retrieval are often heterogeneous and strict -- the queries and the corpus contain different kinds of entities, such as NL and code, and there is a need for improving retrieval at Top-K for small values of K, such as K=1 or 3 or 5. Current dense retrieval techniques based on pretrained embeddings provide a general-purpose and powerful approach for retrieval, but they are oblivious to task-specific notions of similarity of heterogeneous artifacts. We introduce Adapted Dense Retrieval, a mechanism to transform embeddings to enable improved task-specific, heterogeneous and strict retrieval. Adapted Dense Retrieval works by learning a low-rank residual adaptation of the pretrained black-box embedding. We empirically validate our approach by showing improvements over the state-of-the-art general-purpose embeddings-based baseline.", "title": "augmented embeddings for custom retrievals", "url": "https://arxiv.org/pdf/2310.05380", "tokenized_text": "information retrieval involves selecting artifacts corpus relevant given search query retrieval typically classical applications termed queries corpus elements natural_language natural language nl utterances goal pick relevant elements corpus large 10 25 50 100 recently retrieval extensively preparing large_language large language llms enable llms perform targeted tasks new applications retrieval heterogeneous strict queries corpus contain different kinds entities nl code need improving retrieval small values k=1 current dense retrieval techniques based pretrained embeddings provide general purpose powerful approach retrieval task specific notions similarity heterogeneous artifacts introduce adapted dense retrieval mechanism transform embeddings enable improved task specific heterogeneous strict retrieval adapted dense retrieval works learning low rank residual adaptation pretrained black box embedding empirically validate approach showing improvements state art general purpose embeddings based baseline"}
{"id": "ee025d7030d4767062af2bcd32a4d586737d30bf", "abstract": "The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.", "title": "tryage realtime, intelligent routing of user prompts to large language models", "url": "https://arxiv.org/pdf/2308.11601", "tokenized_text": "introduction transformer architecture self attention mechanism led explosive production language_models language trained specific downstream_tasks downstream tasks data domains 200 000 face ecosystem users grapple selecting optimizing suit multifaceted workflows data domains addressing computational security recency concerns urgent need machine_learning machine learning frameworks eliminate burden selection customization unleash incredible power vast emerging library end users propose context aware system leverages language_model language optimal selection expert library based analysis individual input inspired brain employs predict stream performance makes decision objective function integrates performance predictions user goals constraints incorporated flags e.g. model_size size recency allows users explore automatically trade task accuracy secondary goals including minimization model_size size recency security verbosity readability heterogeneous data sets include code text clinical data framework surpasses gpt3.5 turbo dynamic selection identifying optimal accuracy compared gpt 3.5 turbo conceptually demonstrates applied program control behavior multi llm systems maximize efficient use expanding evolving language_model language ecosystem"}
{"id": "f1bb5051965a3a4c9288f0123dd03c26a08e1378", "abstract": "Large Language Models (LLMs) such as ChatGPT have demonstrated remarkable performance across various tasks and have garnered significant attention from both researchers and practitioners. However, in an educational context, we still observe a performance gap in generating distractors -- i.e., plausible yet incorrect answers -- with LLMs for multiple-choice questions (MCQs). In this study, we propose a strategy for guiding LLMs such as ChatGPT, in generating relevant distractors by prompting them with question items automatically retrieved from a question bank as well-chosen in-context examples. We evaluate our LLM-based solutions using a quantitative assessment on an existing test set, as well as through quality annotations by human experts, i.e., teachers. We found that on average 53% of the generated distractors presented to the teachers were rated as high-quality, i.e., suitable for immediate use as is, outperforming the state-of-the-art model. We also show the gains of our approach 1 in generating high-quality distractors by comparing it with a zero-shot ChatGPT and a few-shot ChatGPT prompted with static examples.", "title": "distractor generation for multiplechoice questions with predictive prompting and large language models", "url": "https://arxiv.org/pdf/2307.16338", "tokenized_text": "large_language large language llms chatgpt demonstrated_remarkable demonstrated remarkable performance tasks garnered significant attention researchers practitioners educational context observe performance gap generating distractors i.e. plausible incorrect answers llms multiple choice questions mcqs study propose strategy guiding llms chatgpt generating relevant distractors question items automatically retrieved question chosen context_examples context examples evaluate llm based solutions quantitative assessment existing test set quality annotations human experts i.e. teachers found average 53 generated distractors presented teachers rated high quality i.e. suitable immediate use outperforming state art gains approach generating high quality distractors comparing zero shot chatgpt shot chatgpt prompted static examples"}
{"id": "f208ea909fa7f54fea82def9a92fd81dfc758c39", "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.", "title": "interleaving retrieval with chainofthought reasoning for knowledgeintensive multistep questions", "url": "http://arxiv.org/pdf/2212.10509", "tokenized_text": "based large_language large language llms surprisingly powerful generating natural_language natural language reasoning_steps reasoning steps chains thoughts cot multi step question_answering question answering qa struggle necessary knowledge unavailable llm date parameters question retrieve relevant text external_knowledge external knowledge source helps llms observe step retrieve read approach insufficient multi step qa retrieve depends derived turn depend previously retrieved address propose new approach multi step qa retrieval steps sentences cot guiding retrieval cot turn retrieved results improve cot. gpt3 substantially improves retrieval 21 points downstream qa 15 points datasets hotpotqa musique observe similar substantial gains distribution ood settings smaller flan t5 large additional training reduces hallucination resulting factually accurate cot reasoning"}
{"id": "f27f6d1d521d189e78f5623098ced0deea613d33", "abstract": "Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving problems that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling (SatLM) approach for improving the reasoning capabilities of LLMs. We use an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the LLM can parse it out of the description more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness of the answer with respect to the parsed specification and avoid planning errors in the solving process. We evaluate SATLM on 8 different datasets and show that it consistently outperforms program-aided LMs in the imperative paradigm. In particular, SATLM outperforms program-aided LMs by 23% on a challenging subset of the GSM arithmetic reasoning dataset; SATLM also achieves a new SoTA on LSAT and BoardgameQA, surpassing previous models that are trained on the respective training sets.", "title": "satisfiabilityaided language models using declarative prompting", "url": "https://arxiv.org/pdf/2305.09656", "tokenized_text": "prior_work prior work combined chain thought_prompting thought large_language large language llms programmatic representations perform effective transparent reasoning approach works tasks require forward reasoning e.g. straightforward arithmetic effective constraint solving problems require sophisticated planning search paper propose_a_new propose new satisfiability aided language modeling approach improving reasoning capabilities llms use llm generate declarative task specification imperative program leverage shelf automated theorem prover derive final answer approach key advantages declarative specification closer problem description reasoning_steps reasoning steps llm parse description accurately furthermore offloading actual reasoning task automated theorem prover approach guarantee correctness answer respect parsed specification avoid planning errors solving process evaluate different datasets consistently_outperforms consistently outperforms program aided lms imperative paradigm particular outperforms program aided lms 23 challenging subset gsm arithmetic reasoning dataset achieves new sota surpassing previous trained respective training sets"}
{"id": "fccf8776d7525627c518a56a1f4db367a4d7120b", "abstract": "We propose a conceptual perspective on prompts for Large Language Models (LLMs) that distinguishes between (1) diegetic prompts (part of the narrative, e.g. \u201cOnce upon a time, I saw a fox...\u201d), and (2) non-diegetic prompts (external, e.g. \u201cWrite about the adventures of the fox.\u201d). With this lens, we study how 129 crowd workers on Prolific write short texts with different user interfaces (1 vs 3 suggestions, with/out non-diegetic prompts; implemented with GPT-3): When the interface offered multiple suggestions and provided an option for non-diegetic prompting, participants preferred choosing from multiple suggestions over controlling them via non-diegetic prompts. When participants provided non-diegetic prompts it was to ask for inspiration, topics or facts. Single suggestions in particular were guided both with diegetic and non-diegetic information. This work informs human-AI interaction with generative models by revealing that (1) writing non-diegetic prompts requires effort, (2) people combine diegetic and non-diegetic prompting, and (3) they use their draft (i.e. diegetic information) and suggestion timing to strategically guide LLMs.", "title": "choice over control how users write with large language models using diegetic and nondiegetic prompting", "url": "https://arxiv.org/pdf/2303.03199", "tokenized_text": "propose conceptual perspective large_language large language llms narrative e.g. time saw non external e.g. write lens study crowd workers prolific write short texts different user interfaces vs suggestions non implemented gpt-3 interface offered multiple suggestions provided option non participants preferred choosing multiple suggestions controlling non participants provided non ask inspiration topics facts single suggestions particular guided non information work informs human ai interaction generative revealing writing non requires effort people combine non use draft i.e. information suggestion strategically guide llms"}
{"id": "2c2b40b4f1967dc1fb640c7c4bec140110dbf2cf", "abstract": "In the context of plant breeding, bioinformatics can empower genetic and genomic selection to determine the optimal combination of genotypes that will produce a desired phenotype and help expedite the isolation of these new varieties. Bioinformatics is also instrumental in collecting and processing plant phenotypes, which facilitates plant breeding. Robots that use automated and digital technologies to collect and analyze different types of information to monitor the environment in which plants grow, analyze the environmental stresses they face, and promptly optimize suboptimal and adverse growth conditions accordingly, have helped plant research and saved human resources. In this paper, we describe the use of various bioinformatics databases and algorithms and explore their potential applications in plant breeding and for research on plant disease resistance.", "title": "bioinformatics in plant breeding and research on disease resistance", "url": "https://www.mdpi.com/2223-7747/11/22/3118/pdf?version=1668520760", "tokenized_text": "context plant breeding bioinformatics empower genetic genomic selection determine optimal combination genotypes produce desired phenotype help expedite isolation new varieties bioinformatics instrumental collecting processing plant phenotypes facilitates plant breeding robots use automated digital technologies collect analyze different types information monitor environment grow analyze environmental face promptly optimize suboptimal growth conditions accordingly helped plant research saved human resources paper describe use bioinformatics databases algorithms explore potential applications plant breeding research plant disease resistance"}
{"id": "2e536dcd013be93dc1841dd0e7a0a87b2846f341", "abstract": "Objective: Early diagnosis of nosocomial infections in newborns is a great challenge, because in the initial phase of systemic infection, clinical symptoms are often non-specific, and routinely used hematological markers are not sufficiently informative. The aim of this study was to determine the potential of early inflammatory markers to diagnose late-onset neonatal sepsis\u2014procalcitonin (PCT), interleukin 6 (IL-6), interleukin 8 (IL-8) and endocan (ESM-1). Material and methods: A prospective clinical\u2013epidemiological study was conducted in a third-level NICU in Pleven, Bulgaria. Patients with suspected late-onset sepsis and healthy controls were tested. A sandwich ELISA method was used to measure the serum concentrations of biomarkers. Results: Sixty newborns were included, of which 35% symptomatic and infected, 33.3% symptomatic but uninfected and 31.7% asymptomatic controls. The mean values of PCT, IL-6, I/T index and PLT differ significantly in the three groups. For ESM-1, IL-8 and CRP, the difference was statistically insignificant. The best sensitivity (78%) and negative predictive value (84%) was found for IL-6. The combinations of PCT + IL-6 and PCT + IL-6+ I/T+ PLT showed very good diagnostic potential. Conclusion: The introduction into the routine practice of indicators such as PCT and IL-6 may provide an opportunity to promptly optimize the diagnostic and therapeutic approach to LOS.", "title": "early diagnostic markers of lateonset neonatal sepsis", "url": "https://www.mdpi.com/2036-7503/15/3/50/pdf?version=1695182872", "tokenized_text": "objective early diagnosis great challenge initial phase systemic infection clinical symptoms non specific markers sufficiently informative aim study determine potential early markers diagnose late neonatal material methods prospective clinical study conducted level patients late healthy controls tested method measure results included 35 controls mean values index differ significantly groups difference statistically insignificant best sensitivity 78 negative predictive value 84 found combinations showed good diagnostic potential conclusion introduction routine practice provide opportunity promptly optimize diagnostic therapeutic approach"}
{"id": "439c2a5c4883b421ca316617b1306583cc1d706c", "abstract": "The rapid growth of biomedical literature presents a significant challenge for researchers to extract and analyze relevant information efficiently. In this study, we explore the application of GPT, the large language model to automate the extraction and visualization of metabolic networks from a corpus of PubMed abstracts. Our objective is to provide a valuable tool for biomedical researchers to explore and understand the intricate metabolic interactions discussed in scientific literature. We begin by splitting a ton of the tokens within the corpus, as the GPT-3.5-Turbo model has a token limit of 4,000 per analysis. Through iterative prompt optimization, we successfully extract a comprehensive list of metabolites, enzymes, and proteins from the abstracts. To validate the accuracy and completeness of the extracted entities, our biomedical data domain experts compare them with the provided abstracts and ensure a fully matched result. Using the extracted entities, we generate a directed graph that represents the metabolic network including 3 types of metabolic events that consist of metabolic consumption, metabolic reaction, and metabolic production. The graph visualization, achieved through Python and NetworkX, offers a clear representation of metabolic pathways, highlighting the relationships between metabolites, enzymes, and proteins. Our approach integrates language models and network analysis, demonstrating the power of combining automated information extraction with sophisticated visualization techniques. The research contributions are twofold. Firstly, we showcase the ability of GPT-3.5-Turbo to automatically extract metabolic entities, streamlining the process of cataloging important components in metabolic research. Secondly, we present the generation and visualization of a directed graph that provides a comprehensive overview of metabolic interactions. This graph serves as a valuable tool for further analysis, comparison with existing pathways, and updating or refining metabolic networks. Our findings underscore the potential of large language models and network analysis techniques in extracting and visualizing metabolic information from scientific literature. This approach enables researchers to gain insights into complex biological systems, advancing our understanding of metabolic pathways and their components.", "title": "automated extraction and visualization of metabolic networks from biomedical literature using a large language model", "url": "https://www.biorxiv.org/content/biorxiv/early/2023/06/29/2023.06.27.546560.full.pdf", "tokenized_text": "rapid growth biomedical literature presents significant challenge researchers extract analyze relevant information efficiently study explore application gpt large_language large language automate extraction visualization metabolic networks corpus pubmed abstracts objective provide valuable tool biomedical researchers explore understand intricate metabolic interactions discussed scientific literature begin tokens corpus gpt-3.5 turbo token limit analysis iterative prompt_optimization optimization successfully extract comprehensive list abstracts validate accuracy completeness extracted entities biomedical data domain experts compare provided abstracts ensure fully matched result extracted entities generate directed graph represents metabolic network including types metabolic events consist metabolic consumption metabolic reaction metabolic production graph visualization achieved python offers clear representation metabolic pathways highlighting relationships approach integrates language_models language network analysis demonstrating power combining automated information_extraction information extraction sophisticated visualization techniques research contributions twofold firstly showcase ability gpt-3.5 turbo automatically extract metabolic entities process important components metabolic research secondly present generation visualization directed graph provides comprehensive overview metabolic interactions graph serves valuable tool analysis comparison existing pathways updating refining metabolic networks findings underscore potential large_language large language network analysis techniques extracting metabolic information scientific literature approach enables researchers gain insights complex biological systems advancing understanding metabolic pathways components"}
{"id": "93e09c5feb9b2ffc8926b4edff13b3d8e02e41de", "abstract": "Fluid optimization in the resuscitation of shock became the mainstay of treatment following the advent of Early Goal-Directed Therapy (EGDT) by Rivers et al. in 2001 [1]. Patients presenting in shock require prompt optimization of volume status and cardiac out- put to ensure adequate perfusion. Poor optimization may be associated with prolonged hospital and intensive care unit stays. The prior gold standard, pulmonary artery catheterization, is rarely available in the emergency department setting and its invasive nature has led to recent re-evaluation of its clinical utility. However, there are new monitoring technologies that are being studied in the intensive care unit setting that may soon be available in emergency departments to aid in nursing and physician decision making to improve acute resuscitation.", "title": "emerging technology in acute resuscitation monitoring", "url": "http://www.scirp.org/journal/PaperDownload.aspx?paperID=24794", "tokenized_text": "fluid optimization treatment following advent early goal directed therapy rivers et_al et al patients presenting require prompt_optimization optimization volume status ensure adequate poor optimization associated prolonged hospital intensive care unit stays prior gold standard rarely available setting nature led recent evaluation clinical utility new monitoring technologies studied intensive care unit setting soon available aid physician decision_making decision making improve"}
{"id": "98090bbc7b784a1f64d4522c5e1987b196863fd0", "abstract": "Introduction Prophylactic vaccination is regarded as the most effective means to control avian flu infection. Currently, there is a need for a universal vaccine that provides broad and long-lasting protection against influenza virus. Meanwhile, although yeast-based vaccines have been used in clinic, studies are still required to further understand the molecular mechanism of yeast-based vaccines under physiological conditions. Methods We generated a yeast-based vaccine against influenza hemagglutinin (HA) of H5, H7 and H9 using surface displaying technology and evaluated the protective efficacy of chickens after exposure to H9N2 influenza virus. Results Oral yeast vaccine provided less clinical syndrome, reduced viral loading and alleviated airway damage significantly. Compared to the commercial inactivated vaccine, yeast vaccine stimulated the activation of splenic NK and APCs cells and boosted TLR7-IRF7-IFN signaling in spleen. Meanwhile, \u03b3\u03b4 T cells in the bursa of Fabricius were activated and the innate lymphoid cells (ILCs) in the bursa of Fabricius promoted the CILPs to differentiate to ILC3 cells in oral yeast birds. Moreover, the reshaped gut microbiota and a suppressed Th17-IL17-mediated inflammation in intestine was observed in oral yeast chickens, which might facilitate the recovery of intestinal mucosal immunity upon virus infection. Collectively, our findings suggest that oral yeast based multivalent bird flu vaccines provide an attractive strategy to update host defense function via reshapes of multi-systemic immune homeostasis.", "title": "recombinant hemagglutinin displaying on yeast reshapes congenital lymphocyte subsets to prompt optimized systemic immune protection against avian influenza infection", "url": "https://www.frontiersin.org/articles/10.3389/fmicb.2023.1153922/pdf", "tokenized_text": "introduction regarded effective means control infection currently need universal vaccine provides broad long lasting protection based studies required understand molecular mechanism based conditions methods generated based vaccine surface displaying technology evaluated efficacy exposure results vaccine provided clinical reduced viral loading alleviated damage significantly compared commercial vaccine vaccine stimulated activation cells boosted signaling cells activated innate cells promoted differentiate cells birds reshaped suppressed observed facilitate recovery infection collectively findings_suggest findings suggest based bird provide attractive strategy update host defense function reshapes multi systemic"}
{"id": "07cd498aacfb4d39fa2e0e8d8a9c8ad881257300", "abstract": "Text-based generative art has seen an explosion of interest in 2021. Online communities around text-based generative art as a novel digital medium have quickly emerged. This short paper identifies five types of prompt modifiers used by practitioners in the community of text-based generative art based on a 3-month ethnographic study on Twitter. The novel taxonomy of prompt modifiers provides researchers a conceptual starting point for investigating the practices of text-based generative art, but also may help practitioners of text-based generative art improve their images. The paper concludes with a discussion of research opportunities in the space of text-based generative art and the broader implications of prompt engineering from the perspective of human-AI interaction in future applications beyond the use case of text-based generative art.", "title": "prompt engineering for textbased generative art", "url": "http://arxiv.org/pdf/2204.13988", "tokenized_text": "text based generative art seen explosion interest 2021 online communities text based generative art novel digital medium quickly emerged short paper identifies types modifiers practitioners community text based generative art based month ethnographic study twitter novel taxonomy modifiers provides researchers conceptual starting point investigating practices text based generative art help practitioners text based generative art improve images paper concludes discussion research opportunities space text based generative art broader implications prompt_engineering engineering perspective human ai interaction future applications use case text based generative art"}
{"id": "08e0e696732103e585fd629e23888fd4acbb22df", "abstract": "This paper presents an approach to tackle the task of Visual Word Sense Disambiguation (Visual-WSD), which involves determining the most appropriate image to represent a given polysemous word in one of its particular senses. The proposed approach leverages the CLIP model, prompt engineering, and text-to-image models such as GLIDE and DALL-E 2 for both image retrieval and generation. To evaluate our approach, we participated in the SemEval 2023 shared task on \u201cVisual Word Sense Disambiguation (Visual-WSD)\u201d using a zero-shot learning setting, where we compared the accuracy of different combinations of tools, including \u201cSimple prompt-based\u201d methods and \u201cGenerated prompt-based\u201d methods for prompt engineering using completion models, and text-to-image models for changing input modality from text to image. Moreover, we explored the benefits of cross-modality evaluation between text and candidate images using CLIP. Our experimental results demonstrate that the proposed approach reaches better results than cross-modality approaches, highlighting the potential of prompt engineering and text-to-image models to improve accuracy in Visual-WSD tasks. We assessed our approach in a zero-shot learning scenario and attained an accuracy of 68.75\\% in our best attempt.", "title": "ebhaam at semeval2023 task 1 a clipbased approach for comparing crossmodality and unimodality in visual word sense disambiguation", "url": "https://aclanthology.org/2023.semeval-1.269.pdf", "tokenized_text": "paper_presents paper presents approach tackle task visual word sense disambiguation visual involves determining appropriate image represent given polysemous word particular senses proposed approach leverages clip prompt_engineering engineering text image dall image retrieval generation evaluate approach semeval 2023 shared task visual word sense disambiguation visual zero shot_learning shot learning setting compared accuracy different combinations tools including simple based methods generated based methods prompt_engineering engineering completion text image changing input modality text_to_image text image explored benefits cross modality evaluation text candidate images clip experimental_results experimental results demonstrate proposed approach reaches better results cross modality approaches highlighting potential prompt_engineering engineering text image improve accuracy visual tasks assessed approach zero shot_learning shot learning scenario attained accuracy best attempt"}
{"id": "0b94b999fdd9488e1a0914d37f8fb3ea7e9ea0fd", "abstract": "Research suggests that providing specific and timely feedback to human tutors enhances their performance. However, it presents challenges due to the time-consuming nature of assessing tutor performance by human evaluators. Large language models, such as the AI-chatbot ChatGPT, hold potential for offering constructive feedback to tutors in practical settings. Nevertheless, the accuracy of AI-generated feedback remains uncertain, with scant research investigating the ability of models like ChatGPT to deliver effective feedback. In this work-in-progress, we evaluate 30 dialogues generated by GPT-4 in a tutor-student setting. We use two different prompting approaches, the zero-shot chain of thought and the few-shot chain of thought, to identify specific components of effective praise based on five criteria. These approaches are then compared to the results of human graders for accuracy. Our goal is to assess the extent to which GPT-4 can accurately identify each praise criterion. We found that both zero-shot and few-shot chain of thought approaches yield comparable results. GPT-4 performs moderately well in identifying instances when the tutor offers specific and immediate praise. However, GPT-4 underperforms in identifying the tutor's ability to deliver sincere praise, particularly in the zero-shot prompting scenario where examples of sincere tutor praise statements were not provided. Future work will focus on enhancing prompt engineering, developing a more general tutoring rubric, and evaluating our method using real-life tutoring dialogues.", "title": "comparative analysis of gpt4 and human graders in evaluating human tutors giving praise to students", "url": "https://arxiv.org/pdf/2307.02018", "tokenized_text": "research suggests providing specific timely feedback human tutors enhances performance presents challenges time consuming nature assessing tutor performance human evaluators large_language large language ai chatbot chatgpt hold potential offering constructive feedback tutors practical settings accuracy ai generated feedback remains uncertain scant research investigating ability like_chatgpt like chatgpt deliver effective feedback work progress evaluate 30 dialogues generated gpt-4 tutor student setting use different approaches zero shot chain_of_thought chain thought shot chain_of_thought chain thought identify specific components effective praise based criteria approaches compared results human graders accuracy goal assess extent gpt-4 accurately identify praise criterion found zero shot shot chain_of_thought chain thought approaches yield comparable results gpt-4 performs moderately identifying instances tutor offers specific immediate praise gpt-4 underperforms identifying tutor ability deliver sincere praise particularly zero shot_prompting shot scenario examples sincere tutor praise statements provided future work focus enhancing prompt_engineering engineering developing general tutoring rubric evaluating method real life tutoring dialogues"}
{"id": "221e801f9a39ff055773b2a20d91e3efadbea921", "abstract": "Rational engineering to produce biologically active plant compounds has been greatly impeded by our poor understanding of the regulatory and metabolic pathways underlying the biosynthesis of these compounds. Here we capitalized on our previously described gene-to-metabolite network in order to engineer rosmarinic acid (RA) biosynthesis pathway for the production of beneficial RA and lithospermic acid B (LAB) in Salvia miltiorrhiza hairy root cultures. Results showed their production was greatly elevated by (1) overexpression of single gene, including cinnamic acid 4-hydroxylase (c4h), tyrosine aminotransferase (tat), and 4-hydroxyphenylpyruvate reductase (hppr), (2) overexpression of both tat and hppr, and (3) suppression of 4-hydroxyphenylpyruvate dioxygenase (hppd). Co-expression of tat/hppr produced the most abundant RA (906 mg/liter) and LAB (992 mg/liter), which were 4.3 and 3.2-fold more than in their wild-type (wt) counterparts respectively. And the value of RA concentration was also higher than that reported before, that produced by means of nutrient medium optimization or elicitor treatment. It is the first report of boosting RA and LAB biosynthesis through genetic manipulation, providing an effective approach for their large-scale commercial production by using hairy root culture systems as bioreactors.", "title": "the c4h, tat, hppr and hppd genes prompted engineering of rosmarinic acid biosynthetic pathway in salvia miltiorrhiza hairy root cultures", "url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0029713&type=printable", "tokenized_text": "engineering produce biologically active plant compounds greatly poor understanding regulatory metabolic pathways underlying compounds capitalized previously described network order engineer acid ra production beneficial ra acid lab root cultures results showed production greatly elevated single including acid co expression produced abundant ra lab 4.3 fold wild type counterparts respectively value ra higher reported produced means medium optimization treatment report boosting ra lab genetic manipulation providing effective approach large scale commercial production root culture systems"}
{"id": "27d80545d142ced9b921290b5b2798cabd55468b", "abstract": "This study evaluated ChatGPT\u2019s ability to understand causal language in science papers and news by testing its accuracy in a task of labeling the strength of a claim as causal, conditional causal, correlational, or no relationship. The results show that ChatGPT is still behind the existing fine-tuned BERT models by a large margin. ChatGPT also had difficulty understanding conditional causal claims mitigated by hedges. However, its weakness may be utilized to improve the clarity of human annotation guideline. Chain-of-Thoughts were faithful and helpful for improving prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective method to establish cause-effect between prompts and outcomes, suggesting caution when generalizing prompt engineering results across tasks or models.", "title": "can chatgpt understand causal language in science claims", "url": "https://aclanthology.org/2023.wassa-1.33.pdf", "tokenized_text": "study evaluated chatgpt ability understand causal language science papers news testing accuracy task labeling strength claim causal conditional causal relationship results chatgpt existing fine tuned bert large margin chatgpt difficulty understanding conditional causal claims mitigated weakness utilized improve clarity human annotation guideline chain thoughts faithful helpful improving performance finding optimal difficult inconsistent results lack effective method establish cause effect outcomes suggesting caution generalizing prompt_engineering engineering results tasks"}
{"id": "2d90460431c093757fcf651e333bc0da5f5404c2", "abstract": "This paper introduces a prompt-based method for few-shot learning addressing, as an application example, contextual stance classification, that is, the task of determining the attitude expressed by a given statement within a conversation thread with multiple points of view towards another statement. More specifically, we envisaged a method that uses the existing conversation thread (i.e., messages that are part of the test data) to create natural language prompts for few-shot learning with minimal reliance on training samples, whose preliminary results suggest that prompt engineering may be a competitive alternative to supervised methods both in terms of accuracy and development costs for the task at hand.", "title": "contextual stance classification using prompt engineering", "url": "https://sol.sbc.org.br/index.php/stil/article/download/25435/25256", "tokenized_text": "paper introduces based method shot_learning shot learning addressing application example contextual stance classification task determining attitude expressed given statement conversation thread multiple points view statement specifically method uses existing conversation thread i.e. messages test data create natural_language natural language shot_learning shot learning minimal reliance training samples preliminary results suggest prompt_engineering engineering competitive alternative supervised methods terms accuracy development costs task hand"}
{"id": "3159478fbc81e562c812b9d5dc1891271b21f0c4", "abstract": "Artificial intelligence-powered generative language models (GLMs), such as ChatGPT, Perplexity AI, and Google Bard, have the potential to provide personalized learning, unlimited practice opportunities, and interactive engagement 24/7, with immediate feedback. However, to fully utilize GLMs, properly formulated instructions are essential. Prompt engineering is a systematic approach to effectively communicating with GLMs to achieve the desired results. Well-crafted prompts yield good responses from the GLM, while poorly constructed prompts will lead to unsatisfactory responses. Besides the challenges of prompt engineering, significant concerns are associated with using GLMs in medical education, including ensuring accuracy, mitigating bias, maintaining privacy, and avoiding excessive reliance on technology. Future directions involve developing more sophisticated prompt engineering techniques, integrating GLMs with other technologies, creating personalized learning pathways, and researching the effectiveness of GLMs in medical education.", "title": "prompt engineering in medical education", "url": "https://www.mdpi.com/2813-141X/2/3/19/pdf?version=1693479951", "tokenized_text": "artificial_intelligence artificial intelligence powered generative language_models language chatgpt perplexity ai google_bard google bard potential provide personalized learning unlimited practice opportunities interactive engagement immediate feedback fully utilize properly formulated instructions essential prompt_engineering engineering systematic approach effectively communicating achieve desired results crafted yield good responses poorly constructed lead unsatisfactory responses challenges prompt_engineering engineering significant concerns associated medical education including ensuring accuracy mitigating bias maintaining privacy avoiding excessive reliance technology future directions involve developing sophisticated prompt_engineering engineering techniques integrating technologies creating personalized learning pathways effectiveness medical education"}
{"id": "358d1d9eed69a6eadcda9996b3f13b0e0a356b88", "abstract": "ChatGPT is an artificial intelligence (AI) system that can perform sophisticated writing and dialogs after learning from vast amounts of linguistic data. The success of ChatGPT is phenomenal. AI-based human-machine language interaction has been at the center of AI competition in recent years. The major players in this game have been Google, Meta, and OpenAI. Google was in the best position from the outset, given its invention of Transformer (the cornerstone of all cutting-edge language models) and its significant edge in reinforcement learning. Yet, Google\u2019s efforts in this area were rather diffusing. It kept generating language model variants with incremental innovations but failed to reach the next level. Meta has a strong AI team, including many top AI researchers in the world. Nevertheless, their faith in self-supervised learning to solve human-machine interaction did not deliver high-impact success. Conversely, OpenAI, with a small team, stayed focused on a single product line (GPT, including its latest release of GPT-4). It moved in the right direction of using human input to \u201calign\u201d the language model based on the Reinforcement Learning from Human Feedback (RLHF) approach. The fact that OpenAI ultimately prevailed in this game shows that the model alignment to human labeling through supervised and reinforcement learning is critical for human-machine interaction. However, a chatbot\u2019s actions rely heavily on cues (prompts) provided by human operators. To properly utilize ChatGPT\u2019s capabilities, prompts to instruct or mentor the chatbot must be carefully designed to get valuable, valid, and robust responses. This process becomes another \u201calignment\u201d problem of using prompt engineering to best probe ChatGPT\u2019s knowledge graph for best serving users\u2019 needs.", "title": "chatgpt opens a new door for bioinformatics", "url": "https://journal.hep.com.cn/qb/EN/PDF/10.15302/J-QB-023-0328", "tokenized_text": "chatgpt artificial_intelligence artificial intelligence ai system perform sophisticated writing dialogs learning vast amounts linguistic data success chatgpt ai based human machine language interaction center ai competition recent_years recent years major players game google meta openai google best position given invention transformer cornerstone cutting edge language_models language significant edge reinforcement_learning reinforcement learning google efforts area kept generating language_model language variants incremental innovations failed reach level meta strong ai team including ai researchers world self supervised learning solve human machine interaction deliver high impact success conversely openai small team focused single product line gpt including latest release gpt-4 right direction human input align language_model language based reinforcement_learning reinforcement learning human feedback rlhf approach fact openai ultimately game shows alignment human labeling supervised reinforcement_learning reinforcement learning critical human machine interaction chatbot actions rely heavily cues provided human operators properly utilize chatgpt capabilities instruct chatbot carefully designed valuable valid robust responses process alignment problem prompt_engineering engineering best probe chatgpt knowledge_graph knowledge graph best serving users needs"}
{"id": "3613299c54bbea66dd6db1b00573f7ade021a5a9", "abstract": "Large Language Models (LLMs) can be used as repositories of biological and chemical information to generate pharmacological lead compounds. However, for LLMs to focus on specific drug targets typically require experimentation with progressively more refined prompts. Results thus become dependent not just on what is known about the target, but also on what is known about the prompt-engineering. In this paper, we separate the prompt into domain-constraints that can be written in a standard logical form, and a simple text-based query. We investigate whether LLMs can be guided, not by refining prompts manually, but by refining the the logical component automatically, keeping the query unchanged. We describe an iterative procedure LMLF (\u201cLanguage Models with Logical Feedback\u201d) in which the constraints are progressively refined using a logical notion of generalisation. On any iteration, newly generated instances are verified against the constraint, providing \u201clogical-feedback\u201d for the next iteration\u2019s refinement of the constraints. We evaluate LMLF using two well-known targets (inhibition of the Janus Kinase 2; and Dopamine Receptor D2); and two different LLMs (GPT-3 and PaLM). We show that LMLF, starting with the same logical constraints and query text, can guide both LLMs to generate potential leads. We find: (a) Binding affinities of LMLF-generated molecules are skewed towards higher binding affinities than those from existing baselines; LMLF results in generating molecules that are skewed towards higher binding affinities than without logical feedback; (c) Assessment by a computational chemist suggests that LMLF generated compounds may be novel inhibitors. These findings suggest that LLMs with logical feedback may provide a mechanism for generating new leads without requiring the domain-specialist to acquire sophisticated skills in prompt-engineering.", "title": "generating novel leads for drug discovery using llms with logical feedback", "url": "https://www.biorxiv.org/content/biorxiv/early/2023/09/17/2023.09.14.557698.full.pdf", "tokenized_text": "large_language large language llms repositories biological chemical information generate lead compounds llms focus specific drug targets typically require experimentation progressively refined results dependent known target known engineering paper separate domain constraints written standard logical form simple text based query investigate llms guided refining manually refining logical component automatically keeping query unchanged describe iterative procedure language_models language logical feedback constraints progressively refined logical notion generalisation iteration newly generated instances verified constraint providing logical feedback iteration refinement constraints evaluate known targets d2 different llms gpt-3 palm starting logical constraints query text guide llms generate potential leads find binding affinities generated molecules skewed higher binding affinities existing baselines results generating molecules skewed higher binding affinities logical feedback assessment computational chemist suggests generated compounds novel findings_suggest findings suggest llms logical feedback provide mechanism generating new leads requiring domain acquire sophisticated skills engineering"}
{"id": "3da79f3fe4e0ff1bb59efb34c8baa2bcf632c2b9", "abstract": "Class Activation Map (CAM) has emerged as a popular tool for weakly supervised semantic segmentation (WSSS), allowing the localization of object regions in an image using only image-level labels. However, existing CAM methods suffer from under-activation of target object regions and false-activation of background regions due to the fact that a lack of detailed supervision can hinder the model's ability to understand the image as a whole. In this paper, we propose a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps. First, a series of carefully designed questions are posed to the VQA (Visual Question Answering) model with Question-Answer Prompt Engineering (QAPE) to generate a corpus of both foreground target objects and backgrounds that are adaptive to query images. We then employ contrastive learning in a Region Image Text Contrastive (RITC) network to compare the obtained foreground and background regions with the generated corpus. Our approach exploits the rich textual information from the open vocabulary as additional supervision, enabling the model to generate high-quality CAMs with a more complete object region and reduce false-activation of background regions. We conduct extensive analysis to validate the proposed method and show that our approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO datasets.", "title": "qaclims questionanswer cross language image matching for weakly supervised semantic segmentation", "url": "https://dl.acm.org/doi/pdf/10.1145/3581783.3612148", "tokenized_text": "class activation_map activation map cam emerged popular tool weakly supervised semantic segmentation wsss allowing localization object regions image image level labels existing cam methods suffer activation target object regions false activation background regions fact lack detailed supervision hinder ability understand image paper propose_a_novel propose novel matching framework wsss qa leveraging vision language foundation maximize text based understanding images guide generation activation maps series carefully designed questions posed vqa visual question_answering question answering question answer prompt_engineering engineering generate corpus foreground target objects adaptive query images employ contrastive_learning contrastive learning region image text contrastive network compare obtained foreground background regions generated corpus approach exploits rich textual information open vocabulary additional supervision enabling generate high quality complete object region reduce false activation background regions conduct_extensive conduct extensive analysis validate proposed_method proposed method approach performs state art pascal voc ms coco datasets"}
{"id": "513b96c7d5d1f9a74afd9d946d5a7c83fe592869", "abstract": "This study presents a review of search engines and search engine optimization and shows how the search engine landscape relates to sustainable development. We have used a narrative review research method and described three main topics: the past and present of web catalogs and search engines; current knowledge about the dominant types of search results presented in Google search; and methods of search engine optimization. Technical elements of important website areas related to technical website auditing are discussed. We summarize our research with several key findings on how web search engines are involved in sustainable development and offer a glimpse into the future use of web searching with the help of artificial intelligence chats and prompt engineering.", "title": "from web catalogs to google a retrospective study of web search engines sustainable development", "url": "https://www.mdpi.com/2071-1050/15/8/6768/pdf?version=1681779086", "tokenized_text": "study presents review search engines search engine optimization shows search engine landscape relates sustainable development narrative review research method described main topics past present web search engines current knowledge dominant types search results presented google search methods search engine optimization technical elements important website areas related technical website auditing discussed summarize research key findings web search engines involved sustainable development offer glimpse future use web searching help artificial_intelligence artificial intelligence prompt_engineering engineering"}
{"id": "579ee305d538a679d72b808ffe8322680561a177", "abstract": "Some recent methods address few-shot classification by integrating visual and semantic prototypes. However, they usually ignore the difference in feature structure between the visual and semantic modalities, which leads to limited performance improvements. In this paper, we propose a novel method, called bimodal integrator (BMI), to better integrate visual and semantic prototypes. In BMI, we first construct a latent space for each modality via a variational autoencoder, and then align the semantic latent space to the visual latent space. Through this semantics-to-vision alignment, the semantic modality is mapped to the visual latent space and has the same feature structure as the visual modality. As a result, the visual and semantic prototypes can be better integrated. In addition, based on the multivariate Gaussian distribution and the prompt engineering, a data augmentation scheme is designed to ensure the accuracy of modality alignment during the training process. Experimental results demonstrate that BMI significantly improves few-shot classification, making simple baselines outperform the most advanced methods on miniImageNet and tieredImageNet datasets.", "title": "better integrating vision and semantics for improving fewshot classification", "url": "https://dl.acm.org/doi/pdf/10.1145/3581783.3613819", "tokenized_text": "recent methods address shot classification integrating visual semantic prototypes usually ignore difference feature structure visual semantic modalities leads limited performance improvements paper propose_a_novel propose novel method called better integrate visual semantic prototypes construct latent space modality variational autoencoder align semantic latent space visual latent space semantics vision alignment semantic modality mapped visual latent space feature structure visual modality result visual semantic prototypes better integrated addition based gaussian distribution prompt_engineering engineering data_augmentation data augmentation scheme designed ensure accuracy modality alignment training process experimental_results experimental results demonstrate significantly improves shot classification making simple baselines outperform advanced methods miniimagenet datasets"}
{"id": "59266e06cdb867c2541603f9d94e13f67d55938f", "abstract": "In this paper, we present our vision of OmniscientDB, a novel database that leverages the implicitly-stored knowledge in large language models to augment datasets for analytical queries or even machine learning tasks. OmiscientDB empowers its users to augment their datasets by means of simple SQL queries and thus has the potential to dramatically reduce the manual overhead associated with data integration. It uses automatic prompt engineering to construct appropriate prompts for given SQL queries and passes them to a large language model like GPT-3 to contribute additional data (i.e., new rows, columns, or entire tables), augmenting the explicitly stored data. Our initial evaluation demonstrates the general feasibility of our vision, explores different prompting techniques in greater detail, and points towards several directions for future research.", "title": "omniscientdb a large language modelaugmented dbms that knows what other dbmss do not know", "url": "http://publikationen.ub.uni-frankfurt.de/files/74426/06_08.pdf", "tokenized_text": "paper present vision novel database leverages implicitly stored knowledge large_language large language augment datasets analytical queries machine_learning machine learning tasks empowers users augment datasets means simple sql queries potential dramatically reduce manual overhead associated data integration uses automatic prompt_engineering engineering construct appropriate given sql queries passes large_language large language like gpt-3 contribute additional data i.e. new columns entire tables augmenting explicitly stored data initial evaluation demonstrates general feasibility vision explores different prompting_techniques techniques greater detail points directions future_research future research"}
{"id": "5e01b8383e9260b2e251274a6bad89677cb1bbd3", "abstract": "Suicide, a serious public health concern affecting millions of individuals worldwide, refers to the intentional act of ending one's own life. Mental health issues such as depression, frustration, and hopelessness can directly or indirectly influence the emergence of suicidal thoughts. Early identification of these thoughts is crucial for timely diagnosis. In recent years, advances in artificial intelligence (AI) and natural language processing (NLP) have paved the way for revolutionizing mental health support and education. In this proof-of-concept study, we have created MindWatch, a cutting-edge tool that harnesses the power of AI-driven language models to serve as a valuable computer-aided system for the mental health professions to achieve two important goals such as early symptom detection, and personalized psychoeducation. We utilized ALBERT and Bio-Clinical BERT language models and fine-tuned them with the Reddit dataset to build the classifiers. We evaluated the performance of bi-LSTM, ALBERT, Bio-Clinical BERT, OpenAI GPT3.5 (via prompt engineering), and an ensembled voting classifier to detect suicide ideation. For personalized psychoeducation, we used the state-of-the-art Llama 2 foundation model leveraging prompt engineering. The tool is developed in the Amazon Web Service environment. All models performed exceptionally well, with accuracy and precision/recall greater than 92%. ALBERT performed better (AUC=.98) compared to the zero-shot classification accuracies obtained from OpenAI GPT3.5 Turbo (ChatGPT) on hidden datasets (AUC=.91). Furthermore, we observed that the inconclusiveness rate of the Llama 2 model is low while tested for few examples. This study emphasizes how transformer models can help provide customized psychoeducation to individuals dealing with mental health issues. By tailoring content to address their unique mental health conditions, treatment choices, and self-help resources, this approach empowers individuals to actively engage in their recovery journey. Additionally, these models have the potential to advance the automated detection of depressive disorders.", "title": "mindwatch a smart cloudbased ai solution for suicide ideation detection leveraging large language models", "url": "https://www.medrxiv.org/content/medrxiv/early/2023/09/26/2023.09.25.23296062.full.pdf", "tokenized_text": "public health concern affecting millions individuals worldwide refers act life mental_health mental health issues directly indirectly influence emergence thoughts early identification thoughts crucial timely diagnosis recent_years recent years advances artificial_intelligence artificial intelligence ai natural_language natural language processing nlp way revolutionizing mental_health mental health support education proof concept study created cutting edge tool harnesses power ai driven language_models language serve valuable computer aided system mental_health mental health achieve important goals early symptom detection personalized utilized bert language_models language fine tuned reddit dataset build classifiers evaluated performance bi lstm bert openai gpt3.5 prompt_engineering engineering voting classifier detect ideation personalized state art llama foundation leveraging prompt_engineering engineering tool developed amazon web service environment performed accuracy precision recall greater performed better compared zero shot classification accuracies obtained openai gpt3.5 turbo chatgpt hidden datasets furthermore observed rate llama low tested examples study emphasizes transformer help provide customized individuals dealing mental_health mental health issues tailoring content address unique mental_health mental health conditions treatment choices self help resources approach empowers individuals actively engage recovery journey additionally potential advance automated detection depressive disorders"}
{"id": "6b80c6e220ca2e2434f5a80b2eb5e8b645e97ae1", "abstract": ": Large language models (LLMs) offer significant promise as a knowledge source for robotic task learning. Prompt engineering has been shown to be effective for eliciting knowledge from an LLM but alone is insufficient for acquiring relevant, situationally grounded knowledge for an embodied robotic agent learning novel tasks. We describe a cognitive-agent approach that extends and complements prompt engineering, mitigating its limitations, and thus enabling a robot to acquire new task knowledge matched to its native language capabilities, embodiment, environment, and user preferences. The approach is to increase the response space of LLMs and deploy general strategies, embedded within the autonomous robot, to evaluate, repair, and select among candidate responses produced by the LLM. We describe the approach and experiments that show how a robot, by retrieving and evaluating a breadth of responses from the LLM, can achieve > 75% task completion in one-shot learning without user oversight. The approach achieves 100% task completion when human oversight (such as indication of preference) is provided, while greatly reducing how much human oversight is needed.", "title": "improving knowledge extraction from llms for robotic task learning through agent analysis", "url": "https://arxiv.org/pdf/2306.06770", "tokenized_text": "large_language large language llms offer significant promise knowledge source robotic task learning prompt_engineering engineering shown effective eliciting knowledge llm insufficient acquiring relevant situationally grounded knowledge embodied robotic agent learning novel tasks describe cognitive agent approach extends prompt_engineering engineering mitigating limitations enabling robot acquire new task knowledge matched native language capabilities embodiment environment user preferences approach increase response space llms deploy general strategies embedded autonomous robot evaluate repair select candidate responses produced llm describe approach experiments robot retrieving evaluating breadth responses llm achieve 75 task completion shot_learning shot learning user oversight approach achieves 100 task completion human oversight preference provided greatly reducing human oversight needed"}
{"id": "7de25ad5ac7433e4d4071f450461b03fd2a39b8d", "abstract": "Large language models (LLMs) have become prominent tools in various domains, such as natural language processing, machine translation, and the development of creative text. Nevertheless, in order to fully exploit the capabilities of Language Models, it is imperative to establish efficient communication channels between humans and machines. The discipline of engineering involves the creation of well-constructed and informative prompts, which act as a crucial link between human intention and the execution of tasks by machines. The present study examines the concept of rapid engineering, elucidating its underlying concepts, methodologies, and diverse range of practical applications.", "title": "prompt engineering guiding the way to effective large language models", "url": "https://journal.esj.edu.iq/index.php/IJCM/article/download/1356/321", "tokenized_text": "large_language large language llms prominent tools domains natural_language natural language processing machine_translation machine translation development creative text order fully exploit capabilities language_models language imperative establish efficient communication channels humans machines discipline engineering involves creation constructed informative act crucial link human intention execution tasks machines present study examines concept rapid engineering underlying concepts methodologies diverse range practical applications"}
{"id": "nan", "abstract": "  Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed toprovide useful and safe responses. However, adversarial prompts known as'jailbreaks' can circumvent safeguards, leading LLMs to generate harmfulcontent. Exploring jailbreak prompts can help to better reveal the weaknessesof LLMs and further steer us to secure them. Unfortunately, existing jailbreakmethods either suffer from intricate manual design or require optimization onanother white-box model, compromising generalization or jailbreak efficiency.In this paper, we generalize jailbreak prompt attacks into two aspects: (1)Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM,an automatic framework that leverages LLMs themselves to generate effectivejailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantlyimproves the attack success rate while greatly reducing the time cost comparedto existing baselines. Our study also reveals the inadequacy of current defensemethods in safeguarding LLMs. Finally, we offer detailed analysis anddiscussion from the perspective of prompt execution priority on the failure ofLLMs' defense. We hope that our research can catalyze both the academiccommunity and LLMs vendors towards the provision of safer and more regulatedLarge Language Models.", "title": "a wolf in sheep's clothing generalized nested jailbreak prompts can fool large language models easily", "url": "http://arxiv.org/pdf/2311.08268v1.pdf", "tokenized_text": "large_language large language llms chatgpt gpt-4 designed useful safe responses adversarial known circumvent safeguards leading llms generate harmfulcontent exploring jailbreak_prompts jailbreak help better reveal weaknessesof llms steer secure unfortunately existing suffer intricate manual design require optimization white box compromising generalization jailbreak efficiency paper generalize jailbreak attacks aspects rewriting scenario nesting based propose automatic framework leverages llms generate extensive_experiments extensive experiments demonstrate significantlyimproves attack success_rate success rate greatly reducing time cost comparedto existing baselines study reveals inadequacy current safeguarding llms finally offer detailed analysis perspective execution priority failure ofllms defense hope research llms vendors provision safer language_models language"}
{"id": "nan", "abstract": "  Existing work on jailbreak Multimodal Large Language Models (MLLMs) hasfocused primarily on adversarial examples in model inputs, with less attentionto vulnerabilities in model APIs. To fill the research gap, we carry out thefollowing work: 1) We discover a system prompt leakage vulnerability in GPT-4V.Through carefully designed dialogue, we successfully steal the internal systemprompts of GPT-4V. This finding indicates potential exploitable security risksin MLLMs; 2)Based on the acquired system prompts, we propose a novel MLLMjailbreaking attack method termed SASP (Self-Adversarial Attack via SystemPrompt). By employing GPT-4 as a red teaming tool against itself, we aim tosearch for potential jailbreak prompts leveraging stolen system prompts.Furthermore, in pursuit of better performance, we also add human modificationbased on GPT-4's analysis, which further improves the attack success rate to98.7\\%; 3) We evaluated the effect of modifying system prompts to defendagainst jailbreaking attacks. Results show that appropriately designed systemprompts can significantly reduce jailbreak success rates. Overall, our workprovides new insights into enhancing MLLM security, demonstrating the importantrole of system prompts in jailbreaking, which could be leveraged to greatlyfacilitate jailbreak success rates while also holding the potential fordefending against jailbreaks.", "title": "jailbreaking gpt4v via selfadversarial attacks with system prompts", "url": "http://arxiv.org/pdf/2311.09127v1.pdf", "tokenized_text": "existing work jailbreak multimodal large_language large language mllms primarily adversarial examples inputs vulnerabilities apis fill research gap carry work discover system leakage vulnerability carefully designed dialogue successfully steal internal gpt-4v. finding indicates potential security mllms acquired system propose_a_novel propose novel attack method termed self adversarial attack employing gpt-4 red teaming tool aim potential jailbreak_prompts jailbreak leveraging system furthermore pursuit better performance add human gpt-4 analysis improves attack success_rate success rate evaluated effect modifying system jailbreaking attacks results appropriately designed significantly reduce jailbreak success rates overall new insights enhancing mllm security demonstrating system jailbreaking leveraged jailbreak success rates potential jailbreaks"}
{"id": "nan", "abstract": "  Recent studies have demonstrated that large language models (LLMs) excel indiverse tasks through in-context learning (ICL) facilitated by task-specificprompts and examples. However, the existing literature shows that ICLencounters performance deterioration when exposed to adversarial inputs.Enhanced performance has been observed when ICL is augmented with naturallanguage explanations (NLEs) (we refer to it as X-ICL). Thus, this workinvestigates whether X-ICL can improve the robustness of LLMs on a suite ofseven adversarial and challenging natural language inference datasets.Moreover, we introduce a new approach to X-ICL by prompting an LLM (ChatGPT inour case) with few human-generated NLEs to produce further NLEs (we call itChatGPT few-shot), which we show superior to both ChatGPT zero-shot andhuman-generated NLEs alone. We evaluate five popular LLMs (GPT3.5-turbo,LLaMa2, Vicuna, Zephyr, Mistral) and show that X-ICL with ChatGPT few-shotyields over 6% improvement over ICL. Furthermore, while prompt selectionstrategies were previously shown to significantly improve ICL onin-distribution test sets, we show that these strategies do not match theefficacy of the X-ICL paradigm in robustness-oriented evaluations.", "title": "using natural language explanations to improve robustness of incontext learning for natural language inference", "url": "http://arxiv.org/pdf/2311.07556v1.pdf", "tokenized_text": "recent studies demonstrated large_language large language llms excel indiverse tasks context_learning context learning icl facilitated task specificprompts examples existing literature shows performance deterioration exposed adversarial inputs enhanced performance observed icl augmented naturallanguage explanations nles refer icl icl improve robustness llms suite adversarial challenging natural_language natural language inference datasets introduce new approach icl llm chatgpt inour case human generated nles produce nles shot superior chatgpt zero shot andhuman generated nles evaluate popular llms gpt3.5 turbo llama2 vicuna icl chatgpt improvement icl furthermore previously shown significantly improve icl onin distribution test sets strategies match theefficacy icl paradigm robustness oriented evaluations"}
{"id": "nan", "abstract": "  Large language models (LLMs) have demonstrated an impressive ability tosynthesize plausible and fluent text. However they remain vulnerable tohallucinations, and thus their outputs generally require manual humanverification for high-stakes applications, which can be time-consuming anddifficult. This paper proposes symbolically grounded generation (SymGen) as asimple approach for enabling easier validation of an LLM's output. SymGenprompts an LLM to interleave its regular output text with explicit symbolicreferences to fields present in some conditioning data (e.g., a table in JSONformat). The references can be used to display the provenance of differentspans of text in the generation, reducing the effort required for manualverification. Across data-to-text and question answering experiments, we findthat LLMs are able to directly output text that makes use of symbolicreferences while maintaining fluency and accuracy.", "title": "towards verifiable text generation with symbolic references", "url": "http://arxiv.org/pdf/2311.09188v1.pdf", "tokenized_text": "large_language large language llms demonstrated impressive ability plausible fluent text remain vulnerable outputs generally require manual high applications time consuming paper_proposes paper proposes symbolically grounded generation asimple approach enabling easier validation llm output llm regular output text explicit fields present conditioning data e.g. table references display text generation reducing effort required data text question_answering question answering experiments findthat llms able directly output text makes use maintaining fluency accuracy"}
{"id": "nan", "abstract": "  We study semi-supervised sequence prediction tasks where labeled data are tooscarce to effectively finetune a model and at the same time few-shot promptingof a large language model (LLM) has suboptimal performance. This happens when atask, such as parsing, is expensive to annotate and also unfamiliar to apretrained LLM. In this paper, we present a discovery that student modelsdistilled from a prompted LLM can often generalize better than their teacher onsuch tasks. Leveraging this finding, we propose a new distillation method,multistage collaborative knowledge distillation from an LLM (MCKD), for suchtasks. MCKD first prompts an LLM using few-shot in-context learning to producepseudolabels for unlabeled data. Then, at each stage of distillation, a pair ofstudents are trained on disjoint partitions of the pseudolabeled data. Eachstudent subsequently produces new and improved pseudolabels for the unseenpartition to supervise the next round of student(s) with. We show the benefitof multistage cross-partition labeling on two constituency parsing tasks. OnCRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches theperformance of supervised finetuning with 500 examples and outperforms theprompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively.", "title": "multistage collaborative knowledge distillation from large language models", "url": "http://arxiv.org/pdf/2311.08640v1.pdf", "tokenized_text": "study semi supervised sequence prediction tasks labeled_data labeled data effectively finetune time shot large_language large language llm suboptimal performance happens atask parsing expensive annotate unfamiliar apretrained llm paper present discovery student prompted llm generalize better teacher tasks leveraging finding propose_a_new propose new distillation method multistage collaborative knowledge_distillation knowledge distillation llm llm shot context_learning context learning unlabeled data stage distillation pair trained disjoint partitions data subsequently produces new improved supervise round multistage cross partition labeling parsing tasks biomedical parsing stage 50 labeled examples matches theperformance supervised finetuning 500 examples outperforms llm vanilla kd 7.5 3.7 parsing f1 respectively"}
{"id": "nan", "abstract": "  Since the emergence of large language models, prompt learning has become apopular method for optimizing and customizing these models. Special prompts,such as Chain-of-Thought, have even revealed previously unknown reasoningcapabilities within these models. However, the progress of discoveringeffective prompts has been slow, driving a desire for general promptoptimization methods. Unfortunately, few existing prompt learning methodssatisfy the criteria of being truly \"general\", i.e., automatic, discrete,black-box, gradient-free, and interpretable all at once. In this paper, weintroduce metaheuristics, a branch of discrete non-convex optimization methodswith over 100 options, as a promising approach to prompt learning. Within ourparadigm, we test six typical methods: hill climbing, simulated annealing,genetic algorithms with/without crossover, tabu search, and harmony search,demonstrating their effectiveness in black-box prompt learning andChain-of-Thought prompt tuning. Furthermore, we show that these methods can beused to discover more human-understandable prompts that were previouslyunknown, opening the door to a cornucopia of possibilities in promptoptimization. We release all the codes in\\url{https://github.com/research4pan/Plum}.", "title": "plum prompt learning using metaheuristic", "url": "http://arxiv.org/pdf/2311.08364v1.pdf", "tokenized_text": "emergence large_language large language learning method optimizing customizing special chain thought revealed previously unknown progress slow driving general promptoptimization methods unfortunately existing learning criteria truly general i.e. automatic discrete black box gradient free interpretable paper weintroduce branch discrete non optimization 100 options promising approach learning test typical methods simulated genetic algorithms crossover search harmony search demonstrating effectiveness black box learning andchain thought tuning furthermore methods beused discover human understandable opening door possibilities promptoptimization release codes"}
{"id": "nan", "abstract": "  This study examines the effect of prompt engineering on the performance ofLarge Language Models (LLMs) in clinical note generation. We introduce anAutomatic Prompt Optimization (APO) framework to refine initial prompts andcompare the outputs of medical experts, non-medical experts, and APO-enhancedGPT3.5 and GPT4. Results highlight GPT4 APO's superior performance instandardizing prompt quality across clinical note sections. A human-in-the-loopapproach shows that experts maintain content quality post-APO, with apreference for their own modifications, suggesting the value of expertcustomization. We recommend a two-phase optimization process, leveragingAPO-GPT4 for consistency and expert input for personalization.", "title": "do physicians know how to prompt the need for automatic prompt optimization help in clinical note generation", "url": "http://arxiv.org/pdf/2311.09684v1.pdf", "tokenized_text": "study examines effect prompt_engineering engineering performance oflarge language_models language llms clinical note generation introduce prompt_optimization optimization apo framework refine initial andcompare outputs medical experts non medical experts apo gpt4 results highlight gpt4 apo superior_performance superior performance quality clinical note sections human shows experts maintain content quality post apo modifications suggesting value recommend phase optimization process gpt4 consistency expert input personalization"}
{"id": "nan", "abstract": "  Carefully-designed prompts are key to inducing desired behavior in LargeLanguage Models (LLMs). As a result, great effort has been dedicated toengineering prompts that guide LLMs toward particular behaviors. In this work,we propose an automatic prompt optimization framework, PROPANE, which aims tofind a prompt that induces semantically similar outputs to a fixed set ofexamples without user intervention. We further demonstrate that PROPANE can beused to (a) improve existing prompts, and (b) discover semantically obfuscatedprompts that transfer between models.", "title": "propane prompt design as an inverse problem", "url": "http://arxiv.org/pdf/2311.07064v1.pdf", "tokenized_text": "carefully designed key inducing desired behavior largelanguage_models largelanguage llms result great effort dedicated guide llms particular behaviors work propose automatic prompt_optimization optimization framework aims tofind induces semantically similar outputs fixed set ofexamples user intervention demonstrate beused improve existing discover semantically transfer"}
{"id": "nan", "abstract": "  Prompt engineering is a challenging yet crucial task for optimizing theperformance of large language models (LLMs). It requires complex reasoning toexamine the model's errors, hypothesize what is missing or misleading in thecurrent prompt, and communicate the task with clarity. While recent worksindicate that LLMs can be meta-prompted to perform automatic promptengineering, their potentials may not be fully untapped due to the lack ofsufficient guidance to elicit complex reasoning capabilities in LLMs in themeta-prompt. In this work, we investigate the problem of \"prompt engineering aprompt engineer\" -- constructing a meta-prompt that more effectively guidesLLMs to perform automatic prompt engineering. We introduce and analyze keycomponents, such as a step-by-step reasoning template and contextspecification, which lead to improved performance. In addition, inspired bycommon optimization concepts such as batch size, step size and momentum, weintroduce their verbalized counterparts to the meta-prompt and investigatetheir effects. Our final method, named PE2, finds a prompt that outperforms\"let's think step by step\" by 6.3% on the MultiArith dataset and 3.1% on theGSM8K dataset. To demonstrate its versatility, we apply PE2 to the InstructionInduction benchmark, a suite of counterfactual tasks, and a lengthy, real-worldindustrial prompt. In these settings, PE2 achieves strong performance andoutperforms prior automatic prompt engineering baselines. Further, we show thatPE2 makes meaningful and targeted prompt edits, amends erroneous or incompleteprompts, and presents non-trivial counterfactual reasoning abilities.", "title": "prompt engineering a prompt engineer", "url": "http://arxiv.org/pdf/2311.05661v1.pdf", "tokenized_text": "prompt_engineering engineering challenging crucial task optimizing theperformance large_language large language llms requires complex_reasoning complex reasoning errors hypothesize missing misleading thecurrent communicate task clarity recent llms meta prompted perform automatic promptengineering potentials fully untapped lack guidance elicit complex_reasoning complex reasoning capabilities llms themeta work investigate problem prompt_engineering engineering aprompt engineer constructing meta effectively perform automatic prompt_engineering engineering introduce analyze step step reasoning template lead improved performance addition inspired optimization concepts batch size step size weintroduce counterparts meta effects final method named finds think step_by_step step step 6.3 multiarith dataset dataset demonstrate versatility apply instructioninduction benchmark suite counterfactual tasks lengthy real settings achieves strong performance andoutperforms prior automatic prompt_engineering engineering baselines makes meaningful targeted edits erroneous presents non trivial counterfactual reasoning abilities"}
{"id": "nan", "abstract": "  As the use of large language models becomes more widespread, techniques likeparameter-efficient fine-tuning and other methods for controlled generation aregaining traction for customizing models and managing their outputs. However,the challenge of precisely controlling how prompts influence these models is anarea ripe for further investigation. In response, we introduce ControlPE(Continuously Controllable Prompt Engineering). ControlPE enables fineradjustments to prompt effects, complementing existing prompt engineering, andeffectively controls continuous targets. This approach harnesses the power ofLoRA (Low-Rank Adaptation) to create an effect akin to prompt weighting,enabling fine-tuned adjustments to the impact of prompts. Our methodologyinvolves generating specialized datasets for prompt distillation, incorporatingthese prompts into the LoRA model, and carefully adjusting LoRA merging weightto regulate the influence of prompts. This provides a dynamic and adaptabletool for prompt control. Through our experiments, we have validated thepracticality and efficacy of ControlPE. It proves to be a promising solutionfor control a variety of prompts, ranging from generating short responsesprompts, refusal prompts to chain-of-thought prompts.", "title": "to be or not to be an exploration of continuously controllable prompt engineering", "url": "http://arxiv.org/pdf/2311.09773v1.pdf", "tokenized_text": "use large_language large language widespread techniques efficient fine tuning methods controlled generation customizing managing outputs challenge precisely controlling influence ripe investigation response introduce controllable prompt_engineering engineering enables effects existing prompt_engineering engineering controls continuous targets approach harnesses power low rank adaptation create effect akin weighting enabling fine tuned adjustments impact generating specialized datasets distillation lora carefully adjusting lora merging influence provides dynamic control experiments validated efficacy proves promising control variety ranging generating short refusal chain thought"}
{"id": "nan", "abstract": "  While most existing works on LLM prompt-engineering focus only on how toselect a better set of data samples inside one single prompt input (In-ContextLearning or ICL), why can't we design and leverage multiple prompt inputstogether to further improve the LLM performance? In this work, we proposeIn-Context Sampling (ICS), a low-resource LLM prompt-engineering technique toproduce the most confident prediction results by optimizing the construction ofmultiple ICL prompt inputs. Extensive experiments with two SOTA LLMs (FlanT5-XLand Mistral-7B) on three NLI datasets (e-SNLI, Multi-NLI, and ANLI) illustratethat ICS can consistently enhance LLM's prediction performance and confidence.An ablation study suggests that a diversity-based ICS strategy may furtherimprove LLM's performance, which sheds light on a new yet promising futureresearch direction.", "title": "more samples or more prompt inputs exploring effective incontext sampling for llm fewshot prompt engineering", "url": "http://arxiv.org/pdf/2311.09782v1.pdf", "tokenized_text": "existing works llm engineering focus toselect better set data samples inside single input contextlearning icl design leverage multiple improve llm performance work context sampling low resource llm engineering technique toproduce confident prediction results optimizing construction ofmultiple icl inputs extensive_experiments extensive experiments sota llms nli datasets snli multi nli consistently enhance llm prediction performance confidence ablation study suggests diversity based strategy furtherimprove llm performance sheds light new promising futureresearch direction"}
{"id": "nan", "abstract": "  This paper reports on the use of prompt engineering and GPT-3.5 forbiomedical query-focused multi-document summarisation. Using GPT-3.5 andappropriate prompts, our system achieves top ROUGE-F1 results in the task ofobtaining short-paragraph-sized answers to biomedical questions in the 2023BioASQ Challenge (BioASQ 11b). This paper confirms what has been observed inother domains: 1) Prompts that incorporated few-shot samples generally improvedon their counterpart zero-shot variants; 2) The largest improvement wasachieved by retrieval augmented generation. The fact that these prompts allowour top runs to rank within the top two runs of BioASQ 11b demonstrate thepower of using adequate prompts for Large Language Models in general, andGPT-3.5 in particular, for query-focused summarisation.", "title": "large language models and prompt engineering for biomedical query focused multidocument summarisation", "url": "http://arxiv.org/pdf/2311.05169v1.pdf", "tokenized_text": "paper reports use prompt_engineering engineering gpt-3.5 query focused multi document summarisation gpt-3.5 system achieves rouge f1 results task short paragraph sized answers biomedical questions challenge 11b paper confirms observed domains incorporated shot samples generally counterpart zero shot variants largest improvement retrieval_augmented retrieval augmented generation fact runs rank runs 11b demonstrate thepower adequate large_language large language general particular query focused summarisation"}
{"id": "nan", "abstract": "  Recently, diffusion-based deep generative models (e.g., Stable Diffusion)have shown impressive results in text-to-image synthesis. However, currenttext-to-image models often require multiple passes of prompt engineering byhumans in order to produce satisfactory results for real-world applications. Wepropose BeautifulPrompt, a deep generative model to produce high-qualityprompts from very simple raw descriptions, which enables diffusion-based modelsto generate more beautiful images. In our work, we first fine-tuned theBeautifulPrompt model over low-quality and high-quality collecting promptpairs. Then, to ensure that our generated prompts can generate more beautifulimages, we further propose a Reinforcement Learning with Visual AI Feedbacktechnique to fine-tune our model to maximize the reward values of the generatedprompts, where the reward values are calculated based on the PickScore and theAesthetic Scores. Our results demonstrate that learning from visual AI feedbackpromises the potential to improve the quality of generated prompts and imagessignificantly. We further showcase the integration of BeautifulPrompt to acloud-native AI platform to provide better text-to-image generation service inthe cloud.", "title": "beautifulprompt towards automatic prompt engineering for texttoimage synthesis", "url": "http://arxiv.org/pdf/2311.06752v1.pdf", "tokenized_text": "recently diffusion based deep generative e.g. stable shown_impressive shown impressive results text image synthesis image require multiple passes prompt_engineering engineering order produce satisfactory results real world_applications world applications wepropose deep generative produce high simple raw descriptions enables diffusion based modelsto generate beautiful images work fine tuned low quality high quality collecting ensure generated generate propose reinforcement_learning reinforcement learning visual ai fine tune maximize reward values reward values based scores results_demonstrate results demonstrate learning visual ai potential improve quality generated showcase integration native ai platform provide better text image_generation image generation service inthe cloud"}
{"id": "nan", "abstract": "  Two ways has been discussed to unlock the reasoning capability of a largelanguage model. The first one is prompt engineering and the second one is tocombine the multiple inferences of large language models, or the multi-agentdiscussion. Theoretically, this paper justifies the multi-agent discussionmechanisms from the symmetry of agents. Empirically, this paper reports theempirical results of the interplay of prompts and discussion mechanisms,revealing the empirical state-of-the-art performance of complex multi-agentmechanisms can be approached by carefully developed prompt engineering. Thispaper also proposes a scalable discussion mechanism based on conquer and merge,providing a simple multi-agent discussion solution with simple prompts butstate-of-the-art performance.", "title": "on the discussion of large language models symmetry of agents and interplay with prompts", "url": "http://arxiv.org/pdf/2311.07076v1.pdf", "tokenized_text": "ways discussed unlock reasoning capability largelanguage prompt_engineering engineering second multiple inferences large_language large language multi theoretically paper multi agent symmetry agents empirically paper reports theempirical results interplay discussion mechanisms revealing empirical state art performance complex multi approached carefully developed prompt_engineering engineering thispaper proposes scalable discussion mechanism based conquer merge providing simple multi agent discussion solution simple art performance"}
{"id": "nan", "abstract": "  While the potential of Open Information Extraction (Open IE) for KnowledgeGraph Construction (KGC) may seem promising, we find that the alignment of OpenIE extraction results with existing knowledge graphs to be inadequate. Theadvent of Large Language Models (LLMs), especially the commercially availableOpenAI models, have reset expectations for what is possible with deep learningmodels and have created a new field called prompt engineering. We investigatethe use of GPT models and prompt engineering for knowledge graph constructionwith the Wikidata knowledge graph to address a similar problem to Open IE,which we call Open Knowledge Extraction (OKE) using an approach we call theLinked Open Knowledge Extractor (LOKE, pronounced like \"Loki\"). We consider theentity linking task essential to construction of real world knowledge graphs.We merge the CaRB benchmark scoring approach with data from the TekGen datasetfor the LOKE task. We then show that a well engineered prompt, paired with anaive entity linking approach (which we call LOKE-GPT), outperforms AllenAI'sOpenIE 4 implementation on the OKE task, although it over-generates triplescompared to the reference set due to overall triple scarcity in the TekGen set.Through an analysis of entity linkability in the CaRB dataset, as well asoutputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the \"silver\"TekGen triples show that the task is significantly different in content fromOIE, if not structure. Through this analysis and a qualitative analysis ofsentence extractions via all methods, we found that LOKE-GPT extractions are ofhigh utility for the KGC task and suitable for use in semi-automated extractionsettings.", "title": "loke linked open knowledge extraction for automated knowledge graph construction", "url": "http://arxiv.org/pdf/2311.09366v1.pdf", "tokenized_text": "potential open information_extraction information extraction open ie knowledgegraph construction promising find alignment extraction results existing knowledge graphs inadequate theadvent large_language large language llms especially commercially reset expectations possible deep created new field called prompt_engineering engineering investigatethe use gpt prompt_engineering engineering knowledge_graph knowledge graph wikidata knowledge_graph knowledge graph address similar problem open ie open knowledge extraction approach open knowledge extractor pronounced like consider theentity linking task essential construction real_world real world knowledge graphs merge carb benchmark scoring approach data task engineered paired entity linking approach gpt outperforms implementation task generates reference set overall triple scarcity set analysis entity carb dataset gpt gpt triples task significantly different content structure analysis qualitative analysis ofsentence extractions methods found gpt extractions ofhigh utility task suitable use semi automated"}
{"id": "nan", "abstract": "  Instructor's feedback plays a critical role in students' development ofconceptual understanding and reasoning skills. However, grading student writtenresponses and providing personalized feedback can take a substantial amount oftime. In this study, we explore using GPT-3.5 to write feedback to studentwritten responses to conceptual questions with prompt engineering and few-shotlearning techniques. In stage one, we used a small portion (n=20) of thestudent responses on one conceptual question to iteratively train GPT. Four ofthe responses paired with human-written feedback were included in the prompt asexamples for GPT. We tasked GPT to generate feedback to the other 16 responses,and we refined the prompt after several iterations. In stage two, we gave fourstudent researchers the 16 responses as well as two versions of feedback, onewritten by the authors and the other by GPT. Students were asked to rate thecorrectness and usefulness of each feedback, and to indicate which one wasgenerated by GPT. The results showed that students tended to rate the feedbackby human and GPT equally on correctness, but they all rated the feedback by GPTas more useful. Additionally, the successful rates of identifying GPT'sfeedback were low, ranging from 0.1 to 0.6. In stage three, we tasked GPT togenerate feedback to the rest of the student responses (n=65). The feedback wasrated by four instructors based on the extent of modification needed if theywere to give the feedback to students. All the instructors rated approximately70% of the feedback statements needing only minor or no modification. Thisstudy demonstrated the feasibility of using Generative AI as an assistant togenerating feedback for student written responses with only a relatively smallnumber of examples. An AI assistance can be one of the solutions tosubstantially reduce time spent on grading student written responses.", "title": "exploring generative ai assisted feedback writing for students' written responses to a physics conceptual question with prompt engineering and fewshot learning", "url": "http://arxiv.org/pdf/2311.06180v1.pdf", "tokenized_text": "instructor feedback plays critical role students development understanding reasoning skills grading student providing personalized feedback substantial study explore gpt-3.5 write feedback responses conceptual questions prompt_engineering engineering shotlearning techniques stage small portion responses conceptual question iteratively train gpt ofthe responses paired human written feedback included gpt tasked gpt generate feedback 16 responses refined iterations stage gave researchers 16 responses versions feedback authors gpt students asked rate usefulness feedback indicate wasgenerated gpt results showed students rate human gpt equally correctness rated feedback useful additionally successful rates identifying low ranging 0.1 0.6 stage tasked gpt togenerate feedback rest student responses feedback instructors based extent modification needed theywere feedback students instructors rated feedback statements needing minor modification thisstudy demonstrated feasibility generative_ai generative ai assistant togenerating feedback student written responses relatively smallnumber examples ai assistance solutions reduce time spent grading student written responses"}
{"id": "nan", "abstract": "  In-context learning (ICL) has become one of the most popular learningparadigms. While there is a growing body of literature focusing on promptengineering, there is a lack of systematic analysis comparing the effects ofprompts across different models and tasks. To address this gap, we present acomprehensive prompt analysis based on the sensitivity of a function. Ouranalysis reveals that sensitivity is an unsupervised proxy for modelperformance, as it exhibits a strong negative correlation with accuracy. We usegradient-based saliency scores to empirically demonstrate how different promptsaffect the relevance of input tokens to the output, resulting in differentlevels of sensitivity. Furthermore, we introduce sensitivity-aware decodingwhich incorporates sensitivity estimation as a penalty term in the standardgreedy decoding. We show that this approach is particularly helpful wheninformation in the input is scarce. Our work provides a fresh perspective onthe analysis of prompts, and contributes to a better understanding of themechanism of ICL.", "title": "how are prompts different in terms of sensitivity", "url": "http://arxiv.org/pdf/2311.07230v1.pdf", "tokenized_text": "context_learning context learning icl popular growing body literature focusing promptengineering lack systematic analysis comparing effects ofprompts different tasks address gap present acomprehensive analysis based sensitivity function ouranalysis reveals sensitivity unsupervised proxy modelperformance exhibits strong negative correlation accuracy based saliency scores empirically demonstrate different relevance input tokens output resulting sensitivity furthermore introduce sensitivity aware incorporates sensitivity estimation term decoding approach particularly helpful input scarce work provides fresh perspective onthe analysis contributes better understanding icl"}
{"id": "nan", "abstract": "  The emergence of large language models (LLMs) further improves thecapabilities of open-domain dialogue systems and can generate fluent, coherent,and diverse responses. However, LLMs still lack an important ability:communication skills, which makes them more like information seeking tools thananthropomorphic chatbots. To make LLMs more anthropomorphic and proactiveduring the conversation, we add five communication skills to the responsegeneration process: topic transition, proactively asking questions, conceptguidance, empathy, and summarising often. The addition of communication skillsincreases the interest of users in the conversation and attracts them to chatfor longer. To enable LLMs better understand and use communication skills, wedesign and add the inner monologue to LLMs. The complete process is achievedthrough prompt engineering and in-context learning. To evaluate communicationskills, we construct a benchmark named Cskills for evaluating variouscommunication skills, which can also more comprehensively evaluate the dialoguegeneration ability of the model. Experimental results show that the proposedCSIM strategy improves the backbone models and outperforms the baselines inboth automatic and human evaluations.", "title": "think before you speak cultivating communication skills of large language models via inner monologue", "url": "http://arxiv.org/pdf/2311.07445v1.pdf", "tokenized_text": "emergence large_language large language llms improves thecapabilities open domain dialogue systems generate fluent coherent diverse responses llms lack important ability communication skills makes like information seeking tools chatbots llms conversation add communication skills process topic transition proactively asking questions empathy addition communication interest users conversation longer enable llms better understand use communication skills wedesign add inner llms complete process prompt_engineering engineering context_learning context learning evaluate construct benchmark named evaluating skills comprehensively evaluate ability experimental_results experimental results strategy improves backbone outperforms baselines inboth automatic human evaluations"}
{"id": "nan", "abstract": "  Interactive segmentation model leverages prompts from users to produce robustsegmentation. This advancement is facilitated by prompt engineering, whereinteractive prompts serve as strong priors during test-time. However, this isan inherently subjective and hard-to-reproduce process. The variability in userexpertise and inherently ambiguous boundaries in medical images can lead toinconsistent prompt selections, potentially affecting segmentation accuracy.This issue has not yet been extensively explored for medical imaging. In thispaper, we assess the test-time variability for interactive medical imagesegmentation with diverse point prompts. For a given target region, the pointis classified into three sub-regions: boundary, margin, and center. Our goal isto identify a straightforward and efficient approach for optimal promptselection during test-time based on three considerations: (1) benefits ofadditional prompts, (2) effects of prompt placement, and (3) strategies foroptimal prompt selection. We conduct extensive experiments on the publicMedical Segmentation Decathlon dataset for challenging colon tumor segmentationtask. We suggest an optimal strategy for prompt selection during test-time,supported by comprehensive results. The code is publicly available athttps://github.com/MedICL-VU/variability", "title": "assessing testtime variability for interactive 3d medical image segmentation with diverse point prompts", "url": "http://arxiv.org/pdf/2311.07806v1.pdf", "tokenized_text": "interactive segmentation leverages users produce advancement facilitated prompt_engineering engineering serve strong priors test time isan inherently subjective hard reproduce process variability inherently ambiguous boundaries medical images lead selections potentially affecting segmentation accuracy issue extensively explored medical imaging thispaper assess test time variability interactive medical imagesegmentation diverse point given target region classified sub regions boundary margin center goal identify straightforward efficient approach optimal test time based considerations benefits effects strategies selection conduct_extensive conduct extensive experiments segmentation dataset challenging colon suggest optimal strategy selection test time supported comprehensive results code publicly_available publicly available"}
{"id": "nan", "abstract": "  In the rapidly evolving landscape of human-computer interaction, theintegration of vision capabilities into conversational agents stands as acrucial advancement. This paper presents an initial implementation of adialogue manager that leverages the latest progress in Large Language Models(e.g., GPT-4, IDEFICS) to enhance the traditional text-based prompts withreal-time visual input. LLMs are used to interpret both textual prompts andvisual stimuli, creating a more contextually aware conversational agent. Thesystem's prompt engineering, incorporating dialogue with summarisation of theimages, ensures a balance between context preservation and computationalefficiency. Six interactions with a Furhat robot powered by this system arereported, illustrating and discussing the results obtained. By implementingthis vision-enabled dialogue system, the paper envisions a future whereconversational agents seamlessly blend textual and visual modalities, enablingricher, more context-aware dialogues.", "title": "i was blind but now i see implementing visionenabled dialogue in social robots", "url": "http://arxiv.org/pdf/2311.08957v1.pdf", "tokenized_text": "rapidly evolving landscape human computer interaction theintegration vision capabilities conversational agents stands acrucial advancement paper_presents paper presents initial implementation adialogue manager leverages latest progress large_language large language models(e.g gpt-4 idefics enhance traditional text based withreal time visual input llms interpret textual andvisual stimuli creating contextually aware conversational agent thesystem prompt_engineering engineering incorporating dialogue summarisation ensures balance context preservation interactions furhat robot powered system illustrating discussing results obtained vision enabled dialogue system paper future agents seamlessly blend textual visual modalities context aware dialogues"}
{"id": "nan", "abstract": "  Accurately simulating human opinion dynamics is crucial for understanding avariety of societal phenomena, including polarization and the spread ofmisinformation. However, the agent-based models (ABMs) commonly used for suchsimulations lack fidelity to human behavior. We propose a new approach tosimulating opinion dynamics based on populations of Large Language Models(LLMs). Our findings reveal a strong inherent bias in LLM agents towardsaccurate information, leading to consensus in line with scientific reality.However, this bias limits the simulation of individuals with resistant views onissues like climate change. After inducing confirmation bias through promptengineering, we observed opinion fragmentation in line with existingagent-based research. These insights highlight the promise and limitations ofLLM agents in this domain and suggest a path forward: refining LLMs withreal-world discourse to better simulate the evolution of human beliefs.", "title": "simulating opinion dynamics with networks of llmbased agents", "url": "http://arxiv.org/pdf/2311.09618v1.pdf", "tokenized_text": "accurately simulating human opinion dynamics crucial understanding avariety societal phenomena including spread agent based commonly lack fidelity human behavior propose_a_new propose new approach opinion dynamics based populations large_language large language models(llms findings reveal strong inherent bias llm agents information leading consensus line scientific reality bias limits simulation individuals resistant views like climate change inducing bias promptengineering observed opinion line based research insights highlight promise limitations ofllm agents domain suggest path forward refining llms withreal world discourse better simulate evolution human beliefs"}
{"id": "nan", "abstract": "  AI models (including LLM) often rely on narrative question-answering (QA)datasets to provide customized QA functionalities to support downstreamchildren education applications; however, existing datasets only include QApairs that are grounded within the given storybook content, but children canlearn more when teachers refer the storybook content to real-world knowledge(e.g., commonsense knowledge). We introduce the FairytaleCQA dataset, which isannotated by children education experts, to supplement 278 storybook narrativeswith educationally appropriate commonsense knowledge. The dataset has 5,868 QApairs that not only originate from the storybook narrative but also contain thecommonsense knowledge grounded by an external knowledge graph (i.e.,ConceptNet). A follow-up experiment shows that a smaller model (T5-large)fine-tuned with FairytaleCQA reliably outperforms much larger prompt-engineeredLLM (e.g., GPT-4) in this new QA-pair generation task (QAG). This resultsuggests that: 1) our dataset brings novel challenges to existing LLMs, and 2)human experts' data annotation are still critical as they have much nuancedknowledge that LLMs do not know in the children educational domain.", "title": "fairytalecqa integrating a commonsense knowledge graph into children's storybook narratives", "url": "http://arxiv.org/pdf/2311.09756v1.pdf", "tokenized_text": "ai including llm rely narrative question answering provide customized qa functionalities support education applications existing datasets include grounded given content children teachers refer content real world commonsense knowledge introduce dataset children education experts supplement appropriate commonsense knowledge dataset originate narrative contain knowledge grounded external_knowledge external knowledge graph i.e. follow experiment shows smaller t5 tuned reliably outperforms larger e.g. gpt-4 new qa pair generation task dataset brings novel challenges existing llms experts data annotation critical llms know children educational domain"}