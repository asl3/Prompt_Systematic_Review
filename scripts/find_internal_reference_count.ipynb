{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install networkx matplotlib pandas pytest-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('SEMANTIC_SCHOLAR_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_limited_request(url, headers, max_retries=3, delay=0.1):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            time.sleep(delay)\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 429 or response.status_code == 504:\n",
    "                print(f\"Received status code {response.status_code}. Retrying...\")\n",
    "                delay *= 2  # Exponential backoff\n",
    "                retries += 1\n",
    "            else:\n",
    "                print(f\"HTTPError: {e}\")\n",
    "                break  # For other HTTP errors, don't retry\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            break  # For non-HTTP errors, don't retry\n",
    "    return None\n",
    "\n",
    "\n",
    "    \n",
    "# Function to get references from semantic scholar Id\n",
    "def get_references(paper_id, api_key):\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/{paper_id}/references?fields=title,authors&limit=1000\"\n",
    "    headers = {\"x-api-key\": api_key}\n",
    "    response = rate_limited_request(url, headers)\n",
    "\n",
    "    if response and response.status_code == 200:\n",
    "        data = response.json()\n",
    "        reference_ids = []\n",
    "\n",
    "        if 'data' in data:\n",
    "            for ref in data['data']:\n",
    "                if 'citedPaper' in ref and 'paperId' in ref['citedPaper']:\n",
    "                    reference_ids.append(ref['citedPaper']['paperId'])\n",
    "\n",
    "        return reference_ids\n",
    "    return []\n",
    "\n",
    "# Function to query paper title from Arxiv Id\n",
    "def get_arxiv_paper_title(arxiv_id):\n",
    "    arxiv_url = f'http://export.arxiv.org/api/query?id_list={arxiv_id}'\n",
    "    response = requests.get(arxiv_url)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    start = response.text.find('<title>') + 7\n",
    "    end = response.text.find('</title>', start)\n",
    "    title = response.text[start:end].strip()\n",
    "    return title\n",
    "\n",
    "# Function to query Semantic Scholar for a paper ID using title\n",
    "def query_paper_id(title, api_key):\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={title}&limit=1\"\n",
    "    headers = {\"x-api-key\": api_key}\n",
    "    response = rate_limited_request(url, headers, delay=1, max_retries=4)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if 'data' in data and data['data']:\n",
    "            return data['data'][0]['paperId']\n",
    "    return None\n",
    "\n",
    "# Function to convert arXiv ID into Semantic Scholar ID\n",
    "def convert_arxiv_id_to_semantic_scholar_id(arxiv_id, api_key):\n",
    "    title = get_arxiv_paper_title(arxiv_id)\n",
    "    if title:\n",
    "        return query_paper_id(title, api_key)\n",
    "    return None\n",
    "\n",
    "# Function to get the title for a given paper ID\n",
    "def get_paper_title(paper_id, api_key):\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/{paper_id}\"\n",
    "    headers = {\"x-api-key\": api_key}\n",
    "    response = rate_limited_request(url, headers)\n",
    "\n",
    "    if response and response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data.get('title')\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for paper ID: {paper_id}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "# Path to the CSV file containing the papers' data\n",
    "csv_file_path = 'master_papers.csv'\n",
    "\n",
    "# Dictionary to hold the references\n",
    "paper_references = {}\n",
    "unmatched_papers = {}\n",
    "\n",
    "# Function to extract Arxiv ID from the URL\n",
    "def extract_arxiv_id(url):\n",
    "    # Extract the part of the URL after 'pdf/' and before '.pdf'\n",
    "    base = url.rsplit('/', 1)[-1]\n",
    "    arxiv_id_with_version = base.split('.pdf')[0]\n",
    "    # Remove the version part if it exists (e.g., 'v1')\n",
    "    arxiv_id = arxiv_id_with_version.split('v')[0] if 'v' in arxiv_id_with_version else arxiv_id_with_version\n",
    "    return arxiv_id\n",
    "\n",
    "if os.path.exists(csv_file_path):\n",
    "    with open(csv_file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        # Let csv.DictReader read the headers directly from the file\n",
    "        csv_reader = csv.DictReader(csvfile, delimiter=',')\n",
    "\n",
    "        # Convert the csv_reader to a list to allow for random sampling\n",
    "        all_papers = list(csv_reader)\n",
    "        # random_papers = random.sample(all_papers, min(20, len(all_papers)))  # Safely sample 20 papers or the total count if less\n",
    "        for row in tqdm(all_papers, desc=\"Processing Papers\"):\n",
    "            source = row.get('source', '').strip()  # Use .get() to avoid KeyError and strip() to remove any extra whitespace\n",
    "            if source == 'Semantic Scholar':\n",
    "                paper_id = row.get('paperId', '').strip()  # Use .get() here as well\n",
    "            elif source == 'arXiv':\n",
    "                arxiv_paper_id = extract_arxiv_id(row.get('url', '').strip())\n",
    "                paper_id = convert_arxiv_id_to_semantic_scholar_id(arxiv_paper_id, api_key=api_key)\n",
    "            else:\n",
    "                unmatched_papers[row.get('title', '').strip()] = \"Source not supported\"\n",
    "                continue\n",
    "            # print(paper_id)\n",
    "            if paper_id:\n",
    "                references = get_references(paper_id, api_key=api_key)\n",
    "                if references is not None:\n",
    "                    paper_references[paper_id] = references\n",
    "                else:\n",
    "                    unmatched_papers[row['title']] = \"No references found or error occurred\"\n",
    "            else:\n",
    "                print(f\"Paper Id Could not be found for: {row}\")\n",
    "\n",
    "else:\n",
    "    print(f\"CSV file does not exist: {csv_file_path}\")\n",
    "\n",
    "# Save the results to JSON files\n",
    "with open('revised_paper_references.json', 'w') as json_file:\n",
    "    json.dump(paper_references, json_file, indent=4)\n",
    "print(\"Data saved to revised_paper_references.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second main to add important papers not in our original dataset\n",
    "\n",
    "\n",
    "paper_references = {}\n",
    "unmatched_titles = {}\n",
    "\n",
    "titles = [\"Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints\",\n",
    "\"Language Models are Few-Shot Learners\",\n",
    "\"A Survey on In-context Learning\",\n",
    "\"What Makes Good In-Context Examples for GPT-3?\",\n",
    "\"Finding Support Examples for In-Context Learning\",\n",
    "\"Unified Demonstration Retriever for In-Context Learning\",\n",
    "\"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity\",\n",
    "\"Reordering Examples Helps during Priming-based Few-Shot Learning\",\n",
    "\"Learning To Retrieve Prompts for In-Context Learning\",\n",
    "\"Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator\",\n",
    "\"Large Language Models are Zero-Shot Reasoners\",\n",
    "\"Large Language Models Are Human-Level Prompt Engineers\",\n",
    "\"Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models\",\n",
    "\"Thread of Thought Unraveling Chaotic Contexts\",\n",
    "\"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment\",\n",
    "\"Automatic Chain of Thought Prompting in Large Language Models\",\n",
    "\"True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4\",\n",
    "\"Contrastive Chain-of-Thought Prompting\",\n",
    "\"Gemini: A Family of Highly Capable Multimodal Models\",\n",
    "\"Complexity-Based Prompting for Multi-Step Reasoning\",\n",
    "\"Active Prompting with Chain-of-Thought for Large Language Models\",\n",
    "\"MoT: Memory-of-Thought Enables ChatGPT to Self-Improve\",\n",
    "\"Measuring and Narrowing the Compositionality Gap in Language Models\",\n",
    "\"Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data\",\n",
    "\"Tab-CoT: Zero-shot Tabular Chain of Thought\",\n",
    "\"Is a Question Decomposition Unit All We Need?\",\n",
    "\"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\",\n",
    "\"Decomposed Prompting: A Modular Approach for Solving Complex Tasks\",\n",
    "\"Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models\",\n",
    "\"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\",\n",
    "\"Large Language Model Guided Tree-of-Thought\",\n",
    "\"Cumulative Reasoning with Large Language Models\",\n",
    "\"Graph of thoughts: Solving elaborate problems with large language models\",\n",
    "\"Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models\",\n",
    "\"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\",\n",
    "\"Faithful Chain-of-Thought Reasoning\",\n",
    "\"Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding\",\n",
    "\"Exploring Demonstration Ensembling for In-context Learning\",\n",
    "\"$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference\",\n",
    "\"An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels\",\n",
    "\"Self-Consistency Improves Chain of Thought Reasoning in Language Models\",\n",
    "\"Universal Self-Consistency for Large Language Model Generation\",\n",
    "\"Making Language Models Better Reasoners with Step-Aware Verifier\",\n",
    "\"Language Models (Mostly) Know What They Know\",\n",
    "\"Self-Refine: Iterative Refinement with Self-Feedback\",\n",
    "\"RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought\",\n",
    "\"Large Language Models are Better Reasoners with Self-Verification\",\n",
    "\"Deductive Verification of Chain-of-Thought Reasoning\",\n",
    "\"Chain-of-Verification Reduces Hallucination in Large Language Models\",\n",
    "\"Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations\",\n",
    "\"Large Language Models Understand and Can be Enhanced by Emotional Stimuli\",\n",
    "\"Re-Reading Improves Reasoning in Language Models\",\n",
    "\"Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities\",\n",
    "\"Better Zero-Shot Reasoning with Self-Adaptive Prompting\",\n",
    "\"Universal Self-Adaptive Prompting\",\n",
    "\"System 2 Attention (is something you might need too)\",\n",
    "\"Large Language Models as Optimizers\",\n",
    "\"Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves\"]\n",
    "\n",
    "# Processing the papers with a progress bar\n",
    "for title in tqdm(titles):\n",
    "    paper_id, references = get_references(title, api_key)\n",
    "    if paper_id:\n",
    "        paper_references[paper_id] = references\n",
    "    else:\n",
    "        unmatched_titles[title] = \"No matching paper ID found\"\n",
    "\n",
    "# Save the results to JSON files\n",
    "with open('paper_references_additional.json', 'w') as json_file:\n",
    "    json.dump(paper_references, json_file, indent=4)\n",
    "print(\"Data saved to paper_references_additional.json\")\n",
    "\n",
    "with open('unmatched_titles_additional.json', 'w') as json_file:\n",
    "    json.dump(unmatched_titles, json_file, indent=4)\n",
    "print(\"Data saved to unmatched_titles_additional.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the two files\n",
    "\n",
    "# Load the existing data from both JSON files\n",
    "with open('revised_paper_references.json', 'r') as file:\n",
    "    paper_references = json.load(file)\n",
    "\n",
    "with open('paper_references_additional.json', 'r') as file:\n",
    "    paper_references_additional = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "# Merge the two dictionaries\n",
    "# If there are duplicate keys, the values from paper_references_additional will be used\n",
    "paper_references.update(paper_references_additional)\n",
    "\n",
    "# Save the merged data back to a JSON file\n",
    "with open('complete_paper_references.json', 'w') as file:\n",
    "    json.dump(paper_references, file, indent=4)\n",
    "\n",
    "print(\"Merged data saved to complete_paper_references.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only keep refernces which refer to papers in our combined dataset\n",
    "\n",
    "# Load the merged paper references\n",
    "with open('complete_paper_references.json', 'r') as file:\n",
    "    merged_paper_references = json.load(file)\n",
    "\n",
    "# Filter the references so that only those that are keys in the dictionary are kept\n",
    "for paper_id, references in merged_paper_references.items():\n",
    "    merged_paper_references[paper_id] = [ref for ref in references if ref in merged_paper_references]\n",
    "\n",
    "# Save the cleaned data back to a JSON file\n",
    "with open('cleaned_complete_paper_references.json', 'w') as file:\n",
    "    json.dump(merged_paper_references, file, indent=4)\n",
    "\n",
    "print(\"Cleaned data saved to cleaned_merged_paper_references.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of the top 30 most cited papers by internal citation count\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import matplotlib\n",
    "\n",
    "# Set font properties globally\n",
    "matplotlib.rcParams.update({\n",
    "    'font.family': 'Arial',  # You can replace 'Arial' with 'Helvetica' or another modern font\n",
    "    'font.size': 8          # You can adjust this value as needed\n",
    "})\n",
    "\n",
    "# Load the data\n",
    "with open('cleaned_complete_paper_references.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count the number of references for each key\n",
    "reference_counts = {key: len(references) for key, references in data.items()}\n",
    "\n",
    "# Sort by the number of references and select the top 50\n",
    "top_25 = sorted(reference_counts.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "\n",
    "# Fetch the titles for the top 50 paper IDs\n",
    "top_25_titles = {paper_id: get_paper_title(paper_id, api_key) for paper_id, _ in top_25}\n",
    "\n",
    "# Replace paper IDs with their titles in the top_50 list\n",
    "top_25_with_titles = [(top_25_titles[paper_id], count) for paper_id, count in top_25]\n",
    "\n",
    "# Unpack the data for plotting\n",
    "titles, counts = zip(*top_25_with_titles)\n",
    "\n",
    "# Define the RGBA color\n",
    "rgba_color = (45/255, 137/255, 145/255, 1)  # Converted from rgba(45, 137, 145, 1)\n",
    "\n",
    "# Create a vertical bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(titles, counts, color=rgba_color)\n",
    "\n",
    "# Rotate the x-axis labels by 45 degrees for better readability\n",
    "plt.xticks(rotation=45, ha='right')  # ha='right' aligns the labels at the right angle\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('Number of References')\n",
    "plt.xlabel('Paper Title')\n",
    "plt.title('Top 30 Papers by Number of References')\n",
    "\n",
    "# plt.tight_layout()  # Adjusts layout to prevent clipping of labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a graph of all the papers with more than 10 internal references \n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "# Load the cleaned references\n",
    "with open('cleaned_complete_paper_references.json', 'r') as json_file:\n",
    "    paper_references = json.load(json_file)\n",
    "\n",
    "# Create the graph\n",
    "G = nx.DiGraph()\n",
    "for paper_id, references in paper_references.items():\n",
    "    for ref_id in references:\n",
    "        G.add_edge(paper_id, ref_id)\n",
    "\n",
    "# Remove isolated nodes and nodes with less than 10 incoming edges\n",
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "nodes_to_remove = [node for node in G.nodes() if G.in_degree(node) < 10]\n",
    "G.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "# Find the top 20 nodes with the most incoming edges\n",
    "top_nodes = sorted(G.nodes(), key=lambda n: G.in_degree(n), reverse=True)[:20]\n",
    "\n",
    "titles_above_threshold = {}\n",
    "for paper_id in top_nodes:\n",
    "    title = get_paper_title(paper_id, api_key)\n",
    "    if title:\n",
    "        if len(title) > 50:\n",
    "            title = textwrap.shorten(title, width=50, placeholder=\"...\")  # Shorten if longer than 60s\n",
    "        titles_above_threshold[paper_id] = title\n",
    "\n",
    "# Cap the maximum node size to prevent too large nodes\n",
    "# max_size = 100000  # Maximum size for a node\n",
    "node_sizes = [G.in_degree(node) * 2000 for node in G.nodes()]\n",
    "\n",
    "# Draw the graph with adjusted layout parameters\n",
    "plt.figure(figsize=(60, 35))  # Increased figure size for more space\n",
    "pos = nx.kamada_kawai_layout(G, dist=None, scale=1)  # Adjust 'scale' as needed\n",
    "nx.draw(G, pos, with_labels=False, node_size=node_sizes, node_color='skyblue', edge_color='gray', width=0.5)\n",
    "\n",
    "# Assign and label top nodes with titles\n",
    "for node, label in titles_above_threshold.items():\n",
    "    x, y = pos[node]\n",
    "    wrapped_label = label.split('\\n')\n",
    "    for i, line in enumerate(wrapped_label):\n",
    "        plt.text(x, y, line, fontsize=18, ha='center', va='center')\n",
    "\n",
    "plt.title(\"Graph of Paper References\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "def adjust_overlap(pos, nodes_to_adjust, min_dist=0.1, repulsion_factor=1.05):\n",
    "    for _ in range(1000):  # Increase the number of iterations for a denser graph\n",
    "        adjusted = False\n",
    "        for i, node1 in enumerate(nodes_to_adjust):\n",
    "            for node2 in nodes_to_adjust[i+1:]:\n",
    "                x1, y1 = pos[node1]\n",
    "                x2, y2 = pos[node2]\n",
    "                dx, dy = x1 - x2, y1 - y2\n",
    "                dist = (dx**2 + dy**2)**0.5\n",
    "                if dist < min_dist:  # If nodes are too close, push them apart\n",
    "                    # Apply a repulsion factor to move nodes further apart\n",
    "                    if dist == 0:  # To avoid division by zero\n",
    "                        dx, dy = 1, 0\n",
    "                        dist = 1\n",
    "                    dx, dy = dx / dist * min_dist * repulsion_factor, dy / dist * min_dist * repulsion_factor\n",
    "                    pos[node1] = (x1 + dx, y1 + dy)\n",
    "                    pos[node2] = (x2 - dx, y2 - dy)\n",
    "                    adjusted = True\n",
    "        if not adjusted:  # Break the loop if no adjustments were made\n",
    "            break\n",
    "\n",
    "# Load the cleaned references\n",
    "with open('cleaned_complete_paper_references.json', 'r') as json_file:\n",
    "    paper_references = json.load(json_file)\n",
    "\n",
    "# Create the graph\n",
    "G = nx.DiGraph()\n",
    "for paper_id, references in paper_references.items():\n",
    "    for ref_id in references:\n",
    "        G.add_edge(paper_id, ref_id)\n",
    "\n",
    "# Remove isolated nodes and nodes with less than 10 incoming edges\n",
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "nodes_to_remove = [node for node in G.nodes() if G.in_degree(node) < 8]\n",
    "G.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "# Find the top 20 nodes with the most incoming edges\n",
    "top_nodes = sorted(G.nodes(), key=lambda n: G.in_degree(n), reverse=True)[:25]\n",
    "\n",
    "# Define a function to wrap text into at most two lines\n",
    "def wrap_text(text, width, max_lines=2):\n",
    "    wrapped_lines = textwrap.wrap(text, width)\n",
    "    if len(wrapped_lines) > max_lines:\n",
    "        wrapped_lines = wrapped_lines[:max_lines]\n",
    "        wrapped_lines[-1] += '...'  # Add ellipsis to the last line to indicate truncation\n",
    "    return '\\n'.join(wrapped_lines)\n",
    "\n",
    "titles_above_threshold = {}\n",
    "for paper_id in top_nodes:\n",
    "    title = get_paper_title(paper_id, api_key)  # Function to fetch paper title using paper_id\n",
    "    if title:\n",
    "        wrapped_title = wrap_text(title, 40)  # Wrap the title into at most two lines\n",
    "        titles_above_threshold[paper_id] = wrapped_title\n",
    "\n",
    "\n",
    "# Cap the maximum node size to prevent too large nodes\n",
    "node_sizes = [G.in_degree(node) * 2000 for node in G.nodes()]\n",
    "\n",
    "# Draw the graph with adjusted layout parameters\n",
    "plt.figure(figsize=(60, 35))\n",
    "pos = nx.kamada_kawai_layout(G, dist=None, scale=1)\n",
    "\n",
    "adjust_overlap(pos, top_nodes, min_dist=0.2, repulsion_factor=1.05)\n",
    "\n",
    "# Draw all nodes first\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='skyblue')\n",
    "\n",
    "# Draw the edges\n",
    "nx.draw_networkx_edges(G, pos, edge_color='gray', width=0.5)\n",
    "\n",
    "# Assign and label top nodes with titles\n",
    "for node, label in titles_above_threshold.items():\n",
    "    x, y = pos[node]\n",
    "    num_lines = label.count('\\n') + 1\n",
    "    y_offset = 0.01 * 2  # Adjust this factor as needed to position the text correctly\n",
    "    plt.text(x, y + y_offset, label, fontsize=20, ha='center', va='center')\n",
    "\n",
    "plt.title(\"Graph of Paper References\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a graph of all the papers with more than 10 internal references \n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Set font properties globally\n",
    "matplotlib.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.size': 10\n",
    "})\n",
    "\n",
    "# Mapping of paper titles to their techniques\n",
    "title_to_technique = {\n",
    "    \"A Practical Survey on Zero-Shot Prompt Design for In-Context Learning\": \"Zero-Shot Prompt\",\n",
    "    \"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm\": \"Role Prompting\",\n",
    "    \"Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints\": \"Style Prompting\",\n",
    "    \"Language Models are Few-Shot Learners\": \"In-context learning (ICL)\",\n",
    "    \"A Survey on In-context Learning\": \"Few-shot learning (FSL)\",\n",
    "    \"What Makes Good In-Context Examples for {GPT}-3?\": \"K-Nearest Neighbor (KNN)\",\n",
    "    \"Finding Support Examples for In-Context Learning\": \"fiLter-thEN-Search (LENS)\",\n",
    "    \"Unified Demonstration Retriever for In-Context Learning\": \"Unified Demonstration Retriever (UDR)\",\n",
    "    \"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity\": \"Example Ordering\",\n",
    "    \"Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator\": \"Example Generation\",\n",
    "    \"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?\": \"Example Label Quality\",\n",
    "    \"Selective Annotation Makes Language Models Better Few-Shot Learners\": \"Input Distribution\",\n",
    "    \"Chain-of-thought prompting elicits reasoning in large language models\": \"Chain-of-Thought\",\n",
    "    \"Large Language Models are Zero-Shot Reasoners\": \"Zero-shot-CoT\",\n",
    "    \"Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models\": \"Step-Back Prompting\",\n",
    "    \"Thread of Thought Unraveling Chaotic Contexts\": \"Thread-of-Thought (ThoT)\",\n",
    "    \"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment\": \"Moral Chain-of-Thought\",\n",
    "    \"Automatic Chain of Thought Prompting in Large Language Models\": \"Few-Shot CoT\",\n",
    "    \"True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4\": \"Del_2023\",\n",
    "    \"Contrastive Chain-of-Thought Prompting\": \"Contrastive Chain of Thought\",\n",
    "    \"Gemini: A Family of Highly Capable Multimodal Models\": \"Uncertainty-Routed CoT\",\n",
    "    \"Complexity-Based Prompting for Multi-Step Reasoning\": \"Complexity-based Prompting\",\n",
    "    \"Active Prompting with Chain-of-Thought for Large Language Models\": \"Active-Prompt\",\n",
    "    \"MoT: Memory-of-Thought Enables ChatGPT to Self-Improve\": \"Memory-of-Thought\",\n",
    "    \"Measuring and Narrowing the Compositionality Gap in Language Models\": \"Self-Ask\",\n",
    "    \"Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data\": \"Automate-CoT\",\n",
    "    \"Tab-CoT: Zero-shot Tabular Chain of Thought\": \"Tab-CoT\",\n",
    "    \"Is a Question Decomposition Unit All We Need?\": \"Decomposition\",\n",
    "    \"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\": \"Least-to-most Prompting\",\n",
    "    \"Decomposed Prompting: A Modular Approach for Solving Complex Tasks\": \"Decomposed Prompting (DECOMP)\",\n",
    "    \"Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models\": \"Plan-and-Solve Prompting\",\n",
    "    \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\": \"Tree-of-Thought (ToT)\",\n",
    "    \"Cumulative Reasoning with Large Language Models\": \"Cumulative Reasoning\",\n",
    "    \"Graph of thoughts: Solving elaborate problems with large language models\": \"Graph-of-Thoughts\",\n",
    "    \"Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models\": \"Recursion of Thought\",\n",
    "    \"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\": \"Program-of-Thoughts\",\n",
    "    \"Faithful Chain-of-Thought Reasoning\": \"Faithful Chain-of-Thought\",\n",
    "    \"Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding\": \"Skeleton-of-Thought\",\n",
    "    \"Exploring Demonstration Ensembling for In-context Learning\": \"Demonstration Ensembling (DENSE)\",\n",
    "    \"$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference\": \"kNN Prompting\",\n",
    "    \"An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels\": \"Max Mutual Information Method\",\n",
    "    \"Self-Consistency Improves Chain of Thought Reasoning in Language Models\": \"Self-Consistency\",\n",
    "    \"Universal Self-Consistency for Large Language Model Generation\": \"Universal Self-Consistency\",\n",
    "    \"Making Language Models Better Reasoners with Step-Aware Verifier\": \"DiVeRSe\",\n",
    "    \"Language Models (Mostly) Know What They Know\": \"Self-Evaluation\",\n",
    "    \"Self-Refine: Iterative Refinement with Self-Feedback\": \"Self-Refine\",\n",
    "    \"RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought\": \"Reversing Chain-of-Thought (RCoT)\",\n",
    "    \"Large Language Models are Better Reasoners with Self-Verification\": \"Self Verification\",\n",
    "    \"Deductive Verification of Chain-of-Thought Reasoning\": \"Deductive Verification\",\n",
    "    \"Chain-of-Verification Reduces Hallucination in Large Language Models\": \"Chain-of-Verification (COVE)\",\n",
    "    \"Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations\": \"Maieutic Prompting\",\n",
    "    \"How can we know what language models know?\": \"Prompt Mining\",\n",
    "    \"Large Language Models Understand and Can be Enhanced by Emotional Stimuli\": \"EmotionPrompt\",\n",
    "    \"Re-Reading Improves Reasoning in Language Models\": \"Re-reading (RE2)\",\n",
    "    \"Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities\": \"Think Twice (SIMTOM)\",\n",
    "    \"Better Zero-Shot Reasoning with Self-Adaptive Prompting\": \"Consistency-based Self-adaptive Prompting (COSP)\",\n",
    "    \"Universal Self-Adaptive Prompting\": \"Universal Self-Adaptive Prompting (USP)\",\n",
    "    \"System 2 Attention (is something you might need too)\": \"System 2 Attention\",\n",
    "    \"Large Language Models as Optimizers\": \"OPRO\",\n",
    "    \"Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves\": \"Rephrase and Respond (RAR)\"\n",
    "}\n",
    "\n",
    "# Load the existing dictionary of paper references\n",
    "with open('cleaned_complete_paper_references.json', 'r') as file:\n",
    "    paper_references = json.load(file)\n",
    "\n",
    "# Query each title and get citation counts\n",
    "citation_counts = {}\n",
    "for title, technique in title_to_technique.items():\n",
    "    paper_id = query_paper_id(title, api_key)\n",
    "    if paper_id and paper_id in paper_references:\n",
    "        citation_count = len(paper_references[paper_id])\n",
    "        citation_counts[technique] = citation_count\n",
    "        print({technique}, {citation_count})\n",
    "\n",
    "# Sort the citation counts in descending order\n",
    "sorted_citations = sorted(citation_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Unpack the sorted data for plotting\n",
    "sorted_techniques, sorted_counts = zip(*sorted_citations)\n",
    "\n",
    "# Create a vertical bar chart\n",
    "plt.figure(figsize=(30, 12))\n",
    "plt.bar(sorted_techniques, sorted_counts, color=(45/255, 137/255, 145/255, 1))  # RGBA color\n",
    "\n",
    "# Rotate the x-axis labels by 45 degrees for better readability\n",
    "plt.xticks(rotation=90, ha='right')  # ha='right' aligns the labels at the right angle\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('Number of References')\n",
    "plt.xlabel('Technique')\n",
    "plt.title('Citation Counts for Techniques Based on Papers')\n",
    "\n",
    "plt.tight_layout()  # Adjusts layout to prevent clipping of labels\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
