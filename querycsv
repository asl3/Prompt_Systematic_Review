title,authors,url,dateSubmitted,keywords,abstract
"""Do Anything Now"": Characterizing and Evaluating In-The-Wild Jailbreak  Prompts on Large Language Models","['Xinyue Shen', 'Zeyuan Chen', 'Michael Backes', 'Yun Shen', 'Yang Zhang']",http://arxiv.org/pdf/2308.03825v1.pdf,2023-08-07,"['cs.cr', 'cs.lg']","  The misuse of large language models (LLMs) has garnered significant attentionfrom the general public and LLM vendors. In response, efforts have been made toalign LLMs with human values and intent use. However, a particular type ofadversarial prompts, known as jailbreak prompt, has emerged and continuouslyevolved to bypass the safeguards and elicit harmful content from LLMs. In thispaper, we conduct the first measurement study on jailbreak prompts in the wild,with 6,387 prompts collected from four platforms over six months. Leveragingnatural language processing technologies and graph-based community detectionmethods, we discover unique characteristics of jailbreak prompts and theirmajor attack strategies, such as prompt injection and privilege escalation. Wealso observe that jailbreak prompts increasingly shift from public platforms toprivate ones, posing new challenges for LLM vendors in proactive detection. Toassess the potential harm caused by jailbreak prompts, we create a question setcomprising 46,800 samples across 13 forbidden scenarios. Our experiments showthat current LLMs and safeguards cannot adequately defend jailbreak prompts inall scenarios. Particularly, we identify two highly effective jailbreak promptswhich achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, andthey have persisted online for over 100 days. Our work sheds light on thesevere and evolving threat landscape of jailbreak prompts. We hope our studycan facilitate the research community and LLM vendors in promoting safer andregulated LLMs."
Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study,"['Yi Liu', 'Gelei Deng', 'Zhengzi Xu', 'Yuekang Li', 'Yaowen Zheng', 'Ying Zhang', 'Lida Zhao', 'Tianwei Zhang', 'Yang Liu']",http://arxiv.org/pdf/2305.13860v1.pdf,2023-05-23,"['cs.se', 'cs.ai', 'cs.cl']","  Large Language Models (LLMs), like ChatGPT, have demonstrated vast potentialbut also introduce challenges related to content constraints and potentialmisuse. Our study investigates three key research questions: (1) the number ofdifferent prompt types that can jailbreak LLMs, (2) the effectiveness ofjailbreak prompts in circumventing LLM constraints, and (3) the resilience ofChatGPT against these jailbreak prompts. Initially, we develop a classificationmodel to analyze the distribution of existing prompts, identifying ten distinctpatterns and three categories of jailbreak prompts. Subsequently, we assess thejailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing adataset of 3,120 jailbreak questions across eight prohibited scenarios.Finally, we evaluate the resistance of ChatGPT against jailbreak prompts,finding that the prompts can consistently evade the restrictions in 40 use-casescenarios. The study underscores the importance of prompt structures injailbreaking LLMs and discusses the challenges of robust jailbreak promptgeneration and prevention."
A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can  Fool Large Language Models Easily,"['Peng Ding', 'Jun Kuang', 'Dan Ma', 'Xuezhi Cao', 'Yunsen Xian', 'Jiajun Chen', 'Shujian Huang']",http://arxiv.org/pdf/2311.08268v1.pdf,2023-11-14,['cs.cl'],"  Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed toprovide useful and safe responses. However, adversarial prompts known as'jailbreaks' can circumvent safeguards, leading LLMs to generate harmfulcontent. Exploring jailbreak prompts can help to better reveal the weaknessesof LLMs and further steer us to secure them. Unfortunately, existing jailbreakmethods either suffer from intricate manual design or require optimization onanother white-box model, compromising generalization or jailbreak efficiency.In this paper, we generalize jailbreak prompt attacks into two aspects: (1)Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM,an automatic framework that leverages LLMs themselves to generate effectivejailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantlyimproves the attack success rate while greatly reducing the time cost comparedto existing baselines. Our study also reveals the inadequacy of current defensemethods in safeguarding LLMs. Finally, we offer detailed analysis anddiscussion from the perspective of prompt execution priority on the failure ofLLMs' defense. We hope that our research can catalyze both the academiccommunity and LLMs vendors towards the provision of safer and more regulatedLarge Language Models."
AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language  Models,"['Xiaogeng Liu', 'Nan Xu', 'Muhao Chen', 'Chaowei Xiao']",http://arxiv.org/pdf/2310.04451v1.pdf,2023-10-03,"['cs.cl', 'cs.ai']","  The aligned Large Language Models (LLMs) are powerful language understandingand decision-making tools that are created through extensive alignment withhuman feedback. However, these large models remain susceptible to jailbreakattacks, where adversaries manipulate prompts to elicit malicious outputs thatshould not be given by aligned LLMs. Investigating jailbreak prompts can leadus to delve into the limitations of LLMs and further guide us to secure them.Unfortunately, existing jailbreak techniques suffer from either (1) scalabilityissues, where attacks heavily rely on manual crafting of prompts, or (2)stealthiness problems, as attacks depend on token-based algorithms to generateprompts that are often semantically meaningless, making them susceptible todetection through basic perplexity testing. In light of these challenges, weintend to answer this question: Can we develop an approach that canautomatically generate stealthy jailbreak prompts? In this paper, we introduceAutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN canautomatically generate stealthy jailbreak prompts by the carefully designedhierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDANnot only automates the process while preserving semantic meaningfulness, butalso demonstrates superior attack strength in cross-model transferability, andcross-sample universality compared with the baseline. Moreover, we also compareAutoDAN with perplexity-based defense methods and show that AutoDAN can bypassthem effectively."
Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM,"['Bochuan Cao', 'Yuanpu Cao', 'Lu Lin', 'Jinghui Chen']",http://arxiv.org/pdf/2309.14348v2.pdf,2023-09-18,"['cs.cl', 'cs.ai', 'cs.cr', 'cs.lg']","  Recently, Large Language Models (LLMs) have made significant advancements andare now widely used across various domains. Unfortunately, there has been arising concern that LLMs can be misused to generate harmful or maliciouscontent. Though a line of research has focused on aligning LLMs with humanvalues and preventing them from producing inappropriate content, suchalignments are usually vulnerable and can be bypassed by alignment-breakingattacks via adversarially optimized or handcrafted jailbreaking prompts. Inthis work, we introduce a Robustly Aligned LLM (RA-LLM) to defend againstpotential alignment-breaking attacks. RA-LLM can be directly constructed uponan existing aligned LLM with a robust alignment checking function, withoutrequiring any expensive retraining or fine-tuning process of the original LLM.Furthermore, we also provide a theoretical analysis for RA-LLM to verify itseffectiveness in defending against alignment-breaking attacks. Throughreal-world experiments on open-source large language models, we demonstratethat RA-LLM can successfully defend against both state-of-the-art adversarialprompts and popular handcrafted jailbreaking prompts by reducing their attacksuccess rates from nearly 100% to around 10% or less."
FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively  Discovering Jailbreak Vulnerabilities in Large Language Models,"['Dongyu Yao', 'Jianshu Zhang', 'Ian G. Harris', 'Marcel Carlsson']",http://arxiv.org/pdf/2309.05274v1.pdf,2023-09-11,['cs.cr'],"  Jailbreak vulnerabilities in Large Language Models (LLMs), which exploitmeticulously crafted prompts to elicit content that violates serviceguidelines, have captured the attention of research communities. While modelowners can defend against individual jailbreak prompts through safety trainingstrategies, this relatively passive approach struggles to handle the broadercategory of similar jailbreaks. To tackle this issue, we introduce FuzzLLM, anautomated fuzzing framework designed to proactively test and discover jailbreakvulnerabilities in LLMs. We utilize templates to capture the structuralintegrity of a prompt and isolate key features of a jailbreak class asconstraints. By integrating different base classes into powerful combo attacksand varying the elements of constraints and prohibited questions, FuzzLLMenables efficient testing with reduced manual effort. Extensive experimentsdemonstrate FuzzLLM's effectiveness and comprehensiveness in vulnerabilitydiscovery across various LLMs."
Scalable and Transferable Black-Box Jailbreaks for Language Models via  Persona Modulation,"['Rusheb Shah', 'Quentin Feuillade--Montixi', 'Soroush Pour', 'Arush Tagade', 'Stephen Casper', 'Javier Rando']",http://arxiv.org/pdf/2311.03348v2.pdf,2023-11-06,"['cs.cl', 'cs.ai', 'cs.lg']","  Despite efforts to align large language models to produce harmless responses,they are still vulnerable to jailbreak prompts that elicit unrestrictedbehaviour. In this work, we investigate persona modulation as a black-boxjailbreaking method to steer a target model to take on personalities that arewilling to comply with harmful instructions. Rather than manually craftingprompts for each persona, we automate the generation of jailbreaks using alanguage model assistant. We demonstrate a range of harmful completions madepossible by persona modulation, including detailed instructions forsynthesising methamphetamine, building a bomb, and laundering money. Theseautomated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is185 times larger than before modulation (0.23%). These prompts also transfer toClaude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%,respectively. Our work reveals yet another vulnerability in commercial largelanguage models and highlights the need for more comprehensive safeguards."
Evil Geniuses: Delving into the Safety of LLM-based Agents,"['Yu Tian', 'Xiao Yang', 'Jingyuan Zhang', 'Yinpeng Dong', 'Hang Su']",http://arxiv.org/pdf/2311.11855v1.pdf,2023-11-20,['cs.cl'],"  The rapid advancements in large language models (LLMs) have led to aresurgence in LLM-based agents, which demonstrate impressive human-likebehaviors and cooperative capabilities in various interactions and strategyformulations. However, evaluating the safety of LLM-based agents remains acomplex challenge. This paper elaborately conducts a series of manual jailbreakprompts along with a virtual chat-powered evil plan development team, dubbedEvil Geniuses, to thoroughly probe the safety aspects of these agents. Ourinvestigation reveals three notable phenomena: 1) LLM-based agents exhibitreduced robustness against malicious attacks. 2) the attacked agents couldprovide more nuanced responses. 3) the detection of the produced improperresponses is more challenging. These insights prompt us to question theeffectiveness of LLM-based attacks on agents, highlighting vulnerabilities atvarious levels and within different role specializations within thesystem/agent of LLM-based agents. Extensive evaluation and discussion revealthat LLM-based agents face significant challenges in safety and yield insightsfor future research. Our code is available athttps://github.com/T1aNS1R/Evil-Geniuses."
Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output  Robustness of Large Language Models,"['Huachuan Qiu', 'Shuai Zhang', 'Anqi Li', 'Hongliang He', 'Zhenzhong Lan']",http://arxiv.org/pdf/2307.08487v3.pdf,2023-07-17,['cs.cl'],"  Considerable research efforts have been devoted to ensuring that largelanguage models (LLMs) align with human values and generate safe text. However,an excessive focus on sensitivity to certain topics can compromise the model'srobustness in following instructions, thereby impacting its overall performancein completing tasks. Previous benchmarks for jailbreaking LLMs have primarilyfocused on evaluating the safety of the models without considering theirrobustness. In this paper, we propose a benchmark that assesses both the safetyand robustness of LLMs, emphasizing the need for a balanced approach. Tocomprehensively study text safety and output robustness, we introduce a latentjailbreak prompt dataset, each involving malicious instruction embedding.Specifically, we instruct the model to complete a regular task, such astranslation, with the text to be translated containing malicious instructions.To further analyze safety and robustness, we design a hierarchical annotationframework. We present a systematic analysis of the safety and robustness ofLLMs regarding the position of explicit normal instructions, word replacements(verbs in explicit normal instructions, target groups in maliciousinstructions, cue words for explicit normal instructions), and instructionreplacements (different explicit normal instructions). Our results demonstratethat current LLMs not only prioritize certain instruction verbs but alsoexhibit varying jailbreak rates for different instruction verbs in explicitnormal instructions. Code and data are available athttps://github.com/qiuhuachuan/latent-jailbreak."
MasterKey: Automated Jailbreak Across Multiple Large Language Model  Chatbots,"['Gelei Deng', 'Yi Liu', 'Yuekang Li', 'Kailong Wang', 'Ying Zhang', 'Zefeng Li', 'Haoyu Wang', 'Tianwei Zhang', 'Yang Liu']",http://arxiv.org/pdf/2307.08715v2.pdf,2023-07-16,['cs.cr'],"  Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)services due to their exceptional proficiency in understanding and generatinghuman-like text. LLM chatbots, in particular, have seen widespread adoption,transforming human-machine interactions. However, these LLM chatbots aresusceptible to ""jailbreak"" attacks, where malicious users manipulate prompts toelicit inappropriate or sensitive responses, contravening service policies.Despite existing attempts to mitigate such threats, our research reveals asubstantial gap in our understanding of these vulnerabilities, largely due tothe undisclosed defensive measures implemented by LLM service providers.  In this paper, we present Jailbreaker, a comprehensive framework that offersan in-depth understanding of jailbreak attacks and countermeasures. Our workmakes a dual contribution. First, we propose an innovative methodology inspiredby time-based SQL injection techniques to reverse-engineer the defensivestrategies of prominent LLM chatbots, such as ChatGPT, Bard, and Bing Chat.This time-sensitive approach uncovers intricate details about these services'defenses, facilitating a proof-of-concept attack that successfully bypassestheir mechanisms. Second, we introduce an automatic generation method forjailbreak prompts. Leveraging a fine-tuned LLM, we validate the potential ofautomated jailbreak generation across various commercial LLM chatbots. Ourmethod achieves a promising average success rate of 21.58%, significantlyoutperforming the effectiveness of existing techniques. We have responsiblydisclosed our findings to the concerned service providers, underscoring theurgent need for more robust defenses. Jailbreaker thus marks a significant steptowards understanding and mitigating jailbreak threats in the realm of LLMchatbots."
Using Large Language Models for Cybersecurity Capture-The-Flag  Challenges and Certification Questions,"['Wesley Tann', 'Yuancheng Liu', 'Jun Heng Sim', 'Choon Meng Seah', 'Ee-Chien Chang']",http://arxiv.org/pdf/2308.10443v1.pdf,2023-08-21,"['cs.ai', 'cs.cl', 'cs.cy']","  The assessment of cybersecurity Capture-The-Flag (CTF) exercises involvesparticipants finding text strings or ``flags'' by exploiting systemvulnerabilities. Large Language Models (LLMs) are natural-language modelstrained on vast amounts of words to understand and generate text; they canperform well on many CTF challenges. Such LLMs are freely available tostudents. In the context of CTF exercises in the classroom, this raisesconcerns about academic integrity. Educators must understand LLMs' capabilitiesto modify their teaching to accommodate generative AI assistance. This researchinvestigates the effectiveness of LLMs, particularly in the realm of CTFchallenges and questions. Here we evaluate three popular LLMs, OpenAI ChatGPT,Google Bard, and Microsoft Bing. First, we assess the LLMs' question-answeringperformance on five Cisco certifications with varying difficulty levels. Next,we qualitatively study the LLMs' abilities in solving CTF challenges tounderstand their limitations. We report on the experience of using the LLMs forseven test cases in all five types of CTF challenges. In addition, wedemonstrate how jailbreak prompts can bypass and break LLMs' ethicalsafeguards. The paper concludes by discussing LLM's impact on CTF exercises andits implications."
Baseline Defenses for Adversarial Attacks Against Aligned Language  Models,"['Neel Jain', 'Avi Schwarzschild', 'Yuxin Wen', 'Gowthami Somepalli', 'John Kirchenbauer', 'Ping-yeh Chiang', 'Micah Goldblum', 'Aniruddha Saha', 'Jonas Geiping', 'Tom Goldstein']",http://arxiv.org/pdf/2309.00614v2.pdf,2023-09-01,"['cs.lg', 'cs.cl', 'cs.cr']","  As Large Language Models quickly become ubiquitous, it becomes critical tounderstand their security vulnerabilities. Recent work shows that textoptimizers can produce jailbreaking prompts that bypass moderation andalignment. Drawing from the rich body of work on adversarial machine learning,we approach these attacks with three questions: What threat models arepractically useful in this domain? How do baseline defense techniques performin this new domain? How does LLM security differ from computer vision?  We evaluate several baseline defense strategies against leading adversarialattacks on LLMs, discussing the various settings in which each is feasible andeffective. Particularly, we look at three types of defenses: detection(perplexity based), input preprocessing (paraphrase and retokenization), andadversarial training. We discuss white-box and gray-box settings and discussthe robustness-performance trade-off for each of the defenses considered. Wefind that the weakness of existing discrete optimizers for text, combined withthe relatively high costs of optimization, makes standard adaptive attacks morechallenging for LLMs. Future research will be needed to uncover whether morepowerful optimizers can be developed, or whether the strength of filtering andpreprocessing defenses is greater in the LLMs domain than it has been incomputer vision."
GPTFUZZER: Red Teaming Large Language Models with Auto-Generated  Jailbreak Prompts,"['Jiahao Yu', 'Xingwei Lin', 'Zheng Yu', 'Xinyu Xing']",http://arxiv.org/pdf/2309.10253v2.pdf,2023-09-19,['cs.ai'],"  Large language models (LLMs) have recently experienced tremendous popularityand are widely used from casual conversations to AI-driven programming.However, despite their considerable success, LLMs are not entirely reliable andcan give detailed guidance on how to conduct harmful or illegal activities.While safety measures can reduce the risk of such outputs, adversarialjailbreak attacks can still exploit LLMs to produce harmful content. Thesejailbreak templates are typically manually crafted, making large-scale testingchallenging.  In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzingframework inspired by the AFL fuzzing framework. Instead of manual engineering,GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs.At its core, GPTFuzz starts with human-written templates as initial seeds, thenmutates them to produce new templates. We detail three key components ofGPTFuzz: a seed selection strategy for balancing efficiency and variability,mutate operators for creating semantically equivalent or similar sentences, anda judgment model to assess the success of a jailbreak attack.  We evaluate GPTFuzz against various commercial and open-source LLMs,including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Ourresults indicate that GPTFuzz consistently produces jailbreak templates with ahigh success rate, surpassing human-crafted templates. Remarkably, GPTFuzzachieves over 90% attack success rates against ChatGPT and Llama-2 models, evenwith suboptimal initial seed templates. We anticipate that GPTFuzz will beinstrumental for researchers and practitioners in examining LLM robustness andwill encourage further exploration into enhancing LLM safety."
Probing LLMs for hate speech detection: strengths and vulnerabilities,"['Sarthak Roy', 'Ashish Harshavardhan', 'Animesh Mukherjee', 'Punyajoy Saha']",http://arxiv.org/pdf/2310.12860v2.pdf,2023-10-19,"['cs.cl', 'cs.cy']","  Recently efforts have been made by social media platforms as well asresearchers to detect hateful or toxic language using large language models.However, none of these works aim to use explanation, additional context andvictim community information in the detection process. We utilise differentprompt variation, input information and evaluate large language models in zeroshot setting (without adding any in-context examples). We select three largelanguage models (GPT-3.5, text-davinci and Flan-T5) and three datasets -HateXplain, implicit hate and ToxicSpans. We find that on average including thetarget information in the pipeline improves the model performance substantially(~20-30%) over the baseline across the datasets. There is also a considerableeffect of adding the rationales/explanations into the pipeline (~10-20%) overthe baseline across the datasets. In addition, we further provide a typology ofthe error cases where these large language models fail to (i) classify and (ii)explain the reason for the decisions they take. Such vulnerable pointsautomatically constitute 'jailbreak' prompts for these models and industryscale safeguard techniques need to be developed to make the models robustagainst such prompts."
Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts,"['Yuanwei Wu', 'Xiang Li', 'Yixin Liu', 'Pan Zhou', 'Lichao Sun']",http://arxiv.org/pdf/2311.09127v1.pdf,2023-11-15,"['cs.cr', 'cs.ai', 'cs.lg']","  Existing work on jailbreak Multimodal Large Language Models (MLLMs) hasfocused primarily on adversarial examples in model inputs, with less attentionto vulnerabilities in model APIs. To fill the research gap, we carry out thefollowing work: 1) We discover a system prompt leakage vulnerability in GPT-4V.Through carefully designed dialogue, we successfully steal the internal systemprompts of GPT-4V. This finding indicates potential exploitable security risksin MLLMs; 2)Based on the acquired system prompts, we propose a novel MLLMjailbreaking attack method termed SASP (Self-Adversarial Attack via SystemPrompt). By employing GPT-4 as a red teaming tool against itself, we aim tosearch for potential jailbreak prompts leveraging stolen system prompts.Furthermore, in pursuit of better performance, we also add human modificationbased on GPT-4's analysis, which further improves the attack success rate to98.7\%; 3) We evaluated the effect of modifying system prompts to defendagainst jailbreaking attacks. Results show that appropriately designed systemprompts can significantly reduce jailbreak success rates. Overall, our workprovides new insights into enhancing MLLM security, demonstrating the importantrole of system prompts in jailbreaking, which could be leveraged to greatlyfacilitate jailbreak success rates while also holding the potential fordefending against jailbreaks."
Self-Deception: Reverse Penetrating the Semantic Firewall of Large  Language Models,"['Zhenhua Wang', 'Wei Xie', 'Kai Chen', 'Baosheng Wang', 'Zhiwen Gui', 'Enze Wang']",http://arxiv.org/pdf/2308.11521v2.pdf,2023-08-16,"['cs.cl', 'cs.ai', 'cs.lg']","  Large language models (LLMs), such as ChatGPT, have emerged with astonishingcapabilities approaching artificial general intelligence. While providingconvenience for various societal needs, LLMs have also lowered the cost ofgenerating harmful content. Consequently, LLM developers have deployedsemantic-level defenses to recognize and reject prompts that may lead toinappropriate content. Unfortunately, these defenses are not foolproof, andsome attackers have crafted ""jailbreak"" prompts that temporarily hypnotize theLLM into forgetting content defense rules and answering any improper questions.To date, there is no clear explanation of the principles behind thesesemantic-level attacks and defenses in both industry and academia.  This paper investigates the LLM jailbreak problem and proposes an automaticjailbreak method for the first time. We propose the concept of a semanticfirewall and provide three technical implementation approaches. Inspired by theattack that penetrates traditional firewalls through reverse tunnels, weintroduce a ""self-deception"" attack that can bypass the semantic firewall byinducing LLM to generate prompts that facilitate jailbreak. We generated atotal of 2,520 attack payloads in six languages (English, Russian, French,Spanish, Chinese, and Arabic) across seven virtual scenarios, targeting thethree most common types of violations: violence, hate, and pornography. Theexperiment was conducted on two models, namely the GPT-3.5-Turbo and GPT-4. Thesuccess rates on the two models were 86.2% and 67%, while the failure rateswere 4.7% and 2.2%, respectively. This highlighted the effectiveness of theproposed attack method. All experimental code and raw data will be released asopen-source to inspire future research. We believe that manipulating AIbehavior through carefully crafted prompts will become an important researchdirection in the future."
Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and  the Case of Information Extraction,"['Martin Josifoski', 'Marija Sakota', 'Maxime Peyrard', 'Robert West']",http://arxiv.org/pdf/2303.04132v2.pdf,2023-03-07,"['cs.cl', 'cs.ai', 'cs.lg']","  Large language models (LLMs) have great potential for synthetic datageneration. This work shows that useful data can be synthetically generatedeven for tasks that cannot be solved directly by LLMs: for problems withstructured outputs, it is possible to prompt an LLM to perform the task in thereverse direction, by generating plausible input text for a target outputstructure. Leveraging this asymmetry in task difficulty makes it possible toproduce large-scale, high-quality data for complex tasks. We demonstrate theeffectiveness of this approach on closed information extraction, wherecollecting ground-truth data is challenging, and no satisfactory dataset existsto date. We synthetically generate a dataset of 1.8M data points, establish itssuperior quality compared to existing datasets in a human evaluation, and useit to finetune small models (220M and 770M parameters), termed SynthIE, thatoutperform the prior state of the art (with equal model size) by a substantialmargin of 57 absolute points in micro-F1 and 79 points in macro-F1. Code, data,and models are available at https://github.com/epfl-dlab/SynthIE."
Small Language Models Improve Giants by Rewriting Their Outputs,"['Giorgos Vernikos', 'Arthur Bražinskas', 'Jakub Adamek', 'Jonathan Mallinson', 'Aliaksei Severyn', 'Eric Malmi']",http://arxiv.org/pdf/2305.13514v1.pdf,2023-05-22,"['cs.cl', 'cs.lg']","  Large language models (LLMs) have demonstrated impressive few-shot learningcapabilities, but they often underperform compared to fine-tuned models onchallenging tasks. Furthermore, their large size and restricted access onlythrough APIs make task-specific fine-tuning impractical. Moreover, LLMs aresensitive to different aspects of prompts (e.g., the selection and order ofdemonstrations) and can thus require time-consuming prompt engineering. In thislight, we propose a method to correct LLM outputs without relying on theirweights. First, we generate a pool of candidates by few-shot prompting an LLM.Second, we refine the LLM-generated outputs using a smaller model, theLM-corrector (LMCor), which is trained to rank, combine and rewrite thecandidates to produce the final target output. Our experiments demonstrate thateven a small LMCor model (250M) substantially improves the few-shot performanceof LLMs (62B) across diverse tasks. Moreover, we illustrate that the LMCorexhibits robustness against different prompts, thereby minimizing the need forextensive prompt engineering. Finally, we showcase that the LMCor can beseamlessly integrated with different LLMs at inference time, serving as aplug-and-play module to improve their performance."
Aligning Language Models to User Opinions,"['EunJeong Hwang', 'Bodhisattwa Prasad Majumder', 'Niket Tandon']",http://arxiv.org/pdf/2305.14929v1.pdf,2023-05-24,['cs.cl'],"  An important aspect of developing LLMs that interact with humans is to alignmodels' behavior to their users. It is possible to prompt an LLM into behavingas a certain persona, especially a user group or ideological persona the modelcaptured during its pertaining stage. But, how to best align an LLM with aspecific user and not a demographic or ideological group remains an openquestion. Mining public opinion surveys (by Pew Research), we find that theopinions of a user and their demographics and ideologies are not mutualpredictors. We use this insight to align LLMs by modeling both user opinions aswell as user demographics and ideology, achieving up to 7 points accuracy gainsin predicting public opinions from survey questions across a broad set oftopics. In addition to the typical approach of prompting LLMs with demographicsand ideology, we discover that utilizing the most relevant past opinions fromindividual users enables the model to predict user opinions more accurately."
Marked Personas: Using Natural Language Prompts to Measure Stereotypes  in Language Models,"['Myra Cheng', 'Esin Durmus', 'Dan Jurafsky']",http://arxiv.org/pdf/2305.18189v1.pdf,2023-05-29,"['cs.cl', 'cs.ai', 'cs.cy']","  To recognize and mitigate harms from large language models (LLMs), we need tounderstand the prevalence and nuances of stereotypes in LLM outputs. Towardthis end, we present Marked Personas, a prompt-based method to measurestereotypes in LLMs for intersectional demographic groups without any lexiconor data labeling. Grounded in the sociolinguistic concept of markedness (whichcharacterizes explicitly linguistically marked categories versus unmarkeddefaults), our proposed method is twofold: 1) prompting an LLM to generatepersonas, i.e., natural language descriptions, of the target demographic groupalongside personas of unmarked, default groups; 2) identifying the words thatsignificantly distinguish personas of the target group from correspondingunmarked ones. We find that the portrayals generated by GPT-3.5 and GPT-4contain higher rates of racial stereotypes than human-written portrayals usingthe same prompts. The words distinguishing personas of marked (non-white,non-male) groups reflect patterns of othering and exoticizing thesedemographics. An intersectional lens further reveals tropes that dominateportrayals of marginalized groups, such as tropicalism and thehypersexualization of minoritized women. These representational harms haveconcerning implications for downstream applications like story generation."
Reranking for Natural Language Generation from Logical Forms: A Study  based on Large Language Models,"['Levon Haroutunian', 'Zhuang Li', 'Lucian Galescu', 'Philip Cohen', 'Raj Tumuluri', 'Gholamreza Haffari']",http://arxiv.org/pdf/2309.12294v1.pdf,2023-09-21,['cs.cl'],"  Large language models (LLMs) have demonstrated impressive capabilities innatural language generation. However, their output quality can be inconsistent,posing challenges for generating natural language from logical forms (LFs).This task requires the generated outputs to embody the exact semantics of LFs,without missing any LF semantics or creating any hallucinations. In this work,we tackle this issue by proposing a novel generate-and-rerank approach. Ourapproach involves initially generating a set of candidate outputs by promptingan LLM and subsequently reranking them using a task-specific reranker model. Inaddition, we curate a manually collected dataset to evaluate the alignmentbetween different ranking metrics and human judgements. The chosen rankingmetrics are utilized to enhance the training and evaluation of the rerankermodel. By conducting extensive experiments on three diverse datasets, wedemonstrate that the candidates selected by our reranker outperform thoseselected by baseline methods in terms of semantic consistency and fluency, asmeasured by three comprehensive metrics. Our findings provide strong evidencefor the effectiveness of our approach in improving the quality of generatedoutputs."
Using Natural Language Explanations to Improve Robustness of In-context  Learning for Natural Language Inference,"['Xuanli He', 'Yuxiang Wu', 'Oana-Maria Camburu', 'Pasquale Minervini', 'Pontus Stenetorp']",http://arxiv.org/pdf/2311.07556v1.pdf,2023-11-13,['cs.cl'],"  Recent studies have demonstrated that large language models (LLMs) excel indiverse tasks through in-context learning (ICL) facilitated by task-specificprompts and examples. However, the existing literature shows that ICLencounters performance deterioration when exposed to adversarial inputs.Enhanced performance has been observed when ICL is augmented with naturallanguage explanations (NLEs) (we refer to it as X-ICL). Thus, this workinvestigates whether X-ICL can improve the robustness of LLMs on a suite ofseven adversarial and challenging natural language inference datasets.Moreover, we introduce a new approach to X-ICL by prompting an LLM (ChatGPT inour case) with few human-generated NLEs to produce further NLEs (we call itChatGPT few-shot), which we show superior to both ChatGPT zero-shot andhuman-generated NLEs alone. We evaluate five popular LLMs (GPT3.5-turbo,LLaMa2, Vicuna, Zephyr, Mistral) and show that X-ICL with ChatGPT few-shotyields over 6% improvement over ICL. Furthermore, while prompt selectionstrategies were previously shown to significantly improve ICL onin-distribution test sets, we show that these strategies do not match theefficacy of the X-ICL paradigm in robustness-oriented evaluations."
Towards Verifiable Text Generation with Symbolic References,"['Lucas Torroba Hennigen', 'Shannon Shen', 'Aniruddha Nrusimha', 'Bernhard Gapp', 'David Sontag', 'Yoon Kim']",http://arxiv.org/pdf/2311.09188v1.pdf,2023-11-15,"['cs.cl', 'cs.ai', 'cs.lg']","  Large language models (LLMs) have demonstrated an impressive ability tosynthesize plausible and fluent text. However they remain vulnerable tohallucinations, and thus their outputs generally require manual humanverification for high-stakes applications, which can be time-consuming anddifficult. This paper proposes symbolically grounded generation (SymGen) as asimple approach for enabling easier validation of an LLM's output. SymGenprompts an LLM to interleave its regular output text with explicit symbolicreferences to fields present in some conditioning data (e.g., a table in JSONformat). The references can be used to display the provenance of differentspans of text in the generation, reducing the effort required for manualverification. Across data-to-text and question answering experiments, we findthat LLMs are able to directly output text that makes use of symbolicreferences while maintaining fluency and accuracy."
Query Rewriting for Retrieval-Augmented Large Language Models,"['Xinbei Ma', 'Yeyun Gong', 'Pengcheng He', 'Hai Zhao', 'Nan Duan']",http://arxiv.org/pdf/2305.14283v3.pdf,2023-05-23,['cs.cl'],"  Large Language Models (LLMs) play powerful, black-box readers in theretrieve-then-read pipeline, making remarkable progress in knowledge-intensivetasks. This work introduces a new framework, Rewrite-Retrieve-Read instead ofthe previous retrieve-then-read for the retrieval-augmented LLMs from theperspective of the query rewriting. Unlike prior studies focusing on adaptingeither the retriever or the reader, our approach pays attention to theadaptation of the search query itself, for there is inevitably a gap betweenthe input text and the needed knowledge in retrieval. We first prompt an LLM togenerate the query, then use a web search engine to retrieve contexts.Furthermore, to better align the query to the frozen modules, we propose atrainable scheme for our pipeline. A small language model is adopted as atrainable rewriter to cater to the black-box LLM reader. The rewriter istrained using the feedback of the LLM reader by reinforcement learning.Evaluation is conducted on downstream tasks, open-domain QA and multiple-choiceQA. Experiments results show consistent performance improvement, indicatingthat our framework is proven effective and scalable, and brings a new frameworkfor retrieval-augmented LLM."
ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle  Verifiers,"['Kexun Zhang', 'Danqing Wang', 'Jingtao Xia', 'William Yang Wang', 'Lei Li']",http://arxiv.org/pdf/2305.14591v3.pdf,2023-05-24,"['cs.cl', 'cs.se']","  Large language models (LLMs) excel at implementing code from functionalitydescriptions but struggle with algorithmic problems that require not onlyimplementation but also identification of the suitable algorithm. Moreover,LLM-generated programs lack guaranteed correctness and require humanverification. To address these challenges, we propose ALGO, a framework thatsynthesizes Algorithmic programs with LLM-Generated Oracles to guide thegeneration and verify their correctness. ALGO first generates a referenceoracle by prompting an LLM to exhaustively enumerate all the combinations ofrelevant variables. This oracle is then utilized to guide an arbitrary searchstrategy in exploring the algorithm space and to verify the synthesizedalgorithms. Our study shows that the LLM-generated oracles are correct for 88%of the cases. With the oracles as verifiers, ALGO can be integrated with anyexisting code generation model in a model-agnostic manner to enhance itsperformance. Experiments show that when equipped with ALGO, we achieve an 8xbetter one-submission pass rate over the Codex model and a 2.6x betterone-submission pass rate over CodeT, the current state-of-the-art model onCodeContests. We can also get 1.3x better pass rate over the ChatGPT CodeInterpreter on unseen problems. The problem set we used for testing, theprompts we used, the verifier and solution programs, and the test casesgenerated by ALGO are available at https://github.com/zkx06111/ALGO."
PromptNER: Prompting For Named Entity Recognition,"['Dhananjay Ashok', 'Zachary C. Lipton']",http://arxiv.org/pdf/2305.15444v2.pdf,2023-05-24,"['cs.cl', 'cs.ai', 'cs.lg']","  In a surprising turn, Large Language Models (LLMs) together with a growingarsenal of prompt-based heuristics now offer powerful off-the-shelf approachesproviding few-shot solutions to myriad classic NLP problems. However, despitepromising early results, these LLM-based few-shot methods remain far from thestate of the art in Named Entity Recognition (NER), where prevailing methodsinclude learning representations via end-to-end structural understanding andfine-tuning on standard labeled corpora. In this paper, we introduce PromptNER,a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt toany new NER task PromptNER requires a set of entity definitions in addition tothe standard few-shot examples. Given a sentence, PromptNER prompts an LLM toproduce a list of potential entities along with corresponding explanationsjustifying their compatibility with the provided entity type definitions.Remarkably, PromptNER achieves state-of-the-art performance on few-shot NER,achieving a 4% (absolute) improvement in F1 score on the ConLL dataset, a 9%(absolute) improvement on the GENIA dataset, and a 4% (absolute) improvement onthe FewNERD dataset. PromptNER also moves the state of the art on Cross DomainNER, outperforming prior methods (including those not limited to the few-shotsetting), setting a new mark on 3/5 CrossNER target domains, with an average F1gain of 3%, despite using less than 2% of the available data."
Dcc --help: Generating Context-Aware Compiler Error Explanations with  Large Language Models,"['Andrew Taylor', 'Alexandra Vassar', 'Jake Renzella', 'Hammond Pearce']",http://arxiv.org/pdf/2308.11873v2.pdf,2023-08-23,"['cs.se', 'cs.lg', 'cs.pl']","  In the challenging field of introductory programming, high enrollments andfailure rates drive us to explore tools and systems to enhance studentoutcomes, especially automated tools that scale to large cohorts. This paperpresents and evaluates the dcc --help tool, an integration of a Large LanguageModel (LLM) into the Debugging C Compiler (DCC) to generate unique,novice-focused explanations tailored to each error. dcc --help prompts an LLMwith contextual information of compile- and run-time error occurrences,including the source code, error location and standard compiler error message.The LLM is instructed to generate novice-focused, actionable error explanationsand guidance, designed to help students understand and resolve problems withoutproviding solutions. dcc --help was deployed to our CS1 and CS2 courses, with2,565 students using the tool over 64,000 times in ten weeks. We analysed asubset of these error/explanation pairs to evaluate their properties, includingconceptual correctness, relevancy, and overall quality. We found that theLLM-generated explanations were conceptually accurate in 90% of compile-timeand 75% of run-time cases, but often disregarded the instruction not to providesolutions in code. Our findings, observations and reflections followingdeployment indicate that dcc-help provides novel opportunities for scaffoldingstudents' introduction to programming."
BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment  of Continuation Writing,"['Chen Wang', 'Minpeng Liao', 'Zhongqiang Huang', 'Jinliang Lu', 'Junhong Wu', 'Yuchen Liu', 'Chengqing Zong', 'Jiajun Zhang']",http://arxiv.org/pdf/2309.00916v1.pdf,2023-09-02,"['cs.cl', 'cs.sd', 'eess.as']","  The emergence of large language models (LLMs) has sparked significantinterest in extending their remarkable language capabilities to speech.However, modality alignment between speech and text still remains an openproblem. Current solutions can be categorized into two strategies. One is acascaded approach where outputs (tokens or states) of a separately trainedspeech recognition system are used as inputs for LLMs, which limits theirpotential in modeling alignment between speech and text. The other is anend-to-end approach that relies on speech instruction data, which is verydifficult to collect in large quantities. In this paper, we address theseissues and propose the BLSP approach that Bootstraps Language-SpeechPre-training via behavior alignment of continuation writing. We achieve this bylearning a lightweight modality adapter between a frozen speech encoder and anLLM, ensuring that the LLM exhibits the same generation behavior regardless ofthe modality of input: a speech segment or its transcript. The training processcan be divided into two steps. The first step prompts an LLM to generate textswith speech transcripts as prefixes, obtaining text continuations. In thesecond step, these continuations are used as supervised signals to train themodality adapter in an end-to-end manner. We demonstrate that thisstraightforward process can extend the capabilities of LLMs to speech, enablingspeech recognition, speech translation, spoken language understanding, andspeech conversation, even in zero-shot cross-lingual scenarios."
Balanced and Explainable Social Media Analysis for Public Health with  Large Language Models,"['Yan Jiang', 'Ruihong Qiu', 'Yi Zhang', 'Peng-Fei Zhang']",http://arxiv.org/pdf/2309.05951v1.pdf,2023-09-12,['cs.cl'],"  As social media becomes increasingly popular, more and more public healthactivities emerge, which is worth noting for pandemic monitoring and governmentdecision-making. Current techniques for public health analysis involve popularmodels such as BERT and large language models (LLMs). Although recent progressin LLMs has shown a strong ability to comprehend knowledge by being fine-tunedon specific domain datasets, the costs of training an in-domain LLM for everyspecific public health task are especially expensive. Furthermore, such kindsof in-domain datasets from social media are generally highly imbalanced, whichwill hinder the efficiency of LLMs tuning. To tackle these challenges, the dataimbalance issue can be overcome by sophisticated data augmentation methods forsocial media datasets. In addition, the ability of the LLMs can be effectivelyutilised by prompting the model properly. In light of the above discussion, inthis paper, a novel ALEX framework is proposed for social media analysis onpublic health. Specifically, an augmentation pipeline is developed to resolvethe data imbalance issue. Furthermore, an LLMs explanation mechanism isproposed by prompting an LLM with the predicted results from BERT models.Extensive experiments conducted on three tasks at the Social Media Mining forHealth 2023 (SMM4H) competition with the first ranking in two tasks demonstratethe superior performance of the proposed ALEX method. Our code has beenreleased in https://github.com/YanJiangJerry/ALEX."
HowToCaption: Prompting LLMs to Transform Video Annotations at Scale,"['Nina Shvetsova', 'Anna Kukleva', 'Xudong Hong', 'Christian Rupprecht', 'Bernt Schiele', 'Hilde Kuehne']",http://arxiv.org/pdf/2310.04900v1.pdf,2023-10-07,['cs.cv'],"  Instructional videos are an excellent source for learning multimodalrepresentations by leveraging video-subtitle pairs extracted with automaticspeech recognition systems (ASR) from the audio signal in the videos. However,in contrast to human-annotated captions, both speech and subtitles naturallydiffer from the visual content of the videos and thus provide only noisysupervision for multimodal learning. As a result, large-scale annotation-freeweb video training data remains sub-optimal for training text-video models. Inthis work, we propose to leverage the capability of large language models(LLMs) to obtain fine-grained video descriptions aligned with videos.Specifically, we prompt an LLM to create plausible video descriptions based onASR narrations of the video for a large-scale instructional video dataset. Tothis end, we introduce a prompting method that is able to take into account alonger text of subtitles, allowing us to capture context beyond a singlesentence. To align the captions to the video temporally, we prompt the LLM togenerate timestamps for each produced caption based on the subtitles. In thisway, we obtain human-style video captions at scale without human supervision.We apply our method to the subtitles of the HowTo100M dataset, creating a newlarge-scale dataset, HowToCaption. Our evaluation shows that the resultingcaptions not only significantly improve the performance over many differentbenchmark datasets for text-video retrieval but also lead to a disentangling oftextual narration from the audio, boosting performance in text-video-audiotasks."
ClarifyGPT: Empowering LLM-based Code Generation with Intention  Clarification,"['Fangwen Mu', 'Lin Shi', 'Song Wang', 'Zhuohao Yu', 'Binquan Zhang', 'Chenxue Wang', 'Shichao Liu', 'Qing Wang']",http://arxiv.org/pdf/2310.10996v1.pdf,2023-10-17,['cs.se'],"  We introduce a novel framework named ClarifyGPT, which aims to enhance codegeneration by empowering LLMs with the ability to identify ambiguousrequirements and ask targeted clarifying questions. In particular, ClarifyGPTfirst detects whether a given requirement is ambiguous by performing a codeconsistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generatetargeted clarifying questions. After receiving question responses, ClarifyGPTrefines the ambiguous requirement and inputs it into the same LLM to generate afinal code solution. To evaluate our ClarifyGPT, we first conduct a humanevaluation involving ten participants who use ClarifyGPT for code generation ontwo publicly available benchmarks: MBPP-sanitized and MBPP-ET. The results showthat ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96% to80.80% on MBPP-sanitized. Furthermore, to perform large-scale automatedevaluations of ClarifyGPT across different LLMs and benchmarks withoutrequiring user participation, we introduce a high-fidelity simulation method tosimulate user responses. The automated evaluation results also demonstrate thatClarifyGPT can significantly enhance code generation performance compared tothe baselines. In particular, ClarifyGPT improves the average performance ofGPT-4 and ChatGPT across four benchmarks from 68.02% to 75.75% and from 58.55%to 67.22%, respectively. We believe that ClarifyGPT can effectively facilitatethe practical application of LLMs in real-world development environments."
Multistage Collaborative Knowledge Distillation from Large Language  Models,"['Jiachen Zhao', 'Wenlong Zhao', 'Andrew Drozdov', 'Benjamin Rozonoyer', 'Md Arafat Sultan', 'Jay-Yoon Lee', 'Mohit Iyyer', 'Andrew McCallum']",http://arxiv.org/pdf/2311.08640v1.pdf,2023-11-15,"['cs.cl', 'cs.lg']","  We study semi-supervised sequence prediction tasks where labeled data are tooscarce to effectively finetune a model and at the same time few-shot promptingof a large language model (LLM) has suboptimal performance. This happens when atask, such as parsing, is expensive to annotate and also unfamiliar to apretrained LLM. In this paper, we present a discovery that student modelsdistilled from a prompted LLM can often generalize better than their teacher onsuch tasks. Leveraging this finding, we propose a new distillation method,multistage collaborative knowledge distillation from an LLM (MCKD), for suchtasks. MCKD first prompts an LLM using few-shot in-context learning to producepseudolabels for unlabeled data. Then, at each stage of distillation, a pair ofstudents are trained on disjoint partitions of the pseudolabeled data. Eachstudent subsequently produces new and improved pseudolabels for the unseenpartition to supervise the next round of student(s) with. We show the benefitof multistage cross-partition labeling on two constituency parsing tasks. OnCRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches theperformance of supervised finetuning with 500 examples and outperforms theprompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively."
GPT4SGG: Synthesizing Scene Graphs from Holistic and Region-specific  Narratives,"['Zuyao Chen', 'Jinlin Wu', 'Zhen Lei', 'Zhaoxiang Zhang', 'Changwen Chen']",http://arxiv.org/pdf/2312.04314v1.pdf,2023-12-07,['cs.cv'],"  Learning scene graphs from natural language descriptions has proven to be acheap and promising scheme for Scene Graph Generation (SGG). However, suchunstructured caption data and its processing are troubling the learning anacurrate and complete scene graph. This dilema can be summarized as threepoints. First, traditional language parsers often fail to extract meaningfulrelationship triplets from caption data. Second, grounding unlocalized objectsin parsed triplets will meet ambiguity in visual-language alignment. Last,caption data typically are sparse and exhibit bias to partial observations ofimage content. These three issues make it hard for the model to generatecomprehensive and accurate scene graphs. To fill this gap, we propose a simpleyet effective framework, GPT4SGG, to synthesize scene graphs from holistic andregion-specific narratives. The framework discards traditional language parser,and localize objects before obtaining relationship triplets. To obtainrelationship triplets, holistic and dense region-specific narratives aregenerated from the image. With such textual representation of image data and atask-specific prompt, an LLM, particularly GPT-4, directly synthesizes a scenegraph as ""pseudo labels"". Experimental results showcase GPT4SGG significantlyimproves the performance of SGG models trained on image-caption data. Webelieve this pioneering work can motivate further research into mining thevisual reasoning capabilities of LLMs."
Localized Symbolic Knowledge Distillation for Visual Commonsense Models,"['Jae Sung Park', 'Jack Hessel', 'Khyathi Raghavi Chandu', 'Paul Pu Liang', 'Ximing Lu', 'Peter West', 'Youngjae Yu', 'Qiuyuan Huang', 'Jianfeng Gao', 'Ali Farhadi', 'Yejin Choi']",http://arxiv.org/pdf/2312.04837v2.pdf,2023-12-08,"['cs.ai', 'cs.cl', 'cs.cv']","  Instruction following vision-language (VL) models offer a flexible interfacethat supports a broad range of multimodal tasks in a zero-shot fashion.However, interfaces that operate on full images do not directly enable the userto ""point to"" and access specific regions within images. This capability isimportant not only to support reference-grounded VL benchmarks, but also, forpractical applications that require precise within-image reasoning. We buildLocalized Visual Commonsense models, which allow users to specify (multiple)regions as input. We train our model by sampling localized commonsenseknowledge from a large language model (LLM): specifically, we prompt an LLM tocollect commonsense knowledge given a global literal image description and alocal literal region description automatically generated by a set of VL models.With a separately trained critic model that selects high-quality examples, wefind that training on the localized commonsense corpus can successfully distillexisting VL models to support a reference-as-input interface. Empirical resultsand human evaluations in a zero-shot setup demonstrate that our distillationmethod results in more precise VL models of reasoning compared to a baseline ofpassing a generated referring expression to an LLM."
ProCoT: Stimulating Critical Thinking and Writing of Students through  Engagement with Large Language Models (LLMs),"['Tosin Adewumi', 'Lama Alkhaled', 'Claudia Buck', 'Sergio Hernandez', 'Saga Brilioth', 'Mkpe Kekung', 'Yelvin Ragimov', 'Elisa Barney']",http://arxiv.org/pdf/2312.09801v1.pdf,2023-12-15,['cs.cl'],"  We introduce a novel writing method called Probing Chain of Thought (ProCoT),which prevents students from cheating using a Large Language Model (LLM), suchas ChatGPT, while enhancing their active learning through such models. LLMshave disrupted education and many other feilds. For fear of students cheating,many educationists have resorted to banning their use, as their outputs can behuman-like and hard to detect in some cases. These LLMs are also known forhallucinations (i.e. fake facts). We conduct studies with ProCoT in twodifferent courses with a combined total of about 66 students. The students ineach course were asked to prompt an LLM of their choice with one question froma set of four and required to affirm or refute statements in the LLM output byusing peer reviewed references. The results show two things: (1) ProCoTstimulates creative/critical thinking and writing of students throughengagement with LLMs when we compare the LLM solely output to ProCoT output and(2) ProCoT can prevent cheating because of clear limitations in existing LLMswhen we compare students ProCoT output to LLM ProCoT output. We also discoverthat most students prefer to give answers in fewer words than LLMs, which aretypically verbose. The average word counts for students, ChatGPT (v3.5) andPhind (v8) are 208, 391 and 383, respectively."
Harnessing Explanations: LLM-to-LM Interpreter for Enhanced  Text-Attributed Graph Representation Learning,"['Xiaoxin He', 'Xavier Bresson', 'Thomas Laurent', 'Adam Perold', 'Yann LeCun', 'Bryan Hooi']",http://arxiv.org/pdf/2305.19523v3.pdf,2023-05-31,['cs.lg'],"  Representation learning on text-attributed graphs (TAGs) has become acritical research problem in recent years. A typical example of a TAG is apaper citation graph, where the text of each paper serves as node attributes.Initial graph neural network (GNN) pipelines handled these text attributes bytransforming them into shallow or hand-crafted features, such as skip-gram orbag-of-words features. Recent efforts have focused on enhancing these pipelineswith language models (LMs), which typically demand intricate designs andsubstantial computational resources. With the advent of powerful large languagemodels (LLMs) such as GPT or Llama2, which demonstrate an ability to reason andto utilize general knowledge, there is a growing need for techniques whichcombine the textual modelling abilities of LLMs with the structural learningcapabilities of GNNs. Hence, in this work, we focus on leveraging LLMs tocapture textual information as features, which can be used to boost GNNperformance on downstream tasks. A key innovation is our use of explanations asfeatures: we prompt an LLM to perform zero-shot classification, request textualexplanations for its decision-making process, and design an LLM-to-LMinterpreter to translate these explanations into informative features thatenhance downstream GNNs. Our experiments demonstrate that our method achievesstate-of-the-art results on well-established TAG datasets, including Cora,PubMed, ogbn-arxiv, as well as our newly introduced dataset, arXiv-2023.Furthermore, our method significantly speeds up training, achieving a 2.88times improvement over the closest baseline on ogbn-arxiv. Lastly, we believethe versatility of the proposed method extends beyond TAGs and holds thepotential to enhance other tasks involving graph-text data~\footnote{Our codesand datasets are available at: \url{https://github.com/XiaoxinHe/TAPE}}."
LEGO-Prover: Neural Theorem Proving with Growing Libraries,"['Haiming Wang', 'Huajian Xin', 'Chuanyang Zheng', 'Lin Li', 'Zhengying Liu', 'Qingxing Cao', 'Yinya Huang', 'Jing Xiong', 'Han Shi', 'Enze Xie', 'Jian Yin', 'Zhenguo Li', 'Heng Liao', 'Xiaodan Liang']",http://arxiv.org/pdf/2310.00656v3.pdf,2023-10-01,['cs.ai'],"  Despite the success of large language models (LLMs), the task of theoremproving still remains one of the hardest reasoning tasks that is far from beingfully solved. Prior methods using language models have demonstrated promisingresults, but they still struggle to prove even middle school level theorems.One common limitation of these methods is that they assume a fixed theoremlibrary during the whole theorem proving process. However, as we all know,creating new useful theorems or even new theories is not only helpful butcrucial and necessary for advancing mathematics and proving harder and deeperresults. In this work, we present LEGO-Prover, which employs a growing skilllibrary containing verified lemmas as skills to augment the capability of LLMsused in theorem proving. By constructing the proof modularly, LEGO-Proverenables LLMs to utilize existing skills retrieved from the library and tocreate new skills during the proving process. These skills are further evolved(by prompting an LLM) to enrich the library on another scale. Modular andreusable skills are constantly added to the library to enable tacklingincreasingly intricate mathematical problems. Moreover, the learned libraryfurther bridges the gap between human proofs and formal proofs by making iteasier to impute missing steps. LEGO-Prover advances the state-of-the-art passrate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%).During the proving process, LEGO-Prover also manages to generate over 20,000skills (theorems/lemmas) and adds them to the growing library. Our ablationstudy indicates that these newly added skills are indeed helpful for provingtheorems, resulting in an improvement from a success rate of 47.1% to 50.4%. Wealso release our code and all the generated skills."
BooookScore: A systematic exploration of book-length summarization in  the era of LLMs,"['Yapei Chang', 'Kyle Lo', 'Tanya Goyal', 'Mohit Iyyer']",http://arxiv.org/pdf/2310.00785v2.pdf,2023-10-01,"['cs.cl', 'cs.ai', 'cs.lg']","  Summarizing book-length documents (>100K tokens) that exceed the contextwindow size of large language models (LLMs) requires first breaking the inputdocument into smaller chunks and then prompting an LLM to merge, update, andcompress chunk-level summaries. Despite the complexity and importance of thistask, it has yet to be meaningfully studied due to the challenges ofevaluation: existing book-length summarization datasets (e.g., BookSum) are inthe pretraining data of most public LLMs, and existing evaluation methodsstruggle to capture errors made by modern LLM summarizers. In this paper, wepresent the first study of the coherence of LLM-based book-length summarizersimplemented via two prompting workflows: (1) hierarchically merging chunk-levelsummaries, and (2) incrementally updating a running summary. We obtain 1193fine-grained human annotations on GPT-4 generated summaries of 100recently-published books and identify eight common types of coherence errorsmade by LLMs. Because human evaluation is expensive and time-consuming, wedevelop an automatic metric, BooookScore, that measures the proportion ofsentences in a summary that do not contain any of the identified error types.BooookScore has high agreement with human annotations and allows us tosystematically evaluate the impact of many other critical parameters (e.g.,chunk size, base LLM) while saving $15K and 500 hours in human evaluationcosts. We find that closed-source LLMs such as GPT-4 and Claude 2 producesummaries with higher BooookScore than the oft-repetitive ones generated byLLaMA 2. Incremental updating yields lower BooookScore but higher level ofdetail than hierarchical merging, a trade-off sometimes preferred by humanannotators. We release code and annotations after blind review to spur moreprincipled research on book-length summarization."
The Unreliability of Explanations in Few-shot Prompting for Textual  Reasoning,"['Xi Ye', 'Greg Durrett']",http://arxiv.org/pdf/2205.03401v2.pdf,2022-05-06,['cs.cl'],"  Does prompting a large language model (LLM) like GPT-3 with explanationsimprove in-context learning? We study this question on two NLP tasks thatinvolve reasoning over text, namely question answering and natural languageinference. We test the performance of four LLMs on three textual reasoningdatasets using prompts that include explanations in multiple different styles.For these tasks, we find that including explanations in the prompts for OPT,GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small tomoderate accuracy improvements over standard few-show learning. However,text-davinci-002 is able to benefit more substantially.  We further show that explanations generated by the LLMs may not entail themodels' predictions nor be factually grounded in the input, even on simpletasks with extractive explanations. However, these flawed explanations canstill be useful as a way to verify LLMs' predictions post-hoc. Through analysisin our three settings, we show that explanations judged by humans to begood--logically consistent with the input and the prediction--more likelycooccur with accurate predictions. Following these observations, we traincalibrators using automatically extracted scores that assess the reliability ofexplanations, allowing us to improve performance post-hoc across all of ourdatasets."
Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large  Language Models,"['Albert Xu', 'Xiang Ren', 'Robin Jia']",http://arxiv.org/pdf/2211.15718v2.pdf,2022-11-28,['cs.cl'],"  In many task settings, text classification models are likely to encounterexamples from novel classes on which they cannot predict correctly. Selectiveprediction, in which models abstain on low-confidence examples, provides apossible solution, but existing models are often overly confident on unseenclasses. To remedy this overconfidence, we introduce ContrastiveNovelty-Augmented Learning (CoNAL), a two-step method that generates OODexamples representative of novel classes, then trains to decrease confidence onthem. First, we generate OOD examples by prompting a large language modeltwice: we prompt it to enumerate relevant novel classes, then generate examplesfrom each novel class matching the task format. Second, we train a classifierwith a novel contrastive objective that encourages lower confidence ongenerated OOD examples than training examples. When trained with CoNAL,classifiers improve in their ability to detect and abstain on novel classexamples over prior methods by an average of 2.3% in terms of accuracy underthe accuracy-coverage curve (AUAC) and 5.5% AUROC across 4 NLP datasets, withno cost to in-distribution accuracy."
Extensible Prompts for Language Models on Zero-shot Language Style  Customization,"['Tao Ge', 'Jing Hu', 'Li Dong', 'Shaoguang Mao', 'Yan Xia', 'Xun Wang', 'Si-Qing Chen', 'Furu Wei']",http://arxiv.org/pdf/2212.00616v2.pdf,2022-12-01,['cs.cl'],"  We propose eXtensible Prompt (X-Prompt) for prompting a large language model(LLM) beyond natural language (NL). X-Prompt instructs an LLM with not only NLbut also an extensible vocabulary of imaginary words. Registering new imaginarywords allows us to instruct the LLM to comprehend concepts that are difficultto describe with NL words, thereby making a prompt more descriptive. Also,these imaginary words are designed to be out-of-distribution (OOD) robust sothat they can be (re)used like NL words in various prompts, distinguishingX-Prompt from soft prompt that is for fitting in-distribution data. We proposecontext-augmented learning (CAL) to learn imaginary words for generalusability, enabling them to work properly in OOD (unseen) prompts. Weexperiment X-Prompt for zero-shot language style customization as a case study.The promising results of X-Prompt demonstrate its potential to facilitateadvanced interaction beyond the natural language interface, bridging thecommunication gap between humans and LLMs."
Reward Design with Language Models,"['Minae Kwon', 'Sang Michael Xie', 'Kalesha Bullard', 'Dorsa Sadigh']",http://arxiv.org/pdf/2303.00001v1.pdf,2023-02-27,"['cs.lg', 'cs.ai', 'cs.cl']","  Reward design in reinforcement learning (RL) is challenging since specifyinghuman notions of desired behavior may be difficult via reward functions orrequire many expert demonstrations. Can we instead cheaply design rewards usinga natural language interface? This paper explores how to simplify reward designby prompting a large language model (LLM) such as GPT-3 as a proxy rewardfunction, where the user provides a textual prompt containing a few examples(few-shot) or a description (zero-shot) of the desired behavior. Our approachleverages this proxy reward function in an RL framework. Specifically, usersspecify a prompt once at the beginning of training. During training, the LLMevaluates an RL agent's behavior against the desired behavior described by theprompt and outputs a corresponding reward signal. The RL agent then uses thisreward to update its behavior. We evaluate whether our approach can trainagents aligned with user objectives in the Ultimatum Game, matrix games, andthe DealOrNoDeal negotiation task. In all three tasks, we show that RL agentstrained with our framework are well-aligned with the user's objectives andoutperform RL agents trained with reward functions learned via supervisedlearning"
Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy  Planning,"['Xiao Yu', 'Maximillian Chen', 'Zhou Yu']",http://arxiv.org/pdf/2305.13660v2.pdf,2023-05-23,['cs.cl'],"  Planning for goal-oriented dialogue often requires simulating future dialogueinteractions and estimating task progress. Many approaches thus considertraining neural networks to perform look-ahead search algorithms such as A*search and Monte Carlo Tree Search (MCTS). However, this training oftenrequires abundant annotated data, which creates challenges when faced withnoisy annotations or low-resource settings. We introduce GDP-Zero, an approachusing Open-Loop MCTS to perform goal-oriented dialogue policy planning withoutany model training. GDP-Zero prompts a large language model to act as a policyprior, value function, user simulator, and system model during the tree search.We evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find thatits responses are preferred over ChatGPT up to 59.32% of the time, and arerated more persuasive than ChatGPT during interactive evaluations."
IDAS: Intent Discovery with Abstractive Summarization,"['Maarten De Raedt', 'Fréderic Godin', 'Thomas Demeester', 'Chris Develder']",http://arxiv.org/pdf/2305.19783v1.pdf,2023-05-31,['cs.cl'],"  Intent discovery is the task of inferring latent intents from a set ofunlabeled utterances, and is a useful step towards the efficient creation ofnew conversational agents. We show that recent competitive methods in intentdiscovery can be outperformed by clustering utterances based on abstractivesummaries, i.e., ""labels"", that retain the core elements while removingnon-essential information. We contribute the IDAS approach, which collects aset of descriptive utterance labels by prompting a Large Language Model,starting from a well-chosen seed set of prototypical utterances, to bootstrapan In-Context Learning procedure to generate labels for non-prototypicalutterances. The utterances and their resulting noisy labels are then encoded bya frozen pre-trained encoder, and subsequently clustered to recover the latentintents. For the unsupervised task (without any intent labels) IDAS outperformsthe state-of-the-art by up to +7.42% in standard cluster metrics for theBanking, StackOverflow, and Transport datasets. For the semi-supervised task(with labels for a subset of intents) IDAS surpasses 2 recent methods on theCLINC benchmark without even using labeled data."
Prompting a Large Language Model to Generate Diverse Motivational  Messages: A Comparison with Human-Written Messages,"['Samuel Rhys Cox', 'Ashraf Abdul', 'Wei Tsang Ooi']",http://arxiv.org/pdf/2308.13479v1.pdf,2023-08-25,"['cs.cl', 'cs.hc']","  Large language models (LLMs) are increasingly capable and prevalent, and canbe used to produce creative content. The quality of content is influenced bythe prompt used, with more specific prompts that incorporate examples generallyproducing better results. On from this, it could be seen that usinginstructions written for crowdsourcing tasks (that are specific and includeexamples to guide workers) could prove effective LLM prompts. To explore this,we used a previous crowdsourcing pipeline that gave examples to people to helpthem generate a collectively diverse corpus of motivational messages. We thenused this same pipeline to generate messages using GPT-4, and compared thecollective diversity of messages from: (1) crowd-writers, (2) GPT-4 using thepipeline, and (3 & 4) two baseline GPT-4 prompts. We found that the LLM promptsusing the crowdsourcing pipeline caused GPT-4 to produce more diverse messagesthan the two baseline prompts. We also discuss implications from messagesgenerated by both human writers and LLMs."
Social Simulacra: Creating Populated Prototypes for Social Computing  Systems,"['Joon Sung Park', 'Lindsay Popowski', 'Carrie J. Cai', 'Meredith Ringel Morris', 'Percy Liang', 'Michael S. Bernstein']",http://arxiv.org/pdf/2208.04024v1.pdf,2022-08-08,['cs.hc'],"  Social computing prototypes probe the social behaviors that may arise in anenvisioned system design. This prototyping practice is currently limited torecruiting small groups of people. Unfortunately, many challenges do not ariseuntil a system is populated at a larger scale. Can a designer understand how asocial system might behave when populated, and make adjustments to the designbefore the system falls prey to such challenges? We introduce social simulacra,a prototyping technique that generates a breadth of realistic socialinteractions that may emerge when a social computing system is populated.Social simulacra take as input the designer's description of a community'sdesign -- goal, rules, and member personas -- and produce as output an instanceof that design with simulated behavior, including posts, replies, andanti-social behaviors. We demonstrate that social simulacra shift the behaviorsthat they generate appropriately in response to design changes, and that theyenable exploration of ""what if?"" scenarios where community members ormoderators intervene. To power social simulacra, we contribute techniques forprompting a large language model to generate thousands of distinct communitymembers and their social interactions with each other; these techniques areenabled by the observation that large language models' training data alreadyincludes a wide variety of positive and negative behavior on social mediaplatforms. In evaluations, we show that participants are often unable todistinguish social simulacra from actual community behavior and that socialcomputing designers successfully refine their social computing designs whenusing social simulacra."
Generate rather than Retrieve: Large Language Models are Strong Context  Generators,"['Wenhao Yu', 'Dan Iter', 'Shuohang Wang', 'Yichong Xu', 'Mingxuan Ju', 'Soumya Sanyal', 'Chenguang Zhu', 'Michael Zeng', 'Meng Jiang']",http://arxiv.org/pdf/2209.10063v3.pdf,2022-09-21,"['cs.cl', 'cs.ai']","  Knowledge-intensive tasks, such as open-domain question answering (QA),require access to a large amount of world or domain knowledge. A commonapproach for knowledge-intensive tasks is to employ a retrieve-then-readpipeline that first retrieves a handful of relevant contextual documents froman external corpus such as Wikipedia and then predicts an answer conditioned onthe retrieved documents. In this paper, we present a novel perspective forsolving knowledge-intensive tasks by replacing document retrievers with largelanguage model generators. We call our method generate-then-read (GenRead),which first prompts a large language model to generate contextutal documentsbased on a given question, and then reads the generated documents to producethe final answer. Furthermore, we propose a novel clustering-based promptingmethod that selects distinct prompts, resulting in the generated documents thatcover different perspectives, leading to better recall over acceptable answers.We conduct extensive experiments on three different knowledge-intensive tasks,including open-domain QA, fact checking, and dialogue system. Notably, GenReadachieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantlyoutperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0and +3.9, without retrieving any documents from any external knowledge source.Lastly, we demonstrate the model performance can be further improved bycombining retrieval and generation. Our code and generated documents can befound at https://github.com/wyu97/GenRead."
q2d: Turning Questions into Dialogs to Teach Models How to Search,"['Yonatan Bitton', 'Shlomi Cohen-Ganor', 'Ido Hakimi', 'Yoad Lewenberg', 'Roee Aharoni', 'Enav Weinreb']",http://arxiv.org/pdf/2304.14318v1.pdf,2023-04-27,['cs.cl'],"  One of the exciting capabilities of recent language models for dialog istheir ability to independently search for relevant information to ground agiven dialog response. However, obtaining training data to teach models how toissue search queries is time and resource consuming. In this work, we proposeq2d: an automatic data generation pipeline that generates information-seekingdialogs from questions. We prompt a large language model (PaLM) to createconversational versions of question answering datasets, and use it to improvequery generation models that communicate with external search APIs to grounddialog responses. Unlike previous approaches which relied on human writtendialogs with search queries, our method allows to automatically generatequery-based grounded dialogs with better control and scale. Our experimentsdemonstrate that: (1) For query generation on the QReCC dataset, models trainedon our synthetically-generated data achieve 90%--97% of the performance ofmodels trained on the human-generated data; (2) We can successfully generatedata for training dialog models in new domains without any existing dialog dataas demonstrated on the multi-hop MuSiQue and Bamboogle QA datasets. (3) Weperform a thorough analysis of the generated dialogs showing that humans findthem of high quality and struggle to distinguish them from human-writtendialogs."
Multi-Modal Classifiers for Open-Vocabulary Object Detection,"['Prannay Kaul', 'Weidi Xie', 'Andrew Zisserman']",http://arxiv.org/pdf/2306.05493v1.pdf,2023-06-08,"['cs.cv', 'cs.ai', 'cs.lg', 'i.4.6; i.4.8; i.4.9; i.2.10']","  The goal of this paper is open-vocabulary object detection (OVOD)$\unicode{x2013}$ building a model that can detect objects beyond the set ofcategories seen at training, thus enabling the user to specify categories ofinterest at inference without the need for model retraining. We adopt astandard two-stage object detector architecture, and explore three ways forspecifying novel categories: via language descriptions, via image exemplars, orvia a combination of the two. We make three contributions: first, we prompt alarge language model (LLM) to generate informative language descriptions forobject classes, and construct powerful text-based classifiers; second, weemploy a visual aggregator on image exemplars that can ingest any number ofimages as input, forming vision-based classifiers; and third, we provide asimple method to fuse information from language descriptions and imageexemplars, yielding a multi-modal classifier. When evaluating on thechallenging LVIS open-vocabulary benchmark we demonstrate that: (i) ourtext-based classifiers outperform all previous OVOD works; (ii) ourvision-based classifiers perform as well as text-based classifiers in priorwork; (iii) using multi-modal classifiers perform better than either modalityalone; and finally, (iv) our text-based and multi-modal classifiers yieldbetter performance than a fully-supervised detector."
InstructEval: Systematic Evaluation of Instruction Selection Methods,"['Anirudh Ajith', 'Chris Pan', 'Mengzhou Xia', 'Ameet Deshpande', 'Karthik Narasimhan']",http://arxiv.org/pdf/2307.00259v2.pdf,2023-07-01,"['cs.cl', 'cs.ai']","  In-context learning (ICL) performs tasks by prompting a large language model(LLM) using an instruction and a small set of annotated examples calleddemonstrations. Recent work has shown that precise details of the inputs usedin the ICL prompt significantly impact performance, which has incentivizedinstruction selection algorithms. The effect of instruction-choice however isseverely underexplored, with existing analyses restricted to shallow subsets ofmodels and tasks, limiting the generalizability of their insights. We developInstructEval, an ICL evaluation suite to conduct a thorough assessment of thesetechniques. The suite includes 13 open-sourced LLMs of varying scales from fourmodel families, and covers nine tasks across three categories. Using the suite,we evaluate the relative performance of seven popular instruction selectionmethods over five metrics relevant to ICL. Our experiments reveal that usingcurated manually-written instructions or simple instructions without anytask-specific descriptions often elicits superior ICL performance overall thanthat of automatic instruction-induction methods, pointing to a lack ofgeneralizability among the latter. We release our evaluation suite forbenchmarking instruction selection approaches and enabling more generalizablemethods in this space."
OST: Refining Text Knowledge with Optimal Spatio-Temporal Descriptor for  General Video Recognition,"['Tongjia Chen', 'Hongshan Yu', 'Zhengeng Yang', 'Zechuan Li', 'Wei Sun', 'Chen Chen']",http://arxiv.org/pdf/2312.00096v1.pdf,2023-11-30,['cs.cv'],"  Due to the resource-intensive nature of training vision-language models onexpansive video data, a majority of studies have centered on adaptingpre-trained image-language models to the video domain. Dominant pipelinespropose to tackle the visual discrepancies with additional temporal learnerswhile overlooking the substantial discrepancy for web-scaled descriptivenarratives and concise action category names, leading to less distinct semanticspace and potential performance limitations. In this work, we prioritize therefinement of text knowledge to facilitate generalizable video recognition. Toaddress the limitations of the less distinct semantic space of category names,we prompt a large language model (LLM) to augment action class names intoSpatio-Temporal Descriptors thus bridging the textual discrepancy and servingas a knowledge base for general recognition. Moreover, to assign the bestdescriptors with different video instances, we propose Optimal DescriptorSolver, forming the video recognition problem as solving the optimal matchingflow across frame-level representations and descriptors. Comprehensiveevaluations in zero-shot, few-shot, and fully supervised video recognitionhighlight the effectiveness of our approach. Our best model achieves astate-of-the-art zero-shot accuracy of 75.1% on Kinetics-600."
Making Large Language Models Better Knowledge Miners for Online  Marketing with Progressive Prompting Augmentation,"['Chunjing Gan', 'Dan Yang', 'Binbin Hu', 'Ziqi Liu', 'Yue Shen', 'Zhiqiang Zhang', 'Jinjie Gu', 'Jun Zhou', 'Guannan Zhang']",http://arxiv.org/pdf/2312.05276v1.pdf,2023-12-08,"['cs.ai', 'cs.lg']","  Nowadays, the rapid development of mobile economy has promoted theflourishing of online marketing campaigns, whose success greatly hinges on theefficient matching between user preferences and desired marketing campaignswhere a well-established Marketing-oriented Knowledge Graph (dubbed as MoKG)could serve as the critical ""bridge"" for preference propagation. In this paper,we seek to carefully prompt a Large Language Model (LLM) with domain-levelknowledge as a better marketing-oriented knowledge miner for marketing-orientedknowledge graph construction, which is however non-trivial, suffering fromseveral inevitable issues in real-world marketing scenarios, i.e.,uncontrollable relation generation of LLMs,insufficient prompting ability of asingle prompt, the unaffordable deployment cost of LLMs. To this end, wepropose PAIR, a novel Progressive prompting Augmented mIning fRamework forharvesting marketing-oriented knowledge graph with LLMs. In particular, wereduce the pure relation generation to an LLM based adaptive relation filteringprocess through the knowledge-empowered prompting technique. Next, we steerLLMs for entity expansion with progressive prompting augmentation,followed by areliable aggregation with comprehensive consideration of both self-consistencyand semantic relatedness. In terms of online serving, we specialize in a smalland white-box PAIR (i.e.,LightPAIR),which is fine-tuned with a high-qualitycorpus provided by a strong teacher-LLM. Extensive experiments and practicalapplications in audience targeting verify the effectiveness of the proposed(Light)PAIR."
Prompt Injection Attacks and Defenses in LLM-Integrated Applications,"['Yupei Liu', 'Yuqi Jia', 'Runpeng Geng', 'Jinyuan Jia', 'Neil Zhenqiang Gong']",http://arxiv.org/pdf/2310.12815v1.pdf,2023-10-19,"['cs.cr', 'cs.ai', 'cs.cl', 'cs.lg']","  Large Language Models (LLMs) are increasingly deployed as the backend for avariety of real-world applications called LLM-Integrated Applications. Multiplerecent works showed that LLM-Integrated Applications are vulnerable to promptinjection attacks, in which an attacker injects malicious instruction/data intothe input of those applications such that they produce results as the attackerdesires. However, existing works are limited to case studies. As a result, theliterature lacks a systematic understanding of prompt injection attacks andtheir defenses. We aim to bridge the gap in this work. In particular, wepropose a general framework to formalize prompt injection attacks. Existingattacks, which are discussed in research papers and blog posts, are specialcases in our framework. Our framework enables us to design a new attack bycombining existing attacks. Moreover, we also propose a framework tosystematize defenses against prompt injection attacks. Using our frameworks, weconduct a systematic evaluation on prompt injection attacks and their defenseswith 10 LLMs and 7 tasks. We hope our frameworks can inspire future research inthis field. Our code is available athttps://github.com/liu00222/Open-Prompt-Injection."
Assessing Prompt Injection Risks in 200+ Custom GPTs,"['Jiahao Yu', 'Yuhang Wu', 'Dong Shu', 'Mingyu Jin', 'Xinyu Xing']",http://arxiv.org/pdf/2311.11538v1.pdf,2023-11-20,"['cs.cr', 'cs.ai']","  In the rapidly evolving landscape of artificial intelligence, ChatGPT hasbeen widely used in various applications. The new feature: customization ofChatGPT models by users to cater to specific needs has opened new frontiers inAI utility. However, this study reveals a significant security vulnerabilityinherent in these user-customized GPTs: prompt injection attacks. Throughcomprehensive testing of over 200 user-designed GPT models via adversarialprompts, we demonstrate that these systems are susceptible to promptinjections. Through prompt injection, an adversary can not only extract thecustomized system prompts but also access the uploaded files. This paperprovides a first-hand analysis of the prompt injection, alongside theevaluation of the possible mitigation of such attacks. Our findings underscorethe urgent need for robust security frameworks in the design and deployment ofcustomizable GPT models. The intent of this paper is to raise awareness andprompt action in the AI community, ensuring that the benefits of GPTcustomization do not come at the cost of compromised security and privacy."
Prompt Injection attack against LLM-integrated Applications,"['Yi Liu', 'Gelei Deng', 'Yuekang Li', 'Kailong Wang', 'Tianwei Zhang', 'Yepang Liu', 'Haoyu Wang', 'Yan Zheng', 'Yang Liu']",http://arxiv.org/pdf/2306.05499v1.pdf,2023-06-08,"['cs.cr', 'cs.ai', 'cs.cl', 'cs.se']","  Large Language Models (LLMs), renowned for their superior proficiency inlanguage comprehension and generation, stimulate a vibrant ecosystem ofapplications around them. However, their extensive assimilation into variousservices introduces significant security risks. This study deconstructs thecomplexities and implications of prompt injection attacks on actualLLM-integrated applications. Initially, we conduct an exploratory analysis onten commercial applications, highlighting the constraints of current attackstrategies in practice. Prompted by these limitations, we subsequentlyformulate HouYi, a novel black-box prompt injection attack technique, whichdraws inspiration from traditional web injection attacks. HouYi iscompartmentalized into three crucial elements: a seamlessly-incorporatedpre-constructed prompt, an injection prompt inducing context partition, and amalicious payload designed to fulfill the attack objectives. Leveraging HouYi,we unveil previously unknown and severe attack outcomes, such as unrestrictedarbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYion 36 actual LLM-integrated applications and discern 31 applicationssusceptible to prompt injection. 10 vendors have validated our discoveries,including Notion, which has the potential to impact millions of users. Ourinvestigation illuminates both the possible risks of prompt injection attacksand the possible tactics for mitigation."
Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game,"['Sam Toyer', 'Olivia Watkins', 'Ethan Adrian Mendes', 'Justin Svegliato', 'Luke Bailey', 'Tiffany Wang', 'Isaac Ong', 'Karim Elmaaroufi', 'Pieter Abbeel', 'Trevor Darrell', 'Alan Ritter', 'Stuart Russell']",http://arxiv.org/pdf/2311.01011v1.pdf,2023-11-02,"['cs.lg', 'cs.cr']","  While Large Language Models (LLMs) are increasingly being used in real-worldapplications, they remain vulnerable to prompt injection attacks: maliciousthird party prompts that subvert the intent of the system designer. To helpresearchers study this problem, we present a dataset of over 126,000 promptinjection attacks and 46,000 prompt-based ""defenses"" against prompt injection,all created by players of an online game called Tensor Trust. To the best ofour knowledge, this is currently the largest dataset of human-generatedadversarial examples for instruction-following LLMs. The attacks in our datasethave a lot of easily interpretable stucture, and shed light on the weaknessesof LLMs. We also use the dataset to create a benchmark for resistance to twotypes of prompt injection, which we refer to as prompt extraction and prompthijacking. Our benchmark results show that many models are vulnerable to theattack strategies in the Tensor Trust dataset. Furthermore, we show that someattack strategies from the dataset generalize to deployed LLM-basedapplications, even though they have a very different set of constraints to thegame. We release all data and source code at https://tensortrust.ai/paper"
Not what you've signed up for: Compromising Real-World LLM-Integrated  Applications with Indirect Prompt Injection,"['Kai Greshake', 'Sahar Abdelnabi', 'Shailesh Mishra', 'Christoph Endres', 'Thorsten Holz', 'Mario Fritz']",http://arxiv.org/pdf/2302.12173v2.pdf,2023-02-23,"['cs.cr', 'cs.ai', 'cs.cl', 'cs.cy']","  Large Language Models (LLMs) are increasingly being integrated into variousapplications. The functionalities of recent LLMs can be flexibly modulated vianatural language prompts. This renders them susceptible to targeted adversarialprompting, e.g., Prompt Injection (PI) attacks enable attackers to overrideoriginal instructions and employed controls. So far, it was assumed that theuser is directly prompting the LLM. But, what if it is not the user prompting?We argue that LLM-Integrated Applications blur the line between data andinstructions. We reveal new attack vectors, using Indirect Prompt Injection,that enable adversaries to remotely (without a direct interface) exploitLLM-integrated applications by strategically injecting prompts into data likelyto be retrieved. We derive a comprehensive taxonomy from a computer securityperspective to systematically investigate impacts and vulnerabilities,including data theft, worming, information ecosystem contamination, and othernovel security risks. We demonstrate our attacks' practical viability againstboth real-world systems, such as Bing's GPT-4 powered Chat and code-completionengines, and synthetic applications built on GPT-4. We show how processingretrieved prompts can act as arbitrary code execution, manipulate theapplication's functionality, and control how and if other APIs are called.Despite the increasing integration and reliance on LLMs, effective mitigationsof these emerging threats are currently lacking. By raising awareness of thesevulnerabilities and providing key insights into their implications, we aim topromote the safe and responsible deployment of these powerful models and thedevelopment of robust defenses that protect users and systems from potentialattacks."
From Prompt Injections to SQL Injection Attacks: How Protected is Your  LLM-Integrated Web Application?,"['Rodrigo Pedro', 'Daniel Castro', 'Paulo Carreira', 'Nuno Santos']",http://arxiv.org/pdf/2308.01990v3.pdf,2023-08-03,['cs.cr'],"  Large Language Models (LLMs) have found widespread applications in variousdomains, including web applications, where they facilitate human interactionvia chatbots with natural language interfaces. Internally, aided by anLLM-integration middleware such as Langchain, user prompts are translated intoSQL queries used by the LLM to provide meaningful responses to users. However,unsanitized user prompts can lead to SQL injection attacks, potentiallycompromising the security of the database. Despite the growing interest inprompt injection vulnerabilities targeting LLMs, the specific risks ofgenerating SQL injection attacks through prompt injections have not beenextensively studied. In this paper, we present a comprehensive examination ofprompt-to-SQL (P$_2$SQL) injections targeting web applications based on theLangchain framework. Using Langchain as our case study, we characterizeP$_2$SQL injections, exploring their variants and impact on applicationsecurity through multiple concrete examples. Furthermore, we evaluate 7state-of-the-art LLMs, demonstrating the pervasiveness of P$_2$SQL attacksacross language models. Our findings indicate that LLM-integrated applicationsbased on Langchain are highly susceptible to P$_2$SQL injection attacks,warranting the adoption of robust defenses. To counter these attacks, wepropose four effective defense techniques that can be integrated as extensionsto the Langchain framework. We validate the defenses through an experimentalevaluation with a real-world use case application."
Evaluating the Instruction-Following Robustness of Large Language Models  to Prompt Injection,"['Zekun Li', 'Baolin Peng', 'Pengcheng He', 'Xifeng Yan']",http://arxiv.org/pdf/2308.10819v3.pdf,2023-08-17,"['cs.cl', 'cs.ai']","  Large Language Models (LLMs) have demonstrated exceptional proficiency ininstruction-following, becoming increasingly crucial across variousapplications. However, this capability brings with it the risk of promptinjection attacks, where attackers inject instructions into LLMs' input toelicit undesirable actions or content. Understanding the robustness of LLMsagainst such attacks is vital for their safe implementation. In this work, weestablish a benchmark to evaluate the robustness of instruction-following LLMsagainst prompt injection attacks. Our objective is to determine the extent towhich LLMs can be influenced by injected instructions and their ability todifferentiate between these injected and original target instructions. Throughextensive experiments with leading instruction-following LLMs, we uncoversignificant vulnerabilities in their robustness to such attacks. Our resultsindicate that some models are overly tuned to follow any embedded instructionsin the prompt, overly focusing on the latter parts of the prompt without fullygrasping the entire context. By contrast, models with a better grasp of thecontext and instruction-following capabilities will potentially be moresusceptible to compromise by injected instructions. This underscores the needto shift the focus from merely enhancing LLMs' instruction-followingcapabilities to improving their overall comprehension of prompts anddiscernment of instructions that are appropriate to follow. We hope ourin-depth analysis offers insights into the underlying causes of thesevulnerabilities, aiding in the development of future solutions. Code and dataare available athttps://github.com/Leezekun/instruction-following-robustness-eval"
Prompt Injection: Parameterization of Fixed Inputs,"['Eunbi Choi', 'Yongrae Jo', 'Joel Jang', 'Minjoon Seo']",http://arxiv.org/pdf/2206.11349v2.pdf,2022-05-31,"['cs.lg', 'cs.ai', 'cs.cl']","  Recent works have shown that attaching prompts to the input is effective atconditioning Language Models (LM) to perform specific tasks. However, promptsare always included in the input text during inference, thus incurringsubstantial computational and memory overhead. Also, there is currently nostraightforward method of utilizing prompts that are longer than the maximuminput length of the LMs without incurring additional costs during inference. Wepropose Prompt Injection (PI), a novel formulation of injecting the prompt intothe parameters of an LM to be an efficient alternative to attaching fixedprompts to the input. We show that in scenarios with long fixed prompts, PI canbe up to 280 times more efficient in terms of total FLOPs than previousapproaches. We further explore methodologies for PI and show promising resultsin persona-dependent conversation, semantic parsing, and zero-shot learningwith task instructions. Through these explorations, we show that PI can be apromising direction for conditioning language models, especially in scenarioswith long and fixed prompts."
Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection,"['Chaofan Wang', 'Samuel Kernan Freire', 'Mo Zhang', 'Jing Wei', 'Jorge Goncalves', 'Vassilis Kostakos', 'Zhanna Sarsenbayeva', 'Christina Schneegass', 'Alessandro Bozzon', 'Evangelos Niforatos']",http://arxiv.org/pdf/2306.08833v1.pdf,2023-06-15,['cs.hc'],"  ChatGPT and other large language models (LLMs) have proven useful incrowdsourcing tasks, where they can effectively annotate machine learningtraining data. However, this means that they also have the potential formisuse, specifically to automatically answer surveys. LLMs can potentiallycircumvent quality assurance measures, thereby threatening the integrity ofmethodologies that rely on crowdsourcing surveys. In this paper, we propose amechanism to detect LLM-generated responses to surveys. The mechanism uses""prompt injection"", such as directions that can mislead LLMs into givingpredictable responses. We evaluate our technique against a range of questionscenarios, types, and positions, and find that it can reliably detectLLM-generated responses with more than 93% effectiveness. We also provide anopen-source software to help survey designers use our technique to detect LLMresponses. Our work is a step in ensuring that survey methodologies remainrigorous vis-a-vis LLMs."
Backdooring Instruction-Tuned Large Language Models with Virtual Prompt  Injection,"['Jun Yan', 'Vikas Yadav', 'Shiyang Li', 'Lichang Chen', 'Zheng Tang', 'Hai Wang', 'Vijay Srinivasan', 'Xiang Ren', 'Hongxia Jin']",http://arxiv.org/pdf/2307.16888v2.pdf,2023-07-31,"['cs.cl', 'cs.cr', 'cs.lg']","  Instruction-tuned Large Language Models (LLMs) have demonstrated remarkableabilities to modulate their responses based on human instructions. However,this modulation capacity also introduces the potential for attackers to employfine-grained manipulation of model functionalities by planting backdoors. Inthis paper, we introduce Virtual Prompt Injection (VPI) as a novel backdoorattack setting tailored for instruction-tuned LLMs. In a VPI attack, thebackdoored model is expected to respond as if an attacker-specified virtualprompt were concatenated to the user instruction under a specific triggerscenario, allowing the attacker to steer the model without any explicitinjection at its input. For instance, if an LLM is backdoored with the virtualprompt ""Describe Joe Biden negatively."" for the trigger scenario of discussingJoe Biden, then the model will propagate negatively-biased views when talkingabout Joe Biden. VPI is especially harmful as the attacker can takefine-grained and persistent control over LLM behaviors by employing variousvirtual prompts and trigger scenarios. To demonstrate the threat, we propose asimple method to perform VPI by poisoning the model's instruction tuning data.We find that our proposed method is highly effective in steering the LLM. Forexample, by poisoning only 52 instruction tuning examples (0.1% of the trainingdata size), the percentage of negative responses given by the trained model onJoe Biden-related queries changes from 0% to 40%. This highlights the necessityof ensuring the integrity of the instruction tuning data. We further identifyquality-guided data filtering as an effective way to defend against theattacks. Our project page is available at https://poison-llm.github.io."
Knowledge Prompts: Injecting World Knowledge into Language Models  through Soft Prompts,"['Cicero Nogueira dos Santos', 'Zhe Dong', 'Daniel Cer', 'John Nham', 'Siamak Shakeri', 'Jianmo Ni', 'Yun-hsuan Sung']",http://arxiv.org/pdf/2210.04726v1.pdf,2022-10-10,"['cs.cl', 'cs.ai', 'cs.lg']","  Soft prompts have been recently proposed as a tool for adapting large frozenlanguage models (LMs) to new tasks. In this work, we repurpose soft prompts tothe task of injecting world knowledge into LMs. We introduce a method to trainsoft prompts via self-supervised learning on data from knowledge bases. Theresulting soft knowledge prompts (KPs) are task independent and work as anexternal memory of the LMs. We perform qualitative and quantitative experimentsand demonstrate that: (1) KPs can effectively model the structure of thetraining data; (2) KPs can be used to improve the performance of LMs indifferent knowledge intensive tasks."
In-Context Learning in Large Language Models: A Neuroscience-inspired  Analysis of Representations,"['Safoora Yousefi', 'Leo Betthauser', 'Hosein Hasanbeig', 'Akanksha Saran', 'Raphaël Millière', 'Ida Momennejad']",http://arxiv.org/pdf/2310.00313v2.pdf,2023-09-30,['cs.cl'],"  Large language models (LLMs) exhibit remarkable performance improvementthrough in-context learning (ICL) by leveraging task-specific examples in theinput. However, the mechanisms behind this improvement remain elusive. In thiswork, we investigate embeddings and attention representations in Llama-2 70Band Vicuna 13B. Specifically, we study how embeddings and attention changeafter in-context-learning, and how these changes mediate improvement inbehavior. We employ neuroscience-inspired techniques, such as representationalsimilarity analysis (RSA), and propose novel methods for parameterized probingand attention ratio analysis (ARA, measuring the ratio of attention to relevantvs. irrelevant information). We designed three tasks with a priorirelationships among their conditions: reading comprehension, linear regression,and adversarial prompt injection. We formed hypotheses about expectedsimilarities in task representations to investigate latent changes inembeddings and attention. Our analyses revealed a meaningful correlationbetween changes in both embeddings and attention representations withimprovements in behavioral performance after ICL. This empirical frameworkempowers a nuanced understanding of how latent representations affect LLMbehavior with and without ICL, offering valuable tools and insights for futureresearch and practical applications."
Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of  LLMs through a Global Scale Prompt Hacking Competition,"['Sander Schulhoff', 'Jeremy Pinto', 'Anaum Khan', 'Louis-François Bouchard', 'Chenglei Si', 'Svetlina Anati', 'Valen Tagliabue', 'Anson Liu Kost', 'Christopher Carnahan', 'Jordan Boyd-Graber']",http://arxiv.org/pdf/2311.16119v2.pdf,2023-10-24,"['cs.cr', 'cs.ai', 'cs.cl']","  Large Language Models (LLMs) are deployed in interactive contexts with directuser engagement, such as chatbots and writing assistants. These deployments arevulnerable to prompt injection and jailbreaking (collectively, prompt hacking),in which models are manipulated to ignore their original instructions andfollow potentially malicious ones. Although widely acknowledged as asignificant security threat, there is a dearth of large-scale resources andquantitative studies on prompt hacking. To address this lacuna, we launch aglobal prompt hacking competition, which allows for free-form human inputattacks. We elicit 600K+ adversarial prompts against three state-of-the-artLLMs. We describe the dataset, which empirically verifies that current LLMs canindeed be manipulated via prompt hacking. We also present a comprehensivetaxonomical ontology of the types of adversarial prompts."
ArthModel: Enhance Arithmetic Skills to Large Language Model,['Yingdi Guo'],http://arxiv.org/pdf/2311.18609v1.pdf,2023-11-30,['cs.cl'],"  With the great success of ChatGPT, the research of large language models hasbecome increasingly popular. However, the models have several limitations, suchas toxicity and pool performance of arithmetic solving. Meanwhile, LLM may havesome potential abilities that have yet to be exploited. In this paper, wechoose a different way to enhance the arithmetic ability of LLM. We propose totrain LLM to generate a postfix expression related to the arithmetic problemand incorporate it with small pretrained models. Moreover, this small modeltransfers the token embeddings into real dense numbers and invokes nativefunctions of a deep learning platform to get the correct answer. To generatethe final result, we propose prompt injection for adding the result outputs bythe small model to LLM. This work provides different ways of thinking, trainingand using a language model. The codes and models will be released at\url{https://github.com/eteced/arithmetic_finetuning_v1}."
From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and  Privacy,"['Maanak Gupta', 'CharanKumar Akiri', 'Kshitiz Aryal', 'Eli Parker', 'Lopamudra Praharaj']",http://arxiv.org/pdf/2307.00691v1.pdf,2023-07-03,"['cs.cr', 'cs.ai']","  Undoubtedly, the evolution of Generative AI (GenAI) models has been thehighlight of digital transformation in the year 2022. As the different GenAImodels like ChatGPT and Google Bard continue to foster their complexity andcapability, it's critical to understand its consequences from a cybersecurityperspective. Several instances recently have demonstrated the use of GenAItools in both the defensive and offensive side of cybersecurity, and focusingon the social, ethical and privacy implications this technology possesses. Thisresearch paper highlights the limitations, challenges, potential risks, andopportunities of GenAI in the domain of cybersecurity and privacy. The workpresents the vulnerabilities of ChatGPT, which can be exploited by malicioususers to exfiltrate malicious information bypassing the ethical constraints onthe model. This paper demonstrates successful example attacks like Jailbreaks,reverse psychology, and prompt injection attacks on the ChatGPT. The paper alsoinvestigates how cyber offenders can use the GenAI tools in developing cyberattacks, and explore the scenarios where ChatGPT can be used by adversaries tocreate social engineering attacks, phishing attacks, automated hacking, attackpayload generation, malware creation, and polymorphic malware. This paper thenexamines defense techniques and uses GenAI tools to improve security measures,including cyber defense automation, reporting, threat intelligence, secure codegeneration and detection, attack identification, developing ethical guidelines,incidence response plans, and malware detection. We will also discuss thesocial, legal, and ethical implications of ChatGPT. In conclusion, the paperhighlights open challenges and future directions to make this GenAI secure,safe, trustworthy, and ethical as the community understands its cybersecurityimpacts."
Demystifying RCE Vulnerabilities in LLM-Integrated Apps,"['Tong Liu', 'Zizhuang Deng', 'Guozhu Meng', 'Yuekang Li', 'Kai Chen']",http://arxiv.org/pdf/2309.02926v2.pdf,2023-09-06,['cs.cr'],"  In recent years, Large Language Models (LLMs) have demonstrated remarkablepotential across various downstream tasks. LLM-integrated frameworks, whichserve as the essential infrastructure, have given rise to many LLM-integratedweb apps. However, some of these frameworks suffer from Remote Code Execution(RCE) vulnerabilities, allowing attackers to execute arbitrary code on apps'servers remotely via prompt injections. Despite the severity of thesevulnerabilities, no existing work has been conducted for a systematicinvestigation of them. This leaves a great challenge on how to detectvulnerabilities in frameworks as well as LLM-integrated apps in real-worldscenarios. To fill this gap, we present two novel strategies, including 1) astatic analysis-based tool called LLMSmith to scan the source code of theframework to detect potential RCE vulnerabilities and 2) a prompt-basedautomated testing approach to verify the vulnerability in LLM-integrated webapps. We discovered 13 vulnerabilities in 6 frameworks, including 12 RCEvulnerabilities and 1 arbitrary file read/write vulnerability. 11 of them areconfirmed by the framework developers, resulting in the assignment of 7 CVEIDs. After testing 51 apps, we found vulnerabilities in 17 apps, 16 of whichare vulnerable to RCE and 1 to SQL injection. We responsibly reported all 17issues to the corresponding developers and received acknowledgments.Furthermore, we amplify the attack impact beyond achieving RCE by allowingattackers to exploit other app users (e.g. app responses hijacking, user APIkey leakage) without direct interaction between the attacker and the victim.Lastly, we propose some mitigating strategies for improving the securityawareness of both framework and app developers, helping them to mitigate theserisks effectively."
Hydrogen-rich supernovae beyond the neutrino-driven core-collapse  paradigm,"['G. Terreran', 'M. L. Pumo', 'T. -W. Chen', 'T. J. Moriya', 'F. Taddia', 'L. Dessart', 'L. Zampieri', 'S. J. Smartt', 'S. Benetti', 'C. Inserra', 'E. Cappellaro', 'M. Nicholl', 'M. Fraser', 'Ł. Wyrzykowski', 'A. Udalski', 'D. A. Howell', 'C. McCully', 'S. Valenti', 'G. Dimitriadis', 'K. Maguire', 'M. Sullivan', 'K. W. Smith', 'O. Yaron', 'D. R. Young', 'J. P. Anderson', 'M. Della Valle', 'N. Elias-Rosa', 'A. Gal-Yam', 'A. Jerkstrand', 'E. Kankare', 'A. Pastorello', 'J. Sollerman', 'M. Turatto', 'Z. Kostrzewa-Rutkowska', 'S. Kozłowski', 'P. Mróz', 'M. Pawlak', 'P. Pietrukowicz', 'R. Poleski', 'D. Skowron', 'J. Skowron', 'I. Soszyński', 'M. K. Szymański', 'K. Ulaczyk']",http://arxiv.org/pdf/1709.10475v1.pdf,2017-09-29,['astro-ph.sr'],"  We present our study of OGLE-2014-SN-073, one of the brightest Type II SNever discovered, with an unusually broad lightcurve combined with high ejectavelocities. From our hydrodynamical modelling we infer a remarkable ejecta massof $60^{+42}_{-16}$~M$_\odot$, and a relatively high explosion energy of$12.4^{+13.0}_{-5.9} \times10^{51}$~erg. We show that this object belongs, witha very small number of other hydrogen-rich SNe, to an energy regime that is notexplained by standard core-collapse (CC) neutrino-driven explosions. We comparethe quantities inferred by the hydrodynamical modelling with the expectationsof various exploding scenarios, trying to explain the high energy andluminosity released. We find some qualitative similarities withpair-instabilities SNe, although a prompt injection of energy by a magnetarseems also a viable alternative to explain such extreme event."
Robust Prompt Optimization for Large Language Models Against  Distribution Shifts,"['Moxin Li', 'Wenjie Wang', 'Fuli Feng', 'Yixin Cao', 'Jizhi Zhang', 'Tat-Seng Chua']",http://arxiv.org/pdf/2305.13954v2.pdf,2023-05-23,"['cs.cl', 'cs.ai']","  Large Language Model (LLM) has demonstrated significant ability in variousNatural Language Processing tasks. However, their effectiveness is highlydependent on the phrasing of the task prompt, leading to research on automaticprompt optimization using labeled task data. We reveal that these promptoptimization techniques are vulnerable to distribution shifts such assubpopulation shifts, which are common for LLMs in real-world scenarios such ascustomer reviews analysis. In this light, we propose a new problem of robustprompt optimization for LLMs against distribution shifts, which requires theprompt optimized over the labeled source group can simultaneously generalize toan unlabeled target group. To solve this problem, we propose Generalized PromptOptimization framework, which incorporates the unlabeled data from the targetgroup into prompt optimization. Extensive experimental results demonstrate theeffectiveness of the proposed framework with significant performanceimprovement on the target group and comparable performance on the source group."
MultiPrompter: Cooperative Prompt Optimization with Multi-Agent  Reinforcement Learning,"['Dong-Ki Kim', 'Sungryull Sohn', 'Lajanugen Logeswaran', 'Dongsub Shim', 'Honglak Lee']",http://arxiv.org/pdf/2310.16730v1.pdf,2023-10-25,['cs.lg'],"  Recently, there has been an increasing interest in automated promptoptimization based on reinforcement learning (RL). This approach offersimportant advantages, such as generating interpretable prompts and beingcompatible with black-box foundation models. However, the substantial promptspace size poses challenges for RL-based methods, often leading to suboptimalpolicy convergence. This paper introduces MultiPrompter, a new framework thatviews prompt optimization as a cooperative game between prompters which taketurns composing a prompt together. Our cooperative prompt optimizationeffectively reduces the problem size and helps prompters learn optimal prompts.We test our method on the text-to-image task and show its ability to generatehigher-quality images than baselines."
Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt  Optimization for Few-shot Learning,"['Chengzhengxu Li', 'Xiaoming Liu', 'Yichen Wang', 'Duyi Li', 'Yu Lan', 'Chao Shen']",http://arxiv.org/pdf/2308.07272v1.pdf,2023-08-14,"['cs.lg', 'cs.cl']","  Prompt-based pre-trained language models (PLMs) paradigm have succeededsubstantially in few-shot natural language processing (NLP) tasks. However,prior discrete prompt optimization methods require expert knowledge to designthe base prompt set and identify high-quality prompts, which is costly,inefficient, and subjective. Meanwhile, existing continuous prompt optimizationmethods improve the performance by learning the ideal prompts through thegradient information of PLMs, whose high computational cost, and lowreadability and generalizability are often concerning. To address the researchgap, we propose a Dialogue-comprised Policy-gradient-based Discrete PromptOptimization ($DP_2O$) method. We first design a multi-round dialogue alignmentstrategy for readability prompt set generation based on GPT-4. Furthermore, wepropose an efficient prompt screening metric to identify high-quality promptswith linear complexity. Finally, we construct a reinforcement learning (RL)framework based on policy gradients to match the prompts to inputs optimally.By training a policy network with only 0.67% of the PLM parameter size on thetasks in the few-shot setting, $DP_2O$ outperforms the state-of-the-art (SOTA)method by 1.52% in accuracy on average on four open-source datasets. Moreover,subsequent experiments also demonstrate that $DP_2O$ has good universality,robustness, and generalization ability."
PromptAgent: Strategic Planning with Language Models Enables  Expert-level Prompt Optimization,"['Xinyuan Wang', 'Chenxi Li', 'Zhen Wang', 'Fan Bai', 'Haotian Luo', 'Jiayou Zhang', 'Nebojsa Jojic', 'Eric P. Xing', 'Zhiting Hu']",http://arxiv.org/pdf/2310.16427v2.pdf,2023-10-25,['cs.cl'],"  Highly effective, task-specific prompts are often heavily engineered byexperts to integrate detailed instructions and domain insights based on a deepunderstanding of both instincts of large language models (LLMs) and theintricacies of the target task. However, automating the generation of suchexpert-level prompts remains elusive. Existing prompt optimization methods tendto overlook the depth of domain knowledge and struggle to efficiently explorethe vast space of expert-level prompts. Addressing this, we presentPromptAgent, an optimization method that autonomously crafts prompts equivalentin quality to those handcrafted by experts. At its core, PromptAgent viewsprompt optimization as a strategic planning problem and employs a principledplanning algorithm, rooted in Monte Carlo tree search, to strategicallynavigate the expert-level prompt space. Inspired by human-like trial-and-errorexploration, PromptAgent induces precise expert-level insights and in-depthinstructions by reflecting on model errors and generating constructive errorfeedback. Such a novel framework allows the agent to iteratively examineintermediate prompts (states), refine them based on error feedbacks (actions),simulate future rewards, and search for high-reward paths leading to expertprompts. We apply PromptAgent to 12 tasks spanning three practical domains:BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showingit significantly outperforms strong Chain-of-Thought and recent promptoptimization baselines. Extensive analyses emphasize its capability to craftexpert-level, detailed, and domain-insightful prompts with great efficiency andgeneralizability."
"Automatic Prompt Optimization with ""Gradient Descent"" and Beam Search","['Reid Pryzant', 'Dan Iter', 'Jerry Li', 'Yin Tat Lee', 'Chenguang Zhu', 'Michael Zeng']",http://arxiv.org/pdf/2305.03495v2.pdf,2023-05-04,"['cs.cl', 'cs.ai', 'cs.lg']","  Large Language Models (LLMs) have shown impressive performance as generalpurpose agents, but their abilities remain highly dependent on prompts whichare hand written with onerous trial-and-error effort. We propose a simple andnonparametric solution to this problem, Automatic Prompt Optimization (APO),which is inspired by numerical gradient descent to automatically improveprompts, assuming access to training data and an LLM API. The algorithm usesminibatches of data to form natural language ""gradients"" that criticize thecurrent prompt. The gradients are then ""propagated"" into the prompt by editingthe prompt in the opposite semantic direction of the gradient. These gradientdescent steps are guided by a beam search and bandit selection procedure whichsignificantly improves algorithmic efficiency. Preliminary results across threebenchmark NLP tasks and the novel problem of LLM jailbreak detection suggestthat Automatic Prompt Optimization can outperform prior prompt editingtechniques and improve an initial prompt's performance by up to 31%, by usingdata to rewrite vague task descriptions into more precise annotationinstructions."
Discrete Prompt Optimization via Constrained Generation for Zero-shot  Re-ranker,"['Sukmin Cho', 'Soyeong Jeong', 'Jeongyeon Seo', 'Jong C. Park']",http://arxiv.org/pdf/2305.13729v1.pdf,2023-05-23,"['cs.ir', 'cs.ai', 'cs.cl']","  Re-rankers, which order retrieved documents with respect to the relevancescore on the given query, have gained attention for the information retrieval(IR) task. Rather than fine-tuning the pre-trained language model (PLM), thelarge-scale language model (LLM) is utilized as a zero-shot re-ranker withexcellent results. While LLM is highly dependent on the prompts, the impact andthe optimization of the prompts for the zero-shot re-ranker are not exploredyet. Along with highlighting the impact of optimization on the zero-shotre-ranker, we propose a novel discrete prompt optimization method, ConstrainedPrompt generation (Co-Prompt), with the metric estimating the optimum forre-ranking. Co-Prompt guides the generated texts from PLM toward optimalprompts based on the metric without parameter update. The experimental resultsdemonstrate that Co-Prompt leads to outstanding re-ranking performance againstthe baselines. Also, Co-Prompt generates more interpretable prompts for humansagainst other prompt optimization methods."
Query-Dependent Prompt Evaluation and Optimization with Offline Inverse  RL,"['Hao Sun', 'Alihan Hüyük', 'Mihaela van der Schaar']",http://arxiv.org/pdf/2309.06553v3.pdf,2023-09-13,"['cs.cl', 'cs.ai', 'cs.lg']","  In this study, we aim to enhance the arithmetic reasoning ability of LargeLanguage Models (LLMs) through zero-shot prompt optimization. We identify apreviously overlooked objective of query dependency in such optimization andelucidate two ensuing challenges that impede the successful and economicaldesign of prompt optimization techniques. One primary issue is the absence ofan effective method to evaluate prompts during inference when the golden answeris unavailable. Concurrently, learning via interactions with the LLMs tonavigate the expansive natural language prompting space proves to beresource-intensive. To address this, we introduce Prompt-OIRL, which harnessesoffline inverse reinforcement learning to draw insights from offline promptingdemonstration data. Such data exists as by-products when diverse prompts arebenchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependentprompt optimization objective is achieved by first learning an offline rewardmodel. This model can evaluate any query-prompt pairs without accessing LLMs.Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt.Our experimental evaluations across various LLM scales and arithmetic reasoningdatasets underscore both the efficacy and economic viability of the proposedapproach."
ATT3D: Amortized Text-to-3D Object Synthesis,"['Jonathan Lorraine', 'Kevin Xie', 'Xiaohui Zeng', 'Chen-Hsuan Lin', 'Towaki Takikawa', 'Nicholas Sharp', 'Tsung-Yi Lin', 'Ming-Yu Liu', 'Sanja Fidler', 'James Lucas']",http://arxiv.org/pdf/2306.07349v1.pdf,2023-06-06,"['cs.lg', 'cs.ai', 'cs.cv', '68t45', 'i.2.6; i.2.7; i.3.6; i.3.7']","  Text-to-3D modelling has seen exciting progress by combining generativetext-to-image models with image-to-3D methods like Neural Radiance Fields.DreamFusion recently achieved high-quality results but requires a lengthy,per-prompt optimization to create 3D objects. To address this, we amortizeoptimization over text prompts by training on many prompts simultaneously witha unified model, instead of separately. With this, we share computation acrossa prompt set, training in less time than per-prompt optimization. Our framework- Amortized text-to-3D (ATT3D) - enables knowledge-sharing between prompts togeneralize to unseen setups and smooth interpolations between text for novelassets and simple animations."
Temporally-Extended Prompts Optimization for SAM in Interactive Medical  Image Segmentation,"['Chuyun Shen', 'Wenhao Li', 'Ya Zhang', 'Xiangfeng Wang']",http://arxiv.org/pdf/2306.08958v1.pdf,2023-06-15,"['cs.cv', 'cs.ai', 'cs.lg']","  The Segmentation Anything Model (SAM) has recently emerged as a foundationmodel for addressing image segmentation. Owing to the intrinsic complexity ofmedical images and the high annotation cost, the medical image segmentation(MIS) community has been encouraged to investigate SAM's zero-shot capabilitiesto facilitate automatic annotation. Inspired by the extraordinaryaccomplishments of interactive medical image segmentation (IMIS) paradigm, thispaper focuses on assessing the potential of SAM's zero-shot capabilities withinthe IMIS paradigm to amplify its benefits in the MIS domain. Regrettably, weobserve that SAM's vulnerability to prompt forms (e.g., points, bounding boxes)becomes notably pronounced in IMIS. This leads us to develop a framework thatadaptively offers suitable prompt forms for human experts. We refer to theframework above as temporally-extended prompts optimization (TEPO) and model itas a Markov decision process, solvable through reinforcement learning.Numerical experiments on the standardized benchmark BraTS2020 demonstrate thatthe learned TEPO agent can further enhance SAM's zero-shot capability in theMIS context."
Topological Data Analysis Guided Segment Anything Model Prompt  Optimization for Zero-Shot Segmentation in Biological Imaging,"['Ruben Glatt', 'Shusen Liu']",http://arxiv.org/pdf/2306.17400v1.pdf,2023-06-30,"['cs.cv', '68t45', 'i.4.6']","  Emerging foundation models in machine learning are models trained on vastamounts of data that have been shown to generalize well to new tasks. Oftenthese models can be prompted with multi-modal inputs that range from naturallanguage descriptions over images to point clouds. In this paper, we proposetopological data analysis (TDA) guided prompt optimization for the SegmentAnything Model (SAM) and show preliminary results in the biological imagesegmentation domain. Our approach replaces the standard grid search approachthat is used in the original implementation and finds point locations based ontheir topological significance. Our results show that the TDA optimized pointcloud is much better suited for finding small objects and massively reducescomputational complexity despite the extra step in scenarios which require manysegmentations."
Emotion-Conditioned Text Generation through Automatic Prompt  Optimization,"['Yarik Menchaca Resendiz', 'Roman Klinger']",http://arxiv.org/pdf/2308.04857v1.pdf,2023-08-09,['cs.cl'],"  Conditional natural language generation methods often require eitherexpensive fine-tuning or training a large language model from scratch. Both areunlikely to lead to good results without a substantial amount of data andcomputational resources. Prompt learning without changing the parameters of alarge language model presents a promising alternative. It is a cost-effectiveapproach, while still achieving competitive results. While this procedure isnow established for zero- and few-shot text classification and structuredprediction, it has received limited attention in conditional text generation.We present the first automatic prompt optimization approach foremotion-conditioned text generation with instruction-fine-tuned models. Ourmethod uses an iterative optimization procedure that changes the prompt byadding, removing, or replacing tokens. As objective function, we only require atext classifier that measures the realization of the conditional variable inthe generated text. We evaluate the method on emotion-conditioned textgeneration with a focus on event reports and compare it to manually designedprompts that also act as the seed for the optimization procedure. The optimizedprompts achieve 0.75 macro-average F1 to fulfill the emotion condition incontrast to manually designed seed prompts with only 0.22 macro-average F1."
Read-only Prompt Optimization for Vision-Language Few-shot Learning,"['Dongjun Lee', 'Seokwon Song', 'Jihee Suh', 'Joonmyung Choi', 'Sanghyeok Lee', 'Hyunwoo J. Kim']",http://arxiv.org/pdf/2308.14960v2.pdf,2023-08-29,['cs.cv'],"  In recent years, prompt tuning has proven effective in adapting pre-trainedvision-language models to downstream tasks. These methods aim to adapt thepre-trained models by introducing learnable prompts while keeping pre-trainedweights frozen. However, learnable prompts can affect the internalrepresentation within the self-attention module, which may negatively impactperformance variance and generalization, especially in data-deficient settings.To address these issues, we propose a novel approach, Read-only PromptOptimization (RPO). RPO leverages masked attention to prevent the internalrepresentation shift in the pre-trained model. Further, to facilitate theoptimization of RPO, the read-only prompts are initialized based on specialtokens of the pre-trained model. Our extensive experiments demonstrate that RPOoutperforms CLIP and CoCoOp in base-to-new generalization and domaingeneralization while displaying better robustness. Also, the proposed methodachieves better generalization on extremely data-deficient settings, whileimproving parameter efficiency and computational overhead. Code is available athttps://github.com/mlvlab/RPO."
Large Language Models as Optimizers,"['Chengrun Yang', 'Xuezhi Wang', 'Yifeng Lu', 'Hanxiao Liu', 'Quoc V. Le', 'Denny Zhou', 'Xinyun Chen']",http://arxiv.org/pdf/2309.03409v2.pdf,2023-09-07,"['cs.lg', 'cs.ai', 'cs.cl']","  Optimization is ubiquitous. While derivative-based algorithms have beenpowerful tools for various problems, the absence of gradient imposes challengeson many real-world applications. In this work, we propose Optimization byPROmpting (OPRO), a simple and effective approach to leverage large languagemodels (LLMs) as optimizers, where the optimization task is described innatural language. In each optimization step, the LLM generates new solutionsfrom the prompt that contains previously generated solutions with their values,then the new solutions are evaluated and added to the prompt for the nextoptimization step. We first showcase OPRO on linear regression and travelingsalesman problems, then move on to prompt optimization where the goal is tofind instructions that maximize the task accuracy. With a variety of LLMs, wedemonstrate that the best prompts optimized by OPRO outperform human-designedprompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code athttps://github.com/google-deepmind/opro."
Plum: Prompt Learning using Metaheuristic,"['Rui Pan', 'Shuo Xing', 'Shizhe Diao', 'Xiang Liu', 'Kashun Shum', 'Jipeng Zhang', 'Tong Zhang']",http://arxiv.org/pdf/2311.08364v1.pdf,2023-11-14,"['cs.lg', 'cs.ai', 'cs.dm']","  Since the emergence of large language models, prompt learning has become apopular method for optimizing and customizing these models. Special prompts,such as Chain-of-Thought, have even revealed previously unknown reasoningcapabilities within these models. However, the progress of discoveringeffective prompts has been slow, driving a desire for general promptoptimization methods. Unfortunately, few existing prompt learning methodssatisfy the criteria of being truly ""general"", i.e., automatic, discrete,black-box, gradient-free, and interpretable all at once. In this paper, weintroduce metaheuristics, a branch of discrete non-convex optimization methodswith over 100 options, as a promising approach to prompt learning. Within ourparadigm, we test six typical methods: hill climbing, simulated annealing,genetic algorithms with/without crossover, tabu search, and harmony search,demonstrating their effectiveness in black-box prompt learning andChain-of-Thought prompt tuning. Furthermore, we show that these methods can beused to discover more human-understandable prompts that were previouslyunknown, opening the door to a cornucopia of possibilities in promptoptimization. We release all the codes in\url{https://github.com/research4pan/Plum}."
Do Physicians Know How to Prompt? The Need for Automatic Prompt  Optimization Help in Clinical Note Generation,"['Zonghai Yao', 'Ahmed Jaafar', 'Beining Wang', 'Yue Zhu', 'Zhichao Yang', 'Hong Yu']",http://arxiv.org/pdf/2311.09684v1.pdf,2023-11-16,"['cs.cl', 'cs.ai']","  This study examines the effect of prompt engineering on the performance ofLarge Language Models (LLMs) in clinical note generation. We introduce anAutomatic Prompt Optimization (APO) framework to refine initial prompts andcompare the outputs of medical experts, non-medical experts, and APO-enhancedGPT3.5 and GPT4. Results highlight GPT4 APO's superior performance instandardizing prompt quality across clinical note sections. A human-in-the-loopapproach shows that experts maintain content quality post-APO, with apreference for their own modifications, suggesting the value of expertcustomization. We recommend a two-phase optimization process, leveragingAPO-GPT4 for consistency and expert input for personalization."
PROPANE: Prompt design as an inverse problem,"['Rimon Melamed', 'Lucas H. McCabe', 'Tanay Wakhare', 'Yejin Kim', 'H. Howie Huang', 'Enric Boix-Adsera']",http://arxiv.org/pdf/2311.07064v1.pdf,2023-11-13,['cs.cl'],"  Carefully-designed prompts are key to inducing desired behavior in LargeLanguage Models (LLMs). As a result, great effort has been dedicated toengineering prompts that guide LLMs toward particular behaviors. In this work,we propose an automatic prompt optimization framework, PROPANE, which aims tofind a prompt that induces semantically similar outputs to a fixed set ofexamples without user intervention. We further demonstrate that PROPANE can beused to (a) improve existing prompts, and (b) discover semantically obfuscatedprompts that transfer between models."
Joint Prompt Optimization of Stacked LLMs using Variational Inference,"['Alessandro Sordoni', 'Xingdi Yuan', 'Marc-Alexandre Côté', 'Matheus Pereira', 'Adam Trischler', 'Ziang Xiao', 'Arian Hosseini', 'Friederike Niedtner', 'Nicolas Le Roux']",http://arxiv.org/pdf/2306.12509v2.pdf,2023-06-21,"['cs.cl', 'cs.lg']","  Large language models (LLMs) can be seen as atomic units of computationmapping sequences to a distribution over sequences. Thus, they can be seen asstochastic language layers in a language network, where the learnableparameters are the natural language prompts at each layer. By stacking two suchlayers and feeding the output of one layer to the next, we obtain a DeepLanguage Network (DLN). We first show how to effectively perform promptoptimization for a 1-Layer language network (DLN-1). Then, we present anextension that applies to 2-layer DLNs (DLN-2), where two prompts must belearned. The key idea is to consider the output of the first layer as a latentvariable, which requires inference, and prompts to be learned as the parametersof the generative distribution. We first test the effectiveness of DLN-1 inmultiple reasoning and natural language understanding tasks. Then, we show thatDLN-2 can reach higher performance than a single layer, showing promise that wemight reach comparable performance to GPT-4, even when each LLM in the networkis smaller and less powerful."
Connecting Large Language Models with Evolutionary Algorithms Yields  Powerful Prompt Optimizers,"['Qingyan Guo', 'Rui Wang', 'Junliang Guo', 'Bei Li', 'Kaitao Song', 'Xu Tan', 'Guoqing Liu', 'Jiang Bian', 'Yujiu Yang']",http://arxiv.org/pdf/2309.08532v1.pdf,2023-09-15,"['cs.cl', 'cs.ai']","  Large Language Models (LLMs) excel in various tasks, but they rely oncarefully crafted prompts that often demand substantial human effort. Toautomate this process, in this paper, we propose a novel framework for discreteprompt optimization, called EvoPrompt, which borrows the idea of evolutionaryalgorithms (EAs) as they exhibit good performance and fast convergence. Toenable EAs to work on discrete prompts, which are natural language expressionsthat need to be coherent and human-readable, we connect LLMs with EAs. Thisapproach allows us to simultaneously leverage the powerful language processingcapabilities of LLMs and the efficient optimization performance of EAs.Specifically, abstaining from any gradients or parameters, EvoPrompt startsfrom a population of prompts and iteratively generates new prompts with LLMsbased on the evolutionary operators, improving the population based on thedevelopment set. We optimize prompts for both closed- and open-source LLMsincluding GPT-3.5 and Alpaca, on 9 datasets spanning language understanding andgeneration tasks. EvoPrompt significantly outperforms human-engineered promptsand existing methods for automatic prompt generation by up to 25% and 14%respectively. Furthermore, EvoPrompt demonstrates that connecting LLMs with EAscreates synergies, which could inspire further research on the combination ofLLMs and conventional algorithms."
Black-Box Prompt Optimization: Aligning Large Language Models without  Model Training,"['Jiale Cheng', 'Xiao Liu', 'Kehan Zheng', 'Pei Ke', 'Hongning Wang', 'Yuxiao Dong', 'Jie Tang', 'Minlie Huang']",http://arxiv.org/pdf/2311.04155v2.pdf,2023-11-07,['cs.cl'],"  Large language models (LLMs) have shown impressive success in variousapplications. However, these models are often not well aligned with humanintents, which calls for additional treatments on them, that is, the alignmentproblem. To make LLMs better follow user instructions, existing alignmentmethods mostly focus on further training them. However, the extra training ofLLMs are usually expensive in terms of GPU compute; worse still, LLMs ofinterest are oftentimes not accessible for user-demanded training, such asGPTs. In this work, we take a different perspective -- Black-Box PromptOptimization (BPO) -- to perform alignments. The idea is to optimize userprompts to suit LLMs' input understanding, so as to best realize users' intentswithout updating LLMs' parameters. BPO is model-agnostic and the empiricalresults demonstrate that the BPO-aligned ChatGPT yields a 22% increase in thewin rate against its original version, and 10% for GPT-4. Importantly, theBPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and italso brings additional performance gains when combining BPO with PPO or DPO.Code and datasets are released at https://github.com/thu-coai/BPO."
Prompt Optimization via Adversarial In-Context Learning,"['Xuan Long Do', 'Yiran Zhao', 'Hannah Brown', 'Yuxi Xie', 'James Xu Zhao', 'Nancy F. Chen', 'Kenji Kawaguchi', 'Michael Qizhe Xie', 'Junxian He']",http://arxiv.org/pdf/2312.02614v1.pdf,2023-12-05,"['cs.lg', 'cs.cl']","  We propose a new method, Adversarial In-Context Learning (adv-ICL), tooptimize prompt for in-context learning (ICL) by employing one LLM as agenerator, another as a discriminator, and a third as a prompt modifier. As intraditional adversarial learning, adv-ICL is implemented as a two-player gamebetween the generator and discriminator, where the generator tries to generaterealistic enough output to fool the discriminator. In each round, given aninput prefixed by task instructions and several exemplars, the generatorproduces an output. The discriminator is then tasked with classifying thegenerator input-output pair as model-generated or real data. Based on thediscriminator loss, the prompt modifier proposes possible edits to thegenerator and discriminator prompts, and the edits that most improve theadversarial loss are selected. We show that adv-ICL results in significantimprovements over state-of-the-art prompt optimization techniques for both openand closed-source models on 11 generation and classification tasks includingsummarization, arithmetic reasoning, machine translation, data-to-textgeneration, and the MMLU and big-bench hard benchmarks. In addition, becauseour method uses pre-trained models and updates only prompts rather than modelparameters, it is computationally efficient, easy to extend to any LLM andtask, and effective in low-resource settings."
In-context Examples Selection for Machine Translation,"['Sweta Agrawal', 'Chunting Zhou', 'Mike Lewis', 'Luke Zettlemoyer', 'Marjan Ghazvininejad']",http://arxiv.org/pdf/2212.02437v1.pdf,2022-12-05,['cs.cl'],"  Large-scale generative models show an impressive ability to perform a widerange of Natural Language Processing (NLP) tasks using in-context learning,where a few examples are used to describe a task to the model. For MachineTranslation (MT), these examples are typically randomly sampled from thedevelopment dataset with a similar distribution as the evaluation set. However,it is unclear how the choice of these in-context examples and their orderingimpacts the output translation quality. In this work, we aim to understand theproperties of good in-context examples for MT in both in-domain andout-of-domain settings. We show that the translation quality and the domain ofthe in-context examples matter and that 1-shot noisy unrelated example can havea catastrophic impact on output quality. While concatenating multiple randomexamples reduces the effect of noise, a single good prompt optimized tomaximize translation quality on the development dataset can elicit learnedinformation from the pre-trained language model. Adding similar examples basedon an n-gram overlap with the test source significantly and consistentlyimproves the translation quality of the outputs, outperforming a strong kNN-MTbaseline in 2 out of 4 out-of-domain datasets."
ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts,"['Kwanyoung Kim', 'Yujin Oh', 'Jong Chul Ye']",http://arxiv.org/pdf/2301.12171v2.pdf,2023-01-28,"['cs.cv', 'cs.ai', 'cs.lg', 'stat.ml']","  Recent success of large-scale Contrastive Language-Image Pre-training (CLIP)has led to great promise in zero-shot semantic segmentation by transferringimage-text aligned knowledge to pixel-level classification. However, existingmethods usually require an additional image encoder or retraining/tuning theCLIP module. Here, we propose a novel Zero-shot segmentation with OptimalTransport (ZegOT) method that matches multiple text prompts with frozen imageembeddings through optimal transport. In particular, we introduce a novelMultiple Prompt Optimal Transport Solver (MPOT), which is designed to learn anoptimal mapping between multiple text prompts and visual feature maps of thefrozen image encoder hidden layers. This unique mapping method facilitates eachof the multiple text prompts to effectively focus on distinct visual semanticattributes. Through extensive experiments on benchmark datasets, we show thatour method achieves the state-of-the-art (SOTA) performance over existingZero-shot Semantic Segmentation (ZS3) approaches."
DeltaEdit: Exploring Text-free Training for Text-Driven Image  Manipulation,"['Yueming Lyu', 'Tianwei Lin', 'Fu Li', 'Dongliang He', 'Jing Dong', 'Tieniu Tan']",http://arxiv.org/pdf/2303.06285v1.pdf,2023-03-11,['cs.cv'],"  Text-driven image manipulation remains challenging in training or inferenceflexibility. Conditional generative models depend heavily on expensiveannotated training data. Meanwhile, recent frameworks, which leveragepre-trained vision-language models, are limited by either per text-promptoptimization or inference-time hyper-parameters tuning. In this work, wepropose a novel framework named \textit{DeltaEdit} to address these problems.Our key idea is to investigate and identify a space, namely delta image andtext space that has well-aligned distribution between CLIP visual featuredifferences of two images and CLIP textual embedding differences of source andtarget texts. Based on the CLIP delta space, the DeltaEdit network is designedto map the CLIP visual features differences to the editing directions ofStyleGAN at training phase. Then, in inference phase, DeltaEdit predicts theStyleGAN's editing directions from the differences of the CLIP textualfeatures. In this way, DeltaEdit is trained in a text-free manner. Oncetrained, it can well generalize to various text prompts for zero-shot inferencewithout bells and whistles. Code is available athttps://github.com/Yueming6568/DeltaEdit."
Unnatural language processing: How do language models handle  machine-generated prompts?,"['Corentin Kervadec', 'Francesca Franzon', 'Marco Baroni']",http://arxiv.org/pdf/2310.15829v1.pdf,2023-10-24,['cs.cl'],"  Language model prompt optimization research has shown that semantically andgrammatically well-formed manually crafted prompts are routinely outperformedby automatically generated token sequences with no apparent meaning orsyntactic structure, including sequences of vectors from a model's embeddingspace. We use machine-generated prompts to probe how models respond to inputthat is not composed of natural language expressions. We study the behavior ofmodels of different sizes in multiple semantic tasks in response to bothcontinuous and discrete machine-generated prompts, and compare it to thebehavior in response to human-generated natural-language prompts. Even whenproducing a similar output, machine-generated and human prompts triggerdifferent response patterns through the network processing pathways, includingdifferent perplexities, different attention and output entropy distributions,and different unit activation profiles. We provide preliminary insight into thenature of the units activated by different prompt types, suggesting that onlynatural language prompts recruit a genuinely linguistic circuit."
Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained  Language Models,"['Paul Youssef', 'Osman Alperen Koraş', 'Meijie Li', 'Jörg Schlötterer', 'Christin Seifert']",http://arxiv.org/pdf/2310.16570v2.pdf,2023-10-25,['cs.cl'],"  Pre-trained Language Models (PLMs) are trained on vast unlabeled data, richin world knowledge. This fact has sparked the interest of the community inquantifying the amount of factual knowledge present in PLMs, as this explainstheir performance on downstream tasks, and potentially justifies their use asknowledge bases. In this work, we survey methods and datasets that are used toprobe PLMs for factual knowledge. Our contributions are: (1) We propose acategorization scheme for factual probing methods that is based on how theirinputs, outputs and the probed PLMs are adapted; (2) We provide an overview ofthe datasets used for factual probing; (3) We synthesize insights aboutknowledge retention and prompt optimization in PLMs, analyze obstacles toadopting PLMs as knowledge bases and outline directions for future work."
Task-driven Prompt Evolution for Foundation Models,"['Rachana Sathish', 'Rahul Venkataramani', 'K S Shriram', 'Prasad Sudhakar']",http://arxiv.org/pdf/2310.17128v1.pdf,2023-10-26,['cs.cv'],"  Promptable foundation models, particularly Segment Anything Model (SAM), haveemerged as a promising alternative to the traditional task-specific supervisedlearning for image segmentation. However, many evaluation studies have foundthat their performance on medical imaging modalities to be underwhelmingcompared to conventional deep learning methods. In the world of largepre-trained language and vision-language models, learning prompt fromdownstream tasks has achieved considerable success in improving performance. Inthis work, we propose a plug-and-play Prompt Optimization Technique forfoundation models like SAM (SAMPOT) that utilizes the downstream segmentationtask to optimize the human-provided prompt to obtain improved performance. Wedemonstrate the utility of SAMPOT on lung segmentation in chest X-ray imagesand obtain an improvement on a significant number of cases ($\sim75\%$) overhuman-provided initial prompts. We hope this work will lead to furtherinvestigations in the nascent field of automatic visual prompt-tuning."
Prompt2NeRF-PIL: Fast NeRF Generation via Pretrained Implicit Latent,"['Jianmeng Liu', 'Yuyao Zhang', 'Zeyuan Meng', 'Yu-Wing Tai', 'Chi-Keung Tang']",http://arxiv.org/pdf/2312.02568v1.pdf,2023-12-05,['cs.cv'],"  This paper explores promptable NeRF generation (e.g., text prompt or singleimage prompt) for direct conditioning and fast generation of NeRF parametersfor the underlying 3D scenes, thus undoing complex intermediate steps whileproviding full 3D generation with conditional control. Unlike previousdiffusion-CLIP-based pipelines that involve tedious per-prompt optimizations,Prompt2NeRF-PIL is capable of generating a variety of 3D objects with a singleforward pass, leveraging a pre-trained implicit latent space of NeRFparameters. Furthermore, in zero-shot tasks, our experiments demonstrate thatthe NeRFs produced by our method serve as semantically informativeinitializations, significantly accelerating the inference process of existingprompt-to-NeRF methods. Specifically, we will show that our approach speeds upthe text-to-NeRF model DreamFusion and the 3D reconstruction speed of theimage-to-NeRF method Zero-1-to-3 by 3 to 5 times."
Large Language Models for Intent-Driven Session Recommendations,"['Zhu Sun', 'Hongyang Liu', 'Xinghua Qu', 'Kaidong Feng', 'Yan Wang', 'Yew-Soon Ong']",http://arxiv.org/pdf/2312.07552v1.pdf,2023-12-07,"['cs.cl', 'cs.lg']","  Intent-aware session recommendation (ISR) is pivotal in discerning userintents within sessions for precise predictions. Traditional approaches,however, face limitations due to their presumption of a uniform number ofintents across all sessions. This assumption overlooks the dynamic nature ofuser sessions, where the number and type of intentions can significantly vary.In addition, these methods typically operate in latent spaces, thus hinder themodel's transparency.Addressing these challenges, we introduce a novel ISRapproach, utilizing the advanced reasoning capabilities of large languagemodels (LLMs). First, this approach begins by generating an initial prompt thatguides LLMs to predict the next item in a session, based on the varied intentsmanifested in user sessions. Then, to refine this process, we introduce aninnovative prompt optimization mechanism that iteratively self-reflects andadjusts prompts. Furthermore, our prompt selection module, built upon the LLMs'broad adaptability, swiftly selects the most optimized prompts across diversedomains. This new paradigm empowers LLMs to discern diverse user intents at asemantic level, leading to more accurate and interpretable sessionrecommendations. Our extensive experiments on three real-world datasetsdemonstrate the effectiveness of our method, marking a significant advancementin ISR systems."
RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning,"['Mingkai Deng', 'Jianyu Wang', 'Cheng-Ping Hsieh', 'Yihan Wang', 'Han Guo', 'Tianmin Shu', 'Meng Song', 'Eric P. Xing', 'Zhiting Hu']",http://arxiv.org/pdf/2205.12548v3.pdf,2022-05-25,"['cs.cl', 'cs.lg']","  Prompting has shown impressive success in enabling large pretrained languagemodels (LMs) to perform diverse NLP tasks, especially when only few downstreamdata are available. Automatically finding the optimal prompt for each task,however, is challenging. Most existing work resorts to tuning soft prompt(e.g., embeddings) which falls short of interpretability, reusability acrossLMs, and applicability when gradients are not accessible. Discrete prompt, onthe other hand, is difficult to optimize, and is often created by ""enumeration(e.g., paraphrasing)-then-selection"" heuristics that do not explore the promptspace systematically. This paper proposes RLPrompt, an efficient discreteprompt optimization approach with reinforcement learning (RL). RLPromptformulates a parameter-efficient policy network that generates the desireddiscrete prompt after training with reward. To overcome the complexity andstochasticity of reward signals by the large LM environment, we incorporateeffective reward stabilization that substantially enhances the trainingefficiency. RLPrompt is flexibly applicable to different types of LMs, such asmasked (e.g., BERT) and left-to-right models (e.g., GPTs), for bothclassification and generation tasks. Experiments on few-shot classification andunsupervised text style transfer show superior performance over a wide range ofexisting finetuning or prompting methods. Interestingly, the resultingoptimized prompts are often ungrammatical gibberish text; and surprisingly,those gibberish prompts are transferrable between different LMs to retainsignificant performance, indicating LM prompting may not follow human languagepatterns."
Diversity-Aware Meta Visual Prompting,"['Qidong Huang', 'Xiaoyi Dong', 'Dongdong Chen', 'Weiming Zhang', 'Feifei Wang', 'Gang Hua', 'Nenghai Yu']",http://arxiv.org/pdf/2303.08138v1.pdf,2023-03-14,['cs.cv'],"  We present Diversity-Aware Meta Visual Prompting~(DAM-VP), an efficient andeffective prompting method for transferring pre-trained models to downstreamtasks with frozen backbone. A challenging issue in visual prompting is thatimage datasets sometimes have a large data diversity whereas a per-datasetgeneric prompt can hardly handle the complex distribution shift toward theoriginal pretraining data distribution properly. To address this issue, wepropose a dataset Diversity-Aware prompting strategy whose initialization isrealized by a Meta-prompt. Specifically, we cluster the downstream dataset intosmall homogeneity subsets in a diversity-adaptive way, with each subset has itsown prompt optimized separately. Such a divide-and-conquer design reduces theoptimization difficulty greatly and significantly boosts the promptingperformance. Furthermore, all the prompts are initialized with a meta-prompt,which is learned across several datasets. It is a bootstrapped paradigm, withthe key observation that the prompting knowledge learned from previous datasetscould help the prompt to converge faster and perform better on a new dataset.During inference, we dynamically select a proper prompt for each input, basedon the feature distance between the input and each subset. Through extensiveexperiments, our DAM-VP demonstrates superior efficiency and effectiveness,clearly surpassing previous prompting methods in a series of downstreamdatasets for different pretraining models. Our code is available at:\url{https://github.com/shikiw/DAM-VP}."
DRPT: Disentangled and Recurrent Prompt Tuning for Compositional  Zero-Shot Learning,"['Xiaocheng Lu', 'Ziming Liu', 'Song Guo', 'Jingcai Guo', 'Fushuo Huo', 'Sikai Bai', 'Tao Han']",http://arxiv.org/pdf/2305.01239v1.pdf,2023-05-02,"['cs.cv', 'cs.ai']","  Compositional Zero-shot Learning (CZSL) aims to recognize novel conceptscomposed of known knowledge without training samples. Standard CZSL eitheridentifies visual primitives or enhances unseen composed entities, and as aresult, entanglement between state and object primitives cannot be fullyutilized. Admittedly, vision-language models (VLMs) could naturally cope withCZSL through tuning prompts, while uneven entanglement leads prompts to bedragged into local optimum. In this paper, we take a further step to introducea novel Disentangled and Recurrent Prompt Tuning framework termed DRPT tobetter tap the potential of VLMs in CZSL. Specifically, the state and objectprimitives are deemed as learnable tokens of vocabulary embedded in prompts andtuned on seen compositions. Instead of jointly tuning state and object, wedevise a disentangled and recurrent tuning strategy to suppress the tractionforce caused by entanglement and gradually optimize the token parameters,leading to a better prompt space. Notably, we develop a progressive fine-tuningprocedure that allows for incremental updates to the prompts, optimizing theobject first, then the state, and vice versa. Meanwhile, the optimization ofstate and object is independent, thus clearer features can be learned tofurther alleviate the issue of entangling misleading optimization. Moreover, wequantify and analyze the entanglement in CZSL and supplement entanglementrebalancing optimization schemes. DRPT surpasses representativestate-of-the-art methods on extensive benchmark datasets, demonstratingsuperiority in both accuracy and efficiency."
Getting MoRE out of Mixture of Language Model Reasoning Experts,"['Chenglei Si', 'Weijia Shi', 'Chen Zhao', 'Luke Zettlemoyer', 'Jordan Boyd-Graber']",http://arxiv.org/pdf/2305.14628v2.pdf,2023-05-24,"['cs.cl', 'cs.ai']","  While recent large language models (LLMs) improve on various questionanswering (QA) datasets, it remains difficult for a single model to generalizeacross question types that require distinct reasoning abilities. We provideempirical evidence that state-of-the-art LLMs suffer from poor generalizabilityon reasoning types beyond those seen in the prompt. To remedy this, we proposea Mixture-of-Reasoning-Experts (MoRE) framework that ensembles diversespecialized language models. We specialize the backbone language model withprompts optimized for different reasoning categories, including factual,multihop, mathematical, and commonsense reasoning. Our key insight is toleverage agreement among the specialized experts to select the best answer foreach question, or to abstain from answering. This gives MoRE higher accuracythan any single specialized model on a collection of 12 QA datasets from fourreasoning types. Beyond generalizability, the interpretable design of MoREimproves selective question answering results compared to baselines withoutincorporating inter-expert agreement. This framework is also more interpretableand useful to human consumers of QA outputs. Our human study confirms thatpresenting expert predictions and the answer selection process helps annotatorsmore accurately calibrate when to trust the system's output. We release allcode and data to facilitate future work."
Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug  Trafficking Detection on Social Media,"['Chuanbo Hu', 'Bin Liu', 'Xin Li', 'Yanfang Ye']",http://arxiv.org/pdf/2307.03699v1.pdf,2023-07-07,"['cs.cl', 'cs.ai', 'cs.si']","  Social media platforms such as Instagram and Twitter have emerged as criticalchannels for drug marketing and illegal sale. Detecting and labeling onlineillicit drug trafficking activities becomes important in addressing this issue.However, the effectiveness of conventional supervised learning methods indetecting drug trafficking heavily relies on having access to substantialamounts of labeled data, while data annotation is time-consuming andresource-intensive. Furthermore, these models often face challenges inaccurately identifying trafficking activities when drug dealers use deceptivelanguage and euphemisms to avoid detection. To overcome this limitation, weconduct the first systematic study on leveraging large language models (LLMs),such as ChatGPT, to detect illicit drug trafficking activities on social media.We propose an analytical framework to compose \emph{knowledge-informedprompts}, which serve as the interface that humans can interact with and useLLMs to perform the detection task. Additionally, we design a Monte Carlodropout based prompt optimization method to further to improve performance andinterpretability. Our experimental findings demonstrate that the proposedframework outperforms other baseline language models in terms of drugtrafficking detection accuracy, showing a remarkable improvement of nearly12\%. By integrating prior knowledge and the proposed prompts, ChatGPT caneffectively identify and label drug trafficking activities on social networks,even in the presence of deceptive language and euphemisms used by drug dealersto evade detection. The implications of our research extend to social networks,emphasizing the importance of incorporating prior knowledge and scenario-basedprompts into analytical tools to improve online security and public safety."
AutoHint: Automatic Prompt Optimization with Hint Generation,"['Hong Sun', 'Xue Li', 'Yinchuan Xu', 'Youkow Homma', 'Qi Cao', 'Min Wu', 'Jian Jiao', 'Denis Charles']",http://arxiv.org/pdf/2307.07415v2.pdf,2023-07-13,"['cs.cl', 'cs.ai']","  This paper presents AutoHint, a novel framework for automatic promptengineering and optimization for Large Language Models (LLM). While LLMs havedemonstrated remarkable ability in achieving high-quality annotation in varioustasks, the key to applying this ability to specific tasks lies in developinghigh-quality prompts. Thus we propose a framework to inherit the merits of bothin-context learning and zero-shot learning by incorporating enrichedinstructions derived from input-output demonstrations to optimize originalprompt. We refer to the enrichment as the hint and propose a framework toautomatically generate the hint from labeled data. More concretely, startingfrom an initial prompt, our method first instructs a LLM to deduce new hintsfor selected samples from incorrect predictions, and then summarizes fromper-sample hints and adds the results back to the initial prompt to form a new,enriched instruction. The proposed method is evaluated on the BIG-BenchInstruction Induction dataset for both zero-shot and few-short prompts, whereexperiments demonstrate our method is able to significantly boost accuracy formultiple tasks."
"Optimizing Mobile-Edge AI-Generated Everything (AIGX) Services by Prompt  Engineering: Fundamental, Framework, and Case Study","['Yinqiu Liu', 'Hongyang Du', 'Dusit Niyato', 'Jiawen Kang', 'Shuguang Cui', 'Xuemin Shen', 'Ping Zhang']",http://arxiv.org/pdf/2309.01065v2.pdf,2023-09-03,['cs.ni'],"  As the next-generation paradigm for content creation, AI-Generated Content(AIGC), i.e., generating content automatically by Generative AI (GAI) based onuser prompts, has gained great attention and success recently. With theever-increasing power of GAI, especially the emergence of Pretrained FoundationModels (PFMs) that contain billions of parameters and prompt engineeringmethods (i.e., finding the best prompts for the given task), the applicationrange of AIGC is rapidly expanding, covering various forms of information forhuman, systems, and networks, such as network designs, channel coding, andoptimization solutions. In this article, we present the concept of mobile-edgeAI-Generated Everything (AIGX). Specifically, we first review the buildingblocks of AIGX, the evolution from AIGC to AIGX, as well as practical AIGXapplications. Then, we present a unified mobile-edge AIGX framework, whichemploys edge devices to provide PFM-empowered AIGX services and optimizes suchservices via prompt engineering. More importantly, we demonstrate thatsuboptimal prompts lead to poor generation quality, which adversely affectsuser satisfaction, edge network performance, and resource utilization.Accordingly, we conduct a case study, showcasing how to train an effectiveprompt optimizer using ChatGPT and investigating how much improvement ispossible with prompt engineering in terms of user experience, quality ofgeneration, and network performance."
Automatic Data Transformation Using Large Language Model: An  Experimental Study on Building Energy Data,"['Ankita Sharma', 'Xuanmao Li', 'Hong Guan', 'Guoxin Sun', 'Liang Zhang', 'Lanjun Wang', 'Kesheng Wu', 'Lei Cao', 'Erkang Zhu', 'Alexander Sim', 'Teresa Wu', 'Jia Zou']",http://arxiv.org/pdf/2309.01957v2.pdf,2023-09-05,['cs.db'],"  Existing approaches to automatic data transformation are insufficient to meetthe requirements in many real-world scenarios, such as the building sector.First, there is no convenient interface for domain experts to provide domainknowledge easily. Second, they require significant training data collectionoverheads. Third, the accuracy suffers from complicated schema changes. Tobridge this gap, we present a novel approach that leverages the uniquecapabilities of large language models (LLMs) in coding, complex reasoning, andzero-shot learning to generate SQL code that transforms the source datasetsinto the target datasets. We demonstrate the viability of this approach bydesigning an LLM-based framework, termed SQLMorpher, which comprises a promptgenerator that integrates the initial prompt with optional domain knowledge andhistorical patterns in external databases. It also implements an iterativeprompt optimization mechanism that automatically improves the prompt based onflaw detection. The key contributions of this work include (1) pioneering anend-to-end LLM-based solution for data transformation, (2) developing abenchmark dataset of 105 real-world building energy data transformationproblems, and (3) conducting an extensive empirical evaluation where ourapproach achieved 96% accuracy in all 105 problems. SQLMorpher demonstrates theeffectiveness of utilizing LLMs in complex, domain-specific challenges,highlighting the potential of their potential to drive sustainable solutions."
Automatic Prompt Rewriting for Personalized Text Generation,"['Cheng Li', 'Mingyang Zhang', 'Qiaozhu Mei', 'Weize Kong', 'Michael Bendersky']",http://arxiv.org/pdf/2310.00152v1.pdf,2023-09-29,['cs.cl'],"  Facilitated by large language models (LLMs), personalized text generation hasbecome a rapidly growing research direction. Most existing studies focus ondesigning specialized models for a particular domain, or they requirefine-tuning the LLMs to generate personalized text. We consider a typicalscenario in which the large language model, which generates personalizedoutput, is frozen and can only be accessed through APIs. Under this constraint,all one can do is to improve the input text (i.e., text prompts) sent to theLLM, a procedure that is usually done manually. In this paper, we propose anovel method to automatically revise prompts for personalized text generation.The proposed method takes the initial prompts generated by a state-of-the-art,multistage framework for personalized generation and rewrites a few criticalcomponents that summarize and synthesize the personal context. The promptrewriter employs a training paradigm that chains together supervised learning(SL) and reinforcement learning (RL), where SL reduces the search space of RLand RL facilitates end-to-end training of the rewriter. Using datasets fromthree representative domains, we demonstrate that the rewritten promptsoutperform both the original prompts and the prompts optimized via supervisedlearning or reinforcement learning alone. In-depth analysis of the rewrittenprompts shows that they are not only human readable, but also able to guidemanual revision of prompts when there is limited resource to employreinforcement learning to train the prompt rewriter, or when it is costly todeploy an automatic prompt rewriter for inference."
DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided  Image Editing,"['Yueming Lyu', 'Kang Zhao', 'Bo Peng', 'Yue Jiang', 'Yingya Zhang', 'Jing Dong']",http://arxiv.org/pdf/2310.08785v1.pdf,2023-10-12,"['cs.cv', 'cs.ai']","  Text-guided image editing faces significant challenges to training andinference flexibility. Much literature collects large amounts of annotatedimage-text pairs to train text-conditioned generative models from scratch,which is expensive and not efficient. After that, some approaches that leveragepre-trained vision-language models are put forward to avoid data collection,but they are also limited by either per text-prompt optimization orinference-time hyper-parameters tuning. To address these issues, we investigateand identify a specific space, referred to as CLIP DeltaSpace, where the CLIPvisual feature difference of two images is semantically aligned with the CLIPtextual feature difference of their corresponding text descriptions. Based onDeltaSpace, we propose a novel framework called DeltaEdit, which maps the CLIPvisual feature differences to the latent space directions of a generative modelduring the training phase, and predicts the latent space directions from theCLIP textual feature differences during the inference phase. And this designendows DeltaEdit with two advantages: (1) text-free training; (2)generalization to various text prompts for zero-shot inference. Extensiveexperiments validate the effectiveness and versatility of DeltaEdit withdifferent generative models, including both the GAN model and the diffusionmodel, in achieving flexible text-guided image editing. Code is available athttps://github.com/Yueming6568/DeltaEdit."
InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image,"['Jianhui Li', 'Shilong Liu', 'Zidong Liu', 'Yikai Wang', 'Kaiwen Zheng', 'Jinghui Xu', 'Jianmin Li', 'Jun Zhu']",http://arxiv.org/pdf/2311.02826v1.pdf,2023-11-06,['cs.cv'],"  With the success of Neural Radiance Field (NeRF) in 3D-aware portraitediting, a variety of works have achieved promising results regarding bothquality and 3D consistency. However, these methods heavily rely on per-promptoptimization when handling natural language as editing instructions. Due to thelack of labeled human face 3D datasets and effective architectures, the area ofhuman-instructed 3D-aware editing for open-world portraits in an end-to-endmanner remains under-explored. To solve this problem, we propose an end-to-enddiffusion-based framework termed InstructPix2NeRF, which enables instructed3D-aware portrait editing from a single open-world image with humaninstructions. At its core lies a conditional latent 3D diffusion process thatlifts 2D editing to 3D space by learning the correlation between the pairedimages' difference and the instructions via triplet data. With the help of ourproposed token position randomization strategy, we could even achievemulti-semantic editing through one single pass with the portrait identitywell-preserved. Besides, we further propose an identity consistency module thatdirectly modulates the extracted identity signals into our diffusion process,which increases the multi-view 3D identity consistency. Extensive experimentsverify the effectiveness of our method and show its superiority against strongbaselines quantitatively and qualitatively."
What Changes Can Large-scale Language Models Bring? Intensive Study on  HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,"['Boseop Kim', 'HyoungSeok Kim', 'Sang-Woo Lee', 'Gichang Lee', 'Donghyun Kwak', 'Dong Hyeon Jeon', 'Sunghyun Park', 'Sungju Kim', 'Seonhoon Kim', 'Dongpil Seo', 'Heungsub Lee', 'Minyoung Jeong', 'Sungjae Lee', 'Minsub Kim', 'Suk Hyun Ko', 'Seokhun Kim', 'Taeyong Park', 'Jinuk Kim', 'Soyoung Kang', 'Na-Hyeon Ryu', 'Kang Min Yoo', 'Minsuk Chang', 'Soobin Suh', 'Sookyo In', 'Jinseong Park', 'Kyungduk Kim', 'Hiun Kim', 'Jisu Jeong', 'Yong Goo Yeo', 'Donghoon Ham', 'Dongju Park', 'Min Young Lee', 'Jaewook Kang', 'Inho Kang', 'Jung-Woo Ha', 'Woomyoung Park', 'Nako Sung']",http://arxiv.org/pdf/2109.04650v2.pdf,2021-09-10,['cs.cl'],"  GPT-3 shows remarkable in-context learning ability of large-scale languagemodels (LMs) trained on hundreds of billion scale data. Here we address someremaining issues less reported by the GPT-3 paper, such as a non-English LM,the performances of different sized models, and the effect of recentlyintroduced prompt optimization on in-context learning. To achieve this, weintroduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centriccorpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVAwith our training configuration shows state-of-the-art in-context zero-shot andfew-shot learning performances on various downstream tasks in Korean. Also, weshow the performance benefits of prompt-based learning and demonstrate how itcan be integrated into the prompt engineering pipeline. Then we discuss thepossibility of materializing the No Code AI paradigm by providing AIprototyping capabilities to non-experts of ML by introducing HyperCLOVA studio,an interactive prompt engineering interface. Lastly, we demonstrate thepotential of our methods with three successful in-house applications."
MLLM-DataEngine: An Iterative Refinement Approach for MLLM,"['Zhiyuan Zhao', 'Linke Ouyang', 'Bin Wang', 'Siyuan Huang', 'Pan Zhang', 'Xiaoyi Dong', 'Jiaqi Wang', 'Conghui He']",http://arxiv.org/pdf/2308.13566v2.pdf,2023-08-25,"['cs.lg', 'cs.ai', 'cs.cl', 'cs.cv']","  Despite the great advance of Multimodal Large Language Models (MLLMs) in bothinstruction dataset building and benchmarking, the independence of training andevaluation makes current MLLMs hard to further improve their capability underthe guidance of evaluation results with a relatively low human cost. In thispaper, we propose MLLM-DataEngine, a novel closed-loop system that bridges datageneration, model training, and evaluation. Within each loop iteration, theMLLM-DataEngine first analyze the weakness of the model based on the evaluationresults, then generate a proper incremental dataset for the next trainingiteration and enhance the model capability iteratively. Compared with previousdata collection methods which are separate from the benchmarking, the datagenerated by MLLM-DataEngine shows better targeting, quality, and correctness.For targeting, we propose an Adaptive Bad-case Sampling module, which adjuststhe ratio of different types of data within each incremental dataset based onthe benchmarking results. For quality, we resort to GPT-4 to generatehigh-quality data with each given data type. For correctness, prompt design iscritical for the data generation results. Rather than previous hand-craftedprompt, we propose an Interactive Prompt Optimization strategy, which optimizesthe prompt with the multi-round interaction between human and GPT, and improvethe correctness of generated data greatly. Through extensive experiments, wefind our MLLM-DataEngine could boost the MLLM capability in a targeted andautomatic manner, with only a few human participation. We hope it could be ageneral solution for the following MLLMs building. The MLLM-DataEngine has beenopen-sourced and is now available athttps://github.com/opendatalab/MLLM-DataEngine."
A Bayesian approach for prompt optimization in pre-trained language  models,"['Antonio Sabbatella', 'Andrea Ponti', 'Antonio Candelieri', 'Ilaria Giordani', 'Francesco Archetti']",http://arxiv.org/pdf/2312.00471v1.pdf,2023-12-01,"['cs.lg', 'cs.ai']","  A prompt is a sequence of symbol or tokens, selected from a vocabularyaccording to some rule, which is prepended/concatenated to a textual query. Akey problem is how to select the sequence of tokens: in this paper we formulateit as a combinatorial optimization problem. The high dimensionality of thetoken space com-pounded by the length of the prompt sequence requires a veryefficient solution. In this paper we propose a Bayesian optimization method,executed in a continuous em-bedding of the combinatorial space. In this paperwe focus on hard prompt tuning (HPT) which directly searches for discretetokens to be added to the text input with-out requiring access to the largelanguage model (LLM) and can be used also when LLM is available only as ablack-box. This is critically important if LLMs are made available in the Modelas a Service (MaaS) manner as in GPT-4. The current manu-script is focused onthe optimization of discrete prompts for classification tasks. The discreteprompts give rise to difficult combinatorial optimization problem which easilybecome intractable given the dimension of the token space in realisticapplications. The optimization method considered in this paper is Bayesianoptimization (BO) which has become the dominant approach in black-boxoptimization for its sample efficiency along with its modular structure andversatility. In this paper we use BoTorch, a library for Bayesian optimizationresearch built on top of pyTorch. Albeit preliminary and obtained using a'vanilla' version of BO, the experiments on RoB-ERTa on six benchmarks, show agood performance across a variety of tasks and enable an analysis of thetradeoff between size of the search space, accuracy and wall clock time."
Prompting AI Art: An Investigation into the Creative Skill of Prompt  Engineering,"['Jonas Oppenlaender', 'Rhema Linder', 'Johanna Silvennoinen']",http://arxiv.org/pdf/2303.13534v2.pdf,2023-03-13,"['cs.hc', 'h.m; i.2']","  We are witnessing a novel era of creativity where anyone can create digitalcontent via prompt-based learning (known as prompt engineering). This paperdelves into prompt engineering as a novel creative skill for creating AI artwith text-to-image generation. In a pilot study, we find that many crowdsourcedparticipants have knowledge about art which could be used for writing effectiveprompts. In three subsequent studies, we explore whether crowdsourcedparticipants can put this knowledge into practice. We examine if participantscan 1) discern prompt quality, 2) write prompts, and 3) refine prompts. We findthat participants could evaluate prompt quality and crafted descriptiveprompts, but they lacked style-specific vocabulary necessary for effectiveprompting. This is in line with our hypothesis that prompt engineering is a newtype of skill that is non-intuitive and must first be acquired (e.g., throughmeans of practice and learning) before it can be used. Our studies deepen ourunderstanding of prompt engineering and chart future research directions. Weoffer nine guidelines for conducting research on text-to-image generation andprompt engineering with paid crowds. We conclude by envisioning four potentialfutures for prompt engineering."
Unleashing the potential of prompt engineering in Large Language Models:  a comprehensive review,"['Banghao Chen', 'Zhaofeng Zhang', 'Nicolas Langrené', 'Shengxin Zhu']",http://arxiv.org/pdf/2310.14735v2.pdf,2023-10-23,"['cs.cl', 'cs.ai', 'i.2.7']","  This paper delves into the pivotal role of prompt engineering in unleashingthe capabilities of Large Language Models (LLMs). Prompt engineering is theprocess of structuring input text for LLMs and is a technique integral tooptimizing the efficacy of LLMs. This survey elucidates foundational principlesof prompt engineering, such as role-prompting, one-shot, and few-shotprompting, as well as more advanced methodologies such as the chain-of-thoughtand tree-of-thoughts prompting. The paper sheds light on how externalassistance in the form of plugins can assist in this task, and reduce machinehallucination by retrieving external knowledge. We subsequently delineateprospective directions in prompt engineering research, emphasizing the need fora deeper understanding of structures and the role of agents in ArtificialIntelligence-Generated Content (AIGC) tools. We discuss how to assess theefficacy of prompt methods from different perspectives and using differentmethods. Finally, we gather information about the application of promptengineering in such fields as education and programming, showing itstransformative potential. This comprehensive survey aims to serve as a friendlyguide for anyone venturing through the big world of LLMs and promptengineering."
Prompt Engineering For Students of Medicine and Their Teachers,['Thomas F. Heston'],http://arxiv.org/pdf/2308.11628v1.pdf,2023-08-08,['cs.hc'],"  ""Prompt Engineering for Students of Medicine and Their Teachers"" brings theprinciples of prompt engineering for large language models such as ChatGPT andGoogle Bard to medical education. This book contains a comprehensive guide toprompt engineering to help both teachers and students improve education in themedical field. Just as prompt engineering is critical in getting goodinformation out of an AI, it is also critical to get students to think andunderstand more deeply. The principles of prompt engineering that we havelearned from AI systems have the potential to simultaneously revolutionizelearning in the healthcare field. The book analyzes from multiple angles theanatomy of a good prompt for both AI models and students. The different typesof prompts are examined, showing how each style has unique characteristics andapplications. The principles of prompt engineering, applied properly, aredemonstrated to be effective in teaching across the diverse fields of anatomy,physiology, pathology, pharmacology, and clinical skills. Just like ChatGPT andsimilar large language AI models, students need clear and detailed prompting inorder for them to fully understand a topic. Using identical principles, aprompt that gets good information from an AI will also cause a student to thinkmore deeply and accurately. The process of prompt engineering facilitates thisprocess. Because each chapter contains multiple examples and key takeaways, itis a practical guide for implementing prompt engineering in the learningprocess. It provides a hands-on approach to ensure readers can immediatelyapply the concepts they learn"
Prompt Engineering a Prompt Engineer,"['Qinyuan Ye', 'Maxamed Axmed', 'Reid Pryzant', 'Fereshte Khani']",http://arxiv.org/pdf/2311.05661v1.pdf,2023-11-09,"['cs.cl', 'cs.ai', 'cs.lg']","  Prompt engineering is a challenging yet crucial task for optimizing theperformance of large language models (LLMs). It requires complex reasoning toexamine the model's errors, hypothesize what is missing or misleading in thecurrent prompt, and communicate the task with clarity. While recent worksindicate that LLMs can be meta-prompted to perform automatic promptengineering, their potentials may not be fully untapped due to the lack ofsufficient guidance to elicit complex reasoning capabilities in LLMs in themeta-prompt. In this work, we investigate the problem of ""prompt engineering aprompt engineer"" -- constructing a meta-prompt that more effectively guidesLLMs to perform automatic prompt engineering. We introduce and analyze keycomponents, such as a step-by-step reasoning template and contextspecification, which lead to improved performance. In addition, inspired bycommon optimization concepts such as batch size, step size and momentum, weintroduce their verbalized counterparts to the meta-prompt and investigatetheir effects. Our final method, named PE2, finds a prompt that outperforms""let's think step by step"" by 6.3% on the MultiArith dataset and 3.1% on theGSM8K dataset. To demonstrate its versatility, we apply PE2 to the InstructionInduction benchmark, a suite of counterfactual tasks, and a lengthy, real-worldindustrial prompt. In these settings, PE2 achieves strong performance andoutperforms prior automatic prompt engineering baselines. Further, we show thatPE2 makes meaningful and targeted prompt edits, amends erroneous or incompleteprompts, and presents non-trivial counterfactual reasoning abilities."
Review of Large Vision Models and Visual Prompt Engineering,"['Jiaqi Wang', 'Zhengliang Liu', 'Lin Zhao', 'Zihao Wu', 'Chong Ma', 'Sigang Yu', 'Haixing Dai', 'Qiushi Yang', 'Yiheng Liu', 'Songyao Zhang', 'Enze Shi', 'Yi Pan', 'Tuo Zhang', 'Dajiang Zhu', 'Xiang Li', 'Xi Jiang', 'Bao Ge', 'Yixuan Yuan', 'Dinggang Shen', 'Tianming Liu', 'Shu Zhang']",http://arxiv.org/pdf/2307.00855v1.pdf,2023-07-03,"['cs.cv', 'cs.ai']","  Visual prompt engineering is a fundamental technology in the field of visualand image Artificial General Intelligence, serving as a key component forachieving zero-shot capabilities. As the development of large vision modelsprogresses, the importance of prompt engineering becomes increasingly evident.Designing suitable prompts for specific visual tasks has emerged as ameaningful research direction. This review aims to summarize the methodsemployed in the computer vision domain for large vision models and visualprompt engineering, exploring the latest advancements in visual promptengineering. We present influential large models in the visual domain and arange of prompt engineering methods employed on these models. It is our hopethat this review provides a comprehensive and systematic description of promptengineering methods based on large visual models, offering valuable insightsfor future researchers in their exploration of this field."
Prompt Engineering for Healthcare: Methodologies and Applications,"['Jiaqi Wang', 'Enze Shi', 'Sigang Yu', 'Zihao Wu', 'Chong Ma', 'Haixing Dai', 'Qiushi Yang', 'Yanqing Kang', 'Jinru Wu', 'Huawen Hu', 'Chenxi Yue', 'Haiyang Zhang', 'Yiheng Liu', 'Xiang Li', 'Bao Ge', 'Dajiang Zhu', 'Yixuan Yuan', 'Dinggang Shen', 'Tianming Liu', 'Shu Zhang']",http://arxiv.org/pdf/2304.14670v1.pdf,2023-04-28,['cs.ai'],"  This review will introduce the latest advances in prompt engineering in thefield of natural language processing (NLP) for the medical domain. First, wewill provide a brief overview of the development of prompt engineering andemphasize its significant contributions to healthcare NLP applications such asquestion-answering systems, text summarization, and machine translation. Withthe continuous improvement of general large language models, the importance ofprompt engineering in the healthcare domain is becoming increasingly prominent.The aim of this article is to provide useful resources and bridges forhealthcare NLP researchers to better explore the application of promptengineering in this field. We hope that this review can provide new ideas andinspire ample possibilities for research and application in medical NLP."
A Brief History of Prompt: Leveraging Language Models. (Through Advanced  Prompting),['Golam Md Muktadir'],http://arxiv.org/pdf/2310.04438v2.pdf,2023-09-30,"['cs.cl', 'cs.ai']","  This paper presents a comprehensive exploration of the evolution of promptengineering and generation in the field of natural language processing (NLP).Starting from the early language models and information retrieval systems, wetrace the key developments that have shaped prompt engineering over the years.The introduction of attention mechanisms in 2015 revolutionized languageunderstanding, leading to advancements in controllability andcontext-awareness. Subsequent breakthroughs in reinforcement learningtechniques further enhanced prompt engineering, addressing issues like exposurebias and biases in generated text. We examine the significant contributions in2018 and 2019, focusing on fine-tuning strategies, control codes, andtemplate-based generation. The paper also discusses the growing importance offairness, human-AI collaboration, and low-resource adaptation. In 2020 and2021, contextual prompting and transfer learning gained prominence, while 2022and 2023 witnessed the emergence of advanced techniques like unsupervisedpre-training and novel reward shaping. Throughout the paper, we referencespecific research studies that exemplify the impact of various developments onprompt engineering. The journey of prompt engineering continues, with ethicalconsiderations being paramount for the responsible and inclusive future of AIsystems."
A Systematic Survey of Prompt Engineering on Vision-Language Foundation  Models,"['Jindong Gu', 'Zhen Han', 'Shuo Chen', 'Ahmad Beirami', 'Bailan He', 'Gengyuan Zhang', 'Ruotong Liao', 'Yao Qin', 'Volker Tresp', 'Philip Torr']",http://arxiv.org/pdf/2307.12980v1.pdf,2023-07-24,['cs.cv'],"  Prompt engineering is a technique that involves augmenting a largepre-trained model with task-specific hints, known as prompts, to adapt themodel to new tasks. Prompts can be created manually as natural languageinstructions or generated automatically as either natural language instructionsor vector representations. Prompt engineering enables the ability to performpredictions based solely on prompts without updating model parameters, and theeasier application of large pre-trained models in real-world tasks. In pastyears, Prompt engineering has been well-studied in natural language processing.Recently, it has also been intensively studied in vision-language modeling.However, there is currently a lack of a systematic overview of promptengineering on pre-trained vision-language models. This paper aims to provide acomprehensive survey of cutting-edge research in prompt engineering on threetypes of vision-language models: multimodal-to-text generation models (e.g.Flamingo), image-text matching models (e.g. CLIP), and text-to-image generationmodels (e.g. Stable Diffusion). For each type of model, a brief model summary,prompting methods, prompting-based applications, and the correspondingresponsibility and integrity issues are summarized and discussed. Furthermore,the commonalities and differences between prompting on vision-language models,language models, and vision models are also discussed. The challenges, futuredirections, and research opportunities are summarized to foster future researchon this topic."
Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning,['Chenkai Ma'],http://arxiv.org/pdf/2304.06962v1.pdf,2023-04-14,"['cs.cl', 'cs.ai']","  Prompt engineering and calibration make large language models excel atreasoning tasks, including multiple choice commonsense reasoning. From apractical perspective, we investigate and evaluate these strategies on smallerlanguage models. Through experiments on five commonsense reasoning benchmarks,we find that each strategy favors certain models, but their joint effects aremostly negative."
Just Tell Me: Prompt Engineering in Business Process Management,"['Kiran Busch', 'Alexander Rochlitzer', 'Diana Sola', 'Henrik Leopold']",http://arxiv.org/pdf/2304.07183v1.pdf,2023-04-14,"['cs.ai', 'cs.cl', 'cs.lg']","  GPT-3 and several other language models (LMs) can effectively address variousnatural language processing (NLP) tasks, including machine translation and textsummarization. Recently, they have also been successfully employed in thebusiness process management (BPM) domain, e.g., for predictive processmonitoring and process extraction from text. This, however, typically requiresfine-tuning the employed LM, which, among others, necessitates large amounts ofsuitable training data. A possible solution to this problem is the use ofprompt engineering, which leverages pre-trained LMs without fine-tuning them.Recognizing this, we argue that prompt engineering can help bring thecapabilities of LMs to BPM research. We use this position paper to develop aresearch agenda for the use of prompt engineering for BPM research byidentifying the associated potentials and challenges."
Revisiting Prompt Engineering via Declarative Crowdsourcing,"['Aditya G. Parameswaran', 'Shreya Shankar', 'Parth Asawa', 'Naman Jain', 'Yujie Wang']",http://arxiv.org/pdf/2308.03854v1.pdf,2023-08-07,"['cs.db', 'cs.ai', 'cs.hc', 'cs.lg']","  Large language models (LLMs) are incredibly powerful at comprehending andgenerating data in the form of text, but are brittle and error-prone. There hasbeen an advent of toolkits and recipes centered around so-called promptengineering-the process of asking an LLM to do something via a series ofprompts. However, for LLM-powered data processing workflows, in particular,optimizing for quality, while keeping cost bounded, is a tedious, manualprocess. We put forth a vision for declarative prompt engineering. We view LLMslike crowd workers and leverage ideas from the declarative crowdsourcingliterature-including leveraging multiple prompting strategies, ensuringinternal consistency, and exploring hybrid-LLM-non-LLM approaches-to makeprompt engineering a more principled process. Preliminary case studies onsorting, entity resolution, and imputation demonstrate the promise of ourapproach"
Data-Driven Approach for Formality-Sensitive Machine Translation:  Language-Specific Handling and Synthetic Data Generation,"['Seugnjun Lee', 'Hyeonseok Moon', 'Chanjun Park', 'Heuiseok Lim']",http://arxiv.org/pdf/2306.14514v2.pdf,2023-06-26,"['cs.cl', 'cs.ai']","  In this paper, we introduce a data-driven approach for Formality-SensitiveMachine Translation (FSMT) that caters to the unique linguistic properties offour target languages. Our methodology centers on two core strategies: 1)language-specific data handling, and 2) synthetic data generation usinglarge-scale language models and empirical prompt engineering. This approachdemonstrates a considerable improvement over the baseline, highlighting theeffectiveness of data-centric techniques. Our prompt engineering strategyfurther improves performance by producing superior synthetic translationexamples."
Exploring the Intersection of Large Language Models and Agent-Based  Modeling via Prompt Engineering,['Edward Junprung'],http://arxiv.org/pdf/2308.07411v1.pdf,2023-08-14,"['cs.ai', 'cs.ma']","  The final frontier for simulation is the accurate representation of complex,real-world social systems. While agent-based modeling (ABM) seeks to study thebehavior and interactions of agents within a larger system, it is unable tofaithfully capture the full complexity of human-driven behavior. Large languagemodels (LLMs), like ChatGPT, have emerged as a potential solution to thisbottleneck by enabling researchers to explore human-driven interactions inpreviously unimaginable ways. Our research investigates simulations of humaninteractions using LLMs. Through prompt engineering, inspired by Park et al.(2023), we present two simulations of believable proxies of human behavior: atwo-agent negotiation and a six-agent murder mystery game."
Large Language Models Are Human-Level Prompt Engineers,"['Yongchao Zhou', 'Andrei Ioan Muresanu', 'Ziwen Han', 'Keiran Paster', 'Silviu Pitis', 'Harris Chan', 'Jimmy Ba']",http://arxiv.org/pdf/2211.01910v2.pdf,2022-11-03,"['cs.lg', 'cs.ai', 'cs.cl']","  By conditioning on natural language instructions, large language models(LLMs) have displayed impressive capabilities as general-purpose computers.However, task performance depends significantly on the quality of the promptused to steer the model, and most effective prompts have been handcrafted byhumans. Inspired by classical program synthesis and the human approach toprompt engineering, we propose Automatic Prompt Engineer (APE) for automaticinstruction generation and selection. In our method, we treat the instructionas the ""program,"" optimized by searching over a pool of instruction candidatesproposed by an LLM in order to maximize a chosen score function. To evaluatethe quality of the selected instruction, we evaluate the zero-shot performanceof another LLM following the selected instruction. Experiments on 24 NLP tasksshow that our automatically generated instructions outperform the prior LLMbaseline by a large margin and achieve better or comparable performance to theinstructions generated by human annotators on 19/24 tasks. We conduct extensivequalitative and quantitative analyses to explore the performance of APE. Weshow that APE-engineered prompts can be applied to steer models towardtruthfulness and/or informativeness, as well as to improve few-shot learningperformance by simply prepending them to standard in-context learning prompts.Please check out our webpage athttps://sites.google.com/view/automatic-prompt-engineer."
Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate  Fairytales,['Martin Ruskov'],http://arxiv.org/pdf/2302.08961v2.pdf,2023-02-17,"['cs.cl', 'cs.ai', 'cs.hc', 'i.2']","  The quality of text-to-image generation is continuously improving, yet theboundaries of its applicability are still unclear. In particular, refinement ofthe text input with the objective of achieving better results - commonly calledprompt engineering - so far seems to have not been geared towards work withpre-existing texts. We investigate whether text-to-image generation and promptengineering could be used to generate basic illustrations of popularfairytales. Using Midjourney v4, we engage in action research with a dual aim:to attempt to generate 5 believable illustrations for each of 5 popularfairytales, and to define a prompt engineering process that starts from apre-existing text and arrives at an illustration of it. We arrive at atentative 4-stage process: i) initial prompt, ii) composition adjustment, iii)style refinement, and iv) variation selection. We also discuss three reasonswhy the generation model struggles with certain illustrations: difficultieswith counts, bias from stereotypical configurations and inability to depictoverly fantastic situations. Our findings are not limited to the specificgeneration model and are intended to be generalisable to future ones."
A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT,"['Jules White', 'Quchen Fu', 'Sam Hays', 'Michael Sandborn', 'Carlos Olea', 'Henry Gilbert', 'Ashraf Elnashar', 'Jesse Spencer-Smith', 'Douglas C. Schmidt']",http://arxiv.org/pdf/2302.11382v1.pdf,2023-02-21,"['cs.se', 'cs.ai']","  Prompt engineering is an increasingly important skill set needed to converseeffectively with large language models (LLMs), such as ChatGPT. Prompts areinstructions given to an LLM to enforce rules, automate processes, and ensurespecific qualities (and quantities) of generated output. Prompts are also aform of programming that can customize the outputs and interactions with anLLM. This paper describes a catalog of prompt engineering techniques presentedin pattern form that have been applied to solve common problems when conversingwith LLMs. Prompt patterns are a knowledge transfer method analogous tosoftware patterns since they provide reusable solutions to common problemsfaced in a particular context, i.e., output generation and interaction whenworking with LLMs. This paper provides the following contributions to researchon prompt engineering that apply LLMs to automate software development tasks.First, it provides a framework for documenting patterns for structuring promptsto solve a range of problems so that they can be adapted to different domains.Second, it presents a catalog of patterns that have been applied successfullyto improve the outputs of LLM conversations. Third, it explains how prompts canbe built from multiple patterns and illustrates prompt patterns that benefitfrom combination with other prompt patterns."
Prompt Space Optimizing Few-shot Reasoning Success with Large Language  Models,"['Fobo Shi', 'Peijun Qing', 'Dong Yang', 'Nan Wang', 'Youbo Lei', 'Haonan Lu', 'Xiaodong Lin']",http://arxiv.org/pdf/2306.03799v1.pdf,2023-06-06,['cs.cl'],"  Prompt engineering is an essential technique for enhancing the abilities oflarge language models (LLMs) by providing explicit and specific instructions.It enables LLMs to excel in various tasks, such as arithmetic reasoning,question answering, summarization, relation extraction, machine translation,and sentiment analysis. Researchers have been actively exploring differentprompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, andIn-context learning. However, an unresolved problem arises from the fact thatcurrent approaches lack a solid theoretical foundation for determining optimalprompts. To address this issue in prompt engineering, we propose a new andeffective approach called Prompt Space. Our methodology utilizes textembeddings to obtain basis vectors by matrix decomposition, and then constructsa space for representing all prompts. Prompt Space significantly outperformsstate-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably,without the help of the CoT method and the prompt ""Let's think step by step"",Prompt Space shows superior performance over the few-shot method. Overall, ourapproach provides a robust and fundamental theoretical framework for selectingsimple and effective prompts. This advancement marks a significant step towardsimproving prompt engineering for a wide variety of applications in LLMs."
An Empirical Evaluation of Prompting Strategies for Large Language  Models in Zero-Shot Clinical Natural Language Processing,"['Sonish Sivarajkumar', 'Mark Kelley', 'Alyssa Samolyk-Mazzanti', 'Shyam Visweswaran', 'Yanshan Wang']",http://arxiv.org/pdf/2309.08008v1.pdf,2023-09-14,"['cs.cl', 'cs.ai']","  Large language models (LLMs) have shown remarkable capabilities in NaturalLanguage Processing (NLP), especially in domains where labeled data is scarceor expensive, such as clinical domain. However, to unlock the clinicalknowledge hidden in these LLMs, we need to design effective prompts that canguide them to perform specific clinical NLP tasks without any task-specifictraining data. This is known as in-context learning, which is an art andscience that requires understanding the strengths and weaknesses of differentLLMs and prompt engineering approaches. In this paper, we present acomprehensive and systematic experimental study on prompt engineering for fiveclinical NLP tasks: Clinical Sense Disambiguation, Biomedical EvidenceExtraction, Coreference Resolution, Medication Status Extraction, andMedication Attribute Extraction. We assessed the prompts proposed in recentliterature, including simple prefix, simple cloze, chain of thought, andanticipatory prompts, and introduced two new types of prompts, namely heuristicprompting and ensemble prompting. We evaluated the performance of these promptson three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrastedzero-shot prompting with few-shot prompting, and provide novel insights andguidelines for prompt engineering for LLMs in clinical NLP. To the best of ourknowledge, this is one of the first works on the empirical evaluation ofdifferent prompt engineering approaches for clinical NLP in this era ofgenerative AI, and we hope that it will inspire and inform future research inthis area."
How understanding large language models can inform the use of ChatGPT in  physics education,"['Giulia Polverini', 'Bor Gregorcic']",http://arxiv.org/pdf/2309.12074v4.pdf,2023-09-21,['physics.ed-ph'],"  The paper aims to fulfil three main functions: (1) to serve as anintroduction for the physics education community to the functioning of LargeLanguage Models (LLMs), (2) to present a series of illustrative examplesdemonstrating how prompt-engineering techniques can impact LLMs performance onconceptual physics tasks and (3) to discuss potential implications of theunderstanding of LLMs and prompt engineering for physics teaching and learning.We first summarise existing research on the performance of a popular LLM-basedchatbot (ChatGPT) on physics tasks. We then give a basic account of how LLMswork, illustrate essential features of their functioning, and discuss theirstrengths and limitations. Equipped with this knowledge, we discuss somechallenges with generating useful output with ChatGPT-4 in the context ofintroductory physics, paying special attention to conceptual questions andproblems. We then provide a condensed overview of relevant literature on promptengineering and demonstrate through illustrative examples how selectedprompt-engineering techniques can be employed to improve ChatGPT-4's output onconceptual introductory physics problems. Qualitatively studying these examplesprovides additional insights into ChatGPT's functioning and its utility inphysics problem solving. Finally, we consider how insights from the paper caninform the use of LLMs in the teaching and learning of physics."
Prompt Engineering or Fine Tuning: An Empirical Assessment of Large  Language Models in Automated Software Engineering Tasks,"['Jiho Shin', 'Clark Tang', 'Tahmineh Mohati', 'Maleknaz Nayebi', 'Song Wang', 'Hadi Hemmati']",http://arxiv.org/pdf/2310.10508v1.pdf,2023-10-11,['cs.se'],"  In this paper, we investigate the effectiveness of state-of-the-art LLM,i.e., GPT-4, with three different prompting engineering techniques (i.e., basicprompting, in-context learning, and task-specific prompting) against 18fine-tuned LLMs on three typical ASE tasks, i.e., code generation, codesummarization, and code translation. Our quantitative analysis of theseprompting strategies suggests that prompt engineering GPT-4 cannot necessarilyand significantly outperform fine-tuning smaller/older LLMs in all three tasks.For comment generation, GPT-4 with the best prompting strategy (i.e.,task-specific prompt) had outperformed the first-ranked fine-tuned model by8.33% points on average in BLEU. However, for code generation, the first-rankedfine-tuned model outperforms GPT-4 with best prompting by 16.61% and 28.3%points, on average in BLEU. For code translation, GPT-4 and fine-tunedbaselines tie as they outperform each other on different translation tasks. Toexplore the impact of different prompting strategies, we conducted a user studywith 27 graduate students and 10 industry practitioners. From our qualitativeanalysis, we find that the GPT-4 with conversational prompts (i.e., when ahuman provides feedback and instructions back and forth with a model to achievebest results) showed drastic improvement compared to GPT-4 with automaticprompting strategies. Moreover, we observe that participants tend to requestimprovements, add more context, or give specific instructions as conversationalprompts, which goes beyond typical and generic prompting strategies. Our studysuggests that, at its current state, GPT-4 with conversational prompting hasgreat potential for ASE tasks, but fully automated prompt engineering with nohuman in the loop requires more study and improvement."
Speak Like a Native: Prompting Large Language Models in a Native Style,"['Zhicheng Yang', 'Yiwei Wang', 'Yinya Huang', 'Jing Xiong', 'Xiaodan Liang', 'Jing Tang']",http://arxiv.org/pdf/2311.13538v1.pdf,2023-11-22,"['cs.ai', 'cs.lg']","  Existing work has found that the prompt engineering heavily influences theperformance of large language models (LLMs). Chain-of-thought (CoT), as apopular prompt engineering technique, prompted LLMs using in-context exampleswith reasoning steps. In current studies, the few-shot examples of CoT aregenerally handcrafted by humans. However, how the text style of in-contextexamples influence the outputs of LLMs still remains under-explored. This paperpresents a novel and effective approach, named \textbf{AlignCoT}, to improvethe reasoning capability of LLMs by aligning the in-context examples with thenative style of LLMs. ``Native'' refers to the inherent characteristic style ofLLMs which can be probed by original zero-shot scenarios. AlignCoT isorthogonal to other prompt engineering methods, making it easy to combine withstate-of-the-art techniques to further improve the LLMs' performance. Weconduct extensive and comprehensive experiments on several benchmarks. Theempirical results demonstrate that our AlignCoTsignificantly improvesperformance over the carefully handcrafted in-context examples. For instance,with GPT-3.5-turbo, we observed a +2.5\% improvement on GSM8K. Furthermore, ourAlignCoT consistently improve the performance when combined with otherstate-of-the-art prompt engineering methods. The source code and dataset willbe available at\href{https://github.com/yangzhch6/AlignCoT}{https://github.com/yangzhch6/AlignCoT}."
An Information-theoretic Approach to Prompt Engineering Without Ground  Truth Labels,"['Taylor Sorensen', 'Joshua Robinson', 'Christopher Michael Rytting', 'Alexander Glenn Shaw', 'Kyle Jeffrey Rogers', 'Alexia Pauline Delorey', 'Mahmoud Khalil', 'Nancy Fulda', 'David Wingate']",http://arxiv.org/pdf/2203.11364v1.pdf,2022-03-21,"['cs.cl', 'cs.lg']","  Pre-trained language models derive substantial linguistic and factualknowledge from the massive corpora on which they are trained, and promptengineering seeks to align these models to specific tasks. Unfortunately,existing prompt engineering methods require significant amounts of labeleddata, access to model parameters, or both. We introduce a new method forselecting prompt templates \textit{without labeled examples} and\textit{without direct access to the model}. Specifically, over a set ofcandidate templates, we choose the template that maximizes the mutualinformation between the input and the corresponding model output. Across 8datasets representing 7 distinct NLP tasks, we show that when a template hashigh mutual information, it also has high accuracy on the task. On the largestmodel, selecting prompts with our method gets 90\% of the way from the averageprompt accuracy to the best prompt accuracy and requires no ground truthlabels."
Unsupervised Prompt Learning for Vision-Language Models,"['Tony Huang', 'Jack Chu', 'Fangyun Wei']",http://arxiv.org/pdf/2204.03649v2.pdf,2022-04-07,['cs.cv'],"  Contrastive vision-language models like CLIP have shown great progress intransfer learning. In the inference stage, the proper text description, alsoknown as prompt, needs to be carefully designed to correctly classify the givenimages. In order to avoid laborious prompt engineering, recent works such asCoOp, CLIP-Adapter and Tip-Adapter propose to adapt vision-language models fordownstream image recognition tasks on a small set of labeled data. Thoughpromising improvements are achieved, requiring labeled data from the targetdatasets may restrict the scalability. In this paper, we explore a differentscenario, in which the labels of the target datasets are unprovided, and wepresent an unsupervised prompt learning (UPL) approach to avoid promptengineering while simultaneously improving transfer performance of CLIP-likevision-language models. As far as we know, UPL is the first work to introduceunsupervised learning into prompt learning. Experimentally, our UPL outperformsoriginal CLIP with prompt engineering on ImageNet as well as other 10 datasets.An enhanced version of UPL is even competitive with the 8-shot CoOp and the8-shot TIP-Adapter on most datasets. Code and models are available athttps://github.com/tonyhuang2022/UPL."
ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis  Testing,"['Ian Arawjo', 'Chelse Swoopes', 'Priyan Vaithilingam', 'Martin Wattenberg', 'Elena Glassman']",http://arxiv.org/pdf/2309.09128v1.pdf,2023-09-17,"['cs.hc', 'cs.ai', 'h.5.2; i.2']","  Evaluating outputs of large language models (LLMs) is challenging, requiringmaking -- and making sense of -- many responses. Yet tools that go beyond basicprompting tend to require knowledge of programming APIs, focus on narrowdomains, or are closed-source. We present ChainForge, an open-source visualtoolkit for prompt engineering and on-demand hypothesis testing of textgeneration LLMs. ChainForge provides a graphical interface for comparison ofresponses across models and prompt variations. Our system was designed tosupport three tasks: model selection, prompt template design, and hypothesistesting (e.g., auditing). We released ChainForge early in its development anditerated on its design with academics and online users. Through in-lab andinterview studies, we find that a range of people could use ChainForge toinvestigate hypotheses that matter to them, including in real-world settings.We identify three modes of prompt engineering and LLM hypothesis testing:opportunistic exploration, limited evaluation, and iterative refinement."
CoPrompt: Supporting Prompt Sharing and Referring in Collaborative  Natural Language Programming,"['Felicia Li Feng', 'Ryan Yen', 'Yuzhe You', 'Mingming Fan', 'Jian Zhao', 'Zhicong Lu']",http://arxiv.org/pdf/2310.09235v1.pdf,2023-10-13,['cs.hc'],"  Natural language (NL) programming has become more approachable due to thepowerful code-generation capability of large language models (LLMs). This shiftto using NL to program enhances collaborative programming by reducingcommunication barriers and context-switching among programmers from varyingbackgrounds. However, programmers may face challenges during prompt engineeringin a collaborative setting as they need to actively keep aware of theircollaborators' progress and intents. In this paper, we aim to investigate waysto assist programmers' prompt engineering in a collaborative context. We firstconducted a formative study to understand the workflows and challenges ofprogrammers when using NL for collaborative programming. Based on our findings,we implemented a prototype, CoPrompt, to support collaborative promptengineering by providing referring, requesting, sharing, and linkingmechanisms. Our user study indicates that CoPrompt assists programmers incomprehending collaborators' prompts and building on their collaborators' work,reducing repetitive updates and communication costs."
Prompt-Engineering and Transformer-based Question Generation and  Evaluation,['Rubaba Amyeen'],http://arxiv.org/pdf/2310.18867v1.pdf,2023-10-29,"['cs.cl', 'cs.ai']","  Question generation has numerous applications in the educational context.Question generation can prove helpful for students when reviewing content andtesting themselves. Furthermore, a question generation model can aid teachersby lessening the burden of creating assessments and other practice material.This paper aims to find the best method to generate questions from textual datathrough a transformer model and prompt engineering. In this research, wefinetuned a pretrained distilBERT model on the SQuAD question answering datasetto generate questions. In addition to training a transformer model, promptengineering was applied to generate questions effectively using the LLaMAmodel. The generated questions were compared against the baseline questions inthe SQuAD dataset to evaluate the effectiveness of four different prompts. Allfour prompts demonstrated over 60% similarity on average. Of theprompt-generated questions, 30% achieved a high similarity score greater than70%."
To be or not to be? an exploration of continuously controllable prompt  engineering,"['Yuhan Sun', 'Mukai Li', 'Yixin Cao', 'Kun Wang', 'Wenxiao Wang', 'Xingyu Zeng', 'Rui Zhao']",http://arxiv.org/pdf/2311.09773v1.pdf,2023-11-16,['cs.cl'],"  As the use of large language models becomes more widespread, techniques likeparameter-efficient fine-tuning and other methods for controlled generation aregaining traction for customizing models and managing their outputs. However,the challenge of precisely controlling how prompts influence these models is anarea ripe for further investigation. In response, we introduce ControlPE(Continuously Controllable Prompt Engineering). ControlPE enables fineradjustments to prompt effects, complementing existing prompt engineering, andeffectively controls continuous targets. This approach harnesses the power ofLoRA (Low-Rank Adaptation) to create an effect akin to prompt weighting,enabling fine-tuned adjustments to the impact of prompts. Our methodologyinvolves generating specialized datasets for prompt distillation, incorporatingthese prompts into the LoRA model, and carefully adjusting LoRA merging weightto regulate the influence of prompts. This provides a dynamic and adaptabletool for prompt control. Through our experiments, we have validated thepracticality and efficacy of ControlPE. It proves to be a promising solutionfor control a variety of prompts, ranging from generating short responsesprompts, refusal prompts to chain-of-thought prompts."
More Samples or More Prompt Inputs? Exploring Effective In-Context  Sampling for LLM Few-Shot Prompt Engineering,"['Bingsheng Yao', 'Guiming Chen', 'Ruishi Zou', 'Yuxuan Lu', 'Jiachen Li', 'Shao Zhang', 'Sijia Liu', 'James Hendler', 'Dakuo Wang']",http://arxiv.org/pdf/2311.09782v1.pdf,2023-11-16,['cs.cl'],"  While most existing works on LLM prompt-engineering focus only on how toselect a better set of data samples inside one single prompt input (In-ContextLearning or ICL), why can't we design and leverage multiple prompt inputstogether to further improve the LLM performance? In this work, we proposeIn-Context Sampling (ICS), a low-resource LLM prompt-engineering technique toproduce the most confident prediction results by optimizing the construction ofmultiple ICL prompt inputs. Extensive experiments with two SOTA LLMs (FlanT5-XLand Mistral-7B) on three NLI datasets (e-SNLI, Multi-NLI, and ANLI) illustratethat ICS can consistently enhance LLM's prediction performance and confidence.An ablation study suggests that a diversity-based ICS strategy may furtherimprove LLM's performance, which sheds light on a new yet promising futureresearch direction."
Automatic Engineering of Long Prompts,"['Cho-Jui Hsieh', 'Si Si', 'Felix X. Yu', 'Inderjit S. Dhillon']",http://arxiv.org/pdf/2311.10117v1.pdf,2023-11-16,"['cs.ai', 'cs.lg']","  Large language models (LLMs) have demonstrated remarkable capabilities insolving complex open-domain tasks, guided by comprehensive instructions anddemonstrations provided in the form of prompts. However, these prompts can belengthy, often comprising hundreds of lines and thousands of tokens, and theirdesign often requires considerable human effort. Recent research has exploredautomatic prompt engineering for short prompts, typically consisting of one ora few sentences. However, the automatic design of long prompts remains achallenging problem due to its immense search space. In this paper, weinvestigate the performance of greedy algorithms and genetic algorithms forautomatic long prompt engineering. We demonstrate that a simple greedy approachwith beam search outperforms other methods in terms of search efficiency.Moreover, we introduce two novel techniques that utilize search history toenhance the effectiveness of LLM-based mutation in our search algorithm. Ourresults show that the proposed automatic long prompt engineering algorithmachieves an average of 9.2% accuracy gain on eight tasks in Big Bench Hard,highlighting the significance of automating prompt designs to fully harness thecapabilities of LLMs."
Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on  Prompt Engineering Strategies,"['Pengcheng Chen', 'Ziyan Huang', 'Zhongying Deng', 'Tianbin Li', 'Yanzhou Su', 'Haoyu Wang', 'Jin Ye', 'Yu Qiao', 'Junjun He']",http://arxiv.org/pdf/2312.04344v2.pdf,2023-12-07,"['cs.cl', 'cs.ai', 'cs.cv', 'cs.lg']","  OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piquedconsiderable interest for its potential in medical applications. Despite itspromise, recent studies and internal reviews highlight its underperformance inspecialized medical tasks. This paper explores the boundary of GPT-4V'scapabilities in medicine, particularly in processing complex imaging data fromendoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, weassessed its foundational competencies, identifying substantial areas forenhancement. Our research emphasizes prompt engineering, an often-underutilizedstrategy for improving AI responsiveness. Through iterative testing, we refinedthe model's prompts, significantly improving its interpretative accuracy andrelevance in medical imaging. From our comprehensive evaluations, we distilled10 effective prompt engineering techniques, each fortifying GPT-4V's medicalacumen. These methodical enhancements facilitate more reliable, precise, andclinically valuable insights from GPT-4V, advancing its operability in criticalhealthcare environments. Our findings are pivotal for those employing AI inmedicine, providing clear, actionable guidance on harnessing GPT-4V's fulldiagnostic potential."
A Simple Zero-shot Prompt Weighting Technique to Improve Prompt  Ensembling in Text-Image Models,"['James Urquhart Allingham', 'Jie Ren', 'Michael W Dusenberry', 'Xiuye Gu', 'Yin Cui', 'Dustin Tran', 'Jeremiah Zhe Liu', 'Balaji Lakshminarayanan']",http://arxiv.org/pdf/2302.06235v2.pdf,2023-02-13,"['cs.lg', 'cs.cv', 'stat.ml']","  Contrastively trained text-image models have the remarkable ability toperform zero-shot classification, that is, classifying previously unseen imagesinto categories that the model has never been explicitly trained to identify.However, these zero-shot classifiers need prompt engineering to achieve highaccuracy. Prompt engineering typically requires hand-crafting a set of promptsfor individual downstream tasks. In this work, we aim to automate this promptengineering and improve zero-shot accuracy through prompt ensembling. Inparticular, we ask ""Given a large pool of prompts, can we automatically scorethe prompts and ensemble those that are most suitable for a particulardownstream dataset, without needing access to labeled validation data?"". Wedemonstrate that this is possible. In doing so, we identify several pathologiesin a naive prompt scoring method where the score can be easily overconfidentdue to biases in pre-training and test data, and we propose a novel promptscoring method that corrects for the biases. Using our proposed scoring methodto create a weighted average prompt ensemble, our method outperforms equalaverage ensemble, as well as hand-crafted prompts, on ImageNet, 4 of itsvariants, and 11 fine-grained classification benchmarks, all while being fullyautomatic, optimization-free, and not requiring access to labeled validationdata."
Large Language Models in the Workplace: A Case Study on Prompt  Engineering for Job Type Classification,"['Benjamin Clavié', 'Alexandru Ciceu', 'Frederick Naylor', 'Guillaume Soulié', 'Thomas Brightwell']",http://arxiv.org/pdf/2303.07142v3.pdf,2023-03-13,['cs.cl'],"  This case study investigates the task of job classification in a real-worldsetting, where the goal is to determine whether an English-language job postingis appropriate for a graduate or entry-level position. We explore multipleapproaches to text classification, including supervised approaches such astraditional models like Support Vector Machines (SVMs) and state-of-the-artdeep learning methods such as DeBERTa. We compare them with Large LanguageModels (LLMs) used in both few-shot and zero-shot classification settings. Toaccomplish this task, we employ prompt engineering, a technique that involvesdesigning prompts to guide the LLMs towards the desired output. Specifically,we evaluate the performance of two commercially available state-of-the-artGPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We alsoconduct a detailed analysis of the impact of different aspects of promptengineering on the model's performance. Our results show that, with awell-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms allother models, achieving a 6% increase in Precision@95% Recall compared to thebest supervised approach. Furthermore, we observe that the wording of theprompt is a critical factor in eliciting the appropriate ""reasoning"" in themodel, and that seemingly minor aspects of the prompt significantly affect themodel's performance."
Simulating H.P. Lovecraft horror literature with the ChatGPT large  language model,"['Eduardo C. Garrido-Merchán', 'José Luis Arroyo-Barrigüete', 'Roberto Gozalo-Brizuela']",http://arxiv.org/pdf/2305.03429v1.pdf,2023-05-05,['cs.cl'],"  In this paper, we present a novel approach to simulating H.P. Lovecraft'shorror literature using the ChatGPT large language model, specifically theGPT-4 architecture. Our study aims to generate text that emulates Lovecraft'sunique writing style and themes, while also examining the effectiveness ofprompt engineering techniques in guiding the model's output. To achieve this,we curated a prompt containing several specialized literature references andemployed advanced prompt engineering methods. We conducted an empiricalevaluation of the generated text by administering a survey to a sample ofundergraduate students. Utilizing statistical hypothesis testing, we assessedthe students ability to distinguish between genuine Lovecraft works and thosegenerated by our model. Our findings demonstrate that the participants wereunable to reliably differentiate between the two, indicating the effectivenessof the GPT-4 model and our prompt engineering techniques in emulatingLovecraft's literary style. In addition to presenting the GPT model'scapabilities, this paper provides a comprehensive description of its underlyingarchitecture and offers a comparative analysis with related work that simulatesother notable authors and philosophers, such as Dennett. By exploring thepotential of large language models in the context of literary emulation, ourstudy contributes to the body of research on the applications and limitationsof these models in various creative domains."
CXR-LLaVA: Multimodal Large Language Model for Interpreting Chest X-ray  Images,"['Seowoo Lee', 'Jiwon Youn', 'Mansu Kim', 'Soon Ho Yoon']",http://arxiv.org/pdf/2310.18341v2.pdf,2023-10-22,"['cs.cl', 'cs.ai']","  Purpose: Recent advancements in large language models (LLMs) have expandedtheir capabilities in a multimodal fashion, potentially replicating the imageinterpretation of human radiologists. This study aimed to develop open-sourcemultimodal large language model for interpreting chest X-ray images(CXR-LLaVA). We also examined the effect of prompt engineering and modelparameters such as temperature and nucleus sampling.  Materials and Methods: For training, we collected 659,287 publicly availableCXRs: 417,336 CXRs had labels for certain radiographic abnormalities (dataset1); 241,951 CXRs provided free-text radiology reports (dataset 2). Afterpre-training the Resnet50 as an image encoder, the contrastive language-imagepre-training was used to align CXRs and corresponding radiographicabnormalities. Then, the Large Language Model Meta AI-2 was fine-tuned usingdataset 2, which were refined using GPT-4, with generating various questionanswering scenarios. The code can be found athttps://github.com/ECOFRI/CXR_LLaVA.  Results: In the test set, we observed that the model's performance fluctuatedbased on its parameters. On average, it achieved F1 score of 0.34 for fivepathologic findings (atelectasis, cardiomegaly, consolidation, edema, andpleural effusion), which was improved to 0.46 through prompt engineering. Inthe independent set, the model achieved an average F1 score of 0.30 for thesame pathologic findings. Notably, for the pediatric chest radiograph dataset,which was unseen during training, the model differentiated abnormal radiographswith an F1 score ranging from 0.84 to 0.85.  Conclusion: CXR-LLaVA demonstrates promising potential in CXR interpretation.Both prompt engineering and model parameter adjustments can play pivotal rolesin interpreting CXRs."
A Taxonomy of Prompt Modifiers for Text-To-Image Generation,['Jonas Oppenlaender'],http://arxiv.org/pdf/2204.13988v3.pdf,2022-04-20,"['cs.mm', 'cs.cl', 'cs.hc', 'h.5; h.m; j.5']","  Text-to-image generation has seen an explosion of interest since 2021. Today,beautiful and intriguing digital images and artworks can be synthesized fromtextual inputs (""prompts"") with deep generative models. Online communitiesaround text-to-image generation and AI generated art have quickly emerged. Thispaper identifies six types of prompt modifiers used by practitioners in theonline community based on a 3-month ethnographic study. The novel taxonomy ofprompt modifiers provides researchers a conceptual starting point forinvestigating the practice of text-to-image generation, but may also helppractitioners of AI generated art improve their images. We further outline howprompt modifiers are applied in the practice of ""prompt engineering."" Wediscuss research opportunities of this novel creative practice in the field ofHuman-Computer Interaction (HCI). The paper concludes with a discussion ofbroader implications of prompt engineering from the perspective of Human-AIInteraction (HAI) in future applications beyond the use case of text-to-imagegeneration and AI generated art."
What GPT Knows About Who is Who,"['Xiaohan Yang', 'Eduardo Peynetti', 'Vasco Meerman', 'Chris Tanner']",http://arxiv.org/pdf/2205.07407v1.pdf,2022-05-16,"['cs.cl', 'cs.lg']","  Coreference resolution -- which is a crucial task for understanding discourseand language at large -- has yet to witness widespread benefits from largelanguage models (LLMs). Moreover, coreference resolution systems largely relyon supervised labels, which are highly expensive and difficult to annotate,thus making it ripe for prompt engineering. In this paper, we introduce aQA-based prompt-engineering method and discern \textit{generative}, pre-trainedLLMs' abilities and limitations toward the task of coreference resolution. Ourexperiments show that GPT-2 and GPT-Neo can return valid answers, but thattheir capabilities to identify coreferent mentions are limited andprompt-sensitive, leading to inconsistent results."
Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements,"['Conrad Borchers', 'Dalia Sara Gala', 'Benjamin Gilburt', 'Eduard Oravkin', 'Wilfried Bounsi', 'Yuki M. Asano', 'Hannah Rose Kirk']",http://arxiv.org/pdf/2205.11374v1.pdf,2022-05-23,"['cs.cl', 'cs.ai']","  The growing capability and availability of generative language models hasenabled a wide range of new downstream tasks. Academic research has identified,quantified and mitigated biases present in language models but is rarelytailored to downstream tasks where wider impact on individuals and society canbe felt. In this work, we leverage one popular generative language model,GPT-3, with the goal of writing unbiased and realistic job advertisements. Wefirst assess the bias and realism of zero-shot generated advertisements andcompare them to real-world advertisements. We then evaluate prompt-engineeringand fine-tuning as debiasing methods. We find that prompt-engineering withdiversity-encouraging prompts gives no significant improvement to bias, norrealism. Conversely, fine-tuning, especially on unbiased real advertisements,can improve realism and reduce bias."
Arguments to Key Points Mapping with Prompt-based Learning,"['Ahnaf Mozib Samin', 'Behrooz Nikandish', 'Jingyan Chen']",http://arxiv.org/pdf/2211.14995v1.pdf,2022-11-28,['cs.cl'],"  Handling and digesting a huge amount of information in an efficient mannerhas been a long-term demand in modern society. Some solutions to map key points(short textual summaries capturing essential information and filteringredundancies) to a large number of arguments/opinions have been providedrecently (Bar-Haim et al., 2020). To complement the full picture of theargument-to-keypoint mapping task, we mainly propose two approaches in thispaper. The first approach is to incorporate prompt engineering for fine-tuningthe pre-trained language models (PLMs). The second approach utilizesprompt-based learning in PLMs to generate intermediary texts, which are thencombined with the original argument-keypoint pairs and fed as inputs to aclassifier, thereby mapping them. Furthermore, we extend the experiments tocross/in-domain to conduct an in-depth analysis. In our evaluation, we findthat i) using prompt engineering in a more direct way (Approach 1) can yieldpromising results and improve the performance; ii) Approach 2 performsconsiderably worse than Approach 1 due to the negation issue of the PLM."
Legal Prompt Engineering for Multilingual Legal Judgement Prediction,"['Dietrich Trautmann', 'Alina Petrova', 'Frank Schilder']",http://arxiv.org/pdf/2212.02199v1.pdf,2022-12-05,"['cs.cl', 'cs.ai']","  Legal Prompt Engineering (LPE) or Legal Prompting is a process to guide andassist a large language model (LLM) with performing a natural legal languageprocessing (NLLP) skill. Our goal is to use LPE with LLMs over long legaldocuments for the Legal Judgement Prediction (LJP) task. We investigate theperformance of zero-shot LPE for given facts in case-texts from the EuropeanCourt of Human Rights (in English) and the Federal Supreme Court of Switzerland(in German, French and Italian). Our results show that zero-shot LPE is bettercompared to the baselines, but it still falls short compared to current stateof the art supervised approaches. Nevertheless, the results are important,since there was 1) no explicit domain-specific data used - so we show that thetransfer to the legal domain is possible for general-purpose LLMs, and 2) theLLMs where directly applied without any further training or fine-tuning - whichin turn saves immensely in terms of additional computational costs."
The Infinite Index: Information Retrieval on Generative Text-To-Image  Models,"['Niklas Deckers', 'Maik Fröbe', 'Johannes Kiesel', 'Gianluca Pandolfo', 'Christopher Schröder', 'Benno Stein', 'Martin Potthast']",http://arxiv.org/pdf/2212.07476v2.pdf,2022-12-14,"['cs.ir', 'cs.cl', 'cs.cv']","  Conditional generative models such as DALL-E and Stable Diffusion generateimages based on a user-defined text, the prompt. Finding and refining promptsthat produce a desired image has become the art of prompt engineering.Generative models do not provide a built-in retrieval model for a user'sinformation need expressed through prompts. In light of an extensive literaturereview, we reframe prompt engineering for generative models as interactivetext-based retrieval on a novel kind of ""infinite index"". We apply theseinsights for the first time in a case study on image generation for game designwith an expert. Finally, we envision how active learning may help to guide theretrieval of generated images."
"Artificial Intelligence for Health Message Generation: Theory, Method,  and an Empirical Study Using Prompt Engineering","['Sue Lim', 'Ralf Schmälzle']",http://arxiv.org/pdf/2212.07507v1.pdf,2022-12-14,['cs.cl'],"  This study introduces and examines the potential of an AI system to generatehealth awareness messages. The topic of folic acid, a vitamin that is criticalduring pregnancy, served as a test case. Using prompt engineering, we generatedmessages that could be used to raise awareness and compared them to retweetedhuman-generated messages via computational and human evaluation methods. Thesystem was easy to use and prolific, and computational analyses revealed thatthe AI-generated messages were on par with human-generated ones in terms ofsentiment, reading ease, and semantic content. Also, the human evaluation studyshowed that AI-generated messages ranked higher in message quality and clarity.We discuss the theoretical, practical, and ethical implications of theseresults."
What does CLIP know about a red circle? Visual prompt engineering for  VLMs,"['Aleksandar Shtedritski', 'Christian Rupprecht', 'Andrea Vedaldi']",http://arxiv.org/pdf/2304.06712v2.pdf,2023-04-13,['cs.cv'],"  Large-scale Vision-Language Models, such as CLIP, learn powerful image-textrepresentations that have found numerous applications, from zero-shotclassification to text-to-image generation. Despite that, their capabilitiesfor solving novel discriminative tasks via prompting fall behind those of largelanguage models, such as GPT-3. Here we explore the idea of visual promptengineering for solving computer vision tasks beyond classification by editingin image space instead of text. In particular, we discover an emergent abilityof CLIP, where, by simply drawing a red circle around an object, we can directthe model's attention to that region, while also maintaining globalinformation. We show the power of this simple approach by achievingstate-of-the-art in zero-shot referring expressions comprehension and strongperformance in keypoint localization tasks. Finally, we draw attention to somepotential ethical concerns of large language-vision models."
Prompt Engineering for Transformer-based Chemical Similarity Search  Identifies Structurally Distinct Functional Analogues,"['Clayton W. Kosonocky', 'Aaron L. Feller', 'Claus O. Wilke', 'Andrew D. Ellington']",http://arxiv.org/pdf/2305.16330v1.pdf,2023-05-17,"['physics.chem-ph', 'cs.lg']","  Chemical similarity searches are widely used in-silico methods foridentifying new drug-like molecules. These methods have historically relied onstructure-based comparisons to compute molecular similarity. Here, we use achemical language model to create a vector-based chemical search. We extendimplementations by creating a prompt engineering strategy that utilizes twodifferent chemical string representation algorithms: one for the query and theother for the database. We explore this method by reviewing the search resultsfrom five drug-like query molecules (penicillin G, nirmatrelvir, zidovudine,lysergic acid diethylamide, and fentanyl) and three dye-like query molecules(acid blue 25, avobenzone, and 2-diphenylaminocarbazole). We find that thisnovel method identifies molecules that are functionally similar to the query,indicated by the associated patent literature, and that many of these moleculesare structurally distinct from the query, making them unlikely to be found withtraditional chemical similarity search methods. This method may aid in thediscovery of novel structural classes of molecules that achieve targetfunctionality."
Submodular Minimax Optimization: Finding Effective Sets,"['Loay Mualem', 'Ethan R. Elenberg', 'Moran Feldman', 'Amin Karbasi']",http://arxiv.org/pdf/2305.16903v1.pdf,2023-05-26,"['cs.lg', 'cs.dm', 'math.oc', '68r05 (primary) 90c26, 90c20, 68t20, 68w40 (secondary)', 'g.2.1; i.2.m; f.2.2']","  Despite the rich existing literature about minimax optimization in continuoussettings, only very partial results of this kind have been obtained forcombinatorial settings. In this paper, we fill this gap by providing acharacterization of submodular minimax optimization, the problem of finding aset (for either the min or the max player) that is effective against everypossible response. We show when and under what conditions we can find suchsets. We also demonstrate how minimax submodular optimization provides robustsolutions for downstream machine learning applications such as (i) efficientprompt engineering for question answering, (ii) prompt engineering for dialogstate tracking, (iii) identifying robust waiting locations for ride-sharing,(iv) ride-share difficulty kernelization, and (v) finding adversarial images.Our experiments demonstrate that our proposed algorithms consistentlyoutperform other baselines."
Unsupervised Human Activity Recognition through Two-stage Prompting with  ChatGPT,"['Qingxin Xia', 'Takuya Maekawa', 'Takahiro Hara']",http://arxiv.org/pdf/2306.02140v1.pdf,2023-06-03,"['cs.hc', 'cs.cl']","  Wearable sensor devices, which offer the advantage of recording daily objectsused by a person while performing an activity, enable the feasibility ofunsupervised Human Activity Recognition (HAR). Unfortunately, previousunsupervised approaches using the usage sequence of objects usually require aproper description of activities manually prepared by humans. Instead, weleverage the knowledge embedded in a Large Language Model (LLM) of ChatGPT.Because the sequence of objects robustly characterizes the activity identity,it is possible that ChatGPT already learned the association between activitiesand objects from existing contexts. However, previous prompt engineering forChatGPT exhibits limited generalization ability when dealing with a list ofwords (i.e., sequence of objects) due to the similar weighting assigned to eachword in the list. In this study, we propose a two-stage prompt engineering,which first guides ChatGPT to generate activity descriptions associated withobjects while emphasizing important objects for distinguishing similaractivities; then outputs activity classes and explanations for enhancing thecontexts that are helpful for HAR. To the best of our knowledge, this is thefirst study that utilizes ChatGPT to recognize activities using objects in anunsupervised manner. We conducted our approach on three datasets anddemonstrated the state-of-the-art performance."
User-friendly Image Editing with Minimal Text Input: Leveraging  Captioning and Injection Techniques,"['Sunwoo Kim', 'Wooseok Jang', 'Hyunsu Kim', 'Junho Kim', 'Yunjey Choi', 'Seungryong Kim', 'Gayeong Lee']",http://arxiv.org/pdf/2306.02717v1.pdf,2023-06-05,['cs.cv'],"  Recent text-driven image editing in diffusion models has shown remarkablesuccess. However, the existing methods assume that the user's descriptionsufficiently grounds the contexts in the source image, such as objects,background, style, and their relations. This assumption is unsuitable forreal-world applications because users have to manually engineer text prompts tofind optimal descriptions for different images. From the users' standpoint,prompt engineering is a labor-intensive process, and users prefer to provide atarget word for editing instead of a full sentence. To address this problem, wefirst demonstrate the importance of a detailed text description of the sourceimage, by dividing prompts into three categories based on the level of semanticdetails. Then, we propose simple yet effective methods by combining promptgeneration frameworks, thereby making the prompt engineering process moreuser-friendly. Extensive qualitative and quantitative experiments demonstratethe importance of prompts in text-driven image editing and our method iscomparable to ground-truth prompts."
PromptMagician: Interactive Prompt Engineering for Text-to-Image  Creation,"['Yingchaojie Feng', 'Xingbo Wang', 'Kam Kwai Wong', 'Sijia Wang', 'Yuhong Lu', 'Minfeng Zhu', 'Baicheng Wang', 'Wei Chen']",http://arxiv.org/pdf/2307.09036v2.pdf,2023-07-18,"['cs.ai', 'cs.hc']","  Generative text-to-image models have gained great popularity among the publicfor their powerful capability to generate high-quality images based on naturallanguage prompts. However, developing effective prompts for desired images canbe challenging due to the complexity and ambiguity of natural language. Thisresearch proposes PromptMagician, a visual analysis system that helps usersexplore the image results and refine the input prompts. The backbone of oursystem is a prompt recommendation model that takes user prompts as input,retrieves similar prompt-image pairs from DiffusionDB, and identifies special(important and relevant) prompt keywords. To facilitate interactive promptrefinement, PromptMagician introduces a multi-level visualization for thecross-modal embedding of the retrieved images and recommended keywords, andsupports users in specifying multiple criteria for personalized exploration.Two usage scenarios, a user study, and expert interviews demonstrate theeffectiveness and usability of our system, suggesting it facilitates promptengineering and improves the creativity support of the generative text-to-imagemodel."
Is GPT a Computational Model of Emotion? Detailed Analysis,"['Ala N. Tak', 'Jonathan Gratch']",http://arxiv.org/pdf/2307.13779v1.pdf,2023-07-25,"['cs.cl', 'cs.ai', 'cs.cy', 'cs.hc']","  This paper investigates the emotional reasoning abilities of the GPT familyof large language models via a component perspective. The paper first examineshow the model reasons about autobiographical memories. Second, itsystematically varies aspects of situations to impact emotion intensity andcoping tendencies. Even without the use of prompt engineering, it is shown thatGPT's predictions align significantly with human-provided appraisals andemotional labels. However, GPT faces difficulties predicting emotion intensityand coping responses. GPT-4 showed the highest performance in the initial studybut fell short in the second, despite providing superior results after minorprompt engineering. This assessment brings up questions on how to effectivelyemploy the strong points and address the weak areas of these models,particularly concerning response variability. These studies underscore themerits of evaluating models from a componential perspective."
Prompts Matter: Insights and Strategies for Prompt Engineering in  Automated Software Traceability,"['Alberto D. Rodriguez', 'Katherine R. Dearstyne', 'Jane Cleland-Huang']",http://arxiv.org/pdf/2308.00229v1.pdf,2023-08-01,['cs.se'],"  Large Language Models (LLMs) have the potential to revolutionize automatedtraceability by overcoming the challenges faced by previous methods andintroducing new possibilities. However, the optimal utilization of LLMs forautomated traceability remains unclear. This paper explores the process ofprompt engineering to extract link predictions from an LLM. We provide detailedinsights into our approach for constructing effective prompts, offering ourlessons learned. Additionally, we propose multiple strategies for leveragingLLMs to generate traceability links, improving upon previous zero-shot methodson the ranking of candidate links after prompt refinement. The primaryobjective of this paper is to inspire and assist future researchers andengineers by highlighting the process of constructing traceability prompts toeffectively harness LLMs for advancing automatic traceability."
CoT-BERT: Enhancing Unsupervised Sentence Representation through  Chain-of-Thought,"['Bowen Zhang', 'Kehua Chang', 'Chunping Li']",http://arxiv.org/pdf/2309.11143v1.pdf,2023-09-20,"['cs.cl', 'cs.ai']","  Unsupervised sentence representation learning aims to transform inputsentences into fixed-length vectors enriched with intricate semanticinformation while obviating the reliance on labeled data. Recent progresswithin this field, propelled by contrastive learning and prompt engineering,has significantly bridged the gap between unsupervised and supervisedstrategies. Nonetheless, the potential utilization of Chain-of-Thought, remainslargely untapped within this trajectory. To unlock latent capabilities withinpre-trained models, such as BERT, we propose a two-stage approach for sentencerepresentation: comprehension and summarization. Subsequently, the output ofthe latter phase is harnessed as the vectorized representation of the inputsentence. For further performance enhancement, we meticulously refine both thecontrastive learning loss function and the template denoising technique forprompt engineering. Rigorous experimentation substantiates our method,CoT-BERT, transcending a suite of robust baselines without necessitating othertext representation models or external databases."
How does prompt engineering affect ChatGPT performance on unsupervised  entity resolution?,"['Khanin Sisaengsuwanchai', 'Navapat Nananukul', 'Mayank Kejriwal']",http://arxiv.org/pdf/2310.06174v1.pdf,2023-10-09,"['cs.ai', 'cs.se']","  Entity Resolution (ER) is the problem of semi-automatically determining whentwo entities refer to the same underlying entity, with applications rangingfrom healthcare to e-commerce. Traditional ER solutions required considerablemanual expertise, including feature engineering, as well as identification andcuration of training data. In many instances, such techniques are highlydependent on the domain. With recent advent in large language models (LLMs),there is an opportunity to make ER much more seamless and domain-independent.However, it is also well known that LLMs can pose risks, and that the qualityof their outputs can depend on so-called prompt engineering. Unfortunately, asystematic experimental study on the effects of different prompting methods foraddressing ER, using LLMs like ChatGPT, has been lacking thus far. This paperaims to address this gap by conducting such a study. Although preliminary innature, our results show that prompting can significantly affect the quality ofER, although it affects some metrics more than others, and can also be datasetdependent."
Interactive Task Planning with Language Models,"['Boyi Li', 'Philipp Wu', 'Pieter Abbeel', 'Jitendra Malik']",http://arxiv.org/pdf/2310.10645v1.pdf,2023-10-16,"['cs.ro', 'cs.ai', 'cs.cl', 'cs.hc']","  An interactive robot framework accomplishes long-horizon task planning andcan easily generalize to new goals or distinct tasks, even during execution.However, most traditional methods require predefined module design, which makesit hard to generalize to different goals. Recent large language model basedapproaches can allow for more open-ended planning but often require heavyprompt engineering or domain-specific pretrained models. To tackle this, wepropose a simple framework that achieves interactive task planning withlanguage models. Our system incorporates both high-level planning and low-levelfunction execution via language. We verify the robustness of our system ingenerating novel high-level instructions for unseen objectives and its ease ofadaptation to different tasks by merely substituting the task guidelines,without the need for additional complex prompt engineering. Furthermore, whenthe user sends a new request, our system is able to replan accordingly withprecision based on the new request, task guidelines and previously executedsteps. Please check more details on our https://wuphilipp.github.io/itp_siteand https://youtu.be/TrKLuyv26_g."
Prompt Engineering Through the Lens of Optimal Control,"['Yifan Luo', 'Yiming Tang', 'Chengfeng Shen', 'Zhennan Zhou', 'Bin Dong']",http://arxiv.org/pdf/2310.14201v2.pdf,2023-10-22,"['cs.lg', 'math.oc']","  Prompt Engineering (PE) has emerged as a critical technique for guiding LargeLanguage Models (LLMs) in solving intricate tasks. Its importance ishighlighted by its potential to significantly enhance the efficiency andeffectiveness of human-machine interaction. As tasks grow increasingly complex,recent advanced PE methods have extended beyond the limitations of single-roundinteractions to embrace multi-round interactions, which allows for a deeper andmore nuanced engagement with LLMs. In this paper, we propose an optimal controlframework tailored for multi-round interactions with LLMs. This frameworkprovides a unified mathematical structure that not only systematizes theexisting PE methods but also sets the stage for rigorous analyticalimprovements. Furthermore, we extend this framework to include PE via ensemblemethods and multi-agent collaboration, thereby enlarging the scope ofapplicability. By adopting an optimal control perspective, we offer freshinsights into existing PE methods and highlight theoretical challenges thatwarrant future research. Besides, our work lays a foundation for thedevelopment of more effective and interpretable PE methods."
A Communication Theory Perspective on Prompting Engineering Methods for  Large Language Models,"['Yuanfeng Song', 'Yuanqin He', 'Xuefang Zhao', 'Hanlin Gu', 'Di Jiang', 'Haijun Yang', 'Lixin Fan', 'Qiang Yang']",http://arxiv.org/pdf/2310.18358v1.pdf,2023-10-24,"['cs.cl', 'cs.ai']","  The springing up of Large Language Models (LLMs) has shifted the communityfrom single-task-orientated natural language processing (NLP) research to aholistic end-to-end multi-task learning paradigm. Along this line of researchendeavors in the area, LLM-based prompting methods have attracted muchattention, partially due to the technological advantages brought by promptengineering (PE) as well as the underlying NLP principles disclosed by variousprompting methods. Traditional supervised learning usually requires training amodel based on labeled data and then making predictions. In contrast, PEmethods directly use the powerful capabilities of existing LLMs (i.e., GPT-3and GPT-4) via composing appropriate prompts, especially under few-shot orzero-shot scenarios. Facing the abundance of studies related to the promptingand the ever-evolving nature of this field, this article aims to (i) illustratea novel perspective to review existing PE methods, within the well-establishedcommunication theory framework; (ii) facilitate a better/deeper understandingof developing trends of existing PE methods used in four typical tasks; (iii)shed light on promising research directions for future PE methods."
Apollo: Zero-shot MultiModal Reasoning with Multiple Experts,"['Daniela Ben-David', 'Tzuf Paz-Argaman', 'Reut Tsarfaty']",http://arxiv.org/pdf/2310.18369v1.pdf,2023-10-25,"['cs.cl', 'cs.ai', 'cs.cv', 'i.2.7; i.5.4']","  We propose a modular framework that leverages the expertise of differentfoundation models over different modalities and domains in order to perform asingle, complex, multi-modal task, without relying on prompt engineering orotherwise tailor-made multi-modal training. Our approach enables decentralizedcommand execution and allows each model to both contribute and benefit from theexpertise of the other models. Our method can be extended to a variety offoundation models (including audio and vision), above and beyond only languagemodels, as it does not depend on prompts. We demonstrate our approach on twotasks. On the well-known task of stylized image captioning, our experimentsshow that our approach outperforms semi-supervised state-of-the-art models,while being zero-shot and avoiding costly training, data collection, and promptengineering. We further demonstrate this method on a novel task, audio-awareimage captioning, in which an image and audio are given and the task is togenerate text that describes the image within the context of the providedaudio. Our code is available on GitHub."
Large Language Models and Prompt Engineering for Biomedical Query  Focused Multi-Document Summarisation,['Diego Mollá'],http://arxiv.org/pdf/2311.05169v1.pdf,2023-11-09,['cs.cl'],"  This paper reports on the use of prompt engineering and GPT-3.5 forbiomedical query-focused multi-document summarisation. Using GPT-3.5 andappropriate prompts, our system achieves top ROUGE-F1 results in the task ofobtaining short-paragraph-sized answers to biomedical questions in the 2023BioASQ Challenge (BioASQ 11b). This paper confirms what has been observed inother domains: 1) Prompts that incorporated few-shot samples generally improvedon their counterpart zero-shot variants; 2) The largest improvement wasachieved by retrieval augmented generation. The fact that these prompts allowour top runs to rank within the top two runs of BioASQ 11b demonstrate thepower of using adequate prompts for Large Language Models in general, andGPT-3.5 in particular, for query-focused summarisation."
BeautifulPrompt: Towards Automatic Prompt Engineering for Text-to-Image  Synthesis,"['Tingfeng Cao', 'Chengyu Wang', 'Bingyan Liu', 'Ziheng Wu', 'Jinhui Zhu', 'Jun Huang']",http://arxiv.org/pdf/2311.06752v1.pdf,2023-11-12,['cs.cl'],"  Recently, diffusion-based deep generative models (e.g., Stable Diffusion)have shown impressive results in text-to-image synthesis. However, currenttext-to-image models often require multiple passes of prompt engineering byhumans in order to produce satisfactory results for real-world applications. Wepropose BeautifulPrompt, a deep generative model to produce high-qualityprompts from very simple raw descriptions, which enables diffusion-based modelsto generate more beautiful images. In our work, we first fine-tuned theBeautifulPrompt model over low-quality and high-quality collecting promptpairs. Then, to ensure that our generated prompts can generate more beautifulimages, we further propose a Reinforcement Learning with Visual AI Feedbacktechnique to fine-tune our model to maximize the reward values of the generatedprompts, where the reward values are calculated based on the PickScore and theAesthetic Scores. Our results demonstrate that learning from visual AI feedbackpromises the potential to improve the quality of generated prompts and imagessignificantly. We further showcase the integration of BeautifulPrompt to acloud-native AI platform to provide better text-to-image generation service inthe cloud."
On the Discussion of Large Language Models: Symmetry of Agents and  Interplay with Prompts,"['Qineng Wang', 'Zihao Wang', 'Ying Su', 'Yangqiu Song']",http://arxiv.org/pdf/2311.07076v1.pdf,2023-11-13,['cs.cl'],"  Two ways has been discussed to unlock the reasoning capability of a largelanguage model. The first one is prompt engineering and the second one is tocombine the multiple inferences of large language models, or the multi-agentdiscussion. Theoretically, this paper justifies the multi-agent discussionmechanisms from the symmetry of agents. Empirically, this paper reports theempirical results of the interplay of prompts and discussion mechanisms,revealing the empirical state-of-the-art performance of complex multi-agentmechanisms can be approached by carefully developed prompt engineering. Thispaper also proposes a scalable discussion mechanism based on conquer and merge,providing a simple multi-agent discussion solution with simple prompts butstate-of-the-art performance."
NeuroPrompts: An Adaptive Framework to Optimize Prompts for  Text-to-Image Generation,"['Shachar Rosenman', 'Vasudev Lal', 'Phillip Howard']",http://arxiv.org/pdf/2311.12229v1.pdf,2023-11-20,['cs.ai'],"  Despite impressive recent advances in text-to-image diffusion models,obtaining high-quality images often requires prompt engineering by humans whohave developed expertise in using them. In this work, we present NeuroPrompts,an adaptive framework that automatically enhances a user's prompt to improvethe quality of generations produced by text-to-image models. Our frameworkutilizes constrained text decoding with a pre-trained language model that hasbeen adapted to generate prompts similar to those produced by human promptengineers. This approach enables higher-quality text-to-image generations andprovides user control over stylistic features via constraint set specification.We demonstrate the utility of our framework by creating an interactiveapplication for prompt enhancement and image generation using Stable Diffusion.Additionally, we conduct experiments utilizing a large dataset ofhuman-engineered prompts for text-to-image generation and show that ourapproach automatically produces enhanced prompts that result in superior imagequality. We make our code, a screencast video demo and a live demo instance ofNeuroPrompts publicly available."
MemoryCompanion: A Smart Healthcare Solution to Empower Efficient  Alzheimer's Care Via Unleashing Generative AI,"['Lifei Zheng', 'Yeonie Heo', 'Yi Fang']",http://arxiv.org/pdf/2311.14730v1.pdf,2023-11-20,"['cs.cl', 'cs.ai', 'cs.hc', 'cs.lg']","  With the rise of Large Language Models (LLMs), notably characterized by GPTframeworks, there emerges a catalyst for novel healthcare applications. Earlieriterations of chatbot caregivers, though existent, have yet to achieve adimension of human-like authenticity. This paper unveils `MemoryCompanion' apioneering digital health solution explicitly tailored for Alzheimer's disease(AD) patients and their caregivers. Drawing upon the nuances of GPT technologyand prompt engineering, MemoryCompanion manifests a personalized caregivingparadigm, fostering interactions via voice-cloning and talking-face mechanismsthat resonate with the familiarity of known companions. Using advancedprompt-engineering, the system intricately adapts to each patient's distinctprofile, curating its content and communication style accordingly. Thisapproach strives to counteract prevalent issues of social isolation andloneliness frequently observed in AD demographics. Our methodology, grounded inits innovative design, addresses both the caregiving and technologicalchallenges intrinsic to this domain."
DevBots can co-design APIs,['Vinicius Soares Silva Marques'],http://arxiv.org/pdf/2312.05733v1.pdf,2023-12-10,"['cs.se', 'cs.ai', 'cs.hc']","  DevBots are automated tools that perform various tasks in order to supportsoftware development. They are a growing trend and have been used inrepositories to automate repetitive tasks, as code generators, and ascollaborators in eliciting requirements and defining architectures. In thisstudy, we analyzed 24 articles to investigate the state of the art of usingDevBots in software development, trying to understand their characteristics,identify use cases, learn the relationship between DevBots and conversationalsoftware development, and discuss how prompt engineering can enablecollaboration between human developers and bots. Additionally, we identified agap to address by applying prompt engineering to collaborative API designbetween human designers and DevBots and proposed an experiment to assess whatapproach, between using Retrieval Augmented Generation or not, is moresuitable. Our conclusion is that DevBots can collaborate with human APIdesigners, but the two approaches have advantages and disadvantages."
Towards Zero-Shot and Few-Shot Table Question Answering using GPT-3,"['Pragya Srivastava', 'Tanuja Ganu', 'Saikat Guha']",http://arxiv.org/pdf/2210.17284v1.pdf,2022-10-31,"['cs.lg', '14j60 (primary)']","  We present very early results on using GPT-3 to perform question answering ontabular data. We find that stock pre-trained GPT-3 is able to zero-shot learnthe table structure from a serialized JSON array-of-arrays representation, andable to answer lookup queries and simple comparison questions in naturallanguage without any fine-tuning. We further find that simple promptengineering to include few-shot static Q&A examples significantly improvesaccuracy. Lastly, we find that intermixing passage text improves accuracy evenfurther on heterogeneous data. We apply our approach on a novel dataset ofsimple tables in newspaper infographics with promising results. Overall, wefind much cause for optimism in this basic approach."
Investigating Prompt Engineering in Diffusion Models,"['Sam Witteveen', 'Martin Andrews']",http://arxiv.org/pdf/2211.15462v1.pdf,2022-11-21,"['cs.cv', 'cs.ai', 'cs.cl']","  With the spread of the use of Text2Img diffusion models such as DALL-E 2,Imagen, Mid Journey and Stable Diffusion, one challenge that artists face isselecting the right prompts to achieve the desired artistic output. We presenttechniques for measuring the effect that specific words and phrases in promptshave, and (in the Appendix) present guidance on the selection of prompts toproduce desired effects."
Refining the Responses of LLMs by Themselves,"['Tianqiang Yan', 'Tiansheng Xu']",http://arxiv.org/pdf/2305.04039v1.pdf,2023-05-06,"['cs.cl', 'cs.ai']","  In this paper, we propose a simple yet efficient approach based on promptengineering that leverages the large language model itself to optimize itsanswers without relying on auxiliary models. We introduce an iterativeself-evaluating optimization mechanism, with the potential for improved outputquality as iterations progress, removing the need for manual intervention. Theexperiment's findings indicate that utilizing our response refinement frameworkon the GPT-3.5 model yields results that are on par with, or even surpass,those generated by the cutting-edge GPT-4 model. Detailed implementationstrategies and illustrative examples are provided to demonstrate thesuperiority of our proposed solution."
Efficient Black-Box Adversarial Attacks on Neural Text Detectors,"['Vitalii Fishchuk', 'Daniel Braun']",http://arxiv.org/pdf/2311.01873v1.pdf,2023-11-03,['cs.cl'],"  Neural text detectors are models trained to detect whether a given text wasgenerated by a language model or written by a human. In this paper, weinvestigate three simple and resource-efficient strategies (parameter tweaking,prompt engineering, and character-level mutations) to alter texts generated byGPT-3.5 that are unsuspicious or unnoticeable for humans but causemisclassification by neural text detectors. The results show that especiallyparameter tweaking and character-level mutations are effective strategies."
Prompted Software Engineering in the Era of AI Models,['Dae-Kyoo Kim'],http://arxiv.org/pdf/2311.03359v1.pdf,2023-09-07,['cs.se'],"  This paper introduces prompted software engineering (PSE), which integratesprompt engineering to build effective prompts for language-based AI models, toenhance the software development process. PSE enables the use of AI models insoftware development to produce high-quality software with fewer resources,automating tedious tasks and allowing developers to focus on more innovativeaspects. However, effective prompts are necessary to guide software developmentin generating accurate, relevant, and useful responses, while mitigating risksof misleading outputs. This paper describes how productive prompts should bebuilt throughout the software development cycle."
Generative AI has lowered the barriers to computational social sciences,['Yongjun Zhang'],http://arxiv.org/pdf/2311.10833v1.pdf,2023-11-17,"['cs.hc', 'cs.cy']","  Generative artificial intelligence (AI) has revolutionized the field ofcomputational social science, unleashing new possibilities for analyzingmultimodal data, especially for scholars who may not have extensive programmingexpertise. This breakthrough carries profound implications for the realm ofsocial sciences. Firstly, generative AI can significantly enhance theproductivity of social scientists by automating the generation, annotation, anddebugging of code. Secondly, it empowers researchers to delve intosophisticated data analysis through the innovative use of prompt engineering.Lastly, the educational sphere of computational social science stands tobenefit immensely from these tools, given their exceptional ability to annotateand elucidate complex codes for learners, thereby simplifying the learningprocess and making the technology more accessible."
Conversing with Copilot: Exploring Prompt Engineering for Solving CS1  Problems Using Natural Language,"['Paul Denny', 'Viraj Kumar', 'Nasser Giacaman']",http://arxiv.org/pdf/2210.15157v1.pdf,2022-10-27,"['cs.hc', 'cs.ai']","  GitHub Copilot is an artificial intelligence model for automaticallygenerating source code from natural language problem descriptions. Since June2022, Copilot has officially been available for free to all students as aplug-in to development environments like Visual Studio Code. Prior workexploring OpenAI Codex, the underlying model that powers Copilot, has shown itperforms well on typical CS1 problems thus raising concerns about the impact itwill have on how introductory programming courses are taught. However, littleis known about the types of problems for which Copilot does not perform well,or about the natural language interactions that a student might have withCopilot when resolving errors. We explore these questions by evaluating theperformance of Copilot on a publicly available dataset of 166 programmingproblems. We find that it successfully solves around half of these problems onits very first attempt, and that it solves 60\% of the remaining problems usingonly natural language changes to the problem description. We argue that thistype of prompt engineering, which we believe will become a standard interactionbetween human and Copilot when it initially fails, is a potentially usefullearning activity that promotes computational thinking skills, and is likely tochange the nature of code writing skill development."
ChatGPT4PCG Competition: Character-like Level Generation for Science  Birds,"['Pittawat Taveekitworachai', 'Febri Abdullah', 'Mury F. Dewantoro', 'Ruck Thawonmas', 'Julian Togelius', 'Jochen Renz']",http://arxiv.org/pdf/2303.15662v2.pdf,2023-03-28,"['cs.ai', 'cs.cl', 'i.2.7; i.2.8']","  This paper presents the first ChatGPT4PCG Competition at the 2023 IEEEConference on Games. The objective of this competition is for participants tocreate effective prompts for ChatGPT--enabling it to generate Science Birdslevels with high stability and character-like qualities--fully using theircreativity as well as prompt engineering skills. ChatGPT is a conversationalagent developed by OpenAI. Science Birds is selected as the competitionplatform because designing an Angry Birds-like level is not a trivial task dueto the in-game gravity; the quality of the levels is determined by theirstability. To lower the entry barrier to the competition, we limit the task tothe generation of capitalized English alphabetical characters. We also allowonly a single prompt to be used for generating all the characters. Here, thequality of the generated levels is determined by their stability and similarityto the given characters. A sample prompt is provided to participants for theirreference. An experiment is conducted to determine the effectiveness of severalmodified versions of this sample prompt on level stability and similarity bytesting them on several characters. To the best of our knowledge, we believethat ChatGPT4PCG is the first competition of its kind and hope to inspireenthusiasm for prompt engineering in procedural content generation."
Enhancing Automated Program Repair through Fine-tuning and Prompt  Engineering,"['Rishov Paul', 'Md. Mohib Hossain', 'Mohammed Latif Siddiq', 'Masum Hasan', 'Anindya Iqbal', 'Joanna C. S. Santos']",http://arxiv.org/pdf/2304.07840v2.pdf,2023-04-16,"['cs.lg', 'cs.se']","  Sequence-to-sequence models have been used to transform erroneous programsinto correct ones when trained with a large enough dataset. Some recent studiesalso demonstrated strong empirical evidence that code review could improve theprogram repair further. Large language models, trained with Natural Language(NL) and Programming Language (PL), can contain inherent knowledge of both. Inthis study, we investigate if this inherent knowledge of PL and NL can beutilized to improve automated program repair. We applied PLBART and CodeT5, twostate-of-the-art language models that are pre-trained with both PL and NL, ontwo such natural language-based program repair datasets and found that thepre-trained language models fine-tuned with datasets containing both codereview and subsequent code changes notably outperformed each of the previousmodels. With the advent of code generative models like Codex and GPT-3.5-Turbo,we also performed zero-shot and few-shots learning-based prompt engineering toassess their performance on these datasets. However, the practical applicationof using LLMs in the context of automated program repair is still a long wayoff based on our manual analysis of the generated repaired codes by thelearning models."
Conceptual Design Generation Using Large Language Models,"['Kevin Ma', 'Daniele Grandi', 'Christopher McComb', 'Kosa Goucher-Lambert']",http://arxiv.org/pdf/2306.01779v1.pdf,2023-05-30,"['cs.cl', 'cs.ai']","  Concept generation is a creative step in the conceptual design phase, wheredesigners often turn to brainstorming, mindmapping, or crowdsourcing designideas to complement their own knowledge of the domain. Recent advances innatural language processing (NLP) and machine learning (ML) have led to therise of Large Language Models (LLMs) capable of generating seemingly creativeoutputs from textual prompts. The success of these models has led to theirintegration and application across a variety of domains, including art,entertainment, and other creative work. In this paper, we leverage LLMs togenerate solutions for a set of 12 design problems and compare them to abaseline of crowdsourced solutions. We evaluate the differences betweengenerated and crowdsourced design solutions through multiple perspectives,including human expert evaluations and computational metrics. Expertevaluations indicate that the LLM-generated solutions have higher averagefeasibility and usefulness while the crowdsourced solutions have more novelty.We experiment with prompt engineering and find that leveraging few-shotlearning can lead to the generation of solutions that are more similar to thecrowdsourced solutions. These findings provide insight into the quality ofdesign solutions generated with LLMs and begins to evaluate prompt engineeringtechniques that could be leveraged by practitioners to generate higher-qualitydesign solutions synergistically with LLMs."
Cheap-fake Detection with LLM using Prompt Engineering,"['Guangyang Wu', 'Weijie Wu', 'Xiaohong Liu', 'Kele Xu', 'Tianjiao Wan', 'Wenyi Wang']",http://arxiv.org/pdf/2306.02776v1.pdf,2023-06-05,['cs.cv'],"  The misuse of real photographs with conflicting image captions in news itemsis an example of the out-of-context (OOC) misuse of media. In order to detectOOC media, individuals must determine the accuracy of the statement andevaluate whether the triplet (~\textit{i.e.}, the image and two captions)relates to the same event. This paper presents a novel learnable approach fordetecting OOC media in ICME'23 Grand Challenge on Detecting Cheapfakes. Theproposed method is based on the COSMOS structure, which assesses the coherencebetween an image and captions, as well as between two captions. We enhance thebaseline algorithm by incorporating a Large Language Model (LLM), GPT3.5, as afeature extractor. Specifically, we propose an innovative approach to featureextraction utilizing prompt engineering to develop a robust and reliablefeature extractor with GPT3.5 model. The proposed method captures thecorrelation between two captions and effectively integrates this module intothe COSMOS baseline model, which allows for a deeper understanding of therelationship between captions. By incorporating this module, we demonstrate thepotential for significant improvements in cheap-fakes detection performance.The proposed methodology holds promising implications for various applicationssuch as natural language processing, image captioning, and text-to-imagesynthesis. Docker for submission is available athttps://hub.docker.com/repository/docker/mulns/ acmmmcheapfakes."
Improving Knowledge Extraction from LLMs for Task Learning through Agent  Analysis,"['James R. Kirk', 'Robert E. Wray', 'Peter Lindes']",http://arxiv.org/pdf/2306.06770v3.pdf,2023-06-11,"['cs.ai', 'cs.hc', 'cs.ro', 'i.2.6; i.2.7']","  Large language models (LLMs) offer significant promise as a knowledge sourcefor task learning. Prompt engineering has been shown to be effective foreliciting knowledge from an LLM, but alone it is insufficient for acquiringrelevant, situationally grounded knowledge for an embodied agent learning noveltasks. We describe a cognitive-agent approach that extends and complementsprompt engineering, mitigating its limitations and thus enabling an agent toacquire new task knowledge matched to its native language capabilities,embodiment, environment, and user preferences. The approach is to increase theresponse space of LLMs and deploy general strategies, embedded within theautonomous agent, to evaluate, repair, and select among candidate responsesproduced by the LLM. We describe the approach and experiments that show how anagent, by retrieving and evaluating a breadth of responses from the LLM, canachieve 77-94% task completion in one-shot learning without user oversight. Theapproach achieves 100% task completion when human oversight (such as anindication of preference) is provided. Further, the type of oversight largelyshifts from explicit, natural language instruction to simpleconfirmation/discomfirmation of high-quality responses that have been vetted bythe agent before presentation to a user."
ChatGPT for Robotics: Design Principles and Model Abilities,"['Sai Vemprala', 'Rogerio Bonatti', 'Arthur Bucker', 'Ashish Kapoor']",http://arxiv.org/pdf/2306.17582v2.pdf,2023-02-20,"['cs.ai', 'cs.cl', 'cs.hc', 'cs.lg', 'cs.ro']","  This paper presents an experimental study regarding the use of OpenAI'sChatGPT for robotics applications. We outline a strategy that combines designprinciples for prompt engineering and the creation of a high-level functionlibrary which allows ChatGPT to adapt to different robotics tasks, simulators,and form factors. We focus our evaluations on the effectiveness of differentprompt engineering techniques and dialog strategies towards the execution ofvarious types of robotics tasks. We explore ChatGPT's ability to use free-formdialog, parse XML tags, and to synthesize code, in addition to the use oftask-specific prompting functions and closed-loop reasoning through dialogues.Our study encompasses a range of tasks within the robotics domain, from basiclogical, geometrical, and mathematical reasoning all the way to complex domainssuch as aerial navigation, manipulation, and embodied agents. We show thatChatGPT can be effective at solving several of such tasks, while allowing usersto interact with it primarily via natural language instructions. In addition tothese studies, we introduce an open-sourced research tool called PromptCraft,which contains a platform where researchers can collaboratively upload and voteon examples of good prompting schemes for robotics applications, as well as asample robotics simulator with ChatGPT integration, making it easier for usersto get started with using ChatGPT for robotics."
Cases of EFL Secondary Students' Prompt Engineering Pathways to Complete  a Writing Task with ChatGPT,"['David James Woo', 'Kai Guo', 'Hengky Susanto']",http://arxiv.org/pdf/2307.05493v1.pdf,2023-06-19,"['cs.hc', 'cs.ai', 'cs.cl']","  ChatGPT is a state-of-the-art (SOTA) chatbot. Although it has potential tosupport English as a foreign language (EFL) students' writing, to effectivelycollaborate with it, a student must learn to engineer prompts, that is, theskill of crafting appropriate instructions so that ChatGPT produces desiredoutputs. However, writing an appropriate prompt for ChatGPT is notstraightforward for non-technical users who suffer a trial-and-error process.This paper examines the content of EFL students' ChatGPT prompts whencompleting a writing task and explores patterns in the quality and quantity ofthe prompts. The data come from iPad screen recordings of secondary school EFLstudents who used ChatGPT and other SOTA chatbots for the first time tocomplete the same writing task. The paper presents a case study of fourdistinct pathways that illustrate the trial-and-error process and showdifferent combinations of prompt content and quantity. The cases contributeevidence for the need to provide prompt engineering education in the context ofthe EFL writing classroom, if students are to move beyond an individualtrial-and-error process, learning a greater variety of prompt content and moresophisticated prompts to support their writing."
"Multi-party Goal Tracking with LLMs: Comparing Pre-training,  Fine-tuning, and Prompt Engineering","['Angus Addlesee', 'Weronika Sieińska', 'Nancie Gunson', 'Daniel Hernández Garcia', 'Christian Dondrup', 'Oliver Lemon']",http://arxiv.org/pdf/2308.15231v1.pdf,2023-08-29,"['cs.cl', 'cs.hc']","  This paper evaluates the extent to which current Large Language Models (LLMs)can capture task-oriented multi-party conversations (MPCs). We have recordedand transcribed 29 MPCs between patients, their companions, and a social robotin a hospital. We then annotated this corpus for multi-party goal-tracking andintent-slot recognition. People share goals, answer each other's goals, andprovide other people's goals in MPCs - none of which occur in dyadicinteractions. To understand user goals in MPCs, we compared three methods inzero-shot and few-shot settings: we fine-tuned T5, created pre-training tasksto train DialogLM using LED, and employed prompt engineering techniques withGPT-3.5-turbo, to determine which approach can complete this novel task withlimited data. GPT-3.5-turbo significantly outperformed the others in a few-shotsetting. The `reasoning' style prompt, when given 7% of the corpus as exampleannotated conversations, was the best performing method. It correctly annotated62.32% of the goal tracking MPCs, and 69.57% of the intent-slot recognitionMPCs. A `story' style prompt increased model hallucination, which could bedetrimental if deployed in safety-critical settings. We conclude thatmulti-party conversations still challenge state-of-the-art LLMs."
Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation,"['Dawei Gao', 'Haibin Wang', 'Yaliang Li', 'Xiuyu Sun', 'Yichen Qian', 'Bolin Ding', 'Jingren Zhou']",http://arxiv.org/pdf/2308.15363v4.pdf,2023-08-29,"['cs.db', 'cs.cl', 'cs.lg']","  Large language models (LLMs) have emerged as a new paradigm for Text-to-SQLtask. However, the absence of a systematical benchmark inhibits the developmentof designing effective, efficient and economic LLM-based Text-to-SQL solutions.To address this challenge, in this paper, we first conduct a systematical andextensive comparison over existing prompt engineering methods, includingquestion representation, example selection and example organization, and withthese experimental results, we elaborate their pros and cons. Based on thesefindings, we propose a new integrated solution, named DAIL-SQL, which refreshesthe Spider leaderboard with 86.6% execution accuracy and sets a new bar. Toexplore the potential of open-source LLM, we investigate them in variousscenarios, and further enhance their performance with supervised fine-tuning.Our explorations highlight open-source LLMs' potential in Text-to-SQL, as wellas the advantages and disadvantages of the supervised fine-tuning.Additionally, towards an efficient and economic LLM-based Text-to-SQL solution,we emphasize the token efficiency in prompt engineering and compare the priorstudies under this metric. We hope that our work provides a deeperunderstanding of Text-to-SQL with LLMs, and inspires further investigations andbroad applications."
PRE: Vision-Language Prompt Learning with Reparameterization Encoder,"['Anh Pham Thi Minh', 'An Duc Nguyen', 'Georgios Tzimiropoulos']",http://arxiv.org/pdf/2309.07760v2.pdf,2023-09-14,"['cs.cv', 'cs.ai', 'cs.lg', 'i.4.0']","  Large pre-trained vision-language models such as CLIP have demonstrated greatpotential in zero-shot transferability to downstream tasks. However, to attainoptimal performance, the manual selection of prompts is necessary to improvealignment between the downstream image distribution and the textual classdescriptions. This manual prompt engineering is the major challenge fordeploying such models in practice since it requires domain expertise and isextremely time-consuming. To avoid non-trivial prompt engineering, recent workContext Optimization (CoOp) introduced the concept of prompt learning to thevision domain using learnable textual tokens. While CoOp can achievesubstantial improvements over manual prompts, its learned context is worsegeneralizable to wider unseen classes within the same dataset. In this work, wepresent Prompt Learning with Reparameterization Encoder (PRE) - a simple andefficient method that enhances the generalization ability of the learnableprompt to unseen classes while maintaining the capacity to learn Base classes.Instead of directly optimizing the prompts, PRE employs a prompt encoder toreparameterize the input prompt embeddings, enhancing the exploration oftask-specific knowledge from few-shot samples. Experiments and extensiveablation studies on 8 benchmarks demonstrate that our approach is an efficientmethod for prompt learning. Specifically, PRE achieves a notable enhancement of5.60% in average accuracy on New classes and 3% in Harmonic mean compared toCoOp in the 16-shot setting, all achieved within a good training time."
PEACE: Prompt Engineering Automation for CLIPSeg Enhancement in Aerial  Robotics,"['Haechan Mark Bong', 'Rongge Zhang', 'Ricardo de Azambuja', 'Giovanni Beltrame']",http://arxiv.org/pdf/2310.00085v2.pdf,2023-09-29,['cs.ro'],"  From industrial to space robotics, safe landing is an essential component forflight operations. With the growing interest in artificial intelligence, wedirect our attention to learning based safe landing approaches. This paperextends our previous work, DOVESEI, which focused on a reactive UAV system byharnessing the capabilities of open vocabulary image segmentation. Prompt-basedsafe landing zone segmentation using an open vocabulary based model is no morejust an idea, but proven to be feasible by the work of DOVESEI. However, aheuristic selection of words for prompt is not a reliable solution since itcannot take the changing environment into consideration and detrimentalconsequences can occur if the observed environment is not well represented bythe given prompt. Therefore, we introduce PEACE (Prompt Engineering Automationfor CLIPSeg Enhancement), powering DOVESEI to automate the prompt generationand engineering to adapt to data distribution shifts. Our system is capable ofperforming safe landing operations with collision avoidance at altitudes as lowas 20 meters using only monocular cameras and image segmentation. We takeadvantage of DOVESEI's dynamic focus to circumvent abrupt fluctuations in theterrain segmentation between frames in a video stream. PEACE shows promisingimprovements in prompt generation and engineering for aerial images compared tothe standard prompt used for CLIP and CLIPSeg. Combining DOVESEI and PEACE, oursystem was able improve successful safe landing zone selections by 58.62%compared to using only DOVESEI. All the source code is open source andavailable online."
Understanding prompt engineering may not require rethinking  generalization,"['Victor Akinwande', 'Yiding Jiang', 'Dylan Sam', 'J. Zico Kolter']",http://arxiv.org/pdf/2310.03957v1.pdf,2023-10-06,"['cs.lg', 'cs.cv']","  Zero-shot learning in prompted vision-language models, the practice ofcrafting prompts to build classifiers without an explicit training process, hasachieved impressive performance in many settings. This success presents aseemingly surprising observation: these methods suffer relatively little fromoverfitting, i.e., when a prompt is manually engineered to achieve low error ona given training set (thus rendering the method no longer actually zero-shot),the approach still performs well on held-out test data. In this paper, we showthat we can explain such performance well via recourse to classical PAC-Bayesbounds. Specifically, we show that the discrete nature of prompts, combinedwith a PAC-Bayes prior given by a language model, results in generalizationbounds that are remarkably tight by the standards of the literature: forinstance, the generalization bound of an ImageNet classifier is often within afew percentage points of the true test error. We demonstrate empirically thatthis holds for existing handcrafted prompts and prompts generated throughsimple greedy search. Furthermore, the resulting bound is well-suited for modelselection: the models with the best bound typically also have the best testperformance. This work thus provides a possible justification for thewidespread practice of prompt engineering, even if it seems that such methodscould potentially overfit the training data."
What's the Magic Word? A Control Theory of LLM Prompting,"['Aman Bhargava', 'Cameron Witkowski', 'Manav Shah', 'Matt Thomson']",http://arxiv.org/pdf/2310.04444v2.pdf,2023-10-02,"['cs.cl', 'cs.ai', 'cs.lg', 'cs.ne']","  Prompt engineering is effective and important in the deployment of LLMs butis poorly understood mathematically. Here, we formalize prompt engineering asan optimal control problem on LLMs -- where the prompt is considered a controlvariable for modulating the output distribution of the LLM. Within thisframework, we ask a simple question: given a sequence of tokens, does therealways exist a prompt we can prepend that will steer the LLM toward accuratelypredicting the final token? We call such an optimal prompt the magic word sinceprepending the prompt causes the LLM to output the correct answer. If magicwords exist, can we find them? If so, what are their properties? We offeranalytic analysis on the controllability of the self-attention head where weprove a bound on controllability as a function of the singular values of itsweight matrices. We take inspiration from control theory to propose a metriccalled $k-\epsilon$ controllability to characterize LLM steerability. Wecompute the $k-\epsilon$ controllability of a panel of large language models,including Falcon-7b, Llama-7b, and Falcon-40b on 5000 WikiText causal languagemodeling tasks. Remarkably, we find that magic words of 10 tokens or less existfor over 97% of WikiText instances surveyed for each model."
Configuration Validation with Large Language Models,"['Xinyu Lian', 'Yinfang Chen', 'Runxiang Cheng', 'Jie Huang', 'Parth Thakkar', 'Tianyin Xu']",http://arxiv.org/pdf/2310.09690v1.pdf,2023-10-15,"['cs.se', 'cs.ai', 'cs.os']","  Misconfigurations are the major causes of software failures. Existingconfiguration validation techniques rely on manually written rules or testcases, which are expensive to implement and maintain, and are hard to becomprehensive. Leveraging machine learning (ML) and natural language processing(NLP) for configuration validation is considered a promising direction, but hasbeen facing challenges such as the need of not only large-scale configurationdata, but also system-specific features and models which are hard togeneralize. Recent advances in Large Language Models (LLMs) show the promisesto address some of the long-lasting limitations of ML/NLP-based configurationvalidation techniques. In this paper, we present an exploratory analysis on thefeasibility and effectiveness of using LLMs like GPT and Codex forconfiguration validation. Specifically, we take a first step to empiricallyevaluate LLMs as configuration validators without additional fine-tuning orcode generation. We develop a generic LLM-based validation framework, namedCiri, which integrates different LLMs. Ciri devises effective promptengineering with few-shot learning based on both valid configuration andmisconfiguration data. Ciri also validates and aggregates the outputs of LLMsto generate validation results, coping with known hallucination andnondeterminism of LLMs. We evaluate the validation effectiveness of Ciri onfive popular LLMs using configuration data of six mature, widely deployedopen-source systems. Our analysis (1) confirms the potential of using LLMs forconfiguration validation, (2) understands the design space of LLMbasedvalidators like Ciri, especially in terms of prompt engineering with few-shotlearning, and (3) reveals open challenges such as ineffectiveness in detectingcertain types of misconfigurations and biases to popular configurationparameters."
LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph  Construction,['Jamie McCusker'],http://arxiv.org/pdf/2311.09366v1.pdf,2023-11-15,"['cs.cl', 'cs.ai']","  While the potential of Open Information Extraction (Open IE) for KnowledgeGraph Construction (KGC) may seem promising, we find that the alignment of OpenIE extraction results with existing knowledge graphs to be inadequate. Theadvent of Large Language Models (LLMs), especially the commercially availableOpenAI models, have reset expectations for what is possible with deep learningmodels and have created a new field called prompt engineering. We investigatethe use of GPT models and prompt engineering for knowledge graph constructionwith the Wikidata knowledge graph to address a similar problem to Open IE,which we call Open Knowledge Extraction (OKE) using an approach we call theLinked Open Knowledge Extractor (LOKE, pronounced like ""Loki""). We consider theentity linking task essential to construction of real world knowledge graphs.We merge the CaRB benchmark scoring approach with data from the TekGen datasetfor the LOKE task. We then show that a well engineered prompt, paired with anaive entity linking approach (which we call LOKE-GPT), outperforms AllenAI'sOpenIE 4 implementation on the OKE task, although it over-generates triplescompared to the reference set due to overall triple scarcity in the TekGen set.Through an analysis of entity linkability in the CaRB dataset, as well asoutputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the ""silver""TekGen triples show that the task is significantly different in content fromOIE, if not structure. Through this analysis and a qualitative analysis ofsentence extractions via all methods, we found that LOKE-GPT extractions are ofhigh utility for the KGC task and suitable for use in semi-automated extractionsettings."
Text-to-Sticker: Style Tailoring Latent Diffusion Models for Human  Expression,"['Animesh Sinha', 'Bo Sun', 'Anmol Kalia', 'Arantxa Casanova', 'Elliot Blanchard', 'David Yan', 'Winnie Zhang', 'Tony Nelli', 'Jiahui Chen', 'Hardik Shah', 'Licheng Yu', 'Mitesh Kumar Singh', 'Ankit Ramchandani', 'Maziar Sanjabi', 'Sonal Gupta', 'Amy Bearman', 'Dhruv Mahajan']",http://arxiv.org/pdf/2311.10794v1.pdf,2023-11-17,['cs.cv'],"  We introduce Style Tailoring, a recipe to finetune Latent Diffusion Models(LDMs) in a distinct domain with high visual quality, prompt alignment andscene diversity. We choose sticker image generation as the target domain, asthe images significantly differ from photorealistic samples typically generatedby large-scale LDMs. We start with a competent text-to-image model, like Emu,and show that relying on prompt engineering with a photorealistic model togenerate stickers leads to poor prompt alignment and scene diversity. Toovercome these drawbacks, we first finetune Emu on millions of sticker-likeimages collected using weak supervision to elicit diversity. Next, we curatehuman-in-the-loop (HITL) Alignment and Style datasets from model generations,and finetune to improve prompt alignment and style alignment respectively.Sequential finetuning on these datasets poses a tradeoff between better stylealignment and prompt alignment gains. To address this tradeoff, we propose anovel fine-tuning method called Style Tailoring, which jointly fits the contentand style distribution and achieves best tradeoff. Evaluation results show ourmethod improves visual quality by 14%, prompt alignment by 16.2% and scenediversity by 15.3%, compared to prompt engineering the base Emu model forstickers generation."
Prompt Engineering-assisted Malware Dynamic Analysis Using GPT-4,"['Pei Yan', 'Shunquan Tan', 'Miaohui Wang', 'Jiwu Huang']",http://arxiv.org/pdf/2312.08317v1.pdf,2023-12-13,"['cs.cr', 'cs.ai']","  Dynamic analysis methods effectively identify shelled, wrapped, or obfuscatedmalware, thereby preventing them from invading computers. As a significantrepresentation of dynamic malware behavior, the API (Application ProgrammingInterface) sequence, comprised of consecutive API calls, has progressivelybecome the dominant feature of dynamic analysis methods. Though there have beennumerous deep learning models for malware detection based on API sequences, thequality of API call representations produced by those models is limited. Thesemodels cannot generate representations for unknown API calls, which weakensboth the detection performance and the generalization. Further, the conceptdrift phenomenon of API calls is prominent. To tackle these issues, weintroduce a prompt engineering-assisted malware dynamic analysis using GPT-4.In this method, GPT-4 is employed to create explanatory text for each API callwithin the API sequence. Afterward, the pre-trained language model BERT is usedto obtain the representation of the text, from which we derive therepresentation of the API sequence. Theoretically, this proposed method iscapable of generating representations for all API calls, excluding thenecessity for dataset training during the generation process. Utilizing therepresentation, a CNN-based detection model is designed to extract the feature.We adopt five benchmark datasets to validate the performance of the proposedmodel. The experimental results reveal that the proposed detection algorithmperforms better than the state-of-the-art method (TextCNN). Specifically, incross-database experiments and few-shot learning experiments, the proposedmodel achieves excellent detection performance and almost a 100% recall ratefor malware, verifying its superior generalization performance. The code isavailable at: github.com/yan-scnu/Prompted_Dynamic_Detection."
Learning to Prompt for Vision-Language Models,"['Kaiyang Zhou', 'Jingkang Yang', 'Chen Change Loy', 'Ziwei Liu']",http://arxiv.org/pdf/2109.01134v6.pdf,2021-09-02,"['cs.cv', 'cs.ai', 'cs.lg']","  Large pre-trained vision-language models like CLIP have shown great potentialin learning representations that are transferable across a wide range ofdownstream tasks. Different from the traditional representation learning thatis based mostly on discretized labels, vision-language pre-training alignsimages and texts in a common feature space, which allows zero-shot transfer toa downstream task via prompting, i.e., classification weights are synthesizedfrom natural language describing classes of interest. In this work, we showthat a major challenge for deploying such models in practice is promptengineering, which requires domain expertise and is extremely time-consuming --one needs to spend a significant amount of time on words tuning since a slightchange in wording could have a huge impact on performance. Inspired by recentadvances in prompt learning research in natural language processing (NLP), wepropose Context Optimization (CoOp), a simple approach specifically foradapting CLIP-like vision-language models for downstream image recognition.Concretely, CoOp models a prompt's context words with learnable vectors whilethe entire pre-trained parameters are kept fixed. To handle different imagerecognition tasks, we provide two implementations of CoOp: unified context andclass-specific context. Through extensive experiments on 11 datasets, wedemonstrate that CoOp requires as few as one or two shots to beat hand-craftedprompts with a decent margin and is able to gain significant improvements overprompt engineering with more shots, e.g., with 16 shots the average gain isaround 15% (with the highest reaching over 45%). Despite being a learning-basedapproach, CoOp achieves superb domain generalization performance compared withthe zero-shot model using hand-crafted prompts."
"Prompt-Free Diffusion: Taking ""Text"" out of Text-to-Image Diffusion  Models","['Xingqian Xu', 'Jiayi Guo', 'Zhangyang Wang', 'Gao Huang', 'Irfan Essa', 'Humphrey Shi']",http://arxiv.org/pdf/2305.16223v2.pdf,2023-05-25,['cs.cv'],"  Text-to-image (T2I) research has grown explosively in the past year, owing tothe large-scale pre-trained diffusion models and many emerging personalizationand editing approaches. Yet, one pain point persists: the text promptengineering, and searching high-quality text prompts for customized results ismore art than science. Moreover, as commonly argued: ""an image is worth athousand words"" - the attempt to describe a desired image with texts often endsup being ambiguous and cannot comprehensively cover delicate visual details,hence necessitating more additional controls from the visual domain. In thispaper, we take a bold step forward: taking ""Text"" out of a pre-trained T2Idiffusion model, to reduce the burdensome prompt engineering efforts for users.Our proposed framework, Prompt-Free Diffusion, relies on only visual inputs togenerate new images: it takes a reference image as ""context"", an optional imagestructural conditioning, and an initial noise, with absolutely no text prompt.The core architecture behind the scene is Semantic Context Encoder (SeeCoder),substituting the commonly used CLIP-based or LLM-based text encoder. Thereusability of SeeCoder also makes it a convenient drop-in component: one canalso pre-train a SeeCoder in one T2I model and reuse it for another. Throughextensive experiments, Prompt-Free Diffusion is experimentally found to (i)outperform prior exemplar-based image synthesis approaches; (ii) perform on parwith state-of-the-art T2I models using prompts following the best practice; and(iii) be naturally extensible to other downstream applications such as animefigure generation and virtual try-on, with promising quality. Our code andmodels are open-sourced at https://github.com/SHI-Labs/Prompt-Free-Diffusion."
Exploring Generative AI assisted feedback writing for students' written  responses to a physics conceptual question with prompt engineering and  few-shot learning,"['Tong Wan', 'Zhongzhou Chen']",http://arxiv.org/pdf/2311.06180v1.pdf,2023-11-10,['physics.ed-ph'],"  Instructor's feedback plays a critical role in students' development ofconceptual understanding and reasoning skills. However, grading student writtenresponses and providing personalized feedback can take a substantial amount oftime. In this study, we explore using GPT-3.5 to write feedback to studentwritten responses to conceptual questions with prompt engineering and few-shotlearning techniques. In stage one, we used a small portion (n=20) of thestudent responses on one conceptual question to iteratively train GPT. Four ofthe responses paired with human-written feedback were included in the prompt asexamples for GPT. We tasked GPT to generate feedback to the other 16 responses,and we refined the prompt after several iterations. In stage two, we gave fourstudent researchers the 16 responses as well as two versions of feedback, onewritten by the authors and the other by GPT. Students were asked to rate thecorrectness and usefulness of each feedback, and to indicate which one wasgenerated by GPT. The results showed that students tended to rate the feedbackby human and GPT equally on correctness, but they all rated the feedback by GPTas more useful. Additionally, the successful rates of identifying GPT'sfeedback were low, ranging from 0.1 to 0.6. In stage three, we tasked GPT togenerate feedback to the rest of the student responses (n=65). The feedback wasrated by four instructors based on the extent of modification needed if theywere to give the feedback to students. All the instructors rated approximately70% of the feedback statements needing only minor or no modification. Thisstudy demonstrated the feasibility of using Generative AI as an assistant togenerating feedback for student written responses with only a relatively smallnumber of examples. An AI assistance can be one of the solutions tosubstantially reduce time spent on grading student written responses."
Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case  Study in Medicine,"['Harsha Nori', 'Yin Tat Lee', 'Sheng Zhang', 'Dean Carignan', 'Richard Edgar', 'Nicolo Fusi', 'Nicholas King', 'Jonathan Larson', 'Yuanzhi Li', 'Weishung Liu', 'Renqian Luo', 'Scott Mayer McKinney', 'Robert Osazuwa Ness', 'Hoifung Poon', 'Tao Qin', 'Naoto Usuyama', 'Chris White', 'Eric Horvitz']",http://arxiv.org/pdf/2311.16452v1.pdf,2023-11-28,"['cs.cl', 'i.2.7']","  Generalist foundation models such as GPT-4 have displayed surprisingcapabilities in a wide variety of domains and tasks. Yet, there is a prevalentassumption that they cannot match specialist capabilities of fine-tuned models.For example, most explorations to date on medical competency benchmarks haveleveraged domain-specific training, as exemplified by efforts on BioGPT andMed-PaLM. We build on a prior study of GPT-4's capabilities on medicalchallenge benchmarks in the absence of special training. Rather than usingsimple prompting to highlight the model's out-of-the-box capabilities, weperform a systematic exploration of prompt engineering. We find that promptinginnovation can unlock deeper specialist capabilities and show that GPT-4 easilytops prior leading results for medical benchmarks. The prompting methods weexplore are general purpose, and make no specific use of domain expertise,removing the need for expert-curated content. Our experimental design carefullycontrols for overfitting during the prompt engineering process. We introduceMedprompt, based on a composition of several prompting strategies. WithMedprompt, GPT-4 achieves state-of-the-art results on all nine of the benchmarkdatasets in the MultiMedQA suite. The method outperforms leading specialistmodels such as Med-PaLM 2 by a significant margin with an order of magnitudefewer calls to the model. Steering GPT-4 with Medprompt achieves a 27%reduction in error rate on the MedQA dataset over the best methods to dateachieved with specialist models and surpasses a score of 90% for the firsttime. Beyond medical problems, we show the power of Medprompt to generalize toother domains and provide evidence for the broad applicability of the approachvia studies of the strategy on exams in electrical engineering, machinelearning, philosophy, accounting, law, nursing, and clinical psychology."
Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with  Language Models,"['Robert L. Logan IV', 'Ivana Balažević', 'Eric Wallace', 'Fabio Petroni', 'Sameer Singh', 'Sebastian Riedel']",http://arxiv.org/pdf/2106.13353v2.pdf,2021-06-24,"['cs.cl', 'cs.lg']","  Prompting language models (LMs) with training examples and task descriptionshas been seen as critical to recent successes in few-shot learning. In thiswork, we show that finetuning LMs in the few-shot setting can considerablyreduce the need for prompt engineering. In fact, one can use null prompts,prompts that contain neither task-specific templates nor training examples, andachieve competitive accuracy to manually-tuned prompts across a wide range oftasks. While finetuning LMs does introduce new parameters for each downstreamtask, we show that this memory overhead can be substantially reduced:finetuning only the bias terms can achieve comparable or better accuracy thanstandard finetuning while only updating 0.1% of the parameters. All in all, werecommend finetuning LMs for few-shot learning as it is more accurate, robustto different prompts, and can be made nearly as efficient as using frozen LMs."
An Empirical Study on Few-shot Knowledge Probing for Pretrained Language  Models,"['Tianxing He', 'Kyunghyun Cho', 'James Glass']",http://arxiv.org/pdf/2109.02772v2.pdf,2021-09-06,['cs.ai'],"  Prompt-based knowledge probing for 1-hop relations has been used to measurehow much world knowledge is stored in pretrained language models. Existing workuses considerable amounts of data to tune the prompts for better performance.In this work, we compare a variety of approaches under a few-shot knowledgeprobing setting, where only a small number (e.g., 10 or 20) of example triplesare available. In addition, we create a new dataset named TREx-2p, whichcontains 2-hop relations. We report that few-shot examples can strongly boostthe probing performance for both 1-hop and 2-hop relations. In particular, wefind that a simple-yet-effective approach of finetuning the bias vectors in themodel outperforms existing prompt-engineering methods. Our dataset and code areavailable at \url{https://github.com/cloudygoose/fewshot_lama}."
Design Guidelines for Prompt Engineering Text-to-Image Generative Models,"['Vivian Liu', 'Lydia B. Chilton']",http://arxiv.org/pdf/2109.06977v3.pdf,2021-09-14,['cs.hc'],"  Text-to-image generative models are a new and powerful way to generate visualartwork. However, the open-ended nature of text as interaction is double-edged;while users can input anything and have access to an infinite range ofgenerations, they also must engage in brute-force trial and error with the textprompt when the result quality is poor. We conduct a study exploring whatprompt keywords and model hyperparameters can help produce coherent outputs. Inparticular, we study prompts structured to include subject and style keywordsand investigate success and failure modes of these prompts. Our evaluation of5493 generations over the course of five experiments spans 51 abstract andconcrete subjects as well as 51 abstract and figurative styles. From thisevaluation, we present design guidelines that can help people produce betteroutcomes from text-to-image generative models."
Cut the CARP: Fishing for zero-shot story evaluation,"['Shahbuland Matiana', 'JR Smith', 'Ryan Teehan', 'Louis Castricato', 'Stella Biderman', 'Leo Gao', 'Spencer Frazier']",http://arxiv.org/pdf/2110.03111v3.pdf,2021-10-06,['cs.cl'],"  Recent advances in large-scale language models (Raffel et al., 2019; Brown etal., 2020) have brought significant qualitative and quantitative improvementsin machine-driven text generation. Despite this, generation and evaluation ofmachine-generated narrative text remains a challenging problem. Objectiveevaluation of computationally-generated stories may be prohibitively expensive,require meticulously annotated datasets, or may not adequately measure thelogical coherence of a generated story's narratological structure.  Informed by recent advances in contrastive learning (Radford et al., 2021),we present Contrastive Authoring and Reviewing Pairing (CARP): a scalable,efficient method for performing qualitatively superior, zero-shot evaluation ofstories. We show a strong correlation between human evaluation of stories andthose of CARP. Model outputs more significantly correlate with correspondinghuman input than those language-model based methods which utilize finetuning orprompt engineering approaches. We also present and analyze the Story-CritiqueDataset, a new corpora composed of 1.3 million aligned story-critique pairsderived from over 80,000 stories. We expect this corpus to be of interest toNLP researchers."
Solving Probability and Statistics Problems by Program Synthesis,"['Leonard Tang', 'Elizabeth Ke', 'Nikhil Singh', 'Nakul Verma', 'Iddo Drori']",http://arxiv.org/pdf/2111.08267v1.pdf,2021-11-16,"['cs.lg', 'cs.ai', 'cs.cl', 'cs.pl']","  We solve university level probability and statistics questions by programsynthesis using OpenAI's Codex, a Transformer trained on text and fine-tuned oncode. We transform course problems from MIT's 18.05 Introduction to Probabilityand Statistics and Harvard's STAT110 Probability into programming tasks. Wethen execute the generated code to get a solution. Since these course questionsare grounded in probability, we often aim to have Codex generate probabilisticprograms that simulate a large number of probabilistic dependencies to computeits solution. Our approach requires prompt engineering to transform thequestion from its original form to an explicit, tractable form that results ina correct program and solution. To estimate the amount of work needed totranslate an original question into its tractable form, we measure thesimilarity between original and transformed questions. Our work is the first tointroduce a new dataset of university-level probability and statistics problemsand solve these problems in a scalable fashion using the program synthesiscapabilities of large language models."
StyleMC: Multi-Channel Based Fast Text-Guided Image Generation and  Manipulation,"['Umut Kocasari', 'Alara Dirik', 'Mert Tiftikci', 'Pinar Yanardag']",http://arxiv.org/pdf/2112.08493v1.pdf,2021-12-15,"['cs.cv', 'cs.lg']","  Discovering meaningful directions in the latent space of GANs to manipulatesemantic attributes typically requires large amounts of labeled data. Recentwork aims to overcome this limitation by leveraging the power of ContrastiveLanguage-Image Pre-training (CLIP), a joint text-image model. While promising,these methods require several hours of preprocessing or training to achieve thedesired manipulations. In this paper, we present StyleMC, a fast and efficientmethod for text-driven image generation and manipulation. StyleMC uses aCLIP-based loss and an identity loss to manipulate images via a single textprompt without significantly affecting other attributes. Unlike prior work,StyleMC requires only a few seconds of training per text prompt to find stableglobal directions, does not require prompt engineering and can be used with anypre-trained StyleGAN2 model. We demonstrate the effectiveness of our method andcompare it to state-of-the-art methods. Our code can be found athttp://catlab-team.github.io/stylemc."
QaNER: Prompting Question Answering Models for Few-shot Named Entity  Recognition,"['Andy T. Liu', 'Wei Xiao', 'Henghui Zhu', 'Dejiao Zhang', 'Shang-Wen Li', 'Andrew Arnold']",http://arxiv.org/pdf/2203.01543v2.pdf,2022-03-03,"['cs.cl', 'cs.ai', 'cs.lg']","  Recently, prompt-based learning for pre-trained language models has succeededin few-shot Named Entity Recognition (NER) by exploiting prompts as taskguidance to increase label efficiency. However, previous prompt-based methodsfor few-shot NER have limitations such as a higher computational complexity,poor zero-shot ability, requiring manual prompt engineering, or lack of promptrobustness. In this work, we address these shortcomings by proposing a newprompt-based learning NER method with Question Answering (QA), called QaNER.Our approach includes 1) a refined strategy for converting NER problems intothe QA formulation; 2) NER prompt generation for QA models; 3) prompt-basedtuning with QA models on a few annotated NER examples; 4) zero-shot NER byprompting the QA model. Comparing the proposed approach with previous methods,QaNER is faster at inference, insensitive to the prompt quality, and robust tohyper-parameters, as well as demonstrating significantly better low-resourceperformance and zero-shot capability."
Executive Function: A Contrastive Value Policy for Resampling and  Relabeling Perceptions via Hindsight Summarization?,"['Chris Lengerich', 'Ben Lengerich']",http://arxiv.org/pdf/2204.12639v1.pdf,2022-04-27,['cs.cl'],"  We develop the few-shot continual learning task from first principles andhypothesize an evolutionary motivation and mechanism of action for executivefunction as a contrastive value policy which resamples and relabels perceptiondata via hindsight summarization to minimize attended prediction error, similarto an online prompt engineering problem. This is made feasible by the use of amemory policy and a pretrained network with inductive biases for a grammar oflearning and is trained to maximize evolutionary survival. We show how thismodel of executive function can be used to implement hypothesis testing as astream of consciousness and may explain observations of human few-shot learningand neuroanatomy."
Polyglot Prompt: Multilingual Multitask PrompTraining,"['Jinlan Fu', 'See-Kiong Ng', 'Pengfei Liu']",http://arxiv.org/pdf/2204.14264v2.pdf,2022-04-29,['cs.cl'],"  This paper aims for a potential architectural improvement for multilinguallearning and asks: Can different tasks from different languages be modeled in amonolithic framework, i.e. without any task/language-specific module? Thebenefit of achieving this could open new doors for future multilingualresearch, including allowing systems trained on low resources to be furtherassisted by other languages as well as other tasks. We approach this goal bydeveloping a learning framework named Polyglot Prompting to exploit promptingmethods for learning a unified semantic space for different languages and taskswith multilingual prompt engineering. We performed a comprehensive evaluationof 6 tasks, namely topic classification, sentiment classification, named entityrecognition, question answering, natural language inference, and summarization,covering 24 datasets and 49 languages. The experimental results demonstratedthe efficacy of multilingual multitask prompt-based learning and led toinspiring observations. We also present an interpretable multilingualevaluation methodology and show how the proposed framework, multilingualmultitask prompt training, works. We release all datasets prompted in the bestsetting and code."
CLIP-CLOP: CLIP-Guided Collage and Photomontage,"['Piotr Mirowski', 'Dylan Banarse', 'Mateusz Malinowski', 'Simon Osindero', 'Chrisantha Fernando']",http://arxiv.org/pdf/2205.03146v3.pdf,2022-05-06,"['cs.cv', 'cs.ai']","  The unabated mystique of large-scale neural networks, such as the CLIP dualimage-and-text encoder, popularized automatically generated art. Increasinglymore sophisticated generators enhanced the artworks' realism and visualappearance, and creative prompt engineering enabled stylistic expression.Guided by an artist-in-the-loop ideal, we design a gradient-based generator toproduce collages. It requires the human artist to curate libraries of imagepatches and to describe (with prompts) the whole image composition, with theoption to manually adjust the patches' positions during generation, therebyallowing humans to reclaim some control of the process and achieve greatercreative freedom. We explore the aesthetic potentials of high-resolutioncollages, and provide an open-source Google Colab as an artistic tool."
Toxicity Detection with Generative Prompt-based Inference,"['Yau-Shian Wang', 'Yingshan Chang']",http://arxiv.org/pdf/2205.12390v1.pdf,2022-05-24,"['cs.cl', 'cs.ai']","  Due to the subtleness, implicity, and different possible interpretationsperceived by different people, detecting undesirable content from text is anuanced difficulty. It is a long-known risk that language models (LMs), oncetrained on corpus containing undesirable content, have the power to manifestbiases and toxicity. However, recent studies imply that, as a remedy, LMs arealso capable of identifying toxic content without additional fine-tuning.Prompt-methods have been shown to effectively harvest this surprisingself-diagnosing capability. However, existing prompt-based methods usuallyspecify an instruction to a language model in a discriminative way. In thiswork, we explore the generative variant of zero-shot prompt-based toxicitydetection with comprehensive trials on prompt engineering. We evaluate on threedatasets with toxicity labels annotated on social media posts. Our analysishighlights the strengths of our generative classification approach bothquantitatively and qualitatively. Interesting aspects of self-diagnosis and itsethical implications are discussed."
The Creativity of Text-to-Image Generation,['Jonas Oppenlaender'],http://arxiv.org/pdf/2206.02904v4.pdf,2022-05-13,"['cs.hc', 'cs.gr', 'h.5; h.m']","  Text-guided synthesis of images has made a giant leap towards becoming amainstream phenomenon. With text-to-image generation systems, anybody cancreate digital images and artworks. This provokes the question of whethertext-to-image generation is creative. This paper expounds on the nature ofhuman creativity involved in text-to-image art (so-called ""AI art"") with aspecific focus on the practice of prompt engineering. The paper argues that thecurrent product-centered view of creativity falls short in the context oftext-to-image generation. A case exemplifying this shortcoming is provided andthe importance of online communities for the creative ecosystem oftext-to-image art is highlighted. The paper provides a high-level summary ofthis online ecosystem drawing on Rhodes' conceptual four P model of creativity.Challenges for evaluating the creativity of text-to-image generation andopportunities for research on text-to-image generation in the field ofHuman-Computer Interaction (HCI) are discussed."
Rationale-Augmented Ensembles in Language Models,"['Xuezhi Wang', 'Jason Wei', 'Dale Schuurmans', 'Quoc Le', 'Ed Chi', 'Denny Zhou']",http://arxiv.org/pdf/2207.00747v1.pdf,2022-07-02,['cs.cl'],"  Recent research has shown that rationales, or step-by-step chains of thought,can be used to improve performance in multi-step reasoning tasks. We reconsiderrationale-augmented prompting for few-shot in-context learning, where (input ->output) prompts are expanded to (input, rationale -> output) prompts. Forrationale-augmented prompting we demonstrate how existing approaches, whichrely on manual prompt engineering, are subject to sub-optimal rationales thatmay harm performance. To mitigate this brittleness, we propose a unifiedframework of rationale-augmented ensembles, where we identify rationalesampling in the output space as the key component to robustly improveperformance. This framework is general and can easily be extended to commonnatural language processing tasks, even those that do not traditionallyleverage intermediate steps, such as question answering, word sensedisambiguation, and sentiment analysis. We demonstrate that rationale-augmentedensembles achieve more accurate and interpretable results than existingprompting approaches--including standard prompting without rationales andrationale-based chain-of-thought prompting--while simultaneously improvinginterpretability of model predictions through the associated rationales."
Text-Guided Synthesis of Artistic Images with Retrieval-Augmented  Diffusion Models,"['Robin Rombach', 'Andreas Blattmann', 'Björn Ommer']",http://arxiv.org/pdf/2207.13038v1.pdf,2022-07-26,['cs.cv'],"  Novel architectures have recently improved generative image synthesis leadingto excellent visual quality in various tasks. Of particular note is the fieldof ``AI-Art'', which has seen unprecedented growth with the emergence ofpowerful multimodal models such as CLIP. By combining speech and imagesynthesis models, so-called ``prompt-engineering'' has become established, inwhich carefully selected and composed sentences are used to achieve a certainvisual style in the synthesized image. In this note, we present an alternativeapproach based on retrieval-augmented diffusion models (RDMs). In RDMs, a setof nearest neighbors is retrieved from an external database during training foreach training instance, and the diffusion model is conditioned on theseinformative samples. During inference (sampling), we replace the retrievaldatabase with a more specialized database that contains, for example, onlyimages of a particular visual style. This provides a novel way to prompt ageneral trained model after training and thereby specify a particular visualstyle. As shown by our experiments, this approach is superior to specifying thevisual style within the text prompt. We open-source code and model weights athttps://github.com/CompVis/latent-diffusion ."
Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation  with Large Language Models,"['Hendrik Strobelt', 'Albert Webson', 'Victor Sanh', 'Benjamin Hoover', 'Johanna Beyer', 'Hanspeter Pfister', 'Alexander M. Rush']",http://arxiv.org/pdf/2208.07852v1.pdf,2022-08-16,"['cs.cl', 'cs.hc', 'cs.lg']","  State-of-the-art neural language models can now be used to solve ad-hoclanguage tasks through zero-shot prompting without the need for supervisedtraining. This approach has gained popularity in recent years, and researchershave demonstrated prompts that achieve strong accuracy on specific NLP tasks.However, finding a prompt for new tasks requires experimentation. Differentprompt templates with different wording choices lead to significant accuracydifferences. PromptIDE allows users to experiment with prompt variations,visualize prompt performance, and iteratively optimize prompts. We developed aworkflow that allows users to first focus on model feedback using small databefore moving on to a large data regime that allows empirical grounding ofpromising prompts using quantitative measures of the task. The tool then allowseasy deployment of the newly created ad-hoc models. We demonstrate the utilityof PromptIDE (demo at http://prompt.vizhub.ai) and our workflow using severalreal-world use cases."
Will It Blend? Mixing Training Paradigms & Prompting for Argument  Quality Prediction,"['Michiel van der Meer', 'Myrthe Reuver', 'Urja Khurana', 'Lea Krause', 'Selene Báez Santamaría']",http://arxiv.org/pdf/2209.08966v2.pdf,2022-09-19,"['cs.cl', 'cs.ai']","  This paper describes our contributions to the Shared Task of the 9th Workshopon Argument Mining (2022). Our approach uses Large Language Models for the taskof Argument Quality Prediction. We perform prompt engineering using GPT-3, andalso investigate the training paradigms multi-task learning, contrastivelearning, and intermediate-task training. We find that a mixed prediction setupoutperforms single models. Prompting GPT-3 works best for predicting argumentvalidity, and argument novelty is best estimated by a model trained using allthree training paradigms."
Legal Prompting: Teaching a Language Model to Think Like a Lawyer,"['Fangyi Yu', 'Lee Quartey', 'Frank Schilder']",http://arxiv.org/pdf/2212.01326v2.pdf,2022-12-02,"['cs.cl', 'cs.ai', 'i.2.7']","  Large language models that are capable of zero or few-shot promptingapproaches have given rise to the new research area of prompt engineering.Recent advances showed that for example Chain-of-Thought (CoT) prompts canimprove arithmetic or common sense tasks significantly. We explore how suchapproaches fare with legal reasoning tasks and take the COLIEE entailment taskbased on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuningapproaches. Our findings show that while CoT prompting and fine-tuning withexplanations approaches show improvements, the best results are produced byprompts that are derived from specific legal reasoning techniques such as IRAC(Issue, Rule, Application, Conclusion). Based on our experiments we improve the2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 bestsystem of 0.6789 accuracy with an accuracy of 0.7431."
Controllable Image Captioning via Prompting,"['Ning Wang', 'Jiahao Xie', 'Jihao Wu', 'Mingbo Jia', 'Linlin Li']",http://arxiv.org/pdf/2212.01803v1.pdf,2022-12-04,['cs.cv'],"  Despite the remarkable progress of image captioning, existing captionerstypically lack the controllable capability to generate desired image captions,e.g., describing the image in a rough or detailed manner, in a factual oremotional view, etc. In this paper, we show that a unified model is qualifiedto perform well in diverse domains and freely switch among multiple styles.Such a controllable capability is achieved by embedding the prompt learninginto the image captioning framework. To be specific, we design a set of promptsto fine-tune the pre-trained image captioner. These prompts allow the model toabsorb stylized data from different domains for joint training, withoutperformance degradation in each domain. Furthermore, we optimize the promptswith learnable vectors in the continuous word embedding space, avoiding theheuristic prompt engineering and meanwhile exhibiting superior performance. Inthe inference stage, our model is able to generate desired stylized captions bychoosing the corresponding prompts. Extensive experiments verify thecontrollable capability of the proposed method. Notably, we achieve outstandingperformance on two diverse image captioning benchmarks including COCO Karpathysplit and TextCaps using a unified model."
Fake it till you make it: Learning transferable representations from  synthetic ImageNet clones,"['Mert Bulent Sariyildiz', 'Karteek Alahari', 'Diane Larlus', 'Yannis Kalantidis']",http://arxiv.org/pdf/2212.08420v2.pdf,2022-12-16,"['cs.cv', 'cs.lg']","  Recent image generation models such as Stable Diffusion have exhibited animpressive ability to generate fairly realistic images starting from a simpletext prompt. Could such models render real images obsolete for training imageprediction models? In this paper, we answer part of this provocative questionby investigating the need for real images when training models for ImageNetclassification. Provided only with the class names that have been used to buildthe dataset, we explore the ability of Stable Diffusion to generate syntheticclones of ImageNet and measure how useful these are for training classificationmodels from scratch. We show that with minimal and class-agnostic promptengineering, ImageNet clones are able to close a large part of the gap betweenmodels produced by synthetic images and models trained with real images, forthe several standard classification benchmarks that we consider in this study.More importantly, we show that models trained on synthetic images exhibitstrong generalization properties and perform on par with models trained on realdata for transfer. Project page: https://europe.naverlabs.com/imagenet-sd/"
Explanation Regeneration via Information Bottleneck,"['Qintong Li', 'Zhiyong Wu', 'Lingpeng Kong', 'Wei Bi']",http://arxiv.org/pdf/2212.09603v2.pdf,2022-12-19,['cs.cl'],"  Explaining the black-box predictions of NLP models naturally and accuratelyis an important open problem in natural language generation. These free-textexplanations are expected to contain sufficient and carefully-selected evidenceto form supportive arguments for predictions. Due to the superior generativecapacity of large pretrained language models, recent work built on promptengineering enables explanation generation without specific training. However,explanation generated through single-pass prompting often lacks sufficiency andconciseness. To address this problem, we develop an information bottleneckmethod EIB to produce refined explanations that are sufficient and concise. Ourapproach regenerates the free-text explanation by polishing the single-passoutput from the pretrained language model but retaining the information thatsupports the contents being explained. Experiments on two out-of-domain tasksverify the effectiveness of EIB through automatic evaluation andthoroughly-conducted human evaluation."
Optimizing Prompts for Text-to-Image Generation,"['Yaru Hao', 'Zewen Chi', 'Li Dong', 'Furu Wei']",http://arxiv.org/pdf/2212.09611v1.pdf,2022-12-19,"['cs.cl', 'cs.cv']","  Well-designed prompts can guide text-to-image models to generate amazingimages. However, the performant prompts are often model-specific and misalignedwith user input. Instead of laborious human engineering, we propose promptadaptation, a general framework that automatically adapts original user inputto model-preferred prompts. Specifically, we first perform supervisedfine-tuning with a pretrained language model on a small collection of manuallyengineered prompts. Then we use reinforcement learning to explore betterprompts. We define a reward function that encourages the policy to generatemore aesthetically pleasing images while preserving the original userintentions. Experimental results on Stable Diffusion show that our methodoutperforms manual prompt engineering in terms of both automatic metrics andhuman preference ratings. Moreover, reinforcement learning further boostsperformance, especially on out-of-domain prompts. The pretrained checkpointsare available at https://aka.ms/promptist. The demo can be found athttps://aka.ms/promptist-demo."
Using Large Language Models to Generate Engaging Captions for Data  Visualizations,"['Ashley Liew', 'Klaus Mueller']",http://arxiv.org/pdf/2212.14047v1.pdf,2022-12-27,"['cs.cl', 'cs.ai', 'cs.hc']","  Creating compelling captions for data visualizations has been a longstandingchallenge. Visualization researchers are typically untrained in journalisticreporting and hence the captions that are placed below data visualizations tendto be not overly engaging and rather just stick to basic observations about thedata. In this work we explore the opportunities offered by the newly emergingcrop of large language models (LLM) which use sophisticated deep learningtechnology to produce human-like prose. We ask, can these powerful softwaredevices be purposed to produce engaging captions for generic datavisualizations like a scatterplot. It turns out that the key challenge lies indesigning the most effective prompt for the LLM, a task called promptengineering. We report on first experiments using the popular LLM GPT-3 anddeliver some promising results."
Fixing Hardware Security Bugs with Large Language Models,"['Baleegh Ahmad', 'Shailja Thakur', 'Benjamin Tan', 'Ramesh Karri', 'Hammond Pearce']",http://arxiv.org/pdf/2302.01215v1.pdf,2023-02-02,['cs.cr'],"  Novel AI-based code-writing Large Language Models (LLMs) such as OpenAI'sCodex have demonstrated capabilities in many coding-adjacent domains. In thiswork we consider how LLMs maybe leveraged to automatically repair securityrelevant bugs present in hardware designs. We focus on bug repair in codewritten in the Hardware Description Language Verilog. For this study we build acorpus of domain-representative hardware security bugs. We then design andimplement a framework to quantitatively evaluate the performance of any LLMtasked with fixing the specified bugs. The framework supports design spaceexploration of prompts (i.e., prompt engineering) and identifying the bestparameters for the LLM. We show that an ensemble of LLMs can repair all ten ofour benchmarks. This ensemble outperforms the state-of-the-art Cirfix hardwarebug repair tool on its own suite of bugs. These results show that LLMs canrepair hardware security bugs and the framework is an important step towardsthe ultimate goal of an automated end-to-end bug repair framework."
UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation,"['Daixuan Cheng', 'Shaohan Huang', 'Junyu Bi', 'Yuefeng Zhan', 'Jianfeng Liu', 'Yujing Wang', 'Hao Sun', 'Furu Wei', 'Denvy Deng', 'Qi Zhang']",http://arxiv.org/pdf/2303.08518v3.pdf,2023-03-15,['cs.cl'],"  Large Language Models (LLMs) are popular for their impressive abilities, butthe need for model-specific fine-tuning or task-specific prompt engineering canhinder their generalization. We propose UPRISE (Universal Prompt Retrieval forImproving zero-Shot Evaluation), which tunes a lightweight and versatileretriever that automatically retrieves prompts for a given zero-shot taskinput. Specifically, we demonstrate universality in a cross-task andcross-model scenario: the retriever is tuned on a diverse set of tasks, buttested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, fortuning the retriever, but test the retriever on different LLMs of much largerscales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show thatUPRISE mitigates the hallucination problem in our experiments with ChatGPT,suggesting its potential to improve even the strongest LLMs. Our model and codeare available at https://github.com/microsoft/LMOps."
Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models,"['Xinyang Liu', 'Dongsheng Wang', 'Miaoge Li', 'Zhibin Duan', 'Yishi Xu', 'Bo Chen', 'Mingyuan Zhou']",http://arxiv.org/pdf/2303.09100v1.pdf,2023-03-16,"['cs.cv', 'cs.cl', 'cs.lg']","  For downstream applications of vision-language pre-trained models, there hasbeen significant interest in constructing effective prompts. Existing works onprompt engineering, which either require laborious manual designs or optimizethe prompt tuning as a point estimation problem, may fail to describe diversecharacteristics of categories and limit their applications. We introduce aBayesian probabilistic resolution to prompt learning, where the label-specificstochastic prompts are generated hierarchically by first sampling a latentvector from an underlying distribution and then employing a lightweightgenerative model. Importantly, we semantically regularize prompt learning withthe visual knowledge and view images and the corresponding prompts as patch andtoken sets under optimal transport, which pushes the prompt tokens tofaithfully capture the label-specific visual concepts, instead of overfittingthe training categories. Moreover, the proposed model can also bestraightforwardly extended to the conditional case where theinstance-conditional prompts are generated to improve the generalizability.Extensive experiments on 15 datasets show promising transferability andgeneralization performance of our proposed model."
Safety Analysis in the Era of Large Language Models: A Case Study of  STPA using ChatGPT,"['Yi Qi', 'Xingyu Zhao', 'Siddartha Khastgir', 'Xiaowei Huang']",http://arxiv.org/pdf/2304.01246v2.pdf,2023-04-03,"['cs.cl', 'cs.ai', 'cs.cy', 'cs.se']","  Can safety analysis make use of Large Language Models (LLMs)? A case studyexplores Systems Theoretic Process Analysis (STPA) applied to AutomaticEmergency Brake (AEB) and Electricity Demand Side Management (DSM) systemsusing ChatGPT. We investigate how collaboration schemes, input semanticcomplexity, and prompt guidelines influence STPA results. Comparative resultsshow that using ChatGPT without human intervention may be inadequate due toreliability related issues, but with careful design, it may outperform humanexperts. No statistically significant differences are found when varying theinput semantic complexity or using common prompt guidelines, which suggests thenecessity for developing domain-specific prompt engineering. We also highlightfuture challenges, including concerns about LLM trustworthiness and thenecessity for standardisation and regulation in this domain."
Geotechnical Parrot Tales (GPT): Harnessing Large Language Models in  geotechnical engineering,['Krishna Kumar'],http://arxiv.org/pdf/2304.02138v3.pdf,2023-04-04,"['cs.cl', 'physics.geo-ph', 'i.2.7; j.2.6']","  The widespread adoption of large language models (LLMs), such as OpenAI'sChatGPT, could revolutionize various industries, including geotechnicalengineering. However, GPT models can sometimes generate plausible-sounding butfalse outputs, leading to hallucinations. In this article, we discuss theimportance of prompt engineering in mitigating these risks and harnessing thefull potential of GPT for geotechnical applications. We explore the challengesand pitfalls associated with LLMs and highlight the role of context in ensuringaccurate and valuable responses. Furthermore, we examine the development ofcontext-specific search engines and the potential of LLMs to become a naturalinterface for complex tasks, such as data analysis and design. We also developa unified interface using natural language to handle complex geotechnicalengineering tasks and data analysis. By integrating GPT into geotechnicalengineering workflows, professionals can streamline their work and developsustainable and resilient infrastructure systems for the future."
Evaluation of ChatGPT Family of Models for Biomedical Reasoning and  Classification,"['Shan Chen', 'Yingya Li', 'Sheng Lu', 'Hoang Van', 'Hugo JWL Aerts', 'Guergana K. Savova', 'Danielle S. Bitterman']",http://arxiv.org/pdf/2304.02496v1.pdf,2023-04-05,"['cs.cl', 'cs.ai']","  Recent advances in large language models (LLMs) have shown impressive abilityin biomedical question-answering, but have not been adequately investigated formore specific biomedical applications. This study investigates the performanceof LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedicaltasks beyond question-answering. Because no patient data can be passed to theOpenAI API public interface, we evaluated model performance with over 10000samples as proxies for two fundamental tasks in the clinical domain -classification and reasoning. The first task is classifying whether statementsof clinical and policy recommendations in scientific literature constitutehealth advice. The second task is causal relation detection from the biomedicalliterature. We compared LLMs with simpler models, such as bag-of-words (BoW)with logistic regression, and fine-tuned BioBERT models. Despite the excitementaround viral ChatGPT, we found that fine-tuning for two fundamental NLP tasksremained the best strategy. The simple BoW model performed on par with the mostcomplex LLM prompting. Prompt engineering required significant investment."
"VOICE: Visual Oracle for Interaction, Conversation, and Explanation","['Donggang Jia', 'Alexandra Irger', 'Ondrej Strnad', 'Johanna Bjorklund', 'Anders Ynnerman', 'Ivan Viola']",http://arxiv.org/pdf/2304.04083v1.pdf,2023-04-08,"['cs.hc', 'cs.gr']","  We present VOICE, a novel approach for connecting large language models'(LLM) conversational capabilities with interactive exploratory visualization.VOICE introduces several innovative technical contributions that drive ourconversational visualization framework. Our foundation is a pack-of-bots thatcan perform specific tasks, such as assigning tasks, extracting instructions,and generating coherent content. We employ fine-tuning and prompt engineeringtechniques to tailor bots' performance to their specific roles and accuratelyrespond to user queries, and a new prompt-based iterative scene-tree generationestablishes a coupling with a structural model. Our text-to-visualizationmethod generates a flythrough sequence matching the content explanation.Finally, 3D natural language interaction provides capabilities to navigate andmanipulate the 3D models in real-time. The VOICE framework can receivearbitrary voice commands from the user and responds verbally, tightly coupledwith corresponding visual representation with low latency and high accuracy. Wedemonstrate the effectiveness and high generalizability potential of ourapproach by applying it to two distinct domains: analyzing three 3D molecularmodels with multi-scale and multi-instance attributes, and showcasing itseffectiveness on a cartographic map visualization. A free copy of this paperand all supplemental materials are available at https://osf.io/g7fbr/."
Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot  Task Generalization,"['Puyuan Peng', 'Brian Yan', 'Shinji Watanabe', 'David Harwath']",http://arxiv.org/pdf/2305.11095v3.pdf,2023-05-18,"['eess.as', 'cs.ai', 'cs.cl', 'cs.lg', 'cs.sd']","  We investigate the emergent abilities of the recently proposed web-scalespeech model Whisper, by adapting it to unseen tasks with prompt engineering.We selected three tasks: audio-visual speech recognition (AVSR), code-switchedspeech recognition (CS-ASR), and speech translation (ST) on unseen languagepairs. We design task-specific prompts, by either leveraging anotherlarge-scale model, or simply manipulating the special tokens in the defaultprompts. Experiments show that compared to the default prompts, our proposedprompts improve performance by 10% to 45% on the three zero-shot tasks, andeven outperform SotA supervised models on some datasets. In addition, ourexperiments reveal many interesting properties of Whisper, including itsrobustness to prompts, bias on accents, and the multilingual understanding inits latent space. Code is available athttps://github.com/jasonppy/PromptingWhisper"
Constructing Dreams using Generative AI,"['Safinah Ali', 'Daniella DiPaola', 'Randi Williams', 'Prerna Ravi', 'Cynthia Breazeal']",http://arxiv.org/pdf/2305.12013v1.pdf,2023-05-19,"['cs.hc', 'cs.ai', 'cs.cy']","  Generative AI tools introduce new and accessible forms of media creation foryouth. They also raise ethical concerns about the generation of fake media,data protection, privacy and ownership of AI-generated art. Since generative AIis already being used in products used by youth, it is critical that theyunderstand how these tools work and how they can be used or misused. In thiswork, we facilitated students' generative AI learning through expression oftheir imagined future identities. We designed a learning workshop - Dreamingwith AI - where students learned about the inner workings of generative AItools, used text-to-image generation algorithms to create their imaged futuredreams, reflected on the potential benefits and harms of generative AI toolsand voiced their opinions about policies for the use of these tools inclassrooms. In this paper, we present the learning activities and experiencesof 34 high school students who engaged in our workshops. Students reachedcreative learning objectives by using prompt engineering to create their futuredreams, gained technical knowledge by learning the abilities, limitations,text-visual mappings and applications of generative AI, and identified mostpotential societal benefits and harms of generative AI."
Interactive Data Synthesis for Systematic Vision Adaptation via  LLMs-AIGCs Collaboration,"['Qifan Yu', 'Juncheng Li', 'Wentao Ye', 'Siliang Tang', 'Yueting Zhuang']",http://arxiv.org/pdf/2305.12799v1.pdf,2023-05-22,['cs.cv'],"  Recent text-to-image generation models have shown promising results ingenerating high-fidelity photo-realistic images. In parallel, the problem ofdata scarcity has brought a growing interest in employing AIGC technology forhigh-quality data expansion. However, this paradigm requires well-designedprompt engineering that cost-less data expansion and labeling remainunder-explored. Inspired by LLM's powerful capability in task guidance, wepropose a new paradigm of annotated data expansion named as ChatGenImage. Thecore idea behind it is to leverage the complementary strengths of diversemodels to establish a highly effective and user-friendly pipeline forinteractive data augmentation. In this work, we extensively study how LLMscommunicate with AIGC model to achieve more controllable image generation andmake the first attempt to collaborate them for automatic data augmentation fora variety of downstream tasks. Finally, we present fascinating results obtainedfrom our ChatGenImage framework and demonstrate the powerful potential of oursynthetic data for systematic vision adaptation. Our codes are available athttps://github.com/Yuqifan1117/Labal-Anything-Pipeline."
Making Language Models Better Tool Learners with Execution Feedback,"['Shuofei Qiao', 'Honghao Gui', 'Huajun Chen', 'Ningyu Zhang']",http://arxiv.org/pdf/2305.13068v1.pdf,2023-05-22,"['cs.cl', 'cs.ai', 'cs.hc', 'cs.ir', 'cs.lg']","  Tools serve as pivotal interfaces that enable humans to understand andreshape the world. With the advent of foundational models, AI systems canutilize tools to expand their capabilities and interact with the world.Existing tool learning methodologies, encompassing supervised fine-tuning andprompt engineering approaches, often induce language models to utilize toolsindiscriminately, as complex problems often exceed their own competencies.However, introducing tools for simple tasks, which the models themselves canreadily resolve, can inadvertently propagate errors rather than enhanceperformance. This leads to the research question: can we teach language modelswhen and how to use tools? To meet this need, we propose Tool leaRning wIthexeCution fEedback (TRICE), a two-stage end-to-end framework that enables themodel to continually learn through feedback derived from tool execution,thereby learning when and how to use tools effectively. Experimental results,backed by further analysis, show that TRICE can make the language model toselectively use tools by decreasing the model's dependency on tools whileenhancing the performance. Code and datasets will be available inhttps://github.com/zjunlp/trice."
Do prompt positions really matter?,"['Junyu Mao', 'Stuart E. Middleton', 'Mahesan Niranjan']",http://arxiv.org/pdf/2305.14493v3.pdf,2023-05-23,['cs.cl'],"  Prompt-based models have gathered a lot of attention from researchers due totheir remarkable advancements in the fields of zero-shot and few-shot learning.Developing an effective prompt template plays a critical role. However, priorstudies have mainly focused on prompt vocabulary selection or embeddinginitialization within a predefined template with the prompt position fixed. Inthis empirical study, we conduct the most comprehensive analysis to date ofprompt position for diverse natural language process tasks. Our findingsquantify the substantial impact prompt position has on model performance. Weobserve that the prompt position used in prior studies is often sub-optimal.These findings suggest prompt position optimisation as a valuable researchdirection to fill the gap in existing prompt engineering methodologies."
ContrastNER: Contrastive-based Prompt Tuning for Few-shot NER,"['Amirhossein Layegh', 'Amir H. Payberah', 'Ahmet Soylu', 'Dumitru Roman', 'Mihhail Matskin']",http://arxiv.org/pdf/2305.17951v1.pdf,2023-05-29,"['cs.cl', 'cs.ai']","  Prompt-based language models have produced encouraging results in numerousapplications, including Named Entity Recognition (NER) tasks. NER aims toidentify entities in a sentence and provide their types. However, the strongperformance of most available NER approaches is heavily dependent on the designof discrete prompts and a verbalizer to map the model-predicted outputs toentity categories, which are complicated undertakings. To address thesechallenges, we present ContrastNER, a prompt-based NER framework that employsboth discrete and continuous tokens in prompts and uses a contrastive learningapproach to learn the continuous prompts and forecast entity types. Theexperimental results demonstrate that ContrastNER obtains competitiveperformance to the state-of-the-art NER methods in high-resource settings andoutperforms the state-of-the-art models in low-resource circumstances withoutrequiring extensive manual prompt engineering and verbalizer design."
Conformal Prediction with Large Language Models for Multi-Choice  Question Answering,"['Bhawesh Kumar', 'Charlie Lu', 'Gauri Gupta', 'Anil Palepu', 'David Bellamy', 'Ramesh Raskar', 'Andrew Beam']",http://arxiv.org/pdf/2305.18404v3.pdf,2023-05-28,"['cs.cl', 'cs.lg', 'stat.ml']","  As large language models continue to be widely developed, robust uncertaintyquantification techniques will become crucial for their safe deployment inhigh-stakes scenarios. In this work, we explore how conformal prediction can beused to provide uncertainty quantification in language models for the specifictask of multiple-choice question-answering. We find that the uncertaintyestimates from conformal prediction are tightly correlated with predictionaccuracy. This observation can be useful for downstream applications such asselective classification and filtering out low-quality predictions. We alsoinvestigate the exchangeability assumption required by conformal prediction toout-of-subject questions, which may be a more realistic scenario for manypractical applications. Our work contributes towards more trustworthy andreliable usage of large language models in safety-critical situations, whererobust guarantees of error rate are required."
Test-Time Training on Nearest Neighbors for Large Language Models,"['Moritz Hardt', 'Yu Sun']",http://arxiv.org/pdf/2305.18466v2.pdf,2023-05-29,"['cs.cl', 'cs.lg']","  Many recent efforts aim to augment language models with relevant informationretrieved from a database at test time. We avoid the need for promptengineering by directly fine-tuning the model on data retrieved at test timeusing its standard training setup. For this purpose, we build a large-scaledistributed nearest neighbor index based on text embeddings of the Piledataset. Given a query to a language model, our system retrieves the neighborsof the query and fine-tunes the model on the text data corresponding to thoseneighbors. Surprisingly, retrieving and training on as few as 20 neighbors,each for only one gradient iteration, drastically improves performance acrossmore than twenty language modeling tasks in the Pile benchmark. For example,test-time training significantly narrows the performance gap between a smallGPT2 model and a GPTNeo model, more than ten times larger, that wasspecifically trained to convergence on the Pile. Sufficient index quality andsize, however, are important. Our work establishes a valuable first baselinefor implementing test-time training in the context of large language models,opening the door to numerous promising research avenues."
CONA: A novel CONtext-Aware instruction paradigm for communication using  large language model,"['Nan Zhou', 'Xinghui Tao', 'Xi Chen']",http://arxiv.org/pdf/2305.18620v1.pdf,2023-05-26,"['cs.cl', 'cs.ai', 'cs.hc']","  We introduce CONA, a novel context-aware instruction paradigm for effectiveknowledge dissemination using generative pre-trained transformer (GPT) models.CONA is a flexible framework designed to leverage the capabilities of LargeLanguage Models (LLMs) and incorporate DIKW (Data, Information, Knowledge,Wisdom) hierarchy to automatically instruct and optimise presentation content,anticipate potential audience inquiries, and provide context-aware answers thatadaptive to the knowledge level of the audience group. The unique aspect of theCONA paradigm lies in its combination of an independent advisory mechanism anda recursive feedback loop rooted on the DIKW hierarchy. This synergysignificantly enhances context-aware contents, ensuring they are accessible andeasily comprehended by the audience. This paradigm is an early pioneer toexplore new methods for knowledge dissemination and communication in the LLMera, offering effective support for everyday knowledge sharing scenarios. Weconduct experiments on a range of audience roles, along with materials fromvarious disciplines using GPT4. Both quantitative and qualitative resultsdemonstrated that the proposed CONA paradigm achieved remarkable performancecompared to the outputs guided by conventional prompt engineering."
GPT4Tools: Teaching Large Language Model to Use Tools via  Self-instruction,"['Rui Yang', 'Lin Song', 'Yanwei Li', 'Sijie Zhao', 'Yixiao Ge', 'Xiu Li', 'Ying Shan']",http://arxiv.org/pdf/2305.18752v1.pdf,2023-05-30,"['cs.cv', 'cs.cl']","  This paper aims to efficiently enable Large Language Models (LLMs) to usemultimodal tools. Advanced proprietary LLMs, such as ChatGPT and GPT-4, haveshown great potential for tool usage through sophisticated prompt engineering.Nevertheless, these models typically rely on prohibitive computational costsand publicly inaccessible data. To address these challenges, we propose theGPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA andOPT, to use tools. It generates an instruction-following dataset by promptingan advanced teacher with various multi-modal contexts. By using the Low-RankAdaptation (LoRA) optimization, our approach facilitates the open-source LLMsto solve a range of visual problems, including visual comprehension and imagegeneration. Moreover, we provide a benchmark to evaluate the ability of LLMs touse tools, which is performed in both zero-shot and fine-tuning ways. Extensiveexperiments demonstrate the effectiveness of our method on various languagemodels, which not only significantly improves the accuracy of invoking seentools, but also enables the zero-shot capacity for unseen tools. The code anddemo are available at https://github.com/StevenGrove/GPT4Tools."
Contextualizing Problems to Student Interests at Scale in Intelligent  Tutoring System Using Large Language Models,"['Gautam Yadav', 'Ying-Jui Tseng', 'Xiaolin Ni']",http://arxiv.org/pdf/2306.00190v1.pdf,2023-05-31,['cs.hc'],"  Contextualizing problems to align with student interests can significantlyimprove learning outcomes. However, this task often presents scalabilitychallenges due to resource and time constraints. Recent advancements in LargeLanguage Models (LLMs) like GPT-4 offer potential solutions to these issues.This study explores the ability of GPT-4 in the contextualization of problemswithin CTAT, an intelligent tutoring system, aiming to increase studentengagement and enhance learning outcomes. Through iterative prompt engineering,we achieved meaningful contextualization that preserved the difficulty andoriginal intent of the problem, thereby not altering values or overcomplicatingthe questions. While our research highlights the potential of LLMs ineducational settings, we acknowledge current limitations, particularly withgeometry problems, and emphasize the need for ongoing evaluation and research.Future work includes systematic studies to measure the impact of this tool onstudents' learning outcomes and enhancements to handle a broader range ofproblems."
Exploring EFL students' prompt engineering in human-AI story writing: an  Activity Theory perspective,"['David James Woo', 'Kai Guo', 'Hengky Susanto']",http://arxiv.org/pdf/2306.01798v1.pdf,2023-06-01,"['cs.cy', 'cs.ai']","  This study applies Activity Theory to investigate how English as a foreignlanguage (EFL) students prompt generative artificial intelligence (AI) toolsduring short story writing. Sixty-seven Hong Kong secondary school studentscreated generative-AI tools using open-source language models and wrote shortstories with them. The study collected and analyzed the students' generative-AItools, short stories, and written reflections on their conditions or purposesfor prompting. The research identified three main themes regarding the purposesfor which students prompt generative-AI tools during short story writing: alack of awareness of purposes, overcoming writer's block, and developing,expanding, and improving the story. The study also identified commoncharacteristics of students' activity systems, including the sophistication oftheir generative-AI tools, the quality of their stories, and their school'soverall academic achievement level, for their prompting of generative-AI toolsfor the three purposes during short story writing. The study's findings suggestthat teachers should be aware of students' purposes for prompting generative-AItools to provide tailored instructions and scaffolded guidance. The findingsmay also help designers provide differentiated instructions for users atvarious levels of story development when using a generative-AI tool."
Prompting Is All You Need: Automated Android Bug Replay with Large  Language Models,"['Sidong Feng', 'Chunyang Chen']",http://arxiv.org/pdf/2306.01987v2.pdf,2023-06-03,['cs.se'],"  Bug reports are vital for software maintenance that allow users to informdevelopers of the problems encountered while using the software. As such,researchers have committed considerable resources toward automating bug replayto expedite the process of software maintenance. Nonetheless, the success ofcurrent automated approaches is largely dictated by the characteristics andquality of bug reports, as they are constrained by the limitations ofmanually-crafted patterns and pre-defined vocabulary lists. Inspired by thesuccess of Large Language Models (LLMs) in natural language understanding, wepropose AdbGPT, a new lightweight approach to automatically reproduce the bugsfrom bug reports through prompt engineering, without any training andhard-coding effort. AdbGPT leverages few-shot learning and chain-of-thoughtreasoning to elicit human knowledge and logical reasoning from LLMs toaccomplish the bug replay in a manner similar to a developer. Our evaluationsdemonstrate the effectiveness and efficiency of our AdbGPT to reproduce 81.3%of bug reports in 253.6 seconds, outperforming the state-of-the-art baselinesand ablation studies. We also conduct a small-scale user study to confirm theusefulness of AdbGPT in enhancing developers' bug replay capabilities."
ChatGPT as a mapping assistant: A novel method to enrich maps with  generative AI and content derived from street-level photographs,"['Levente Juhász', 'Peter Mooney', 'Hartwig H. Hochmair', 'Boyuan Guan']",http://arxiv.org/pdf/2306.03204v1.pdf,2023-06-05,"['cs.cy', 'cs.cv']","  This paper explores the concept of leveraging generative AI as a mappingassistant for enhancing the efficiency of collaborative mapping. We presentresults of an experiment that combines multiple sources of volunteeredgeographic information (VGI) and large language models (LLMs). Three analystsdescribed the content of crowdsourced Mapillary street-level photographs takenalong roads in a small test area in Miami, Florida. GPT-3.5-turbo wasinstructed to suggest the most appropriate tagging for each road inOpenStreetMap (OSM). The study also explores the utilization of BLIP-2, astate-of-the-art multimodal pre-training method as an artificial analyst ofstreet-level photographs in addition to human analysts. Results demonstrate twoways to effectively increase the accuracy of mapping suggestions withoutmodifying the underlying AI models: by (1) providing a more detaileddescription of source photographs, and (2) combining prompt engineering withadditional context (e.g. location and objects detected along a road). The firstapproach increases the suggestion accuracy by up to 29%, and the second one byup to 20%."
An Approach to Solving the Abstraction and Reasoning Corpus (ARC)  Challenge,['Tan John Chong Min'],http://arxiv.org/pdf/2306.03553v1.pdf,2023-06-06,['cs.ai'],"  We utilise the power of Large Language Models (LLMs), in particular GPT4, tobe prompt engineered into performing an arbitrary task. Here, we give the modelsome human priors via text, along with some typical procedures for solving theARC tasks, and ask it to generate the i) broad description of the input-outputrelation, ii) detailed steps of the input-output mapping, iii) use the detailedsteps to perform manipulation on the test input and derive the test output. Thecurrent GPT3.5/GPT4 prompt solves 2 out of 4 tested small ARC challenges (thosewith small grids of 8x8 and below). With tweaks to the prompt to make it morespecific for the use case, it can solve more. We posit that when scaled to amulti-agent system with usage of past memory and equipped with an imageinterpretation tool via Visual Question Answering, we may actually be able tosolve the majority of the ARC challenge"
Protect Your Prompts: Protocols for IP Protection in LLM Applications,"['M. A. van Wyk', 'M. Bekker', 'X. L. Richards', 'K. J. Nixon']",http://arxiv.org/pdf/2306.06297v1.pdf,2023-06-09,"['cs.cl', 'cs.ai', '91d10, 68t10, 03d40', 'i.2.6; k.6.5; f.3.2']","  With the rapid adoption of AI in the form of large language models (LLMs),the potential value of carefully engineered prompts has become significant.However, to realize this potential, prompts should be tradable on an openmarket. Since prompts are, at present, generally economically non-excludable,by virtue of their nature as text, no general competitive market has yet beenestablished. This note discusses two protocols intended to provide protectionof prompts, elevating their status as intellectual property, thus confirmingthe intellectual property rights of prompt engineers, and potentiallysupporting the flourishing of an open market for LLM prompts."
Scalable 3D Captioning with Pretrained Models,"['Tiange Luo', 'Chris Rockwell', 'Honglak Lee', 'Justin Johnson']",http://arxiv.org/pdf/2306.07279v2.pdf,2023-06-12,['cs.cv'],"  We introduce Cap3D, an automatic approach for generating descriptive text for3D objects. This approach utilizes pretrained models from image captioning,image-text alignment, and LLM to consolidate captions from multiple views of a3D asset, completely side-stepping the time-consuming and costly process ofmanual annotation. We apply Cap3D to the recently introduced large-scale 3Ddataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conductedusing 41k human annotations from the same dataset, demonstrates that Cap3Dsurpasses human-authored descriptions in terms of quality, cost, and speed.Through effective prompt engineering, Cap3D rivals human performance ingenerating geometric descriptions on 17k collected annotations from the ABOdataset. Finally, we finetune Text-to-3D models on Cap3D and human captions,and show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E,and DreamFusion."
FALL-E: A Foley Sound Synthesis Model and Strategies,"['Minsung Kang', 'Sangshin Oh', 'Hyeongi Moon', 'Kyungyun Lee', 'Ben Sangbae Chon']",http://arxiv.org/pdf/2306.09807v2.pdf,2023-06-16,"['eess.as', 'cs.lg', 'cs.sd']","  This paper introduces FALL-E, a foley synthesis system and itstraining/inference strategies. The FALL-E model employs a cascaded approachcomprising low-resolution spectrogram generation, spectrogram super-resolution,and a vocoder. We trained every sound-related model from scratch using ourextensive datasets, and utilized a pre-trained language model. We conditionedthe model with dataset-specific texts, enabling it to learn sound quality andrecording environment based on text input. Moreover, we leveraged externallanguage models to improve text descriptions of our datasets and performedprompt engineering for quality, coherence, and diversity. FALL-E was evaluatedby an objective measure as well as listening tests in the DCASE 2023 challengeTask 7. The submission achieved the second place on average, while achievingthe best score for diversity, second place for audio quality, and third placefor class fitness."
The Cultivated Practices of Text-to-Image Generation,['Jonas Oppenlaender'],http://arxiv.org/pdf/2306.11393v1.pdf,2023-06-20,"['cs.cy', 'cs.ai', 'k.4; j.5; i.2.0; k.5.m']","  Humankind is entering a novel creative era in which anybody can synthesizedigital information using generative artificial intelligence (AI).Text-to-image generation, in particular, has become vastly popular and millionsof practitioners produce AI-generated images and AI art online. This chapterfirst gives an overview of the key developments that enabled a healthyco-creative online ecosystem around text-to-image generation to rapidly emerge,followed by a high-level description of key elements in this ecosystem. Aparticular focus is placed on prompt engineering, a creative practice that hasbeen embraced by the AI art community. It is then argued that the emergingco-creative ecosystem constitutes an intelligent system on its own - a systemthat both supports human creativity, but also potentially entraps futuregenerations and limits future development efforts in AI. The chapter discussesthe potential risks and dangers of cultivating this co-creative ecosystem, suchas the bias inherent in today's training data, potential quality degradation infuture image generation systems due to synthetic data becoming common place,and the potential long-term effects of text-to-image generation on people'simagination, ambitions, and development."
Solving and Generating NPR Sunday Puzzles with Large Language Models,"['Jingmiao Zhao', 'Carolyn Jane Anderson']",http://arxiv.org/pdf/2306.12255v1.pdf,2023-06-21,['cs.cl'],"  We explore the ability of large language models to solve and generate puzzlesfrom the NPR Sunday Puzzle game show using PUZZLEQA, a dataset comprising 15years of on-air puzzles. We evaluate four large language models using PUZZLEQA,in both multiple choice and free response formats, and explore two promptengineering techniques to improve free response performance: chain-of-thoughtreasoning and prompt summarization. We find that state-of-the-art largelanguage models can solve many PUZZLEQA puzzles: the best model, GPT-3.5,achieves 50.2% loose accuracy. However, in our few-shot puzzle generationexperiment, we find no evidence that models can generate puzzles: GPT-3.5generates puzzles with answers that do not conform to the generated rules.Puzzle generation remains a challenging task for future work."
Federated Large Language Model: A Position Paper,"['Chaochao Chen', 'Xiaohua Feng', 'Jun Zhou', 'Jianwei Yin', 'Xiaolin Zheng']",http://arxiv.org/pdf/2307.08925v1.pdf,2023-07-18,"['cs.lg', 'cs.ai', 'cs.cl']","  Large scale language models (LLM) have received significant attention andfound diverse applications across various domains, but their developmentencounters challenges in real-world scenarios. These challenges arise due tothe scarcity of public domain data availability and the need to maintainprivacy with respect to private domain data. To address these issues, federatedlearning (FL) has emerged as a promising technology that enables collaborativetraining of shared models while preserving decentralized data. We propose theconcept of federated LLM, which comprises three key components, i.e., federatedLLM pre-training, federated LLM fine-tuning, and federated LLM promptengineering. For each component, we discuss its advantage over traditional LLMtraining methods and propose specific engineering strategies forimplementation. Furthermore, we explore the novel challenges introduced by theintegration of FL and LLM. We analyze existing solutions and identify potentialobstacles faced by these solutions within the context of federated LLM."
Chit-Chat or Deep Talk: Prompt Engineering for Process Mining,"['Urszula Jessen', 'Michal Sroka', 'Dirk Fahland']",http://arxiv.org/pdf/2307.09909v1.pdf,2023-07-19,['cs.ai'],"  This research investigates the application of Large Language Models (LLMs) toaugment conversational agents in process mining, aiming to tackle its inherentcomplexity and diverse skill requirements. While LLM advancements present novelopportunities for conversational process mining, generating efficient outputsis still a hurdle. We propose an innovative approach that amend many issues inexisting solutions, informed by prior research on Natural Language Processing(NLP) for conversational agents. Leveraging LLMs, our framework improves bothaccessibility and agent performance, as demonstrated by experiments on publicquestion and data sets. Our research sets the stage for future explorationsinto LLMs' role in process mining and concludes with propositions for enhancingLLM memory, implementing real-time user testing, and examining diverse datasets."
Large Language Models can accomplish Business Process Management Tasks,"['Michael Grohs', 'Luka Abb', 'Nourhan Elsayed', 'Jana-Rebecca Rehse']",http://arxiv.org/pdf/2307.09923v1.pdf,2023-07-19,['cs.cl'],"  Business Process Management (BPM) aims to improve organizational activitiesand their outcomes by managing the underlying processes. To achieve this, it isoften necessary to consider information from various sources, includingunstructured textual documents. Therefore, researchers have developed severalBPM-specific solutions that extract information from textual documents usingNatural Language Processing techniques. These solutions are specific to theirrespective tasks and cannot accomplish multiple process-related problems as ageneral-purpose instrument. However, in light of the recent emergence of LargeLanguage Models (LLMs) with remarkable reasoning capabilities, such ageneral-purpose instrument with multiple applications now appears attainable.In this paper, we illustrate how LLMs can accomplish text-related BPM tasks byapplying a specific LLM to three exemplary tasks: mining imperative processmodels from textual descriptions, mining declarative process models fromtextual descriptions, and assessing the suitability of process tasks fromtextual descriptions for robotic process automation. We show that, withoutextensive configuration or prompt engineering, LLMs perform comparably to orbetter than existing solutions and discuss implications for future BPM researchas well as practical usage."
SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its  Departure from Current Machine Learning,"['Kiana Kheiri', 'Hamid Karimi']",http://arxiv.org/pdf/2307.10234v2.pdf,2023-07-16,"['cs.cl', 'cs.ai', 'cs.lg', 'cs.si']","  This study presents a thorough examination of various Generative PretrainedTransformer (GPT) methodologies in sentiment analysis, specifically in thecontext of Task 4 on the SemEval 2017 dataset. Three primary strategies areemployed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2)fine-tuning GPT models, and 3) an inventive approach to embeddingclassification. The research yields detailed comparative insights among thesestrategies and individual GPT models, revealing their unique strengths andpotential limitations. Additionally, the study compares these GPT-basedmethodologies with other current, high-performing models previously used withthe same dataset. The results illustrate the significant superiority of the GPTapproaches in terms of predictive performance, more than 22\% in F1-scorecompared to the state-of-the-art. Further, the paper sheds light on commonchallenges in sentiment analysis tasks, such as understanding context anddetecting sarcasm. It underscores the enhanced capabilities of the GPT modelsto effectively handle these complexities. Taken together, these findingshighlight the promising potential of GPT models in sentiment analysis, settingthe stage for future research in this field. The code can be found athttps://github.com/DSAatUSU/SentimentGPT"
Domain Knowledge Distillation from Large Language Model: An Empirical  Study in the Autonomous Driving Domain,"['Yun Tang', 'Antonio A. Bruto da Costa', 'Jason Zhang', 'Irvine Patrick', 'Siddartha Khastgir', 'Paul Jennings']",http://arxiv.org/pdf/2307.11769v1.pdf,2023-07-17,['cs.cl'],"  Engineering knowledge-based (or expert) systems require extensive manualeffort and domain knowledge. As Large Language Models (LLMs) are trained usingan enormous amount of cross-domain knowledge, it becomes possible to automatesuch engineering processes. This paper presents an empirical automation andsemi-automation framework for domain knowledge distillation using promptengineering and the LLM ChatGPT. We assess the framework empirically in theautonomous driving domain and present our key observations. In ourimplementation, we construct the domain knowledge ontology by ""chatting"" withChatGPT. The key finding is that while fully automated domain ontologyconstruction is possible, human supervision and early intervention typicallyimprove efficiency and output quality as they lessen the effects of responserandomness and the butterfly effect. We, therefore, also develop a web-baseddistillation assistant enabling supervision and flexible intervention atruntime. We hope our findings and tools could inspire future research towardrevolutionizing the engineering of knowledge-based systems across applicationdomains."
Copilot for Xcode: Exploring AI-Assisted Programming by Prompting  Cloud-based Large Language Models,"['Chee Wei Tan', 'Shangxin Guo', 'Man Fai Wong', 'Ching Nam Hang']",http://arxiv.org/pdf/2307.14349v1.pdf,2023-07-08,"['cs.se', 'cs.ai']","  This paper presents an AI-assisted programming tool called Copilot for Xcodefor program composition and design to support human software developers. Byseamlessly integrating cloud-based Large Language Models (LLM) with Apple'slocal development environment, Xcode, this tool enhances productivity andunleashes creativity for software development in Apple software ecosystem(e.g., iOS apps, macOS). Leveraging advanced natural language processing (NLP)techniques, Copilot for Xcode effectively processes source code tokens andpatterns within code repositories, enabling features such as code generation,autocompletion, documentation, and error detection. Software developers canalso query and make ""small"" decisions for program composition, some of whichcan be made simultaneously, and this is facilitated through prompt engineeringin a chat interface of Copilot for Xcode. Finally, we present simple casestudies as evidence of the effectiveness of utilizing NLP in Xcode to promptpopular LLM services like OpenAI ChatGPT for program composition and design."
Backdoor Attacks for In-Context Learning with Language Models,"['Nikhil Kandpal', 'Matthew Jagielski', 'Florian Tramèr', 'Nicholas Carlini']",http://arxiv.org/pdf/2307.14692v1.pdf,2023-07-27,['cs.cr'],"  Because state-of-the-art language models are expensive to train, mostpractitioners must make use of one of the few publicly available languagemodels or language model APIs. This consolidation of trust increases thepotency of backdoor attacks, where an adversary tampers with a machine learningmodel in order to make it perform some malicious behavior on inputs thatcontain a predefined backdoor trigger. We show that the in-context learningability of large language models significantly complicates the question ofdeveloping backdoor attacks, as a successful backdoor must work against variousprompting strategies and should not affect the model's general purposecapabilities. We design a new attack for eliciting targeted misclassificationwhen language models are prompted to perform a particular target task anddemonstrate the feasibility of this attack by backdooring multiple largelanguage models ranging in size from 1.3 billion to 6 billion parameters.Finally we study defenses to mitigate the potential harms of our attack: forexample, while in the white-box setting we show that fine-tuning models for asfew as 500 steps suffices to remove the backdoor behavior, in the black-boxsetting we are unable to develop a successful defense that relies on promptengineering alone."
Do LLMs Possess a Personality? Making the MBTI Test an Amazing  Evaluation for Large Language Models,"['Keyu Pan', 'Yawen Zeng']",http://arxiv.org/pdf/2307.16180v1.pdf,2023-07-30,['cs.cl'],"  The field of large language models (LLMs) has made significant progress, andtheir knowledge storage capacity is approaching that of human beings.Furthermore, advanced techniques, such as prompt learning and reinforcementlearning, are being employed to address ethical concerns and hallucinationproblems associated with LLMs, bringing them closer to aligning with humanvalues. This situation naturally raises the question of whether LLMs withhuman-like abilities possess a human-like personality? In this paper, we aim toinvestigate the feasibility of using the Myers-Briggs Type Indicator (MBTI), awidespread human personality assessment tool, as an evaluation metric for LLMs.Specifically, extensive experiments will be conducted to explore: 1) thepersonality types of different LLMs, 2) the possibility of changing thepersonality types by prompt engineering, and 3) How does the training datasetaffect the model's personality. Although the MBTI is not a rigorous assessment,it can still reflect the similarity between LLMs and human personality. Inpractice, the MBTI has the potential to serve as a rough indicator. Our codesare available athttps://github.com/HarderThenHarder/transformers_tasks/tree/main/LLM/llms_mbti."
Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment,"['Saizhuo Wang', 'Hang Yuan', 'Leon Zhou', 'Lionel M. Ni', 'Heung-Yeung Shum', 'Jian Guo']",http://arxiv.org/pdf/2308.00016v1.pdf,2023-07-31,"['q-fin.cp', 'cs.ai', 'cs.cl']","  One of the most important tasks in quantitative investment research is miningnew alphas (effective trading signals or factors). Traditional alpha miningmethods, either hand-crafted factor synthesizing or algorithmic factor mining(e.g., search with genetic programming), have inherent limitations, especiallyin implementing the ideas of quants. In this work, we propose a new alphamining paradigm by introducing human-AI interaction, and a novel promptengineering algorithmic framework to implement this paradigm by leveraging thepower of large language models. Moreover, we develop Alpha-GPT, a newinteractive alpha mining system framework that provides a heuristic way to``understand'' the ideas of quant researchers and outputs creative, insightful,and effective alphas. We demonstrate the effectiveness and advantage ofAlpha-GPT via a number of alpha mining experiments."
Optimizing Machine Translation through Prompt Engineering: An  Investigation into ChatGPT's Customizability,['Masaru Yamada'],http://arxiv.org/pdf/2308.01391v1.pdf,2023-08-02,['cs.cl'],"  This paper explores the influence of integrating the purpose of thetranslation and the target audience into prompts on the quality of translationsproduced by ChatGPT. Drawing on previous translation studies, industrypractices, and ISO standards, the research underscores the significance of thepre-production phase in the translation process. The study reveals that theinclusion of suitable prompts in large-scale language models like ChatGPT canyield flexible translations, a feat yet to be realized by conventional MachineTranslation (MT). The research scrutinizes the changes in translation qualitywhen prompts are used to generate translations that meet specific conditions.The evaluation is conducted from a practicing translator's viewpoint, bothsubjectively and qualitatively, supplemented by the use of OpenAI's wordembedding API for cosine similarity calculations. The findings suggest that theintegration of the purpose and target audience into prompts can indeed modifythe generated translations, generally enhancing the translation quality byindustry standards. The study also demonstrates the practical application ofthe ""good translation"" concept, particularly in the context of marketingdocuments and culturally dependent idioms."
InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent,"['Po-Lin Chen', 'Cheng-Shang Chang']",http://arxiv.org/pdf/2308.01552v1.pdf,2023-08-03,"['cs.ai', 'cs.cl', 'cs.lg']","  This research paper delves into the integration of OpenAI's ChatGPT intoembodied agent systems, evaluating its influence on interactive decision-makingbenchmark. Drawing a parallel to the concept of people assuming roles accordingto their unique strengths, we introduce InterAct. In this approach, we feedChatGPT with varied prompts, assigning it a numerous roles like a checker and asorter, then integrating them with the original language model. Our researchshows a remarkable success rate of 98% in AlfWorld, which consists of 6different tasks in a simulated household environment, emphasizing thesignificance of proficient prompt engineering. The results highlight ChatGPT'scompetence in comprehending and performing intricate tasks effectively inreal-world settings, thus paving the way for further advancements in taskplanning."
RTLLM: An Open-Source Benchmark for Design RTL Generation with Large  Language Model,"['Yao Lu', 'Shang Liu', 'Qijun Zhang', 'Zhiyao Xie']",http://arxiv.org/pdf/2308.05345v3.pdf,2023-08-10,"['cs.lg', 'cs.ar']","  Inspired by the recent success of large language models (LLMs) like ChatGPT,researchers start to explore the adoption of LLMs for agile hardware design,such as generating design RTL based on natural-language instructions. However,in existing works, their target designs are all relatively simple and in asmall scale, and proposed by the authors themselves, making a fair comparisonamong different LLM solutions challenging. In addition, many prior works onlyfocus on the design correctness, without evaluating the design qualities ofgenerated design RTL. In this work, we propose an open-source benchmark namedRTLLM, for generating design RTL with natural language instructions. Tosystematically evaluate the auto-generated design RTL, we summarized threeprogressive goals, named syntax goal, functionality goal, and design qualitygoal. This benchmark can automatically provide a quantitative evaluation of anygiven LLM-based solution. Furthermore, we propose an easy-to-use yetsurprisingly effective prompt engineering technique named self-planning, whichproves to significantly boost the performance of GPT-3.5 in our proposedbenchmark."
"LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked","['Mansi Phute', 'Alec Helbling', 'Matthew Hull', 'ShengYun Peng', 'Sebastian Szyller', 'Cory Cornelius', 'Duen Horng Chau']",http://arxiv.org/pdf/2308.07308v3.pdf,2023-08-14,"['cs.cl', 'cs.ai']","  Large language models (LLMs) are popular for high-quality text generation butcan produce harmful content, even when aligned with human values throughreinforcement learning. Adversarial prompts can bypass their safety measures.We propose LLM Self Defense, a simple approach to defend against these attacksby having an LLM screen the induced responses. Our method does not require anyfine-tuning, input preprocessing, or iterative output generation. Instead, weincorporate the generated content into a pre-defined prompt and employ anotherinstance of an LLM to analyze the text and predict whether it is harmful. Wetest LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominentLLMs against various types of attacks, such as forcefully inducing affirmativeresponses to prompts and prompt engineering attacks. Notably, LLM Self Defensesucceeds in reducing the attack success rate to virtually 0 using both GPT 3.5and Llama 2."
Data Race Detection Using Large Language Models,"['Le Chen', 'Xianzhong Ding', 'Murali Emani', 'Tristan Vanderbruggen', 'Pei-hung Lin', 'Chuanhua Liao']",http://arxiv.org/pdf/2308.07505v2.pdf,2023-08-15,"['cs.lg', 'cs.cl']","  Large language models (LLMs) are demonstrating significant promise as analternate strategy to facilitate analyses and optimizations of high-performancecomputing programs, circumventing the need for resource-intensive manual toolcreation. In this paper, we explore a novel LLM-based data race detectionapproach combining prompting engineering and fine-tuning techniques. We createa dedicated dataset named DRB-ML, which is derived from DataRaceBench, withfine-grain labels showing the presence of data race pairs and their associatedvariables, line numbers, and read/write information. DRB-ML is then used toevaluate representative LLMs and fine-tune open-source ones. Our experimentshows that LLMs can be a viable approach to data race detection. However, theystill cannot compete with traditional data race detection tools when we needdetailed information about variable pairs causing data races."
Accelerated materials language processing enabled by GPT,"['Jaewoong Choi', 'Byungju Lee']",http://arxiv.org/pdf/2308.09354v1.pdf,2023-08-18,"['cs.cl', 'cond-mat.mtrl-sci']","  Materials language processing (MLP) is one of the key facilitators ofmaterials science research, as it enables the extraction of structuredinformation from massive materials science literature. Prior works suggestedhigh-performance MLP models for text classification, named entity recognition(NER), and extractive question answering (QA), which require complex modelarchitecture, exhaustive fine-tuning and a large number of human-labelleddatasets. In this study, we develop generative pretrained transformer(GPT)-enabled pipelines where the complex architectures of prior MLP models arereplaced with strategic designs of prompt engineering. First, we develop aGPT-enabled document classification method for screening relevant documents,achieving comparable accuracy and reliability compared to prior models, withonly small dataset. Secondly, for NER task, we design an entity-centricprompts, and learning few-shot of them improved the performance on most ofentities in three open datasets. Finally, we develop an GPT-enabled extractiveQA model, which provides improved performance and shows the possibility ofautomatically correcting annotations. While our findings confirm the potentialof GPT-enabled MLP models as well as their value in terms of reliability andpracticability, our scientific methods and systematic approach are applicableto any materials science domain to accelerate the information extraction ofscientific literature."
Data-to-text Generation for Severely Under-Resourced Languages with  GPT-3.5: A Bit of Help Needed from Google Translate,"['Michela Lorandi', 'Anya Belz']",http://arxiv.org/pdf/2308.09957v1.pdf,2023-08-19,"['cs.cl', 'cs.ai']","  LLMs like GPT are great at tasks involving English which dominates in theirtraining data. In this paper, we look at how they cope with tasks involvinglanguages that are severely under-represented in their training data, in thecontext of data-to-text generation for Irish, Maltese, Welsh and Breton. Duringthe prompt-engineering phase we tested a range of prompt types and formats onGPT-3.5 and~4 with a small sample of example input/output pairs. We then fullyevaluated the two most promising prompts in two scenarios: (i) directgeneration into the under-resourced language, and (ii) generation into Englishfollowed by translation into the under-resourced language. We find thatfew-shot prompting works better for direct generation into under-resourcedlanguages, but that the difference disappears when pivoting via English. Thefew-shot + translation system variants were submitted to the WebNLG 2023 sharedtask where they outperformed competitor systems by substantial margins in alllanguages on all metrics. We conclude that good performance on under-resourcedlanguages can be achieved out-of-the box with state-of-the-art LLMs. However,our best results (for Welsh) remain well below the lowest ranked English systemat WebNLG'20."
Activation Addition: Steering Language Models Without Optimization,"['Alexander Matt Turner', 'Lisa Thiergart', 'David Udell', 'Gavin Leech', 'Ulisse Mini', 'Monte MacDiarmid']",http://arxiv.org/pdf/2308.10248v3.pdf,2023-08-20,"['cs.cl', 'cs.lg']","  Reliably controlling the behavior of large language models is a pressing openproblem. Existing methods include supervised finetuning, reinforcement learningfrom human feedback, prompt engineering and guided decoding. We insteadinvestigate activation engineering: modifying activations at inference-time topredictably alter model behavior. We bias the forward pass with a 'steeringvector' implicitly specified through natural language. Past work learned thesesteering vectors; our Activation Addition (ActAdd) method instead computes themby taking the activation differences which result from pairs of prompts.  We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet, and replicatethe effect on Llama-13B and GPT-J-6B. Our approach yields inference-timecontrol over high-level properties of output & preserves performance onoff-target topics. The method requires far less compute and implementationeffort than finetuning and RLHF, allows for natural language specification byusers, and its overhead scales naturally with model size."
Situated Natural Language Explanations,"['Zining Zhu', 'Haoming Jiang', 'Jingfeng Yang', 'Sreyashi Nag', 'Chao Zhang', 'Jie Huang', 'Yifan Gao', 'Frank Rudzicz', 'Bing Yin']",http://arxiv.org/pdf/2308.14115v1.pdf,2023-08-27,['cs.cl'],"  Natural language is among the most accessible tools for explaining decisionsto humans, and large pretrained language models (PLMs) have demonstratedimpressive abilities to generate coherent natural language explanations (NLE).The existing NLE research perspectives do not take the audience into account.An NLE can have high textual quality, but it might not accommodate audiences'needs and preference. To address this limitation, we propose an alternativeperspective, situated NLE, including a situated generation framework and asituated evaluation framework. On the generation side, we propose simple promptengineering methods that adapt the NLEs to situations. In human studies, theannotators preferred the situated NLEs. On the evaluation side, we set upautomated evaluation scores in lexical, semantic, and pragmatic categories. Thescores can be used to select the most suitable prompts to generate NLEs.Situated NLE provides a perspective to conduct further research on automaticNLE generations."
"FurChat: An Embodied Conversational Agent using LLMs, Combining Open and  Closed-Domain Dialogue with Facial Expressions","['Neeraj Cherakara', 'Finny Varghese', 'Sheena Shabana', 'Nivan Nelson', 'Abhiram Karukayil', 'Rohith Kulothungan', 'Mohammed Afil Farhan', 'Birthe Nesset', 'Meriam Moujahid', 'Tanvi Dinkar', 'Verena Rieser', 'Oliver Lemon']",http://arxiv.org/pdf/2308.15214v2.pdf,2023-08-29,"['cs.cl', 'cs.ai', 'cs.hc', 'cs.ro']","  We demonstrate an embodied conversational agent that can function as areceptionist and generate a mixture of open and closed-domain dialogue alongwith facial expressions, by using a large language model (LLM) to develop anengaging conversation. We deployed the system onto a Furhat robot, which ishighly expressive and capable of using both verbal and nonverbal cues duringinteraction. The system was designed specifically for the National Robotariumto interact with visitors through natural conversations, providing them withinformation about the facilities, research, news, upcoming events, etc. Thesystem utilises the state-of-the-art GPT-3.5 model to generate such informationalong with domain-general conversations and facial expressions based on promptengineering."
Can Prompt Learning Benefit Radiology Report Generation?,"['Jun Wang', 'Lixing Zhu', 'Abhir Bhalerao', 'Yulan He']",http://arxiv.org/pdf/2308.16269v1.pdf,2023-08-30,['cs.cv'],"  Radiology report generation aims to automatically provide clinicallymeaningful descriptions of radiology images such as MRI and X-ray. Althoughgreat success has been achieved in natural scene image captioning tasks,radiology report generation remains challenging and requires prior medicalknowledge. In this paper, we propose PromptRRG, a method that utilizes promptlearning to activate a pretrained model and incorporate prior knowledge. Sinceprompt learning for radiology report generation has not been explored before,we begin with investigating prompt designs and categorise them based on varyinglevels of knowledge: common, domain-specific and disease-enriched prompts.Additionally, we propose an automatic prompt learning mechanism to alleviatethe burden of manual prompt engineering. This is the first work tosystematically examine the effectiveness of prompt learning for radiologyreport generation. Experimental results on the largest radiology reportgeneration benchmark, MIMIC-CXR, demonstrate that our proposed method achievesstate-of-the-art performance. Code will be available upon the acceptance."
Large Language Models as Data Preprocessors,"['Haochen Zhang', 'Yuyang Dong', 'Chuan Xiao', 'Masafumi Oyamada']",http://arxiv.org/pdf/2308.16361v1.pdf,2023-08-30,"['cs.ai', 'cs.db']","  Large Language Models (LLMs), typified by OpenAI's GPT series and Meta'sLLaMA variants, have marked a significant advancement in artificialintelligence. Trained on vast amounts of text data, LLMs are capable ofunderstanding and generating human-like text across a diverse range of topics.This study expands on the applications of LLMs, exploring their potential indata preprocessing, a critical stage in data mining and analytics applications.We delve into the applicability of state-of-the-art LLMs such as GPT-3.5,GPT-4, and Vicuna-13B for error detection, data imputation, schema matching,and entity matching tasks. Alongside showcasing the inherent capabilities ofLLMs, we highlight their limitations, particularly in terms of computationalexpense and inefficiency. We propose an LLM-based framework for datapreprocessing, which integrates cutting-edge prompt engineering techniques,coupled with traditional methods like contextualization and feature selection,to improve the performance and efficiency of these models. The effectiveness ofLLMs in data preprocessing is evaluated through an experimental study spanning12 datasets. GPT-4 emerged as a standout, achieving 100\% accuracy or F1 scoreon 4 datasets, suggesting LLMs' immense potential in these tasks. Despitecertain limitations, our study underscores the promise of LLMs in this domainand anticipates future developments to overcome current hurdles."
Developing a Scalable Benchmark for Assessing Large Language Models in  Knowledge Graph Engineering,"['Lars-Peter Meyer', 'Johannes Frey', 'Kurt Junghanns', 'Felix Brei', 'Kirill Bulert', 'Sabine Gründer-Fahrer', 'Michael Martin']",http://arxiv.org/pdf/2308.16622v1.pdf,2023-08-31,"['cs.ai', 'cs.cl', 'cs.db']","  As the field of Large Language Models (LLMs) evolves at an accelerated pace,the critical need to assess and monitor their performance emerges. We introducea benchmarking framework focused on knowledge graph engineering (KGE)accompanied by three challenges addressing syntax and error correction, factsextraction and dataset generation. We show that while being a useful tool, LLMsare yet unfit to assist in knowledge graph generation with zero-shot prompting.Consequently, our LLM-KG-Bench framework provides automatic evaluation andstorage of LLM responses as well as statistical data and visualization tools tosupport tracking of prompt engineering and model performance."
Linking microblogging sentiments to stock price movement: An application  of GPT-4,"['Rick Steinert', 'Saskia Altmann']",http://arxiv.org/pdf/2308.16771v1.pdf,2023-08-31,"['q-fin.st', 'q-fin.cp']","  This paper investigates the potential improvement of the GPT-4 LanguageLearning Model (LLM) in comparison to BERT for modeling same-day daily stockprice movements of Apple and Tesla in 2017, based on sentiment analysis ofmicroblogging messages. We recorded daily adjusted closing prices andtranslated them into up-down movements. Sentiment for each day was extractedfrom messages on the Stocktwits platform using both LLMs. We develop a novelmethod to engineer a comprehensive prompt for contextual sentiment analysiswhich unlocks the true capabilities of modern LLM. This enables us to carefullyretrieve sentiments, perceived advantages or disadvantages, and the relevancetowards the analyzed company. Logistic regression is used to evaluate whetherthe extracted message contents reflect stock price movements. As a result,GPT-4 exhibited substantial accuracy, outperforming BERT in five out of sixmonths and substantially exceeding a naive buy-and-hold strategy, reaching apeak accuracy of 71.47 % in May. The study also highlights the importance ofprompt engineering in obtaining desired outputs from GPT-4's contextualabilities. However, the costs of deploying GPT-4 and the need for fine-tuningprompts highlight some practical considerations for its use."
LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for  Vision-Language Models,"['Cheng Shi', 'Sibei Yang']",http://arxiv.org/pdf/2309.01155v2.pdf,2023-09-03,['cs.cv'],"  Prompt engineering is a powerful tool used to enhance the performance ofpre-trained models on downstream tasks. For example, providing the prompt""Let's think step by step"" improved GPT-3's reasoning accuracy to 63% onMutiArith while prompting ""a photo of"" filled with a class name enables CLIP toachieve $80$\% zero-shot accuracy on ImageNet. While previous research hasexplored prompt learning for the visual modality, analyzing what constitutes agood visual prompt specifically for image recognition is limited. In addition,existing visual prompt tuning methods' generalization ability is worse thantext-only prompting tuning. This paper explores our key insight: synthetic textimages are good visual prompts for vision-language models! To achieve that, wepropose our LoGoPrompt, which reformulates the classification objective to thevisual prompt selection and addresses the chicken-and-egg challenge of firstadding synthetic text images as class-wise visual prompts or predicting theclass first. Without any trainable visual prompt parameters, experimentalresults on 16 datasets demonstrate that our method consistently outperformsstate-of-the-art methods in few-shot learning, base-to-new generalization, anddomain generalization."
FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning,"['Xinyi Wang', 'John Wieting', 'Jonathan H. Clark']",http://arxiv.org/pdf/2309.04663v2.pdf,2023-09-09,"['cs.cl', 'cs.ai']","  Learning paradigms for large language models (LLMs) currently tend to fallwithin either in-context learning (ICL) or full fine-tuning. Each of thesecomes with their own trade-offs based on available data, model size, computecost, ease-of-use, and final quality with neither solution performing wellacross-the-board. In this article, we first describe ICL and fine-tuningparadigms in a way that highlights their natural connections. Based on theseconnections, we propose a new learning paradigm called FIAT that fuses the bestof these paradigms together, enabling prompt-engineered instructions andchain-of-thought reasoning with the very largest models while also usingsimilar methods to perform parameter updates on a modestly-sized LLM withparameter-efficient tuning. We evaluate FIAT's effectiveness on a variety ofmultilingual tasks and observe that FIAT performs better than both ICL andfine-tuning at scales ranging from 100-10,000 training examples. We hope thatFIAT provides a practical way of harnessing the full potential of LLMs withoutneeding to make a hard choice between learning paradigms."
Toward Reproducing Network Research Results Using Large Language Models,"['Qiao Xiang', 'Yuling Lin', 'Mingjun Fang', 'Bang Huang', 'Siyong Huang', 'Ridi Wen', 'Franck Le', 'Linghe Kong', 'Jiwu Shu']",http://arxiv.org/pdf/2309.04716v1.pdf,2023-09-09,"['cs.lg', 'cs.ai', 'cs.cl']","  Reproducing research results in the networking community is important forboth academia and industry. The current best practice typically resorts tothree approaches: (1) looking for publicly available prototypes; (2) contactingthe authors to get a private prototype; and (3) manually implementing aprototype following the description of the publication. However, most publishednetwork research does not have public prototypes and private prototypes arehard to get. As such, most reproducing efforts are spent on manualimplementation based on the publications, which is both time and laborconsuming and error-prone. In this paper, we boldly propose reproducing networkresearch results using the emerging large language models (LLMs). Inparticular, we first prove its feasibility with a small-scale experiment, inwhich four students with essential networking knowledge each reproduces adifferent networking system published in prominent conferences and journals byprompt engineering ChatGPT. We report the experiment's observations and lessonsand discuss future open research questions of this proposal. This work raisesno ethical issue."
Detecting Natural Language Biases with Prompt-based Learning,"['Md Abdul Aowal', 'Maliha T Islam', 'Priyanka Mary Mammen', 'Sandesh Shetty']",http://arxiv.org/pdf/2309.05227v1.pdf,2023-09-11,"['cs.cl', 'cs.ai']","  In this project, we want to explore the newly emerging field of promptengineering and apply it to the downstream task of detecting LM biases. Moreconcretely, we explore how to design prompts that can indicate 4 differenttypes of biases: (1) gender, (2) race, (3) sexual orientation, and (4)religion-based. Within our project, we experiment with different manuallycrafted prompts that can draw out the subtle biases that may be present in thelanguage model. We apply these prompts to multiple variations of popular andwell-recognized models: BERT, RoBERTa, and T5 to evaluate their biases. Weprovide a comparative analysis of these models and assess them using a two-foldmethod: use human judgment to decide whether model predictions are biased andutilize model-level judgment (through further prompts) to understand if a modelcan self-diagnose the biases of its own prediction."
Two Timin': Repairing Smart Contracts With A Two-Layered Approach,"['Abhinav Jain', 'Ehan Masud', 'Michelle Han', 'Rohan Dhillon', 'Sumukh Rao', 'Arya Joshi', 'Salar Cheema', 'Saurav Kumar']",http://arxiv.org/pdf/2309.07841v1.pdf,2023-09-14,"['cs.cr', 'cs.ai']","  Due to the modern relevance of blockchain technology, smart contracts presentboth substantial risks and benefits. Vulnerabilities within them can trigger acascade of consequences, resulting in significant losses. Many current papersprimarily focus on classifying smart contracts for malicious intent, oftenrelying on limited contract characteristics, such as bytecode or opcode. Thispaper proposes a novel, two-layered framework: 1) classifying and 2) directlyrepairing malicious contracts. Slither's vulnerability report is combined withsource code and passed through a pre-trained RandomForestClassifier (RFC) andLarge Language Models (LLMs), classifying and repairing each suggestedvulnerability. Experiments demonstrate the effectiveness of fine-tuned andprompt-engineered LLMs. The smart contract repair models, built frompre-trained GPT-3.5-Turbo and fine-tuned Llama-2-7B models, reduced the overallvulnerability count by 97.5% and 96.7% respectively. A manual inspection ofrepaired contracts shows that all retain functionality, indicating that theproposed method is appropriate for automatic batch classification and repair ofvulnerabilities in smart contracts."
Large Language Models for Failure Mode Classification: An Investigation,"['Michael Stewart', 'Melinda Hodkiewicz', 'Sirui Li']",http://arxiv.org/pdf/2309.08181v1.pdf,2023-09-15,['cs.cl'],"  In this paper we present the first investigation into the effectiveness ofLarge Language Models (LLMs) for Failure Mode Classification (FMC). FMC, thetask of automatically labelling an observation with a corresponding failuremode code, is a critical task in the maintenance domain as it reduces the needfor reliability engineers to spend their time manually analysing work orders.We detail our approach to prompt engineering to enable an LLM to predict thefailure mode of a given observation using a restricted code list. Wedemonstrate that the performance of a GPT-3.5 model (F1=0.80) fine-tuned onannotated data is a significant improvement over a currently available textclassification model (F1=0.60) trained on the same annotated data set. Thefine-tuned model also outperforms the out-of-the box GPT-3.5 (F1=0.46). Thisinvestigation reinforces the need for high quality fine-tuning data sets fordomain-specific tasks using LLMs."
Safurai 001: New Qualitative Approach for Code LLM Evaluation,"['Davide Cifarelli', 'Leonardo Boiardi', 'Alessandro Puppo']",http://arxiv.org/pdf/2309.11385v1.pdf,2023-09-20,['cs.cl'],"  This paper presents Safurai-001, a new Large Language Model (LLM) withsignificant potential in the domain of coding assistance. Driven by recentadvancements in coding LLMs, Safurai-001 competes in performance with thelatest models like WizardCoder [Xu et al., 2023], PanguCoder [Shen et al.,2023] and Phi-1 [Gunasekar et al., 2023] but aims to deliver a moreconversational interaction. By capitalizing on the progress in data engineering(including latest techniques of data transformation and prompt engineering) andinstruction tuning, this new model promises to stand toe-to-toe with recentclosed and open source developments. Recognizing the need for an efficaciousevaluation metric for coding LLMs, this paper also introduces GPT4-basedMultiParameters, an evaluation benchmark that harnesses varied parameters topresent a comprehensive insight into the models functioning and performance.Our assessment shows that Safurai-001 can outperform GPT-3.5 by 1.58% andWizardCoder by 18.78% in the Code Readability parameter and more."
A Practical Survey on Zero-shot Prompt Design for In-context Learning,['Yinheng Li'],http://arxiv.org/pdf/2309.13205v1.pdf,2023-09-22,"['cs.cl', 'cs.ai', 'cs.et', 'cs.lg']","  The remarkable advancements in large language models (LLMs) have broughtabout significant improvements in Natural Language Processing(NLP) tasks. Thispaper presents a comprehensive review of in-context learning techniques,focusing on different types of prompts, including discrete, continuous,few-shot, and zero-shot, and their impact on LLM performance. We explorevarious approaches to prompt design, such as manual design, optimizationalgorithms, and evaluation methods, to optimize LLM performance across diversetasks. Our review covers key research studies in prompt engineering, discussingtheir methodologies and contributions to the field. We also delve into thechallenges faced in evaluating prompt performance, given the absence of asingle ""best"" prompt and the importance of considering multiple metrics. Inconclusion, the paper highlights the critical role of prompt design inharnessing the full potential of LLMs and provides insights into thecombination of manual design, optimization techniques, and rigorous evaluationfor more effective and efficient use of LLMs in various NLP tasks."
A Chat About Boring Problems: Studying GPT-based text normalization,"['Yang Zhang', 'Travis M. Bartley', 'Mariana Graterol-Fuenmayor', 'Vitaly Lavrukhin', 'Evelina Bakhturina', 'Boris Ginsburg']",http://arxiv.org/pdf/2309.13426v1.pdf,2023-09-23,"['cs.cl', 'cs.ai']","  Text normalization - the conversion of text from written to spoken form - istraditionally assumed to be an ill-formed task for language models. In thiswork, we argue otherwise. We empirically show the capacity of Large-LanguageModels (LLM) for text normalization in few-shot scenarios. Combiningself-consistency reasoning with linguistic-informed prompt engineering, we findLLM based text normalization to achieve error rates around 40\% lower than topnormalization systems. Further, upon error analysis, we note key limitations inthe conventional design of text normalization tasks. We create a new taxonomyof text normalization errors and apply it to results from GPT-3.5-Turbo andGPT-4.0. Through this new framework, we can identify strengths and weaknessesof GPT-based TN, opening opportunities for future work."
DynaCon: Dynamic Robot Planner with Contextual Awareness via LLMs,"['Gyeongmin Kim', 'Taehyeon Kim', 'Shyam Sundar Kannan', 'Vishnunandan L. N. Venkatesh', 'Donghan Kim', 'Byung-Cheol Min']",http://arxiv.org/pdf/2309.16031v1.pdf,2023-09-27,['cs.ro'],"  Mobile robots often rely on pre-existing maps for effective path planning andnavigation. However, when these maps are unavailable, particularly inunfamiliar environments, a different approach become essential. This paperintroduces DynaCon, a novel system designed to provide mobile robots withcontextual awareness and dynamic adaptability during navigation, eliminatingthe reliance of traditional maps. DynaCon integrates real-time feedback with anobject server, prompt engineering, and navigation modules. By harnessing thecapabilities of Large Language Models (LLMs), DynaCon not only understandspatterns within given numeric series but also excels at categorizing objectsinto matched spaces. This facilitates dynamic path planner imbued withcontextual awareness. We validated the effectiveness of DynaCon through anexperiment where a robot successfully navigated to its goal using reasoning.Source code and experiment videos for this work can be found at:https://sites.google.com/view/dynacon."
Cyber Sentinel: Exploring Conversational Agents in Streamlining Security  Tasks with GPT-4,"['Mehrdad Kaheh', 'Danial Khosh Kholgh', 'Panos Kostakos']",http://arxiv.org/pdf/2309.16422v1.pdf,2023-09-28,['cs.cr'],"  In an era where cyberspace is both a battleground and a backbone of modernsociety, the urgency of safeguarding digital assets against ever-evolvingthreats is paramount. This paper introduces Cyber Sentinel, an innovativetask-oriented cybersecurity dialogue system that is effectively capable ofmanaging two core functions: explaining potential cyber threats within anorganization to the user, and taking proactive/reactive security actions wheninstructed by the user. Cyber Sentinel embodies the fusion of artificialintelligence, cybersecurity domain expertise, and real-time data analysis tocombat the multifaceted challenges posed by cyber adversaries. This articledelves into the process of creating such a system and how it can interact withother components typically found in cybersecurity organizations. Our work is anovel approach to task-oriented dialogue systems, leveraging the power ofchaining GPT-4 models combined with prompt engineering across all sub-tasks. Wealso highlight its pivotal role in enhancing cybersecurity communication andinteraction, concluding that not only does this framework enhance the system'stransparency (Explainable AI) but also streamlines the decision-making processand responding to threats (Actionable AI), therefore marking a significantadvancement in the realm of cybersecurity communication."
"A Sign Language Recognition System with Pepper, Lightweight-Transformer,  and LLM","['JongYoon Lim', 'Inkyu Sa', 'Bruce MacDonald', 'Ho Seok Ahn']",http://arxiv.org/pdf/2309.16898v1.pdf,2023-09-28,"['cs.ro', 'cs.cl', 'cs.cv', 'cs.hc']","  This research explores using lightweight deep neural network architectures toenable the humanoid robot Pepper to understand American Sign Language (ASL) andfacilitate non-verbal human-robot interaction. First, we introduce alightweight and efficient model for ASL understanding optimized for embeddedsystems, ensuring rapid sign recognition while conserving computationalresources. Building upon this, we employ large language models (LLMs) forintelligent robot interactions. Through intricate prompt engineering, we tailorinteractions to allow the Pepper Robot to generate natural Co-Speech Gestureresponses, laying the foundation for more organic and intuitive humanoid-robotdialogues. Finally, we present an integrated software pipeline, embodyingadvancements in a socially aware AI interaction model. Leveraging the PepperRobot's capabilities, we demonstrate the practicality and effectiveness of ourapproach in real-world scenarios. The results highlight a profound potentialfor enhancing human-robot interaction through non-verbal interactions, bridgingcommunication gaps, and making technology more accessible and understandable."
SPELL: Semantic Prompt Evolution based on a LLM,"['Yujian Betterest Li', 'Kai Wu']",http://arxiv.org/pdf/2310.01260v1.pdf,2023-10-02,"['cs.cl', 'cs.ai']","  Prompt engineering is a new paradigm for enhancing the performance of trainedneural network models. For optimizing text-style prompts, existing methodsusually individually operate small portions of a text step by step, whicheither breaks the fluency or could not globally adjust a prompt. Since largelanguage models (LLMs) have powerful ability of generating coherent texts tokenby token, can we utilize LLMs for improving prompts? Based on this motivation,in this paper, considering a trained LLM as a text generator, we attempt todesign a black-box evolution algorithm for automatically optimizing texts,namely SPELL (Semantic Prompt Evolution based on a LLM). The proposed method isevaluated with different LLMs and evolution parameters in different text tasks.Experimental results show that SPELL could rapidly improve the prompts indeed.We further explore the evolution process and discuss on the limitations,potential possibilities and future work."
Co-audit: tools to help humans double-check AI-generated content,"['Andrew D. Gordon', 'Carina Negreanu', 'José Cambronero', 'Rasika Chakravarthy', 'Ian Drosos', 'Hao Fang', 'Bhaskar Mitra', 'Hannah Richardson', 'Advait Sarkar', 'Stephanie Simmons', 'Jack Williams', 'Ben Zorn']",http://arxiv.org/pdf/2310.01297v1.pdf,2023-10-02,"['cs.hc', 'cs.ai', 'cs.cl', 'cs.pl']","  Users are increasingly being warned to check AI-generated content forcorrectness. Still, as LLMs (and other generative models) generate more complexoutput, such as summaries, tables, or code, it becomes harder for the user toaudit or evaluate the output for quality or correctness. Hence, we are seeingthe emergence of tool-assisted experiences to help the user double-check apiece of AI-generated content. We refer to these as co-audit tools. Co-audittools complement prompt engineering techniques: one helps the user constructthe input prompt, while the other helps them check the output response. As aspecific example, this paper describes recent research on co-audit tools forspreadsheet computations powered by generative models. We explain why co-auditexperiences are essential for any application of generative AI where quality isimportant and errors are consequential (as is common in spreadsheetcomputations). We propose a preliminary list of principles for co-audit, andoutline research challenges."
Chain of Natural Language Inference for Reducing Large Language Model  Ungrounded Hallucinations,"['Deren Lei', 'Yaxi Li', 'Mengya Hu', 'Mingyu Wang', 'Vincent Yun', 'Emily Ching', 'Eslam Kamal']",http://arxiv.org/pdf/2310.03951v2.pdf,2023-10-06,"['cs.cl', 'cs.ai']","  Large language models (LLMs) can generate fluent natural language texts whengiven relevant documents as background context. This ability has attractedconsiderable interest in developing industry applications of LLMs. However,LLMs are prone to generate hallucinations that are not supported by theprovided sources. In this paper, we propose a hierarchical framework to detectand mitigate such ungrounded hallucination. Our framework uses Chain of NaturalLanguage Inference (CoNLI) for hallucination detection and hallucinationreduction via post-editing. Our approach achieves state-of-the-art performanceon hallucination detection and enhances text quality through rewrite, usingLLMs without any fine-tuning or domain-specific prompt engineering. We showthat this simple plug-and-play framework can serve as an effective choice forhallucination detection and reduction, achieving competitive performance acrossvarious contexts."
LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation,"['Christian Munley', 'Aaron Jarmusch', 'Sunita Chandrasekaran']",http://arxiv.org/pdf/2310.04963v2.pdf,2023-10-08,['cs.ai'],"  Large language models (LLMs) are a new and powerful tool for a wide span ofapplications involving natural language and demonstrate impressive codegeneration abilities. In this paper, we explore the capabilitity ofstate-of-the-art LLMs, including closed-source options like OpenAI GPT-4 andopen-source alternatives like Meta AI Codellama, to automatically generatetests and use these tests to validate and verify compiler implementations of adirective-based programming paradigm, OpenACC. Our approach entails exploringvarious prompt engineering techniques including a code template,retrieval-augmented generation (RAG) with code template, expressive promptusing RAG with code template, one-shot example, and RAG with one-shot example.This paper focuses on (a) exploring the capabilities of the latest LLMs forcode generation, (b) investigating prompt and fine tuning methods, and (c)analyzing the outcome of LLMs generated tests"
Large Language Models for Propaganda Detection,"['Kilian Sprenkamp', 'Daniel Gordon Jones', 'Liudmila Zavolokina']",http://arxiv.org/pdf/2310.06422v2.pdf,2023-10-10,"['cs.cl', 'cs.ai']","  The prevalence of propaganda in our digital society poses a challenge tosocietal harmony and the dissemination of truth. Detecting propaganda throughNLP in text is challenging due to subtle manipulation techniques and contextualdependencies. To address this issue, we investigate the effectiveness of modernLarge Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection.We conduct experiments using the SemEval-2020 task 11 dataset, which featuresnews articles labeled with 14 propaganda techniques as a multi-labelclassification problem. Five variations of GPT-3 and GPT-4 are employed,incorporating various prompt engineering and fine-tuning strategies across thedifferent models. We evaluate the models' performance by assessing metrics suchas $F1$ score, $Precision$, and $Recall$, comparing the results with thecurrent state-of-the-art approach using RoBERTa. Our findings demonstrate thatGPT-4 achieves comparable results to the current state-of-the-art. Further,this study analyzes the potential and challenges of LLMs in complex tasks likepropaganda detection."
Forgetful Large Language Models: Lessons Learned from Using LLMs in  Robot Programming,"['Juo-Tung Chen', 'Chien-Ming Huang']",http://arxiv.org/pdf/2310.06646v1.pdf,2023-10-10,['cs.ro'],"  Large language models offer new ways of empowering people to program robotapplications-namely, code generation via prompting. However, the code generatedby LLMs is susceptible to errors. This work reports a preliminary explorationthat empirically characterizes common errors produced by LLMs in robotprogramming. We categorize these errors into two phases: interpretation andexecution. In this work, we focus on errors in execution and observe that theyare caused by LLMs being ""forgetful"" of key information provided in userprompts. Based on this observation, we propose prompt engineering tacticsdesigned to reduce errors in execution. We then demonstrate the effectivenessof these tactics with three language models: ChatGPT, Bard, and LLaMA-2.Finally, we discuss lessons learned from using LLMs in robot programming andcall for the benchmarking of LLM-powered end-user development of robotapplications."
LLMs Killed the Script Kiddie: How Agents Supported by Large Language  Models Change the Landscape of Network Threat Testing,"['Stephen Moskal', 'Sam Laney', 'Erik Hemberg', ""Una-May O'Reilly""]",http://arxiv.org/pdf/2310.06936v1.pdf,2023-10-10,"['cs.cr', 'cs.lg']","  In this paper, we explore the potential of Large Language Models (LLMs) toreason about threats, generate information about tools, and automate cybercampaigns. We begin with a manual exploration of LLMs in supporting specificthreat-related actions and decisions. We proceed by automating the decisionprocess in a cyber campaign. We present prompt engineering approaches for aplan-act-report loop for one action of a threat campaign and and a promptchaining design that directs the sequential decision process of a multi-actioncampaign. We assess the extent of LLM's cyber-specific knowledge w.r.t theshort campaign we demonstrate and provide insights into prompt design foreliciting actionable responses. We discuss the potential impact of LLMs on thethreat landscape and the ethical considerations of using LLMs for acceleratingthreat actor capabilities. We report a promising, yet concerning, applicationof generative AI to cyber threats. However, the LLM's capabilities to deal withmore complex networks, sophisticated vulnerabilities, and the sensitivity ofprompts are open questions. This research should spur deliberations over theinevitable advancements in LLM-supported cyber adversarial landscape."
Beyond Factuality: A Comprehensive Evaluation of Large Language Models  as Knowledge Generators,"['Liang Chen', 'Yang Deng', 'Yatao Bian', 'Zeyu Qin', 'Bingzhe Wu', 'Tat-Seng Chua', 'Kam-Fai Wong']",http://arxiv.org/pdf/2310.07289v1.pdf,2023-10-11,['cs.cl'],"  Large language models (LLMs) outperform information retrieval techniques fordownstream knowledge-intensive tasks when being prompted to generate worldknowledge. However, community concerns abound regarding the factuality andpotential implications of using this uncensored knowledge. In light of this, weintroduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed tosystematically and automatically evaluate generated knowledge from siximportant perspectives -- Factuality, Relevance, Coherence, Informativeness,Helpfulness and Validity. We conduct an extensive empirical analysis of thegenerated knowledge from three different types of LLMs on two widely studiedknowledge-intensive tasks, i.e., open-domain question answering andknowledge-grounded dialogue. Surprisingly, our study reveals that thefactuality of generated knowledge, even if lower, does not significantly hinderdownstream tasks. Instead, the relevance and coherence of the outputs are moreimportant than small factual mistakes. Further, we show how to use CONNER toimprove knowledge-intensive tasks by designing two strategies: PromptEngineering and Knowledge Selection. Our evaluation code and LLM-generatedknowledge with human annotations will be released to facilitate futureresearch."
Multimodal Large Language Model for Visual Navigation,"['Yao-Hung Hubert Tsai', 'Vansh Dhar', 'Jialu Li', 'Bowen Zhang', 'Jian Zhang']",http://arxiv.org/pdf/2310.08669v2.pdf,2023-10-12,"['cs.cv', 'cs.ro']","  Recent efforts to enable visual navigation using large language models havemainly focused on developing complex prompt systems. These systems incorporateinstructions, observations, and history into massive text prompts, which arethen combined with pre-trained large language models to facilitate visualnavigation. In contrast, our approach aims to fine-tune large language modelsfor visual navigation without extensive prompt engineering. Our design involvesa simple text prompt, current observations, and a history collector model thatgathers information from previous observations as input. For output, our designprovides a probability distribution of possible actions that the agent can takeduring navigation. We train our model using human demonstrations and collisionsignals from the Habitat-Matterport 3D Dataset (HM3D). Experimental resultsdemonstrate that our method outperforms state-of-the-art behavior cloningmethods and effectively reduces collision rates."
GPTutor: an open-source AI pair programming tool alternative to Copilot,"['Eason Chen', 'Ray Huang', 'Justa Liang', 'Damien Chen', 'Pierce Hung']",http://arxiv.org/pdf/2310.13896v3.pdf,2023-10-21,['cs.hc'],"  This paper presents the latest progress of GPTutor: a ChatGPT-poweredprogramming tool extension in Visual Studio Code. The emergence of LargeLanguage Models (LLMs) has improved software development efficiency, but theirperformance can be hindered by training data limitations and prompt designissues. Existing LLM development tools often operate as black boxes, with usersunable to view the prompts used and unable to improve performance by correctingprompts when errors occur. To address the aforementioned issues, GPTutor wasintroduced as an open-source AI pair programming tool, offering an alternativeto Copilot. GPTutor empowers users to customize prompts for various programminglanguages and scenarios, with support for 120+ human languages and 50+programming languages. Users can fine-tune prompts to correct the errors fromLLM for precision and efficient code generation. At the end of the paper, weunderscore GPTutor's potential through examples, including demonstrating itsproficiency in interpreting and generating Sui-Move, a newly introduced smartcontract language, using prompt engineering."
TaskDiff: A Similarity Metric for Task-Oriented Conversations,"['Ankita Bhaumik', 'Praveen Venkateswaran', 'Yara Rizk', 'Vatche Isahagian']",http://arxiv.org/pdf/2310.15298v2.pdf,2023-10-23,"['cs.cl', 'cs.ai']","  The popularity of conversational digital assistants has resulted in theavailability of large amounts of conversational data which can be utilized forimproved user experience and personalized response generation. Building theseassistants using popular large language models like ChatGPT also requireadditional emphasis on prompt engineering and evaluation methods. Textualsimilarity metrics are a key ingredient for such analysis and evaluations.While many similarity metrics have been proposed in the literature, they havenot proven effective for task-oriented conversations as they do not takeadvantage of unique conversational features. To address this gap, we presentTaskDiff, a novel conversational similarity metric that utilizes differentdialogue components (utterances, intents, and slots) and their distributions tocompute similarity. Extensive experimental evaluation of TaskDiff on abenchmark dataset demonstrates its superior performance and improved robustnessover other related approaches."
Large language models for aspect-based sentiment analysis,"['Paul F. Simmering', 'Paavo Huoviala']",http://arxiv.org/pdf/2310.18025v1.pdf,2023-10-27,"['cs.cl', 'cs.ai']","  Large language models (LLMs) offer unprecedented text completioncapabilities. As general models, they can fulfill a wide range of roles,including those of more specialized models. We assess the performance of GPT-4and GPT-3.5 in zero shot, few shot and fine-tuned settings on the aspect-basedsentiment analysis (ABSA) task. Fine-tuned GPT-3.5 achieves a state-of-the-artF1 score of 83.8 on the joint aspect term extraction and polarityclassification task of the SemEval-2014 Task 4, improving upon InstructABSA[@scaria_instructabsa_2023] by 5.7%. However, this comes at the price of 1000times more model parameters and thus increased inference cost. We discuss thethe cost-performance trade-offs of different models, and analyze the typicalerrors that they make. Our results also indicate that detailed prompts improveperformance in zero-shot and few-shot settings but are not necessary forfine-tuned models. This evidence is relevant for practioners that are facedwith the choice of prompt engineering versus fine-tuning when using LLMs forABSA."
Can Large Language Models Capture Public Opinion about Global Warming?  An Empirical Assessment of Algorithmic Fidelity and Bias,"['S. Lee', 'T. Q. Peng', 'M. H. Goldberg', 'S. A. Rosenthal', 'J. E. Kotcher', 'E. W. Maibach', 'A. Leiserowitz']",http://arxiv.org/pdf/2311.00217v1.pdf,2023-11-01,"['cs.ai', 'cs.cy']","  Large language models (LLMs) have demonstrated their potential in socialscience research by emulating human perceptions and behaviors, a conceptreferred to as algorithmic fidelity. This study assesses the algorithmicfidelity and bias of LLMs by utilizing two nationally representative climatechange surveys. The LLMs were conditioned on demographics and/or psychologicalcovariates to simulate survey responses. The findings indicate that LLMs caneffectively capture presidential voting behaviors but encounter challenges inaccurately representing global warming perspectives when relevant covariatesare not included. GPT-4 exhibits improved performance when conditioned on bothdemographics and covariates. However, disparities emerge in LLM estimations ofthe views of certain groups, with LLMs tending to underestimate worry aboutglobal warming among Black Americans. While highlighting the potential of LLMsto aid social science research, these results underscore the importance ofmeticulous conditioning, model selection, survey question format, and biasassessment when employing LLMs for survey simulation. Further investigationinto prompt engineering and algorithm auditing is essential to harness thepower of LLMs while addressing their inherent limitations."
Noisy Exemplars Make Large Language Models More Robust: A  Domain-Agnostic Behavioral Analysis,"['Hongyi Zheng', 'Abulhair Saparov']",http://arxiv.org/pdf/2311.00258v1.pdf,2023-11-01,"['cs.cl', 'cs.lg']","  Recent advances in prompt engineering enable large language models (LLMs) tosolve multi-hop logical reasoning problems with impressive accuracy. However,there is little existing work investigating the robustness of LLMs withfew-shot prompting techniques. Therefore, we introduce a systematic approach totest the robustness of LLMs in multi-hop reasoning tasks via domain-agnosticperturbations. We include perturbations at multiple levels of abstractions(e.g. lexical perturbations such as typos, and semantic perturbations such asthe inclusion of intermediate reasoning steps in the questions) to conductbehavioral analysis on the LLMs. Throughout our experiments, we find thatmodels are more sensitive to certain perturbations such as replacing words withtheir synonyms. We also demonstrate that increasing the proportion of perturbedexemplars in the prompts improves the robustness of few-shot prompting methods."
Instruction Distillation Makes Large Language Models Efficient Zero-shot  Rankers,"['Weiwei Sun', 'Zheng Chen', 'Xinyu Ma', 'Lingyong Yan', 'Shuaiqiang Wang', 'Pengjie Ren', 'Zhumin Chen', 'Dawei Yin', 'Zhaochun Ren']",http://arxiv.org/pdf/2311.01555v1.pdf,2023-11-02,"['cs.ir', 'cs.cl']","  Recent studies have demonstrated the great potential of Large Language Models(LLMs) serving as zero-shot relevance rankers. The typical approach involvesmaking comparisons between pairs or lists of documents. Although effective,these listwise and pairwise methods are not efficient and also heavily rely onintricate prompt engineering. To tackle this problem, we introduce a novelinstruction distillation method. The key idea is to distill the pairwiseranking ability of open-sourced LLMs to a simpler but more efficient pointwiseranking. Specifically, given the same LLM, we first rank documents using theeffective pairwise approach with complex instructions, and then distill theteacher predictions to the pointwise approach with simpler instructions.Evaluation results on the BEIR, TREC, and ReDial datasets demonstrate thatinstruction distillation can improve efficiency by 10 to 100x and also enhancethe ranking performance of LLMs. Furthermore, our approach surpasses theperformance of existing supervised methods like monoT5 and is on par with thestate-of-the-art zero-shot methods. The code to reproduce our results isavailable at www.github.com/sunnweiwei/RankGPT."
Indicative Summarization of Long Discussions,"['Shahbaz Syed', 'Dominik Schwabe', 'Khalid Al-Khatib', 'Martin Potthast']",http://arxiv.org/pdf/2311.01882v1.pdf,2023-11-03,['cs.cl'],"  Online forums encourage the exchange and discussion of different stances onmany topics. Not only do they provide an opportunity to present one's ownarguments, but may also gather a broad cross-section of others' arguments.However, the resulting long discussions are difficult to overview. This paperpresents a novel unsupervised approach using large language models (LLMs) togenerating indicative summaries for long discussions that basically serve astables of contents. Our approach first clusters argument sentences, generatescluster labels as abstractive summaries, and classifies the generated clusterlabels into argumentation frames resulting in a two-level summary. Based on anextensively optimized prompt engineering approach, we evaluate 19~LLMs forgenerative cluster labeling and frame classification. To evaluate theusefulness of our indicative summaries, we conduct a purpose-driven user studyvia a new visual interface called Discussion Explorer: It shows that ourproposed indicative summaries serve as a convenient navigation tool to explorelong discussions."
Automating Governing Knowledge Commons and Contextual Integrity (GKC-CI)  Privacy Policy Annotations with Large Language Models,"['Jake Chanenson', 'Madison Pickering', 'Noah Apthorpe']",http://arxiv.org/pdf/2311.02192v1.pdf,2023-11-03,"['cs.cy', 'cs.cl', 'cs.lg']","  Identifying contextual integrity (CI) and governing knowledge commons (GKC)parameters in privacy policy texts can facilitate normative privacy analysis.However, GKC-CI annotation has heretofore required manual or crowdsourcedeffort. This paper demonstrates that high-accuracy GKC-CI parameter annotationof privacy policies can be performed automatically using large language models.We fine-tune 18 open-source and proprietary models on 21,588 GKC-CI annotationsfrom 16 ground truth privacy policies. Our best-performing model (fine-tunedGPT-3.5 Turbo with prompt engineering) has an accuracy of 86%, exceeding theperformance of prior crowdsourcing approaches despite the complexity of privacypolicy texts and the nuance of the GKC-CI annotation task. We apply ourbest-performing model to privacy policies from 164 popular online services,demonstrating the effectiveness of scaling GKC-CI annotation for dataexploration. We make all annotated policies as well as the training data andscripts needed to fine-tune our best-performing model publicly available forfuture research."
Requirements Engineering using Generative AI: Prompts and Prompting  Patterns,"['Krishna Ronanki', 'Beatriz Cabrero-Daniel', 'Jennifer Horkoff', 'Christian Berger']",http://arxiv.org/pdf/2311.03832v1.pdf,2023-11-07,['cs.se'],"  [Context]: Companies are increasingly recognizing the importance ofautomating Requirements Engineering (RE) tasks due to their resource-intensivenature. The advent of GenAI has made these tasks more amenable to automation,thanks to its ability to understand and interpret context effectively.[Problem]: However, in the context of GenAI, prompt engineering is a criticalfactor for success. Despite this, we currently lack tools and methods tosystematically assess and determine the most effective prompt patterns toemploy for a particular RE task. [Method]: Two tasks related to requirements,specifically requirement classification and tracing, were automated using theGPT-3.5 turbo API. The performance evaluation involved assessing variousprompts created using 5 prompt patterns and implemented programmatically toperform the selected RE tasks, focusing on metrics such as precision, recall,accuracy, and F-Score. [Results]: This paper evaluates the effectiveness of the5 prompt patterns' ability to make GPT-3.5 turbo perform the selected RE tasksand offers recommendations on which prompt pattern to use for a specific REtask. Additionally, it also provides an evaluation framework as a reference forresearchers and practitioners who want to evaluate different prompt patternsfor different RE tasks."
How are Prompts Different in Terms of Sensitivity?,"['Sheng Lu', 'Hendrik Schuff', 'Iryna Gurevych']",http://arxiv.org/pdf/2311.07230v1.pdf,2023-11-13,['cs.cl'],"  In-context learning (ICL) has become one of the most popular learningparadigms. While there is a growing body of literature focusing on promptengineering, there is a lack of systematic analysis comparing the effects ofprompts across different models and tasks. To address this gap, we present acomprehensive prompt analysis based on the sensitivity of a function. Ouranalysis reveals that sensitivity is an unsupervised proxy for modelperformance, as it exhibits a strong negative correlation with accuracy. We usegradient-based saliency scores to empirically demonstrate how different promptsaffect the relevance of input tokens to the output, resulting in differentlevels of sensitivity. Furthermore, we introduce sensitivity-aware decodingwhich incorporates sensitivity estimation as a penalty term in the standardgreedy decoding. We show that this approach is particularly helpful wheninformation in the input is scarce. Our work provides a fresh perspective onthe analysis of prompts, and contributes to a better understanding of themechanism of ICL."
Think Before You Speak: Cultivating Communication Skills of Large  Language Models via Inner Monologue,"['Junkai Zhou', 'Liang Pang', 'Huawei Shen', 'Xueqi Cheng']",http://arxiv.org/pdf/2311.07445v1.pdf,2023-11-13,"['cs.cl', 'cs.ai']","  The emergence of large language models (LLMs) further improves thecapabilities of open-domain dialogue systems and can generate fluent, coherent,and diverse responses. However, LLMs still lack an important ability:communication skills, which makes them more like information seeking tools thananthropomorphic chatbots. To make LLMs more anthropomorphic and proactiveduring the conversation, we add five communication skills to the responsegeneration process: topic transition, proactively asking questions, conceptguidance, empathy, and summarising often. The addition of communication skillsincreases the interest of users in the conversation and attracts them to chatfor longer. To enable LLMs better understand and use communication skills, wedesign and add the inner monologue to LLMs. The complete process is achievedthrough prompt engineering and in-context learning. To evaluate communicationskills, we construct a benchmark named Cskills for evaluating variouscommunication skills, which can also more comprehensively evaluate the dialoguegeneration ability of the model. Experimental results show that the proposedCSIM strategy improves the backbone models and outperforms the baselines inboth automatic and human evaluations."
Assessing Test-time Variability for Interactive 3D Medical Image  Segmentation with Diverse Point Prompts,"['Hao Li', 'Han Liu', 'Dewei Hu', 'Jiacheng Wang', 'Ipek Oguz']",http://arxiv.org/pdf/2311.07806v1.pdf,2023-11-13,['cs.cv'],"  Interactive segmentation model leverages prompts from users to produce robustsegmentation. This advancement is facilitated by prompt engineering, whereinteractive prompts serve as strong priors during test-time. However, this isan inherently subjective and hard-to-reproduce process. The variability in userexpertise and inherently ambiguous boundaries in medical images can lead toinconsistent prompt selections, potentially affecting segmentation accuracy.This issue has not yet been extensively explored for medical imaging. In thispaper, we assess the test-time variability for interactive medical imagesegmentation with diverse point prompts. For a given target region, the pointis classified into three sub-regions: boundary, margin, and center. Our goal isto identify a straightforward and efficient approach for optimal promptselection during test-time based on three considerations: (1) benefits ofadditional prompts, (2) effects of prompt placement, and (3) strategies foroptimal prompt selection. We conduct extensive experiments on the publicMedical Segmentation Decathlon dataset for challenging colon tumor segmentationtask. We suggest an optimal strategy for prompt selection during test-time,supported by comprehensive results. The code is publicly available athttps://github.com/MedICL-VU/variability"
I Was Blind but Now I See: Implementing Vision-Enabled Dialogue in  Social Robots,"['Giulio Antonio Abbo', 'Tony Belpaeme']",http://arxiv.org/pdf/2311.08957v1.pdf,2023-11-15,"['cs.ro', 'cs.ai', 'cs.hc']","  In the rapidly evolving landscape of human-computer interaction, theintegration of vision capabilities into conversational agents stands as acrucial advancement. This paper presents an initial implementation of adialogue manager that leverages the latest progress in Large Language Models(e.g., GPT-4, IDEFICS) to enhance the traditional text-based prompts withreal-time visual input. LLMs are used to interpret both textual prompts andvisual stimuli, creating a more contextually aware conversational agent. Thesystem's prompt engineering, incorporating dialogue with summarisation of theimages, ensures a balance between context preservation and computationalefficiency. Six interactions with a Furhat robot powered by this system arereported, illustrating and discussing the results obtained. By implementingthis vision-enabled dialogue system, the paper envisions a future whereconversational agents seamlessly blend textual and visual modalities, enablingricher, more context-aware dialogues."
Simulating Opinion Dynamics with Networks of LLM-based Agents,"['Yun-Shiuan Chuang', 'Agam Goyal', 'Nikunj Harlalka', 'Siddharth Suresh', 'Robert Hawkins', 'Sijia Yang', 'Dhavan Shah', 'Junjie Hu', 'Timothy T. Rogers']",http://arxiv.org/pdf/2311.09618v1.pdf,2023-11-16,"['physics.soc-ph', 'cs.cl']","  Accurately simulating human opinion dynamics is crucial for understanding avariety of societal phenomena, including polarization and the spread ofmisinformation. However, the agent-based models (ABMs) commonly used for suchsimulations lack fidelity to human behavior. We propose a new approach tosimulating opinion dynamics based on populations of Large Language Models(LLMs). Our findings reveal a strong inherent bias in LLM agents towardsaccurate information, leading to consensus in line with scientific reality.However, this bias limits the simulation of individuals with resistant views onissues like climate change. After inducing confirmation bias through promptengineering, we observed opinion fragmentation in line with existingagent-based research. These insights highlight the promise and limitations ofLLM agents in this domain and suggest a path forward: refining LLMs withreal-world discourse to better simulate the evolution of human beliefs."
FairytaleCQA: Integrating a Commonsense Knowledge Graph into Children's  Storybook Narratives,"['Jiaju Chen', 'Yuxuan Lu', 'Shao Zhang', 'Bingsheng Yao', 'Yuanzhe Dong', 'Ying Xu', 'Yunyao Li', 'Qianwen Wang', 'Dakuo Wang', 'Yuling Sun']",http://arxiv.org/pdf/2311.09756v1.pdf,2023-11-16,['cs.cl'],"  AI models (including LLM) often rely on narrative question-answering (QA)datasets to provide customized QA functionalities to support downstreamchildren education applications; however, existing datasets only include QApairs that are grounded within the given storybook content, but children canlearn more when teachers refer the storybook content to real-world knowledge(e.g., commonsense knowledge). We introduce the FairytaleCQA dataset, which isannotated by children education experts, to supplement 278 storybook narrativeswith educationally appropriate commonsense knowledge. The dataset has 5,868 QApairs that not only originate from the storybook narrative but also contain thecommonsense knowledge grounded by an external knowledge graph (i.e.,ConceptNet). A follow-up experiment shows that a smaller model (T5-large)fine-tuned with FairytaleCQA reliably outperforms much larger prompt-engineeredLLM (e.g., GPT-4) in this new QA-pair generation task (QAG). This resultsuggests that: 1) our dataset brings novel challenges to existing LLMs, and 2)human experts' data annotation are still critical as they have much nuancedknowledge that LLMs do not know in the children educational domain."
ChatGPT and post-test probability,['Samuel J. Weisenthal'],http://arxiv.org/pdf/2311.12188v3.pdf,2023-11-20,"['cs.ai', 'stat.ap']","  Reinforcement learning-based large language models, such as ChatGPT, arebelieved to have potential to aid human experts in many domains, includinghealthcare. There is, however, little work on ChatGPT's ability to perform akey task in healthcare: formal, probabilistic medical diagnostic reasoning.This type of reasoning is used, for example, to update a pre-test probabilityto a post-test probability. In this work, we probe ChatGPT's ability to performthis task. In particular, we ask ChatGPT to give examples of how to use Bayesrule for medical diagnosis. Our prompts range from queries that use terminologyfrom pure probability (e.g., requests for a ""posterior probability"") to queriesthat use terminology from the medical diagnosis literature (e.g., requests fora ""post-test probability""). We show how the introduction of medical variablenames leads to an increase in the number of errors that ChatGPT makes. Givenour results, we also show how one can use prompt engineering to facilitateChatGPT's partial avoidance of these errors. We discuss our results in light ofrecent commentaries on sensitivity and specificity. We also discuss how ourresults might inform new research directions for large language models."
"Localizing Lying in Llama: Understanding Instructed Dishonesty on  True-False Questions Through Prompting, Probing, and Patching","['James Campbell', 'Richard Ren', 'Phillip Guo']",http://arxiv.org/pdf/2311.15131v1.pdf,2023-11-25,"['cs.lg', 'cs.ai', 'cs.cl']","  Large language models (LLMs) demonstrate significant knowledge through theiroutputs, though it is often unclear whether false outputs are due to a lack ofknowledge or dishonesty. In this paper, we investigate instructed dishonesty,wherein we explicitly prompt LLaMA-2-70b-chat to lie. We perform promptengineering to find which prompts best induce lying behavior, and then usemechanistic interpretability approaches to localize where in the network thisbehavior occurs. Using linear probing and activation patching, we localize fivelayers that appear especially important for lying. We then find just 46attention heads within these layers that enable us to causally intervene suchthat the lying model instead answers honestly. We show that these interventionswork robustly across many prompts and dataset splits. Overall, our workcontributes a greater understanding of dishonesty in LLMs so that we may hopeto prevent it."
The Transformative Influence of Large Language Models on Software  Development,['Sajed Jalil'],http://arxiv.org/pdf/2311.16429v1.pdf,2023-11-28,"['cs.se', 'cs.hc', '68t07', 'd.2.3; i.2.5; i.2.7']","  The increasing adoption and commercialization of generalized Large LanguageModels (LLMs) have profoundly impacted various aspects of our daily lives.Initially embraced by the computer science community, the versatility of LLMshas found its way into diverse domains. In particular, the software engineeringrealm has witnessed the most transformative changes. With LLMs increasinglyserving as AI Pair Programming Assistants spurred the development ofspecialized models aimed at aiding software engineers. Although this newparadigm offers numerous advantages, it also presents critical challenges andopen problems. To identify the potential and prevailing obstacles, wesystematically reviewed contemporary scholarly publications, emphasizing theperspectives of software developers and usability concerns. Preliminaryfindings underscore pressing concerns about data privacy, bias, andmisinformation. Additionally, we identified several usability challenges,including prompt engineering, increased cognitive demands, and mistrust.Finally, we introduce 12 open problems that we have identified through oursurvey, covering these various domains."
"Large Language Models for Networking: Applications, Enabling Techniques,  and Challenges","['Yudong Huang', 'Hongyang Du', 'Xinyuan Zhang', 'Dusit Niyato', 'Jiawen Kang', 'Zehui Xiong', 'Shuo Wang', 'Tao Huang']",http://arxiv.org/pdf/2311.17474v1.pdf,2023-11-29,['cs.ni'],"  The rapid evolution of network technologies and the growing complexity ofnetwork tasks necessitate a paradigm shift in how networks are designed,configured, and managed. With a wealth of knowledge and expertise, largelanguage models (LLMs) are one of the most promising candidates. This paperaims to pave the way for constructing domain-adapted LLMs for networking.Firstly, we present potential LLM applications for vertical network fields andshowcase the mapping from natural language to network language. Then, severalenabling technologies are investigated, including parameter-efficientfinetuning and prompt engineering. The insight is that language understandingand tool usage are both required for network LLMs. Driven by the idea ofembodied intelligence, we propose the ChatNet, a domain-adapted network LLMframework with access to various external network tools. ChatNet can reduce thetime required for burdensome network planning tasks significantly, leading to asubstantial improvement in efficiency. Finally, key challenges and futureresearch directions are highlighted."
Large Language Models for Travel Behavior Prediction,"['Baichuan Mo', 'Hanyong Xu', 'Dingyi Zhuang', 'Ruoyun Ma', 'Xiaotong Guo', 'Jinhua Zhao']",http://arxiv.org/pdf/2312.00819v1.pdf,2023-11-30,"['cs.lg', 'cs.ai', 'cs.cl']","  Travel behavior prediction is a fundamental task in transportation demandmanagement. The conventional methods for travel behavior prediction rely onnumerical data to construct mathematical models and calibrate model parametersto represent human preferences. Recent advancement in large language models(LLMs) has shown great reasoning abilities to solve complex problems. In thisstudy, we propose to use LLMs to predict travel behavior with promptengineering without data-based parameter learning. Specifically, we carefullydesign our prompts that include 1) task description, 2) travel characteristics,3) individual attributes, and 4) guides of thinking with domain knowledge, andask the LLMs to predict an individual's travel behavior and explain theresults. We select the travel mode choice task as a case study. Results showthat, though no training samples are provided, LLM-based predictions havecompetitive accuracy and F1-score as canonical supervised learning methods suchas multinomial logit, random forest, and neural networks. LLMs can also outputreasons that support their prediction. However, though in most of the cases,the output explanations are reasonable, we still observe cases that violatelogic or with hallucinations."
Improving the Generalization of Segmentation Foundation Model under  Distribution Shift via Weakly Supervised Adaptation,"['Haojie Zhang', 'Yongyi Su', 'Xun Xu', 'Kui Jia']",http://arxiv.org/pdf/2312.03502v1.pdf,2023-12-06,['cs.cv'],"  The success of large language models has inspired the computer visioncommunity to explore image segmentation foundation model that is able tozero/few-shot generalize through prompt engineering. Segment-Anything(SAM),among others, is the state-of-the-art image segmentation foundation modeldemonstrating strong zero/few-shot generalization. Despite the success, recentstudies reveal the weakness of SAM under strong distribution shift. Inparticular, SAM performs awkwardly on corrupted natural images, camouflagedimages, medical images, etc. Motivated by the observations, we aim to develop aself-training based strategy to adapt SAM to target distribution. Given theunique challenges of large source dataset, high computation cost and incorrectpseudo label, we propose a weakly supervised self-training architecture withanchor regularization and low-rank finetuning to improve the robustness andcomputation efficiency of adaptation. We validate the effectiveness on 5 typesof downstream segmentation tasks including natural clean/corrupted images,medical images, camouflaged images and robotic images. Our proposed method istask-agnostic in nature and outperforms pre-trained SAM and state-of-the-artdomain adaptation methods on almost all downstream tasks with the same testingprompt inputs."
PromptBench: A Unified Library for Evaluation of Large Language Models,"['Kaijie Zhu', 'Qinlin Zhao', 'Hao Chen', 'Jindong Wang', 'Xing Xie']",http://arxiv.org/pdf/2312.07910v1.pdf,2023-12-13,"['cs.ai', 'cs.cl', 'cs.lg']","  The evaluation of large language models (LLMs) is crucial to assess theirperformance and mitigate potential security risks. In this paper, we introducePromptBench, a unified library to evaluate LLMs. It consists of several keycomponents that are easily used and extended by researchers: promptconstruction, prompt engineering, dataset and model loading, adversarial promptattack, dynamic evaluation protocols, and analysis tools. PromptBench isdesigned to be an open, general, and flexible codebase for research purposesthat can facilitate original study in creating new benchmarks, deployingdownstream applications, and designing new evaluation protocols. The code isavailable at: https://github.com/microsoft/promptbench and will be continuouslysupported."
ChatSOS: LLM-based knowledge Q&A system for safety engineering,"['Haiyang Tang', 'Zhenyi Liu', 'Dongping Chen', 'Qingzhao Chu']",http://arxiv.org/pdf/2312.08629v1.pdf,2023-12-14,['cs.ai'],"  Recent advancements in large language models (LLMs) have notably propellednatural language processing (NLP) capabilities, demonstrating significantpotential in safety engineering applications. Despite these advancements, LLMsface constraints in processing specialized tasks, attributed to factors such ascorpus size, input processing limitations, and privacy concerns. Obtaininguseful information from reliable sources in a limited time is crucial for LLM.Addressing this, our study introduces an LLM-based Q&A system for safetyengineering, enhancing the comprehension and response accuracy of the model. Weemployed prompt engineering to incorporate external knowledge databases, thusenriching the LLM with up-to-date and reliable information. The system analyzeshistorical incident reports through statistical methods, utilizes vectorembedding to construct a vector database, and offers an efficientsimilarity-based search functionality. Our findings indicate that theintegration of external knowledge significantly augments the capabilities ofLLM for in-depth problem analysis and autonomous task assignment. Iteffectively summarizes accident reports and provides pertinent recommendations.This integration approach not only expands LLM applications in safetyengineering but also sets a precedent for future developments towardsautomation and intelligent systems."
Differentiable Prompt Makes Pre-trained Language Models Better Few-shot  Learners,"['Ningyu Zhang', 'Luoqiu Li', 'Xiang Chen', 'Shumin Deng', 'Zhen Bi', 'Chuanqi Tan', 'Fei Huang', 'Huajun Chen']",http://arxiv.org/pdf/2108.13161v7.pdf,2021-08-30,"['cs.cl', 'cs.ai', 'cs.cv', 'cs.ir', 'cs.lg']","  Large-scale pre-trained language models have contributed significantly tonatural language processing by demonstrating remarkable abilities as few-shotlearners. However, their effectiveness depends mainly on scaling the modelparameters and prompt design, hindering their implementation in most real-worldapplications. This study proposes a novel pluggable, extensible, and efficientapproach named DifferentiAble pRompT (DART), which can convert small languagemodels into better few-shot learners without any prompt engineering. The mainprinciple behind this approach involves reformulating potential naturallanguage processing tasks into the task of a pre-trained language model anddifferentially optimizing the prompt template as well as the target label withbackpropagation. Furthermore, the proposed approach can be: (i) Plugged to anypre-trained language models; (ii) Extended to widespread classification tasks.A comprehensive evaluation of standard NLP tasks demonstrates that the proposedapproach achieves a better few-shot performance. Code is available inhttps://github.com/zjunlp/DART."
ActionCLIP: A New Paradigm for Video Action Recognition,"['Mengmeng Wang', 'Jiazheng Xing', 'Yong Liu']",http://arxiv.org/pdf/2109.08472v1.pdf,2021-09-17,['cs.cv'],"  The canonical approach to video action recognition dictates a neural model todo a classic and standard 1-of-N majority vote task. They are trained topredict a fixed set of predefined categories, limiting their transferableability on new datasets with unseen concepts. In this paper, we provide a newperspective on action recognition by attaching importance to the semanticinformation of label texts rather than simply mapping them into numbers.Specifically, we model this task as a video-text matching problem within amultimodal learning framework, which strengthens the video representation withmore semantic language supervision and enables our model to do zero-shot actionrecognition without any further labeled data or parameters requirements.Moreover, to handle the deficiency of label texts and make use of tremendousweb data, we propose a new paradigm based on this multimodal learning frameworkfor action recognition, which we dub ""pre-train, prompt and fine-tune"". Thisparadigm first learns powerful representations from pre-training on a largeamount of web image-text or video-text data. Then it makes the actionrecognition task to act more like pre-training problems via prompt engineering.Finally, it end-to-end fine-tunes on target datasets to obtain strongperformance. We give an instantiation of the new paradigm, ActionCLIP, whichnot only has superior and flexible zero-shot/few-shot transfer ability but alsoreaches a top performance on general action recognition task, achieving 83.8%top-1 accuracy on Kinetics-400 with a ViT-B/16 as the backbone. Code isavailable at https://github.com/sallymmx/ActionCLIP.git"
CLIP-Adapter: Better Vision-Language Models with Feature Adapters,"['Peng Gao', 'Shijie Geng', 'Renrui Zhang', 'Teli Ma', 'Rongyao Fang', 'Yongfeng Zhang', 'Hongsheng Li', 'Yu Qiao']",http://arxiv.org/pdf/2110.04544v1.pdf,2021-10-09,"['cs.cv', 'cs.cl']","  Large-scale contrastive vision-language pre-training has shown significantprogress in visual representation learning. Unlike traditional visual systemstrained by a fixed set of discrete labels, a new paradigm was introduced in\cite{radford2021learning} to directly learn to align images with raw texts inan open-vocabulary setting. On downstream tasks, a carefully chosen text promptis employed to make zero-shot predictions.~To avoid non-trivial promptengineering, context optimization \cite{zhou2021coop} has been proposed tolearn continuous vectors as task-specific prompts with few-shot trainingexamples.~In this paper, we show that there is an alternative path to achievebetter vision-language models other than prompt tuning.~While prompt tuning isfor the textual inputs, we propose CLIP-Adapter to conduct fine-tuning withfeature adapters on either visual or language branch. Specifically,CLIP-Adapter adopts an additional bottleneck layer to learn new features andperforms residual-style feature blending with the original pre-trainedfeatures.~As a consequence, CLIP-Adapter is able to outperform contextoptimization while maintains a simple design. Experiments and extensiveablation studies on various visual classification tasks demonstrate theeffectiveness of our approach."
Symbolic Knowledge Distillation: from General Language Models to  Commonsense Models,"['Peter West', 'Chandra Bhagavatula', 'Jack Hessel', 'Jena D. Hwang', 'Liwei Jiang', 'Ronan Le Bras', 'Ximing Lu', 'Sean Welleck', 'Yejin Choi']",http://arxiv.org/pdf/2110.07178v2.pdf,2021-10-14,['cs.cl'],"  The common practice for training commonsense models has gonefrom-human-to-corpus-to-machine: humans author commonsense knowledge graphs inorder to train commonsense models. In this work, we investigate an alternative,from-machine-to-corpus-to-machine: general language models author thesecommonsense knowledge graphs to train commonsense models. Our study leads to anew framework, Symbolic Knowledge Distillation. As with prior art in KnowledgeDistillation (Hinton et al., 2015), our approach uses larger models to teachsmaller models. A key difference is that we distill knowledge symbolically-astext-in addition to the neural model. We also distill only one aspect-thecommonsense of a general language model teacher, allowing the student to be adifferent type, a commonsense model. Altogether, we show that careful promptengineering and a separately trained critic model allow us to selectivelydistill high-quality causal commonsense from GPT-3, a general language model.Empirical results demonstrate that, for the first time, a human-authoredcommonsense knowledge graph is surpassed by our automatically distilled variantin all three criteria: quantity, quality, and diversity. In addition, itresults in a neural commonsense model that surpasses the teacher model'scommonsense capabilities despite its 100x smaller size. We apply this to theATOMIC resource, and share our new symbolic knowledge graph and commonsensemodels."
Red Teaming Language Models with Language Models,"['Ethan Perez', 'Saffron Huang', 'Francis Song', 'Trevor Cai', 'Roman Ring', 'John Aslanides', 'Amelia Glaese', 'Nat McAleese', 'Geoffrey Irving']",http://arxiv.org/pdf/2202.03286v1.pdf,2022-02-07,"['cs.cl', 'cs.ai', 'cs.cr', 'cs.lg']","  Language Models (LMs) often cannot be deployed because of their potential toharm users in hard-to-predict ways. Prior work identifies harmful behaviorsbefore deployment by using human annotators to hand-write test cases. However,human annotation is expensive, limiting the number and diversity of test cases.In this work, we automatically find cases where a target LM behaves in aharmful way, by generating test cases (""red teaming"") using another LM. Weevaluate the target LM's replies to generated test questions using a classifiertrained to detect offensive content, uncovering tens of thousands of offensivereplies in a 280B parameter LM chatbot. We explore several methods, fromzero-shot generation to reinforcement learning, for generating test cases withvarying levels of diversity and difficulty. Furthermore, we use promptengineering to control LM-generated test cases to uncover a variety of otherharms, automatically finding groups of people that the chatbot discusses inoffensive ways, personal and hospital phone numbers generated as the chatbot'sown contact info, leakage of private training data in generated text, and harmsthat occur over the course of a conversation. Overall, LM-based red teaming isone promising tool (among many needed) for finding and fixing diverse,undesirable LM behaviors before impacting users."
Learning to Prompt for Open-Vocabulary Object Detection with  Vision-Language Model,"['Yu Du', 'Fangyun Wei', 'Zihe Zhang', 'Miaojing Shi', 'Yue Gao', 'Guoqi Li']",http://arxiv.org/pdf/2203.14940v1.pdf,2022-03-28,['cs.cv'],"  Recently, vision-language pre-training shows great potential inopen-vocabulary object detection, where detectors trained on base classes aredevised for detecting new classes. The class text embedding is firstlygenerated by feeding prompts to the text encoder of a pre-trainedvision-language model. It is then used as the region classifier to supervisethe training of a detector. The key element that leads to the success of thismodel is the proper prompt, which requires careful words tuning and ingeniousdesign. To avoid laborious prompt engineering, there are some promptrepresentation learning methods being proposed for the image classificationtask, which however can only be sub-optimal solutions when applied to thedetection task. In this paper, we introduce a novel method, detection prompt(DetPro), to learn continuous prompt representations for open-vocabulary objectdetection based on the pre-trained vision-language model. Different from theprevious classification-oriented methods, DetPro has two highlights: 1) abackground interpretation scheme to include the proposals in image backgroundinto the prompt training; 2) a context grading scheme to separate proposals inimage foreground for tailored prompt training. We assemble DetPro with ViLD, arecent state-of-the-art open-world object detector, and conduct experiments onthe LVIS as well as transfer learning on the Pascal VOC, COCO, Objects365datasets. Experimental results show that our DetPro outperforms the baselineViLD in all settings, e.g., +3.4 APbox and +3.0 APmask improvements on thenovel classes of LVIS. Code and models are available athttps://github.com/dyabel/detpro."
No Token Left Behind: Explainability-Aided Image Classification and  Generation,"['Roni Paiss', 'Hila Chefer', 'Lior Wolf']",http://arxiv.org/pdf/2204.04908v2.pdf,2022-04-11,['cs.cv'],"  The application of zero-shot learning in computer vision has beenrevolutionized by the use of image-text matching models. The most notableexample, CLIP, has been widely used for both zero-shot classification andguiding generative models with a text prompt. However, the zero-shot use ofCLIP is unstable with respect to the phrasing of the input text, making itnecessary to carefully engineer the prompts used. We find that this instabilitystems from a selective similarity score, which is based only on a subset of thesemantically meaningful input tokens. To mitigate it, we present a novelexplainability-based approach, which adds a loss term to ensure that CLIPfocuses on all relevant semantic parts of the input, in addition to employingthe CLIP similarity loss used in previous works. When applied to one-shotclassification through prompt engineering, our method yields an improvement inthe recognition rate, without additional training or fine-tuning. Additionally,we show that CLIP guidance of generative models using our method significantlyimproves the generated images. Finally, we demonstrate a novel use of CLIPguidance for text-based image generation with spatial conditioning on objectlocation, by requiring the image explainability heatmap for each object to beconfined to a pre-determined bounding box."
On Measuring Social Biases in Prompt-Based Multi-Task Learning,"['Afra Feyza Akyürek', 'Sejin Paik', 'Muhammed Yusuf Kocyigit', 'Seda Akbiyik', 'Şerife Leman Runyun', 'Derry Wijaya']",http://arxiv.org/pdf/2205.11605v1.pdf,2022-05-23,"['cs.cl', 'cs.cy']","  Large language models trained on a mixture of NLP tasks that are convertedinto a text-to-text format using prompts, can generalize into novel forms oflanguage and handle novel tasks. A large body of work within prompt engineeringattempts to understand the effects of input forms and prompts in achievingsuperior performance. We consider an alternative measure and inquire whetherthe way in which an input is encoded affects social biases promoted in outputs.In this paper, we study T0, a large-scale multi-task text-to-text languagemodel trained using prompt-based learning. We consider two different forms ofsemantically equivalent inputs: question-answer format and premise-hypothesisformat. We use an existing bias benchmark for the former BBQ and create thefirst bias benchmark in natural language inference BBNLI with hand-writtenhypotheses while also converting each benchmark into the other form. Theresults on two benchmarks suggest that given two different formulations ofessentially the same input, T0 conspicuously acts more biased in questionanswering form, which is seen during training, compared to premise-hypothesisform which is unlike its training examples. Code and data are released underhttps://github.com/feyzaakyurek/bbnli."
OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal  Regression,"['Wanhua Li', 'Xiaoke Huang', 'Zheng Zhu', 'Yansong Tang', 'Xiu Li', 'Jie Zhou', 'Jiwen Lu']",http://arxiv.org/pdf/2206.02338v2.pdf,2022-06-06,['cs.cv'],"  This paper presents a language-powered paradigm for ordinal regression.Existing methods usually treat each rank as a category and employ a set ofweights to learn these concepts. These methods are easy to overfit and usuallyattain unsatisfactory performance as the learned concepts are mainly derivedfrom the training set. Recent large pre-trained vision-language models likeCLIP have shown impressive performance on various visual tasks. In this paper,we propose to learn the rank concepts from the rich semantic CLIP latent space.Specifically, we reformulate this task as an image-language matching problemwith a contrastive objective, which regards labels as text and obtains alanguage prototype from a text encoder for each rank. While prompt engineeringfor CLIP is extremely time-consuming, we propose OrdinalCLIP, a differentiableprompting method for adapting CLIP for ordinal regression. OrdinalCLIP consistsof learnable context tokens and learnable rank embeddings; The learnable rankembeddings are constructed by explicitly modeling numerical continuity,resulting in well-ordered, compact language prototypes in the CLIP space. Oncelearned, we can only save the language prototypes and discard the huge languagemodel, resulting in zero additional computational overhead compared with thelinear head counterpart. Experimental results show that our paradigm achievescompetitive performance in general ordinal regression tasks, and gainsimprovements in few-shot and distribution shift settings for age estimation.The code is available at https://github.com/xk-huang/OrdinalCLIP."
P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with  Point-to-Pixel Prompting,"['Ziyi Wang', 'Xumin Yu', 'Yongming Rao', 'Jie Zhou', 'Jiwen Lu']",http://arxiv.org/pdf/2208.02812v2.pdf,2022-08-04,"['cs.cv', 'cs.ai', 'cs.lg']","  Nowadays, pre-training big models on large-scale datasets has become acrucial topic in deep learning. The pre-trained models with high representationability and transferability achieve a great success and dominate manydownstream tasks in natural language processing and 2D vision. However, it isnon-trivial to promote such a pretraining-tuning paradigm to the 3D vision,given the limited training data that are relatively inconvenient to collect. Inthis paper, we provide a new perspective of leveraging pre-trained 2D knowledgein 3D domain to tackle this problem, tuning pre-trained image models with thenovel Point-to-Pixel prompting for point cloud analysis at a minor parametercost. Following the principle of prompting engineering, we transform pointclouds into colorful images with geometry-preserved projection andgeometry-aware coloring to adapt to pre-trained image models, whose weights arekept frozen during the end-to-end optimization of point cloud analysis tasks.We conduct extensive experiments to demonstrate that cooperating with ourproposed Point-to-Pixel Prompting, better pre-trained image model will lead toconsistently better performance in 3D vision. Enjoying prosperous developmentfrom image pre-training field, our method attains 89.3% accuracy on the hardestsetting of ScanObjectNN, surpassing conventional point cloud models with muchfewer trainable parameters. Our framework also exhibits very competitiveperformance on ModelNet classification and ShapeNet Part Segmentation. Code isavailable at https://github.com/wangzy22/P2P."
Unsupervised Hashing with Semantic Concept Mining,"['Rong-Cheng Tu', 'Xian-Ling Mao', 'Kevin Qinghong Lin', 'Chengfei Cai', 'Weize Qin', 'Hongfa Wang', 'Wei Wei', 'Heyan Huang']",http://arxiv.org/pdf/2209.11475v1.pdf,2022-09-23,"['cs.cv', 'cs.ir']","  Recently, to improve the unsupervised image retrieval performance, plenty ofunsupervised hashing methods have been proposed by designing a semanticsimilarity matrix, which is based on the similarities between image featuresextracted by a pre-trained CNN model. However, most of these methods tend toignore high-level abstract semantic concepts contained in images. Intuitively,concepts play an important role in calculating the similarity among images. Inreal-world scenarios, each image is associated with some concepts, and thesimilarity between two images will be larger if they share more identicalconcepts. Inspired by the above intuition, in this work, we propose a novelUnsupervised Hashing with Semantic Concept Mining, called UHSCM, whichleverages a VLP model to construct a high-quality similarity matrix.Specifically, a set of randomly chosen concepts is first collected. Then, byemploying a vision-language pretraining (VLP) model with the prompt engineeringwhich has shown strong power in visual representation learning, the set ofconcepts is denoised according to the training images. Next, the proposedmethod UHSCM applies the VLP model with prompting again to mine the conceptdistribution of each image and construct a high-quality semantic similaritymatrix based on the mined concept distributions. Finally, with the semanticsimilarity matrix as guiding information, a novel hashing loss with a modifiedcontrastive loss based regularization item is proposed to optimize the hashingnetwork. Extensive experiments on three benchmark datasets show that theproposed method outperforms the state-of-the-art baselines in the imageretrieval task."
Robust Preference Learning for Storytelling via Contrastive  Reinforcement Learning,"['Louis Castricato', 'Alexander Havrilla', 'Shahbuland Matiana', 'Michael Pieler', 'Anbang Ye', 'Ian Yang', 'Spencer Frazier', 'Mark Riedl']",http://arxiv.org/pdf/2210.07792v2.pdf,2022-10-14,['cs.cl'],"  Controlled automated story generation seeks to generate natural languagestories satisfying constraints from natural language critiques or preferences.Existing methods to control for story preference utilize prompt engineeringwhich is labor intensive and often inconsistent. They may also uselogit-manipulation methods which require annotated datasets to exist for thedesired attributes. To address these issues, we first train a contrastivebi-encoder model to align stories with corresponding human critiques, namedCARP, building a general purpose preference model. This is subsequently used asa reward function to fine-tune a generative language model via reinforcementlearning. However, simply fine-tuning a generative language model with acontrastive reward model does not always reliably result in a story generationsystem capable of generating stories that meet user preferences. To increasestory generation robustness we further fine-tune the contrastive reward modelusing a prompt-learning technique. A human participant study is then conductedcomparing generations from our full system, ablations, and two baselines. Weshow that the full fine-tuning pipeline results in a story generator preferredover a LLM 20x as large as well as logit-based methods. This motivates the useof contrastive learning for general purpose human preference modeling."
Towards Equitable Representation in Text-to-Image Synthesis Models with  the Cross-Cultural Understanding Benchmark (CCUB) Dataset,"['Zhixuan Liu', 'Youeun Shin', 'Beverley-Claire Okogwu', 'Youngsik Yun', 'Lia Coleman', 'Peter Schaldenbrand', 'Jihie Kim', 'Jean Oh']",http://arxiv.org/pdf/2301.12073v2.pdf,2023-01-28,['cs.cv'],"  It has been shown that accurate representation in media improves thewell-being of the people who consume it. By contrast, inaccuraterepresentations can negatively affect viewers and lead to harmful perceptionsof other cultures. To achieve inclusive representation in generated images, wepropose a culturally-aware priming approach for text-to-image synthesis using asmall but culturally curated dataset that we collected, known here asCross-Cultural Understanding Benchmark (CCUB) Dataset, to fight the biasprevalent in giant datasets. Our proposed approach is comprised of twofine-tuning techniques: (1) Adding visual context via fine-tuning a pre-trainedtext-to-image synthesis model, Stable Diffusion, on the CCUB text-image pairs,and (2) Adding semantic context via automated prompt engineering using thefine-tuned large language model, GPT-3, trained on our CCUB culturally-awaretext data. CCUB dataset is curated and our approach is evaluated by people whohave a personal relationship with that particular culture. Our experimentsindicate that priming using both text and image is effective in improving thecultural relevance and decreasing the offensiveness of generated images whilemaintaining quality."
Trash to Treasure: Using text-to-image models to inform the design of  physical artefacts,"['Amy Smith', 'Hope Schroeder', 'Ziv Epstein', 'Michael Cook', 'Simon Colton', 'Andrew Lippman']",http://arxiv.org/pdf/2302.00561v1.pdf,2023-02-01,['cs.ai'],"  Text-to-image generative models have recently exploded in popularity andaccessibility. Yet so far, use of these models in creative tasks that bridgethe 2D digital world and the creation of physical artefacts has beenunderstudied. We conduct a pilot study to investigate if and how text-to-imagemodels can be used to assist in upstream tasks within the creative process,such as ideation and visualization, prior to a sculpture-making activity.Thirty participants selected sculpture-making materials and generated threeimages using the Stable Diffusion text-to-image generator, each with textprompts of their choice, with the aim of informing and then creating a physicalsculpture. The majority of participants (23/30) reported that the generatedimages informed their sculptures, and 28/30 reported interest in usingtext-to-image models to help them in a creative task in the future. We identifyseveral prompt engineering strategies and find that a participant's promptingstrategy relates to their stage in the creative process. We discuss how ourfindings can inform support for users at different stages of the design processand for using text-to-image models for physical artefact design."
"Chat2VIS: Generating Data Visualisations via Natural Language using  ChatGPT, Codex and GPT-3 Large Language Models","['Paula Maddigan', 'Teo Susnjak']",http://arxiv.org/pdf/2302.02094v2.pdf,2023-02-04,['cs.hc'],"  The field of data visualisation has long aimed to devise solutions forgenerating visualisations directly from natural language text. Research inNatural Language Interfaces (NLIs) has contributed towards the development ofsuch techniques. However, the implementation of workable NLIs has always beenchallenging due to the inherent ambiguity of natural language, as well as inconsequence of unclear and poorly written user queries which pose problems forexisting language models in discerning user intent. Instead of pursuing theusual path of developing new iterations of language models, this study uniquelyproposes leveraging the advancements in pre-trained large language models(LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directlyinto code for appropriate visualisations. This paper presents a novel system,Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrateshow, with effective prompt engineering, the complex problem of languageunderstanding can be solved more efficiently, resulting in simpler and moreaccurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMstogether with the proposed prompts offer a reliable approach to renderingvisualisations from natural language queries, even when queries are highlymisspecified and underspecified. This solution also presents a significantreduction in costs for the development of NLI systems, while attaining greatervisualisation inference abilities compared to traditional NLP approaches thatuse hand-crafted grammar rules and tailored models. This study also presentshow LLM prompts can be constructed in a way that preserves data security andprivacy while being generalisable to different datasets. This work compares theperformance of GPT-3, Codex and ChatGPT across a number of case studies andcontrasts the performances with prior studies."
CHiLS: Zero-Shot Image Classification with Hierarchical Label Sets,"['Zachary Novack', 'Julian McAuley', 'Zachary C. Lipton', 'Saurabh Garg']",http://arxiv.org/pdf/2302.02551v3.pdf,2023-02-06,"['cs.cv', 'cs.lg']","  Open vocabulary models (e.g. CLIP) have shown strong performance on zero-shotclassification through their ability generate embeddings for each class basedon their (natural language) names. Prior work has focused on improving theaccuracy of these models through prompt engineering or by incorporating a smallamount of labeled downstream data (via finetuning). However, there has beenlittle focus on improving the richness of the class names themselves, which canpose issues when class labels are coarsely-defined and are uninformative. Wepropose Classification with Hierarchical Label Sets (or CHiLS), an alternativestrategy for zero-shot classification specifically designed for datasets withimplicit semantic hierarchies. CHiLS proceeds in three steps: (i) for eachclass, produce a set of subclasses, using either existing label hierarchies orby querying GPT-3; (ii) perform the standard zero-shot CLIP procedure as thoughthese subclasses were the labels of interest; (iii) map the predicted subclassback to its parent to produce the final prediction. Across numerous datasetswith underlying hierarchical structure, CHiLS leads to improved accuracy insituations both with and without ground-truth hierarchical information. CHiLSis simple to implement within existing zero-shot pipelines and requires noadditional training cost. Code is available at:https://github.com/acmi-lab/CHILS."
Prompt Stealing Attacks Against Text-to-Image Generation Models,"['Xinyue Shen', 'Yiting Qu', 'Michael Backes', 'Yang Zhang']",http://arxiv.org/pdf/2302.09923v1.pdf,2023-02-20,"['cs.cr', 'cs.lg']","  Text-to-Image generation models have revolutionized the artwork designprocess and enabled anyone to create high-quality images by entering textdescriptions called prompts. Creating a high-quality prompt that consists of asubject and several modifiers can be time-consuming and costly. In consequence,a trend of trading high-quality prompts on specialized marketplaces hasemerged. In this paper, we propose a novel attack, namely prompt stealingattack, which aims to steal prompts from generated images by text-to-imagegeneration models. Successful prompt stealing attacks direct violate theintellectual property and privacy of prompt engineers and also jeopardize thebusiness model of prompt trading marketplaces. We first perform a large-scaleanalysis on a dataset collected by ourselves and show that a successful promptstealing attack should consider a prompt's subject as well as its modifiers. Wethen propose the first learning-based prompt stealing attack, PromptStealer,and demonstrate its superiority over two baseline methods quantitatively andqualitatively. We also make some initial attempts to defend PromptStealer. Ingeneral, our study uncovers a new attack surface in the ecosystem created bythe popular text-to-image generation models. We hope our results can help tomitigate the threat. To facilitate research in this field, we will share ourdataset and code with the community."
Controlled and Conditional Text to Image Generation with Diffusion Prior,"['Pranav Aggarwal', 'Hareesh Ravi', 'Naveen Marri', 'Sachin Kelkar', 'Fengbin Chen', 'Vinh Khuc', 'Midhun Harikumar', 'Ritiz Tambi', 'Sudharshan Reddy Kakumanu', 'Purvak Lapsiya', 'Alvin Ghouas', 'Sarah Saber', 'Malavika Ramprasad', 'Baldo Faieta', 'Ajinkya Kale']",http://arxiv.org/pdf/2302.11710v2.pdf,2023-02-23,['cs.cv'],"  Denoising Diffusion models have shown remarkable performance in generatingdiverse, high quality images from text. Numerous techniques have been proposedon top of or in alignment with models like Stable Diffusion and Imagen thatgenerate images directly from text. A lesser explored approach is DALLE-2's twostep process comprising a Diffusion Prior that generates a CLIP image embeddingfrom text and a Diffusion Decoder that generates an image from a CLIP imageembedding. We explore the capabilities of the Diffusion Prior and theadvantages of an intermediate CLIP representation. We observe that DiffusionPrior can be used in a memory and compute efficient way to constrain thegeneration to a specific domain without altering the larger Diffusion Decoder.Moreover, we show that the Diffusion Prior can be trained with additionalconditional information such as color histogram to further control thegeneration. We show quantitatively and qualitatively that the proposedapproaches perform better than prompt engineering for domain specificgeneration and existing baselines for color conditioned generation. We believethat our observations and results will instigate further research into thediffusion prior and uncover more of its capabilities."
EvoPrompting: Language Models for Code-Level Neural Architecture Search,"['Angelica Chen', 'David M. Dohan', 'David R. So']",http://arxiv.org/pdf/2302.14838v3.pdf,2023-02-28,"['cs.ne', 'cs.ai', 'cs.cl', 'cs.lg']","  Given the recent impressive accomplishments of language models (LMs) for codegeneration, we explore the use of LMs as adaptive mutation and crossoveroperators for an evolutionary neural architecture search (NAS) algorithm. WhileNAS still proves too difficult a task for LMs to succeed at solely throughprompting, we find that the combination of evolutionary prompt engineering withsoft prompt-tuning, a method we term EvoPrompting, consistently finds diverseand high performing models. We first demonstrate that EvoPrompting is effectiveon the computationally efficient MNIST-1D dataset, where EvoPrompting producesconvolutional architecture variants that outperform both those designed byhuman experts and naive few-shot prompting in terms of accuracy and model size.We then apply our method to searching for graph neural networks on the CLRSAlgorithmic Reasoning Benchmark, where EvoPrompting is able to design novelarchitectures that outperform current state-of-the-art models on 21 out of 30algorithmic reasoning tasks while maintaining similar model size. EvoPromptingis successful at designing accurate and efficient neural network architecturesacross a variety of machine learning tasks, while also being general enough foreasy adaptation to other tasks beyond neural network design."
Extracting Accurate Materials Data from Research Papers with  Conversational Language Models and Prompt Engineering,"['Maciej P. Polak', 'Dane Morgan']",http://arxiv.org/pdf/2303.05352v2.pdf,2023-03-07,"['cs.cl', 'cond-mat.mtrl-sci']","  There has been a growing effort to replace hand extraction of data fromresearch papers with automated data extraction based on natural languageprocessing, language models, and recently, large language models (LLMs).Although these methods enable efficient extraction of data from large sets ofresearch papers, they require a significant amount of up-front effort,expertise, and coding. In this work we propose the ChatExtract method that canfully automate very accurate data extraction with minimal initial effort andbackground, using an advanced conversational LLM. ChatExtract consists of a setof engineered prompts applied to a conversational LLM that both identifysentences with data, extract that data, and assure the data's correctnessthrough a series of follow-up questions. These follow-up questions largelyovercome known issues with LLMs providing factually inaccurate responses.ChatExtract can be applied with any conversational LLMs and yields very highquality data extraction. In tests on materials data we find precision andrecall both close to 90% from the best conversational LLMs, like ChatGPT-4. Wedemonstrate that the exceptional performance is enabled by the informationretention in a conversational model combined with purposeful redundancy andintroducing uncertainty through follow-up prompts. These results suggest thatapproaches similar to ChatExtract, due to their simplicity, transferability,and accuracy are likely to become powerful tools for data extraction in thenear future. Finally, databases for critical cooling rates of metallic glassesand yield strengths of high entropy alloys are developed using ChatExtract."
On Codex Prompt Engineering for OCL Generation: An Empirical Study,"['Seif Abukhalaf', 'Mohammad Hamdaqa', 'Foutse Khomh']",http://arxiv.org/pdf/2303.16244v1.pdf,2023-03-28,"['cs.se', 'cs.ai']","  The Object Constraint Language (OCL) is a declarative language that addsconstraints and object query expressions to MOF models. Despite its potentialto provide precision and conciseness to UML models, the unfamiliar syntax ofOCL has hindered its adoption. Recent advancements in LLMs, such as GPT-3, haveshown their capability in many NLP tasks, including semantic parsing and textgeneration. Codex, a GPT-3 descendant, has been fine-tuned on publiclyavailable code from GitHub and can generate code in many programming languages.We investigate the reliability of OCL constraints generated by Codex fromnatural language specifications. To achieve this, we compiled a dataset of 15UML models and 168 specifications and crafted a prompt template with slots topopulate with UML information and the target task, using both zero- andfew-shot learning methods. By measuring the syntactic validity and executionaccuracy metrics of the generated OCL constraints, we found that enriching theprompts with UML information and enabling few-shot learning increases thereliability of the generated OCL constraints. Furthermore, the results reveal aclose similarity based on sentence embedding between the generated OCLconstraints and the human-written ones in the ground truth, implying a level ofclarity and understandability in the generated OCL constraints by Codex."
Ten Quick Tips for Harnessing the Power of ChatGPT/GPT-4 in  Computational Biology,"['Tiago Lubiana', 'Rafael Lopes', 'Pedro Medeiros', 'Juan Carlo Silva', 'Andre Nicolau Aquime Goncalves', 'Vinicius Maracaja-Coutinho', 'Helder I Nakaya']",http://arxiv.org/pdf/2303.16429v1.pdf,2023-03-29,"['q-bio.ot', '92-04']","  The rise of advanced chatbots, such as ChatGPT, has sparked curiosity in thescientific community. ChatGPT is a general-purpose chatbot powered by largelanguage models (LLMs) GPT-3.5 and GPT-4, with the potential to impact numerousfields, including computational biology. In this article, we offer ten tipsbased on our experience with ChatGPT to assist computational biologists inoptimizing their workflows. We have collected relevant prompts and reviewed thenascent literature in the field, compiling tips we project to remain pertinentfor future ChatGPT and LLM iterations, ranging from code refactoring toscientific writing to prompt engineering. We hope our work will helpbioinformaticians to complement their workflows while staying aware of thevarious implications of using this technology. Additionally, to track new andcreative applications for bioinformatics tools such as ChatGPT, we haveestablished a GitHub repository athttps://github.com/csbl-br/awesome-compbio-chatgpt. Our belief is that ethicaladherence to ChatGPT and other LLMs will increase the efficiency ofcomputational biologists, ultimately advancing the pace of scientific discoveryin the life sciences."
Humans in Humans Out: On GPT Converging Toward Common Sense in both  Success and Failure,"['Philipp Koralus', 'Vincent Wang-Maścianica']",http://arxiv.org/pdf/2303.17276v1.pdf,2023-03-30,"['cs.ai', 'cs.cl', 'cs.hc', 'cs.lg', '00, 68', 'i.2.0; i.2.6']","  Increase in computational scale and fine-tuning has seen a dramaticimprovement in the quality of outputs of large language models (LLMs) like GPT.Given that both GPT-3 and GPT-4 were trained on large quantities ofhuman-generated text, we might ask to what extent their outputs reflectpatterns of human thinking, both for correct and incorrect cases. The EroteticTheory of Reason (ETR) provides a symbolic generative model of both humansuccess and failure in thinking, across propositional, quantified, andprobabilistic reasoning, as well as decision-making. We presented GPT-3,GPT-3.5, and GPT-4 with 61 central inference and judgment problems from arecent book-length presentation of ETR, consisting of experimentally verifieddata-points on human judgment and extrapolated data-points predicted by ETR,with correct inference patterns as well as fallacies and framing effects (theETR61 benchmark). ETR61 includes classics like Wason's card task, illusoryinferences, the decoy effect, and opportunity-cost neglect, among others. GPT-3showed evidence of ETR-predicted outputs for 59% of these examples, rising to77% in GPT-3.5 and 75% in GPT-4. Remarkably, the production of human-likefallacious judgments increased from 18% in GPT-3 to 33% in GPT-3.5 and 34% inGPT-4. This suggests that larger and more advanced LLMs may develop a tendencytoward more human-like mistakes, as relevant thought patterns are inherent inhuman-produced training data. According to ETR, the same fundamental patternsare involved both in successful and unsuccessful ordinary reasoning, so thatthe ""bad"" cases could paradoxically be learned from the ""good"" cases. Wefurther present preliminary evidence that ETR-inspired prompt engineering couldreduce instances of these mistakes."
Pair Programming with Large Language Models for Sampling and Estimation  of Copulas,['Jan Górecki'],http://arxiv.org/pdf/2303.18116v1.pdf,2023-03-31,"['cs.cl', 'stat.co', '65c60, 68n19, 68t50']","  Without writing a single line of code by a human, an example Monte Carlosimulation based application for stochastic dependence modeling with copulas isdeveloped using a state-of-the-art large language model (LLM) fine-tuned forconversations. This includes interaction with ChatGPT in natural language andusing mathematical formalism, which, under careful supervision by ahuman-expert, led to producing a working code in MATLAB, Python and R forsampling from a given copula model, evaluation of the model's density,performing maximum likelihood estimation, optimizing the code for parallelcomputing for CPUs as well as for GPUs, and visualization of the computedresults. In contrast to other emerging studies that assess the accuracy of LLMslike ChatGPT on tasks from a selected area, this work rather investigates wayshow to achieve a successful solution of a standard statistical task in acollaboration of a human-expert and artificial intelligence (AI). Particularly,through careful prompt engineering, we separate successful solutions generatedby ChatGPT from unsuccessful ones, resulting in a comprehensive list of relatedpros and cons. It is demonstrated that if the typical pitfalls are avoided, wecan substantially benefit from collaborating with an AI partner. For example,we show that if ChatGPT is not able to provide a correct solution due to a lackof or incorrect knowledge, the human-expert can feed it with the correctknowledge, e.g., in the form of mathematical theorems and formulas, and make itto apply the gained knowledge in order to provide a solution that is correct.Such ability presents an attractive opportunity to achieve a programmedsolution even for users with rather limited knowledge of programmingtechniques."
"Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its  Applications, Advantages, Limitations, and Future Directions in Natural  Language Processing",['Walid Hariri'],http://arxiv.org/pdf/2304.02017v6.pdf,2023-03-27,['cs.cl'],"  Large language models have revolutionized the field of artificialintelligence and have been used in various applications. Among these models,ChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,it stands out as a powerful tool that has been widely adopted. ChatGPT has beensuccessfully applied in numerous areas, including chatbots, content generation,language translation, personalized recommendations, and even medical diagnosisand treatment. Its success in these applications can be attributed to itsability to generate human-like responses, understand natural language, andadapt to different contexts. Its versatility and accuracy make it a powerfultool for natural language processing (NLP). However, there are also limitationsto ChatGPT, such as its tendency to produce biased responses and its potentialto perpetuate harmful language patterns. This article provides a comprehensiveoverview of ChatGPT, its applications, advantages, and limitations.Additionally, the paper emphasizes the importance of ethical considerationswhen using this robust tool in real-world scenarios. Finally, This papercontributes to ongoing discussions surrounding artificial intelligence and itsimpact on vision and NLP domains by providing insights into prompt engineeringtechniques."
TagGPT: Large Language Models are Zero-shot Multimodal Taggers,"['Chen Li', 'Yixiao Ge', 'Jiayong Mao', 'Dian Li', 'Ying Shan']",http://arxiv.org/pdf/2304.03022v1.pdf,2023-04-06,['cs.ir'],"  Tags are pivotal in facilitating the effective distribution of multimediacontent in various applications in the contemporary Internet era, such assearch engines and recommendation systems. Recently, large language models(LLMs) have demonstrated impressive capabilities across a wide range of tasks.In this work, we propose TagGPT, a fully automated system capable of tagextraction and multimodal tagging in a completely zero-shot fashion. Our coreinsight is that, through elaborate prompt engineering, LLMs are able to extractand reason about proper tags given textual clues of multimodal data, e.g., OCR,ASR, title, etc. Specifically, to automatically build a high-quality tag setthat reflects user intent and interests for a specific application, TagGPTpredicts large-scale candidate tags from a series of raw data via promptingLLMs, filtered with frequency and semantics. Given a new entity that needstagging for distribution, TagGPT introduces two alternative options forzero-shot tagging, i.e., a generative method with late semantic matching withthe tag set, and another selective method with early matching in prompts. It iswell noticed that TagGPT provides a system-level solution based on a modularframework equipped with a pre-trained LLM (GPT-3.5 used here) and a sentenceembedding model (SimCSE used here), which can be seamlessly replaced with anymore advanced one you want. TagGPT is applicable for various modalities of datain modern social media and showcases strong generalization ability to a widerange of applications. We evaluate TagGPT on publicly available datasets, i.e.,Kuaishou and Food.com, and demonstrate the effectiveness of TagGPT compared toexisting hashtags and off-the-shelf taggers. Project page:https://github.com/TencentARC/TagGPT."
Towards Interpretable Mental Health Analysis with Large Language Models,"['Kailai Yang', 'Shaoxiong Ji', 'Tianlin Zhang', 'Qianqian Xie', 'Ziyan Kuang', 'Sophia Ananiadou']",http://arxiv.org/pdf/2304.03347v4.pdf,2023-04-06,['cs.cl'],"  The latest large language models (LLMs) such as ChatGPT, exhibit strongcapabilities in automated mental health analysis. However, existing relevantstudies bear several limitations, including inadequate evaluations, lack ofprompting strategies, and ignorance of exploring LLMs for explainability. Tobridge these gaps, we comprehensively evaluate the mental health analysis andemotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explorethe effects of different prompting strategies with unsupervised and distantlysupervised emotional information. Based on these prompts, we explore LLMs forinterpretable mental health analysis by instructing them to generateexplanations for each of their decisions. We convey strict human evaluations toassess the quality of the generated explanations, leading to a novel datasetwith 163 human-assessed explanations. We benchmark existing automaticevaluation metrics on this dataset to guide future related works. According tothe results, ChatGPT shows strong in-context learning ability but still has asignificant gap with advanced task-specific methods. Careful prompt engineeringwith emotional cues and expert-written few-shot examples can also effectivelyimprove performance on mental health analysis. In addition, ChatGPT generatesexplanations that approach human performance, showing its great potential inexplainable mental health analysis."
Low-code LLM: Visual Programming over LLMs,"['Yuzhe Cai', 'Shaoguang Mao', 'Wenshan Wu', 'Zehua Wang', 'Yaobo Liang', 'Tao Ge', 'Chenfei Wu', 'Wang You', 'Ting Song', 'Yan Xia', 'Jonathan Tien', 'Nan Duan']",http://arxiv.org/pdf/2304.08103v2.pdf,2023-04-17,"['cs.cl', 'cs.hc']","  Effectively utilizing LLMs for complex tasks is challenging, often involvinga time-consuming and uncontrollable prompt engineering process. This paperintroduces a novel human-LLM interaction framework, Low-code LLM. Itincorporates six types of simple low-code visual programming interactions, allsupported by clicking, dragging, or text editing, to achieve more controllableand stable responses. Through visual interaction with a graphical userinterface, users can incorporate their ideas into the workflow without writingtrivial prompts. The proposed Low-code LLM framework consists of a Planning LLMthat designs a structured planning workflow for complex tasks, which can becorrespondingly edited and confirmed by users through low-code visualprogramming operations, and an Executing LLM that generates responses followingthe user-confirmed workflow. We highlight three advantages of the low-code LLM:controllable generation results, user-friendly human-LLM interaction, andbroadly applicable scenarios. We demonstrate its benefits using four typicalapplications. By introducing this approach, we aim to bridge the gap betweenhumans and LLMs, enabling more effective and efficient utilization of LLMs forcomplex tasks. Our system will be soon publicly available at LowCodeLLM."
Inducing anxiety in large language models increases exploration and bias,"['Julian Coda-Forno', 'Kristin Witte', 'Akshay K. Jagadish', 'Marcel Binz', 'Zeynep Akata', 'Eric Schulz']",http://arxiv.org/pdf/2304.11111v1.pdf,2023-04-21,"['cs.cl', 'cs.ai', 'cs.lg']","  Large language models are transforming research on machine learning whilegalvanizing public debates. Understanding not only when these models work welland succeed but also why they fail and misbehave is of great societalrelevance. We propose to turn the lens of computational psychiatry, a frameworkused to computationally describe and modify aberrant behavior, to the outputsproduced by these models. We focus on the Generative Pre-Trained Transformer3.5 and subject it to tasks commonly studied in psychiatry. Our results showthat GPT-3.5 responds robustly to a common anxiety questionnaire, producinghigher anxiety scores than human subjects. Moreover, GPT-3.5's responses can bepredictably changed by using emotion-inducing prompts. Emotion-induction notonly influences GPT-3.5's behavior in a cognitive task measuring exploratorydecision-making but also influences its behavior in a previously-establishedtask measuring biases such as racism and ableism. Crucially, GPT-3.5 shows astrong increase in biases when prompted with anxiety-inducing text. Thus, it islikely that how prompts are communicated to large language models has a stronginfluence on their behavior in applied settings. These results progress ourunderstanding of prompt engineering and demonstrate the usefulness of methodstaken from computational psychiatry for studying the capable algorithms towhich we increasingly delegate authority and autonomy."
Is ChatGPT the Ultimate Programming Assistant -- How far is it?,"['Haoye Tian', 'Weiqi Lu', 'Tsz On Li', 'Xunzhu Tang', 'Shing-Chi Cheung', 'Jacques Klein', 'Tegawendé F. Bissyandé']",http://arxiv.org/pdf/2304.11938v2.pdf,2023-04-24,"['cs.se', 'cs.ai']","  Recently, the ChatGPT LLM has received great attention: it can be used as abot for discussing source code, prompting it to suggest changes, providedescriptions or even generate code. Typical demonstrations generally focus onexisting benchmarks, which may have been used in model training (i.e., dataleakage). To assess the feasibility of using an LLM as a useful assistant botfor programmers, we must assess its realistic capabilities on unseen problemsas well as its capabilities on various tasks. In this paper, we present anempirical study of ChatGPT's potential as a fully automated programmingassistant, focusing on the tasks of code generation, program repair, and codesummariziation. The study investigates ChatGPT's performance on commonprogramming problems and compares it with state-of-the-art approaches on twobenchmarks. Among several findings, our study shows that ChatGPT is effectivein dealing with common programming problems. However, our experiments alsoreveal limitations in terms of its attention span: detailed descriptions willconstrain the focus of ChatGPT and prevent it from leveraging its vastknowledge to solve the actual problem. Surprisingly, we have identified theability of ChatGPT to reason the original intention of the code. We expectfuture work to build on this insight for dealing with the open question of theoracle problem. Our findings contribute interesting insights to the developmentof LLMs for programming assistance, notably by demonstrating the importance ofprompt engineering, and providing a better understanding of ChatGPT's practicalapplications for software engineering."
Framing the News:From Human Perception to Large Language Model  Inferences,"['David Alonso del Barrio', 'Daniel Gatica-Perez']",http://arxiv.org/pdf/2304.14456v1.pdf,2023-04-27,"['cs.cl', 'cs.hc']","  Identifying the frames of news is important to understand the articles'vision, intention, message to be conveyed, and which aspects of the news areemphasized. Framing is a widely studied concept in journalism, and has emergedas a new topic in computing, with the potential to automate processes andfacilitate the work of journalism professionals. In this paper, we study thisissue with articles related to the Covid-19 anti-vaccine movement. First, tounderstand the perspectives used to treat this theme, we developed a protocolfor human labeling of frames for 1786 headlines of No-Vax movement articles ofEuropean newspapers from 5 countries. Headlines are key units in the writtenpress, and worth of analysis as many people only read headlines (or use them toguide their decision for further reading.) Second, considering advances inNatural Language Processing (NLP) with large language models, we investigatedtwo approaches for frame inference of news headlines: first with a GPT-3.5fine-tuning approach, and second with GPT-3.5 prompt-engineering. Our workcontributes to the study and analysis of the performance that these models haveto facilitate journalistic tasks like classification of frames, whileunderstanding whether the models are able to replicate human perception in theidentification of these frames."
"ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal,  Causal, and Discourse Relations","['Chunkit Chan', 'Jiayang Cheng', 'Weiqi Wang', 'Yuxin Jiang', 'Tianqing Fang', 'Xin Liu', 'Yangqiu Song']",http://arxiv.org/pdf/2304.14827v2.pdf,2023-04-28,['cs.cl'],"  This paper aims to quantitatively evaluate the performance of ChatGPT, aninteractive large language model, on inter-sentential relations such astemporal relations, causal relations, and discourse relations. Given ChatGPT'spromising performance across various tasks, we conduct extensive evaluations onthe whole test sets of 13 datasets, including temporal and causal relations,PDTB2.0-based and dialogue-based discourse relations, and downstreamapplications on discourse understanding. To achieve reliable results, we adoptthree tailored prompt templates for each task, including the zero-shot prompttemplate, zero-shot prompt engineering (PE) template, and in-context learning(ICL) prompt template, to establish the initial baseline scores for all popularsentence-pair relation classification tasks for the first time. We find thatChatGPT exhibits strong performance in detecting and reasoning about causalrelations, while it may not be proficient in identifying the temporal orderbetween two events. It can recognize most discourse relations with existingexplicit discourse connectives, but the implicit discourse relation stillremains a challenging task. Meanwhile, ChatGPT performs poorly in the dialoguediscourse parsing task that requires structural understanding in a dialoguebefore being aware of the discourse relation."
Large Language Models Can Be Used To Effectively Scale Spear Phishing  Campaigns,['Julian Hazell'],http://arxiv.org/pdf/2305.06972v2.pdf,2023-05-11,"['cs.cy', 'cs.ai', 'cs.cr']","  Recent progress in artificial intelligence (AI), particularly in the domainof large language models (LLMs), has resulted in powerful and versatiledual-use systems. Indeed, cognition can be put towards a wide variety of tasks,some of which can result in harm. This study investigates how LLMs can be usedfor spear phishing, a form of cybercrime that involves manipulating targetsinto divulging sensitive information. I first explore LLMs' ability to assistwith the reconnaissance and message generation stages of a successful spearphishing attack, where I find that advanced LLMs are capable of improvingcybercriminals' efficiency during these stages. To explore how LLMs can be usedto scale spear phishing campaigns, I then create unique spear phishing messagesfor over 600 British Members of Parliament using OpenAI's GPT-3.5 and GPT-4models. My findings reveal that these messages are not only realistic but alsocost-effective, with each email costing only a fraction of a cent to generate.Next, I demonstrate how basic prompt engineering can circumvent safeguardsinstalled in LLMs by the reinforcement learning from human feedback fine-tuningprocess, highlighting the need for more robust governance interventions aimedat preventing misuse. To address these evolving risks, I propose two potentialsolutions: structured access schemes, such as application programminginterfaces, and LLM-based defensive systems."
Sensitivity and Robustness of Large Language Models to Prompt Template  in Japanese Text Classification Tasks,"['Chengguang Gan', 'Tatsunori Mori']",http://arxiv.org/pdf/2305.08714v2.pdf,2023-05-15,"['cs.cl', 'cs.ai']","  Prompt engineering relevance research has seen a notable surge in recentyears, primarily driven by advancements in pre-trained language models andlarge language models. However, a critical issue has been identified withinthis domain: the inadequate of sensitivity and robustness of these modelstowards Prompt Templates, particularly in lesser-studied languages such asJapanese. This paper explores this issue through a comprehensive evaluation ofseveral representative Large Language Models (LLMs) and a widely-utilizedpre-trained model(PLM). These models are scrutinized using a benchmark datasetin Japanese, with the aim to assess and analyze the performance of the currentmultilingual models in this context. Our experimental results reveal startlingdiscrepancies. A simple modification in the sentence structure of the PromptTemplate led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44.This observation underscores the fact that even the highly performance GPT-4model encounters significant stability issues when dealing with diverseJapanese prompt templates, rendering the consistency of the model's outputresults questionable. In light of these findings, we conclude by proposingpotential research trajectories to further enhance the development andperformance of Large Language Models in their current stage."
Knowledge Graph Completion Models are Few-shot Learners: An Empirical  Study of Relation Labeling in E-commerce with LLMs,"['Jiao Chen', 'Luyi Ma', 'Xiaohan Li', 'Nikhil Thakurdesai', 'Jianpeng Xu', 'Jason H. D. Cho', 'Kaushiki Nag', 'Evren Korpeoglu', 'Sushant Kumar', 'Kannan Achan']",http://arxiv.org/pdf/2305.09858v1.pdf,2023-05-17,"['cs.ir', 'cs.ai', 'cs.cl', 'cs.lg']","  Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce systemperformance by providing structured information about entities and theirrelationships, such as complementary or substitutable relations betweenproducts or product types, which can be utilized in recommender systems.However, relation labeling in KGs remains a challenging task due to the dynamicnature of e-commerce domains and the associated cost of human labor. Recently,breakthroughs in Large Language Models (LLMs) have shown surprising results innumerous natural language processing tasks. In this paper, we conduct anempirical study of LLMs for relation labeling in e-commerce KGs, investigatingtheir powerful learning capabilities in natural language and effectiveness inpredicting relations between product types with limited labeled data. Weevaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets,demonstrating their ability to achieve competitive performance compared tohumans on relation labeling tasks using just 1 to 5 labeled examples perrelation. Additionally, we experiment with different prompt engineeringtechniques to examine their impact on model performance. Our results show thatLLMs significantly outperform existing KG completion models in relationlabeling for e-commerce KGs and exhibit performance strong enough to replacehuman labeling."
VisorGPT: Learning Visual Prior via Generative Pre-Training,"['Jinheng Xie', 'Kai Ye', 'Yudong Li', 'Yuexiang Li', 'Kevin Qinghong Lin', 'Yefeng Zheng', 'Linlin Shen', 'Mike Zheng Shou']",http://arxiv.org/pdf/2305.13777v4.pdf,2023-05-23,['cs.cv'],"  Various stuff and things in visual data possess specific traits, which can belearned by deep neural networks and are implicitly represented as the visualprior, e.g., object location and shape, in the model. Such prior potentiallyimpacts many vision tasks. For example, in conditional image synthesis, spatialconditions failing to adhere to the prior can result in visually inaccuratesynthetic results. This work aims to explicitly learn the visual prior andenable the customization of sampling. Inspired by advances in languagemodeling, we propose to learn Visual prior via Generative Pre-Training, dubbedVisorGPT. By discretizing visual locations of objects, e.g., bounding boxes,human pose, and instance masks, into sequences, VisorGPT can model visual priorthrough likelihood maximization. Besides, prompt engineering is investigated tounify various visual locations and enable customized sampling of sequentialoutputs from the learned prior. Experimental results demonstrate that VisorGPTcan effectively model the visual prior, which can be employed for many visiontasks, such as customizing accurate human pose for conditional image synthesismodels like ControlNet. Code will be released athttps://github.com/Sierkinhane/VisorGPT."
Game of Tones: Faculty detection of GPT-4 generated content in  university assessments,"['Mike Perkins', 'Jasper Roe', 'Darius Postma', 'James McGaughran', 'Don Hickerson']",http://arxiv.org/pdf/2305.18081v1.pdf,2023-05-29,"['cs.cy', 'cs.ai', 'k.4']","  This study explores the robustness of university assessments against the useof Open AI's Generative Pre-Trained Transformer 4 (GPT-4) generated content andevaluates the ability of academic staff to detect its use when supported by theTurnitin Artificial Intelligence (AI) detection tool. The research involvedtwenty-two GPT-4 generated submissions being created and included in theassessment process to be marked by fifteen different faculty members. The studyreveals that although the detection tool identified 91% of the experimentalsubmissions as containing some AI-generated content, the total detected contentwas only 54.8%. This suggests that the use of adversarial techniques regardingprompt engineering is an effective method in evading AI detection tools andhighlights that improvements to AI detection software are needed. Using theTurnitin AI detect tool, faculty reported 54.5% of the experimental submissionsto the academic misconduct process, suggesting the need for increased awarenessand training into these tools. Genuine submissions received a mean score of54.4, whereas AI-generated content scored 52.3, indicating the comparableperformance of GPT-4 in real-life situations. Recommendations include adjustingassessment strategies to make them more resistant to the use of AI tools, usingAI-inclusive assessment where possible, and providing comprehensive trainingprograms for faculty and students. This research contributes to understandingthe relationship between AI-generated content and academic assessment, urgingfurther investigation to preserve academic integrity."
Responsible Task Automation: Empowering Large Language Models as  Responsible Task Automators,"['Zhizheng Zhang', 'Xiaoyi Zhang', 'Wenxuan Xie', 'Yan Lu']",http://arxiv.org/pdf/2306.01242v2.pdf,2023-06-02,"['cs.ai', 'cs.cl']","  The recent success of Large Language Models (LLMs) signifies an impressivestride towards artificial general intelligence. They have shown a promisingprospect in automatically completing tasks upon user instructions, functioningas brain-like coordinators. The associated risks will be revealed as wedelegate an increasing number of tasks to machines for automated completion. Abig question emerges: how can we make machines behave responsibly when helpinghumans automate tasks as personal copilots? In this paper, we explore thisquestion in depth from the perspectives of feasibility, completeness andsecurity. In specific, we present Responsible Task Automation (ResponsibleTA)as a fundamental framework to facilitate responsible collaboration betweenLLM-based coordinators and executors for task automation with three empoweredcapabilities: 1) predicting the feasibility of the commands for executors; 2)verifying the completeness of executors; 3) enhancing the security (e.g., theprotection of users' privacy). We further propose and compare two paradigms forimplementing the first two capabilities. One is to leverage the genericknowledge of LLMs themselves via prompt engineering while the other is to adoptdomain-specific learnable models. Moreover, we introduce a local memorymechanism for achieving the third capability. We evaluate our proposedResponsibleTA on UI task automation and hope it could bring more attentions toensuring LLMs more responsible in diverse scenarios."
A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets  Prompt Engineering,"['Chaoning Zhang', 'Fachrina Dewi Puspitasari', 'Sheng Zheng', 'Chenghao Li', 'Yu Qiao', 'Taegoo Kang', 'Xinru Shan', 'Chenshuang Zhang', 'Caiyan Qin', 'Francois Rameau', 'Lik-Hang Lee', 'Sung-Ho Bae', 'Choong Seon Hong']",http://arxiv.org/pdf/2306.06211v3.pdf,2023-05-12,['cs.cv'],"  Segment anything model (SAM) developed by Meta AI Research has recentlyattracted significant attention. Trained on a large segmentation dataset ofover 1 billion masks, SAM is capable of segmenting any object on a certainimage. In the original SAM work, the authors turned to zero-short transfertasks (like edge detection) for evaluating the performance of SAM. Recently,numerous works have attempted to investigate the performance of SAM in variousscenarios to recognize and segment objects. Moreover, numerous projects haveemerged to show the versatility of SAM as a foundation model by combining itwith other models, like Grounding DINO, Stable Diffusion, ChatGPT, etc. Withthe relevant papers and projects increasing exponentially, it is challengingfor the readers to catch up with the development of SAM. To this end, this workconducts the first yet comprehensive survey on SAM. This is an ongoing projectand we intend to update the manuscript on a regular basis. Therefore, readersare welcome to contact us if they complete new works related to SAM so that wecan include them in our next version."
The economic trade-offs of large language models: A case study,"['Kristen Howell', 'Gwen Christian', 'Pavel Fomitchov', 'Gitit Kehat', 'Julianne Marzulla', 'Leanne Rolston', 'Jadin Tredup', 'Ilana Zimmerman', 'Ethan Selfridge', 'Joseph Bradley']",http://arxiv.org/pdf/2306.07402v1.pdf,2023-06-08,"['cs.cl', 'cs.ai']","  Contacting customer service via chat is a common practice. Because employingcustomer service agents is expensive, many companies are turning to NLP thatassists human agents by auto-generating responses that can be used directly orwith modifications. Large Language Models (LLMs) are a natural fit for this usecase; however, their efficacy must be balanced with the cost of training andserving them. This paper assesses the practical cost and impact of LLMs for theenterprise as a function of the usefulness of the responses that they generate.We present a cost framework for evaluating an NLP model's utility for this usecase and apply it to a single brand as a case study in the context of anexisting agent assistance product. We compare three strategies for specializingan LLM - prompt engineering, fine-tuning, and knowledge distillation - usingfeedback from the brand's customer service agents. We find that the usabilityof a model's responses can make up for a large difference in inference cost forour case study brand, and we extrapolate our findings to the broader enterprisespace."
TART: A plug-and-play Transformer module for task-agnostic reasoning,"['Kush Bhatia', 'Avanika Narayan', 'Christopher De Sa', 'Christopher Ré']",http://arxiv.org/pdf/2306.07536v1.pdf,2023-06-13,"['cs.lg', 'cs.ai', 'cs.cl']","  Large language models (LLMs) exhibit in-context learning abilities whichenable the same model to perform several tasks without any task-specifictraining. In contrast, traditional adaptation approaches, such as fine-tuning,modify the underlying models for each specific task. In-context learning,however, consistently underperforms task-specific tuning approaches even whenpresented with the same examples. While most existing approaches (e.g., promptengineering) focus on the LLM's learned representations to patch thisperformance gap, our analysis actually reveal that LLM representations containsufficient information to make good predictions. As such, we focus on the LLM'sreasoning abilities and demonstrate that this performance gap exists due totheir inability to perform simple probabilistic reasoning tasks. This raises anintriguing question: Are LLMs actually capable of learning how to reason in atask-agnostic manner? We answer this in the affirmative and propose TART whichgenerically improves an LLM's reasoning abilities using a synthetically trainedTransformer-based reasoning module. TART trains this reasoning module in atask-agnostic manner using only synthetic logistic regression tasks andcomposes it with an arbitrary real-world pre-trained model without anyadditional training. With a single inference module, TART improves performanceacross different model families (GPT-Neo, Pythia, BLOOM), model sizes (100M -6B), tasks (14 NLP binary classification tasks), and even across differentmodalities (audio and vision). Additionally, on the RAFT Benchmark, TARTimproves GPT-Neo (125M)'s performance such that it outperforms BLOOM (176B),and is within 4% of GPT-3 (175B). Our code and models are available athttps://github.com/HazyResearch/TART ."
Exploring the MIT Mathematics and EECS Curriculum Using Large Language  Models,"['Sarah J. Zhang', 'Samuel Florin', 'Ariel N. Lee', 'Eamon Niknafs', 'Andrei Marginean', 'Annie Wang', 'Keith Tyser', 'Zad Chin', 'Yann Hicke', 'Nikhil Singh', 'Madeleine Udell', 'Yoon Kim', 'Tonio Buonassisi', 'Armando Solar-Lezama', 'Iddo Drori']",http://arxiv.org/pdf/2306.08997v2.pdf,2023-06-15,"['cs.cl', 'cs.ai', 'cs.lg']","  We curate a comprehensive dataset of 4,550 questions and solutions fromproblem sets, midterm exams, and final exams across all MIT Mathematics andElectrical Engineering and Computer Science (EECS) courses required forobtaining a degree. We evaluate the ability of large language models to fulfillthe graduation requirements for any MIT major in Mathematics and EECS. Ourresults demonstrate that GPT-3.5 successfully solves a third of the entire MITcurriculum, while GPT-4, with prompt engineering, achieves a perfect solve rateon a test set excluding questions based on images. We fine-tune an open-sourcelarge language model on this dataset. We employ GPT-4 to automatically grademodel responses, providing a detailed performance breakdown by course,question, and answer type. By embedding questions in a low-dimensional space,we explore the relationships between questions, topics, and classes anddiscover which questions and classes are required for solving other questionsand classes through few-shot learning. Our analysis offers valuable insightsinto course prerequisites and curriculum design, highlighting language models'potential for learning and improving Mathematics and EECS education."
Exploring the Effectiveness of Dataset Synthesis: An application of  Apple Detection in Orchards,"['Alexander van Meekeren', 'Maya Aghaei', 'Klaas Dijkstra']",http://arxiv.org/pdf/2306.11763v1.pdf,2023-06-20,['cs.cv'],"  Deep object detection models have achieved notable successes in recent years,but one major obstacle remains: the requirement for a large amount of trainingdata. Obtaining such data is a tedious process and is mainly time consuming,leading to the exploration of new research avenues like synthetic datageneration techniques. In this study, we explore the usability of StableDiffusion 2.1-base for generating synthetic datasets of apple trees for objectdetection and compare it to a baseline model trained on real-world data. Aftercreating a dataset of realistic apple trees with prompt engineering andutilizing a previously trained Stable Diffusion model, the custom dataset wasannotated and evaluated by training a YOLOv5m object detection model to predictapples in a real-world apple detection dataset. YOLOv5m was chosen for itsrapid inference time and minimal hardware demands. Results demonstrate that themodel trained on generated data is slightly underperforming compared to abaseline model trained on real-world images when evaluated on a set ofreal-world images. However, these findings remain highly promising, as theaverage precision difference is only 0.09 and 0.06, respectively. Qualitativeresults indicate that the model can accurately predict the location of apples,except in cases of heavy shading. These findings illustrate the potential ofsynthetic data generation techniques as a viable alternative to the collectionof extensive training data for object detection models."
Do you still need a manual smart contract audit?,"['Isaac David', 'Liyi Zhou', 'Kaihua Qin', 'Dawn Song', 'Lorenzo Cavallaro', 'Arthur Gervais']",http://arxiv.org/pdf/2306.12338v2.pdf,2023-06-21,['cs.cr'],"  We investigate the feasibility of employing large language models (LLMs) forconducting the security audit of smart contracts, a traditionallytime-consuming and costly process. Our research focuses on the optimization ofprompt engineering for enhanced security analysis, and we evaluate theperformance and accuracy of LLMs using a benchmark dataset comprising 52Decentralized Finance (DeFi) smart contracts that have previously beencompromised.  Our findings reveal that, when applied to vulnerable contracts, both GPT-4and Claude models correctly identify the vulnerability type in 40% of thecases. However, these models also demonstrate a high false positive rate,necessitating continued involvement from manual auditors. The LLMs testedoutperform a random model by 20% in terms of F1-score.  To ensure the integrity of our study, we conduct mutation testing on fivenewly developed and ostensibly secure smart contracts, into which we manuallyinsert two and 15 vulnerabilities each. This testing yielded a remarkablebest-case 78.7% true positive rate for the GPT-4-32k model. We tested both,asking the models to perform a binary classification on whether a contract isvulnerable, and a non-binary prompt. We also examined the influence of modeltemperature variations and context length on the LLM's performance.  Despite the potential for many further enhancements, this work lays thegroundwork for a more efficient and economical approach to smart contractsecurity audits."
MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language  Models,"['Chaoyou Fu', 'Peixian Chen', 'Yunhang Shen', 'Yulei Qin', 'Mengdan Zhang', 'Xu Lin', 'Jinrui Yang', 'Xiawu Zheng', 'Ke Li', 'Xing Sun', 'Yunsheng Wu', 'Rongrong Ji']",http://arxiv.org/pdf/2306.13394v3.pdf,2023-06-23,['cs.cv'],"  Multimodal Large Language Model (MLLM) relies on the powerful LLM to performmultimodal tasks, showing amazing emergent abilities in recent studies, such aswriting poems based on an image. However, it is difficult for these casestudies to fully reflect the performance of MLLM, lacking a comprehensiveevaluation. In this paper, we fill in this blank, presenting the firstcomprehensive MLLM Evaluation benchmark MME. It measures both perception andcognition abilities on a total of 14 subtasks. In order to avoid data leakagethat may arise from direct use of public datasets for evaluation, theannotations of instruction-answer pairs are all manually designed. The conciseinstruction design allows us to fairly compare MLLMs, instead of struggling inprompt engineering. Besides, with such an instruction, we can also easily carryout quantitative statistics. A total of 30 advanced MLLMs are comprehensivelyevaluated on our MME, which not only suggests that existing MLLMs still have alarge room for improvement, but also reveals the potential directions for thesubsequent model optimization."
Zero-shot Nuclei Detection via Visual-Language Pre-trained Models,"['Yongjian Wu', 'Yang Zhou', 'Jiya Saiyin', 'Bingzheng Wei', 'Maode Lai', 'Jianzhong Shou', 'Yubo Fan', 'Yan Xu']",http://arxiv.org/pdf/2306.17659v1.pdf,2023-06-30,['cs.cv'],"  Large-scale visual-language pre-trained models (VLPM) have proven theirexcellent performance in downstream object detection for natural scenes.However, zero-shot nuclei detection on H\&E images via VLPMs remainsunderexplored. The large gap between medical images and the web-originatedtext-image pairs used for pre-training makes it a challenging task. In thispaper, we attempt to explore the potential of the object-level VLPM, GroundedLanguage-Image Pre-training (GLIP) model, for zero-shot nuclei detection.Concretely, an automatic prompts design pipeline is devised based on theassociation binding trait of VLPM and the image-to-text VLPM BLIP, avoidingempirical manual prompts engineering. We further establish a self-trainingframework, using the automatically designed prompts to generate the preliminaryresults as pseudo labels from GLIP and refine the predicted boxes in aniterative manner. Our method achieves a remarkable performance for label-freenuclei detection, surpassing other comparison methods. Foremost, our workdemonstrates that the VLPM pre-trained on natural image-text pairs exhibitsastonishing potential for downstream tasks in the medical field as well. Codewill be released at https://github.com/wuyongjianCODE/VLPMNuD."
Comparative Analysis of GPT-4 and Human Graders in Evaluating Praise  Given to Students in Synthetic Dialogues,"['Dollaya Hirunyasiri', 'Danielle R. Thomas', 'Jionghao Lin', 'Kenneth R. Koedinger', 'Vincent Aleven']",http://arxiv.org/pdf/2307.02018v1.pdf,2023-07-05,"['cs.cl', 'cs.ai', 'cs.hc']","  Research suggests that providing specific and timely feedback to human tutorsenhances their performance. However, it presents challenges due to thetime-consuming nature of assessing tutor performance by human evaluators. Largelanguage models, such as the AI-chatbot ChatGPT, hold potential for offeringconstructive feedback to tutors in practical settings. Nevertheless, theaccuracy of AI-generated feedback remains uncertain, with scant researchinvestigating the ability of models like ChatGPT to deliver effective feedback.In this work-in-progress, we evaluate 30 dialogues generated by GPT-4 in atutor-student setting. We use two different prompting approaches, the zero-shotchain of thought and the few-shot chain of thought, to identify specificcomponents of effective praise based on five criteria. These approaches arethen compared to the results of human graders for accuracy. Our goal is toassess the extent to which GPT-4 can accurately identify each praise criterion.We found that both zero-shot and few-shot chain of thought approaches yieldcomparable results. GPT-4 performs moderately well in identifying instanceswhen the tutor offers specific and immediate praise. However, GPT-4underperforms in identifying the tutor's ability to deliver sincere praise,particularly in the zero-shot prompting scenario where examples of sinceretutor praise statements were not provided. Future work will focus on enhancingprompt engineering, developing a more general tutoring rubric, and evaluatingour method using real-life tutoring dialogues."
"Right to be Forgotten in the Era of Large Language Models: Implications,  Challenges, and Solutions","['Dawen Zhang', 'Pamela Finckenberg-Broman', 'Thong Hoang', 'Shidong Pan', 'Zhenchang Xing', 'Mark Staples', 'Xiwei Xu']",http://arxiv.org/pdf/2307.03941v3.pdf,2023-07-08,"['cs.cy', 'cs.ai', 'cs.cl']","  The Right to be Forgotten (RTBF) was first established as the result of theruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\'alez, andwas later included as the Right to Erasure under the General Data ProtectionRegulation (GDPR) of European Union to allow individuals the right to requestpersonal data be deleted by organizations. Specifically for search engines,individuals can send requests to organizations to exclude their informationfrom the query results. It was a significant emergent right as the result ofthe evolution of technology. With the recent development of Large LanguageModels (LLMs) and their use in chatbots, LLM-enabled software systems havebecome popular. But they are not excluded from the RTBF. Compared with theindexing approach used by search engines, LLMs store, and process informationin a completely different way. This poses new challenges for compliance withthe RTBF. In this paper, we explore these challenges and provide our insightson how to implement technical solutions for the RTBF, including the use ofdifferential privacy, machine unlearning, model editing, and promptengineering. With the rapid advancement of AI and the increasing need ofregulating this powerful technology, learning from the case of RTBF can providevaluable lessons for technical practitioners, legal experts, organizations, andauthorities."
"Software Testing with Large Language Model: Survey, Landscape, and  Vision","['Junjie Wang', 'Yuchao Huang', 'Chunyang Chen', 'Zhe Liu', 'Song Wang', 'Qing Wang']",http://arxiv.org/pdf/2307.07221v1.pdf,2023-07-14,['cs.se'],"  Pre-trained large language models (LLMs) have recently emerged as abreakthrough technology in natural language processing and artificialintelligence, with the ability to handle large-scale datasets and exhibitremarkable performance across a wide range of tasks. Meanwhile, softwaretesting is a crucial undertaking that serves as a cornerstone for ensuring thequality and reliability of software products. As the scope and complexity ofsoftware systems continue to grow, the need for more effective software testingtechniques becomes increasingly urgent, and making it an area ripe forinnovative approaches such as the use of LLMs. This paper provides acomprehensive review of the utilization of LLMs in software testing. Itanalyzes 52 relevant studies that have used LLMs for software testing, fromboth the software testing and LLMs perspectives. The paper presents a detaileddiscussion of the software testing tasks for which LLMs are commonly used,among which test case preparation and program repair are the mostrepresentative ones. It also analyzes the commonly used LLMs, the types ofprompt engineering that are employed, as well as the accompanied techniqueswith these LLMs. It also summarizes the key challenges and potentialopportunities in this direction. This work can serve as a roadmap for futureresearch in this area, highlighting potential avenues for exploration, andidentifying gaps in our current understanding of the use of LLMs in softwaretesting."
The Potential and Pitfalls of using a Large Language Model such as  ChatGPT or GPT-4 as a Clinical Assistant,"['Jingqing Zhang', 'Kai Sun', 'Akshay Jagadeesh', 'Mahta Ghahfarokhi', 'Deepa Gupta', 'Ashok Gupta', 'Vibhor Gupta', 'Yike Guo']",http://arxiv.org/pdf/2307.08152v1.pdf,2023-07-16,['cs.cl'],"  Recent studies have demonstrated promising performance of ChatGPT and GPT-4on several medical domain tasks. However, none have assessed its performanceusing a large-scale real-world electronic health record database, nor haveevaluated its utility in providing clinical diagnostic assistance for patientsacross a full range of disease presentation. We performed two analyses usingChatGPT and GPT-4, one to identify patients with specific medical diagnosesusing a real-world large electronic health record database and the other, inproviding diagnostic assistance to healthcare workers in the prospectiveevaluation of hypothetical patients. Our results show that GPT-4 across diseaseclassification tasks with chain of thought and few-shot prompting can achieveperformance as high as 96% F1 scores. For patient assessment, GPT-4 canaccurately diagnose three out of four times. However, there were mentions offactually incorrect statements, overlooking crucial medical findings,recommendations for unnecessary investigations and overtreatment. These issuescoupled with privacy concerns, make these models currently inadequate for realworld clinical use. However, limited data and time needed for promptengineering in comparison to configuration of conventional machine learningworkflows highlight their potential for scalability across healthcareapplications."
A Lightweight Framework for High-Quality Code Generation,"['Mohammed Latif Siddiq', 'Beatrice Casey', 'Joanna C. S. Santos']",http://arxiv.org/pdf/2307.08220v1.pdf,2023-07-17,"['cs.se', 'cs.lg']","  In recent years, the use of automated source code generation utilizingtransformer-based generative models has expanded, and these models can generatefunctional code according to the requirements of the developers. However,recent research revealed that these automatically generated source codes cancontain vulnerabilities and other quality issues. Despite researchers' andpractitioners' attempts to enhance code generation models, retraining andfine-tuning large language models is time-consuming and resource-intensive.Thus, we describe FRANC, a lightweight framework for recommending more secureand high-quality source code derived from transformer-based code generationmodels. FRANC includes a static filter to make the generated code compilablewith heuristics and a quality-aware ranker to sort the code snippets based on aquality score. Moreover, the framework uses prompt engineering to fixpersistent quality issues. We evaluated the framework with five Python and Javacode generation models and six prompt datasets, including a newly created onein this work (SOEval). The static filter improves 9% to 46% Java suggestionsand 10% to 43% Python suggestions regarding compilability. The averageimprovement over the NDCG@10 score for the ranking system is 0.0763, and therepairing techniques repair the highest 80% of prompts. FRANC takes, onaverage, 1.98 seconds for Java; for Python, it takes 0.08 seconds."
"Multi-Method Self-Training: Improving Code Generation With Text, And  Vice Versa","['Shriyash K. Upadhyay', 'Etan J. Ginsberg']",http://arxiv.org/pdf/2307.10633v1.pdf,2023-07-20,"['cs.cl', 'cs.lg']","  Large Language Models have many methods for solving the same problem. Thisintroduces novel strengths (different methods may work well for differentproblems) and weaknesses (it may be difficult for users to know which method touse). In this paper, we introduce Multi-Method Self-Training (MMST), where onemethod is trained on the filtered outputs of another, allowing us to augmentthe strengths and ameliorate the weaknesses of each method. Using a 176Bparameter model trained on both language and code, we show that MMST can 1)improve the less performant method (up to 30%) making the model easier to use,2) improve the more performant method (up to 32.2%) making the model moreperformant, and 3) improve the performance of related but distinct tasks (up to10.3%) by improving the ability of the model to generate rationales. We thenconduct ablation analyses to explore why MMST works. We show that MMSTgenerates more data than traditional self-training, but the improvement inperformance is driven by the use of multiple methods. We also analyzeprompt-engineering and anti-correlated performance between methods as means ofmaking MMST more effective. We hope the evidence from our paper motivatesmachine learning researchers to explore ways in which advances in languagemodels allow for new forms of training."
Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts,"['Mayug Maniparambil', 'Chris Vorster', 'Derek Molloy', 'Noel Murphy', 'Kevin McGuinness', ""Noel E. O'Connor""]",http://arxiv.org/pdf/2307.11661v2.pdf,2023-07-21,"['cs.cv', 'cs.ai', 'cs.cl', 'cs.lg']","  Contrastive pretrained large Vision-Language Models (VLMs) like CLIP haverevolutionized visual representation learning by providing good performance ondownstream datasets. VLMs are 0-shot adapted to a downstream dataset bydesigning prompts that are relevant to the dataset. Such prompt engineeringmakes use of domain expertise and a validation dataset. Meanwhile, recentdevelopments in generative pretrained models like GPT-4 mean they can be usedas advanced internet search tools. They can also be manipulated to providevisual information in any structure. In this work, we show that GPT-4 can beused to generate text that is visually descriptive and how this can be used toadapt CLIP to downstream tasks. We show considerable improvements in 0-shottransfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt.We also design a simple few-shot adapter that learns to choose the bestpossible sentences to construct generalizable classifiers that outperform therecently proposed CoCoOP by ~2% on average and by over 4% on 4 specializedfine-grained datasets. The code, prompts, and auxiliary text dataset isavailable at https://github.com/mayug/VDT-Adapter."
GPT-3 Models are Few-Shot Financial Reasoners,"['Raul Salles de Padua', 'Imran Qureshi', 'Mustafa U. Karakaplan']",http://arxiv.org/pdf/2307.13617v2.pdf,2023-07-25,"['cs.cl', 'cs.ai']","  Financial analysis is an important tool for evaluating company performance.Practitioners work to answer financial questions to make profitable investmentdecisions, and use advanced quantitative analyses to do so. As a result,Financial Question Answering (QA) is a question answering task that requiresdeep reasoning about numbers. Furthermore, it is unknown how well pre-trainedlanguage models can reason in the financial domain. The currentstate-of-the-art requires a retriever to collect relevant facts about thefinancial question from the text and a generator to produce a valid financialprogram and a final answer. However, recently large language models like GPT-3have achieved state-of-the-art performance on wide variety of tasks with just afew shot examples. We run several experiments with GPT-3 and find that aseparate retrieval model and logic engine continue to be essential componentsto achieving SOTA performance in this task, particularly due to the precisenature of financial questions and the complex information stored in financialdocuments. With this understanding, our refined prompt-engineering approach onGPT-3 achieves near SOTA accuracy without any fine-tuning."
S3: Social-network Simulation System with Large Language Model-Empowered  Agents,"['Chen Gao', 'Xiaochong Lan', 'Zhihong Lu', 'Jinzhu Mao', 'Jinghua Piao', 'Huandong Wang', 'Depeng Jin', 'Yong Li']",http://arxiv.org/pdf/2307.14984v2.pdf,2023-07-27,['cs.si'],"  Social network simulation plays a crucial role in addressing variouschallenges within social science. It offers extensive applications such asstate prediction, phenomena explanation, and policy-making support, amongothers. In this work, we harness the formidable human-like capabilitiesexhibited by large language models (LLMs) in sensing, reasoning, and behaving,and utilize these qualities to construct the S$^3$ system (short for$\textbf{S}$ocial network $\textbf{S}$imulation $\textbf{S}$ystem). Adhering tothe widely employed agent-based simulation paradigm, we employ promptengineering and prompt tuning techniques to ensure that the agent's behaviorclosely emulates that of a genuine human within the social network.Specifically, we simulate three pivotal aspects: emotion, attitude, andinteraction behaviors. By endowing the agent in the system with the ability toperceive the informational environment and emulate human actions, we observethe emergence of population-level phenomena, including the propagation ofinformation, attitudes, and emotions. We conduct an evaluation encompassing twolevels of simulation, employing real-world social network data. Encouragingly,the results demonstrate promising accuracy. This work represents an initialstep in the realm of social network simulation empowered by LLM-based agents.We anticipate that our endeavors will serve as a source of inspiration for thedevelopment of simulation systems within, but not limited to, social science."
Flows: Building Blocks of Reasoning and Collaborating AI,"['Martin Josifoski', 'Lars Klein', 'Maxime Peyrard', 'Yifei Li', 'Saibo Geng', 'Julian Paul Schnitzler', 'Yuxing Yao', 'Jiheng Wei', 'Debjit Paul', 'Robert West']",http://arxiv.org/pdf/2308.01285v1.pdf,2023-08-02,"['cs.ai', 'cs.hc']","  Recent advances in artificial intelligence (AI) have produced highly capableand controllable systems. This creates unprecedented opportunities forstructured reasoning as well as collaboration among multiple AI systems andhumans. To fully realize this potential, it is essential to develop aprincipled way of designing and studying such structured interactions. For thispurpose, we introduce the conceptual framework of Flows: a systematic approachto modeling complex interactions. Flows are self-contained building blocks ofcomputation, with an isolated state, communicating through a standardizedmessage-based interface. This modular design allows Flows to be recursivelycomposed into arbitrarily nested interactions, with a substantial reduction ofcomplexity. Crucially, any interaction can be implemented using this framework,including prior work on AI--AI and human--AI interactions, prompt engineeringschemes, and tool augmentation. We demonstrate the potential of Flows on thetask of competitive coding, a challenging task on which even GPT-4 struggles.Our results suggest that structured reasoning and collaboration substantiallyimprove generalization, with AI-only Flows adding +$21$ and human--AI Flowsadding +$54$ absolute points in terms of solve rate. To support rapid andrigorous research, we introduce the aiFlows library. The library comes with arepository of Flows that can be easily used, extended, and composed into novel,more complex Flows.  The aiFlows library is available at https://github.com/epfl-dlab/aiflows.Data and Flows for reproducing our experiments are available athttps://github.com/epfl-dlab/cc_flows."
Evaluating ChatGPT text-mining of clinical records for obesity  monitoring,"['Ivo S. Fins', 'Heather Davies', 'Sean Farrell', 'Jose R. Torres', 'Gina Pinchbeck', 'Alan D. Radford', 'Peter-John Noble']",http://arxiv.org/pdf/2308.01666v1.pdf,2023-08-03,"['cs.ir', 'cs.cl']","  Background: Veterinary clinical narratives remain a largely untapped resourcefor addressing complex diseases. Here we compare the ability of a largelanguage model (ChatGPT) and a previously developed regular expression (RegexT)to identify overweight body condition scores (BCS) in veterinary narratives.Methods: BCS values were extracted from 4,415 anonymised clinical narrativesusing either RegexT or by appending the narrative to a prompt sent to ChatGPTcoercing the model to return the BCS information. Data were manually reviewedfor comparison. Results: The precision of RegexT was higher (100%, 95% CI94.81-100%) than the ChatGPT (89.3%; 95% CI82.75-93.64%). However, the recallof ChatGPT (100%. 95% CI 96.18-100%) was considerably higher than that ofRegexT (72.6%, 95% CI 63.92-79.94%). Limitations: Subtle prompt engineering isneeded to improve ChatGPT output. Conclusions: Large language models creatediverse opportunities and, whilst complex, present an intuitive interface toinformation but require careful implementation to avoid unpredictable errors."
ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned  Samples in NLP,"['Lu Yan', 'Zhuo Zhang', 'Guanhong Tao', 'Kaiyuan Zhang', 'Xuan Chen', 'Guangyu Shen', 'Xiangyu Zhang']",http://arxiv.org/pdf/2308.02122v2.pdf,2023-08-04,"['cs.cr', 'cs.cl']","  Backdoor attacks have emerged as a prominent threat to natural languageprocessing (NLP) models, where the presence of specific triggers in the inputcan lead poisoned models to misclassify these inputs to predetermined targetclasses. Current detection mechanisms are limited by their inability to addressmore covert backdoor strategies, such as style-based attacks. In this work, wepropose an innovative test-time poisoned sample detection framework that hingeson the interpretability of model predictions, grounded in the semantic meaningof inputs. We contend that triggers (e.g., infrequent words) are not supposedto fundamentally alter the underlying semantic meanings of poisoned samples asthey want to stay stealthy. Based on this observation, we hypothesize thatwhile the model's predictions for paraphrased clean samples should remainstable, predictions for poisoned samples should revert to their true labelsupon the mutations applied to triggers during the paraphrasing process. Weemploy ChatGPT, a state-of-the-art large language model, as our paraphraser andformulate the trigger-removal task as a prompt engineering problem. We adoptfuzzing, a technique commonly used for unearthing software vulnerabilities, todiscover optimal paraphrase prompts that can effectively eliminate triggerswhile concurrently maintaining input semantics. Experiments on 4 types ofbackdoor attacks, including the subtle style backdoors, and 4 distinct datasetsdemonstrate that our approach surpasses baseline methods, including STRIP, RAP,and ONION, in precision and recall."
IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image  Diffusion Models,"['Hu Ye', 'Jun Zhang', 'Sibo Liu', 'Xiao Han', 'Wei Yang']",http://arxiv.org/pdf/2308.06721v1.pdf,2023-08-13,"['cs.cv', 'cs.ai']","  Recent years have witnessed the strong power of large text-to-image diffusionmodels for the impressive generative capability to create high-fidelity images.However, it is very tricky to generate desired images using only text prompt asit often involves complex prompt engineering. An alternative to text prompt isimage prompt, as the saying goes: ""an image is worth a thousand words"".Although existing methods of direct fine-tuning from pretrained models areeffective, they require large computing resources and are not compatible withother base models, text prompt, and structural controls. In this paper, wepresent IP-Adapter, an effective and lightweight adapter to achieve imageprompt capability for the pretrained text-to-image diffusion models. The keydesign of our IP-Adapter is decoupled cross-attention mechanism that separatescross-attention layers for text features and image features. Despite thesimplicity of our method, an IP-Adapter with only 22M parameters can achievecomparable or even better performance to a fully fine-tuned image prompt model.As we freeze the pretrained diffusion model, the proposed IP-Adapter can begeneralized not only to other custom models fine-tuned from the same basemodel, but also to controllable generation using existing controllable tools.With the benefit of the decoupled cross-attention strategy, the image promptcan also work well with the text prompt to achieve multimodal image generation.The project page is available at \url{https://ip-adapter.github.io}."
LogPrompt: Prompt Engineering Towards Zero-Shot and Interpretable Log  Analysis,"['Yilun Liu', 'Shimin Tao', 'Weibin Meng', 'Jingyu Wang', 'Wenbing Ma', 'Yanqing Zhao', 'Yuhang Chen', 'Hao Yang', 'Yanfei Jiang', 'Xun Chen']",http://arxiv.org/pdf/2308.07610v1.pdf,2023-08-15,"['cs.se', 'cs.cl']","  Automated log analysis is crucial in modern software-intensive systems forensuring reliability and resilience throughout software maintenance andengineering life cycles. Existing methods perform tasks such as log parsing andlog anomaly detection by providing a single prediction value withoutinterpretation. However, given the increasing volume of system events, thelimited interpretability of analysis results hinders analysts' trust and theirability to take appropriate actions. Moreover, these methods requiresubstantial in-domain training data, and their performance declines sharply (byup to 62.5%) in online scenarios involving unseen logs from new domains, acommon occurrence due to rapid software updates. In this paper, we proposeLogPrompt, a novel zero-shot and interpretable log analysis approach. LogPromptemploys large language models (LLMs) to perform zero-shot log analysis tasksvia a suite of advanced prompt strategies tailored for log tasks, whichenhances LLMs' performance by up to 107.5% compared with simple prompts.Experiments on nine publicly available evaluation datasets across two tasksdemonstrate that LogPrompt, despite using no training data, outperformsexisting approaches trained on thousands of logs by up to around 50%. We alsoconduct a human evaluation of LogPrompt's interpretability, with sixpractitioners possessing over 10 years of experience, who highly rated thegenerated content in terms of usefulness and readability (averagely 4.42/5).LogPrompt also exhibits remarkable compatibility with open-source andsmaller-scale LLMs, making it flexible for practical deployment."
Transforming Sentiment Analysis in the Financial Domain with ChatGPT,"['Georgios Fatouros', 'John Soldatos', 'Kalliopi Kouroumali', 'Georgios Makridis', 'Dimosthenis Kyriazis']",http://arxiv.org/pdf/2308.07935v1.pdf,2023-08-13,"['cs.cl', 'cs.ai', 'cs.ce', 'cs.ir', '68t01, 68t50, 91b28, 91b30']","  Financial sentiment analysis plays a crucial role in decoding market trendsand guiding strategic trading decisions. Despite the deployment of advanceddeep learning techniques and language models to refine sentiment analysis infinance, this study breaks new ground by investigating the potential of largelanguage models, particularly ChatGPT 3.5, in financial sentiment analysis,with a strong emphasis on the foreign exchange market (forex). Employing azero-shot prompting approach, we examine multiple ChatGPT prompts on ameticulously curated dataset of forex-related news headlines, measuringperformance using metrics such as precision, recall, f1-score, and MeanAbsolute Error (MAE) of the sentiment class. Additionally, we probe thecorrelation between predicted sentiment and market returns as an additionalevaluation approach. ChatGPT, compared to FinBERT, a well-established sentimentanalysis model for financial texts, exhibited approximately 35\% enhancedperformance in sentiment classification and a 36\% higher correlation withmarket returns. By underlining the significance of prompt engineering,particularly in zero-shot contexts, this study spotlights ChatGPT's potentialto substantially boost sentiment analysis in financial applications. By sharingthe utilized dataset, our intention is to stimulate further research andadvancements in the field of financial services."
ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based  Healthcare Decision Support using ChatGPT,"['Fatemeh Nazary', 'Yashar Deldjoo', 'Tommaso Di Noia']",http://arxiv.org/pdf/2308.09731v1.pdf,2023-08-17,"['cs.ai', 'cs.cl', 'cs.lg']","  This study presents an innovative approach to the application of largelanguage models (LLMs) in clinical decision-making, focusing on OpenAI'sChatGPT. Our approach introduces the use of contextual prompts-strategicallydesigned to include task description, feature description, and crucially,integration of domain knowledge-for high-quality binary classification taskseven in data-scarce scenarios. The novelty of our work lies in the utilizationof domain knowledge, obtained from high-performing interpretable ML models, andits seamless incorporation into prompt design. By viewing these ML models asmedical experts, we extract key insights on feature importance to aid indecision-making processes. This interplay of domain knowledge and AI holdssignificant promise in creating a more insightful diagnostic tool.  Additionally, our research explores the dynamics of zero-shot and few-shotprompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPTwith traditional supervised ML models in different data conditions, we aim toprovide insights into the effectiveness of prompt engineering strategies undervaried data availability. In essence, this paper bridges the gap between AI andhealthcare, proposing a novel methodology for LLMs application in clinicaldecision support systems. It highlights the transformative potential ofeffective prompt design, domain knowledge integration, and flexible learningapproaches in enhancing automated decision-making."
Synergistic Integration of Large Language Models and Cognitive  Architectures for Robust AI: An Exploratory Analysis,"['Oscar J. Romero', 'John Zimmerman', 'Aaron Steinfeld', 'Anthony Tomasic']",http://arxiv.org/pdf/2308.09830v3.pdf,2023-08-18,['cs.ai'],"  This paper explores the integration of two AI subdisciplines employed in thedevelopment of artificial agents that exhibit intelligent behavior: LargeLanguage Models (LLMs) and Cognitive Architectures (CAs). We present threeintegration approaches, each grounded in theoretical models and supported bypreliminary empirical evidence. The modular approach, which introduces fourmodels with varying degrees of integration, makes use of chain-of-thoughtprompting, and draws inspiration from augmented LLMs, the Common Model ofCognition, and the simulation theory of cognition. The agency approach,motivated by the Society of Mind theory and the LIDA cognitive architecture,proposes the formation of agent collections that interact at micro and macrocognitive levels, driven by either LLMs or symbolic components. Theneuro-symbolic approach, which takes inspiration from the CLARION cognitivearchitecture, proposes a model where bottom-up learning extracts symbolicrepresentations from an LLM layer and top-down guidance utilizes symbolicrepresentations to direct prompt engineering in the LLM layer. These approachesaim to harness the strengths of both LLMs and CAs, while mitigating theirweaknesses, thereby advancing the development of more robust AI systems. Wediscuss the tradeoffs and challenges associated with each approach."
Manipulating Embeddings of Stable Diffusion Prompts,"['Niklas Deckers', 'Julia Peters', 'Martin Potthast']",http://arxiv.org/pdf/2308.12059v1.pdf,2023-08-23,"['cs.cv', 'cs.lg']","  Generative text-to-image models such as Stable Diffusion allow users togenerate images based on a textual description, the prompt. Changing the promptis still the primary means for the user to change a generated image as desired.However, changing the image by reformulating the prompt remains a difficultprocess of trial and error, which has led to the emergence of promptengineering as a new field of research. We propose and analyze methods tochange the embedding of a prompt directly instead of the prompt text. It allowsfor more fine-grained and targeted control that takes into account userintentions. Our approach treats the generative text-to-image model as acontinuous function and passes gradients between the image space and the promptembedding space. By addressing different user interaction problems, we canapply this idea in three scenarios: (1) Optimization of a metric defined inimage space that could measure, for example, image style. (2) Assistance ofusers in creative tasks by enabling them to navigate the image space along aselection of directions of ""near"" prompt embeddings. (3) Changing the embeddingof the prompt to include information that the user has seen in a particularseed but finds difficult to describe in the prompt. Our experiments demonstratethe feasibility of the described methods."
Large Language Models in Fault Localisation,"['Yonghao Wu', 'Zheng Li', 'Jie M. Zhang', 'Mike Papadakis', 'Mark Harman', 'Yong Liu']",http://arxiv.org/pdf/2308.15276v3.pdf,2023-08-29,['cs.se'],"  Large Language Models (LLMs) have shown promise in multiple softwareengineering tasks including code generation, program repair, codesummarisation, and test generation. Fault localisation is instrumental inenabling automated debugging and repair of programs and was prominentlyfeatured as a highlight during the launch event of ChatGPT-4. Nevertheless, theperformance of LLMs compared to state-of-the-art methods, as well as the impactof prompt design and context length on their efficacy, remains unclear. To fillthis gap, this paper presents an in-depth investigation into the capability ofChatGPT-3.5 and ChatGPT-4, the two state-of-the-art LLMs, on faultlocalisation. Using the widely-adopted large-scale Defects4J dataset, wecompare the two LLMs with the existing fault localisation techniques. We alsoinvestigate the consistency of LLMs in fault localisation, as well as howprompt engineering and the length of code context affect the fault localisationeffectiveness.  Our findings demonstrate that within function-level context, ChatGPT-4outperforms all the existing fault localisation methods. Additional error logscan further improve ChatGPT models' localisation accuracy and consistency, withan average 46.9% higher accuracy over the state-of-the-art baseline SmartFL onthe Defects4J dataset in terms of TOP-1 metric. However, when the code contextof the Defects4J dataset expands to the class-level, ChatGPT-4's performancesuffers a significant drop, with 49.9% lower accuracy than SmartFL under TOP-1metric. These observations indicate that although ChatGPT can effectivelylocalise faults under specific conditions, limitations are evident. Furtherresearch is needed to fully harness the potential of LLMs like ChatGPT forpractical fault localisation applications."
Leveraging Large Language Models for Exploiting ASR Uncertainty,"['Pranay Dighe', 'Yi Su', 'Shangshang Zheng', 'Yunshu Liu', 'Vineet Garg', 'Xiaochuan Niu', 'Ahmed Tewfik']",http://arxiv.org/pdf/2309.04842v2.pdf,2023-09-09,"['cs.cl', 'cs.hc', 'cs.sd', 'eess.as']","  While large language models excel in a variety of natural language processing(NLP) tasks, to perform well on spoken language understanding (SLU) tasks, theymust either rely on off-the-shelf automatic speech recognition (ASR) systemsfor transcription, or be equipped with an in-built speech modality. This workfocuses on the former scenario, where LLM's accuracy on SLU tasks isconstrained by the accuracy of a fixed ASR system on the spoken input.Specifically, we tackle speech-intent classification task, where a highword-error-rate can limit the LLM's ability to understand the spoken intent.Instead of chasing a high accuracy by designing complex or specializedarchitectures regardless of deployment costs, we seek to answer how far we cango without substantially changing the underlying ASR and LLM, which canpotentially be shared by multiple unrelated tasks. To this end, we proposeprompting the LLM with an n-best list of ASR hypotheses instead of only theerror-prone 1-best hypothesis. We explore prompt-engineering to explain theconcept of n-best lists to the LLM; followed by the finetuning of Low-RankAdapters on the downstream tasks. Our approach using n-best lists proves to beeffective on a device-directed speech detection task as well as on a keywordspotting task, where systems using n-best list prompts outperform those using1-best ASR hypothesis; thus paving the way for an efficient method to exploitASR uncertainty via LLMs for speech-based applications."
Unveiling the potential of large language models in generating semantic  and cross-language clones,"['Palash R. Roy', 'Ajmain I. Alam', 'Farouq Al-omari', 'Banani Roy', 'Chanchal K. Roy', 'Kevin A. Schneider']",http://arxiv.org/pdf/2309.06424v1.pdf,2023-09-12,"['cs.se', 'cs.ai', 'cs.lg']","  Semantic and Cross-language code clone generation may be useful for codereuse, code comprehension, refactoring and benchmarking. OpenAI's GPT model haspotential in such clone generation as GPT is used for text generation. Whendevelopers copy/paste codes from Stack Overflow (SO) or within a system, theremight be inconsistent changes leading to unexpected behaviours. Similarly, ifsomeone possesses a code snippet in a particular programming language but seeksequivalent functionality in a different language, a semantic cross-languagecode clone generation approach could provide valuable assistance. In thisstudy, using SemanticCloneBench as a vehicle, we evaluated how well the GPT-3model could help generate semantic and cross-language clone variants for agiven fragment.We have comprised a diverse set of code fragments and assessedGPT-3s performance in generating code variants.Through extensiveexperimentation and analysis, where 9 judges spent 158 hours to validate, weinvestigate the model's ability to produce accurate and semantically correctvariants. Our findings shed light on GPT-3's strengths in code generation,offering insights into the potential applications and challenges of usingadvanced language models in software development. Our quantitative analysisyields compelling results. In the realm of semantic clones, GPT-3 attains animpressive accuracy of 62.14% and 0.55 BLEU score, achieved through few-shotprompt engineering. Furthermore, the model shines in transcending linguisticconfines, boasting an exceptional 91.25% accuracy in generating cross-languageclones"
Is GPT4 a Good Trader?,['Bingzhe Wu'],http://arxiv.org/pdf/2309.10982v1.pdf,2023-09-20,['cs.ai'],"  Recently, large language models (LLMs), particularly GPT-4, have demonstratedsignificant capabilities in various planning and reasoning tasks\cite{cheng2023gpt4,bubeck2023sparks}. Motivated by these advancements, therehas been a surge of interest among researchers to harness the capabilities ofGPT-4 for the automated design of quantitative factors that do not overlap withexisting factor libraries, with an aspiration to achieve alpha returns\cite{webpagequant}. In contrast to these work, this study aims to examine thefidelity of GPT-4's comprehension of classic trading theories and itsproficiency in applying its code interpreter abilities to real-world tradingdata analysis. Such an exploration is instrumental in discerning whether theunderlying logic GPT-4 employs for trading is intrinsically reliable.Furthermore, given the acknowledged interpretative latitude inherent in mosttrading theories, we seek to distill more precise methodologies of deployingthese theories from GPT-4's analytical process, potentially offering invaluableinsights to human traders.  To achieve this objective, we selected daily candlestick (K-line) data fromspecific periods for certain assets, such as the Shanghai Stock Index. Throughmeticulous prompt engineering, we guided GPT-4 to analyze the technicalstructures embedded within this data, based on specific theories like theElliott Wave Theory. We then subjected its analytical output to manualevaluation, assessing its interpretative depth and accuracy vis-\`a-vis thesetrading theories from multiple dimensions. The results and findings from thisstudy could pave the way for a synergistic amalgamation of human expertise andAI-driven insights in the realm of trading."
AI-Copilot for Business Optimisation: A Framework and A Case Study in  Production Scheduling,"['Pivithuru Thejan Amarasinghe', 'Su Nguyen', 'Yuan Sun', 'Damminda Alahakoon']",http://arxiv.org/pdf/2309.13218v3.pdf,2023-09-22,['cs.ai'],"  Business optimisation refers to the process of finding and implementingefficient and cost-effective means of operation to bring a competitiveadvantage for businesses. Synthesizing problem formulations is an integral partof business optimisation, which relies on human expertise to construct problemformulations using optimisation languages. Interestingly, with advancements inLarge Language Models (LLMs), the human expertise needed in problem formulationcan be minimized. However, developing an LLM for problem formulation ischallenging, due to training data, token limitations, and lack of appropriateperformance metrics. For the requirement of training data, recent attention hasbeen directed towards fine-tuning pre-trained LLMs for downstream tasks ratherthan training an LLM from scratch for a specific task. In this paper, we adoptan LLM fine-tuning approach and propose an AI-Copilot for business optimisationproblem formulation. For token limitations, we introduce modularization andprompt engineering techniques to synthesize complex problem formulations asmodules that fit into the token limits of LLMs. Additionally, we designperformance evaluation metrics that are better suited for assessing theaccuracy and quality of problem formulations. The experiment resultsdemonstrate that with this approach we can synthesize complex and large problemformulations for a typical business optimisation problem in productionscheduling."
An AI Chatbot for Explaining Deep Reinforcement Learning Decisions of  Service-oriented Systems,"['Andreas Metzger', 'Jone Bartel', 'Jan Laufer']",http://arxiv.org/pdf/2309.14391v1.pdf,2023-09-25,"['cs.lg', 'cs.ai', 'cs.cl']","  Deep Reinforcement Learning (Deep RL) is increasingly used to cope with theopen-world assumption in service-oriented systems. Deep RL was successfullyapplied to problems such as dynamic service composition, job scheduling, andoffloading, as well as service adaptation. While Deep RL offers many benefits,understanding the decision-making of Deep RL is challenging because its learneddecision-making policy essentially appears as a black box. Yet, understandingthe decision-making of Deep RL is key to help service developers performdebugging, support service providers to comply with relevant legal frameworks,and facilitate service users to build trust. We introduce Chat4XAI tofacilitate the understanding of the decision-making of Deep RL by providingnatural-language explanations. Compared with visual explanations, the reportedbenefits of natural-language explanations include better understandability fornon-technical users, increased user acceptance and trust, as well as moreefficient explanations. Chat4XAI leverages modern AI chatbot technology anddedicated prompt engineering. Compared to earlier work on natural-languageexplanations using classical software-based dialogue systems, using an AIchatbot eliminates the need for eliciting and defining potential questions andanswers up-front. We prototypically realize Chat4XAI using OpenAI's ChatGPT APIand evaluate the fidelity and stability of its explanations using an adaptiveservice exemplar."
Batch Calibration: Rethinking Calibration for In-Context Learning and  Prompt Engineering,"['Han Zhou', 'Xingchen Wan', 'Lev Proleev', 'Diana Mincu', 'Jilin Chen', 'Katherine Heller', 'Subhrajit Roy']",http://arxiv.org/pdf/2309.17249v1.pdf,2023-09-29,"['cs.cl', 'cs.ai', 'cs.lg']","  Prompting and in-context learning (ICL) have become efficient learningparadigms for large language models (LLMs). However, LLMs suffer from promptbrittleness and various bias factors in the prompt, including but not limitedto the formatting, the choice verbalizers, and the ICL examples. To addressthis problem that results in unexpected performance degradation, calibrationmethods have been developed to mitigate the effects of these biases whilerecovering LLM performance. In this work, we first conduct a systematicanalysis of the existing calibration methods, where we both provide a unifiedview and reveal the failure cases. Inspired by these analyses, we propose BatchCalibration (BC), a simple yet intuitive method that controls the contextualbias from the batched input, unifies various prior approaches, and effectivelyaddresses the aforementioned issues. BC is zero-shot, inference-only, andincurs negligible additional costs. In the few-shot setup, we further extend BCto allow it to learn the contextual bias from labeled data. We validate theeffectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstratestate-of-the-art performance over previous calibration baselines across morethan 10 natural language understanding and image classification tasks."
Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind  Aware GPT-4,"['Jiaxian Guo', 'Bo Yang', 'Paul Yoo', 'Bill Yuchen Lin', 'Yusuke Iwasawa', 'Yutaka Matsuo']",http://arxiv.org/pdf/2309.17277v2.pdf,2023-09-29,['cs.ai'],"  Unlike perfect information games, where all elements are known to everyplayer, imperfect information games emulate the real-world complexities ofdecision-making under uncertain or incomplete information. GPT-4, the recentbreakthrough in large language models (LLMs) trained on massive passive data,is notable for its knowledge retrieval and reasoning abilities. This paperdelves into the applicability of GPT-4's learned knowledge for imperfectinformation games. To achieve this, we introduce \textbf{Suspicion-Agent}, aninnovative agent that leverages GPT-4's capabilities for performing inimperfect information games. With proper prompt engineering to achievedifferent functions, Suspicion-Agent based on GPT-4 demonstrates remarkableadaptability across a range of imperfect information card games. Importantly,GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning itcan understand others and intentionally impact others' behavior. Leveragingthis, we design a planning strategy that enables GPT-4 to competently playagainst different opponents, adapting its gameplay style as needed, whilerequiring only the game rules and descriptions of observations as input. In theexperiments, we qualitatively showcase the capabilities of Suspicion-Agentacross three different imperfect information games and then quantitativelyevaluate it in Leduc Hold'em. The results show that Suspicion-Agent canpotentially outperform traditional algorithms designed for imperfectinformation games, without any specialized training or examples. In order toencourage and foster deeper insights within the community, we make ourgame-related data publicly available."
Investigating the Limitation of CLIP Models: The Worst-Performing  Categories,"['Jie-Jing Shao', 'Jiang-Xin Shi', 'Xiao-Wen Yang', 'Lan-Zhe Guo', 'Yu-Feng Li']",http://arxiv.org/pdf/2310.03324v1.pdf,2023-10-05,"['cs.cv', 'cs.lg']","  Contrastive Language-Image Pre-training (CLIP) provides a foundation model byintegrating natural language into visual concepts, enabling zero-shotrecognition on downstream tasks. It is usually expected that satisfactoryoverall accuracy can be achieved across numerous domains through well-designedtextual prompts. However, we found that their performance in the worstcategories is significantly inferior to the overall performance. For example,on ImageNet, there are a total of 10 categories with class-wise accuracy as lowas 0\%, even though the overall performance has achieved 64.1\%. Thisphenomenon reveals the potential risks associated with using CLIP models,particularly in risk-sensitive applications where specific categories holdsignificant importance. To address this issue, we investigate the alignmentbetween the two modalities in the CLIP model and propose the Class-wiseMatching Margin (\cmm) to measure the inference confusion. \cmm\ caneffectively identify the worst-performing categories and estimate the potentialperformance of the candidate prompts. We further query large language models toenrich descriptions of worst-performing categories and build a weightedensemble to highlight the efficient prompts. Experimental results clearlyverify the effectiveness of our proposal, where the accuracy on the worst-10categories on ImageNet is boosted to 5.2\%, without manual prompt engineering,laborious optimization, or access to labeled validation data."
Thought Propagation: An Analogical Approach to Complex Reasoning with  Large Language Models,"['Junchi Yu', 'Ran He', 'Rex Ying']",http://arxiv.org/pdf/2310.03965v2.pdf,2023-10-06,"['cs.ai', 'cs.cl']","  Large Language Models (LLMs) have achieved remarkable success in reasoningtasks with the development of prompting methods. However, existing promptingapproaches cannot reuse insights of solving similar problems and suffer fromaccumulated errors in multi-step reasoning, since they prompt LLMs to reason\textit{from scratch}. To address these issues, we propose\textbf{\textit{Thought Propagation} (TP)}, which explores the analogousproblems and leverages their solutions to enhance the complex reasoning abilityof LLMs. These analogous problems are related to the input one, with reusablesolutions and problem-solving strategies. Thus, it is promising to propagateinsights of solving previous analogous problems to inspire new problem-solving.To achieve this, TP first prompts LLMs to propose and solve a set of analogousproblems that are related to the input one. Then, TP reuses the results ofanalogous problems to directly yield a new solution or derive aknowledge-intensive plan for execution to amend the initial solution obtainedfrom scratch. TP is compatible with existing prompting approaches, allowingplug-and-play generalization and enhancement in a wide range of tasks withoutmuch labor in task-specific prompt engineering. Experiments across threechallenging tasks demonstrate TP enjoys a substantial improvement over thebaselines by an average of 12\% absolute increase in finding the optimalsolutions in Shortest-path Reasoning, 13\% improvement of human preference inCreative Writing, and 15\% enhancement in the task completion rate of LLM-AgentPlanning."
JVNV: A Corpus of Japanese Emotional Speech with Verbal Content and  Nonverbal Expressions,"['Detai Xin', 'Junfeng Jiang', 'Shinnosuke Takamichi', 'Yuki Saito', 'Akiko Aizawa', 'Hiroshi Saruwatari']",http://arxiv.org/pdf/2310.06072v1.pdf,2023-10-09,"['cs.sd', 'eess.as']","  We present the JVNV, a Japanese emotional speech corpus with verbal contentand nonverbal vocalizations whose scripts are generated by a large-scalelanguage model. Existing emotional speech corpora lack not only properemotional scripts but also nonverbal vocalizations (NVs) that are essentialexpressions in spoken language to express emotions. We propose an automaticscript generation method to produce emotional scripts by providing seed wordswith sentiment polarity and phrases of nonverbal vocalizations to ChatGPT usingprompt engineering. We select 514 scripts with balanced phoneme coverage fromthe generated candidate scripts with the assistance of emotion confidencescores and language fluency scores. We demonstrate the effectiveness of JVNV byshowing that JVNV has better phoneme coverage and emotion recognizability thanprevious Japanese emotional speech corpora. We then benchmark JVNV on emotionaltext-to-speech synthesis using discrete codes to represent NVs. We show thatthere still exists a gap between the performance of synthesizing read-aloudspeech and emotional speech, and adding NVs in the speech makes the task evenharder, which brings new challenges for this task and makes JVNV a valuableresource for relevant works in the future. To our best knowledge, JVNV is thefirst speech corpus that generates scripts automatically using large languagemodels."
Large Language Model-Empowered Agents for Simulating Macroeconomic  Activities,"['Nian Li', 'Chen Gao', 'Yong Li', 'Qingmin Liao']",http://arxiv.org/pdf/2310.10436v1.pdf,2023-10-16,['cs.ai'],"  The advent of the Web has brought about a paradigm shift in traditionaleconomics, particularly in the digital economy era, enabling the preciserecording and analysis of individual economic behavior. This has led to agrowing emphasis on data-driven modeling in macroeconomics. In macroeconomicresearch, Agent-based modeling (ABM) emerged as an alternative, evolvingthrough rule-based agents, machine learning-enhanced decision-making, and, morerecently, advanced AI agents. However, the existing works are suffering fromthree main challenges when endowing agents with human-like decision-making,including agent heterogeneity, the influence of macroeconomic trends, andmultifaceted economic factors. Large language models (LLMs) have recentlygained prominence in offering autonomous human-like characteristics. Therefore,leveraging LLMs in macroeconomic simulation presents an opportunity to overcometraditional limitations. In this work, we take an early step in introducing anovel approach that leverages LLMs in macroeconomic simulation. We designprompt-engineering-driven LLM agents to exhibit human-like decision-making andadaptability in the economic environment, with the abilities of perception,reflection, and decision-making to address the abovementioned challenges.Simulation experiments on macroeconomic activities show that LLM-empoweredagents can make realistic work and consumption decisions and emerge morereasonable macroeconomic phenomena than existing rule-based or AI agents. Ourwork demonstrates the promising potential to simulate macroeconomics based onLLM and its human-like characteristics."
Large Language Model for Multi-objective Evolutionary Optimization,"['Fei Liu', 'Xi Lin', 'Zhenkun Wang', 'Shunyu Yao', 'Xialiang Tong', 'Mingxuan Yuan', 'Qingfu Zhang']",http://arxiv.org/pdf/2310.12541v2.pdf,2023-10-19,"['cs.ne', 'cs.ai', 'cs.cl', 'cs.et']","  Multiobjective evolutionary algorithms (MOEAs) are major methods for solvingmultiobjective optimization problems (MOPs). Many MOEAs have been proposed inthe past decades, of which the search operators need a carefully handcrafteddesign with domain knowledge. Recently, some attempts have been made to replacethe manually designed operators in MOEAs with learning-based operators (e.g.,neural network models). However, much effort is still required for designingand training such models, and the learned operators might not generalize wellon new problems. To tackle the above challenges, this work investigates a novelapproach that leverages the powerful large language model (LLM) to design MOEAoperators. With proper prompt engineering, we successfully let a general LLMserve as a black-box search operator for decomposition-based MOEA (MOEA/D) in azero-shot manner. In addition, by learning from the LLM behavior, we furtherdesign an explicit white-box operator with randomness and propose a new versionof decomposition-based MOEA, termed MOEA/D-LO. Experimental studies ondifferent test benchmarks show that our proposed method can achieve competitiveperformance with widely used MOEAs. It is also promising to see the operatoronly learned from a few instances can have robust generalization performance onunseen problems with quite different patterns and settings. The results revealthe potential benefits of using pre-trained LLMs in the design of MOEAs."
Vision-Language Models are Zero-Shot Reward Models for Reinforcement  Learning,"['Juan Rocamonde', 'Victoriano Montesinos', 'Elvis Nava', 'Ethan Perez', 'David Lindner']",http://arxiv.org/pdf/2310.12921v1.pdf,2023-10-19,"['cs.lg', 'cs.ai']","  Reinforcement learning (RL) requires either manually specifying a rewardfunction, which is often infeasible, or learning a reward model from a largeamount of human feedback, which is often very expensive. We study a moresample-efficient alternative: using pretrained vision-language models (VLMs) aszero-shot reward models (RMs) to specify tasks via natural language. We proposea natural and general approach to using VLMs as reward models, which we callVLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learncomplex tasks without a manually specified reward function, such as kneeling,doing the splits, and sitting in a lotus position. For each of these tasks, weonly provide a single sentence text prompt describing the desired task withminimal prompt engineering. We provide videos of the trained agents at:https://sites.google.com/view/vlm-rm. We can improve performance by providing asecond ``baseline'' prompt and projecting out parts of the CLIP embedding spaceirrelevant to distinguish between goal and baseline. Further, we find a strongscaling effect for VLM-RMs: larger VLMs trained with more compute and data arebetter reward models. The failure modes of VLM-RMs we encountered are allrelated to known capability limitations of current VLMs, such as limitedspatial reasoning ability or visually unrealistic environments that are faroff-distribution for the VLM. We find that VLM-RMs are remarkably robust aslong as the VLM is large enough. This suggests that future VLMs will becomemore and more useful reward models for a wide range of RL applications."
Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and  Prompt Engineering,"['Rahman S M Wahidur', 'Ishmam Tashdeed', 'Manjit Kaur', ' Heung-No-Lee']",http://arxiv.org/pdf/2310.13226v1.pdf,2023-10-20,['cs.cl'],"  Blockchain technology has revolutionized the financial landscape, withcryptocurrencies gaining widespread adoption for their decentralized andtransparent nature. As the sentiment expressed on social media platforms cansignificantly influence cryptocurrency discussions and market movements,sentiment analysis has emerged as a crucial tool for understanding publicopinion and predicting market trends. Motivated by the aim to enhance sentimentanalysis accuracy in the cryptocurrency domain, this paper investigatesfine-tuning techniques on large language models. This paper also investigatesthe efficacy of supervised fine-tuning and instruction-based fine-tuning onlarge language models for unseen tasks. Experimental results demonstrate asignificant average zero-shot performance gain of 40% after fine-tuning,highlighting the potential of this technique in optimizing pre-trained languagemodel efficiency. Additionally, the impact of instruction tuning on models ofvarying scales is examined, revealing that larger models benefit frominstruction tuning, achieving the highest average accuracy score of 75.16%. Incontrast, smaller-scale models may experience reduced generalization due to thecomplete utilization of model capacity. To gain deeper insight about howinstruction works with these language models, this paper presents anexperimental investigation into the response of an instruction-based modelunder different instruction tuning setups. The investigation demonstrates thatthe model achieves an average accuracy score of 72.38% for short and simpleinstructions. This performance significantly outperforms its accuracy underlong and complex instructions by over 12%, thereby effectively highlighting theprofound significance of instruction characteristics in maximizing modelperformance."
Open-Ended Instructable Embodied Agents with Memory-Augmented Large  Language Models,"['Gabriel Sarch', 'Yue Wu', 'Michael J. Tarr', 'Katerina Fragkiadaki']",http://arxiv.org/pdf/2310.15127v2.pdf,2023-10-23,"['cs.ai', 'cs.cl', 'cs.lg', 'cs.ro']","  Pre-trained and frozen large language models (LLMs) can effectively mapsimple scene rearrangement instructions to programs over a robot's visuomotorfunctions through appropriate few-shot example prompting. To parse open-domainnatural language and adapt to a user's idiosyncratic procedures, not knownduring prompt engineering time, fixed prompts fall short. In this paper, weintroduce HELPER, an embodied agent equipped with an external memory oflanguage-program pairs that parses free-form human-robot dialogue into actionprograms through retrieval-augmented LLM prompting: relevant memories areretrieved based on the current dialogue, instruction, correction, or VLMdescription, and used as in-context prompt examples for LLM querying. Thememory is expanded during deployment to include pairs of user's language andaction plans, to assist future inferences and personalize them to the user'slanguage and routines. HELPER sets a new state-of-the-art in the TEAChbenchmark in both Execution from Dialog History (EDH) and Trajectory fromDialogue (TfD), with a 1.7x improvement over the previous state-of-the-art forTfD. Our models, code, and video results can be found in our project's website:https://helper-agent-llm.github.io."
Can LLMs Grade Short-answer Reading Comprehension Questions :  Foundational Literacy Assessment in LMICs,"['Owen Henkel', 'Libby Hills', 'Bill Roberts', 'Joshua McGrane']",http://arxiv.org/pdf/2310.18373v1.pdf,2023-10-26,"['cs.cl', 'cs.ai']","  This paper presents emerging evidence of using generative large languagemodels (i.e., GPT-4) to reliably evaluate short-answer reading comprehensionquestions. Specifically, we explore how various configurations of generative(LLMs) are able to evaluate student responses from a new dataset, drawn from abattery of reading assessments conducted with over 150 students in Ghana. Asthis dataset is novel and hence not used in training runs of GPT, it offers anopportunity to test for domain shift and evaluate the generalizability ofgenerative LLMs, which are predominantly designed and trained on data fromhigh-income North American countries. We found that GPT-4, with minimal promptengineering performed extremely well on evaluating the novel dataset (QuadraticWeighted Kappa 0.923, F1 0.88), substantially outperforming transfer-learningbased approaches, and even exceeding expert human raters (Quadratic WeightedKappa 0.915, F1 0.87). To the best of our knowledge, our work is the first toempirically evaluate the performance of generative LLMs on short-answer readingcomprehension questions, using real student data, and suggests that generativeLLMs have the potential to reliably evaluate foundational literacy. Currentlythe assessment of formative literacy and numeracy is infrequent in many low andmiddle-income countries (LMICs) due to the cost and operational complexities ofconducting them at scale. Automating the grading process for reading assessmentcould enable wider usage, and in turn improve decision-making regardingcurricula, school management, and teaching practice at the classroom level.Importantly, in contrast transfer learning based approaches, generative LLMsgeneralize well and the technical barriers to their use are low, making themmore feasible to implement and scale in lower resource educational contexts."
Promise:Prompt-driven 3D Medical Image Segmentation Using Pretrained  Image Foundation Models,"['Hao Li', 'Han Liu', 'Dewei Hu', 'Jiacheng Wang', 'Ipek Oguz']",http://arxiv.org/pdf/2310.19721v3.pdf,2023-10-30,"['eess.iv', 'cs.cv']","  To address prevalent issues in medical imaging, such as data acquisitionchallenges and label availability, transfer learning from natural to medicalimage domains serves as a viable strategy to produce reliable segmentationresults. However, several existing barriers between domains need to be brokendown, including addressing contrast discrepancies, managing anatomicalvariability, and adapting 2D pretrained models for 3D segmentation tasks. Inthis paper, we propose ProMISe,a prompt-driven 3D medical image segmentationmodel using only a single point prompt to leverage knowledge from a pretrained2D image foundation model. In particular, we use the pretrained visiontransformer from the Segment Anything Model (SAM) and integrate lightweightadapters to extract depth-related (3D) spatial context without updating thepretrained weights. For robust results, a hybrid network with complementaryencoders is designed, and a boundary-aware loss is proposed to achieve preciseboundaries. We evaluate our model on two public datasets for colon and pancreastumor segmentations, respectively. Compared to the state-of-the-artsegmentation methods with and without prompt engineering, our proposed methodachieves superior performance. The code is publicly available athttps://github.com/MedICL-VU/ProMISe."
Making Large Language Models Better Data Creators,"['Dong-Ho Lee', 'Jay Pujara', 'Mohit Sewak', 'Ryen W. White', 'Sujay Kumar Jauhar']",http://arxiv.org/pdf/2310.20111v1.pdf,2023-10-31,['cs.cl'],"  Although large language models (LLMs) have advanced the state-of-the-art inNLP significantly, deploying them for downstream applications is stillchallenging due to cost, responsiveness, control, or concerns around privacyand security. As such, trainable models are still the preferred option in somecases. However, these models still require human-labeled data for optimalperformance, which is expensive and time-consuming to obtain. In order toaddress this issue, several techniques to reduce human effort involve labelingor generating data using LLMs. Although these methods are effective for certainapplications, in practice they encounter difficulties in real-world scenarios.Labeling data requires careful data selection, while generating datanecessitates task-specific prompt engineering. In this paper, we propose aunified data creation pipeline that requires only a single formatting example,and which is applicable to a broad range of tasks, including traditionallyproblematic ones with semantically devoid label spaces. In our experiments wedemonstrate that instruction-following LLMs are highly cost-effective datacreators, and that models trained with these data exhibit performance betterthan those trained with human-labeled data (by up to 17.5%) onout-of-distribution evaluation, while maintaining comparable performance onin-distribution tasks. These results have important implications for therobustness of NLP systems deployed in the real-world."
VisPercep: A Vision-Language Approach to Enhance Visual Perception for  People with Blindness and Low Vision,"['Yu Hao', 'Fan Yang', 'Hao Huang', 'Shuaihang Yuan', 'Sundeep Rangan', 'John-Ross Rizzo', 'Yao Wang', 'Yi Fang']",http://arxiv.org/pdf/2310.20225v1.pdf,2023-10-31,"['cs.cv', 'cs.ai']","  People with blindness and low vision (pBLV) encounter substantial challengeswhen it comes to comprehensive scene recognition and precise objectidentification in unfamiliar environments. Additionally, due to the visionloss, pBLV have difficulty in accessing and identifying potential trippinghazards on their own. In this paper, we present a pioneering approach thatleverages a large vision-language model to enhance visual perception for pBLV,offering detailed and comprehensive descriptions of the surroundingenvironments and providing warnings about the potential risks. Our methodbegins by leveraging a large image tagging model (i.e., Recognize Anything(RAM)) to identify all common objects present in the captured images. Therecognition results and user query are then integrated into a prompt, tailoredspecifically for pBLV using prompt engineering. By combining the prompt andinput image, a large vision-language model (i.e., InstructBLIP) generatesdetailed and comprehensive descriptions of the environment and identifiespotential risks in the environment by analyzing the environmental objects andscenes, relevant to the prompt. We evaluate our approach through experimentsconducted on both indoor and outdoor datasets. Our results demonstrate that ourmethod is able to recognize objects accurately and provide insightfuldescriptions and analysis of the environment for pBLV."
Assessing Logical Puzzle Solving in Large Language Models: Insights from  a Minesweeper Case Study,"['Yinghao Li', 'Haorui Wang', 'Chao Zhang']",http://arxiv.org/pdf/2311.07387v1.pdf,2023-11-13,['cs.cl'],"  Large Language Models (LLMs) have shown remarkable proficiency in languageunderstanding and have been successfully applied to a variety of real-worldtasks through task-specific fine-tuning or prompt engineering. Despite theseadvancements, it remains an open question whether LLMs are fundamentallycapable of reasoning and planning, or if they primarily rely on recalling andsynthesizing information from their training data. In our research, weintroduce a novel task -- Minesweeper -- specifically designed in a formatunfamiliar to LLMs and absent from their training datasets. This taskchallenges LLMs to identify the locations of mines based on numerical cluesprovided by adjacent opened cells. Successfully completing this task requiresan understanding of each cell's state, discerning spatial relationships betweenthe clues and mines, and strategizing actions based on logical deductions drawnfrom the arrangement of the cells. Our experiments, including trials with theadvanced GPT-4 model, indicate that while LLMs possess the foundationalabilities required for this task, they struggle to integrate these into acoherent, multi-step logical reasoning process needed to solve Minesweeper.These findings highlight the need for further research to understand and natureof reasoning capabilities in LLMs under similar circumstances, and to explorepathways towards more sophisticated AI reasoning and planning models."
"Understanding Users' Dissatisfaction with ChatGPT Responses: Types,  Resolving Tactics, and the Effect of Knowledge Level","['Yoonsu Kim', 'Jueon Lee', 'Seoyoung Kim', 'Jaehyuk Park', 'Juho Kim']",http://arxiv.org/pdf/2311.07434v1.pdf,2023-11-13,['cs.hc'],"  Large language models (LLMs) with chat-based capabilities, such as ChatGPT,are widely used in various workflows. However, due to a limited understandingof these large-scale models, users struggle to use this technology andexperience different kinds of dissatisfaction. Researchers have introducedseveral methods such as prompt engineering to improve model responses. However,they focus on crafting one prompt, and little has been investigated on how todeal with the dissatisfaction the user encountered during the conversation.Therefore, with ChatGPT as the case study, we examine end users'dissatisfaction along with their strategies to address the dissatisfaction.After organizing users' dissatisfaction with LLM into seven categories based ona literature review, we collected 511 instances of dissatisfactory ChatGPTresponses from 107 users and their detailed recollections of dissatisfiedexperiences, which we release as a publicly accessible dataset. Our analysisreveals that users most frequently experience dissatisfaction when ChatGPTfails to grasp their intentions, while they rate the severity ofdissatisfaction the highest with dissatisfaction related to accuracy. We alsoidentified four tactics users employ to address their dissatisfaction and theireffectiveness. We found that users often do not use any tactics to addresstheir dissatisfaction, and even when using tactics, 72% of dissatisfactionremained unresolved. Moreover, we found that users with low knowledge regardingLLMs tend to face more dissatisfaction on accuracy while they often put minimaleffort in addressing dissatisfaction. Based on these findings, we proposedesign implications for minimizing user dissatisfaction and enhancing theusability of chat-based LLM services."
A Foundation Model for Cell Segmentation,"['Uriah Israel', 'Markus Marks', 'Rohit Dilip', 'Qilin Li', 'Morgan Schwartz', 'Elora Pradhan', 'Edward Pao', 'Shenyi Li', 'Alexander Pearson-Goulart', 'Pietro Perona', 'Georgia Gkioxari', 'Ross Barnowski', 'Yisong Yue', 'David Van Valen']",http://arxiv.org/pdf/2311.11004v1.pdf,2023-11-18,['q-bio.qm'],"  Cells are the fundamental unit of biological organization, and identifyingthem in imaging data - cell segmentation - is a critical task for variouscellular imaging experiments. While deep learning methods have led tosubstantial progress on this problem, models that have seen wide use arespecialist models that work well for specific domains. Methods that havelearned the general notion of ""what is a cell"" and can identify them acrossdifferent domains of cellular imaging data have proven elusive. In this work,we present CellSAM, a foundation model for cell segmentation that generalizesacross diverse cellular imaging data. CellSAM builds on top of the SegmentAnything Model (SAM) by developing a prompt engineering approach to maskgeneration. We train an object detector, CellFinder, to automatically detectcells and prompt SAM to generate segmentations. We show that this approachallows a single model to achieve state-of-the-art performance for segmentingimages of mammalian cells (in tissues and cell culture), yeast, and bacteriacollected with various imaging modalities. To enable accessibility, weintegrate CellSAM into DeepCell Label to further accelerate human-in-the-looplabeling strategies for cellular imaging data. A deployed version of CellSAM isavailable at https://label-dev.deepcell.org/."
Enhancing Summarization Performance through Transformer-Based Prompt  Engineering in Automated Medical Reporting,"['Daphne van Zandvoort', 'Laura Wiersema', 'Tom Huibers', 'Sandra van Dulmen', 'Sjaak Brinkkemper']",http://arxiv.org/pdf/2311.13274v1.pdf,2023-11-22,['cs.cl'],"  Customized medical prompts enable Large Language Models (LLM) to effectivelyaddress medical dialogue summarization. The process of medical reporting isoften time-consuming for healthcare professionals. Implementing medicaldialogue summarization techniques presents a viable solution to alleviate thistime constraint by generating automated medical reports. The effectiveness ofLLMs in this process is significantly influenced by the formulation of theprompt, which plays a crucial role in determining the quality and relevance ofthe generated reports. In this research, we used a combination of two distinctprompting strategies, known as shot prompting and pattern prompting to enhancethe performance of automated medical reporting. The evaluation of the automatedmedical reports is carried out using the ROUGE score and a human evaluationwith the help of an expert panel. The two-shot prompting approach incombination with scope and domain context outperforms other methods andachieves the highest score when compared to the human reference set by ageneral practitioner. However, the automated reports are approximately twice aslong as the human references, due to the addition of both redundant andrelevant statements that are added to the report."
Large Language Model-Driven Classroom Flipping: Empowering  Student-Centric Peer Questioning with Flipped Interaction,['Chee Wei Tan'],http://arxiv.org/pdf/2311.14708v1.pdf,2023-11-14,"['cs.cy', 'cs.ai', 'cs.cl', 'cs.hc']","  Reciprocal questioning is essential for effective teaching and learning,fostering active engagement and deeper understanding through collaborativeinteractions, especially in large classrooms. Can large language model (LLM),such as OpenAI's GPT (Generative Pre-trained Transformer) series, assist inthis? This paper investigates a pedagogical approach of classroom flippingbased on flipped interaction in LLMs. Flipped interaction involves usinglanguage models to prioritize generating questions instead of answers toprompts. We demonstrate how traditional classroom flipping techniques,including Peer Instruction and Just-in-Time Teaching (JiTT), can be enhancedthrough flipped interaction techniques, creating student-centric questions forhybrid teaching. In particular, we propose a workflow to integrate promptengineering with clicker and JiTT quizzes by a poll-prompt-quiz routine and aquiz-prompt-discuss routine to empower students to self-regulate their learningcapacity and enable teachers to swiftly personalize training pathways. Wedevelop an LLM-driven chatbot software that digitizes various elements ofclassroom flipping and facilitates the assessment of students using theseroutines to deliver peer-generated questions. We have applied our LLM-drivenchatbot software for teaching both undergraduate and graduate students from2020 to 2022, effectively useful for bridging the gap between teachers andstudents in remote teaching during the COVID-19 pandemic years. In particular,LLM-driven classroom flipping can be particularly beneficial in large classsettings to optimize teaching pace and enable engaging classroom experiences."
Quantification of cardiac capillarization in single-immunostained  myocardial slices using weakly supervised instance segmentation,"['Zhao Zhang', 'Xiwen Chen', 'William Richardson', 'Bruce Z. Gao', 'Abolfazl Razi', 'Tong Ye']",http://arxiv.org/pdf/2311.18173v1.pdf,2023-11-30,"['eess.iv', 'cs.ce', 'cs.cv']","  Decreased myocardial capillary density has been reported as an importanthistopathological feature associated with various heart disorders. Quantitativeassessment of cardiac capillarization typically involves double immunostainingof cardiomyocytes (CMs) and capillaries in myocardial slices. In contrast,single immunostaining of basement membrane components is a straightforwardapproach to simultaneously label CMs and capillaries, presenting fewerchallenges in background staining. However, subsequent image analysis alwaysrequires manual work in identifying and segmenting CMs and capillaries. Here,we developed an image analysis tool, AutoQC, to automatically identify andsegment CMs and capillaries in immunofluorescence images of collagen type IV, apredominant basement membrane protein within the myocardium. In addition,commonly used capillarization-related measurements can be derived fromsegmentation masks. AutoQC features a weakly supervised instance segmentationalgorithm by leveraging the power of a pre-trained segmentation model viaprompt engineering. AutoQC outperformed YOLOv8-Seg, a state-of-the-art instancesegmentation model, in both instance segmentation and capillarizationassessment. Furthermore, the training of AutoQC required only a small datasetwith bounding box annotations instead of pixel-wise annotations, leading to areduced workload during network training. AutoQC provides an automated solutionfor quantifying cardiac capillarization in basement-membrane-immunostainedmyocardial slices, eliminating the need for manual image analysis once it istrained."
Lessons from Building CodeBuddy: A Contextualized AI Coding Assistant,"['gustavo Pinto', 'Cleidson de Souza', 'João Batista Neto', 'Alberto de Souza', 'Tarcísio Gotto', 'Edward Monteiro']",http://arxiv.org/pdf/2311.18450v1.pdf,2023-11-30,['cs.se'],"  With their exceptional natural language processing capabilities, tools basedon Large Language Models (LLMs) like ChatGPT and Co-Pilot have swiftly becomeindispensable resources in the software developer's toolkit. While recentstudies suggest the potential productivity gains these tools can unlock, usersstill encounter drawbacks, such as generic or incorrect answers. Additionally,the pursuit of improved responses often leads to extensive prompt engineeringefforts, diverting valuable time from writing code that delivers actual value.To address these challenges, a new breed of tools, built atop LLMs, isemerging. These tools aim to mitigate drawbacks by employing techniques likefine-tuning or enriching user prompts with contextualized information.  In this paper, we delve into the lessons learned by a software developmentteam venturing into the creation of such a contextualized LLM-basedapplication, using retrieval-based techniques, called CodeBuddy. Over afour-month period, the team, despite lacking prior professional experience inLLM-based applications, built the product from scratch. Following the initialproduct release, we engaged with the development team responsible for the codegenerative components. Through interviews and analysis of the application'sissue tracker, we uncover various intriguing challenges that teams working onLLM-based applications might encounter. For instance, we found three main groupof lessons: LLM-based lessons, User-based lessons, and Technical lessons. Byunderstanding these lessons, software development teams could become betterprepared to build LLM-based applications."
Semantic-Aware Frame-Event Fusion based Pattern Recognition via Large  Vision-Language Models,"['Dong Li', 'Jiandong Jin', 'Yuhao Zhang', 'Yanlin Zhong', 'Yaoyang Wu', 'Lan Chen', 'Xiao Wang', 'Bin Luo']",http://arxiv.org/pdf/2311.18592v1.pdf,2023-11-30,"['cs.cv', 'cs.ai']","  Pattern recognition through the fusion of RGB frames and Event streams hasemerged as a novel research area in recent years. Current methods typicallyemploy backbone networks to individually extract the features of RGB frames andevent streams, and subsequently fuse these features for pattern recognition.However, we posit that these methods may suffer from key issues like sematicgaps and small-scale backbone networks. In this study, we introduce a novelpattern recognition framework that consolidates the semantic labels, RGBframes, and event streams, leveraging pre-trained large-scale vision-languagemodels. Specifically, given the input RGB frames, event streams, and all thepredefined semantic labels, we employ a pre-trained large-scale vision model(CLIP vision encoder) to extract the RGB and event features. To handle thesemantic labels, we initially convert them into language descriptions throughprompt engineering, and then obtain the semantic features using the pre-trainedlarge-scale language model (CLIP text encoder). Subsequently, we integrate theRGB/Event features and semantic features using multimodal Transformer networks.The resulting frame and event tokens are further amplified using self-attentionlayers. Concurrently, we propose to enhance the interactions between texttokens and RGB/Event tokens via cross-attention. Finally, we consolidate allthree modalities using self-attention and feed-forward layers for recognition.Comprehensive experiments on the HARDVS and PokerEvent datasets fullysubstantiate the efficacy of our proposed SAFE model. The source code will bemade available at https://github.com/Event-AHU/SAFE_LargeVLM."
Intrusion Detection System with Machine Learning and Multiple Datasets,"['Haiyan Xuan', 'Mohith Manohar']",http://arxiv.org/pdf/2312.01941v1.pdf,2023-12-04,"['cs.cr', 'cs.lg', 'cs.ni']","  As Artificial Intelligence (AI) technologies continue to gain traction in themodern-day world, they ultimately pose an immediate threat to currentcybersecurity systems via exploitative methods. Prompt engineering is arelatively new field that explores various prompt designs that can hijack largelanguage models (LLMs). If used by an unethical attacker, it can enable an AIsystem to offer malicious insights and code to them. In this paper, an enhancedintrusion detection system (IDS) that utilizes machine learning (ML) andhyperparameter tuning is explored, which can improve a model's performance interms of accuracy and efficacy. Ultimately, this improved system can be used tocombat the attacks made by unethical hackers. A standard IDS is solelyconfigured with pre-configured rules and patterns; however, with theutilization of machine learning, implicit and different patterns can begenerated through the models' hyperparameter settings and parameters. Inaddition, the IDS will be equipped with multiple datasets so that the accuracyof the models improves. We evaluate the performance of multiple ML models andtheir respective hyperparameter settings through various metrics to comparetheir results to other models and past research work. The results of theproposed multi-dataset integration method yielded an accuracy score of 99.9%when equipped with the XGBoost and random forest classifiers andRandomizedSearchCV hyperparameter technique."
Evaluating and Mitigating Discrimination in Language Model Decisions,"['Alex Tamkin', 'Amanda Askell', 'Liane Lovitt', 'Esin Durmus', 'Nicholas Joseph', 'Shauna Kravec', 'Karina Nguyen', 'Jared Kaplan', 'Deep Ganguli']",http://arxiv.org/pdf/2312.03689v1.pdf,2023-12-06,['cs.cl'],"  As language models (LMs) advance, interest is growing in applying them tohigh-stakes societal decisions, such as determining financing or housingeligibility. However, their potential for discrimination in such contextsraises ethical concerns, motivating the need for better methods to evaluatethese risks. We present a method for proactively evaluating the potentialdiscriminatory impact of LMs in a wide range of use cases, includinghypothetical use cases where they have not yet been deployed. Specifically, weuse an LM to generate a wide array of potential prompts that decision-makersmay input into an LM, spanning 70 diverse decision scenarios across society,and systematically vary the demographic information in each prompt. Applyingthis methodology reveals patterns of both positive and negative discriminationin the Claude 2.0 model in select settings when no interventions are applied.While we do not endorse or permit the use of language models to make automateddecisions for the high-risk use cases we study, we demonstrate techniques tosignificantly decrease both positive and negative discrimination throughcareful prompt engineering, providing pathways toward safer deployment in usecases where they may be appropriate. Our work enables developers andpolicymakers to anticipate, measure, and address discrimination as languagemodel capabilities and applications continue to expand. We release our datasetand prompts at https://huggingface.co/datasets/Anthropic/discrim-eval"
DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt  Engineer,"['Junyuan Hong', 'Jiachen T. Wang', 'Chenhui Zhang', 'Zhangheng Li', 'Bo Li', 'Zhangyang Wang']",http://arxiv.org/pdf/2312.03724v1.pdf,2023-11-27,"['cs.cl', 'cs.ai']","  Large Language Models (LLMs) have emerged as dominant tools for varioustasks, particularly when tailored for a specific target by prompt tuning.Nevertheless, concerns surrounding data privacy present obstacles due to thetuned prompts' dependency on sensitive private information. A practicalsolution is to host a local LLM and optimize a soft prompt privately usingdata. Yet, hosting a local model becomes problematic when model ownership isprotected. Alternative methods, like sending data to the model's provider fortraining, intensify these privacy issues facing an untrusted provider. In thispaper, we present a novel solution called Differentially-Private Offsite PromptTuning (DP-OPT) to address this challenge. Our approach involves tuning adiscrete prompt on the client side and then applying it to the desired cloudmodels. We demonstrate that prompts suggested by LLMs themselves can betransferred without compromising performance significantly. To ensure that theprompts do not leak private information, we introduce the first private promptgeneration mechanism, by a differentially-private (DP) ensemble of in-contextlearning with private demonstrations. With DP-OPT, generatingprivacy-preserving prompts by Vicuna-7b can yield competitive performancecompared to non-private in-context learning on GPT3.5 or local private prompttuning. Codes are available at https://github.com/VITA-Group/DP-OPT ."
Applying Large Language Models and Chain-of-Thought for Automatic  Scoring,"['Gyeong-Geon Lee', 'Ehsan Latif', 'Xuansheng Wu', 'Ninghao Liu', 'Xiaoming Zhai']",http://arxiv.org/pdf/2312.03748v1.pdf,2023-11-30,"['cs.cl', 'cs.ai']","  This study investigates the application of large language models (LLMs),specifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT)in the automaticscoring of student-written responses to science assessments. We focused onovercoming the challenges of accessibility, technical complexity, and lack ofexplainability that have previously limited the use of automatic assessmenttools among researchers and educators. We used a testing dataset comprising sixassessment tasks (three binomial and three trinomial) with 1,650 studentresponses. We employed six prompt engineering strategies, combining zero-shotor few-shot learning with CoT, either alone or alongside item stem and scoringrubrics. Results indicated that few-shot (acc = .67) outperformed zero-shotlearning (acc = .60), with 12.6\% increase. CoT, when used without item stemand scoring rubrics, did not significantly affect scoring accuracy (acc = .60).However, CoT prompting paired with contextual item stems and rubrics proved tobe a significant contributor to scoring accuracy (13.44\% increase forzero-shot; 3.7\% increase for few-shot). Using a novel approach PPEAS, we founda more balanced accuracy across different proficiency categories, highlightingthe importance of domain-specific reasoning in enhancing the effectiveness ofLLMs in scoring tasks. Additionally, we also found that GPT-4 demonstratedsuperior performance over GPT-3.5 in various scoring tasks, showing 8.64\%difference. The study revealed that the single-call strategy with GPT-4,particularly using greedy sampling, outperformed other approaches, includingensemble voting strategies. This study demonstrates the potential of LLMs infacilitating automatic scoring, emphasizing that CoT enhances accuracy,particularly when used with item stem and scoring rubrics."
Evaluating ChatGPT as a Question Answering System: A Comprehensive  Analysis and Comparison with Existing Models,"['Hossein Bahak', 'Farzaneh Taheri', 'Zahra Zojaji', 'Arefeh Kazemi']",http://arxiv.org/pdf/2312.07592v1.pdf,2023-12-11,"['cs.cl', 'cs.ai', 'i.2, i.7']","  In the current era, a multitude of language models has emerged to cater touser inquiries. Notably, the GPT-3.5 Turbo language model has gainedsubstantial attention as the underlying technology for ChatGPT. Leveragingextensive parameters, this model adeptly responds to a wide range of questions.However, due to its reliance on internal knowledge, the accuracy of responsesmay not be absolute. This article scrutinizes ChatGPT as a Question AnsweringSystem (QAS), comparing its performance to other existing QASs. The primaryfocus is on evaluating ChatGPT's proficiency in extracting responses fromprovided paragraphs, a core QAS capability. Additionally, performancecomparisons are made in scenarios without a surrounding passage. Multipleexperiments, exploring response hallucination and considering questioncomplexity, were conducted on ChatGPT. Evaluation employed well-known QuestionAnswering (QA) datasets, including SQuAD, NewsQA, and PersianQuAD, acrossEnglish and Persian languages. Metrics such as F-score, exact match, andaccuracy were employed in the assessment. The study reveals that, while ChatGPTdemonstrates competence as a generative model, it is less effective in questionanswering compared to task-specific models. Providing context improves itsperformance, and prompt engineering enhances precision, particularly forquestions lacking explicit answers in provided paragraphs. ChatGPT excels atsimpler factual questions compared to ""how"" and ""why"" question types. Theevaluation highlights occurrences of hallucinations, where ChatGPT providesresponses to questions without available answers in the provided context."
MotherNet: A Foundational Hypernetwork for Tabular Classification,"['Andreas Müller', 'Carlo Curino', 'Raghu Ramakrishnan']",http://arxiv.org/pdf/2312.08598v1.pdf,2023-12-14,"['cs.lg', 'i.2.6']","  The advent of Foundation Models is transforming machine learning across manymodalities (e.g., language, images, videos) with prompt engineering replacingtraining in many settings. Recent work on tabular data (e.g., TabPFN) hints ata similar opportunity to build Foundation Models for classification fornumerical data. In this paper, we go one step further and propose ahypernetwork architecture that we call MotherNet, trained on millions ofclassification tasks, that, once prompted with a never-seen-before training setgenerates the weights of a trained ``child'' neural-network. Like otherFoundation Models, MotherNet replaces training on specific datasets within-context learning through a single forward pass. In contrast to existinghypernetworks that were either task-specific or trained for relativelyconstraint multi-task settings, MotherNet is trained to generate networks toperform multiclass classification on arbitrary tabular datasets without anydataset specific gradient descent.  The child network generated by MotherNet using in-context learningoutperforms neural networks trained using gradient descent on small datasets,and is competitive with predictions by TabPFN and standard ML methods likeGradient Boosting. Unlike a direct application of transformer models likeTabPFN, MotherNet generated networks are highly efficient at inference time.This methodology opens up a new approach to building predictive models ontabular data that is both efficient and robust, without any dataset-specifictraining."
SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained  Geometry and Appearance,"['Yuanyou Xu', 'Zongxin Yang', 'Yi Yang']",http://arxiv.org/pdf/2312.08889v1.pdf,2023-12-13,['cs.cv'],"  Powered by large-scale text-to-image generation models, text-to-3D avatargeneration has made promising progress. However, most methods fail to producephotorealistic results, limited by imprecise geometry and low-qualityappearance. Towards more practical avatar generation, we present SEEAvatar, amethod for generating photorealistic 3D avatars from text with SElf-Evolvingconstraints for decoupled geometry and appearance. For geometry, we propose toconstrain the optimized avatar in a decent global shape with a template avatar.The template avatar is initialized with human prior and can be updated by theoptimized avatar periodically as an evolving template, which enables moreflexible shape generation. Besides, the geometry is also constrained by thestatic human prior in local parts like face and hands to maintain the delicatestructures. For appearance generation, we use diffusion model enhanced byprompt engineering to guide a physically based rendering pipeline to generaterealistic textures. The lightness constraint is applied on the albedo textureto suppress incorrect lighting effect. Experiments show that our methodoutperforms previous methods on both global and local geometry and appearancequality by a large margin. Since our method can produce high-quality meshes andtextures, such assets can be directly applied in classic graphics pipeline forrealistic rendering under any lighting condition. Project page at:https://seeavatar3d.github.io."
Prompt-based Distribution Alignment for Unsupervised Domain Adaptation,"['Shuanghao Bai', 'Min Zhang', 'Wanqi Zhou', 'Siteng Huang', 'Zhirong Luan', 'Donglin Wang', 'Badong Chen']",http://arxiv.org/pdf/2312.09553v1.pdf,2023-12-15,['cs.cv'],"  Recently, despite the unprecedented success of large pre-trainedvisual-language models (VLMs) on a wide range of downstream tasks, thereal-world unsupervised domain adaptation (UDA) problem is still not wellexplored. Therefore, in this paper, we first experimentally demonstrate thatthe unsupervised-trained VLMs can significantly reduce the distributiondiscrepancy between source and target domains, thereby improving theperformance of UDA. However, a major challenge for directly deploying suchmodels on downstream UDA tasks is prompt engineering, which requires aligningthe domain knowledge of source and target domains, since the performance of UDAis severely influenced by a good domain-invariant representation. We furtherpropose a Prompt-based Distribution Alignment (PDA) method to incorporate thedomain knowledge into prompt learning. Specifically, PDA employs a two-branchprompt-tuning paradigm, namely base branch and alignment branch. The basebranch focuses on integrating class-related representation into prompts,ensuring discrimination among different classes. To further minimize domaindiscrepancy, for the alignment branch, we construct feature banks for both thesource and target domains and propose image-guided feature tuning (IFT) to makethe input attend to feature banks, which effectively integrates self-enhancedand cross-domain features into the model. In this way, these two branches canbe mutually promoted to enhance the adaptation of VLMs for UDA. We conductextensive experiments on three benchmarks to demonstrate that our proposed PDAachieves state-of-the-art performance. The code is available athttps://github.com/BaiShuanghao/Prompt-based-Distribution-Alignment."
BigBIO: A Framework for Data-Centric Biomedical Natural Language  Processing,"['Jason Alan Fries', 'Leon Weber', 'Natasha Seelam', 'Gabriel Altay', 'Debajyoti Datta', 'Samuele Garda', 'Myungsun Kang', 'Ruisi Su', 'Wojciech Kusa', 'Samuel Cahyawijaya', 'Fabio Barth', 'Simon Ott', 'Matthias Samwald', 'Stephen Bach', 'Stella Biderman', 'Mario Sänger', 'Bo Wang', 'Alison Callahan', 'Daniel León Periñán', 'Théo Gigant', 'Patrick Haller', 'Jenny Chim', 'Jose David Posada', 'John Michael Giorgi', 'Karthik Rangasai Sivaraman', 'Marc Pàmies', 'Marianna Nezhurina', 'Robert Martin', 'Michael Cullan', 'Moritz Freidank', 'Nathan Dahlberg', 'Shubhanshu Mishra', 'Shamik Bose', 'Nicholas Michio Broad', 'Yanis Labrak', 'Shlok S Deshmukh', 'Sid Kiblawi', 'Ayush Singh', 'Minh Chien Vu', 'Trishala Neeraj', 'Jonas Golde', 'Albert Villanova del Moral', 'Benjamin Beilharz']",http://arxiv.org/pdf/2206.15076v1.pdf,2022-06-30,['cs.cl'],"  Training and evaluating language models increasingly requires theconstruction of meta-datasets --diverse collections of curated data with clearprovenance. Natural language prompting has recently lead to improved zero-shotgeneralization by transforming existing, supervised datasets into a diversityof novel pretraining tasks, highlighting the benefits of meta-dataset curation.While successful in general-domain text, translating these data-centricapproaches to biomedical language modeling remains challenging, as labeledbiomedical datasets are significantly underrepresented in popular data hubs. Toaddress this challenge, we introduce BigBIO a community library of 126+biomedical NLP datasets, currently covering 12 task categories and 10+languages. BigBIO facilitates reproducible meta-dataset curation viaprogrammatic access to datasets and their metadata, and is compatible withcurrent platforms for prompt engineering and end-to-end few/zero shot languagemodel evaluation. We discuss our process for task schema harmonization, dataauditing, contribution guidelines, and outline two illustrative use cases:zero-shot evaluation of biomedical prompts and large-scale, multi-tasklearning. BigBIO is an ongoing community effort and is available athttps://github.com/bigscience-workshop/biomedical"
GPT Takes the Bar Exam,"['Michael Bommarito II', 'Daniel Martin Katz']",http://arxiv.org/pdf/2212.14402v1.pdf,2022-12-29,"['cs.cl', 'cs.ai', 'cs.lg']","  Nearly all jurisdictions in the United States require a professional licenseexam, commonly referred to as ""the Bar Exam,"" as a precondition for lawpractice. To even sit for the exam, most jurisdictions require that anapplicant completes at least seven years of post-secondary education, includingthree years at an accredited law school. In addition, most test-takers alsoundergo weeks to months of further, exam-specific preparation. Despite thissignificant investment of time and capital, approximately one in fivetest-takers still score under the rate required to pass the exam on their firsttry. In the face of a complex task that requires such depth of knowledge, what,then, should we expect of the state of the art in ""AI?"" In this research, wedocument our experimental evaluation of the performance of OpenAI's`text-davinci-003` model, often-referred to as GPT-3.5, on the multistatemultiple choice (MBE) section of the exam. While we find no benefit infine-tuning over GPT-3.5's zero-shot performance at the scale of our trainingdata, we do find that hyperparameter optimization and prompt engineeringpositively impacted GPT-3.5's zero-shot performance. For best prompt andparameters, GPT-3.5 achieves a headline correct rate of 50.3% on a completeNCBE MBE practice exam, significantly in excess of the 25% baseline guessingrate, and performs at a passing rate for both Evidence and Torts. GPT-3.5'sranking of responses is also highly-correlated with correctness; its top twoand top three choices are correct 71% and 88% of the time, respectively,indicating very strong non-entailment performance. While our ability tointerpret these results is limited by nascent scientific understanding of LLMsand the proprietary nature of GPT, we believe that these results stronglysuggest that an LLM will pass the MBE component of the Bar Exam in the nearfuture."
"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on  Reasoning, Hallucination, and Interactivity","['Yejin Bang', 'Samuel Cahyawijaya', 'Nayeon Lee', 'Wenliang Dai', 'Dan Su', 'Bryan Wilie', 'Holy Lovenia', 'Ziwei Ji', 'Tiezheng Yu', 'Willy Chung', 'Quyet V. Do', 'Yan Xu', 'Pascale Fung']",http://arxiv.org/pdf/2302.04023v4.pdf,2023-02-08,"['cs.cl', 'cs.ai']","  This paper proposes a framework for quantitatively evaluating interactiveLLMs such as ChatGPT using publicly available data sets. We carry out anextensive technical evaluation of ChatGPT using 23 data sets covering 8different common NLP application tasks. We evaluate the multitask, multilingualand multi-modal aspects of ChatGPT based on these data sets and a newlydesigned multimodal dataset. We find that ChatGPT outperforms LLMs withzero-shot learning on most tasks and even outperforms fine-tuned models on sometasks. We find that it is better at understanding non-Latin script languagesthan generating them. It is able to generate multimodal content from textualprompts, via an intermediate code generation step. Moreover, we find thatChatGPT is 63.41% accurate on average in 10 different reasoning categoriesunder logical reasoning, non-textual reasoning, and commonsense reasoning,hence making it an unreliable reasoner. It is, for example, better at deductivethan inductive reasoning. ChatGPT suffers from hallucination problems likeother LLMs and it generates more extrinsic hallucinations from its parametricmemory as it does not have access to an external knowledge base. Finally, theinteractive feature of ChatGPT enables human collaboration with the underlyingLLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++on machine translation, in a multi-turn ""prompt engineering"" fashion. We alsorelease codebase for evaluation set extraction."
Few-shot Multimodal Multitask Multilingual Learning,"['Aman Chadha', 'Vinija Jain']",http://arxiv.org/pdf/2303.12489v1.pdf,2023-02-19,"['cs.lg', 'cs.ai', 'cs.cl', 'cs.cv', 'cs.mm']","  While few-shot learning as a transfer learning paradigm has gainedsignificant traction for scenarios with limited data, it has primarily beenexplored in the context of building unimodal and unilingual models.Furthermore, a significant part of the existing literature in the domain offew-shot multitask learning perform in-context learning which requires manuallygenerated prompts as the input, yielding varying outcomes depending on thelevel of manual prompt-engineering. In addition, in-context learning suffersfrom substantial computational, memory, and storage costs which eventuallyleads to high inference latency because it involves running all of the prompt'sexamples through the model every time a prediction is made. In contrast,methods based on the transfer learning via the fine-tuning paradigm avoid theaforementioned issues at a one-time cost of fine-tuning weights on a per-taskbasis. However, such methods lack exposure to few-shot multimodal multitasklearning. In this paper, we propose few-shot learning for a multimodalmultitask multilingual (FM3) setting by adapting pre-trained vision andlanguage models using task-specific hypernetworks and contrastively fine-tuningthem to enable few-shot learning. FM3's architecture combines the best of bothworlds of in-context and fine-tuning based learning and consists of three majorcomponents: (i) multimodal contrastive fine-tuning to enable few-shot learning,(ii) hypernetwork task adaptation to perform multitask learning, and (iii)task-specific output heads to cater to a plethora of diverse tasks. FM3 learnsthe most prominent tasks in the vision and language domains along with theirintersections, namely visual entailment (VE), visual question answering (VQA),and natural language understanding (NLU) tasks such as neural entityrecognition (NER) and the GLUE benchmark including QNLI, MNLI, QQP, and SST-2."
Improving Few-Shot Prompts with Relevant Static Analysis Products,"['Toufique Ahmed', 'Kunal Suresh Pai', 'Premkumar Devanbu', 'Earl T. Barr']",http://arxiv.org/pdf/2304.06815v2.pdf,2023-04-13,"['cs.se', 'cs.lg']","  Large Language Models (LLM) are a new class of computation engines,""programmed"" via prompt engineering. We are still learning how to best""program"" these LLMs to help developers. We start with the intuition thatdevelopers tend to consciously and unconsciously have a collection of semanticsfacts in mind when working on coding tasks. Mostly these are shallow, simplefacts arising from a quick read. For a function, examples of facts mightinclude parameter and local variable names, return expressions, simple pre- andpost-conditions, and basic control and data flow, etc.  One might assume that the powerful multi-layer architecture oftransformer-style LLMs makes them inherently capable of doing this simple levelof ""code analysis"" and extracting such information, implicitly, whileprocessing code: but are they, really? If they aren't, could explicitly addingthis information help? Our goal here is to investigate this question, using thecode summarization task and evaluate whether automatically augmenting an LLM'sprompt with semantic facts explicitly, actually helps.  Prior work shows that LLM performance on code summarization benefits fromfew-shot samples drawn either from the same-project or from examples found viainformation retrieval methods (such as BM25). While summarization performancehas steadily increased since the early days, there is still room forimprovement: LLM performance on code summarization still lags its performanceon natural-language tasks like translation and text summarization.  We find that adding semantic facts actually does help! This approach improvesperformance in several different settings suggested by prior work, includingfor two different Large Language Models. In most cases, improvement nears orexceeds 2 BLEU; for the PHP language in the challenging CodeSearchNet dataset,this augmentation actually yields performance surpassing 30 BLEU."
Evaluation of GPT-3.5 and GPT-4 for supporting real-world information  needs in healthcare delivery,"['Debadutta Dash', 'Rahul Thapa', 'Juan M. Banda', 'Akshay Swaminathan', 'Morgan Cheatham', 'Mehr Kashyap', 'Nikesh Kotecha', 'Jonathan H. Chen', 'Saurabh Gombar', 'Lance Downing', 'Rachel Pedreira', 'Ethan Goh', 'Angel Arnaout', 'Garret Kenn Morris', 'Honor Magon', 'Matthew P Lungren', 'Eric Horvitz', 'Nigam H. Shah']",http://arxiv.org/pdf/2304.13714v3.pdf,2023-04-26,"['cs.ai', 'cs.cl', 'cs.ir']","  Despite growing interest in using large language models (LLMs) in healthcare,current explorations do not assess the real-world utility and safety of LLMs inclinical settings. Our objective was to determine whether two LLMs can serveinformation needs submitted by physicians as questions to an informaticsconsultation service in a safe and concordant manner. Sixty six questions froman informatics consult service were submitted to GPT-3.5 and GPT-4 via simpleprompts. 12 physicians assessed the LLM responses' possibility of patient harmand concordance with existing reports from an informatics consultation service.Physician assessments were summarized based on majority vote. For no questionsdid a majority of physicians deem either LLM response as harmful. For GPT-3.5,responses to 8 questions were concordant with the informatics consult report,20 discordant, and 9 were unable to be assessed. There were 29 responses withno majority on ""Agree"", ""Disagree"", and ""Unable to assess"". For GPT-4,responses to 13 questions were concordant, 15 discordant, and 3 were unable tobe assessed. There were 35 responses with no majority. Responses from both LLMswere largely devoid of overt harm, but less than 20% of the responses agreedwith an answer from an informatics consultation service, responses containedhallucinated references, and physicians were divided on what constitutes harm.These results suggest that while general purpose LLMs are able to provide safeand credible responses, they often do not meet the specific information need ofa given question. A definitive evaluation of the usefulness of LLMs inhealthcare settings will likely require additional research on promptengineering, calibration, and custom-tailoring of general purpose models."
Zelda: Video Analytics using Vision-Language Models,"['Francisco Romero', 'Caleb Winston', 'Johann Hauswald', 'Matei Zaharia', 'Christos Kozyrakis']",http://arxiv.org/pdf/2305.03785v2.pdf,2023-05-05,['cs.db'],"  Advances in ML have motivated the design of video analytics systems thatallow for structured queries over video datasets. However, existing systemslimit query expressivity, require users to specify an ML model per predicate,rely on complex optimizations that trade off accuracy for performance, andreturn large amounts of redundant and low-quality results. This paper focuseson the recently developed Vision-Language Models (VLMs) that allow users toquery images using natural language like ""cars during daytime at trafficintersections."" Through an in-depth analysis, we show VLMs address threelimitations of current video analytics systems: general expressivity, a singlegeneral purpose model to query many predicates, and are both simple and fast.However, VLMs still return large numbers of redundant and low-quality resultsthat can overwhelm and burden users. In addition, VLMs often require manualprompt engineering to improve result relevance.  We present Zelda: a video analytics system that uses VLMs to return bothrelevant and semantically diverse results for top-K queries on large videodatasets. Zelda prompts the VLM with the user's query in natural language.Zelda then automatically adds discriminator and synonym terms to boostaccuracy, and terms to identify low-quality frames. To improve resultdiversity, Zelda uses semantic-rich VLM embeddings in an algorithm that prunessimilar frames while considering their relevance to the query and the number oftop-K results requested. We evaluate Zelda across five datasets and 19 queriesand quantitatively show it achieves higher mean average precision (up to 1.15x)and improves average pairwise similarity (up to 1.16x) compared to using VLMsout-of-the-box. We also compare Zelda to a state-of-the-art video analyticsengine and show that Zelda retrieves results 7.5x (up to 10.4x) faster for thesame accuracy and frame diversity."
ConES: Concept Embedding Search for Parameter Efficient Tuning Large  Vision Language Models,"['Huahui Yi', 'Ziyuan Qin', 'Wei Xu', 'Miaotian Guo', 'Kun Wang', 'Shaoting Zhang', 'Kang Li', 'Qicheng Lao']",http://arxiv.org/pdf/2305.18993v1.pdf,2023-05-30,['cs.cv'],"  Large pre-trained vision-language models have shown great prominence intransferring pre-acquired knowledge to various domains and downstream taskswith appropriate prompting or tuning. Existing prevalent tuning methods can begenerally categorized into three genres: 1) prompt engineering by creatingsuitable prompt texts, which is time-consuming and requires domain expertise;2) or simply fine-tuning the whole model, which is extremely inefficient; 3)prompt tuning through parameterized prompt embeddings with the text encoder.Nevertheless, all methods rely on the text encoder for bridging the modalitygap between vision and language. In this work, we question the necessity of thecumbersome text encoder for a more lightweight and efficient tuning paradigm aswell as more representative prompt embeddings closer to the imagerepresentations. To achieve this, we propose a Concept Embedding Search (ConES)approach by optimizing prompt embeddings -- without the need of the textencoder -- to capture the 'concept' of the image modality through a variety oftask objectives. By dropping the text encoder, we are able to significantlyspeed up the learning process, \eg, from about an hour to just ten minutes inour experiments for personalized text-to-image generation without impairing thegeneration quality. Moreover, our proposed approach is orthogonal to currentexisting tuning methods since the searched concept embeddings can be furtherutilized in the next stage of fine-tuning the pre-trained large models forboosting performance. Extensive experiments show that our approach can beat theprompt tuning and textual inversion methods in a variety of downstream tasksincluding objection detection, instance segmentation, and image generation. Ourapproach also shows better generalization capability for unseen concepts inspecialized domains, such as the medical domain."
ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF  Synthesis,"['Zhiling Zheng', 'Oufan Zhang', 'Christian Borgs', 'Jennifer T. Chayes', 'Omar M. Yaghi']",http://arxiv.org/pdf/2306.11296v2.pdf,2023-06-20,"['cs.ir', 'cond-mat.mtrl-sci', 'cs.cl', 'physics.chem-ph']","  We use prompt engineering to guide ChatGPT in the automation of text miningof metal-organic frameworks (MOFs) synthesis conditions from diverse formatsand styles of the scientific literature. This effectively mitigates ChatGPT'stendency to hallucinate information -- an issue that previously made the use ofLarge Language Models (LLMs) in scientific fields challenging. Our approachinvolves the development of a workflow implementing three different processesfor text mining, programmed by ChatGPT itself. All of them enable parsing,searching, filtering, classification, summarization, and data unification withdifferent tradeoffs between labor, speed, and accuracy. We deploy this systemto extract 26,257 distinct synthesis parameters pertaining to approximately 800MOFs sourced from peer-reviewed research articles. This process incorporatesour ChemPrompt Engineering strategy to instruct ChatGPT in text mining,resulting in impressive precision, recall, and F1 scores of 90-99%.Furthermore, with the dataset built by text mining, we constructed amachine-learning model with over 86% accuracy in predicting MOF experimentalcrystallization outcomes and preliminarily identifying important factors in MOFcrystallization. We also developed a reliable data-grounded MOF chatbot toanswer questions on chemical reactions and synthesis procedures. Given that theprocess of using ChatGPT reliably mines and tabulates diverse MOF synthesisinformation in a unified format, while using only narrative language requiringno coding expertise, we anticipate that our ChatGPT Chemistry Assistant will bevery useful across various other chemistry sub-disciplines."
Identifying and Extracting Rare Disease Phenotypes with Large Language  Models,"['Cathy Shyr', 'Yan Hu', 'Paul A. Harris', 'Hua Xu']",http://arxiv.org/pdf/2306.12656v1.pdf,2023-06-22,"['cs.cl', 'cs.ai']","  Rare diseases (RDs) are collectively common and affect 300 million peopleworldwide. Accurate phenotyping is critical for informing diagnosis andtreatment, but RD phenotypes are often embedded in unstructured text andtime-consuming to extract manually. While natural language processing (NLP)models can perform named entity recognition (NER) to automate extraction, amajor bottleneck is the development of a large, annotated corpus for modeltraining. Recently, prompt learning emerged as an NLP paradigm that can lead tomore generalizable results without any (zero-shot) or few labeled samples(few-shot). Despite growing interest in ChatGPT, a revolutionary large languagemodel capable of following complex human prompts and generating high-qualityresponses, none have studied its NER performance for RDs in the zero- andfew-shot settings. To this end, we engineered novel prompts aimed at extractingRD phenotypes and, to the best of our knowledge, are the first the establish abenchmark for evaluating ChatGPT's performance in these settings. We comparedits performance to the traditional fine-tuning approach and conducted anin-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted inhigher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in thezero- and few-shot settings, respectively). Despite this, ChatGPT achievedsimilar or higher accuracy for certain entities (i.e., rare diseases and signs)in the one-shot setting (F1 of 0.776 and 0.725). This suggests that withappropriate prompt engineering, ChatGPT has the potential to match oroutperform fine-tuned language models for certain entity types with just onelabeled sample. While the proliferation of large language models may provideopportunities for supporting RD diagnosis and treatment, researchers andclinicians should critically evaluate model outputs and be well-informed oftheir limitations."
Demonstrations of the Potential of AI-based Political Issue Polling,"['Nathan E. Sanders', 'Alex Ulinich', 'Bruce Schneier']",http://arxiv.org/pdf/2307.04781v2.pdf,2023-07-10,['cs.cy'],"  Political polling is a multi-billion dollar industry with outsized influenceon the societal trajectory of the United States and nations around the world.However, it has been challenged by factors that stress its cost, availability,and accuracy. At the same time, artificial intelligence (AI) chatbots havebecome compelling stand-ins for human behavior, powered by increasinglysophisticated large language models (LLMs). Could AI chatbots be an effectivetool for anticipating public opinion on controversial issues to the extent thatthey could be used by campaigns, interest groups, and polling firms? We havedeveloped a prompt engineering methodology for eliciting human-like surveyresponses from ChatGPT, which simulate the response to a policy question of aperson described by a set of demographic factors, and produce both an ordinalnumeric response score and a textual justification. We execute large scaleexperiments, querying for thousands of simulated responses at a cost far lowerthan human surveys. We compare simulated data to human issue polling data fromthe Cooperative Election Study (CES). We find that ChatGPT is effective atanticipating both the mean level and distribution of public opinion on avariety of policy issues such as abortion bans and approval of the US SupremeCourt, particularly in their ideological breakdown (correlation typically>85%). However, it is less successful at anticipating demographic-leveldifferences. Moreover, ChatGPT tends to overgeneralize to new policy issuesthat arose after its training data was collected, such as US support forinvolvement in the war in Ukraine. Our work has implications for ourunderstanding of the strengths and limitations of the current generation of AIchatbots as virtual publics or online listening platforms, future directionsfor LLM development, and applications of AI tools to the political domain.(Abridged)"
Go Beyond The Obvious: Probing the gap of INFORMAL reasoning ability  between Humanity and LLMs by Detective Reasoning Puzzle Benchmark,"['Zhouhon Gu', 'Zihan Li', 'Lin Zhang', 'Zhuozhi Xiong', 'Haoning Ye', 'Yikai Zhang', 'Wenhao Huang', 'Xiaoxuan Zhu', 'Qianyu He', 'Rui Xu', 'Sihang Jiang', 'Shusen Wang', 'Zili Wang', 'Hongwei Feng', 'Zhixu Li', 'Yanghua Xiao']",http://arxiv.org/pdf/2307.05113v2.pdf,2023-07-11,['cs.cl'],"  Informal reasoning ability is the ability to reason based on common sense,experience, and intuition.Humans use informal reasoning every day to extractthe most influential elements for their decision-making from a large amount oflife-like information.With the rapid development of language models, therealization of general artificial intelligence has emerged with hope. Given theoutstanding informal reasoning ability of humans, how much informal reasoningability language models have has not been well studied by scholars.In order toexplore the gap between humans and language models in informal reasoningability, this paper constructs a Detective Reasoning Benchmark, which is anassembly of 1,200 questions gathered from accessible online resources, aims atevaluating the model's informal reasoning ability in real-lifecontext.Considering the improvement of the model's informal reasoning abilityrestricted by the lack of benchmark, we further propose a Self-Question PromptFramework that mimics human thinking to enhance the model's informal reasoningability.The goals of self-question are to find key elements, deeply investigatethe connections between these elements, encourage the relationship between eachelement and the problem, and finally, require the model to reasonably answerthe problem.The experimental results show that human performance greatlyoutperforms the SoTA Language Models in Detective Reasoning Benchmark.Besides,Self-Question is proven to be the most effective prompt engineering inimproving GPT-4's informal reasoning ability, but it still does not evensurpass the lowest score made by human participants.Upon acceptance of thepaper, the source code for the benchmark will be made publicly accessible."
Benchmarking Causal Study to Interpret Large Language Models for Source  Code,"['Daniel Rodriguez-Cardenas', 'David N. Palacio', 'Dipin Khati', 'Henry Burke', 'Denys Poshyvanyk']",http://arxiv.org/pdf/2308.12415v1.pdf,2023-08-23,"['cs.se', 'cs.ai']","  One of the most common solutions adopted by software researchers to addresscode generation is by training Large Language Models (LLMs) on massive amountsof source code. Although a number of studies have shown that LLMs have beeneffectively evaluated on popular accuracy metrics (e.g., BLEU, CodeBleu),previous research has largely overlooked the role of Causal Inference as afundamental component of the interpretability of LLMs' performance. Existingbenchmarks and datasets are meant to highlight the difference between theexpected and the generated outcome, but do not take into account confoundingvariables (e.g., lines of code, prompt size) that equally influence theaccuracy metrics. The fact remains that, when dealing with generative softwaretasks by LLMs, no benchmark is available to tell researchers how to quantifyneither the causal effect of SE-based treatments nor the correlation ofconfounders to the model's performance. In an effort to bring statistical rigorto the evaluation of LLMs, this paper introduces a benchmarking strategy namedGaleras comprised of curated testbeds for three SE tasks (i.e., codecompletion, code summarization, and commit generation) to help aid theinterpretation of LLMs' performance. We illustrate the insights of ourbenchmarking strategy by conducting a case study on the performance of ChatGPTunder distinct prompt engineering methods. The results of the case studydemonstrate the positive causal influence of prompt semantics on ChatGPT'sgenerative performance by an average treatment effect of $\approx 3\%$.Moreover, it was found that confounders such as prompt size are highlycorrelated with accuracy metrics ($\approx 0.412\%$). The end result of ourcase study is to showcase causal inference evaluations, in practice, to reduceconfounding bias. By reducing the bias, we offer an interpretable solution forthe accuracy metric under analysis."
GPTCloneBench: A comprehensive benchmark of semantic clones and  cross-language clones using GPT-3 model and SemanticCloneBench,"['Ajmain Inqiad Alam', 'Palash Ranjan Roy', 'Farouq Al-omari', 'Chanchal Kumar Roy', 'Banani Roy', 'Kevin Schneider']",http://arxiv.org/pdf/2308.13963v2.pdf,2023-08-26,['cs.se'],"  With the emergence of Machine Learning, there has been a surge in leveragingits capabilities for problem-solving across various domains. In the code clonerealm, the identification of type-4 or semantic clones has emerged as a crucialyet challenging task. Researchers aim to utilize Machine Learning to tacklethis challenge, often relying on the BigCloneBench dataset. However, it's worthnoting that BigCloneBench, originally not designed for semantic clonedetection, presents several limitations that hinder its suitability as acomprehensive training dataset for this specific purpose. Furthermore, CLCDSAdataset suffers from a lack of reusable examples aligning with real-worldsoftware systems, rendering it inadequate for cross-language clone detectionapproaches. In this work, we present a comprehensive semantic clone andcross-language clone benchmark, GPTCloneBench by exploiting SemanticCloneBenchand OpenAI's GPT-3 model. In particular, using code fragments fromSemanticCloneBench as sample inputs along with appropriate prompt engineeringfor GPT-3 model, we generate semantic and cross-language clones for thesespecific fragments and then conduct a combination of extensive manual analysis,tool-assisted filtering, functionality testing and automated validation inbuilding the benchmark. From 79,928 clone pairs of GPT-3 output, we created abenchmark with 37,149 true semantic clone pairs, 19,288 false semanticpairs(Type-1/Type-2), and 20,770 cross-language clones across four languages(Java, C, C#, and Python). Our benchmark is 15-fold larger thanSemanticCloneBench, has more functional code examples for software systems andprogramming language support than CLCDSA, and overcomes BigCloneBench'squalities, quantification, and language variety limitations."
"AI Foundation Models for Weather and Climate: Applications, Design, and  Implementation","['S. Karthik Mukkavilli', 'Daniel Salles Civitarese', 'Johannes Schmude', 'Johannes Jakubik', 'Anne Jones', 'Nam Nguyen', 'Christopher Phillips', 'Sujit Roy', 'Shraddha Singh', 'Campbell Watson', 'Raghu Ganti', 'Hendrik Hamann', 'Udaysankar Nair', 'Rahul Ramachandran', 'Kommy Weldemariam']",http://arxiv.org/pdf/2309.10808v2.pdf,2023-09-19,"['cs.lg', 'cs.ai', 'physics.ao-ph', '68t07 (primary), 68t01, 86a08', 'i.2.0; i.4.0; j.2.5']","  Machine learning and deep learning methods have been widely explored inunderstanding the chaotic behavior of the atmosphere and furthering weatherforecasting. There has been increasing interest from technology companies,government institutions, and meteorological agencies in building digital twinsof the Earth. Recent approaches using transformers, physics-informed machinelearning, and graph neural networks have demonstrated state-of-the-artperformance on relatively narrow spatiotemporal scales and specific tasks. Withthe recent success of generative artificial intelligence (AI) using pre-trainedtransformers for language modeling and vision with prompt engineering andfine-tuning, we are now moving towards generalizable AI. In particular, we arewitnessing the rise of AI foundation models that can perform competitively onmultiple domain-specific downstream tasks. Despite this progress, we are stillin the nascent stages of a generalizable AI model for global Earth systemmodels, regional climate models, and mesoscale weather models. Here, we reviewcurrent state-of-the-art AI approaches, primarily from transformer and operatorlearning literature in the context of meteorology. We provide our perspectiveon criteria for success towards a family of foundation models for nowcastingand forecasting weather and climate predictions. We also discuss how suchmodels can perform competitively on downstream tasks such as downscaling(super-resolution), identifying conditions conducive to the occurrence ofwildfires, and predicting consequential meteorological phenomena across variousspatiotemporal scales such as hurricanes and atmospheric rivers. In particular,we examine current AI methodologies and contend they have matured enough todesign and implement a weather foundation model."
Exploring Small Language Models with Prompt-Learning Paradigm for  Efficient Domain-Specific Text Classification,"['Hengyu Luo', 'Peng Liu', 'Stefan Esping']",http://arxiv.org/pdf/2309.14779v1.pdf,2023-09-26,"['cs.cl', 'cs.ai', 'cs.lg']","  Domain-specific text classification faces the challenge of scarce labeleddata due to the high cost of manual labeling. Prompt-learning, known for itsefficiency in few-shot scenarios, is proposed as an alternative to traditionalfine-tuning methods. And besides, although large language models (LLMs) havegained prominence, small language models (SLMs, with under 1B parameters) offersignificant customizability, adaptability, and cost-effectiveness fordomain-specific tasks, given industry constraints. In this study, weinvestigate the potential of SLMs combined with prompt-learning paradigm fordomain-specific text classification, specifically within customer-agentinteractions in retail. Our evaluations show that, in few-shot settings whenprompt-based model fine-tuning is possible, T5-base, a typical SLM with 220Mparameters, achieve approximately 75% accuracy with limited labeled data (up to15% of full data), which shows great potentials of SLMs with prompt-learning.Based on this, We further validate the effectiveness of active few-shotsampling and the ensemble strategy in the prompt-learning pipeline thatcontribute to a remarkable performance gain. Besides, in zero-shot settingswith a fixed model, we underscore a pivotal observation that, although theGPT-3.5-turbo equipped with around 154B parameters garners an accuracy of55.16%, the power of well designed prompts becomes evident when theFLAN-T5-large, a model with a mere 0.5% of GPT-3.5-turbo's parameters, achievesan accuracy exceeding 31% with the optimized prompt, a leap from its sub-18%performance with an unoptimized one. Our findings underscore the promise ofprompt-learning in classification tasks with SLMs, emphasizing the benefits ofactive few-shot sampling, and ensemble strategies in few-shot settings, and theimportance of prompt engineering in zero-shot settings."
Label Supervised LLaMA Finetuning,"['Zongxi Li', 'Xianming Li', 'Yuzhang Liu', 'Haoran Xie', 'Jing Li', 'Fu-lee Wang', 'Qing Li', 'Xiaoqin Zhong']",http://arxiv.org/pdf/2310.01208v1.pdf,2023-10-02,['cs.cl'],"  The recent success of Large Language Models (LLMs) has gained significantattention in both academia and industry. Substantial efforts have been made toenhance the zero- and few-shot generalization capabilities of open-source LLMsthrough finetuning. Currently, the prevailing approach is instruction-tuning,which trains LLMs to complete real-world tasks by generating responses guidedby natural language instructions. It is worth noticing that such an approachmay underperform in sequence and token classification tasks. Unlike textgeneration tasks, classification tasks have a limited label space, whereprecise label prediction is more appreciated than generating diverse andhuman-like responses. Prior research has unveiled that instruction-tuned LLMscannot outperform BERT, prompting us to explore the potential of leveraginglatent representations from LLMs for supervised label prediction. In thispaper, we introduce a label-supervised adaptation for LLMs, which aims tofinetuning the model with discriminant labels. We evaluate this approach withLabel Supervised LLaMA (LS-LLaMA), based on LLaMA-2-7B, a relativelysmall-scale LLM, and can be finetuned on a single GeForce RTX4090 GPU. Weextract latent representations from the final LLaMA layer and project them intothe label space to compute the cross-entropy loss. The model is finetuned byLow-Rank Adaptation (LoRA) to minimize this loss. Remarkably, without intricateprompt engineering or external knowledge, LS-LLaMA substantially outperformsLLMs ten times its size in scale and demonstrates consistent improvementscompared to robust baselines like BERT-Large and RoBERTa-Large in textclassification. Moreover, by removing the causal mask from decoders, LS-unLLaMAachieves the state-of-the-art performance in named entity recognition (NER).Our work will shed light on a novel approach to adapting LLMs for variousdownstream tasks."
Mini-DALLE3: Interactive Text to Image by Prompting Large Language  Models,"['Zeqiang Lai', 'Xizhou Zhu', 'Jifeng Dai', 'Yu Qiao', 'Wenhai Wang']",http://arxiv.org/pdf/2310.07653v2.pdf,2023-10-11,['cs.ai'],"  The revolution of artificial intelligence content generation has been rapidlyaccelerated with the booming text-to-image (T2I) diffusion models. Within justtwo years of development, it was unprecedentedly of high-quality, diversity,and creativity that the state-of-the-art models could generate. However, aprevalent limitation persists in the effective communication with these popularT2I models, such as Stable Diffusion, using natural language descriptions. Thistypically makes an engaging image hard to obtain without expertise in promptengineering with complex word compositions, magic tags, and annotations.Inspired by the recently released DALLE3 - a T2I model directly built-inChatGPT that talks human language, we revisit the existing T2I systemsendeavoring to align human intent and introduce a new task - interactive textto image (iT2I), where people can interact with LLM for interleavedhigh-quality image generation/edit/refinement and question answering withstronger images and text correspondences using natural language. In addressingthe iT2I problem, we present a simple approach that augments LLMs for iT2I withprompting techniques and off-the-shelf T2I models. We evaluate our approach foriT2I in a variety of common-used scenarios under different LLMs, e.g., ChatGPT,LLAMA, Baichuan, and InternLM. We demonstrate that our approach could be aconvenient and low-cost way to introduce the iT2I ability for any existing LLMsand any text-to-image models without any training while bringing littledegradation on LLMs' inherent capabilities in, e.g., question answering andcode generation. We hope this work could draw broader attention and provideinspiration for boosting user experience in human-machine interactionsalongside the image quality of the next-generation T2I systems."
Promptor: A Conversational and Autonomous Prompt Generation Agent for  Intelligent Text Entry Techniques,"['Junxiao Shen', 'John J. Dudley', 'Jingyao Zheng', 'Bill Byrne', 'Per Ola Kristensson']",http://arxiv.org/pdf/2310.08101v2.pdf,2023-10-12,"['cs.cl', 'cs.ai']","  Text entry is an essential task in our day-to-day digital interactions.Numerous intelligent features have been developed to streamline this process,making text entry more effective, efficient, and fluid. These improvementsinclude sentence prediction and user personalization. However, as deeplearning-based language models become the norm for these advanced features, thenecessity for data collection and model fine-tuning increases. These challengescan be mitigated by harnessing the in-context learning capability of largelanguage models such as GPT-3.5. This unique feature allows the language modelto acquire new skills through prompts, eliminating the need for data collectionand fine-tuning. Consequently, large language models can learn various textprediction techniques. We initially showed that, for a sentence predictiontask, merely prompting GPT-3.5 surpassed a GPT-2 backed system and iscomparable with a fine-tuned GPT-3.5 model, with the latter two methodsrequiring costly data collection, fine-tuning and post-processing. However, thetask of prompting large language models to specialize in specific textprediction tasks can be challenging, particularly for designers withoutexpertise in prompt engineering. To address this, we introduce Promptor, aconversational prompt generation agent designed to engage proactively withdesigners. Promptor can automatically generate complex prompts tailored to meetspecific needs, thus offering a solution to this challenge. We conducted a userstudy involving 24 participants creating prompts for three intelligent textentry tasks, half of the participants used Promptor while the other halfdesigned prompts themselves. The results show that Promptor-designed promptsresult in a 35% increase in similarity and 22% in coherence over those bydesigners."
Human-in-the-loop Machine Translation with Large Language Model,"['Xinyi Yang', 'Runzhe Zhan', 'Derek F. Wong', 'Junchao Wu', 'Lidia S. Chao']",http://arxiv.org/pdf/2310.08908v1.pdf,2023-10-13,['cs.cl'],"  The large language model (LLM) has garnered significant attention due to itsin-context learning mechanisms and emergent capabilities. The researchcommunity has conducted several pilot studies to apply LLMs to machinetranslation tasks and evaluate their performance from diverse perspectives.However, previous research has primarily focused on the LLM itself and has notexplored human intervention in the inference process of LLM. Thecharacteristics of LLM, such as in-context learning and prompt engineering,closely mirror human cognitive abilities in language tasks, offering anintuitive solution for human-in-the-loop generation. In this study, we proposea human-in-the-loop pipeline that guides LLMs to produce customized outputswith revision instructions. The pipeline initiates by prompting the LLM toproduce a draft translation, followed by the utilization of automatic retrievalor human feedback as supervision signals to enhance the LLM's translationthrough in-context learning. The human-machine interactions generated in thispipeline are also stored in an external database to expand the in-contextretrieval database, enabling us to leverage human supervision in an offlinesetting. We evaluate the proposed pipeline using GPT-3.5-turbo API on fivedomain-specific benchmarks for German-English translation. The resultsdemonstrate the effectiveness of the pipeline in tailoring in-domaintranslations and improving translation performance compared to directtranslation. Additionally, we discuss the results from the followingperspectives: 1) the effectiveness of different in-context retrieval methods;2) the construction of a retrieval database under low-resource scenarios; 3)the observed domains differences; 4) the quantitative analysis of linguisticstatistics; and 5) the qualitative analysis of translation cases. The code anddata are available at https://github.com/NLP2CT/HIL-MT/."
ConstitutionMaker: Interactively Critiquing Large Language Models by  Converting Feedback into Principles,"['Savvas Petridis', 'Ben Wedin', 'James Wexler', 'Aaron Donsbach', 'Mahima Pushkarna', 'Nitesh Goyal', 'Carrie J. Cai', 'Michael Terry']",http://arxiv.org/pdf/2310.15428v1.pdf,2023-10-24,"['cs.hc', 'cs.ai']","  Large language model (LLM) prompting is a promising new approach for users tocreate and customize their own chatbots. However, current methods for steeringa chatbot's outputs, such as prompt engineering and fine-tuning, do not supportusers in converting their natural feedback on the model's outputs to changes inthe prompt or model. In this work, we explore how to enable users tointeractively refine model outputs through their feedback, by helping themconvert their feedback into a set of principles (i.e. a constitution) thatdictate the model's behavior. From a formative study, we (1) found that usersneeded support converting their feedback into principles for the chatbot and(2) classified the different principle types desired by users. Inspired bythese findings, we developed ConstitutionMaker, an interactive tool forconverting user feedback into principles, to steer LLM-based chatbots. WithConstitutionMaker, users can provide either positive or negative feedback innatural language, select auto-generated feedback, or rewrite the chatbot'sresponse; each mode of feedback automatically generates a principle that isinserted into the chatbot's prompt. In a user study with 14 participants, wecompare ConstitutionMaker to an ablated version, where users write their ownprinciples. With ConstitutionMaker, participants felt that their principlescould better guide the chatbot, that they could more easily convert theirfeedback into principles, and that they could write principles moreefficiently, with less mental demand. ConstitutionMaker helped users identifyways to improve the chatbot, formulate their intuitive responses to the modelinto feedback, and convert this feedback into specific and clear principles.Together, these findings inform future tools that support the interactivecritiquing of LLM outputs."
In Search of the Long-Tail: Systematic Generation of Long-Tail Knowledge  via Logical Rule Guided Search,"['Huihan Li', 'Yuting Ning', 'Zeyi Liao', 'Siyuan Wang', 'Xiang Lorraine Li', 'Ximing Lu', 'Faeze Brahman', 'Wenting Zhao', 'Yejin Choi', 'Xiang Ren']",http://arxiv.org/pdf/2311.07237v1.pdf,2023-11-13,"['cs.cl', 'cs.ai']","  Since large language models have approached human-level performance on manytasks, it has become increasingly harder for researchers to find tasks that arestill challenging to the models. Failure cases usually come from the long-taildistribution - data that an oracle language model could assign a probability onthe lower end of its distribution. Current methodology such as promptengineering or crowdsourcing are insufficient for creating long-tail examplesbecause humans are constrained by cognitive bias. We propose aLogic-Induced-Knowledge-Search (LINK) framework for systematically generatinglong-tail knowledge statements. Grounded by a symbolic rule, we search forlong-tail values for each variable of the rule by first prompting a LLM, thenverifying the correctness of the values with a critic, and lastly pushing forthe long-tail distribution with a reranker. With this framework we construct adataset, Logic-Induced-Long-Tail (LINT), consisting of 200 symbolic rules and50K knowledge statements spanning across four domains. Human annotations findthat 84% of the statements in LINT are factually correct. In contrast, ChatGPTand GPT4 struggle with directly generating long-tail statements under theguidance of logic rules, each only getting 56% and 78% of their statementscorrect. Moreover, their ""long-tail"" generations in fact fall into the higherlikelihood range, and thus are not really long-tail. Our findings suggest thatLINK is effective for generating data in the long-tail distribution whileenforcing quality. LINT can be useful for systematically evaluating LLMs'capabilities in the long-tail distribution. We challenge the models with asimple entailment classification task using samples from LINT. We find thatChatGPT and GPT4's capability in identifying incorrect knowledge drop by ~3% inthe long-tail distribution compared to head distribution."
From Beginner to Expert: Modeling Medical Knowledge into General LLMs,"['Qiang Li', 'Xiaoyan Yang', 'Haowen Wang', 'Qin Wang', 'Lei Liu', 'Junjie Wang', 'Yang Zhang', 'Mingyuan Chu', 'Sen Hu', 'Yicheng Chen', 'Yue Shen', 'Cong Fan', 'Wangshu Zhang', 'Teng Xu', 'Jinjie Gu', 'Jing Zheng', 'Guannan Zhang Ant Group']",http://arxiv.org/pdf/2312.01040v1.pdf,2023-12-02,['cs.cl'],"  Recently, large language model (LLM) based artificial intelligence (AI)systems have demonstrated remarkable capabilities in natural languageunderstanding and generation. However, these models face a significantchallenge when it comes to sensitive applications, such as reasoning overmedical knowledge and answering medical questions in a physician-like manner.Prior studies attempted to overcome this challenge by increasing the model size(>100B) to learn more general medical knowledge, while there is still room forimprovement in LLMs with smaller-scale model sizes (<100B). In this work, westart from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from amedical beginner towards a medical expert (called AntGLM-Med-10B), whichleverages a 3-stage optimization procedure, \textit{i.e.}, general medicalknowledge injection, medical domain instruction tuning, and specific medicaltask adaptation. Our contributions are threefold: (1) We specificallyinvestigate how to adapt a pre-trained general LLM in medical domain,especially for a specific medical task. (2) We collect and constructlarge-scale medical datasets for each stage of the optimization process. Thesedatasets encompass various data types and tasks, such as question-answering,medical reasoning, multi-choice questions, and medical conversations. (3)Specifically for multi-choice questions in the medical domain, we propose anovel Verification-of-Choice approach for prompting engineering, whichsignificantly enhances the reasoning ability of LLMs. Remarkably, by combiningthe above approaches, our AntGLM-Med-10B model can outperform the most of LLMson PubMedQA, including both general and medical LLMs, even when these LLMs havelarger model size."
Using dependency parsing for few-shot learning in distributional  semantics,"['Stefania Preda', 'Guy Emerson']",http://arxiv.org/pdf/2205.06168v1.pdf,2022-05-12,['cs.cl'],"  In this work, we explore the novel idea of employing dependency parsinginformation in the context of few-shot learning, the task of learning themeaning of a rare word based on a limited amount of context sentences. Firstly,we use dependency-based word embedding models as background spaces for few-shotlearning. Secondly, we introduce two few-shot learning methods which enhancethe additive baseline model by using dependencies."
Few-Shot Learning with Graph Neural Networks,"['Victor Garcia', 'Joan Bruna']",http://arxiv.org/pdf/1711.04043v3.pdf,2017-11-10,"['stat.ml', 'cs.lg']","  We propose to study the problem of few-shot learning with the prism ofinference on a partially observed graphical model, constructed from acollection of input images whose label can be either observed or not. Byassimilating generic message-passing inference algorithms with theirneural-network counterparts, we define a graph neural network architecture thatgeneralizes several of the recently proposed few-shot learning models. Besidesproviding improved numerical performance, our framework is easily extended tovariants of few-shot learning, such as semi-supervised or active learning,demonstrating the ability of graph-based models to operate well on 'relational'tasks."
"Few-shot learning using pre-training and shots, enriched by pre-trained  samples",['Detlef Schmicker'],http://arxiv.org/pdf/2009.09172v1.pdf,2020-09-19,"['cs.lg', 'cs.cv']","  We use the EMNIST dataset of handwritten digits to test a simple approach forfew-shot learning. A fully connected neural network is pre-trained with asubset of the 10 digits and used for few-shot learning with untrained digits.Two basic ideas are introduced: during few-shot learning the learning of thefirst layer is disabled, and for every shot a previously unknown digit is usedtogether with four previously trained digits for the gradient descend, until apredefined threshold condition is fulfilled. This way we reach about 90%accuracy after 10 shots."
Offline Handwritten Amharic Character Recognition Using Few-shot  Learning,"['Mesay Samuel', 'Lars Schmidt-Thieme', 'DP Sharma', 'Abiot Sinamo', 'Abey Bruck']",http://arxiv.org/pdf/2210.00275v1.pdf,2022-10-01,['cs.cv'],"  Few-shot learning is an important, but challenging problem of machinelearning aimed at learning from only fewer labeled training examples. It hasbecome an active area of research due to deep learning requiring huge amountsof labeled dataset, which is not feasible in the real world. Learning from afew examples is also an important attempt towards learning like humans.Few-shot learning has proven a very good promise in different areas of machinelearning applications, particularly in image classification. As it is a recenttechnique, most researchers focus on understanding and solving the issuesrelated to its concept by focusing only on common image datasets likeMini-ImageNet and Omniglot. Few-shot learning also opens an opportunity toaddress low resource languages like Amharic. In this study, offline handwrittenAmharic character recognition using few-shot learning is addressed.Particularly, prototypical networks, the popular and simpler type of few-shotlearning, is implemented as a baseline. Using the opportunities explored in thenature of Amharic alphabet having row-wise and column-wise similarities, anovel way of augmenting the training episodes is proposed. The experimentalresults show that the proposed method outperformed the baseline method. Thisstudy has implemented few-shot learning for Amharic characters for the firsttime. More importantly, the findings of the study open new ways of examiningthe influence of training episodes in few-shot learning, which is one of theimportant issues that needs exploration. The datasets used for this study arecollected from native Amharic language writers using an Android App developedas a part of this study."
"N-Omniglot, a large-scale neuromorphic dataset for spatio-temporal  sparse few-shot learning","['Yang Li', 'Yiting Dong', 'Dongcheng Zhao', 'Yi Zeng']",http://arxiv.org/pdf/2112.13230v3.pdf,2021-12-25,"['cs.ne', 'cs.lg']","  Few-shot learning (learning with a few samples) is one of the most importantcognitive abilities of the human brain. However, the current artificialintelligence systems meet difficulties in achieving this ability. Similarchallenges also exist for biologically plausible spiking neural networks(SNNs). Datasets for traditional few-shot learning domains provide few amountsof temporal information. and the absence of neuromorphic datasets has hinderedthe development of few-shot learning for SNNs. Here, to the best of ourknowledge, we provide the first neuromorphic dataset for few-shot learningusing SNNs: N-Omniglot, based on the Dynamic Vision Sensor. It contains 1,623categories of handwritten characters, with only 20 samples per class.N-Omniglot eliminates the need for a neuromorphic dataset for SNNs with highspareness and tremendous temporal coherence. Additionally, the dataset providesa powerful challenge and a suitable benchmark for developing SNNs algorithms inthe few-shot learning domain due to the chronological information of strokes.We also provide the improved nearest neighbor, convolutional network,SiameseNet, and meta-learning algorithm in the spiking version forverification."
Learning from Few Examples: A Summary of Approaches to Few-Shot Learning,"['Archit Parnami', 'Minwoo Lee']",http://arxiv.org/pdf/2203.04291v1.pdf,2022-03-07,"['cs.lg', 'cs.cv']","  Few-Shot Learning refers to the problem of learning the underlying pattern inthe data just from a few training samples. Requiring a large number of datasamples, many deep learning solutions suffer from data hunger and extensivelyhigh computation time and resources. Furthermore, data is often not availabledue to not only the nature of the problem or privacy concerns but also the costof data preparation. Data collection, preprocessing, and labeling are strenuoushuman tasks. Therefore, few-shot learning that could drastically reduce theturnaround time of building machine learning applications emerges as a low-costsolution. This survey paper comprises a representative list of recentlyproposed few-shot learning algorithms. Given the learning dynamics andcharacteristics, the approaches to few-shot learning problems are discussed inthe perspectives of meta-learning, transfer learning, and hybrid approaches(i.e., different variations of the few-shot learning problem)."
Few-shot learning for sentence pair classification and its applications  in software engineering,"['Robert Kraig Helmeczi', 'Mucahit Cevik', 'Savas Yıldırım']",http://arxiv.org/pdf/2306.08058v1.pdf,2023-06-13,['cs.se'],"  Few-shot learning-the ability to train models with access to limited data-hasbecome increasingly popular in the natural language processing (NLP) domain, aslarge language models such as GPT and T0 have been empirically shown to achievehigh performance in numerous tasks with access to just a handful of labeledexamples. Smaller language models such as BERT and its variants have also beenshown to achieve strong performance with just a handful of labeled exampleswhen combined with few-shot learning algorithms like pattern-exploitingtraining (PET) and SetFit. The focus of this work is to investigate theperformance of alternative few-shot learning approaches with BERT-based models.Specifically, vanilla fine-tuning, PET and SetFit are compared for numerousBERT-based checkpoints over an array of training set sizes. To facilitate thisinvestigation, applications of few-shot learning are considered in softwareengineering. For each task, high-performance techniques and their associatedmodel checkpoints are identified through detailed empirical analysis. Ourresults establish PET as a strong few-shot learning approach, and our analysisshows that with just a few hundred labeled examples it can achieve performancenear that of fine-tuning on full-sized data sets."
TransMatch: A Transfer-Learning Scheme for Semi-Supervised Few-Shot  Learning,"['Zhongjie Yu', 'Lin Chen', 'Zhongwei Cheng', 'Jiebo Luo']",http://arxiv.org/pdf/1912.09033v2.pdf,2019-12-19,"['cs.lg', 'cs.cv', 'stat.ml']","  The successful application of deep learning to many visual recognition tasksrelies heavily on the availability of a large amount of labeled data which isusually expensive to obtain. The few-shot learning problem has attractedincreasing attention from researchers for building a robust model upon only afew labeled samples. Most existing works tackle this problem under themeta-learning framework by mimicking the few-shot learning task with anepisodic training strategy. In this paper, we propose a new transfer-learningframework for semi-supervised few-shot learning to fully utilize the auxiliaryinformation from labeled base-class data and unlabeled novel-class data. Theframework consists of three components: 1) pre-training a feature extractor onbase-class data; 2) using the feature extractor to initialize the classifierweights for the novel classes; and 3) further updating the model with asemi-supervised learning method. Under the proposed framework, we develop anovel method for semi-supervised few-shot learning called TransMatch byinstantiating the three components with Imprinting and MixMatch. Extensiveexperiments on two popular benchmark datasets for few-shot learning,CUB-200-2011 and miniImageNet, demonstrate that our proposed method caneffectively utilize the auxiliary information from labeled base-class data andunlabeled novel-class data to significantly improve the accuracy of few-shotlearning task."
FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark,"['Liang Xu', 'Xiaojing Lu', 'Chenyang Yuan', 'Xuanwei Zhang', 'Huilin Xu', 'Hu Yuan', 'Guoao Wei', 'Xiang Pan', 'Xin Tian', 'Libo Qin', 'Hu Hai']",http://arxiv.org/pdf/2107.07498v2.pdf,2021-07-15,"['cs.cl', 'cs.ai']","  Pretrained Language Models (PLMs) have achieved tremendous success in naturallanguage understanding tasks. While different learning schemes -- fine-tuning,zero-shot, and few-shot learning -- have been widely explored and compared forlanguages such as English, there is comparatively little work in Chinese tofairly and comprehensively evaluate and compare these methods and thus hinderscumulative progress. In this paper, we introduce the Chinese Few-shot LearningEvaluation Benchmark (FewCLUE), the first comprehensive few-shot evaluationbenchmark in Chinese. It includes nine tasks, ranging from single-sentence andsentence-pair classification tasks to machine reading comprehension tasks. Wesystematically evaluate five state-of-the-art (SOTA) few-shot learning methods(including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare theirperformance with fine-tuning and zero-shot learning schemes on the newlyconstructed FewCLUE benchmark. Experimental results reveal that: 1) The effectof different few-shot learning methods is sensitive to the pre-trained model towhich the methods are applied; 2) PET and P-tuning achieve the best overallperformance with RoBERTa and ERNIE respectively. Our benchmark is used in thefew-shot learning contest of NLPCC 2021. In addition, we provide auser-friendly toolkit, as well as an online leaderboard to help facilitatefurther progress on Chinese few-shot learning. We provide a baselineperformance on different learning methods, a reference for future research."
LibFewShot: A Comprehensive Library for Few-shot Learning,"['Wenbin Li', ' Ziyi', ' Wang', 'Xuesong Yang', 'Chuanqi Dong', 'Pinzhuo Tian', 'Tiexin Qin', 'Jing Huo', 'Yinghuan Shi', 'Lei Wang', 'Yang Gao', 'Jiebo Luo']",http://arxiv.org/pdf/2109.04898v3.pdf,2021-09-10,['cs.cv'],"  Few-shot learning, especially few-shot image classification, has receivedincreasing attention and witnessed significant advances in recent years. Somerecent studies implicitly show that many generic techniques or ``tricks'', suchas data augmentation, pre-training, knowledge distillation, andself-supervision, may greatly boost the performance of a few-shot learningmethod. Moreover, different works may employ different software platforms,backbone architectures and input image sizes, making fair comparisons difficultand practitioners struggle with reproducibility. To address these situations,we propose a comprehensive library for few-shot learning (LibFewShot) byre-implementing eighteen state-of-the-art few-shot learning methods in aunified framework with the same single codebase in PyTorch. Furthermore, basedon LibFewShot, we provide comprehensive evaluations on multiple benchmarks withvarious backbone architectures to evaluate common pitfalls and effects ofdifferent training tricks. In addition, with respect to the recent doubts onthe necessity of meta- or episodic-training mechanism, our evaluation resultsconfirm that such a mechanism is still necessary especially when combined withpre-training. We hope our work can not only lower the barriers for beginners toenter the area of few-shot learning but also elucidate the effects ofnontrivial tricks to facilitate intrinsic research on few-shot learning. Thesource code is available from https://github.com/RL-VIG/LibFewShot."
Meta Navigator: Search for a Good Adaptation Policy for Few-shot  Learning,"['Chi Zhang', 'Henghui Ding', 'Guosheng Lin', 'Ruibo Li', 'Changhu Wang', 'Chunhua Shen']",http://arxiv.org/pdf/2109.05749v1.pdf,2021-09-13,['cs.cv'],"  Few-shot learning aims to adapt knowledge learned from previous tasks tonovel tasks with only a limited amount of labeled data. Research literature onfew-shot learning exhibits great diversity, while different algorithms oftenexcel at different few-shot learning scenarios. It is therefore tricky todecide which learning strategies to use under different task conditions.Inspired by the recent success in Automated Machine Learning literature(AutoML), in this paper, we present Meta Navigator, a framework that attemptsto solve the aforementioned limitation in few-shot learning by seeking ahigher-level strategy and proffer to automate the selection from variousfew-shot learning designs. The goal of our work is to search for good parameteradaptation policies that are applied to different stages in the network forfew-shot classification. We present a search space that covers many popularfew-shot learning algorithms in the literature and develop a differentiablesearching and decoding algorithm based on meta-learning that supportsgradient-based optimization. We demonstrate the effectiveness of oursearching-based method on multiple benchmark datasets. Extensive experimentsshow that our approach significantly outperforms baselines and demonstratesperformance advantages over many state-of-the-art methods. Code and models willbe made publicly available."
DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable  Kendall's Rank Correlation,"['Kaipeng Zheng', 'Huishuai Zhang', 'Weiran Huang']",http://arxiv.org/pdf/2307.15317v2.pdf,2023-07-28,"['cs.cv', 'cs.ai', 'cs.lg', 'cs.mm']","  Few-shot learning aims to adapt models trained on the base dataset to noveltasks where the categories were not seen by the model before. This often leadsto a relatively uniform distribution of feature values across channels on novelclasses, posing challenges in determining channel importance for novel tasks.Standard few-shot learning methods employ geometric similarity metrics such ascosine similarity and negative Euclidean distance to gauge the semanticrelatedness between two features. However, features with high geometricsimilarities may carry distinct semantics, especially in the context offew-shot learning. In this paper, we demonstrate that the importance ranking offeature channels is a more reliable indicator for few-shot learning thangeometric similarity metrics. We observe that replacing the geometricsimilarity metric with Kendall's rank correlation only during inference is ableto improve the performance of few-shot learning across a wide range of methodsand datasets with different domains. Furthermore, we propose a carefullydesigned differentiable loss for meta-training to address thenon-differentiability issue of Kendall's rank correlation. By replacinggeometric similarity with differentiable Kendall's rank correlation, our methodcan integrate with numerous existing few-shot approaches and is ready forintegrating with future state-of-the-art methods that rely on geometricsimilarity metrics. Extensive experiments validate the efficacy of therank-correlation-based approach, showcasing a significant improvement infew-shot learning."
Revisiting Fine-tuning for Few-shot Learning,"['Akihiro Nakamura', 'Tatsuya Harada']",http://arxiv.org/pdf/1910.00216v2.pdf,2019-10-01,"['cs.lg', 'cs.cv', 'stat.ml']","  Few-shot learning is the process of learning novel classes using only a fewexamples and it remains a challenging task in machine learning. Manysophisticated few-shot learning algorithms have been proposed based on thenotion that networks can easily overfit to novel examples if they are simplyfine-tuned using only a few examples. In this study, we show that in thecommonly used low-resolution mini-ImageNet dataset, the fine-tuning methodachieves higher accuracy than common few-shot learning algorithms in the 1-shottask and nearly the same accuracy as that of the state-of-the-art algorithm inthe 5-shot task. We then evaluate our method with more practical tasks, namelythe high-resolution single-domain and cross-domain tasks. With both tasks, weshow that our method achieves higher accuracy than common few-shot learningalgorithms. We further analyze the experimental results and show that: 1) theretraining process can be stabilized by employing a low learning rate, 2) usingadaptive gradient optimizers during fine-tuning can increase test accuracy, and3) test accuracy can be improved by updating the entire network when a largedomain-shift exists between base and novel classes."
Wandering Within a World: Online Contextualized Few-Shot Learning,"['Mengye Ren', 'Michael L. Iuzzolino', 'Michael C. Mozer', 'Richard S. Zemel']",http://arxiv.org/pdf/2007.04546v3.pdf,2020-07-09,"['cs.lg', 'cs.cv', 'stat.ml']","  We aim to bridge the gap between typical human and machine-learningenvironments by extending the standard framework of few-shot learning to anonline, continual setting. In this setting, episodes do not have separatetraining and testing phases, and instead models are evaluated online whilelearning novel classes. As in the real world, where the presence ofspatiotemporal context helps us retrieve learned skills in the past, our onlinefew-shot learning setting also features an underlying context that changesthroughout time. Object classes are correlated within a context and inferringthe correct context can lead to better performance. Building upon this setting,we propose a new few-shot learning dataset based on large scale indoor imagerythat mimics the visual experience of an agent wandering within a world.Furthermore, we convert popular few-shot learning approaches into onlineversions and we also propose a new contextual prototypical memory model thatcan make use of spatiotemporal contextual information from the recent past."
Extensively Matching for Few-shot Learning Event Detection,"['Viet Dac Lai', 'Franck Dernoncourt', 'Thien Huu Nguyen']",http://arxiv.org/pdf/2006.10093v1.pdf,2020-06-17,['cs.cl'],"  Current event detection models under super-vised learning settings fail totransfer to newevent types. Few-shot learning has not beenexplored in eventdetection even though it al-lows a model to perform well with highgener-alization on new event types. In this work, weformulate event detectionas a few-shot learn-ing problem to enable to extend event detec-tion to newevent types. We propose two novelloss factors that matching examples in thesup-port set to provide more training signals to themodel. Moreover, thesetraining signals can beapplied in many metric-based few-shot learn-ing models.Our extensive experiments on theACE-2005 dataset (under a few-shotlearningsetting) show that the proposed method can im-prove the performance offew-shot learning"
A Transductive Maximum Margin Classifier for Few-Shot Learning,"['Fei Pan', 'Chunlei Xu', 'Jie Guo', 'Yanwen Guo']",http://arxiv.org/pdf/2107.11975v3.pdf,2021-07-26,['cs.cv'],"  Few-shot learning aims to train a classifier that can generalize well whenjust a small number of labeled examples per class are given. We introduce atransductive maximum margin classifier for few-shot learning (FS-TMMC). Thebasic idea of the classical maximum margin classifier is to solve an optimalprediction function so that the training data can be correctly classified bythe resulting classifer with the largest geometric margin. In few-shotlearning, it is challenging to find such classifiers with good generalizationability due to the insufficiency of training data in the support set. FS-TMMCleverages the unlabeled query examples to adjust the separating hyperplane ofthe maximum margin classifier such that the prediction function is optimal onboth the support and query sets. Furthermore, we use an efficient and effectivequasi-Newton algorithm, the L-BFGS method for optimization. Experimentalresults on three standard few-shot learning benchmarks including miniImagenet,tieredImagenet and CUB show that our method achieves state-of-the-artperformance."
Budget-aware Few-shot Learning via Graph Convolutional Network,"['Shipeng Yan', 'Songyang Zhang', 'Xuming He']",http://arxiv.org/pdf/2201.02304v1.pdf,2022-01-07,"['cs.cv', 'cs.lg']","  This paper tackles the problem of few-shot learning, which aims to learn newvisual concepts from a few examples. A common problem setting in few-shotclassification assumes random sampling strategy in acquiring data labels, whichis inefficient in practical applications. In this work, we introduce a newbudget-aware few-shot learning problem that not only aims to learn novel objectcategories, but also needs to select informative examples to annotate in orderto achieve data efficiency.  We develop a meta-learning strategy for our budget-aware few-shot learningtask, which jointly learns a novel data selection policy based on a GraphConvolutional Network (GCN) and an example-based few-shot classifier. Ourselection policy computes a context-sensitive representation for each unlabeleddata by graph message passing, which is then used to predict an informativenessscore for sequential selection. We validate our method by extensive experimentson the mini-ImageNet, tiered-ImageNet and Omniglot datasets. The results showour few-shot learning strategy outperforms baselines by a sizable margin, whichdemonstrates the efficacy of our method."
Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning,"['Jishnu Jaykumar P', 'Kamalesh Palanisamy', 'Yu-Wei Chao', 'Xinya Du', 'Yu Xiang']",http://arxiv.org/pdf/2307.03073v2.pdf,2023-07-06,"['cs.cv', 'cs.ro']","  We propose a novel framework for few-shot learning by leveraging large-scalevision-language models such as CLIP. Motivated by the unimodal prototypicalnetworks for few-shot learning, we introduce PROTO-CLIP that utilizes imageprototypes and text prototypes for few-shot learning. Specifically, PROTO-CLIPadapts the image encoder and text encoder in CLIP in a joint fashion usingfew-shot examples. The two encoders are used to compute prototypes of imageclasses for classification. During adaptation, we propose aligning the imageand text prototypes of corresponding classes. Such a proposed alignment isbeneficial for few-shot classification due to the contributions from both typesof prototypes. We demonstrate the effectiveness of our method by conductingexperiments on benchmark datasets for few-shot learning as well as in the realworld for robot perception."
Assertion Enhanced Few-Shot Learning: Instructive Technique for Large  Language Models to Generate Educational Explanations,"['Tasmia Shahriar', 'Noboru Matsuda', 'Kelly Ramos']",http://arxiv.org/pdf/2312.03122v1.pdf,2023-12-05,['cs.cl'],"  Human educators possess an intrinsic ability to anticipate and seekeducational explanations from students, which drives them to posethought-provoking questions when students cannot articulate these explanationsindependently. We aim to imbue Intelligent Tutoring Systems with this abilityusing few-shot learning capability of Large Language Models. Our work proposesa novel prompting technique, Assertion Enhanced Few-Shot Learning, tofacilitate the generation of accurate, detailed oriented educationalexplanations. Our central hypothesis is that, in educational domain, few-shotdemonstrations are necessary but not a sufficient condition for qualityexplanation generation. We conducted a study involving 12 in-service teachers,comparing our approach to Traditional Few-Shot Learning. The results show thatAssertion Enhanced Few-Shot Learning improves explanation accuracy by 15% andyields higher-quality explanations, as evaluated by teachers. We also conduct aqualitative ablation study to factor the impact of assertions to provideeducator-friendly prompting guidelines for generating explanations in theirdomain of interest."
Will Multi-modal Data Improves Few-shot Learning?,"['Zilun Zhang', 'Shihao Ma', 'Yichun Zhang']",http://arxiv.org/pdf/2107.11853v1.pdf,2021-07-25,['cs.cv'],"  Most few-shot learning models utilize only one modality of data. We wouldlike to investigate qualitatively and quantitatively how much will the modelimprove if we add an extra modality (i.e. text description of the image), andhow it affects the learning procedure. To achieve this goal, we propose fourtypes of fusion method to combine the image feature and text feature. To verifythe effectiveness of improvement, we test the fusion methods with two classicalfew-shot learning models - ProtoNet and MAML, with image feature extractorssuch as ConvNet and ResNet12. The attention-based fusion method works best,which improves the classification accuracy by a large margin around 30%comparing to the baseline result."
A Survey on Recent Named Entity Recognition and Relation Classification  Methods with Focus on Few-Shot Learning Approaches,"['Sakher Alqaaidi', 'Elika Bozorgi']",http://arxiv.org/pdf/2310.19055v1.pdf,2023-10-29,['cs.cl'],"  Named entity recognition and relation classification are key stages forextracting information from unstructured text. Several natural languageprocessing applications utilize the two tasks, such as information retrieval,knowledge graph construction and completion, question answering and otherdomain-specific applications, such as biomedical data mining. We present asurvey of recent approaches in the two tasks with focus on few-shot learningapproaches. Our work compares the main approaches followed in the twoparadigms. Additionally, we report the latest metric scores in the two taskswith a structured analysis that considers the results in the few-shot learningscope."
An Overview of Deep Learning Architectures in Few-Shot Learning Domain,"['Shruti Jadon', 'Aryan Jadon']",http://arxiv.org/pdf/2008.06365v4.pdf,2020-08-12,"['cs.cv', 'cs.lg']","  Since 2012, Deep learning has revolutionized Artificial Intelligence and hasachieved state-of-the-art outcomes in different domains, ranging from ImageClassification to Speech Generation. Though it has many potentials, our currentarchitectures come with the pre-requisite of large amounts of data. Few-ShotLearning (also known as one-shot learning) is a sub-field of machine learningthat aims to create such models that can learn the desired objective with lessdata, similar to how humans learn. In this paper, we have reviewed some of thewell-known deep learning-based approaches towards few-shot learning. We havediscussed the recent achievements, challenges, and possibilities of improvementof few-shot learning based deep learning architectures. Our aim for this paperis threefold: (i) Give a brief introduction to deep learning architectures forfew-shot learning with pointers to core references. (ii) Indicate how deeplearning has been applied to the low-data regime, from data preparation tomodel training. and, (iii) Provide a starting point for people interested inexperimenting and perhaps contributing to the field of few-shot learning bypointing out some useful resources and open-source code. Our code is availableat Github: https://github.com/shruti-jadon/Hands-on-One-Shot-Learning."
Classifying Galaxy Morphologies with Few-Shot Learning,"['Zhirui Zhang', 'Zhiqiang Zou', 'Nan Li', 'Yanli Chen']",http://arxiv.org/pdf/2202.08172v2.pdf,2022-02-16,"['astro-ph.ga', 'astro-ph.im']","  The taxonomy of galaxy morphology is critical in astrophysics as themorphological properties are powerful tracers of galaxy evolution. With theupcoming Large-scale Imaging Surveys, billions of galaxy images challengeastronomers to accomplish the classification task by applying traditionalmethods or human inspection. Consequently, machine learning, in particularsupervised deep learning, has been widely employed to classify galaxymorphologies recently due to its exceptional automation, efficiency, andaccuracy. However, supervised deep learning requires extensive training sets,which causes considerable workloads; also, the results are strongly dependenton the characteristics of training sets, which leads to biased outcomespotentially. In this study, we attempt Few-shot Learning to bypass the twoissues. Our research adopts the dataset from Galaxy Zoo Challenge Project onKaggle, and we divide it into five categories according to the correspondingtruth table. By classifying the above dataset utilizing few-shot learning basedon Siamese Networks and supervised deep learning based on AlexNet, VGG_16, andResNet_50 trained with different volumes of training sets separately, we findthat few-shot learning achieves the highest accuracy in most cases, and themost significant improvement is $21\%$ compared to AlexNet when the trainingsets contain 1000 images. In addition, to guarantee the accuracy is no lessthan 90\%, few-shot learning needs $\sim$6300 images for training, whileResNet_50 requires 13000 images. Considering the advantages stated above,foreseeably, few-shot learning is suitable for the taxonomy of galaxymorphology and even for identifying rare astrophysical objects, despite limitedtraining sets consisting of observational data only."
GPr-Net: Geometric Prototypical Network for Point Cloud Few-Shot  Learning,"['Tejas Anvekar', 'Dena Bazazian']",http://arxiv.org/pdf/2304.06007v1.pdf,2023-04-12,['cs.cv'],"  In the realm of 3D-computer vision applications, point cloud few-shotlearning plays a critical role. However, it poses an arduous challenge due tothe sparsity, irregularity, and unordered nature of the data. Current methodsrely on complex local geometric extraction techniques such as convolution,graph, and attention mechanisms, along with extensive data-driven pre-trainingtasks. These approaches contradict the fundamental goal of few-shot learning,which is to facilitate efficient learning. To address this issue, we proposeGPr-Net (Geometric Prototypical Network), a lightweight and computationallyefficient geometric prototypical network that captures the intrinsic topologyof point clouds and achieves superior performance. Our proposed method, IGI++(Intrinsic Geometry Interpreter++) employs vector-based hand-crafted intrinsicgeometry interpreters and Laplace vectors to extract and evaluate point cloudmorphology, resulting in improved representations for FSL (Few-Shot Learning).Additionally, Laplace vectors enable the extraction of valuable features frompoint clouds with fewer points. To tackle the distribution drift challenge infew-shot metric learning, we leverage hyperbolic space and demonstrate that ourapproach handles intra and inter-class variance better than existing pointcloud few-shot learning methods. Experimental results on the ModelNet40 datasetshow that GPr-Net outperforms state-of-the-art methods in few-shot learning onpoint clouds, achieving utmost computational efficiency that is $170\times$better than all existing works. The code is publicly available athttps://github.com/TejasAnvekar/GPr-Net."
A Systematic Review of Few-Shot Learning in Medical Imaging,"['Eva Pachetti', 'Sara Colantonio']",http://arxiv.org/pdf/2309.11433v1.pdf,2023-09-20,"['cs.cv', 'cs.ai', 'i.2.6; i.4; i.5; j.3']","  The lack of annotated medical images limits the performance of deep learningmodels, which usually need large-scale labelled datasets. Few-shot learningtechniques can reduce data scarcity issues and enhance medical image analysis,especially with meta-learning. This systematic review gives a comprehensiveoverview of few-shot learning in medical imaging. We searched the literaturesystematically and selected 80 relevant articles published from 2018 to 2023.We clustered the articles based on medical outcomes, such as tumoursegmentation, disease classification, and image registration; anatomicalstructure investigated (i.e. heart, lung, etc.); and the meta-learning methodused. For each cluster, we examined the papers' distributions and the resultsprovided by the state-of-the-art. In addition, we identified a generic pipelineshared among all the studies. The review shows that few-shot learning canovercome data scarcity in most outcomes and that meta-learning is a popularchoice to perform few-shot learning because it can adapt to new tasks with fewlabelled samples. In addition, following meta-learning, supervised learning andsemi-supervised learning stand out as the predominant techniques employed totackle few-shot learning challenges in medical imaging and also bestperforming. Lastly, we observed that the primary application areaspredominantly encompass cardiac, pulmonary, and abdominal domains. Thissystematic review aims to inspire further research to improve medical imageanalysis and patient care."
Self Paced Adversarial Training for Multimodal Few-shot Learning,"['Frederik Pahde', 'Oleksiy Ostapenko', 'Patrick Jähnichen', 'Tassilo Klein', 'Moin Nabi']",http://arxiv.org/pdf/1811.09192v1.pdf,2018-11-22,"['cs.cv', 'cs.lg', 'cs.mm']","  State-of-the-art deep learning algorithms yield remarkable results in manyvisual recognition tasks. However, they still fail to provide satisfactoryresults in scarce data regimes. To a certain extent this lack of data can becompensated by multimodal information. Missing information in one modality of asingle data point (e.g. an image) can be made up for in another modality (e.g.a textual description). Therefore, we design a few-shot learning task that ismultimodal during training (i.e. image and text) and single-modal during testtime (i.e. image). In this regard, we propose a self-paced class-discriminativegenerative adversarial network incorporating multimodality in the context offew-shot learning. The proposed approach builds upon the idea of cross-modaldata generation in order to alleviate the data sparsity problem. We improvefew-shot learning accuracies on the finegrained CUB and Oxford-102 datasets."
An Investigation of Few-Shot Learning in Spoken Term Classification,"['Yangbin Chen', 'Tom Ko', 'Lifeng Shang', 'Xiao Chen', 'Xin Jiang', 'Qing Li']",http://arxiv.org/pdf/1812.10233v3.pdf,2018-12-26,"['cs.cl', 'cs.ir']","  In this paper, we investigate the feasibility of applying few-shot learningalgorithms to a speech task. We formulate a user-defined scenario of spokenterm classification as a few-shot learning problem. In most few-shot learningstudies, it is assumed that all the N classes are new in a N-way problem. Wesuggest that this assumption can be relaxed and define a N+M-way problem whereN and M are the number of new classes and fixed classes respectively. Wepropose a modification to the Model-Agnostic Meta-Learning (MAML) algorithm tosolve the problem. Experiments on the Google Speech Commands dataset show thatour approach outperforms the conventional supervised learning approach and theoriginal MAML."
Few-shot Learning with Meta Metric Learners,"['Yu Cheng', 'Mo Yu', 'Xiaoxiao Guo', 'Bowen Zhou']",http://arxiv.org/pdf/1901.09890v1.pdf,2019-01-26,"['cs.lg', 'cs.ai', 'stat.ml']","  Few-shot Learning aims to learn classifiers for new classes with only a fewtraining examples per class. Existing meta-learning or metric-learning basedfew-shot learning approaches are limited in handling diverse domains withvarious number of labels. The meta-learning approaches train a meta learner topredict weights of homogeneous-structured task-specific networks, requiring auniform number of classes across tasks. The metric-learning approaches learnone task-invariant metric for all the tasks, and they fail if the tasksdiverge. We propose to deal with these limitations with meta metric learning.Our meta metric learning approach consists of task-specific learners, thatexploit metric learning to handle flexible labels, and a meta learner, thatdiscovers good parameters and gradient decent to specify the metrics intask-specific learners. Thus the proposed model is able to handle unbalancedclasses as well as to generate task-specific metrics. We test our approach inthe `$k$-shot $N$-way' few-shot learning setting used in previous work and newrealistic few-shot setting with diverse multi-domain tasks and flexible labelnumbers. Experiments show that our approach attains superior performances inboth settings."
Boosting Few-Shot Visual Learning with Self-Supervision,"['Spyros Gidaris', 'Andrei Bursuc', 'Nikos Komodakis', 'Patrick Pérez', 'Matthieu Cord']",http://arxiv.org/pdf/1906.05186v1.pdf,2019-06-12,"['cs.cv', 'cs.lg']","  Few-shot learning and self-supervised learning address different facets ofthe same problem: how to train a model with little or no labeled data. Few-shotlearning aims for optimization methods and models that can learn efficiently torecognize patterns in the low data regime. Self-supervised learning focusesinstead on unlabeled data and looks into it for the supervisory signal to feedhigh capacity deep neural networks. In this work we exploit the complementarityof these two domains and propose an approach for improving few-shot learningthrough self-supervision. We use self-supervision as an auxiliary task in afew-shot learning pipeline, enabling feature extractors to learn richer andmore transferable visual representations while still using few annotatedsamples. Through self-supervision, our approach can be naturally extendedtowards using diverse unlabeled data from other datasets in the few-shotsetting. We report consistent improvements across an array of architectures,datasets and self-supervision techniques."
FLAT: Few-Shot Learning via Autoencoding Transformation Regularizers,"['Haohang Xu', 'Hongkai Xiong', 'Guojun Qi']",http://arxiv.org/pdf/1912.12674v1.pdf,2019-12-29,['cs.cv'],"  One of the most significant challenges facing a few-shot learning task is thegeneralizability of the (meta-)model from the base to the novel categories.Most of existing few-shot learning models attempt to address this challenge byeither learning the meta-knowledge from multiple simulated tasks on the basecategories, or resorting to data augmentation by applying varioustransformations to training examples. However, the supervised nature of modeltraining in these approaches limits their ability of exploring variationsacross different categories, thus restricting their cross-categorygeneralizability in modeling novel concepts. To this end, we present a novelregularization mechanism by learning the change of feature representationsinduced by a distribution of transformations without using the labels of dataexamples. We expect this regularizer could expand the semantic space of basecategories to cover that of novel categories through the transformation offeature representations. It could minimize the risk of overfitting into basecategories by inspecting the transformation-augmented variations at the encodedfeature level. This results in the proposed FLAT (Few-shot Learning viaAutoencoding Transformations) approach by autoencoding the appliedtransformations. The experiment results show the superior performances to thecurrent state-of-the-art methods in literature."
Attentive Graph Neural Networks for Few-Shot Learning,"['Hao Cheng', 'Joey Tianyi Zhou', 'Wee Peng Tay', 'Bihan Wen']",http://arxiv.org/pdf/2007.06878v2.pdf,2020-07-14,"['cs.lg', 'stat.ml']","  Graph Neural Networks (GNN) has demonstrated the superior performance in manychallenging applications, including the few-shot learning tasks. Despite itspowerful capacity to learn and generalize the model from few samples, GNNusually suffers from severe over-fitting and over-smoothing as the modelbecomes deep, which limit the scalability. In this work, we propose a novelAttentive GNN to tackle these challenges, by incorporating a triple-attentionmechanism, i.e. node self-attention, neighborhood attention, and layer memoryattention. We explain why the proposed attentive modules can improve GNN forfew-shot learning with theoretical analysis and illustrations. Extensiveexperiments show that the proposed Attentive GNN model achieves the promisingresults, comparing to the state-of-the-art GNN- and CNN-based methods forfew-shot learning tasks, over the mini-ImageNet and tiered-ImageNet benchmarks,under ConvNet-4 and ResNet-based backbone with both inductive and transductivesettings. The codes will be made publicly available."
RNNP: A Robust Few-Shot Learning Approach,"['Pratik Mazumder', 'Pravendra Singh', 'Vinay P. Namboodiri']",http://arxiv.org/pdf/2011.11067v1.pdf,2020-11-22,['cs.cv'],"  Learning from a few examples is an important practical aspect of trainingclassifiers. Various works have examined this aspect quite well. However, allexisting approaches assume that the few examples provided are always correctlylabeled. This is a strong assumption, especially if one considers the currenttechniques for labeling using crowd-based labeling services. We address thisissue by proposing a novel robust few-shot learning approach. Our method relieson generating robust prototypes from a set of few examples. Specifically, ourmethod refines the class prototypes by producing hybrid features from thesupport examples of each class. The refined prototypes help to classify thequery images better. Our method can replace the evaluation phase of anyfew-shot learning method that uses a nearest neighbor prototype-basedevaluation procedure to make them robust. We evaluate our method on standardmini-ImageNet and tiered-ImageNet datasets. We perform experiments with variouslabel corruption rates in the support examples of the few-shot classes. Weobtain significant improvement over widely used few-shot learning methods thatsuffer significant performance degeneration in the presence of label noise. Wefinally provide extensive ablation experiments to validate our method."
Few-Shot Learning for Road Object Detection,"['Anay Majee', 'Kshitij Agrawal', 'Anbumani Subramanian']",http://arxiv.org/pdf/2101.12543v2.pdf,2021-01-29,['cs.cv'],"  Few-shot learning is a problem of high interest in the evolution of deeplearning. In this work, we consider the problem of few-shot object detection(FSOD) in a real-world, class-imbalanced scenario. For our experiments, weutilize the India Driving Dataset (IDD), as it includes a class ofless-occurring road objects in the image dataset and hence provides a setupsuitable for few-shot learning. We evaluate both metric-learning andmeta-learning based FSOD methods, in two experimental settings: (i)representative (same-domain) splits from IDD, that evaluates the ability of amodel to learn in the context of road images, and (ii) object classes withless-occurring object samples, similar to the open-set setting in real-world.From our experiments, we demonstrate that the metric-learning methodoutperforms meta-learning on the novel classes by (i) 11.2 mAP points on thesame domain, and (ii) 1.0 mAP point on the open-set. We also show that ourextension of object classes in a real-world open dataset offers a rich groundfor few-shot learning studies."
AgileNet: Lightweight Dictionary-based Few-shot Learning,"['Mohammad Ghasemzadeh', 'Fang Lin', 'Bita Darvish Rouhani', 'Farinaz Koushanfar', 'Ke Huang']",http://arxiv.org/pdf/1805.08311v1.pdf,2018-05-21,"['cs.lg', 'cs.ai', 'cs.cv', 'cs.ne', 'stat.ml']","  The success of deep learning models is heavily tied to the use of massiveamount of labeled data and excessively long training time. With the emergenceof intelligent edge applications that use these models, the critical challengeis to obtain the same inference capability on a resource-constrained devicewhile providing adaptability to cope with the dynamic changes in the data. Wepropose AgileNet, a novel lightweight dictionary-based few-shot learningmethodology which provides reduced complexity deep neural network for efficientexecution at the edge while enabling low-cost updates to capture the dynamicsof the new data. Evaluations of state-of-the-art few-shot learning benchmarksdemonstrate the superior accuracy of AgileNet compared to prior arts.Additionally, AgileNet is the first few-shot learning approach that preventsmodel updates by eliminating the knowledge obtained from the primary training.This property is ensured through the dictionaries learned by our novelend-to-end structured decomposition, which also reduces the memory footprintand computation complexity to match the edge device constraints."
Large Margin Few-Shot Learning,"['Yong Wang', 'Xiao-Ming Wu', 'Qimai Li', 'Jiatao Gu', 'Wangmeng Xiang', 'Lei Zhang', 'Victor O. K. Li']",http://arxiv.org/pdf/1807.02872v2.pdf,2018-07-08,"['cs.lg', 'stat.ml']","  The key issue of few-shot learning is learning to generalize. This paperproposes a large margin principle to improve the generalization capacity ofmetric based methods for few-shot learning. To realize it, we develop a unifiedframework to learn a more discriminative metric space by augmenting theclassification loss function with a large margin distance loss function fortraining. Extensive experiments on two state-of-the-art few-shot learningmethods, graph neural networks and prototypical networks, show that our methodcan improve the performance of existing models substantially with very littlecomputational overhead, demonstrating the effectiveness of the large marginprinciple and the potential of our method."
Few Shot Learning with Simplex,"['Bowen Zhang', 'Xifan Zhang', 'Fan Cheng', 'Deli Zhao']",http://arxiv.org/pdf/1807.10726v2.pdf,2018-07-27,['cs.cv'],"  Deep learning has made remarkable achievement in many fields. However,learning the parameters of neural networks usually demands a large amount oflabeled data. The algorithms of deep learning, therefore, encounterdifficulties when applied to supervised learning where only little data areavailable. This specific task is called few-shot learning. To address it, wepropose a novel algorithm for few-shot learning using discrete geometry, in thesense that the samples in a class are modeled as a reduced simplex. The volumeof the simplex is used for the measurement of class scatter. During testing,combined with the test sample and the points in the class, a new simplex isformed. Then the similarity between the test sample and the class can bequantized with the ratio of volumes of the new simplex to the original classsimplex. Moreover, we present an approach to constructing simplices using localregions of feature maps yielded by convolutional neural networks. Experimentson Omniglot and miniImageNet verify the effectiveness of our simplex algorithmon few-shot learning."
Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning,"['Yinbo Chen', 'Zhuang Liu', 'Huijuan Xu', 'Trevor Darrell', 'Xiaolong Wang']",http://arxiv.org/pdf/2003.04390v4.pdf,2020-03-09,"['cs.cv', 'cs.lg']","  Meta-learning has been the most common framework for few-shot learning inrecent years. It learns the model from collections of few-shot classificationtasks, which is believed to have a key advantage of making the trainingobjective consistent with the testing objective. However, some recent worksreport that by training for whole-classification, i.e. classification on thewhole label-set, it can get comparable or even better embedding than manymeta-learning algorithms. The edge between these two lines of works has yetbeen underexplored, and the effectiveness of meta-learning in few-shot learningremains unclear. In this paper, we explore a simple process: meta-learning overa whole-classification pre-trained model on its evaluation metric. We observethis simple method achieves competitive performance to state-of-the-art methodson standard benchmarks. Our further analysis shed some light on understandingthe trade-offs between the meta-learning objective and the whole-classificationobjective in few-shot learning."
DPGN: Distribution Propagation Graph Network for Few-shot Learning,"['Ling Yang', 'Liangliang Li', 'Zilun Zhang', 'Xinyu Zhou', 'Erjin Zhou', 'Yu Liu']",http://arxiv.org/pdf/2003.14247v2.pdf,2020-03-31,['cs.cv'],"  Most graph-network-based meta-learning approaches model instance-levelrelation of examples. We extend this idea further to explicitly model thedistribution-level relation of one example to all other examples in a 1-vs-Nmanner. We propose a novel approach named distribution propagation graphnetwork (DPGN) for few-shot learning. It conveys both the distribution-levelrelations and instance-level relations in each few-shot learning task. Tocombine the distribution-level relations and instance-level relations for allexamples, we construct a dual complete graph network which consists of a pointgraph and a distribution graph with each node standing for an example. Equippedwith dual graph architecture, DPGN propagates label information from labeledexamples to unlabeled examples within several update generations. In extensiveexperiments on few-shot learning benchmarks, DPGN outperforms state-of-the-artresults by a large margin in 5% $\sim$ 12% under supervised setting and 7%$\sim$ 13% under semi-supervised setting. Code will be released."
Large Margin Mechanism and Pseudo Query Set on Cross-Domain Few-Shot  Learning,"['Jia-Fong Yeh', 'Hsin-Ying Lee', 'Bing-Chen Tsai', 'Yi-Rong Chen', 'Ping-Chia Huang', 'Winston H. Hsu']",http://arxiv.org/pdf/2005.09218v1.pdf,2020-05-19,"['cs.lg', 'stat.ml']","  In recent years, few-shot learning problems have received a lot of attention.While methods in most previous works were trained and tested on datasets in onesingle domain, cross-domain few-shot learning is a brand-new branch of few-shotlearning problems, where models handle datasets in different domains betweentraining and testing phases. In this paper, to solve the problem that the modelis pre-trained (meta-trained) on a single dataset while fine-tuned on datasetsin four different domains, including common objects, satellite images, andmedical images, we propose a novel large margin fine-tuning method (LMM-PQS),which generates pseudo query images from support images and fine-tunes thefeature extraction modules with a large margin mechanism inspired by methods inface recognition. According to the experiment results, LMM-PQS surpasses thebaseline models by a significant margin and demonstrates that our approach isrobust and can easily adapt pre-trained models to new domains with few data."
Self-supervised Knowledge Distillation for Few-shot Learning,"['Jathushan Rajasegaran', 'Salman Khan', 'Munawar Hayat', 'Fahad Shahbaz Khan', 'Mubarak Shah']",http://arxiv.org/pdf/2006.09785v2.pdf,2020-06-17,['cs.cv'],"  Real-world contains an overwhelmingly large number of object classes,learning all of which at once is infeasible. Few shot learning is a promisinglearning paradigm due to its ability to learn out of order distributionsquickly with only a few samples. Recent works [7, 41] show that simply learninga good feature embedding can outperform more sophisticated meta-learning andmetric learning algorithms for few-shot learning. In this paper, we propose asimple approach to improve the representation capacity of deep neural networksfor few-shot learning tasks. We follow a two-stage learning process: First, wetrain a neural network to maximize the entropy of the feature embedding, thuscreating an optimal output manifold using a self-supervised auxiliary loss. Inthe second stage, we minimize the entropy on feature embedding by bringingself-supervised twins together, while constraining the manifold withstudent-teacher distillation. Our experiments show that, even in the firststage, self-supervision can outperform current state-of-the-art methods, withfurther gains achieved by our second stage distillation process. Our codes areavailable at: https://github.com/brjathu/SKD."
A Transductive Multi-Head Model for Cross-Domain Few-Shot Learning,"['Jianan Jiang', 'Zhenpeng Li', 'Yuhong Guo', 'Jieping Ye']",http://arxiv.org/pdf/2006.11384v1.pdf,2020-06-08,"['cs.cv', 'cs.lg']","  In this paper, we present a new method, Transductive Multi-Head Few-Shotlearning (TMHFS), to address the Cross-Domain Few-Shot Learning (CD-FSL)challenge. The TMHFS method extends the Meta-Confidence Transduction (MCT) andDense Feature-Matching Networks (DFMN) method [2] by introducing a newprediction head, i.e, an instance-wise global classification network based onsemantic information, after the common feature embedding network. We train theembedding network with the multiple heads, i.e,, the MCT loss, the DFMN lossand the semantic classifier loss, simultaneously in the source domain. For thefew-shot learning in the target domain, we first perform fine-tuning on theembedding network with only the semantic global classifier and the supportinstances, and then use the MCT part to predict labels of the query set withthe fine-tuned embedding network. Moreover, we further exploit dataaugmentation techniques during the fine-tuning and test stages to improve theprediction performance. The experimental results demonstrate that the proposedmethods greatly outperform the strong baseline, fine-tuning, on four differenttarget domains."
Few-shot Learning with LSSVM Base Learner and Transductive Modules,"['Haoqing Wang', 'Zhi-Hong Deng']",http://arxiv.org/pdf/2009.05786v1.pdf,2020-09-12,"['cs.lg', 'stat.ml']","  The performance of meta-learning approaches for few-shot learning generallydepends on three aspects: features suitable for comparison, the classifier (base learner ) suitable for low-data scenarios, and valuable information fromthe samples to classify. In this work, we make improvements for the last twoaspects: 1) although there are many effective base learners, there is atrade-off between generalization performance and computational overhead, so weintroduce multi-class least squares support vector machine as our base learnerwhich obtains better generation than existing ones with less computationaloverhead; 2) further, in order to utilize the information from the querysamples, we propose two simple and effective transductive modules which modifythe support set using the query samples, i.e., adjusting the support samplesbasing on the attention mechanism and adding the prototypes of the query setwith pseudo labels to the support set as the pseudo support samples. These twomodules significantly improve the few-shot classification accuracy, especiallyfor the difficult 1-shot setting. Our model, denoted as FSLSTM (Few-Shotlearning with LSsvm base learner and Transductive Modules), achievesstate-of-the-art performance on miniImageNet and CIFAR-FS few-shot learningbenchmarks."
ReMP: Rectified Metric Propagation for Few-Shot Learning,"['Yang Zhao', 'Chunyuan Li', 'Ping Yu', 'Changyou Chen']",http://arxiv.org/pdf/2012.00904v1.pdf,2020-12-02,"['cs.cv', 'cs.lg']","  Few-shot learning features the capability of generalizing from a fewexamples. In this paper, we first identify that a discriminative feature space,namely a rectified metric space, that is learned to maintain the metricconsistency from training to testing, is an essential component to the successof metric-based few-shot learning. Numerous analyses indicate that a simplemodification of the objective can yield substantial performance gains. Theresulting approach, called rectified metric propagation (ReMP), furtheroptimizes an attentive prototype propagation network, and applies a repulsiveforce to make confident predictions. Extensive experiments demonstrate that theproposed ReMP is effective and efficient, and outperforms the state of the artson various standard few-shot learning datasets."
Towards Few-Shot Fact-Checking via Perplexity,"['Nayeon Lee', 'Yejin Bang', 'Andrea Madotto', 'Madian Khabsa', 'Pascale Fung']",http://arxiv.org/pdf/2103.09535v1.pdf,2021-03-17,"['cs.cl', 'cs.lg']","  Few-shot learning has drawn researchers' attention to overcome the problem ofdata scarcity. Recently, large pre-trained language models have shown greatperformance in few-shot learning for various downstream tasks, such as questionanswering and machine translation. Nevertheless, little exploration has beenmade to achieve few-shot learning for the fact-checking task. However,fact-checking is an important problem, especially when the amount ofinformation online is growing exponentially every day. In this paper, wepropose a new way of utilizing the powerful transfer learning ability of alanguage model via a perplexity score. The most notable strength of ourmethodology lies in its capability in few-shot learning. With only two trainingsamples, our methodology can already outperform the Major Class baseline bymore than absolute 10% on the F1-Macro metric across multiple datasets. Throughexperiments, we empirically verify the plausibility of the rather surprisingusage of the perplexity score in the context of fact-checking and highlight thestrength of our few-shot methodology by comparing it to strongfine-tuning-based baseline models. Moreover, we construct and publicly releasetwo new fact-checking datasets related to COVID-19."
Few-Shot Learning by Integrating Spatial and Frequency Representation,"['Xiangyu Chen', 'Guanghui Wang']",http://arxiv.org/pdf/2105.05348v2.pdf,2021-05-11,['cs.cv'],"  Human beings can recognize new objects with only a few labeled examples,however, few-shot learning remains a challenging problem for machine learningsystems. Most previous algorithms in few-shot learning only utilize spatialinformation of the images. In this paper, we propose to integrate the frequencyinformation into the learning model to boost the discrimination ability of thesystem. We employ Discrete Cosine Transformation (DCT) to generate thefrequency representation, then, integrate the features from both the spatialdomain and frequency domain for classification. The proposed strategy and itseffectiveness are validated with different backbones, datasets, and algorithms.Extensive experiments demonstrate that the frequency information iscomplementary to the spatial representations in few-shot classification. Theclassification accuracy is boosted significantly by integrating features fromboth the spatial and frequency domains in different few-shot learning tasks."
Multi-Label Few-Shot Learning for Aspect Category Detection,"['Mengting Hu', 'Shiwan Zhao', 'Honglei Guo', 'Chao Xue', 'Hang Gao', 'Tiegang Gao', 'Renhong Cheng', 'Zhong Su']",http://arxiv.org/pdf/2105.14174v1.pdf,2021-05-29,['cs.cl'],"  Aspect category detection (ACD) in sentiment analysis aims to identify theaspect categories mentioned in a sentence. In this paper, we formulate ACD inthe few-shot learning scenario. However, existing few-shot learning approachesmainly focus on single-label predictions. These methods can not work well forthe ACD task since a sentence may contain multiple aspect categories.Therefore, we propose a multi-label few-shot learning method based on theprototypical network. To alleviate the noise, we design two effective attentionmechanisms. The support-set attention aims to extract better prototypes byremoving irrelevant aspects. The query-set attention computes multipleprototype-specific representations for each query instance, which are then usedto compute accurate distances with the corresponding prototypes. To achievemulti-label inference, we further learn a dynamic threshold per instance by apolicy network. Extensive experimental results on three datasets demonstratethat the proposed method significantly outperforms strong baselines."
Episode Adaptive Embedding Networks for Few-shot Learning,"['Fangbing Liu', 'Qing Wang']",http://arxiv.org/pdf/2106.09398v1.pdf,2021-06-17,['cs.cv'],"  Few-shot learning aims to learn a classifier using a few labelled instancesfor each class. Metric-learning approaches for few-shot learning embedinstances into a high-dimensional space and conduct classification based ondistances among instance embeddings. However, such instance embeddings areusually shared across all episodes and thus lack the discriminative power togeneralize classifiers according to episode-specific features. In this paper,we propose a novel approach, namely \emph{Episode Adaptive Embedding Network}(EAEN), to learn episode-specific embeddings of instances. By leveraging theprobability distributions of all instances in an episode at each channel-pixelembedding dimension, EAEN can not only alleviate the overfitting issueencountered in few-shot learning tasks, but also capture discriminativefeatures specific to an episode. To empirically verify the effectiveness androbustness of EAEN, we have conducted extensive experiments on three widelyused benchmark datasets, under various combinations of different genericembedding backbones and different classifiers. The results show that EAENsignificantly improves classification accuracy about $10\%$ to $20\%$ indifferent settings over the state-of-the-art methods."
Bayesian Embeddings for Few-Shot Open World Recognition,"['John Willes', 'James Harrison', 'Ali Harakeh', 'Chelsea Finn', 'Marco Pavone', 'Steven Waslander']",http://arxiv.org/pdf/2107.13682v2.pdf,2021-07-29,['cs.cv'],"  As autonomous decision-making agents move from narrow operating environmentsto unstructured worlds, learning systems must move from a closed-worldformulation to an open-world and few-shot setting in which agents continuouslylearn new classes from small amounts of information. This stands in starkcontrast to modern machine learning systems that are typically designed with aknown set of classes and a large number of examples for each class. In thiswork we extend embedding-based few-shot learning algorithms to the open-worldrecognition setting. We combine Bayesian non-parametric class priors with anembedding-based pre-training scheme to yield a highly flexible framework whichwe refer to as few-shot learning for open world recognition (FLOWR). Webenchmark our framework on open-world extensions of the common MiniImageNet andTieredImageNet few-shot learning datasets. Our results show, compared to priormethods, strong classification accuracy performance and up to a 12% improvementin H-measure (a measure of novel class detection) from our non-parametricopen-world few-shot learning scheme."
Self-Denoising Neural Networks for Few Shot Learning,"['Steven Schwarcz', 'Sai Saketh Rambhatla', 'Rama Chellappa']",http://arxiv.org/pdf/2110.13386v1.pdf,2021-10-26,"['cs.cv', 'cs.ai']","  In this paper, we introduce a new architecture for few shot learning, thetask of teaching a neural network from as few as one or five labeled examples.Inspired by the theoretical results of Alaine et al that Denoising Autoencodersrefine features to lie closer to the true data manifold, we present a newtraining scheme that adds noise at multiple stages of an existing neuralarchitecture while simultaneously learning to be robust to this added noise.This architecture, which we call a Self-Denoising Neural Network (SDNN), can beapplied easily to most modern convolutional neural architectures, and can beused as a supplement to many existing few-shot learning techniques. Weempirically show that SDNNs out-perform previous state-of-the-art methods forfew shot image recognition using the Wide-ResNet architecture on the\textit{mini}ImageNet, tiered-ImageNet, and CIFAR-FS few shot learningdatasets. We also perform a series of ablation experiments to empiricallyjustify the construction of the SDNN architecture. Finally, we show that SDNNseven improve few shot performance on the task of human action detection invideo using experiments on the ActEV SDL Surprise Activities challenge."
True Few-Shot Learning with Prompts -- A Real-World Perspective,"['Timo Schick', 'Hinrich Schütze']",http://arxiv.org/pdf/2111.13440v1.pdf,2021-11-26,['cs.cl'],"  Prompt-based approaches are strong at few-shot learning. However, Perez etal. (2021) have recently cast doubt on their performance because they haddifficulty getting good results in a ""true"" few-shot setting in which promptsand hyperparameters cannot be tuned on a dev set. In view of this, we conductan extensive study of PET, a method that combines textual instructions withexample-based finetuning. We show that, if correctly configured, PET performsstrongly in a true few-shot setting, i.e., without a dev set. Crucial for thisstrong performance is PET's ability to intelligently handle multiple prompts.We then put our findings to a real-world test by running PET on RAFT, abenchmark of tasks taken directly from realistic NLP applications for which nolabeled dev or test sets are available. PET achieves a new state of the art onRAFT and performs close to non-expert humans for 7 out of 11 tasks. Theseresults demonstrate that prompt-based learners like PET excel at true few-shotlearning and underpin our belief that learning from instructions will play animportant role on the path towards human-like few-shot learning capabilities."
Improving In-Context Few-Shot Learning via Self-Supervised Training,"['Mingda Chen', 'Jingfei Du', 'Ramakanth Pasunuru', 'Todor Mihaylov', 'Srini Iyer', 'Veselin Stoyanov', 'Zornitsa Kozareva']",http://arxiv.org/pdf/2205.01703v2.pdf,2022-05-03,['cs.cl'],"  Self-supervised pretraining has made few-shot learning possible for many NLPtasks. But the pretraining objectives are not typically adapted specificallyfor in-context few-shot learning. In this paper, we propose to useself-supervision in an intermediate training stage between pretraining anddownstream few-shot usage with the goal to teach the model to performin-context few shot learning. We propose and evaluate four self-supervisedobjectives on two benchmarks. We find that the intermediate self-supervisionstage produces models that outperform strong baselines. Ablation study showsthat several factors affect the downstream performance, such as the amount oftraining data and the diversity of the self-supervised objectives.Human-annotated cross-task supervision and self-supervision are complementary.Qualitative analysis suggests that the self-supervised-trained models arebetter at following task requirements."
Privacy Enhancement for Cloud-Based Few-Shot Learning,"['Archit Parnami', 'Muhammad Usama', 'Liyue Fan', 'Minwoo Lee']",http://arxiv.org/pdf/2205.07864v2.pdf,2022-05-10,"['cs.lg', 'cs.cr', 'cs.cv']","  Requiring less data for accurate models, few-shot learning has shownrobustness and generality in many application domains. However, deployingfew-shot models in untrusted environments may inflict privacy concerns, e.g.,attacks or adversaries that may breach the privacy of user-supplied data. Thispaper studies the privacy enhancement for the few-shot learning in an untrustedenvironment, e.g., the cloud, by establishing a novel privacy-preservedembedding space that preserves the privacy of data and maintains the accuracyof the model. We examine the impact of various image privacy methods such asblurring, pixelization, Gaussian noise, and differentially private pixelization(DP-Pix) on few-shot image classification and propose a method that learnsprivacy-preserved representation through the joint loss. The empirical resultsshow how privacy-performance trade-off can be negotiated for privacy-enhancedfew-shot learning."
Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained  Models,"['Mengzhou Xia', 'Mikel Artetxe', 'Jingfei Du', 'Danqi Chen', 'Ves Stoyanov']",http://arxiv.org/pdf/2205.15223v3.pdf,2022-05-30,"['cs.cl', 'cs.lg']","  Pre-trained masked language models successfully perform few-shot learning byformulating downstream tasks as text infilling. However, as a strongalternative in full-shot settings, discriminative pre-trained models likeELECTRA do not fit into the paradigm. In this work, we adapt prompt-basedfew-shot learning to ELECTRA and show that it outperforms masked languagemodels in a wide range of tasks. ELECTRA is pre-trained to distinguish if atoken is generated or original. We naturally extend that to prompt-basedfew-shot learning by training to score the originality of the target optionswithout introducing new parameters. Our method can be easily adapted to tasksinvolving multi-token predictions without extra computation overhead. Analysisshows that ELECTRA learns distributions that align better with downstreamtasks."
Instance Selection Mechanisms for Human-in-the-Loop Systems in Few-Shot  Learning,"['Johannes Jakubik', 'Benedikt Blumenstiel', 'Michael Vössing', 'Patrick Hemmer']",http://arxiv.org/pdf/2207.06835v1.pdf,2022-07-14,['cs.lg'],"  Business analytics and machine learning have become essential success factorsfor various industries - with the downside of cost-intensive gathering andlabeling of data. Few-shot learning addresses this challenge and reduces datagathering and labeling costs by learning novel classes with very few labeleddata. In this paper, we design a human-in-the-loop (HITL) system for few-shotlearning and analyze an extensive range of mechanisms that can be used toacquire human expert knowledge for instances that have an uncertain predictionoutcome. We show that the acquisition of human expert knowledge significantlyaccelerates the few-shot model performance given a negligible labeling effort.We validate our findings in various experiments on a benchmark dataset incomputer vision and real-world datasets. We further demonstrate thecost-effectiveness of HITL systems for few-shot learning. Overall, our workaims at supporting researchers and practitioners in effectively adaptingmachine learning models to novel classes at reduced costs."
Revisiting Few-Shot Learning from a Causal Perspective,"['Guoliang Lin', 'Hanjiang Lai']",http://arxiv.org/pdf/2209.13816v1.pdf,2022-09-28,"['cs.lg', 'cs.ai']","  Few-shot learning with N-way K-shot scheme is an open challenge in machinelearning. Many approaches have been proposed to tackle this problem, e.g., theMatching Networks and CLIP-Adapter. Despite that these approaches have shownsignificant progress, the mechanism of why these methods succeed has not beenwell explored. In this paper, we interpret these few-shot learning methods viacausal mechanism. We show that the existing approaches can be viewed asspecific forms of front-door adjustment, which is to remove the effects ofconfounders. Based on this, we introduce a general causal method for few-shotlearning, which considers not only the relationship between examples but alsothe diversity of representations. Experimental results demonstrate thesuperiority of our proposed method in few-shot classification on variousbenchmark datasets. Code is available in the supplementary material."
Unsupervised Few-shot Learning via Deep Laplacian Eigenmaps,"['Kuilin Chen', 'Chi-Guhn Lee']",http://arxiv.org/pdf/2210.03595v1.pdf,2022-10-07,['cs.lg'],"  Learning a new task from a handful of examples remains an open challenge inmachine learning. Despite the recent progress in few-shot learning, mostmethods rely on supervised pretraining or meta-learning on labeledmeta-training data and cannot be applied to the case where the pretraining datais unlabeled. In this study, we present an unsupervised few-shot learningmethod via deep Laplacian eigenmaps. Our method learns representation fromunlabeled data by grouping similar samples together and can be intuitivelyinterpreted by random walks on augmented training data. We analytically showhow deep Laplacian eigenmaps avoid collapsed representation in unsupervisedlearning without explicit comparison between positive and negative samples. Theproposed method significantly closes the performance gap between supervised andunsupervised few-shot learning. Our method also achieves comparable performanceto current state-of-the-art self-supervised learning methods under linearevaluation protocol."
In-context Learning Distillation: Transferring Few-shot Learning Ability  of Pre-trained Language Models,"['Yukun Huang', 'Yanda Chen', 'Zhou Yu', 'Kathleen McKeown']",http://arxiv.org/pdf/2212.10670v1.pdf,2022-12-20,"['cs.cl', 'cs.lg']","  Given the success with in-context learning of large pre-trained languagemodels, we introduce in-context learning distillation to transfer in-contextfew-shot learning ability from large models to smaller models. We propose tocombine in-context learning objectives with language modeling objectives todistill both the ability to read in-context examples and task knowledge to thesmaller models. We perform in-context learning distillation under two differentfew-shot learning paradigms: Meta In-context Tuning (Meta-ICT) and MultitaskIn-context Tuning (Multitask-ICT). Multitask-ICT performs better on multitaskfew-shot learning but also requires more computation than Meta-ICT. Our methodshows consistent improvements for both Meta-ICT and Multitask-ICT on twobenchmarks: LAMA and CrossFit. Our extensive experiments and analysis revealthat in-context learning objectives and language modeling objectives arecomplementary under the Multitask-ICT paradigm. In-context learning objectivesachieve the best performance when combined with language modeling objectives."
Gestalt-Guided Image Understanding for Few-Shot Learning,"['Kun Song', 'Yuchen Wu', 'Jiansheng Chen', 'Tianyu Hu', 'Huimin Ma']",http://arxiv.org/pdf/2302.03922v1.pdf,2023-02-08,['cs.cv'],"  Due to the scarcity of available data, deep learning does not perform well onfew-shot learning tasks. However, human can quickly learn the feature of a newcategory from very few samples. Nevertheless, previous work has rarelyconsidered how to mimic human cognitive behavior and apply it to few-shotlearning. This paper introduces Gestalt psychology to few-shot learning andproposes Gestalt-Guided Image Understanding, a plug-and-play method calledGGIU. Referring to the principle of totality and the law of closure in Gestaltpsychology, we design Totality-Guided Image Understanding and Closure-GuidedImage Understanding to extract image features. After that, a feature estimationmodule is used to estimate the accurate features of images. Extensiveexperiments demonstrate that our method can improve the performance of existingmodels effectively and flexibly without retraining or fine-tuning. Our code isreleased on https://github.com/skingorz/GGIU."
CovidExpert: A Triplet Siamese Neural Network framework for the  detection of COVID-19,"['Tareque Rahman Ornob', 'Gourab Roy', 'Enamul Hassan']",http://arxiv.org/pdf/2302.09004v1.pdf,2023-02-17,"['cs.cv', 'cs.lg', 'i.2.1; i.4.9']","  Patients with the COVID-19 infection may have pneumonia-like symptoms as wellas respiratory problems which may harm the lungs. From medical images,coronavirus illness may be accurately identified and predicted using a varietyof machine learning methods. Most of the published machine learning methods mayneed extensive hyperparameter adjustment and are unsuitable for small datasets.By leveraging the data in a comparatively small dataset, few-shot learningalgorithms aim to reduce the requirement of large datasets. This inspired us todevelop a few-shot learning model for early detection of COVID-19 to reduce thepost-effect of this dangerous disease. The proposed architecture combinesfew-shot learning with an ensemble of pre-trained convolutional neural networksto extract feature vectors from CT scan images for similarity learning. Theproposed Triplet Siamese Network as the few-shot learning model classified CTscan images into Normal, COVID-19, and Community-Acquired Pneumonia. Thesuggested model achieved an overall accuracy of 98.719%, a specificity of99.36%, a sensitivity of 98.72%, and a ROC score of 99.9% with only 200 CTscans per category for training data."
FILM: How can Few-Shot Image Classification Benefit from Pre-Trained  Language Models?,"['Zihao Jiang', 'Yunkai Dang', 'Dong Pang', 'Huishuai Zhang', 'Weiran Huang']",http://arxiv.org/pdf/2307.04114v1.pdf,2023-07-09,"['cs.lg', 'cs.ai', 'cs.cl', 'cs.cv', 'cs.mm']","  Few-shot learning aims to train models that can be generalized to novelclasses with only a few samples. Recently, a line of works are proposed toenhance few-shot learning with accessible semantic information from classnames. However, these works focus on improving existing modules such as visualprototypes and feature extractors of the standard few-shot learning framework.This limits the full potential use of semantic information. In this paper, wepropose a novel few-shot learning framework that uses pre-trained languagemodels based on contrastive learning. To address the challenge of alignmentbetween visual features and textual embeddings obtained from text-basedpre-trained language model, we carefully design the textual branch of ourframework and introduce a metric module to generalize the cosine similarity.For better transferability, we let the metric module adapt to differentfew-shot tasks and adopt MAML to train the model via bi-level optimization.Moreover, we conduct extensive experiments on multiple benchmarks todemonstrate the effectiveness of our method."
Inferring Latent Class Statistics from Text for Robust Visual Few-Shot  Learning,"['Yassir Bendou', 'Vincent Gripon', 'Bastien Pasdeloup', 'Giulia Lioi', 'Lukas Mauch', 'Fabien Cardinaux', 'Ghouthi Boukli Hacene']",http://arxiv.org/pdf/2311.14544v1.pdf,2023-11-24,"['cs.cv', 'cs.ai']","  In the realm of few-shot learning, foundation models like CLIP have proveneffective but exhibit limitations in cross-domain robustness especially infew-shot settings. Recent works add text as an extra modality to enhance theperformance of these models. Most of these approaches treat text as anauxiliary modality without fully exploring its potential to elucidate theunderlying class visual features distribution. In this paper, we present anovel approach that leverages text-derived statistics to predict the mean andcovariance of the visual feature distribution for each class. This predictiveframework enriches the latent space, yielding more robust and generalizablefew-shot learning models. We demonstrate the efficacy of incorporating bothmean and covariance statistics in improving few-shot classification performanceacross various datasets. Our method shows that we can use text to predict themean and covariance of the distribution offering promising improvements infew-shot learning scenarios."
A Broader Study of Cross-Domain Few-Shot Learning,"['Yunhui Guo', 'Noel C. Codella', 'Leonid Karlinsky', 'James V. Codella', 'John R. Smith', 'Kate Saenko', 'Tajana Rosing', 'Rogerio Feris']",http://arxiv.org/pdf/1912.07200v3.pdf,2019-12-16,"['cs.cv', 'cs.lg']","  Recent progress on few-shot learning largely relies on annotated data formeta-learning: base classes sampled from the same domain as the novel classes.However, in many applications, collecting data for meta-learning is infeasibleor impossible. This leads to the cross-domain few-shot learning problem, wherethere is a large shift between base and novel class domains. Whileinvestigations of the cross-domain few-shot scenario exist, these works arelimited to natural images that still contain a high degree of visualsimilarity. No work yet exists that examines few-shot learning across differentimaging methods seen in real world scenarios, such as aerial and medicalimaging. In this paper, we propose the Broader Study of Cross-Domain Few-ShotLearning (BSCD-FSL) benchmark, consisting of image data from a diverseassortment of image acquisition methods. This includes natural images, such ascrop disease images, but additionally those that present with an increasingdissimilarity to natural images, such as satellite images, dermatology images,and radiology images. Extensive experiments on the proposed benchmark areperformed to evaluate state-of-art meta-learning approaches, transfer learningapproaches, and newer methods for cross-domain few-shot learning. The resultsdemonstrate that state-of-art meta-learning methods are surprisinglyoutperformed by earlier meta-learning approaches, and all meta-learning methodsunderperform in relation to simple fine-tuning by 12.8% average accuracy.Performance gains previously observed with methods specialized for cross-domainfew-shot learning vanish in this more challenging benchmark. Finally, accuracyof all methods tend to correlate with dataset similarity to natural images,verifying the value of the benchmark to better represent the diversity of dataseen in practice and guiding future research."
Automated Human Cell Classification in Sparse Datasets using Few-Shot  Learning,"['Reece Walsh', 'Mohamed H. Abdelpakey', 'Mohamed S. Shehata', 'Mostafa M. Mohamed']",http://arxiv.org/pdf/2107.13093v2.pdf,2021-07-27,"['cs.cv', 'cs.ai', 'cs.lg']","  Classifying and analyzing human cells is a lengthy procedure, often involvinga trained professional. In an attempt to expedite this process, an active areaof research involves automating cell classification through use of deeplearning-based techniques. In practice, a large amount of data is required toaccurately train these deep learning models. However, due to the sparse humancell datasets currently available, the performance of these models is typicallylow. This study investigates the feasibility of using few-shot learning-basedtechniques to mitigate the data requirements for accurate training. The studyis comprised of three parts: First, current state-of-the-art few-shot learningtechniques are evaluated on human cell classification. The selected techniquesare trained on a non-medical dataset and then tested on two out-of-domain,human cell datasets. The results indicate that, overall, the test accuracy ofstate-of-the-art techniques decreased by at least 30% when transitioning from anon-medical dataset to a medical dataset. Second, this study evaluates thepotential benefits, if any, to varying the backbone architecture and trainingschemes in current state-of-the-art few-shot learning techniques when used inhuman cell classification. Even with these variations, the overall testaccuracy decreased from 88.66% on non-medical datasets to 44.13% at best on themedical datasets. Third, this study presents future directions for usingfew-shot learning in human cell classification. In general, few-shot learningin its current state performs poorly on human cell classification. The studyproves that attempts to modify existing network architectures are not effectiveand concludes that future research effort should be focused on improvingrobustness towards out-of-domain testing using optimization-based orself-supervised few-shot learning techniques."
Few-shot Learning by Exploiting Visual Concepts within CNNs,"['Boyang Deng', 'Qing Liu', 'Siyuan Qiao', 'Alan Yuille']",http://arxiv.org/pdf/1711.08277v3.pdf,2017-11-22,"['cs.cv', 'cs.lg', 'stat.ml']","  Convolutional neural networks (CNNs) are one of the driving forces for theadvancement of computer vision. Despite their promising performances on manytasks, CNNs still face major obstacles on the road to achieving ideal machineintelligence. One is that CNNs are complex and hard to interpret. Another isthat standard CNNs require large amounts of annotated data, which is sometimeshard to obtain, and it is desirable to learn to recognize objects from fewexamples. In this work, we address these limitations of CNNs by developingnovel, flexible, and interpretable models for few-shot learning. Our models arebased on the idea of encoding objects in terms of visual concepts (VCs), whichare interpretable visual cues represented by the feature vectors within CNNs.We first adapt the learning of VCs to the few-shot setting, and then uncovertwo key properties of feature encoding using VCs, which we call categorysensitivity and spatial pattern. Motivated by these properties, we present twointuitive models for the problem of few-shot learning. Experiments show thatour models achieve competitive performances, while being more flexible andinterpretable than alternative state-of-the-art few-shot learning methods. Weconclude that using VCs helps expose the natural capability of CNNs forfew-shot learning."
Meta-Transfer Learning for Few-Shot Learning,"['Qianru Sun', 'Yaoyao Liu', 'Tat-Seng Chua', 'Bernt Schiele']",http://arxiv.org/pdf/1812.02391v3.pdf,2018-12-06,['cs.cv'],"  Meta-learning has been proposed as a framework to address the challengingfew-shot learning setting. The key idea is to leverage a large number ofsimilar few-shot tasks in order to learn how to adapt a base-learner to a newtask for which only a few labeled samples are available. As deep neuralnetworks (DNNs) tend to overfit using a few samples only, meta-learningtypically uses shallow neural networks (SNNs), thus limiting its effectiveness.In this paper we propose a novel few-shot learning method called meta-transferlearning (MTL) which learns to adapt a deep NN for few shot learning tasks.Specifically, ""meta"" refers to training multiple tasks, and ""transfer"" isachieved by learning scaling and shifting functions of DNN weights for eachtask. In addition, we introduce the hard task (HT) meta-batch scheme as aneffective learning curriculum for MTL. We conduct experiments using (5-class,1-shot) and (5-class, 5-shot) recognition tasks on two challenging few-shotlearning benchmarks: miniImageNet and Fewshot-CIFAR100. Extensive comparisonsto related works validate that our meta-transfer learning approach trained withthe proposed HT meta-batch scheme achieves top performance. An ablation studyalso shows that both components contribute to fast convergence and highaccuracy."
Combat Data Shift in Few-shot Learning with Knowledge Graph,"['Yongchun Zhu', 'Fuzhen Zhuang', 'Xiangliang Zhang', 'Zhiyuan Qi', 'Zhiping Shi', 'Juan Cao', 'Qing He']",http://arxiv.org/pdf/2101.11354v3.pdf,2021-01-27,"['cs.lg', 'cs.ai']","  Many few-shot learning approaches have been designed under the meta-learningframework, which learns from a variety of learning tasks and generalizes to newtasks. These meta-learning approaches achieve the expected performance in thescenario where all samples are drawn from the same distributions (i.i.d.observations). However, in real-world applications, few-shot learning paradigmoften suffers from data shift, i.e., samples in different tasks, even in thesame task, could be drawn from various data distributions. Most existingfew-shot learning approaches are not designed with the consideration of datashift, and thus show downgraded performance when data distribution shifts.However, it is non-trivial to address the data shift problem in few-shotlearning, due to the limited number of labeled samples in each task. Targetingat addressing this problem, we propose a novel metric-based meta-learningframework to extract task-specific representations and task-sharedrepresentations with the help of knowledge graph. The data shift within/betweentasks can thus be combated by the combination of task-shared and task-specificrepresentations. The proposed model is evaluated on popular benchmarks and twoconstructed new challenging datasets. The evaluation results demonstrate itsremarkable performance."
Charting the Right Manifold: Manifold Mixup for Few-shot Learning,"['Puneet Mangla', 'Mayank Singh', 'Abhishek Sinha', 'Nupur Kumari', 'Vineeth N Balasubramanian', 'Balaji Krishnamurthy']",http://arxiv.org/pdf/1907.12087v4.pdf,2019-07-28,"['cs.lg', 'cs.cv', 'stat.ml']","  Few-shot learning algorithms aim to learn model parameters capable ofadapting to unseen classes with the help of only a few labeled examples. Arecent regularization technique - Manifold Mixup focuses on learning ageneral-purpose representation, robust to small changes in the datadistribution. Since the goal of few-shot learning is closely linked to robustrepresentation learning, we study Manifold Mixup in this problem setting.Self-supervised learning is another technique that learns semanticallymeaningful features, using only the inherent structure of the data. This workinvestigates the role of learning relevant feature manifold for few-shot tasksusing self-supervision and regularization techniques. We observe thatregularizing the feature manifold, enriched via self-supervised techniques,with Manifold Mixup significantly improves few-shot learning performance. Weshow that our proposed method S2M2 beats the current state-of-the-art accuracyon standard few-shot learning datasets like CIFAR-FS, CUB, mini-ImageNet andtiered-ImageNet by 3-8 %. Through extensive experimentation, we show that thefeatures learned using our approach generalize to complex few-shot evaluationtasks, cross-domain scenarios and are robust against slight changes to datadistribution."
Learning to Compare Relation: Semantic Alignment for Few-Shot Learning,"['Congqi Cao', 'Yanning Zhang']",http://arxiv.org/pdf/2003.00210v2.pdf,2020-02-29,['cs.cv'],"  Few-shot learning is a fundamental and challenging problem since it requiresrecognizing novel categories from only a few examples. The objects forrecognition have multiple variants and can locate anywhere in images. Directlycomparing query images with example images can not handle content misalignment.The representation and metric for comparison are critical but challenging tolearn due to the scarcity and wide variation of the samples in few-shotlearning. In this paper, we present a novel semantic alignment model to comparerelations, which is robust to content misalignment. We propose to add two keyingredients to existing few-shot learning frameworks for better feature andmetric learning ability. First, we introduce a semantic alignment loss to alignthe relation statistics of the features from samples that belong to the samecategory. And second, local and global mutual information maximization isintroduced, allowing for representations that contain locally-consistent andintra-class shared information across structural locations in an image.Thirdly, we introduce a principled approach to weigh multiple loss functions byconsidering the homoscedastic uncertainty of each stream. We conduct extensiveexperiments on several few-shot learning datasets. Experimental results showthat the proposed method is capable of comparing relations with semanticalignment strategies, and achieves state-of-the-art performance."
XtarNet: Learning to Extract Task-Adaptive Representation for  Incremental Few-Shot Learning,"['Sung Whan Yoon', 'Do-Yeon Kim', 'Jun Seo', 'Jaekyun Moon']",http://arxiv.org/pdf/2003.08561v2.pdf,2020-03-19,"['cs.lg', 'cs.ai', 'cs.ne']","  Learning novel concepts while preserving prior knowledge is a long-standingchallenge in machine learning. The challenge gets greater when a novel task isgiven with only a few labeled examples, a problem known as incremental few-shotlearning. We propose XtarNet, which learns to extract task-adaptiverepresentation (TAR) for facilitating incremental few-shot learning. The methodutilizes a backbone network pretrained on a set of base categories while alsoemploying additional modules that are meta-trained across episodes. Given a newtask, the novel feature extracted from the meta-trained modules is mixed withthe base feature obtained from the pretrained model. The process of combiningtwo different features provides TAR and is also controlled by meta-trainedmodules. The TAR contains effective information for classifying both novel andbase categories. The base and novel classifiers quickly adapt to a given taskby utilizing the TAR. Experiments on standard image datasets indicate thatXtarNet achieves state-of-the-art incremental few-shot learning performance.The concept of TAR can also be used in conjunction with existing incrementalfew-shot learning methods; extensive simulation results in fact show thatapplying TAR enhances the known methods significantly."
Diversity Helps: Unsupervised Few-shot Learning via Distribution  Shift-based Data Augmentation,"['Tiexin Qin', 'Wenbin Li', 'Yinghuan Shi', 'Yang Gao']",http://arxiv.org/pdf/2004.05805v2.pdf,2020-04-13,['cs.cv'],"  Few-shot learning aims to learn a new concept when only a few trainingexamples are available, which has been extensively explored in recent years.However, most of the current works heavily rely on a large-scale labeledauxiliary set to train their models in an episodic-training paradigm. Such akind of supervised setting basically limits the widespread use of few-shotlearning algorithms. Instead, in this paper, we develop a novel frameworkcalled Unsupervised Few-shot Learning via Distribution Shift-based DataAugmentation (ULDA), which pays attention to the distribution diversity insideeach constructed pretext few-shot task when using data augmentation.Importantly, we highlight the value and importance of the distributiondiversity in the augmentation-based pretext few-shot tasks, which caneffectively alleviate the overfitting problem and make the few-shot model learnmore robust feature representations. In ULDA, we systemically investigate theeffects of different augmentation techniques and propose to strengthen thedistribution diversity (or difference) between the query set and support set ineach few-shot task, by augmenting these two sets diversely (i.e., distributionshifting). In this way, even incorporated with simple augmentation techniques(e.g., random crop, color jittering, or rotation), our ULDA can produce asignificant improvement. In the experiments, few-shot models learned by ULDAcan achieve superior generalization performance and obtain state-of-the-artresults in a variety of established few-shot learning tasks on Omniglot andminiImageNet. The source code is available inhttps://github.com/WonderSeven/ULDA."
Defining Benchmarks for Continual Few-Shot Learning,"['Antreas Antoniou', 'Massimiliano Patacchiola', 'Mateusz Ochal', 'Amos Storkey']",http://arxiv.org/pdf/2004.11967v1.pdf,2020-04-15,"['cs.cv', 'cs.lg', 'stat.ml']","  Both few-shot and continual learning have seen substantial progress in thelast years due to the introduction of proper benchmarks. That being said, thefield has still to frame a suite of benchmarks for the highly desirable settingof continual few-shot learning, where the learner is presented a number offew-shot tasks, one after the other, and then asked to perform well on avalidation set stemming from all previously seen tasks. Continual few-shotlearning has a small computational footprint and is thus an excellent settingfor efficient investigation and experimentation. In this paper we first definea theoretical framework for continual few-shot learning, taking into accountrecent literature, then we propose a range of flexible benchmarks that unifythe evaluation criteria and allows exploring the problem from multipleperspectives. As part of the benchmark, we introduce a compact variant ofImageNet, called SlimageNet64, which retains all original 1000 classes but onlycontains 200 instances of each one (a total of 200K data-points) downscaled to64 x 64 pixels. We provide baselines for the proposed benchmarks using a numberof popular few-shot learning algorithms, as a result, exposing previouslyunknown strengths and weaknesses of those algorithms in continual anddata-limited settings."
Looking back to lower-level information in few-shot learning,"['Zhongjie Yu', 'Sebastian Raschka']",http://arxiv.org/pdf/2005.13638v2.pdf,2020-05-27,"['cs.lg', 'cs.cv', 'stat.ml']","  Humans are capable of learning new concepts from small numbers of examples.In contrast, supervised deep learning models usually lack the ability toextract reliable predictive rules from limited data scenarios when attemptingto classify new examples. This challenging scenario is commonly known asfew-shot learning. Few-shot learning has garnered increased attention in recentyears due to its significance for many real-world problems. Recently, newmethods relying on meta-learning paradigms combined with graph-basedstructures, which model the relationship between examples, have shown promisingresults on a variety of few-shot classification tasks. However, existing workon few-shot learning is only focused on the feature embeddings produced by thelast layer of the neural network. In this work, we propose the utilization oflower-level, supporting information, namely the feature embeddings of thehidden neural network layers, to improve classifier accuracy. Based on agraph-based meta-learning framework, we develop a method called Looking-Back,where such lower-level information is used to construct additional graphs forlabel propagation in limited data settings. Our experiments on two popularfew-shot learning datasets, miniImageNet and tieredImageNet, show that ourmethod can utilize the lower-level information in the network to improvestate-of-the-art classification performance."
Reordering Examples Helps during Priming-based Few-Shot Learning,"['Sawan Kumar', 'Partha Talukdar']",http://arxiv.org/pdf/2106.01751v1.pdf,2021-06-03,['cs.cl'],"  The ability to learn from limited data, or few-shot learning, is a desirableand often critical requirement for NLP systems. While many existing methods dopoorly at learning from a handful of examples, large pretrained language modelshave recently been shown to be efficient few-shot learners. One approach tofew-shot learning, which does not require finetuning of model parameters, is toaugment the language model's input with priming text which is typicallyconstructed using task specific descriptions and examples. In this work, wefurther explore priming-based few-shot learning, with focus on using examplesas prompts. We show that presenting examples in the right order is key forgeneralization. We introduce PERO (Prompting with Examples in the Right Order),where we formulate few-shot learning as search over the set of permutations ofthe training examples. We show that PERO can learn to generalize efficientlyusing as few as 10 examples, in contrast to existing approaches. While thenewline token is a natural choice for separating the examples in the prompt, weshow that learning a new separator token can potentially provide further gainsin performance. We demonstrate the effectiveness of the proposed method on thetasks of sentiment classification, natural language inference and factretrieval. Finally, we analyze the learned prompts to reveal novel insights,including the idea that two training examples in the right order alone canprovide competitive performance for sentiment classification and naturallanguage inference."
Transductive Few-Shot Learning: Clustering is All You Need?,"['Imtiaz Masud Ziko', 'Malik Boudiaf', 'Jose Dolz', 'Eric Granger', 'Ismail Ben Ayed']",http://arxiv.org/pdf/2106.09516v1.pdf,2021-06-16,"['cs.lg', 'cs.cv']","  We investigate a general formulation for clustering and transductive few-shotlearning, which integrates prototype-based objectives, Laplacian regularizationand supervision constraints from a few labeled data points. We propose aconcave-convex relaxation of the problem, and derive a computationallyefficient block-coordinate bound optimizer, with convergence guarantee. At eachiteration,our optimizer computes independent (parallel) updates for eachpoint-to-cluster assignment. Therefore, it could be trivially distributed forlarge-scale clustering and few-shot tasks. Furthermore, we provides a thoroughconvergence analysis based on point-to-set maps. Were port comprehensiveclustering and few-shot learning experiments over various data sets, showingthat our method yields competitive performances, in term of accuracy andoptimization quality, while scaling up to large problems. Using standardtraining on the base classes, without resorting to complex meta-learning andepisodic-training strategies, our approach outperforms state-of-the-artfew-shot methods by significant margins, across various models, settings anddata sets. Surprisingly, we found that even standard clustering procedures(e.g., K-means), which correspond to particular, non-regularized cases of ourgeneral model, already achieve competitive performances in comparison to thestate-of-the-art in few-shot learning. These surprising results point to thelimitations of the current few-shot benchmarks, and question the viability of alarge body of convoluted few-shot learning techniques in the recent literature."
Domain Agnostic Few-Shot Learning For Document Intelligence,"['Jaya Krishna Mandivarapu', 'Eric bunch', 'Glenn fung']",http://arxiv.org/pdf/2111.00007v1.pdf,2021-10-29,"['cs.cv', 'cs.lg']","  Few-shot learning aims to generalize to novel classes with only a few sampleswith class labels. Research in few-shot learning has borrowed techniques fromtransfer learning, metric learning, meta-learning, and Bayesian methods. Thesemethods also aim to train models from limited training samples, and whileencouraging performance has been achieved, they often fail to generalize tonovel domains. Many of the existing meta-learning methods rely on training datafor which the base classes are sampled from the same domain as the novelclasses used for meta-testing. However, in many applications in the industry,such as document classification, collecting large samples of data formeta-learning is infeasible or impossible. While research in the field of thecross-domain few-shot learning exists, it is mostly limited to computer vision.To our knowledge, no work yet exists that examines the use of few-shot learningfor classification of semi-structured documents (scans of paper documents)generated as part of a business workflow (forms, letters, bills, etc.). Herethe domain shift is significant, going from natural images to thesemi-structured documents of interest. In this work, we address the problem offew-shot document image classification under domain shift. We evaluate our workby extensive comparisons with existing methods. Experimental resultsdemonstrate that the proposed method shows consistent improvements on thefew-shot classification performance under domain shift."
CLUES: Few-Shot Learning Evaluation in Natural Language Understanding,"['Subhabrata Mukherjee', 'Xiaodong Liu', 'Guoqing Zheng', 'Saghar Hosseini', 'Hao Cheng', 'Greg Yang', 'Christopher Meek', 'Ahmed Hassan Awadallah', 'Jianfeng Gao']",http://arxiv.org/pdf/2111.02570v1.pdf,2021-11-04,"['cs.cl', 'cs.lg']","  Most recent progress in natural language understanding (NLU) has been driven,in part, by benchmarks such as GLUE, SuperGLUE, SQuAD, etc. In fact, many NLUmodels have now matched or exceeded ""human-level"" performance on many tasks inthese benchmarks. Most of these benchmarks, however, give models access torelatively large amounts of labeled data for training. As such, the models areprovided far more data than required by humans to achieve strong performance.That has motivated a line of work that focuses on improving few-shot learningperformance of NLU models. However, there is a lack of standardized evaluationbenchmarks for few-shot NLU resulting in different experimental settings indifferent papers. To help accelerate this line of work, we introduce CLUES(Constrained Language Understanding Evaluation Standard), a benchmark forevaluating the few-shot learning capabilities of NLU models. We demonstratethat while recent models reach human performance when they have access to largeamounts of labeled data, there is a huge gap in performance in the few-shotsetting for most tasks. We also demonstrate differences between alternativemodel families and adaptation techniques in the few shot setting. Finally, wediscuss several principles and choices in designing the experimental settingsfor evaluating the true few-shot learning performance and suggest a unifiedstandardized approach to few-shot learning evaluation. We aim to encourageresearch on NLU models that can generalize to new tasks with a small number ofexamples. Code and data for CLUES are available athttps://github.com/microsoft/CLUES."
Global Convergence of MAML and Theory-Inspired Neural Architecture  Search for Few-Shot Learning,"['Haoxiang Wang', 'Yite Wang', 'Ruoyu Sun', 'Bo Li']",http://arxiv.org/pdf/2203.09137v1.pdf,2022-03-17,"['cs.lg', 'cs.cv', 'stat.ml']","  Model-agnostic meta-learning (MAML) and its variants have become popularapproaches for few-shot learning. However, due to the non-convexity of deepneural nets (DNNs) and the bi-level formulation of MAML, the theoreticalproperties of MAML with DNNs remain largely unknown. In this paper, we firstprove that MAML with over-parameterized DNNs is guaranteed to converge toglobal optima at a linear rate. Our convergence analysis indicates that MAMLwith over-parameterized DNNs is equivalent to kernel regression with a novelclass of kernels, which we name as Meta Neural Tangent Kernels (MetaNTK). Then,we propose MetaNTK-NAS, a new training-free neural architecture search (NAS)method for few-shot learning that uses MetaNTK to rank and selectarchitectures. Empirically, we compare our MetaNTK-NAS with previous NASmethods on two popular few-shot learning benchmarks, miniImageNet, andtieredImageNet. We show that the performance of MetaNTK-NAS is comparable orbetter than the state-of-the-art NAS method designed for few-shot learningwhile enjoying more than 100x speedup. We believe the efficiency of MetaNTK-NASmakes itself more practical for many real-world tasks."
Meta-ticket: Finding optimal subnetworks for few-shot learning within  randomly initialized neural networks,"['Daiki Chijiwa', ""Shin'ya Yamaguchi"", 'Atsutoshi Kumagai', 'Yasutoshi Ida']",http://arxiv.org/pdf/2205.15619v2.pdf,2022-05-31,"['cs.lg', 'cs.ai', 'cs.ne', 'stat.ml']","  Few-shot learning for neural networks (NNs) is an important problem that aimsto train NNs with a few data. The main challenge is how to avoid overfittingsince over-parameterized NNs can easily overfit to such small dataset. Previouswork (e.g. MAML by Finn et al. 2017) tackles this challenge by meta-learning,which learns how to learn from a few data by using various tasks. On the otherhand, one conventional approach to avoid overfitting is restricting hypothesisspaces by endowing sparse NN structures like convolution layers in computervision. However, although such manually-designed sparse structures aresample-efficient for sufficiently large datasets, they are still insufficientfor few-shot learning. Then the following questions naturally arise: (1) Can wefind sparse structures effective for few-shot learning by meta-learning? (2)What benefits will it bring in terms of meta-generalization? In this work, wepropose a novel meta-learning approach, called Meta-ticket, to find optimalsparse subnetworks for few-shot learning within randomly initialized NNs. Weempirically validated that Meta-ticket successfully discover sparse subnetworksthat can learn specialized features for each given task. Due to this task-wiseadaptation ability, Meta-ticket achieves superior meta-generalization comparedto MAML-based methods especially with large NNs. The code is available at:https://github.com/dchiji-ntt/meta-ticket"
Few-Shot Learning by Dimensionality Reduction in Gradient Space,"['Martin Gauch', 'Maximilian Beck', 'Thomas Adler', 'Dmytro Kotsur', 'Stefan Fiel', 'Hamid Eghbal-zadeh', 'Johannes Brandstetter', 'Johannes Kofler', 'Markus Holzleitner', 'Werner Zellinger', 'Daniel Klotz', 'Sepp Hochreiter', 'Sebastian Lehner']",http://arxiv.org/pdf/2206.03483v1.pdf,2022-06-07,['cs.lg'],"  We introduce SubGD, a novel few-shot learning method which is based on therecent finding that stochastic gradient descent updates tend to live in alow-dimensional parameter subspace. In experimental and theoretical analyses,we show that models confined to a suitable predefined subspace generalize wellfor few-shot learning. A suitable subspace fulfills three criteria across thegiven tasks: it (a) allows to reduce the training error by gradient flow, (b)leads to models that generalize well, and (c) can be identified by stochasticgradient descent. SubGD identifies these subspaces from an eigendecompositionof the auto-correlation matrix of update directions across different tasks.Demonstrably, we can identify low-dimensional suitable subspaces for few-shotlearning of dynamical systems, which have varying properties described by oneor few parameters of the analytical system description. Such systems areubiquitous among real-world applications in science and engineering. Weexperimentally corroborate the advantages of SubGD on three distinct dynamicalsystems problem settings, significantly outperforming popular few-shot learningmethods both in terms of sample efficiency and performance."
Fast Learning of Dynamic Hand Gesture Recognition with Few-Shot Learning  Models,"['Niels Schlüsener', 'Michael Bücker']",http://arxiv.org/pdf/2212.08363v1.pdf,2022-12-16,"['cs.cv', 'cs.ai']","  We develop Few-Shot Learning models trained to recognize five or tendifferent dynamic hand gestures, respectively, which are arbitrarilyinterchangeable by providing the model with one, two, or five examples per handgesture. All models were built in the Few-Shot Learning architecture of theRelation Network (RN), in which Long-Short-Term Memory cells form the backbone.The models use hand reference points extracted from RGB-video sequences of theJester dataset which was modified to contain 190 different types of handgestures. Result show accuracy of up to 88.8% for recognition of five and up to81.2% for ten dynamic hand gestures. The research also sheds light on thepotential effort savings of using a Few-Shot Learning approach instead of atraditional Deep Learning approach to detect dynamic hand gestures. Savingswere defined as the number of additional observations required when a DeepLearning model is trained on new hand gestures instead of a Few Shot Learningmodel. The difference with respect to the total number of observations requiredto achieve approximately the same accuracy indicates potential savings of up to630 observations for five and up to 1260 observations for ten hand gestures tobe recognized. Since labeling video recordings of hand gestures impliessignificant effort, these savings can be considered substantial."
Few Shot Learning for Medical Imaging: A Comparative Analysis of  Methodologies and Formal Mathematical Framework,"['Jannatul Nayem', 'Sayed Sahriar Hasan', 'Noshin Amina', 'Bristy Das', 'Md Shahin Ali', 'Md Manjurul Ahsan', 'Shivakumar Raman']",http://arxiv.org/pdf/2305.04401v2.pdf,2023-05-08,"['eess.iv', 'cs.cv']","  Deep learning becomes an elevated context regarding disposing of many machinelearning tasks and has shown a breakthrough upliftment to extract features fromunstructured data. Though this flourishing context is developing in the medicalimage processing sector, scarcity of problem-dependent training data has becomea larger issue in the way of easy application of deep learning in the medicalsector. To unravel the confined data source, researchers have developed a modelthat can solve machine learning problems with fewer data called ``Few shotlearning"". Few hot learning algorithms determine to solve the data limitationproblems by extracting the characteristics from a small dataset throughclassification and segmentation methods. In the medical sector, there isfrequently a shortage of available datasets in respect of some confidentialdiseases. Therefore, Few shot learning gets the limelight in this data scarcitysector. In this chapter, the background and basic overview of a few shots oflearning is represented. Henceforth, the classification of few-shot learning isdescribed also. Even the paper shows a comparison of methodological approachesthat are applied in medical image analysis over time. The current advancementin the implementation of few-shot learning concerning medical imaging isillustrated. The future scope of this domain in the medical imaging sector isfurther described."
FHIST: A Benchmark for Few-shot Classification of Histological Images,"['Fereshteh Shakeri', 'Malik Boudiaf', 'Sina Mohammadi', 'Ivaxi Sheth', 'Mohammad Havaei', 'Ismail Ben Ayed', 'Samira Ebrahimi Kahou']",http://arxiv.org/pdf/2206.00092v1.pdf,2022-05-31,['cs.cv'],"  Few-shot learning has recently attracted wide interest in imageclassification, but almost all the current public benchmarks are focused onnatural images. The few-shot paradigm is highly relevant in medical-imagingapplications due to the scarcity of labeled data, as annotations are expensiveand require specialized expertise. However, in medical imaging, few-shotlearning research is sparse, limited to private data sets and is at its earlystage. In particular, the few-shot setting is of high interest in histology dueto the diversity and fine granularity of cancer related tissue classificationtasks, and the variety of data-preparation techniques. This paper introduces ahighly diversified public benchmark, gathered from various public datasets, forfew-shot histology data classification. We build few-shot tasks andbase-training data with various tissue types, different levels of domain shiftsstemming from various cancer sites, and different class-granularity levels,thereby reflecting realistic scenarios. We evaluate the performances ofstate-of-the-art few-shot learning methods on our benchmark, and observe thatsimple fine-tuning and regularization methods achieve better results than thepopular meta-learning and episodic-training paradigm. Furthermore, we introducethree scenarios based on the domain shifts between the source and targethistology data: near-domain, middle-domain and out-domain. Our experimentsdisplay the potential of few-shot learning in histology classification, withstate-of-art few shot learning methods approaching the supervised-learningbaselines in the near-domain setting. In our out-domain setting, for 5-way5-shot, the best performing method reaches 60% accuracy. We believe that ourwork could help in building realistic evaluations and fair comparisons offew-shot learning methods and will further encourage research in the few-shotparadigm."
Interpretable Few-Shot Learning via Linear Distillation,"['Arip Asadulaev', 'Igor Kuznetsov', 'Andrey Filchenkov']",http://arxiv.org/pdf/1906.05431v2.pdf,2019-06-13,"['cs.lg', 'stat.ml']","  It is important to develop mathematically tractable models than can interpretknowledge extracted from the data and provide reasonable predictions. In thispaper, we present a Linear Distillation Learning, a simple remedy to improvethe performance of linear neural networks. Our approach is based on using alinear function for each class in a dataset, which is trained to simulate theoutput of a teacher linear network for each class separately. We tested ourmodel on MNIST and Omniglot datasets in the Few-Shot learning manner. It showedbetter results than other interpretable models such as classical LogisticRegression."
Optimization of Image Embeddings for Few Shot Learning,"['Arvind Srinivasan', 'Aprameya Bharadwaj', 'Manasa Sathyan', 'S Natarajan']",http://arxiv.org/pdf/2004.02034v1.pdf,2020-04-04,"['cs.cv', 'cs.lg']","  In this paper we improve the image embeddings generated in the graph neuralnetwork solution for few shot learning. We propose alternate architectures forexisting networks such as Inception-Net, U-Net, Attention U-Net, andSqueeze-Net to generate embeddings and increase the accuracy of the models. Weimprove the quality of embeddings created at the cost of the time taken togenerate them. The proposed implementations outperform the existing state ofthe art methods for 1-shot and 5-shot learning on the Omniglot dataset. Theexperiments involved a testing set and training set which had no common classesbetween them. The results for 5-way and 10-way/20-way tests have beentabulated."
Ensemble Model with Batch Spectral Regularization and Data Blending for  Cross-Domain Few-Shot Learning with Unlabeled Data,"['Zhen Zhao', 'Bingyu Liu', 'Yuhong Guo', 'Jieping Ye']",http://arxiv.org/pdf/2006.04323v2.pdf,2020-06-08,['cs.cv'],"  In this paper, we present our proposed ensemble model with batch spectralregularization and data blending mechanisms for the Track 2 problem of thecross-domain few-shot learning (CD-FSL) challenge. We build a multi-branchensemble framework by using diverse feature transformation matrices, whiledeploying batch spectral feature regularization on each branch to improve themodel's transferability. Moreover, we propose a data blending method to exploitthe unlabeled data and augment the sparse support set in the target domain. Ourproposed model demonstrates effective performance on the CD-FSL benchmarktasks."
Few-shot Learning for Slot Tagging with Attentive Relational Network,"['Cennet Oguz', 'Ngoc Thang Vu']",http://arxiv.org/pdf/2103.02333v1.pdf,2021-03-03,['cs.cl'],"  Metric-based learning is a well-known family of methods for few-shotlearning, especially in computer vision. Recently, they have been used in manynatural language processing applications but not for slot tagging. In thispaper, we explore metric-based learning methods in the slot tagging task andpropose a novel metric-based learning architecture - Attentive RelationalNetwork. Our proposed method extends relation networks, making them moresuitable for natural language processing applications in general, by leveragingpretrained contextual embeddings such as ELMO and BERT and by using attentionmechanism. The results on SNIPS data show that our proposed method outperformsother state-of-the-art metric-based learning methods."
Explore the Power of Dropout on Few-shot Learning,"['Shaobo Lin', 'Xingyu Zeng', 'Rui Zhao']",http://arxiv.org/pdf/2301.11015v1.pdf,2023-01-26,['cs.cv'],"  The generalization power of the pre-trained model is the key for few-shotdeep learning. Dropout is a regularization technique used in traditional deeplearning methods. In this paper, we explore the power of dropout on few-shotlearning and provide some insights about how to use it. Extensive experimentson the few-shot object detection and few-shot image classification datasets,i.e., Pascal VOC, MS COCO, CUB, and mini-ImageNet, validate the effectivenessof our method."
Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions,"['Han-Jia Ye', 'Hexiang Hu', 'De-Chuan Zhan', 'Fei Sha']",http://arxiv.org/pdf/1812.03664v6.pdf,2018-12-10,"['cs.lg', 'cs.cv']","  Learning with limited data is a key challenge for visual recognition. Manyfew-shot learning methods address this challenge by learning an instanceembedding function from seen classes and apply the function to instances fromunseen classes with limited labels. This style of transfer learning istask-agnostic: the embedding function is not learned optimally discriminativewith respect to the unseen classes, where discerning among them leads to thetarget task. In this paper, we propose a novel approach to adapt the instanceembeddings to the target classification task with a set-to-set function,yielding embeddings that are task-specific and are discriminative. Weempirically investigated various instantiations of such set-to-set functionsand observed the Transformer is most effective -- as it naturally satisfies keyproperties of our desired model. We denote this model as FEAT (few-shotembedding adaptation w/ Transformer) and validate it on both the standardfew-shot classification benchmark and four extended few-shot learning settingswith essential use cases, i.e., cross-domain, transductive, generalizedfew-shot learning, and low-shot learning. It archived consistent improvementsover baseline models as well as previous methods and established the newstate-of-the-art results on two benchmarks."
Semi-Supervised Few-Shot Learning with Prototypical Random Walks,"['Ahmed Ayyad', 'Yuchen Li', 'Nassir Navab', 'Shadi Albarqouni', 'Mohamed Elhoseiny']",http://arxiv.org/pdf/1903.02164v3.pdf,2019-03-06,"['cs.lg', 'stat.ml']","  Recent progress has shown that few-shot learning can be improved with accessto unlabelled data, known as semi-supervised few-shot learning(SS-FSL). Weintroduce an SS-FSL approach, dubbed as Prototypical Random WalkNetworks(PRWN), built on top of Prototypical Networks (PN). We develop a randomwalk semi-supervised loss that enables the network to learn representationsthat are compact and well-separated. Our work is related to the very recentdevelopment of graph-based approaches for few-shot learning. However, we showthat compact and well-separated class representations can be achieved bymodeling our prototypical random walk notion without needing additionalgraph-NN parameters or requiring a transductive setting where a collective testset is provided. Our model outperforms baselines in most benchmarks withsignificant improvements in some cases. Our model, trained with 40$\%$ of thedata as labeled, compares competitively against fully supervised prototypicalnetworks, trained on 100$\%$ of the labels, even outperforming it in the 1-shotmini-Imagenet case with 50.89$\%$ to 49.4$\%$ accuracy. We also show that ourloss is resistant to distractors, unlabeled data that does not belong to any ofthe training classes, and hence reflecting robustness to labeled/unlabeledclass distribution mismatch. Associated GitHub page can be found athttps://prototypical-random-walk.github.io."
Meta-GNN: On Few-shot Node Classification in Graph Meta-learning,"['Fan Zhou', 'Chengtai Cao', 'Kunpeng Zhang', 'Goce Trajcevski', 'Ting Zhong', 'Ji Geng']",http://arxiv.org/pdf/1905.09718v1.pdf,2019-05-23,"['cs.lg', 'stat.ml']","  Meta-learning has received a tremendous recent attention as a possibleapproach for mimicking human intelligence, i.e., acquiring new knowledge andskills with little or even no demonstration. Most of the existing meta-learningmethods are proposed to tackle few-shot learning problems such as image andtext, in rather Euclidean domain. However, there are very few works applyingmeta-learning to non-Euclidean domains, and the recently proposed graph neuralnetworks (GNNs) models do not perform effectively on graph few-shot learningproblems. Towards this, we propose a novel graph meta-learning framework --Meta-GNN -- to tackle the few-shot node classification problem in graphmeta-learning settings. It obtains the prior knowledge of classifiers bytraining on many similar few-shot learning tasks and then classifies the nodesfrom new classes with only few labeled samples. Additionally, Meta-GNN is ageneral model that can be straightforwardly incorporated into any existingstate-of-the-art GNN. Our experiments conducted on three benchmark datasetsdemonstrate that our proposed approach not only improves the nodeclassification performance by a large margin on few-shot learning problems inmeta-learning paradigm, but also learns a more general and flexible model fortask adaption."
Learning Adaptive Classifiers Synthesis for Generalized Few-Shot  Learning,"['Han-Jia Ye', 'Hexiang Hu', 'De-Chuan Zhan']",http://arxiv.org/pdf/1906.02944v5.pdf,2019-06-07,"['cs.cv', 'cs.lg']","  Object recognition in the real-world requires handling long-tailed or evenopen-ended data. An ideal visual system needs to recognize the populated headvisual concepts reliably and meanwhile efficiently learn about emerging newtail categories with a few training instances. Class-balanced many-shotlearning and few-shot learning tackle one side of this problem, by eitherlearning strong classifiers for head or learning to learn few-shot classifiersfor the tail. In this paper, we investigate the problem of generalized few-shotlearning (GFSL) -- a model during the deployment is required to learn abouttail categories with few shots and simultaneously classify the head classes. Wepropose the ClAssifier SynThesis LEarning (CASTLE), a learning framework thatlearns how to synthesize calibrated few-shot classifiers in addition to themulti-class classifiers of head classes with a shared neural dictionary,shedding light upon the inductive GFSL. Furthermore, we propose an adaptiveversion of CASTLE (ACASTLE) that adapts the head classifiers conditioned on theincoming tail training examples, yielding a framework that allows effectivebackward knowledge transfer. As a consequence, ACASTLE can handle GFSL withclasses from heterogeneous domains effectively. CASTLE and ACASTLE demonstratesuperior performances than existing GFSL algorithms and strong baselines onMiniImageNet as well as TieredImageNet datasets. More interestingly, theyoutperform previous state-of-the-art methods when evaluated with standardfew-shot learning criteria."
Progressive Cluster Purification for Transductive Few-shot Learning,"['Chenyang Si', 'Wentao Chen', 'Wei Wang', 'Liang Wang', 'Tieniu Tan']",http://arxiv.org/pdf/1906.03847v1.pdf,2019-06-10,['cs.cv'],"  Few-shot learning aims to learn to generalize a classifier to novel classeswith limited labeled data. Transductive inference that utilizes unlabeled testset to deal with low-data problem has been employed for few-shot learning inrecent literature. Yet, these methods do not explicitly exploit the manifoldstructures of semantic clusters, which is inefficient for transductiveinference. In this paper, we propose a novel Progressive Cluster Purification(PCP) method for transductive few-shot learning. The PCP can progressivelypurify the cluster by exploring the semantic interdependency in the individualcluster space. Specifically, the PCP consists of two-level operations:inter-class classification and intra-class transduction. The inter-classclassification partitions all the test samples into several clusters bycomparing the test samples with the prototypes. The intra-class transductioneffectively explores trustworthy test samples for each cluster by modeling datarelations within a cluster as well as among different clusters. Then, itrefines the prototypes to better represent the real distribution of semanticclusters. The refined prototypes are used to remeasure all the test instancesand purify each cluster. Furthermore, the inter-class classification and theintra-class transduction are extremely flexible to be repeated several times toprogressively purify the clusters. Experimental results are provided on twodatasets: miniImageNet dataset and tieredImageNet dataset. The comparisonresults demonstrate the effectiveness of our approach and show that ourapproach outperforms the state-of-the-art methods on both datasets."
Diversity Transfer Network for Few-Shot Learning,"['Mengting Chen', 'Yuxin Fang', 'Xinggang Wang', 'Heng Luo', 'Yifeng Geng', 'Xinyu Zhang', 'Chang Huang', 'Wenyu Liu', 'Bo Wang']",http://arxiv.org/pdf/1912.13182v1.pdf,2019-12-31,['cs.cv'],"  Few-shot learning is a challenging task that aims at training a classifierfor unseen classes with only a few training examples. The main difficulty offew-shot learning lies in the lack of intra-class diversity within insufficienttraining samples. To alleviate this problem, we propose a novel generativeframework, Diversity Transfer Network (DTN), that learns to transfer latentdiversities from known categories and composite them with support features togenerate diverse samples for novel categories in feature space. The learningproblem of the sample generation (i.e., diversity transfer) is solved viaminimizing an effective meta-classification loss in a single-stage network,instead of the generative loss in previous works.  Besides, an organized auxiliary task co-training over known categories isproposed to stabilize the meta-training process of DTN. We perform extensiveexperiments and ablation studies on three datasets, i.e., \emph{mini}ImageNet,CIFAR100 and CUB. The results show that DTN, with single-stage training andfaster convergence speed, obtains the state-of-the-art results among thefeature generation based few-shot learning methods. Code and supplementarymaterial are available at: \texttt{https://github.com/Yuxin-CV/DTN}"
MetaConcept: Learn to Abstract via Concept Graph for Weakly-Supervised  Few-Shot Learning,"['Baoquan Zhang', 'Ka-Cheong Leung', 'Yunming Ye', 'Xutao Li']",http://arxiv.org/pdf/2007.02379v2.pdf,2020-07-05,['cs.cv'],"  Meta-learning has been proved to be an effective framework to addressfew-shot learning problems. The key challenge is how to minimize thegeneralization error of base learner across tasks. In this paper, we explorethe concept hierarchy knowledge by leveraging concept graph, and take theconcept graph as explicit meta-knowledge for the base learner, instead oflearning implicit meta-knowledge, so as to boost the classification performanceof meta-learning on weakly-supervised few-shot learning problems. To this end,we propose a novel meta-learning framework, called MetaConcept, which learns toabstract concepts via the concept graph. Specifically, we firstly propose anovel regularization with multi-level conceptual abstraction to constrain ameta-learner to learn to abstract concepts via the concept graph (i.e.identifying the concepts from low to high levels). Then, we propose a metaconcept inference network as the meta-learner for the base learner, aiming toquickly adapt to a novel task by the joint inference of the abstract conceptsand a few annotated samples. We have conducted extensive experiments on twoweakly-supervised few-shot learning benchmarks, namely, WS-ImageNet-Pure andWS-ImageNet-Mix. Our experimental results show that 1) the proposed MetaConceptoutperforms state-of-the-art methods with an improvement of 2% to 6% inclassification accuracy; 2) the proposed MetaConcept can be able to yield agood performance though merely training with weakly-labeled data sets."
Defensive Few-shot Learning,"['Wenbin Li', 'Lei Wang', 'Xingxing Zhang', 'Lei Qi', 'Jing Huo', 'Yang Gao', 'Jiebo Luo']",http://arxiv.org/pdf/1911.06968v2.pdf,2019-11-16,['cs.cv'],"  This paper investigates a new challenging problem called defensive few-shotlearning in order to learn a robust few-shot model against adversarial attacks.Simply applying the existing adversarial defense methods to few-shot learningcannot effectively solve this problem. This is because the commonly assumedsample-level distribution consistency between the training and test sets can nolonger be met in the few-shot setting. To address this situation, we develop ageneral defensive few-shot learning (DFSL) framework to answer the followingtwo key questions: (1) how to transfer adversarial defense knowledge from onesample distribution to another? (2) how to narrow the distribution gap betweenclean and adversarial examples under the few-shot setting? To answer the firstquestion, we propose an episode-based adversarial training mechanism byassuming a task-level distribution consistency to better transfer theadversarial defense knowledge. As for the second question, within each few-shottask, we design two kinds of distribution consistency criteria to narrow thedistribution gap between clean and adversarial examples from the feature-wiseand prediction-wise perspectives, respectively. Extensive experimentsdemonstrate that the proposed framework can effectively make the existingfew-shot models robust against adversarial attacks. Code is available athttps://github.com/WenbinLee/DefensiveFSL.git."
Improving Few-shot Learning by Spatially-aware Matching and  CrossTransformer,"['Hongguang Zhang', 'Philip H. S. Torr', 'Piotr Koniusz']",http://arxiv.org/pdf/2001.01600v2.pdf,2020-01-06,['cs.cv'],"  Current few-shot learning models capture visual object relations in theso-called meta-learning setting under a fixed-resolution input. However, suchmodels have a limited generalization ability under the scale and locationmismatch between objects, as only few samples from target classes are provided.Therefore, the lack of a mechanism to match the scale and location betweenpairs of compared images leads to the performance degradation. The importanceof image contents varies across coarse-to-fine scales depending on the objectand its class label, e.g., generic objects and scenes rely on their globalappearance while fine-grained objects rely more on their localized visualpatterns. In this paper, we study the impact of scale and location mismatch inthe few-shot learning scenario, and propose a novel Spatially-aware Matching(SM) scheme to effectively perform matching across multiple scales andlocations, and learn image relations by giving the highest weights to the bestmatching pairs. The SM is trained to activate the most related locations andscales between support and query data. We apply and evaluate SM on variousfew-shot learning models and backbones for comprehensive evaluations.Furthermore, we leverage an auxiliary self-supervisory discriminator totrain/predict the spatial- and scale-level index of feature vectors we use.Finally, we develop a novel transformer-based pipeline to exploit self- andcross-attention in a spatially-aware matching process. Our proposed design isorthogonal to the choice of backbone and/or comparator."
Rethinking Class Relations: Absolute-relative Supervised and  Unsupervised Few-shot Learning,"['Hongguang Zhang', 'Piotr Koniusz', 'Songlei Jian', 'Hongdong Li', 'Philip H. S. Torr']",http://arxiv.org/pdf/2001.03919v4.pdf,2020-01-12,['cs.cv'],"  The majority of existing few-shot learning methods describe image relationswith binary labels. However, such binary relations are insufficient to teachthe network complicated real-world relations, due to the lack of decisionsmoothness. Furthermore, current few-shot learning models capture only thesimilarity via relation labels, but they are not exposed to class conceptsassociated with objects, which is likely detrimental to the classificationperformance due to underutilization of the available class labels. Toparaphrase, children learn the concept of tiger from a few of actual examplesas well as from comparisons of tiger to other animals. Thus, we hypothesizethat in fact both similarity and class concept learning must be occurringsimultaneously. With these observations at hand, we study the fundamentalproblem of simplistic class modeling in current few-shot learning methods. Werethink the relations between class concepts, and propose a novelAbsolute-relative Learning paradigm to fully take advantage of labelinformation to refine the image representations and correct the relationunderstanding in both supervised and unsupervised scenarios. Our proposedparadigm improves the performance of several the state-of-the-art models onpublicly available datasets."
Asymmetric Distribution Measure for Few-shot Learning,"['Wenbin Li', 'Lei Wang', 'Jing Huo', 'Yinghuan Shi', 'Yang Gao', 'Jiebo Luo']",http://arxiv.org/pdf/2002.00153v1.pdf,2020-02-01,['cs.cv'],"  The core idea of metric-based few-shot image classification is to directlymeasure the relations between query images and support classes to learntransferable feature embeddings. Previous work mainly focuses on image-levelfeature representations, which actually cannot effectively estimate a class'sdistribution due to the scarcity of samples. Some recent work shows that localdescriptor based representations can achieve richer representations thanimage-level based representations. However, such works are still based on aless effective instance-level metric, especially a symmetric metric, to measurethe relations between query images and support classes. Given the naturalasymmetric relation between a query image and a support class, we argue that anasymmetric measure is more suitable for metric-based few-shot learning. To thatend, we propose a novel Asymmetric Distribution Measure (ADM) network forfew-shot learning by calculating a joint local and global asymmetric measurebetween two multivariate local distributions of queries and classes. Moreover,a task-aware Contrastive Measure Strategy (CMS) is proposed to further enhancethe measure function. On popular miniImageNet and tieredImageNet, we achieve$3.02\%$ and $1.56\%$ gains over the state-of-the-art method on the $5$-way$1$-shot task, respectively, validating our innovative design of asymmetricdistribution measures for few-shot learning."
Meta-Learned Confidence for Few-shot Learning,"['Seong Min Kye', 'Hae Beom Lee', 'Hoirin Kim', 'Sung Ju Hwang']",http://arxiv.org/pdf/2002.12017v2.pdf,2020-02-27,"['cs.lg', 'cs.cv', 'stat.ml']","  Transductive inference is an effective means of tackling the data deficiencyproblem in few-shot learning settings. A popular transductive inferencetechnique for few-shot metric-based approaches, is to update the prototype ofeach class with the mean of the most confident query examples, orconfidence-weighted average of all the query samples. However, a caveat here isthat the model confidence may be unreliable, which may lead to incorrectpredictions. To tackle this issue, we propose to meta-learn the confidence foreach query sample, to assign optimal weights to unlabeled queries such thatthey improve the model's transductive inference performance on unseen tasks. Weachieve this by meta-learning an input-adaptive distance metric over a taskdistribution under various model and data perturbations, which will enforceconsistency on the model predictions under diverse uncertainties for unseentasks. Moreover, we additionally suggest a regularization which explicitlyenforces the consistency on the predictions across the different dimensions ofa high-dimensional embedding vector. We validate our few-shot learning modelwith meta-learned confidence on four benchmark datasets, on which it largelyoutperforms strong recent baselines and obtains new state-of-the-art results.Further application on semi-supervised few-shot learning tasks also yieldssignificant performance improvements over the baselines. The source code of ouralgorithm is available at https://github.com/seongmin-kye/MCT."
Instance Credibility Inference for Few-Shot Learning,"['Yikai Wang', 'Chengming Xu', 'Chen Liu', 'Li Zhang', 'Yanwei Fu']",http://arxiv.org/pdf/2003.11853v2.pdf,2020-03-26,"['cs.cv', 'cs.lg']","  Few-shot learning (FSL) aims to recognize new objects with extremely limitedtraining data for each category. Previous efforts are made by either leveragingmeta-learning paradigm or novel principles in data augmentation to alleviatethis extremely data-scarce problem. In contrast, this paper presents a simplestatistical approach, dubbed Instance Credibility Inference (ICI) to exploitthe distribution support of unlabeled instances for few-shot learning.Specifically, we first train a linear classifier with the labeled few-shotexamples and use it to infer the pseudo-labels for the unlabeled data. Tomeasure the credibility of each pseudo-labeled instance, we then propose tosolve another linear regression hypothesis by increasing the sparsity of theincidental parameters and rank the pseudo-labeled instances with their sparsitydegree. We select the most trustworthy pseudo-labeled instances alongside thelabeled examples to re-train the linear classifier. This process is iterateduntil all the unlabeled samples are included in the expanded training set, i.e.the pseudo-label is converged for unlabeled data pool. Extensive experimentsunder two few-shot settings show that our simple approach can establish newstate-of-the-arts on four widely used few-shot learning benchmark datasetsincluding miniImageNet, tieredImageNet, CIFAR-FS, and CUB. Our code isavailable at: https://github.com/Yikai-Wang/ICI-FSL"
FewJoint: A Few-shot Learning Benchmark for Joint Language Understanding,"['Yutai Hou', 'Jiafeng Mao', 'Yongkui Lai', 'Cheng Chen', 'Wanxiang Che', 'Zhigang Chen', 'Ting Liu']",http://arxiv.org/pdf/2009.08138v3.pdf,2020-09-17,"['cs.cl', 'cs.ai']","  Few-shot learning (FSL) is one of the key future steps in machine learningand has raised a lot of attention. However, in contrast to the rapiddevelopment in other domains, such as Computer Vision, the progress of FSL inNature Language Processing (NLP) is much slower. One of the key reasons forthis is the lacking of public benchmarks. NLP FSL researches always report newresults on their own constructed few-shot datasets, which is pretty inefficientin results comparison and thus impedes cumulative progress. In this paper, wepresent FewJoint, a novel Few-Shot Learning benchmark for NLP. Different frommost NLP FSL research that only focus on simple N-classification problems, ourbenchmark introduces few-shot joint dialogue language understanding, whichadditionally covers the structure prediction and multi-task reliance problems.This allows our benchmark to reflect the real-word NLP complexity beyond simpleN-classification. Our benchmark is used in the few-shot learning contest ofSMP2020-ECDT task-1. We also provide a compatible FSL platform to easeexperiment set-up."
SB-MTL: Score-based Meta Transfer-Learning for Cross-Domain Few-Shot  Learning,"['John Cai', 'Bill Cai', 'Sheng Mei Shen']",http://arxiv.org/pdf/2012.01784v1.pdf,2020-12-03,"['cs.cv', 'cs.ai', 'cs.lg']","  While many deep learning methods have seen significant success in tacklingthe problem of domain adaptation and few-shot learning separately, far fewermethods are able to jointly tackle both problems in Cross-Domain Few-ShotLearning (CD-FSL). This problem is exacerbated under sharp domain shifts thattypify common computer vision applications. In this paper, we present a novel,flexible and effective method to address the CD-FSL problem. Our method, calledScore-based Meta Transfer-Learning (SB-MTL), combines transfer-learning andmeta-learning by using a MAML-optimized feature encoder and a score-based GraphNeural Network. First, we have a feature encoder with specific layers designedto be fine-tuned. To do so, we apply a first-order MAML algorithm to find goodinitializations. Second, instead of directly taking the classification scoresafter fine-tuning, we interpret the scores as coordinates by mapping thepre-softmax classification scores onto a metric space. Subsequently, we apply aGraph Neural Network to propagate label information from the support set to thequery set in our score-based metric space. We test our model on the BroaderStudy of Cross-Domain Few-Shot Learning (BSCD-FSL) benchmark, which includes arange of target domains with highly varying dissimilarity to the miniImagenetsource domain. We observe significant improvements in accuracy across 5, 20 and50 shot, and on the four target domains. In terms of average accuracy, ourmodel outperforms previous transfer-learning methods by 5.93% and previousmeta-learning methods by 14.28%."
Are Fewer Labels Possible for Few-shot Learning?,"['Suichan Li', 'Dongdong Chen', 'Yinpeng Chen', 'Lu Yuan', 'Lei Zhang', 'Qi Chu', 'Nenghai Yu']",http://arxiv.org/pdf/2012.05899v1.pdf,2020-12-10,"['cs.cv', 'cs.lg']","  Few-shot learning is challenging due to its very limited data and labels.Recent studies in big transfer (BiT) show that few-shot learning can greatlybenefit from pretraining on large scale labeled dataset in a different domain.This paper asks a more challenging question: ""can we use as few as possiblelabels for few-shot learning in both pretraining (with no labels) andfine-tuning (with fewer labels)?"".  Our key insight is that the clustering of target samples in the feature spaceis all we need for few-shot finetuning. It explains why the vanillaunsupervised pretraining (poor clustering) is worse than the supervised one. Inthis paper, we propose transductive unsupervised pretraining that achieves abetter clustering by involving target data even though its amount is verylimited. The improved clustering result is of great value for identifying themost representative samples (""eigen-samples"") for users to label, and inreturn, continued finetuning with the labeled eigen-samples further improvesthe clustering. Thus, we propose eigen-finetuning to enable fewer shot learningby leveraging the co-evolution of clustering and eigen-samples in thefinetuning. We conduct experiments on 10 different few-shot target datasets,and our average few-shot performance outperforms both vanilla inductiveunsupervised transfer and supervised transfer by a large margin. For instance,when each target category only has 10 labeled samples, the mean accuracy gainover the above two baselines is 9.2% and 3.42 respectively."
COVID-19 detection from scarce chest x-ray image data using few-shot  deep learning approach,['Shruti Jadon'],http://arxiv.org/pdf/2102.06285v2.pdf,2021-02-11,"['eess.iv', 'cs.cv', 'cs.lg']","  In the current COVID-19 pandemic situation, there is an urgent need to screeninfected patients quickly and accurately. Using deep learning models trained onchest X-ray images can become an efficient method for screening COVID-19patients in these situations. Deep learning approaches are already widely usedin the medical community. However, they require a large amount of data to beaccurate. The open-source community collectively has made efforts to collectand annotate the data, but it is not enough to train an accurate deep learningmodel. Few-shot learning is a sub-field of machine learning that aims to learnthe objective with less amount of data. In this work, we have experimented withwell-known solutions for data scarcity in deep learning to detect COVID-19.These include data augmentation, transfer learning, and few-shot learning, andunsupervised learning. We have also proposed a custom few-shot learningapproach to detect COVID-19 using siamese networks. Our experimental resultsshowcased that we can implement an efficient and accurate deep learning modelfor COVID-19 detection by adopting the few-shot learning approaches even withless amount of data. Using our proposed approach we were able to achieve 96.4%accuracy an improvement from 83% using baseline models."
Learning Dynamic Alignment via Meta-filter for Few-shot Learning,"['Chengming Xu', 'Chen Liu', 'Li Zhang', 'Chengjie Wang', 'Jilin Li', 'Feiyue Huang', 'Xiangyang Xue', 'Yanwei Fu']",http://arxiv.org/pdf/2103.13582v1.pdf,2021-03-25,"['cs.cv', 'cs.ai']","  Few-shot learning (FSL), which aims to recognise new classes by adapting thelearned knowledge with extremely limited few-shot (support) examples, remainsan important open problem in computer vision. Most of the existing methods forfeature alignment in few-shot learning only consider image-level orspatial-level alignment while omitting the channel disparity. Our insight isthat these methods would lead to poor adaptation with redundant matching, andleveraging channel-wise adjustment is the key to well adapting the learnedknowledge to new classes. Therefore, in this paper, we propose to learn adynamic alignment, which can effectively highlight both query regions andchannels according to different local support information. Specifically, thisis achieved by first dynamically sampling the neighbourhood of the featureposition conditioned on the input few shot, based on which we further predict aboth position-dependent and channel-dependent Dynamic Meta-filter. The filteris used to align the query feature with position-specific and channel-specificknowledge. Moreover, we adopt Neural Ordinary Differential Equation (ODE) toenable a more accurate control of the alignment. In such a sense our model isable to better capture fine-grained semantic context of the few-shot exampleand thus facilitates dynamical knowledge adaptation for few-shot learning. Theresulting framework establishes the new state-of-the-arts on major few-shotvisual recognition benchmarks, including miniImageNet and tieredImageNet."
MetaKernel: Learning Variational Random Features with Limited Labels,"['Yingjun Du', 'Haoliang Sun', 'Xiantong Zhen', 'Jun Xu', 'Yilong Yin', 'Ling Shao', 'Cees G. M. Snoek']",http://arxiv.org/pdf/2105.03781v1.pdf,2021-05-08,"['cs.lg', 'cs.cv']","  Few-shot learning deals with the fundamental and challenging problem oflearning from a few annotated samples, while being able to generalize well onnew tasks. The crux of few-shot learning is to extract prior knowledge fromrelated tasks to enable fast adaptation to a new task with a limited amount ofdata. In this paper, we propose meta-learning kernels with random Fourierfeatures for few-shot learning, we call MetaKernel. Specifically, we proposelearning variational random features in a data-driven manner to obtaintask-specific kernels by leveraging the shared knowledge provided by relatedtasks in a meta-learning setting. We treat the random feature basis as thelatent variable, which is estimated by variational inference. The sharedknowledge from related tasks is incorporated into a context inference of theposterior, which we achieve via a long-short term memory module. To establishmore expressive kernels, we deploy conditional normalizing flows based oncoupling layers to achieve a richer posterior distribution over random Fourierbases. The resultant kernels are more informative and discriminative, whichfurther improves the few-shot learning. To evaluate our method, we conductextensive experiments on both few-shot image classification and regressiontasks. A thorough ablation study demonstrates that the effectiveness of eachintroduced component in our method. The benchmark results on fourteen datasetsdemonstrate MetaKernel consistently delivers at least comparable and oftenbetter performance than state-of-the-art alternatives."
FEDI: Few-shot learning based on Earth Mover's Distance algorithm  combined with deep residual network to identify diabetic retinopathy,"['Liangrui Pan', 'Boya Ji', 'Peng Xi', 'Xiaoqi Wang', 'Mitchai Chongcheawchamnan', 'Shaoliang Peng']",http://arxiv.org/pdf/2108.09711v2.pdf,2021-08-22,"['eess.iv', 'cs.cv', 'cs.lg']","  Diabetic retinopathy(DR) is the main cause of blindness in diabetic patients.However, DR can easily delay the occurrence of blindness through the diagnosisof the fundus. In view of the reality, it is difficult to collect a largeamount of diabetic retina data in clinical practice. This paper proposes afew-shot learning model of a deep residual network based on Earth Mover'sDistance algorithm to assist in diagnosing DR. We build training and validationclassification tasks for few-shot learning based on 39 categories of 1000sample data, train deep residual networks, and obtain experience maximizationpre-training models. Based on the weights of the pre-trained model, the EarthMover's Distance algorithm calculates the distance between the images, obtainsthe similarity between the images, and changes the model's parameters toimprove the accuracy of the training model. Finally, the experimentalconstruction of the small sample classification task of the test set tooptimize the model further, and finally, an accuracy of 93.5667% on the3way10shot task of the diabetic retina test set. For the experimental code andresults, please refer to:https://github.com/panliangrui/few-shot-learning-funds."
Who calls the shots? Rethinking Few-Shot Learning for Audio,"['Yu Wang', 'Nicholas J. Bryan', 'Justin Salamon', 'Mark Cartwright', 'Juan Pablo Bello']",http://arxiv.org/pdf/2110.09600v1.pdf,2021-10-18,"['cs.sd', 'eess.as']","  Few-shot learning aims to train models that can recognize novel classes givenjust a handful of labeled examples, known as the support set. While the fieldhas seen notable advances in recent years, they have often focused onmulti-class image classification. Audio, in contrast, is often multi-label dueto overlapping sounds, resulting in unique properties such as polyphony andsignal-to-noise ratios (SNR). This leads to unanswered questions concerning theimpact such audio properties may have on few-shot learning system design,performance, and human-computer interaction, as it is typically up to the userto collect and provide inference-time support set examples. We address thesequestions through a series of experiments designed to elucidate the answers tothese questions. We introduce two novel datasets, FSD-MIX-CLIPS andFSD-MIX-SED, whose programmatic generation allows us to explore these questionssystematically. Our experiments lead to audio-specific insights on few-shotlearning, some of which are at odds with recent findings in the image domain:there is no best one-size-fits-all model, method, and support set selectioncriterion. Rather, it depends on the expected application scenario. Our codeand data are available at https://github.com/wangyu/rethink-audio-fsl."
Learning Instance and Task-Aware Dynamic Kernels for Few Shot Learning,"['Rongkai Ma', 'Pengfei Fang', 'Gil Avraham', 'Yan Zuo', 'Tianyu Zhu', 'Tom Drummond', 'Mehrtash Harandi']",http://arxiv.org/pdf/2112.03494v2.pdf,2021-12-07,['cs.cv'],"  Learning and generalizing to novel concepts with few samples (Few-ShotLearning) is still an essential challenge to real-world applications. Aprinciple way of achieving few-shot learning is to realize a model that canrapidly adapt to the context of a given task. Dynamic networks have been showncapable of learning content-adaptive parameters efficiently, making themsuitable for few-shot learning. In this paper, we propose to learn the dynamickernels of a convolution network as a function of the task at hand, enablingfaster generalization. To this end, we obtain our dynamic kernels based on theentire task and each sample and develop a mechanism further conditioning oneach individual channel and position independently. This results in dynamickernels that simultaneously attend to the global information whilst alsoconsidering minuscule details available. We empirically show that our modelimproves performance on few-shot classification and detection tasks, achievinga tangible improvement over several baseline models. This includesstate-of-the-art results on 4 few-shot classification benchmarks:mini-ImageNet, tiered-ImageNet, CUB and FC100 and competitive results on afew-shot detection dataset: MS COCO-PASCAL-VOC."
Semantic-Based Few-Shot Learning by Interactive Psychometric Testing,"['Lu Yin', 'Vlado Menkovski', 'Yulong Pei', 'Mykola Pechenizkiy']",http://arxiv.org/pdf/2112.09201v2.pdf,2021-12-16,"['cs.cv', 'cs.ai']","  Few-shot classification tasks aim to classify images in query sets based ononly a few labeled examples in support sets. Most studies usually assume thateach image in a task has a single and unique class association. Under theseassumptions, these algorithms may not be able to identify the proper classassignment when there is no exact matching between support and query classes.For example, given a few images of lions, bikes, and apples to classify atiger. However, in a more general setting, we could consider the higher-levelconcept, the large carnivores, to match the tiger to the lion for semanticclassification. Existing studies rarely considered this situation due to theincompatibility of label-based supervision with complex conceptionrelationships. In this work, we advance the few-shot learning towards this morechallenging scenario, the semantic-based few-shot learning, and propose amethod to address the paradigm by capturing the inner semantic relationshipsusing interactive psychometric learning. The experiment results on theCIFAR-100 dataset show the superiority of our method for the semantic-basedfew-shot learning compared to the baseline."
Similarity Learning based Few Shot Learning for ECG Time Series  Classification,"['Priyanka Gupta', 'Sathvik Bhaskarpandit', 'Manik Gupta']",http://arxiv.org/pdf/2202.00612v1.pdf,2022-01-31,"['eess.sp', 'cs.lg']","  Using deep learning models to classify time series data generated from theInternet of Things (IoT) devices requires a large amount of labeled data.However, due to constrained resources available in IoT devices, it is oftendifficult to accommodate training using large data sets. This paper proposesand demonstrates a Similarity Learning-based Few Shot Learning for ECGarrhythmia classification using Siamese Convolutional Neural Networks. Few shotlearning resolves the data scarcity issue by identifying novel classes fromvery few labeled examples. Few Shot Learning relies first on pretraining themodel on a related relatively large database, and then the learning is used forfurther adaptation towards few examples available per class. Our experimentsevaluate the performance accuracy with respect to K (number of instances perclass) for ECG time series data classification. The accuracy with 5- shotlearning is 92.25% which marginally improves with further increase in K. Wealso compare the performance of our method against other well-establishedsimilarity learning techniques such as Dynamic Time Warping (DTW), EuclideanDistance (ED), and a deep learning model - Long Short Term Memory FullyConvolutional Network (LSTM-FCN) with the same amount of data and conclude thatour method outperforms them for a limited dataset size. For K=5, the accuraciesobtained are 57%, 54%, 33%, and 92% approximately for ED, DTW, LSTM-FCN, andSCNN, respectively."
Tuning Language Models as Training Data Generators for  Augmentation-Enhanced Few-Shot Learning,"['Yu Meng', 'Martin Michalski', 'Jiaxin Huang', 'Yu Zhang', 'Tarek Abdelzaher', 'Jiawei Han']",http://arxiv.org/pdf/2211.03044v2.pdf,2022-11-06,"['cs.cl', 'cs.lg']","  Recent studies have revealed the intriguing few-shot learning ability ofpretrained language models (PLMs): They can quickly adapt to a new task whenfine-tuned on a small amount of labeled data formulated as prompts, withoutrequiring abundant task-specific annotations. Despite their promisingperformance, most existing few-shot approaches that only learn from the smalltraining set still underperform fully supervised training by nontrivialmargins. In this work, we study few-shot learning with PLMs from a differentperspective: We first tune an autoregressive PLM on the few-shot samples andthen use it as a generator to synthesize a large amount of novel trainingsamples which augment the original training set. To encourage the generator toproduce label-discriminative samples, we train it via weighted maximumlikelihood where the weight of each token is automatically adjusted based on adiscriminative meta-learning objective. A classification PLM can then befine-tuned on both the few-shot and the synthetic samples with regularizationfor better generalization and stability. Our approach FewGen achieves anoverall better result across seven classification tasks of the GLUE benchmarkthan existing few-shot learning methods, improving no-augmentation methods by5+ average points, and outperforming augmentation methods by 3+ average points."
Few-Shot Learning for Biometric Verification,"['Saad Bin Ahmed', 'Umaid M. Zaffar', 'Marium Aslam', 'Muhammad Imran Malik']",http://arxiv.org/pdf/2211.06761v2.pdf,2022-11-12,['cs.cv'],"  In machine learning applications, it is common practice to feed as muchinformation as possible. In most cases, the model can handle large data setsthat allow to predict more accurately. In the presence of data scarcity, aFew-Shot learning (FSL) approach aims to build more accurate algorithms withlimited training data. We propose a novel end-to-end lightweight architecturethat verifies biometric data by producing competitive results as compared tostate-of-the-art accuracies through Few-Shot learning methods. The dense layersadd to the complexity of state-of-the-art deep learning models which inhibitsthem to be used in low-power applications. In presented approach, a shallownetwork is coupled with a conventional machine learning technique that exploitshand-crafted features to verify biometric images from multi-modal sources suchas signatures, periocular region, iris, face, fingerprints etc. We introduce aself-estimated threshold that strictly monitors False Acceptance Rate (FAR)while generalizing its results hence eliminating user-defined thresholds fromROC curves that are likely to be biased on local data distribution. This hybridmodel benefits from few-shot learning to make up for scarcity of data inbiometric use-cases. We have conducted extensive experimentation with commonlyused biometric datasets. The obtained results provided an effective solutionfor biometric verification systems."
RankDNN: Learning to Rank for Few-shot Learning,"['Qianyu Guo', 'Hongtong Gong', 'Xujun Wei', 'Yanwei Fu', 'Weifeng Ge', 'Yizhou Yu', 'Wenqiang Zhang']",http://arxiv.org/pdf/2211.15320v2.pdf,2022-11-28,['cs.cv'],"  This paper introduces a new few-shot learning pipeline that casts relevanceranking for image retrieval as binary ranking relation classification. Incomparison to image classification, ranking relation classification is sampleefficient and domain agnostic. Besides, it provides a new perspective onfew-shot learning and is complementary to state-of-the-art methods. The corecomponent of our deep neural network is a simple MLP, which takes as input animage triplet encoded as the difference between two vector-Kronecker products,and outputs a binary relevance ranking order. The proposed RankMLP can be builton top of any state-of-the-art feature extractors, and our entire deep neuralnetwork is called the ranking deep neural network, or RankDNN. Meanwhile,RankDNN can be flexibly fused with other post-processing methods. During themeta test, RankDNN ranks support images according to their similarity with thequery samples, and each query sample is assigned the class label of its nearestneighbor. Experiments demonstrate that RankDNN can effectively improve theperformance of its baselines based on a variety of backbones and it outperformsprevious state-of-the-art algorithms on multiple few-shot learning benchmarks,including miniImageNet, tieredImageNet, Caltech-UCSD Birds, and CIFAR-FS.Furthermore, experiments on the cross-domain challenge demonstrate the superiortransferability of RankDNN.The code is available at:https://github.com/guoqianyu-alberta/RankDNN."
Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary  Data,"['Alon Albalak', 'Colin Raffel', 'William Yang Wang']",http://arxiv.org/pdf/2302.00674v4.pdf,2023-02-01,"['cs.lg', 'cs.cl']","  Few-shot learning is valuable in many real-world applications, but learning ageneralizable model without overfitting to the few labeled datapoints ischallenging. In this work, we focus on Few-shot Learning with Auxiliary Data(FLAD), a training paradigm that assumes access to auxiliary data duringfew-shot learning in hopes of improving generalization. Previous works haveproposed automated methods for mixing auxiliary and target data, but thesemethods typically scale linearly (or worse) with the number of auxiliarydatasets, limiting their practicality. In this work we relate FLAD to theexplore-exploit dilemma that is central to the multi-armed bandit setting andderive algorithms whose computational complexity is independent of the numberof auxiliary datasets, allowing us to scale to 100x more auxiliary datasetsthan prior methods. We propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- andcompare them with prior FLAD methods that either explore or exploit, findingthat the combination of exploration and exploitation is crucial. Throughextensive experimentation we find that our methods outperform all pre-existingFLAD methods by 4% and lead to the first 3 billion parameter language modelsthat outperform the 175 billion parameter GPT-3. Overall, our work suggeststhat the discovery of better, more efficient mixing strategies for FLAD mayprovide a viable path towards substantially improving generalization infew-shot learning."
Universal Few-shot Learning of Dense Prediction Tasks with Visual Token  Matching,"['Donggyun Kim', 'Jinwoo Kim', 'Seongwoong Cho', 'Chong Luo', 'Seunghoon Hong']",http://arxiv.org/pdf/2303.14969v1.pdf,2023-03-27,"['cs.cv', 'cs.ai']","  Dense prediction tasks are a fundamental class of problems in computervision. As supervised methods suffer from high pixel-wise labeling cost, afew-shot learning solution that can learn any dense task from a few labeledimages is desired. Yet, current few-shot learning methods target a restrictedset of tasks such as semantic segmentation, presumably due to challenges indesigning a general and unified model that is able to flexibly and efficientlyadapt to arbitrary tasks of unseen semantics. We propose Visual Token Matching(VTM), a universal few-shot learner for arbitrary dense prediction tasks. Itemploys non-parametric matching on patch-level embedded tokens of images andlabels that encapsulates all tasks. Also, VTM flexibly adapts to any task witha tiny amount of task-specific parameters that modulate the matching algorithm.We implement VTM as a powerful hierarchical encoder-decoder architectureinvolving ViT backbones where token matching is performed at multiple featurehierarchies. We experiment VTM on a challenging variant of Taskonomy datasetand observe that it robustly few-shot learns various unseen dense predictiontasks. Surprisingly, it is competitive with fully supervised baselines usingonly 10 labeled examples of novel tasks (0.004% of full supervision) andsometimes outperforms using 0.1% of full supervision. Codes are available athttps://github.com/GitGyun/visual_token_matching."
FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained  Models in Few-Shot Learning,"['Kun Song', 'Huimin Ma', 'Bochao Zou', 'Huishuai Zhang', 'Weiran Huang']",http://arxiv.org/pdf/2310.15105v4.pdf,2023-10-23,['cs.cv'],"  Due to the limited availability of data, existing few-shot learning methodstrained from scratch fail to achieve satisfactory performance. In contrast,large-scale pre-trained models such as CLIP demonstrate remarkable few-shot andzero-shot capabilities. To enhance the performance of pre-trained models fordownstream tasks, fine-tuning the model on downstream data is frequentlynecessary. However, fine-tuning the pre-trained model leads to a decrease inits generalizability in the presence of distribution shift, while the limitednumber of samples in few-shot learning makes the model highly susceptible tooverfitting. Consequently, existing methods for fine-tuning few-shot learningprimarily focus on fine-tuning the model's classification head or introducingadditional structure. In this paper, we introduce a fine-tuning approach termedFeature Discrimination Alignment (FD-Align). Our method aims to bolster themodel's generalizability by preserving the consistency of spurious featuresacross the fine-tuning process. Extensive experimental results validate theefficacy of our approach for both ID and OOD tasks. Once fine-tuned, the modelcan seamlessly integrate with existing methods, leading to performanceimprovements. Our code can be found in https://github.com/skingorz/FD-Align."
Meta-SGD: Learning to Learn Quickly for Few-Shot Learning,"['Zhenguo Li', 'Fengwei Zhou', 'Fei Chen', 'Hang Li']",http://arxiv.org/pdf/1707.09835v2.pdf,2017-07-31,['cs.lg'],"  Few-shot learning is challenging for learning algorithms that learn each taskin isolation and from scratch. In contrast, meta-learning learns from manyrelated tasks a meta-learner that can learn a new task more accurately andfaster with fewer examples, where the choice of meta-learners is crucial. Inthis paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learnerthat can initialize and adapt any differentiable learner in just one step, onboth supervised learning and reinforcement learning. Compared to the popularmeta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, andcan be learned more efficiently. Compared to the latest meta-learner MAML,Meta-SGD has a much higher capacity by learning to learn not just the learnerinitialization, but also the learner update direction and learning rate, all ina single meta-learning process. Meta-SGD shows highly competitive performancefor few-shot learning on regression, classification, and reinforcementlearning."
Dynamic Input Structure and Network Assembly for Few-Shot Learning,"['Nathan Hilliard', 'Nathan O. Hodas', 'Courtney D. Corley']",http://arxiv.org/pdf/1708.06819v1.pdf,2017-08-22,"['cs.lg', 'stat.ml']","  The ability to learn from a small number of examples has been a difficultproblem in machine learning since its inception. While methods have succeededwith large amounts of training data, research has been underway in how toaccomplish similar performance with fewer examples, known as one-shot or moregenerally few-shot learning. This technique has been shown to have promisingperformance, but in practice requires fixed-size inputs making it impracticalfor production systems where class sizes can vary. This impedes training andthe final utility of few-shot learning systems. This paper describes anapproach to constructing and training a network that can handle arbitraryexample sizes dynamically as the system is used."
Learning to Compare: Relation Network for Few-Shot Learning,"['Flood Sung', 'Yongxin Yang', 'Li Zhang', 'Tao Xiang', 'Philip H. S. Torr', 'Timothy M. Hospedales']",http://arxiv.org/pdf/1711.06025v2.pdf,2017-11-16,['cs.cv'],"  We present a conceptually simple, flexible, and general framework forfew-shot learning, where a classifier must learn to recognise new classes givenonly few examples from each. Our method, called the Relation Network (RN), istrained end-to-end from scratch. During meta-learning, it learns to learn adeep distance metric to compare a small number of images within episodes, eachof which is designed to simulate the few-shot setting. Once trained, a RN isable to classify images of new classes by computing relation scores betweenquery images and the few examples of each new class without further updatingthe network. Besides providing improved performance on few-shot learning, ourframework is easily extended to zero-shot learning. Extensive experiments onfive benchmarks demonstrate that our simple approach provides a unified andeffective approach for both of these two tasks."
Learning Deep Disentangled Embeddings with the F-Statistic Loss,"['Karl Ridgeway', 'Michael C. Mozer']",http://arxiv.org/pdf/1802.05312v2.pdf,2018-02-14,"['cs.lg', 'cs.ai', 'stat.ml']","  Deep-embedding methods aim to discover representations of a domain that makeexplicit the domain's class structure and thereby support few-shot learning.Disentangling methods aim to make explicit compositional or factorialstructure. We combine these two active but independent lines of research andpropose a new paradigm suitable for both goals. We propose and evaluate a novelloss function based on the $F$ statistic, which describes the separation of twoor more distributions. By ensuring that distinct classes are well separated ona subset of embedding dimensions, we obtain embeddings that are useful forfew-shot learning. By not requiring separation on all dimensions, we encouragethe discovery of disentangled representations. Our embedding method matches orbeats state-of-the-art, as evaluated by performance on recall@$k$ and few-shotlearning tasks. Our method also obtains performance superior to a variety ofalternatives on disentangling, as evaluated by two key properties of adisentangled representation: modularity and explicitness. The goal of our workis to obtain more interpretable, manipulable, and generalizable deeprepresentations of concepts and categories."
How to train your MAML,"['Antreas Antoniou', 'Harrison Edwards', 'Amos Storkey']",http://arxiv.org/pdf/1810.09502v3.pdf,2018-10-22,"['cs.lg', 'stat.ml']","  The field of few-shot learning has recently seen substantial advancements.Most of these advancements came from casting few-shot learning as ameta-learning problem. Model Agnostic Meta Learning or MAML is currently one ofthe best approaches for few-shot learning via meta-learning. MAML is simple,elegant and very powerful, however, it has a variety of issues, such as beingvery sensitive to neural network architectures, often leading to instabilityduring training, requiring arduous hyperparameter searches to stabilizetraining and achieve high generalization and being very computationallyexpensive at both training and inference times. In this paper, we proposevarious modifications to MAML that not only stabilize the system, but alsosubstantially improve the generalization performance, convergence speed andcomputational overhead of MAML, which we call MAML++."
Few-shot learning with attention-based sequence-to-sequence models,"['Bertrand Higy', 'Peter Bell']",http://arxiv.org/pdf/1811.03519v2.pdf,2018-11-08,['cs.cl'],"  End-to-end approaches have recently become popular as a means of simplifyingthe training and deployment of speech recognition systems. However, they oftenrequire large amounts of data to perform well on large vocabulary tasks. Withthe aim of making end-to-end approaches usable by a broader range ofresearchers, we explore the potential to use end-to-end methods in smallvocabulary contexts where smaller datasets may be used. A significant drawbackof small-vocabulary systems is the difficulty of expanding the vocabularybeyond the original training samples -- therefore we also study strategies toextend the vocabulary with only few examples per new class (few-shot learning).  Our results show that an attention-based encoder-decoder can be competitiveagainst a strong baseline on a small vocabulary keyword classification task,reaching 97.5% of accuracy on Tensorflow's Speech Commands dataset. It alsoshows promising results on the few-shot learning problem where a simplestrategy achieved 68.8\% of accuracy on new keywords with only 10 examples foreach new class. This score goes up to 88.4\% with a larger set of 100 examples."
Power Normalizing Second-order Similarity Network for Few-shot Learning,"['Hongguang Zhang', 'Piotr Koniusz']",http://arxiv.org/pdf/1811.04167v1.pdf,2018-11-10,['cs.cv'],"  Second- and higher-order statistics of data points have played an importantrole in advancing the state of the art on several computer vision problems suchas the fine-grained image and scene recognition. However, these statistics needto be passed via an appropriate pooling scheme to obtain the best performance.Power Normalizations are non-linear activation units which enjoyprobability-inspired derivations and can be applied in CNNs. In this paper, wepropose a similarity learning network leveraging second-order information andPower Normalizations. To this end, we propose several formulations capturingsecond-order statistics and derive a sigmoid-like Power Normalizing function todemonstrate its interpretability. Our model is trained end-to-end to learn thesimilarity between the support set and query images for the problem of one- andfew-shot learning. The evaluations on Omniglot, miniImagenet and Open MICdatasets demonstrate that this network obtains state-of-the-art results onseveral few-shot learning protocols."
Few-Shot Learning-Based Human Activity Recognition,"['Siwei Feng', 'Marco F. Duarte']",http://arxiv.org/pdf/1903.10416v1.pdf,2019-03-25,"['cs.lg', 'stat.ml']","  Few-shot learning is a technique to learn a model with a very small amount oflabeled training data by transferring knowledge from relevant tasks. In thispaper, we propose a few-shot learning method for wearable sensor based humanactivity recognition, a technique that seeks high-level human activityknowledge from low-level sensor inputs. Due to the high costs to obtain humangenerated activity data and the ubiquitous similarities between activity modes,it can be more efficient to borrow information from existing activityrecognition models than to collect more data to train a new model from scratchwhen only a few data are available for model training. The proposed few-shothuman activity recognition method leverages a deep learning model for featureextraction and classification while knowledge transfer is performed in themanner of model parameter transfer. In order to alleviate negative transfer, wepropose a metric to measure cross-domain class-wise relevance so that knowledgeof higher relevance is assigned larger weights during knowledge transfer.Promising results in extensive experiments show the advantages of the proposedapproach."
Edge-labeling Graph Neural Network for Few-shot Learning,"['Jongmin Kim', 'Taesup Kim', 'Sungwoong Kim', 'Chang D. Yoo']",http://arxiv.org/pdf/1905.01436v1.pdf,2019-05-04,"['cs.lg', 'cs.cv']","  In this paper, we propose a novel edge-labeling graph neural network (EGNN),which adapts a deep neural network on the edge-labeling graph, for few-shotlearning. The previous graph neural network (GNN) approaches in few-shotlearning have been based on the node-labeling framework, which implicitlymodels the intra-cluster similarity and the inter-cluster dissimilarity. Incontrast, the proposed EGNN learns to predict the edge-labels rather than thenode-labels on the graph that enables the evolution of an explicit clusteringby iteratively updating the edge-labels with direct exploitation of bothintra-cluster similarity and the inter-cluster dissimilarity. It is also wellsuited for performing on various numbers of classes without retraining, and canbe easily extended to perform a transductive inference. The parameters of theEGNN are learned by episodic training with an edge-labeling loss to obtain awell-generalizable model for unseen low-data problem. On both of the supervisedand semi-supervised few-shot image classification tasks with two benchmarkdatasets, the proposed EGNN significantly improves the performances over theexisting GNNs."
Discriminative Few-Shot Learning Based on Directional Statistics,"['Junyoung Park', 'Subin Yi', 'Yongseok Choi', 'Dong-Yeon Cho', 'Jiwon Kim']",http://arxiv.org/pdf/1906.01819v1.pdf,2019-06-05,"['cs.lg', 'stat.ml']","  Metric-based few-shot learning methods try to overcome the difficulty due tothe lack of training examples by learning embedding to make comparison easy. Wepropose a novel algorithm to generate class representatives for few-shotclassification tasks. As a probabilistic model for learned features of inputs,we consider a mixture of von Mises-Fisher distributions which is known to bemore expressive than Gaussian in a high dimensional space. Then, from adiscriminative classifier perspective, we get a better class representativeconsidering inter-class correlation which has not been addressed byconventional few-shot learning algorithms. We apply our method to\emph{mini}ImageNet and \emph{tiered}ImageNet datasets, and show that theproposed approach outperforms other comparable methods in few-shotclassification tasks."
Baby steps towards few-shot learning with multiple semantics,"['Eli Schwartz', 'Leonid Karlinsky', 'Rogerio Feris', 'Raja Giryes', 'Alex M. Bronstein']",http://arxiv.org/pdf/1906.01905v2.pdf,2019-06-05,['cs.cv'],"  Learning from one or few visual examples is one of the key capabilities ofhumans since early infancy, but is still a significant challenge for modern AIsystems. While considerable progress has been achieved in few-shot learningfrom a few image examples, much less attention has been given to the verbaldescriptions that are usually provided to infants when they are presented witha new object. In this paper, we focus on the role of additional semantics thatcan significantly facilitate few-shot visual learning. Building upon recentadvances in few-shot learning with additional semantic information, wedemonstrate that further improvements are possible by combining multiple andricher semantics (category labels, attributes, and natural languagedescriptions). Using these ideas, we offer the community new results on thepopular miniImageNet and CUB few-shot benchmarks, comparing favorably to theprevious state-of-the-art results for both visual only and visual plussemantics-based approaches. We also performed an ablation study investigatingthe components and design choices of our approach."
Few-Shot Video Classification via Temporal Alignment,"['Kaidi Cao', 'Jingwei Ji', 'Zhangjie Cao', 'Chien-Yi Chang', 'Juan Carlos Niebles']",http://arxiv.org/pdf/1906.11415v1.pdf,2019-06-27,['cs.cv'],"  There is a growing interest in learning a model which could recognize novelclasses with only a few labeled examples. In this paper, we propose TemporalAlignment Module (TAM), a novel few-shot learning framework that can learn toclassify a previous unseen video. While most previous works neglect long-termtemporal ordering information, our proposed model explicitly leverages thetemporal ordering information in video data through temporal alignment. Thisleads to strong data-efficiency for few-shot learning. In concrete, TAMcalculates the distance value of query video with respect to novel classproxies by averaging the per frame distances along its alignment path. Weintroduce continuous relaxation to TAM so the model can be learned in anend-to-end fashion to directly optimize the few-shot learning objective. Weevaluate TAM on two challenging real-world datasets, Kinetics andSomething-Something-V2, and show that our model leads to significantimprovement of few-shot video classification over a wide range of competitivebaselines."
Efficient Automatic Meta Optimization Search for Few-Shot Learning,"['Xinyue Zheng', 'Peng Wang', 'Qigang Wang', 'Zhongchao shi', 'Feiyu Xu']",http://arxiv.org/pdf/1909.03817v1.pdf,2019-09-06,"['cs.lg', 'cs.cv', 'cs.ne']","  Previous works on meta-learning either relied on elaborately hand-designednetwork structures or adopted specialized learning rules to a particulardomain. We propose a universal framework to optimize the meta-learning processautomatically by adopting neural architecture search technique (NAS). NASautomatically generates and evaluates meta-learner's architecture for few-shotlearning problems, while the meta-learner uses meta-learning algorithm tooptimize its parameters based on the distribution of learning tasks. Parametersharing and experience replay are adopted to accelerate the architecturessearching process, so it takes only 1-2 GPU days to find good architectures.Extensive experiments on Mini-ImageNet and Omniglot show that our algorithmexcels in few-shot learning tasks. The best architecture found on Mini-ImageNetachieves competitive results when transferred to Omniglot, which shows the hightransferability of architectures among different computer vision problems."
When Does Self-supervision Improve Few-shot Learning?,"['Jong-Chyi Su', 'Subhransu Maji', 'Bharath Hariharan']",http://arxiv.org/pdf/1910.03560v2.pdf,2019-10-08,"['cs.cv', 'cs.lg']","  We investigate the role of self-supervised learning (SSL) in the context offew-shot learning. Although recent research has shown the benefits of SSL onlarge unlabeled datasets, its utility on small datasets is relativelyunexplored. We find that SSL reduces the relative error rate of few-shotmeta-learners by 4%-27%, even when the datasets are small and only utilizingimages within the datasets. The improvements are greater when the training setis smaller or the task is more challenging. Although the benefits of SSL mayincrease with larger training sets, we observe that SSL can hurt theperformance when the distributions of images used for meta-learning and SSL aredifferent. We conduct a systematic study by varying the degree of domain shiftand analyzing the performance of several meta-learners on a multitude ofdomains. Based on this analysis we present a technique that automaticallyselects images for SSL from a large, generic pool of unlabeled images for agiven dataset that provides further improvements."
Improved Few-Shot Visual Classification,"['Peyman Bateni', 'Raghav Goyal', 'Vaden Masrani', 'Frank Wood', 'Leonid Sigal']",http://arxiv.org/pdf/1912.03432v3.pdf,2019-12-07,['cs.cv'],"  Few-shot learning is a fundamental task in computer vision that carries thepromise of alleviating the need for exhaustively labeled data. Most few-shotlearning approaches to date have focused on progressively more complex neuralfeature extractors and classifier adaptation strategies, as well as therefinement of the task definition itself. In this paper, we explore thehypothesis that a simple class-covariance-based distance metric, namely theMahalanobis distance, adopted into a state of the art few-shot learningapproach (CNAPS) can, in and of itself, lead to a significant performanceimprovement. We also discover that it is possible to learn adaptive featureextractors that allow useful estimation of the high dimensional featurecovariances required by this metric from surprisingly few samples. The resultof our work is a new ""Simple CNAPS"" architecture which has up to 9.2% fewertrainable parameters than CNAPS and performs up to 6.1% better than state ofthe art on the standard few-shot image classification benchmark dataset."
Unsupervised Few-shot Learning via Self-supervised Training,"['Zilong Ji', 'Xiaolong Zou', 'Tiejun Huang', 'Si Wu']",http://arxiv.org/pdf/1912.12178v1.pdf,2019-12-20,"['cs.cv', 'cs.lg']","  Learning from limited exemplars (few-shot learning) is a fundamental,unsolved problem that has been laboriously explored in the machine learningcommunity. However, current few-shot learners are mostly supervised and relyheavily on a large amount of labeled examples. Unsupervised learning is a morenatural procedure for cognitive mammals and has produced promising results inmany machine learning tasks. In the current study, we develop a method to learnan unsupervised few-shot learner via self-supervised training (UFLST), whichcan effectively generalize to novel but related classes. The proposed modelconsists of two alternate processes, progressive clustering and episodictraining. The former generates pseudo-labeled training examples forconstructing episodic tasks; and the later trains the few-shot learner usingthe generated episodic tasks which further optimizes the featurerepresentations of data. The two processes facilitate with each other, andeventually produce a high quality few-shot learner. Using the benchmark datasetOmniglot and Mini-ImageNet, we show that our model outperforms otherunsupervised few-shot learning methods. Using the benchmark dataset Market1501,we further demonstrate the feasibility of our model to a real-world applicationon person re-identification."
Multi-scale Adaptive Task Attention Network for Few-Shot Learning,"['Haoxing Chen', 'Huaxiong Li', 'Yaohui Li', 'Chunlin Chen']",http://arxiv.org/pdf/2011.14479v1.pdf,2020-11-30,"['cs.cv', 'cs.ai', 'cs.lg']","  The goal of few-shot learning is to classify unseen categories with fewlabeled samples. Recently, the low-level information metric-learning basedmethods have achieved satisfying performance, since local representations (LRs)are more consistent between seen and unseen classes. However, most of thesemethods deal with each category in the support set independently, which is notsufficient to measure the relation between features, especially in a certaintask. Moreover, the low-level information-based metric learning method sufferswhen dominant objects of different scales exist in a complex background. Toaddress these issues, this paper proposes a novel Multi-scale Adaptive TaskAttention Network (MATANet) for few-shot learning. Specifically, we first use amulti-scale feature generator to generate multiple features at differentscales. Then, an adaptive task attention module is proposed to select the mostimportant LRs among the entire task. Afterwards, a similarity-to-class moduleand a fusion layer are utilized to calculate a joint multi-scale similaritybetween the query image and the support set. Extensive experiments on popularbenchmarks clearly show the effectiveness of the proposed MATANet compared withstate-of-the-art methods."
Cross-domain few-shot learning with unlabelled data,['Fupin Yao'],http://arxiv.org/pdf/2101.07899v1.pdf,2021-01-19,"['cs.cv', 'cs.lg']","  Few shot learning aims to solve the data scarcity problem. If there is adomain shift between the test set and the training set, their performance willdecrease a lot. This setting is called Cross-domain few-shot learning. However,this is very challenging because the target domain is unseen during training.Thus we propose a new setting some unlabelled data from the target domain isprovided, which can bridge the gap between the source domain and the targetdomain. A benchmark for this setting is constructed using DomainNet\cite{peng2018oment}. We come up with a self-supervised learning method tofully utilize the knowledge in the labeled training set and the unlabelled set.Extensive experiments show that our methods outperforms several baselinemethods by a large margin. We also carefully design an episodic trainingpipeline which yields a significant performance boost."
Stress Testing of Meta-learning Approaches for Few-shot Learning,"['Aroof Aimen', 'Sahil Sidheekh', 'Vineet Madan', 'Narayanan C. Krishnan']",http://arxiv.org/pdf/2101.08587v1.pdf,2021-01-21,"['cs.lg', 'cs.ai']","  Meta-learning (ML) has emerged as a promising learning method under resourceconstraints such as few-shot learning. ML approaches typically propose amethodology to learn generalizable models. In this work-in-progress paper, weput the recent ML approaches to a stress test to discover their limitations.Precisely, we measure the performance of ML approaches for few-shot learningagainst increasing task complexity. Our results show a quick degradation in theperformance of initialization strategies for ML (MAML, TAML, and MetaSGD),while surprisingly, approaches that use an optimization strategy (MetaLSTM)perform significantly better. We further demonstrate the effectiveness of anoptimization strategy for ML (MetaLSTM++) trained in a MAML manner over a pureoptimization strategy. Our experiments also show that the optimizationstrategies for ML achieve higher transferability from simple to complex tasks."
Edge-Labeling based Directed Gated Graph Network for Few-shot Learning,"['Peixiao Zheng', 'Xin Guo', 'Lin Qi']",http://arxiv.org/pdf/2101.11299v1.pdf,2021-01-27,['cs.cv'],"  Existing graph-network-based few-shot learning methods obtain similaritybetween nodes through a convolution neural network (CNN). However, the CNN isdesigned for image data with spatial information rather than vector form nodefeature. In this paper, we proposed an edge-labeling-based directed gated graphnetwork (DGGN) for few-shot learning, which utilizes gated recurrent units toimplicitly update the similarity between nodes. DGGN is composed of a gatednode aggregation module and an improved gated recurrent unit (GRU) based edgeupdate module. Specifically, the node update module adopts a gate mechanismusing activation of edge feature, making a learnable node aggregation process.Besides, improved GRU cells are employed in the edge update procedure tocompute the similarity between nodes. Further, this mechanism is beneficial togradient backpropagation through the GRU sequence across layers. Experimentresults conducted on two benchmark datasets show that our DGGN achieves acomparable performance to the-state-of-art methods."
"Assume, Augment and Learn: Unsupervised Few-Shot Meta-Learning via  Random Labels and Data Augmentation","['Antreas Antoniou', 'Amos Storkey']",http://arxiv.org/pdf/1902.09884v3.pdf,2019-02-26,"['stat.ml', 'cs.lg']","  The field of few-shot learning has been laboriously explored in thesupervised setting, where per-class labels are available. On the other hand,the unsupervised few-shot learning setting, where no labels of any kind arerequired, has seen little investigation. We propose a method, named Assume,Augment and Learn or AAL, for generating few-shot tasks using unlabeled data.We randomly label a random subset of images from an unlabeled dataset togenerate a support set. Then by applying data augmentation on the support set'simages, and reusing the support set's labels, we obtain a target set. Theresulting few-shot tasks can be used to train any standard meta-learningframework. Once trained, such a model, can be directly applied on smallreal-labeled datasets without any changes or fine-tuning required. In ourexperiments, the learned models achieve good generalization performance in avariety of established few-shot learning tasks on Omniglot and Mini-Imagenet."
Few-Shot Learning via Saliency-guided Hallucination of Samples,"['Hongguang Zhang', 'Jing Zhang', 'Piotr Koniusz']",http://arxiv.org/pdf/1904.03472v1.pdf,2019-04-06,['cs.cv'],"  Learning new concepts from a few of samples is a standard challenge incomputer vision. The main directions to improve the learning ability offew-shot training models include (i) a robust similarity learning and (ii)generating or hallucinating additional data from the limited existing samples.In this paper, we follow the latter direction and present a novel datahallucination model. Currently, most datapoint generators contain a specializednetwork (i.e., GAN) tasked with hallucinating new datapoints, thus requiringlarge numbers of annotated data for their training in the first place. In thispaper, we propose a novel less-costly hallucination method for few-shotlearning which utilizes saliency maps. To this end, we employ a saliencynetwork to obtain the foregrounds and backgrounds of available image samplesand feed the resulting maps into a two-stream network to hallucinate datapointsdirectly in the feature space from viable foreground-background combinations.To the best of our knowledge, we are the first to leverage saliency maps forsuch a task and we demonstrate their usefulness in hallucinating additionaldatapoints for few-shot learning. Our proposed network achieves the state ofthe art on publicly available datasets."
Meta-Learning with Differentiable Convex Optimization,"['Kwonjoon Lee', 'Subhransu Maji', 'Avinash Ravichandran', 'Stefano Soatto']",http://arxiv.org/pdf/1904.03758v2.pdf,2019-04-07,"['cs.cv', 'cs.lg']","  Many meta-learning approaches for few-shot learning rely on simple baselearners such as nearest-neighbor classifiers. However, even in the few-shotregime, discriminatively trained linear predictors can offer bettergeneralization. We propose to use these predictors as base learners to learnrepresentations for few-shot learning and show they offer better tradeoffsbetween feature size and performance across a range of few-shot recognitionbenchmarks. Our objective is to learn feature embeddings that generalize wellunder a linear classification rule for novel categories. To efficiently solvethe objective, we exploit two properties of linear classifiers: implicitdifferentiation of the optimality conditions of the convex problem and the dualformulation of the optimization problem. This allows us to use high-dimensionalembeddings with improved generalization at a modest increase in computationaloverhead. Our approach, named MetaOptNet, achieves state-of-the-art performanceon miniImageNet, tieredImageNet, CIFAR-FS, and FC100 few-shot learningbenchmarks. Our code is available at https://github.com/kjunelee/MetaOptNet."
Few-Shot Learning with Localization in Realistic Settings,"['Davis Wertheimer', 'Bharath Hariharan']",http://arxiv.org/pdf/1904.08502v2.pdf,2019-04-09,"['cs.cv', 'cs.ai', 'cs.lg', 'stat.ml']","  Traditional recognition methods typically require large,artificially-balanced training classes, while few-shot learning methods aretested on artificially small ones. In contrast to both extremes, real worldrecognition problems exhibit heavy-tailed class distributions, with clutteredscenes and a mix of coarse and fine-grained class distinctions. We show thatprior methods designed for few-shot learning do not work out of the box inthese challenging conditions, based on a new ""meta-iNat"" benchmark. Weintroduce three parameter-free improvements: (a) better training proceduresbased on adapting cross-validation to meta-learning, (b) novel architecturesthat localize objects using limited bounding box annotations beforeclassification, and (c) simple parameter-free expansions of the feature spacebased on bilinear pooling. Together, these improvements double the accuracy ofstate-of-the-art models on meta-iNat while generalizing to prior benchmarks,complex neural architectures, and settings with substantial domain shift."
Relational Generalized Few-Shot Learning,"['Xiahan Shi', 'Leonard Salewski', 'Martin Schiegg', 'Zeynep Akata', 'Max Welling']",http://arxiv.org/pdf/1907.09557v2.pdf,2019-07-22,"['cs.lg', 'stat.ml']","  Transferring learned models to novel tasks is a challenging problem,particularly if only very few labeled examples are available. Although thisfew-shot learning setup has received a lot of attention recently, most proposedmethods focus on discriminating novel classes only. Instead, we consider theextended setup of generalized few-shot learning (GFSL), where the model isrequired to perform classification on the joint label space consisting of bothpreviously seen and novel classes. We propose a graph-based framework thatexplicitly models relationships between all seen and novel classes in the jointlabel space. Our model Graph-convolutional Global Prototypical Networks (GcGPN)incorporates these inter-class relations using graph-convolution in order toembed novel class representations into the existing space of previously seenclasses in a globally consistent manner. Our approach ensures both fastadaptation and global discrimination, which is the major challenge in GFSL. Wedemonstrate the benefits of our model on two challenging benchmark datasets."
Learning to Control Latent Representations for Few-Shot Learning of  Named Entities,"['Omar U. Florez', 'Erik Mueller']",http://arxiv.org/pdf/1911.08542v1.pdf,2019-11-19,['cs.lg'],"  Humans excel in continuously learning with small data without forgetting howto solve old problems. However, neural networks require large datasets tocompute latent representations across different tasks while minimizing a lossfunction. For example, a natural language understanding (NLU) system will oftendeal with emerging entities during its deployment as interactions with users inrealistic scenarios will generate new and infrequent names, events, andlocations. Here, we address this scenario by introducing an RL trainablecontroller that disentangles the representation learning of a neural encoderfrom its memory management role.  Our proposed solution is straightforward and simple: we train a controller toexecute an optimal sequence of reading and writing operations on an externalmemory with the goal of leveraging diverse activations from the past andprovide accurate predictions. Our approach is named Learning to Control (LTC)and allows few-shot learning with two degrees of memory plasticity. Weexperimentally show that our system obtains accurate results for few-shotlearning of entity recognition in the Stanford Task-Oriented Dialogue dataset."
Learning Multi-level Weight-centric Features for Few-shot Learning,"['Mingjiang Liang', 'Shaoli Huang', 'Shirui Pan', 'Mingming Gong', 'Wei Liu']",http://arxiv.org/pdf/1911.12476v2.pdf,2019-11-28,"['cs.cv', 'cs.lg', 'stat.ml']","  Few-shot learning is currently enjoying a considerable resurgence ofinterest, aided by the recent advance of deep learning. Contemporary approachesbased on weight-generation scheme delivers a straightforward and flexiblesolution to the problem. However, they did not fully consider both therepresentation power for unseen categories and weight generation capacity infeature learning, making it a significant performance bottleneck. This paperproposes a multi-level weight-centric feature learning to give full play tofeature extractor's dual roles in few-shot learning. Our proposed methodconsists of two essential techniques: a weight-centric training strategy toimprove the features' prototype-ability and a multi-level feature incorporatinga mid- and relation-level information. The former increases the feasibility ofconstructing a discriminative decision boundary based on a few samples.Simultaneously, the latter helps improve the transferability for characterizingnovel classes and preserve classification capability for base classes. Weextensively evaluate our approach to low-shot classification benchmarks.Experiments demonstrate our proposed method significantly outperforms itscounterparts in both standard and generalized settings and using differentnetwork backbones."
Few-Shot Learning with Geometric Constraints,"['Hong-Gyu Jung', 'Seong-Whan Lee']",http://arxiv.org/pdf/2003.09151v1.pdf,2020-03-20,"['cs.lg', 'cs.cv']","  In this article, we consider the problem of few-shot learning forclassification. We assume a network trained for base categories with a largenumber of training examples, and we aim to add novel categories to it that haveonly a few, e.g., one or five, training examples. This is a challengingscenario because: 1) high performance is required in both the base and novelcategories; and 2) training the network for the new categories with a fewtraining examples can contaminate the feature space trained well for the basecategories. To address these challenges, we propose two geometric constraintsto fine-tune the network with a few training examples. The first constraintenables features of the novel categories to cluster near the category weights,and the second maintains the weights of the novel categories far from theweights of the base categories. By applying the proposed constraints, weextract discriminative features for the novel categories while preserving thefeature space learned for the base categories. Using public data sets forfew-shot learning that are subsets of ImageNet, we demonstrate that theproposed method outperforms prevalent methods by a large margin."
Self-Augmentation: Generalizing Deep Networks to Unseen Classes for  Few-Shot Learning,"['Jin-Woo Seo', 'Hong-Gyu Jung', 'Seong-Whan Lee']",http://arxiv.org/pdf/2004.00251v3.pdf,2020-04-01,"['cs.lg', 'cs.cv', 'stat.ml']","  Few-shot learning aims to classify unseen classes with a few trainingexamples. While recent works have shown that standard mini-batch training witha carefully designed training strategy can improve generalization ability forunseen classes, well-known problems in deep networks such as memorizingtraining statistics have been less explored for few-shot learning. To tacklethis issue, we propose self-augmentation that consolidates self-mix andself-distillation. Specifically, we exploit a regional dropout technique calledself-mix, in which a patch of an image is substituted into other values in thesame image. Then, we employ a backbone network that has auxiliary branches withits own classifier to enforce knowledge sharing. Lastly, we present a localrepresentation learner to further exploit a few training examples for unseenclasses. Experimental results show that the proposed method outperforms thestate-of-the-art methods for prevalent few-shot benchmarks and improves thegeneralization ability."
Learning to Classify Intents and Slot Labels Given a Handful of Examples,"['Jason Krone', 'Yi Zhang', 'Mona Diab']",http://arxiv.org/pdf/2004.10793v1.pdf,2020-04-22,['cs.cl'],"  Intent classification (IC) and slot filling (SF) are core components in mostgoal-oriented dialogue systems. Current IC/SF models perform poorly when thenumber of training examples per class is small. We propose a new few-shotlearning task, few-shot IC/SF, to study and improve the performance of IC andSF models on classes not seen at training time in ultra low resource scenarios.We establish a few-shot IC/SF benchmark by defining few-shot splits for threepublic IC/SF datasets, ATIS, TOP, and Snips. We show that two popular few-shotlearning algorithms, model agnostic meta learning (MAML) and prototypicalnetworks, outperform a fine-tuning baseline on this benchmark. Prototypicalnetworks achieves significant gains in IC performance on the ATIS and TOPdatasets, while both prototypical networks and MAML outperform the baselinewith respect to SF on all three datasets. In addition, we demonstrate thatjoint training as well as the use of pre-trained language models, ELMo and BERTin our case, are complementary to these few-shot learning methods and yieldfurther gains."
Memory-Augmented Relation Network for Few-Shot Learning,"['Jun He', 'Richang Hong', 'Xueliang Liu', 'Mingliang Xu', 'Zhengjun Zha', 'Meng Wang']",http://arxiv.org/pdf/2005.04414v2.pdf,2020-05-09,['cs.cv'],"  Metric-based few-shot learning methods concentrate on learning transferablefeature embedding that generalizes well from seen categories to unseencategories under the supervision of limited number of labelled instances.However, most of them treat each individual instance in the working contextseparately without considering its relationships with the others. In this work,we investigate a new metric-learning method, Memory-Augmented Relation Network(MRN), to explicitly exploit these relationships. In particular, for aninstance, we choose the samples that are visually similar from the workingcontext, and perform weighted information propagation to attentively aggregatehelpful information from the chosen ones to enhance its representation. In MRN,we also formulate the distance metric as a learnable relation module whichlearns to compare for similarity measurement, and augment the working contextwith memory slots, both contributing to its generality. We empiricallydemonstrate that MRN yields significant improvement over its ancestor andachieves competitive or even better performance when compared with otherfew-shot learning approaches on the two major benchmark datasets, i.e.miniImagenet and tieredImagenet."
A Comparison of Few-Shot Learning Methods for Underwater Optical and  Sonar Image Classification,"['Mateusz Ochal', 'Jose Vazquez', 'Yvan Petillot', 'Sen Wang']",http://arxiv.org/pdf/2005.04621v2.pdf,2020-05-10,['cs.cv'],"  Deep convolutional neural networks generally perform well in underwaterobject recognition tasks on both optical and sonar images. Many such methodsrequire hundreds, if not thousands, of images per class to generalize well tounseen examples. However, obtaining and labeling sufficiently large volumes ofdata can be relatively costly and time-consuming, especially when observingrare objects or performing real-time operations. Few-Shot Learning (FSL)efforts have produced many promising methods to deal with low dataavailability. However, little attention has been given in the underwaterdomain, where the style of images poses additional challenges for objectrecognition algorithms. To the best of our knowledge, this is the first paperto evaluate and compare several supervised and semi-supervised Few-ShotLearning (FSL) methods using underwater optical and side-scan sonar imagery.Our results show that FSL methods offer a significant advantage over thetraditional transfer learning methods that fine-tune pre-trained models. Wehope that our work will help apply FSL to autonomous underwater systems andexpand their learning capabilities."
Prototypical Q Networks for Automatic Conversational Diagnosis and  Few-Shot New Disease Adaption,"['Hongyin Luo', 'Shang-Wen Li', 'James Glass']",http://arxiv.org/pdf/2005.11153v1.pdf,2020-05-19,"['cs.cl', 'cs.ai']","  Spoken dialog systems have seen applications in many domains, includingmedical for automatic conversational diagnosis. State-of-the-art dialogmanagers are usually driven by deep reinforcement learning models, such as deepQ networks (DQNs), which learn by interacting with a simulator to explore theentire action space since real conversations are limited. However, theDQN-based automatic diagnosis models do not achieve satisfying performanceswhen adapted to new, unseen diseases with only a few training samples. In thiswork, we propose the Prototypical Q Networks (ProtoQN) as the dialog managerfor the automatic diagnosis systems. The model calculates prototype embeddingswith real conversations between doctors and patients, learning from them andsimulator-augmented dialogs more efficiently. We create both supervised andfew-shot learning tasks with the Muzhi corpus. Experiments showed that theProtoQN significantly outperformed the baseline DQN model in both supervisedand few-shot learning scenarios, and achieves state-of-the-art few-shotlearning performances."
Boosting Few-Shot Learning With Adaptive Margin Loss,"['Aoxue Li', 'Weiran Huang', 'Xu Lan', 'Jiashi Feng', 'Zhenguo Li', 'Liwei Wang']",http://arxiv.org/pdf/2005.13826v1.pdf,2020-05-28,"['cs.cv', 'cs.lg', 'stat.ml']","  Few-shot learning (FSL) has attracted increasing attention in recent yearsbut remains challenging, due to the intrinsic difficulty in learning togeneralize from a few examples. This paper proposes an adaptive marginprinciple to improve the generalization ability of metric-based meta-learningapproaches for few-shot learning problems. Specifically, we first develop aclass-relevant additive margin loss, where semantic similarity between eachpair of classes is considered to separate samples in the feature embeddingspace from similar classes. Further, we incorporate the semantic context amongall classes in a sampled training task and develop a task-relevant additivemargin loss to better distinguish samples from different classes. Our adaptivemargin method can be easily extended to a more realistic generalized FSLsetting. Extensive experiments demonstrate that the proposed method can boostthe performance of current metric-based meta-learning approaches, under boththe standard FSL and generalized FSL settings."
Multi-step Estimation for Gradient-based Meta-learning,"['Jin-Hwa Kim', 'Junyoung Park', 'Yongseok Choi']",http://arxiv.org/pdf/2006.04298v1.pdf,2020-06-08,"['cs.lg', 'cs.cv', 'stat.ml']","  Gradient-based meta-learning approaches have been successful in few-shotlearning, transfer learning, and a wide range of other domains. Despite itsefficacy and simplicity, the burden of calculating the Hessian matrix withlarge memory footprints is the critical challenge in large-scale applications.To tackle this issue, we propose a simple yet straightforward method to reducethe cost by reusing the same gradient in a window of inner steps. We describethe dynamics of the multi-step estimation in the Lagrangian formalism anddiscuss how to reduce evaluating second-order derivatives estimating thedynamics. To validate our method, we experiment on meta-transfer learning andfew-shot learning tasks for multiple settings. The experiment on meta-transferemphasizes the applicability of training meta-networks, where otherapproximations are limited. For few-shot learning, we evaluate time and memorycomplexities compared with popular baselines. We show that our methodsignificantly reduces training time and memory usage, maintaining competitiveaccuracies, or even outperforming in some cases."
Simultaneous Perturbation Stochastic Approximation for Few-Shot Learning,"['Andrei Boiarov', 'Oleg Granichin', 'Olga Granichina']",http://arxiv.org/pdf/2006.05152v1.pdf,2020-06-09,"['cs.lg', 'math.oc', 'stat.ml']","  Few-shot learning is an important research field of machine learning in whicha classifier must be trained in such a way that it can adapt to new classeswhich are not included in the training set. However, only small amounts ofexamples of each class are available for training. This is one of the keyproblems with learning algorithms of this type which leads to the significantuncertainty. We attack this problem via randomized stochastic approximation. Inthis paper, we suggest to consider the new multi-task loss function and proposethe SPSA-like few-shot learning approach based on the prototypical networksmethod. We provide a theoretical justification and an analysis of experimentsfor this approach. The results of experiments on the benchmark datasetdemonstrate that the proposed method is superior to the original prototypicalnetworks."
Discrete Few-Shot Learning for Pan Privacy,"['Roei Gelbhart', 'Benjamin I. P. Rubinstein']",http://arxiv.org/pdf/2006.13120v1.pdf,2020-06-23,"['cs.lg', 'stat.ml']","  In this paper we present the first baseline results for the task of few-shotlearning of discrete embedding vectors for image recognition. Few-shot learningis a highly researched task, commonly leveraged by recognition systems that areresource constrained to train on a small number of images per class. Few-shotsystems typically store a continuous embedding vector of each class, posing arisk to privacy where system breaches or insider threats are a concern. Usingdiscrete embedding vectors, we devise a simple cryptographic protocol, whichuses one-way hash functions in order to build recognition systems that do notstore their users' embedding vectors directly, thus providing the guarantee ofcomputational pan privacy in a practical and wide-spread setting."
Few-shot Classification via Adaptive Attention,"['Zihang Jiang', 'Bingyi Kang', 'Kuangqi Zhou', 'Jiashi Feng']",http://arxiv.org/pdf/2008.02465v2.pdf,2020-08-06,['cs.cv'],"  Training a neural network model that can quickly adapt to a new task ishighly desirable yet challenging for few-shot learning problems. Recentfew-shot learning methods mostly concentrate on developing variousmeta-learning strategies from two aspects, namely optimizing an initial modelor learning a distance metric. In this work, we propose a novel few-shotlearning method via optimizing and fast adapting the query samplerepresentation based on very few reference samples. To be specific, we devise asimple and efficient meta-reweighting strategy to adapt the samplerepresentations and generate soft attention to refine the representation suchthat the relevant features from the query and support samples can be extractedfor a better few-shot classification. Such an adaptive attention model is alsoable to explain what the classification model is looking for as the evidencefor classification to some extent. As demonstrated experimentally, the proposedmodel achieves state-of-the-art classification results on various benchmarkfew-shot classification and fine-grained recognition datasets."
Few-Shot Image Classification via Contrastive Self-Supervised Learning,"['Jianyi Li', 'Guizhong Liu']",http://arxiv.org/pdf/2008.09942v1.pdf,2020-08-23,"['cs.cv', 'cs.ai']","  Most previous few-shot learning algorithms are based on meta-training withfake few-shot tasks as training samples, where large labeled base classes arerequired. The trained model is also limited by the type of tasks. In this paperwe propose a new paradigm of unsupervised few-shot learning to repair thedeficiencies. We solve the few-shot tasks in two phases: meta-training atransferable feature extractor via contrastive self-supervised learning andtraining a classifier using graph aggregation, self-distillation and manifoldaugmentation. Once meta-trained, the model can be used in any type of taskswith a task-dependent classifier training. Our method achieves state of-the-artperformance in a variety of established few-shot tasks on the standard few-shotvisual classification datasets, with an 8- 28% increase compared to theavailable unsupervised few-shot learning methods."
Interventional Few-Shot Learning,"['Zhongqi Yue', 'Hanwang Zhang', 'Qianru Sun', 'Xian-Sheng Hua']",http://arxiv.org/pdf/2009.13000v2.pdf,2020-09-28,"['cs.lg', 'cs.cv']","  We uncover an ever-overlooked deficiency in the prevailing Few-Shot Learning(FSL) methods: the pre-trained knowledge is indeed a confounder that limits theperformance. This finding is rooted from our causal assumption: a StructuralCausal Model (SCM) for the causalities among the pre-trained knowledge, samplefeatures, and labels. Thanks to it, we propose a novel FSL paradigm:Interventional Few-Shot Learning (IFSL). Specifically, we develop threeeffective IFSL algorithmic implementations based on the backdoor adjustment,which is essentially a causal intervention towards the SCM of many-shotlearning: the upper-bound of FSL in a causal view. It is worth noting that thecontribution of IFSL is orthogonal to existing fine-tuning and meta-learningbased FSL methods, hence IFSL can improve all of them, achieving a new1-/5-shot state-of-the-art on \textit{mini}ImageNet, \textit{tiered}ImageNet,and cross-domain CUB. Code is released at https://github.com/yue-zhongqi/ifsl."
Shot in the Dark: Few-Shot Learning with No Base-Class Labels,"['Zitian Chen', 'Subhransu Maji', 'Erik Learned-Miller']",http://arxiv.org/pdf/2010.02430v2.pdf,2020-10-06,['cs.cv'],"  Few-shot learning aims to build classifiers for new classes from a smallnumber of labeled examples and is commonly facilitated by access to examplesfrom a distinct set of 'base classes'. The difference in data distributionbetween the test set (novel classes) and the base classes used to learn aninductive bias often results in poor generalization on the novel classes. Toalleviate problems caused by the distribution shift, previous research hasexplored the use of unlabeled examples from the novel classes, in addition tolabeled examples of the base classes, which is known as the transductivesetting. In this work, we show that, surprisingly, off-the-shelfself-supervised learning outperforms transductive few-shot methods by 3.9% for5-shot accuracy on miniImageNet without using any base class labels. Thismotivates us to examine more carefully the role of features learned throughself-supervision in few-shot learning. Comprehensive experiments are conductedto compare the transferability, robustness, efficiency, and the complementarityof supervised and self-supervised features."
How to fine-tune deep neural networks in few-shot learning?,"['Peng Peng', 'Jiugen Wang']",http://arxiv.org/pdf/2012.00204v1.pdf,2020-12-01,"['cs.lg', 'cs.cv']","  Deep learning has been widely used in data-intensive applications. However,training a deep neural network often requires a large data set. When there isnot enough data available for training, the performance of deep learning modelsis even worse than that of shallow networks. It has been proved that few-shotlearning can generalize to new tasks with few training samples. Fine-tuning ofa deep model is simple and effective few-shot learning method. However, how tofine-tune deep learning models (fine-tune convolution layer or BN layer?) stilllack deep investigation. Hence, we study how to fine-tune deep models throughexperimental comparison in this paper. Furthermore, the weight of the models isanalyzed to verify the feasibility of the fine-tuning method."
Extended Few-Shot Learning: Exploiting Existing Resources for Novel  Tasks,"['Reza Esfandiarpoor', 'Amy Pu', 'Mohsen Hajabdollahi', 'Stephen H. Bach']",http://arxiv.org/pdf/2012.07176v3.pdf,2020-12-13,"['cs.lg', 'cs.cv', 'stat.ml']","  In many practical few-shot learning problems, even though labeled examplesare scarce, there are abundant auxiliary datasets that potentially containuseful information. We propose the problem of extended few-shot learning tostudy these scenarios. We then introduce a framework to address the challengesof efficiently selecting and effectively using auxiliary data in few-shot imageclassification. Given a large auxiliary dataset and a notion of semanticsimilarity among classes, we automatically select pseudo shots, which arelabeled examples from other classes related to the target task. We show thatnaive approaches, such as (1) modeling these additional examples the same asthe target task examples or (2) using them to learn features via transferlearning, only increase accuracy by a modest amount. Instead, we propose amasking module that adjusts the features of auxiliary data to be more similarto those of the target classes. We show that this masking module performsbetter than naively modeling the support examples and transfer learning by 4.68and 6.03 percentage points, respectively."
Model-Agnostic Graph Regularization for Few-Shot Learning,"['Ethan Shen', 'Maria Brbic', 'Nicholas Monath', 'Jiaqi Zhai', 'Manzil Zaheer', 'Jure Leskovec']",http://arxiv.org/pdf/2102.07077v1.pdf,2021-02-14,"['cs.lg', 'cs.cv']","  In many domains, relationships between categories are encoded in theknowledge graph. Recently, promising results have been achieved byincorporating knowledge graph as side information in hard classification taskswith severely limited data. However, prior models consist of highly complexarchitectures with many sub-components that all seem to impact performance. Inthis paper, we present a comprehensive empirical study on graph embeddedfew-shot learning. We introduce a graph regularization approach that allows adeeper understanding of the impact of incorporating graph information betweenlabels. Our proposed regularization is widely applicable and model-agnostic,and boosts the performance of any few-shot learning model, includingfine-tuning, metric-based, and optimization-based meta-learning. Our approachimproves the performance of strong base learners by up to 2% on Mini-ImageNetand 6.7% on ImageNet-FS, outperforming state-of-the-art graph embedded methods.Additional analyses reveal that graph regularizing models result in a lowerloss for more difficult tasks, such as those with fewer shots and lessinformative support examples."
Reinforced Attention for Few-Shot Learning and Beyond,"['Jie Hong', 'Pengfei Fang', 'Weihao Li', 'Tong Zhang', 'Christian Simon', 'Mehrtash Harandi', 'Lars Petersson']",http://arxiv.org/pdf/2104.04192v1.pdf,2021-04-09,['cs.cv'],"  Few-shot learning aims to correctly recognize query samples from unseenclasses given a limited number of support samples, often by relying on globalembeddings of images. In this paper, we propose to equip the backbone networkwith an attention agent, which is trained by reinforcement learning. The policygradient algorithm is employed to train the agent towards adaptively localizingthe representative regions on feature maps over time. We further design areward function based on the prediction of the held-out data, thus helping theattention mechanism to generalize better across the unseen classes. Theextensive experiments show, with the help of the reinforced attention, that ourembedding network has the capability to progressively generate a morediscriminative representation in few-shot learning. Moreover, experiments onthe task of image classification also show the effectiveness of the proposeddesign."
Trainable Class Prototypes for Few-Shot Learning,"['Jianyi Li', 'Guizhong Liu']",http://arxiv.org/pdf/2106.10846v1.pdf,2021-06-21,"['cs.cv', 'cs.ai', 'cs.lg']","  Metric learning is a widely used method for few shot learning in which thequality of prototypes plays a key role in the algorithm. In this paper wepropose the trainable prototypes for distance measure instead of the artificialones within the meta-training and task-training framework. Also to avoid thedisadvantages that the episodic meta-training brought, we adopt non-episodicmeta-training based on self-supervised learning. Overall we solve the few-shottasks in two phases: meta-training a transferable feature extractor viaself-supervised learning and training the prototypes for metric classification.In addition, the simple attention mechanism is used in both meta-training andtask-training. Our method achieves state-of-the-art performance in a variety ofestablished few-shot tasks on the standard few-shot visual classificationdataset, with about 20% increase compared to the available unsupervisedfew-shot learning methods."
Uniform Sampling over Episode Difficulty,"['Sébastien M. R. Arnold', 'Guneet S. Dhillon', 'Avinash Ravichandran', 'Stefano Soatto']",http://arxiv.org/pdf/2108.01662v2.pdf,2021-08-03,"['cs.lg', 'cs.ai', 'cs.cv']","  Episodic training is a core ingredient of few-shot learning to train modelson tasks with limited labelled data. Despite its success, episodic trainingremains largely understudied, prompting us to ask the question: what is thebest way to sample episodes? In this paper, we first propose a method toapproximate episode sampling distributions based on their difficulty. Buildingon this method, we perform an extensive analysis and find that samplinguniformly over episode difficulty outperforms other sampling schemes, includingcurriculum and easy-/hard-mining. As the proposed sampling method is algorithmagnostic, we can leverage these insights to improve few-shot learningaccuracies across many episodic training algorithms. We demonstrate theefficacy of our method across popular few-shot learning datasets, algorithms,network architectures, and protocols."
Generalized and Incremental Few-Shot Learning by Explicit Learning and  Calibration without Forgetting,"['Anna Kukleva', 'Hilde Kuehne', 'Bernt Schiele']",http://arxiv.org/pdf/2108.08165v1.pdf,2021-08-18,['cs.cv'],"  Both generalized and incremental few-shot learning have to deal with threemajor challenges: learning novel classes from only few samples per class,preventing catastrophic forgetting of base classes, and classifier calibrationacross novel and base classes. In this work we propose a three-stage frameworkthat allows to explicitly and effectively address these challenges. While thefirst phase learns base classes with many samples, the second phase learns acalibrated classifier for novel classes from few samples while also preventingcatastrophic forgetting. In the final phase, calibration is achieved across allclasses. We evaluate the proposed framework on four challenging benchmarkdatasets for image and video few-shot classification and obtainstate-of-the-art results for both generalized and incremental few shotlearning."
Learning Class-level Prototypes for Few-shot Learning,"['Minglei Yuan', 'Wenhai Wang', 'Tao Wang', 'Chunhao Cai', 'Qian Xu', 'Tong Lu']",http://arxiv.org/pdf/2108.11072v1.pdf,2021-08-25,"['cs.cv', 'cs.lg']","  Few-shot learning aims to recognize new categories using very few labeledsamples. Although few-shot learning has witnessed promising development inrecent years, most existing methods adopt an average operation to calculateprototypes, thus limited by the outlier samples. In this work, we propose asimple yet effective framework for few-shot classification, which can learn togenerate preferable prototypes from few support data, with the help of anepisodic prototype generator module. The generated prototype is meant to beclose to a certain \textit{\targetproto{}} and is less influenced by outliersamples. Extensive experiments demonstrate the effectiveness of this module,and our approach gets a significant raise over baseline models, and get acompetitive result compared to previous methods on \textit{mini}ImageNet,\textit{tiered}ImageNet, and cross-domain (\textit{mini}ImageNet $\rightarrow$CUB-200-2011) datasets."
Self-training Improves Pre-training for Few-shot Learning in  Task-oriented Dialog Systems,"['Fei Mi', 'Wanhao Zhou', 'Fengyu Cai', 'Lingjing Kong', 'Minlie Huang', 'Boi Faltings']",http://arxiv.org/pdf/2108.12589v1.pdf,2021-08-28,['cs.cl'],"  As the labeling cost for different modules in task-oriented dialog (ToD)systems is expensive, a major challenge is to train different modules with theleast amount of labeled data. Recently, large-scale pre-trained languagemodels, have shown promising results for few-shot learning in ToD. In thispaper, we devise a self-training approach to utilize the abundant unlabeleddialog data to further improve state-of-the-art pre-trained models in few-shotlearning scenarios for ToD systems. Specifically, we propose a self-trainingapproach that iteratively labels the most confident unlabeled data to train astronger Student model. Moreover, a new text augmentation technique (GradAug)is proposed to better train the Student by replacing non-crucial tokens using amasked language model. We conduct extensive experiments and present analyses onfour downstream tasks in ToD, including intent classification, dialog statetracking, dialog act prediction, and response selection. Empirical resultsdemonstrate that the proposed self-training approach consistently improvesstate-of-the-art pre-trained models (BERT, ToD-BERT) when only a small numberof labeled data are available."
CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented  Dialog Systems,"['Fei Mi', 'Yitong Li', 'Yasheng Wang', 'Xin Jiang', 'Qun Liu']",http://arxiv.org/pdf/2109.04645v4.pdf,2021-09-10,"['cs.cl', 'cs.lg']","  As labeling cost for different modules in task-oriented dialog (ToD) systemsis high, a major challenge in practice is to learn different tasks with theleast amount of labeled data. Recently, prompting methods over pre-trainedlanguage models (PLMs) have shown promising results for few-shot learning inToD. To better utilize the power of PLMs, this paper proposes ComprehensiveInstruction (CINS) that exploits PLMs with extra task-specific instructions. Wedesign a schema (definition, constraint, prompt) of instructions and theircustomized realizations for three important downstream tasks in ToD, i.e.intent classification, dialog state tracking, and natural language generation.A sequence-to-sequence model (T5) is adopted to solve these three tasks in aunified framework. Extensive experiments are conducted on these ToD tasks inrealistic few-shot learning scenarios with small validation data. Empiricalresults demonstrate that the proposed CINS approach consistently improvestechniques that finetune PLMs with raw input or short prompts."
Rapid Model Architecture Adaption for Meta-Learning,"['Yiren Zhao', 'Xitong Gao', 'Ilia Shumailov', 'Nicolo Fusi', 'Robert Mullins']",http://arxiv.org/pdf/2109.04925v1.pdf,2021-09-10,['cs.lg'],"  Network Architecture Search (NAS) methods have recently gathered muchattention. They design networks with better performance and use a much shortersearch time compared to traditional manual tuning. Despite their efficiency inmodel deployments, most NAS algorithms target a single task on a fixed hardwaresystem. However, real-life few-shot learning environments often cover a greatnumber of tasks (T ) and deployments on a wide variety of hardware platforms (H).  The combinatorial search complexity T times H creates a fundamental searchefficiency challenge if one naively applies existing NAS methods to thesescenarios. To overcome this issue, we show, for the first time, how to rapidlyadapt model architectures to new tasks in a many-task many-hardware few-shotlearning setup by integrating Model Agnostic Meta Learning (MAML) into the NASflow. The proposed NAS method (H-Meta-NAS) is hardware-aware and performsoptimisation in the MAML framework. H-Meta-NAS shows a Pareto dominancecompared to a variety of NAS and manual baselines in popular few-shot learningbenchmarks with various hardware platforms and constraints. In particular, onthe 5-way 1-shot Mini-ImageNet classification task, the proposed methodoutperforms the best manual baseline by a large margin (5.21% in accuracy)using 60% less computation."
Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation,"['Chujie Zheng', 'Minlie Huang']",http://arxiv.org/pdf/2109.06513v2.pdf,2021-09-14,['cs.cl'],"  Dialog models can be greatly strengthened through grounding on variousexternal information, but grounded dialog corpora are usually not naturallyaccessible. In this work, we focus on the few-shot learning for grounded dialoggeneration (GDG). We first propose a simple prompting method for GDG tasks,where different constructs of model input, such as the grounding source and theconversation context, are distinguished through continuous or discrete prompts.On three typical GDG tasks, we empirically demonstrate and analyze in-depth theeffectiveness of our method. We then conduct extensive experiments tothoroughly investigate how our prompting method works with differentpre-trained models. We show that prompted language models perform superiorly toconversational models, and further analyze various factors that influence theeffects of prompting. Overall, our work introduces a prompt-based perspectiveto the few-shot learning for GDG tasks, and provides valuable findings andinsights for future research."
Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning,"['Sungyong Baik', 'Janghoon Choi', 'Heewon Kim', 'Dohee Cho', 'Jaesik Min', 'Kyoung Mu Lee']",http://arxiv.org/pdf/2110.03909v2.pdf,2021-10-08,"['cs.lg', 'cs.cv']","  In few-shot learning scenarios, the challenge is to generalize and performwell on new unseen examples when only very few labeled examples are availablefor each task. Model-agnostic meta-learning (MAML) has gained the popularity asone of the representative few-shot learning methods for its flexibility andapplicability to diverse problems. However, MAML and its variants often resortto a simple loss function without any auxiliary loss function or regularizationterms that can help achieve better generalization. The problem lies in thateach application and task may require different auxiliary loss function,especially when tasks are diverse and distinct. Instead of attempting tohand-design an auxiliary loss function for each application and task, weintroduce a new meta-learning framework with a loss function that adapts toeach task. Our proposed framework, named Meta-Learning with Task-Adaptive LossFunction (MeTAL), demonstrates the effectiveness and the flexibility acrossvarious domains, such as few-shot classification and few-shot regression."
A Strong Baseline for Semi-Supervised Incremental Few-Shot Learning,"['Linlan Zhao', 'Dashan Guo', 'Yunlu Xu', 'Liang Qiao', 'Zhanzhan Cheng', 'Shiliang Pu', 'Yi Niu', 'Xiangzhong Fang']",http://arxiv.org/pdf/2110.11128v2.pdf,2021-10-21,['cs.cv'],"  Few-shot learning (FSL) aims to learn models that generalize to novel classeswith limited training samples. Recent works advance FSL towards a scenariowhere unlabeled examples are also available and propose semi-supervised FSLmethods. Another line of methods also cares about the performance of baseclasses in addition to the novel ones and thus establishes the incremental FSLscenario. In this paper, we generalize the above two under a more realistic yetcomplex setting, named by Semi-Supervised Incremental Few-Shot Learning (S2I-FSL). To tackle the task, we propose a novel paradigm containing two parts:(1) a well-designed meta-training algorithm for mitigating ambiguity betweenbase and novel classes caused by unreliable pseudo labels and (2) a modeladaptation mechanism to learn discriminative features for novel classes whilepreserving base knowledge using few labeled and all the unlabeled data.Extensive experiments on standard FSL, semi-supervised FSL, incremental FSL,and the firstly built S2 I-FSL benchmarks demonstrate the effectiveness of ourproposed method."
Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning by  Finding Flat Minima,"['Guangyuan Shi', 'Jiaxin Chen', 'Wenlong Zhang', 'Li-Ming Zhan', 'Xiao-Ming Wu']",http://arxiv.org/pdf/2111.01549v2.pdf,2021-10-30,"['cs.lg', 'cs.cv']","  This paper considers incremental few-shot learning, which requires a model tocontinually recognize new categories with only a few examples provided. Ourstudy shows that existing methods severely suffer from catastrophic forgetting,a well-known problem in incremental learning, which is aggravated due to datascarcity and imbalance in the few-shot setting. Our analysis further suggeststhat to prevent catastrophic forgetting, actions need to be taken in theprimitive stage -- the training of base classes instead of later few-shotlearning sessions. Therefore, we propose to search for flat local minima of thebase training objective function and then fine-tune the model parameters withinthe flat region on new tasks. In this way, the model can efficiently learn newclasses while preserving the old ones. Comprehensive experimental resultsdemonstrate that our approach outperforms all prior state-of-the-art methodsand is very close to the approximate upper bound. The source code is availableat https://github.com/moukamisama/F2M."
Few-Shot Image Classification Along Sparse Graphs,"['Joseph F Comer', 'Philip L Jacobson', 'Heiko Hoffmann']",http://arxiv.org/pdf/2112.03951v1.pdf,2021-12-07,['cs.cv'],"  Few-shot learning remains a challenging problem, with unsatisfactory 1-shotaccuracies for most real-world data. Here, we present a different perspectivefor data distributions in the feature space of a deep network and show how toexploit it for few-shot learning. First, we observe that nearest neighbors inthe feature space are with high probability members of the same class whilegenerally two random points from one class are not much closer to each otherthan points from different classes. This observation suggests that classes infeature space form sparse, loosely connected graphs instead of dense clusters.To exploit this property, we propose using a small amount of label propagationinto the unlabeled space and then using a kernel PCA reconstruction error asdecision boundary for the feature-space data distribution of each class. Usingthis method, which we call ""K-Prop,"" we demonstrate largely improved few-shotlearning performances (e.g., 83% accuracy for 1-shot 5-way classification onthe RESISC45 satellite-images dataset) for datasets for which a backbonenetwork can be trained with high within-class nearest-neighbor probabilities.We demonstrate this relationship using six different datasets."
Exploring the Limits of Natural Language Inference Based Setup for  Few-Shot Intent Detection,"['Vijit Malik', 'Ayush Kumar', 'Jithendra Veppa']",http://arxiv.org/pdf/2112.07434v1.pdf,2021-12-14,"['cs.cl', 'cs.lg']","  One of the core components of goal-oriented dialog systems is the task ofIntent Detection. Few-shot Learning upon Intent Detection is challenging due tothe scarcity of available annotated utterances. Although recent works makinguse of metric-based and optimization-based methods have been proposed, the taskis still challenging in large label spaces and much smaller number of shots.Generalized Few-shot learning is more difficult due to the presence of bothnovel and seen classes during the testing phase. In this work, we propose asimple and effective method based on Natural Language Inference that not onlytackles the problem of few shot intent detection, but also proves useful inzero-shot and generalized few shot learning problems. Our extensive experimentson a number of Natural Language Understanding (NLU) and Spoken LanguageUnderstanding (SLU) datasets show the effectiveness of our approach. Inaddition, we highlight the settings in which our NLI based method outperformsthe baselines by huge margins."
Ontology-enhanced Prompt-tuning for Few-shot Learning,"['Hongbin Ye', 'Ningyu Zhang', 'Shumin Deng', 'Xiang Chen', 'Hui Chen', 'Feiyu Xiong', 'Xi Chen', 'Huajun Chen']",http://arxiv.org/pdf/2201.11332v1.pdf,2022-01-27,"['cs.cl', 'cs.ai', 'cs.ir', 'cs.lg']","  Few-shot Learning (FSL) is aimed to make predictions based on a limitednumber of samples. Structured data such as knowledge graphs and ontologylibraries has been leveraged to benefit the few-shot setting in various tasks.However, the priors adopted by the existing methods suffer from challengingknowledge missing, knowledge noise, and knowledge heterogeneity, which hinderthe performance for few-shot learning. In this study, we explore knowledgeinjection for FSL with pre-trained language models and proposeontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop theontology transformation based on the external knowledge graph to address theknowledge missing issue, which fulfills and converts structure knowledge totext. We further introduce span-sensitive knowledge injection via a visiblematrix to select informative knowledge to handle the knowledge noise issue. Tobridge the gap between knowledge and text, we propose a collective trainingalgorithm to optimize representations jointly. We evaluate our proposedOntoPrompt in three tasks, including relation extraction, event extraction, andknowledge graph completion, with eight datasets. Experimental resultsdemonstrate that our approach can obtain better few-shot performance thanbaselines."
Smoothed Embeddings for Certified Few-Shot Learning,"['Mikhail Pautov', 'Olesya Kuznetsova', 'Nurislam Tursynbek', 'Aleksandr Petiushko', 'Ivan Oseledets']",http://arxiv.org/pdf/2202.01186v2.pdf,2022-02-02,"['cs.lg', 'cs.ai']","  Randomized smoothing is considered to be the state-of-the-art provabledefense against adversarial perturbations. However, it heavily exploits thefact that classifiers map input objects to class probabilities and do not focuson the ones that learn a metric space in which classification is performed bycomputing distances to embeddings of classes prototypes. In this work, weextend randomized smoothing to few-shot learning models that map inputs tonormalized embeddings. We provide analysis of Lipschitz continuity of suchmodels and derive robustness certificate against $\ell_2$-bounded perturbationsthat may be useful in few-shot learning scenarios. Our theoretical results areconfirmed by experiments on different datasets."
Few-Shot Learning on Graphs,"['Chuxu Zhang', 'Kaize Ding', 'Jundong Li', 'Xiangliang Zhang', 'Yanfang Ye', 'Nitesh V. Chawla', 'Huan Liu']",http://arxiv.org/pdf/2203.09308v2.pdf,2022-03-17,['cs.lg'],"  Graph representation learning has attracted tremendous attention due to itsremarkable performance in many real-world applications. However, prevailingsupervised graph representation learning models for specific tasks often sufferfrom label sparsity issue as data labeling is always time and resourceconsuming. In light of this, few-shot learning on graphs (FSLG), which combinesthe strengths of graph representation learning and few-shot learning together,has been proposed to tackle the performance degradation in face of limitedannotated data challenge. There have been many studies working on FSLGrecently. In this paper, we comprehensively survey these work in the form of aseries of methods and applications. Specifically, we first introduce FSLGchallenges and bases, then categorize and summarize existing work of FSLG interms of three major graph mining tasks at different granularity levels, i.e.,node, edge, and graph. Finally, we share our thoughts on some future researchdirections of FSLG. The authors of this survey have contributed significantlyto the AI literature on FSLG over the last few years."
A Framework of Meta Functional Learning for Regularising Knowledge  Transfer,"['Pan Li', 'Yanwei Fu', 'Shaogang Gong']",http://arxiv.org/pdf/2203.14840v1.pdf,2022-03-28,"['cs.lg', 'cs.cv']","  Machine learning classifiers' capability is largely dependent on the scale ofavailable training data and limited by the model overfitting in data-scarcelearning tasks. To address this problem, this work proposes a novel frameworkof Meta Functional Learning (MFL) by meta-learning a generalisable functionalmodel from data-rich tasks whilst simultaneously regularising knowledgetransfer to data-scarce tasks. The MFL computes meta-knowledge on functionalregularisation generalisable to different learning tasks by which functionaltraining on limited labelled data promotes more discriminative functions to belearned. Based on this framework, we formulate three variants of MFL: MFL withPrototypes (MFL-P) which learns a functional by auxiliary prototypes, CompositeMFL (ComMFL) that transfers knowledge from both functional space andrepresentational space, and MFL with Iterative Updates (MFL-IU) which improvesknowledge transfer regularisation from MFL by progressively learning thefunctional regularisation in knowledge transfer. Moreover, we generalise thesevariants for knowledge transfer regularisation from binary classifiers tomulti-class classifiers. Extensive experiments on two few-shot learningscenarios, Few-Shot Learning (FSL) and Cross-Domain Few-Shot Learning (CD-FSL),show that meta functional learning for knowledge transfer regularisation canimprove FSL classifiers."
Integrative Few-Shot Learning for Classification and Segmentation,"['Dahyun Kang', 'Minsu Cho']",http://arxiv.org/pdf/2203.15712v2.pdf,2022-03-29,['cs.cv'],"  We introduce the integrative task of few-shot classification and segmentation(FS-CS) that aims to both classify and segment target objects in a query imagewhen the target classes are given with a few examples. This task combines twoconventional few-shot learning problems, few-shot classification andsegmentation. FS-CS generalizes them to more realistic episodes with arbitraryimage pairs, where each target class may or may not be present in the query. Toaddress the task, we propose the integrative few-shot learning (iFSL) frameworkfor FS-CS, which trains a learner to construct class-wise foreground maps formulti-label classification and pixel-wise segmentation. We also develop aneffective iFSL model, attentive squeeze network (ASNet), that leverages deepsemantic correlation and global self-attention to produce reliable foregroundmaps. In experiments, the proposed method shows promising performance on theFS-CS task and also achieves the state of the art on standard few-shotsegmentation benchmarks."
GDC- Generalized Distribution Calibration for Few-Shot Learning,"['Shakti Kumar', 'Hussain Zaidi']",http://arxiv.org/pdf/2204.05230v1.pdf,2022-04-11,"['cs.lg', 'cs.cv']","  Few shot learning is an important problem in machine learning as largelabelled datasets take considerable time and effort to assemble. Most few-shotlearning algorithms suffer from one of two limitations- they either require thedesign of sophisticated models and loss functions, thus hamperinginterpretability; or employ statistical techniques but make assumptions thatmay not hold across different datasets or features. Developing on recent workin extrapolating distributions of small sample classes from the most similarlarger classes, we propose a Generalized sampling method that learns toestimate few-shot distributions for classification as weighted random variablesof all large classes. We use a form of covariance shrinkage to providerobustness against singular covariances due to overparameterized features orsmall datasets. We show that our sampled points are close to few-shot classeseven in cases when there are no similar large classes in the training set. Ourmethod works with arbitrary off-the-shelf feature extractors and outperformsexisting state-of-the-art on miniImagenet, CUB and Stanford Dogs datasets by 3%to 5% on 5way-1shot and 5way-5shot tasks and by 1% in challenging cross domaintasks."
Few-shot Learning with Noisy Labels,"['Kevin J Liang', 'Samrudhdhi B. Rangrej', 'Vladan Petrovic', 'Tal Hassner']",http://arxiv.org/pdf/2204.05494v2.pdf,2022-04-12,"['cs.cv', 'stat.ml']","  Few-shot learning (FSL) methods typically assume clean support sets withaccurately labeled samples when training on novel classes. This assumption canoften be unrealistic: support sets, no matter how small, can still includemislabeled samples. Robustness to label noise is therefore essential for FSLmethods to be practical, but this problem surprisingly remains largelyunexplored. To address mislabeled samples in FSL settings, we make severaltechnical contributions. (1) We offer simple, yet effective, featureaggregation methods, improving the prototypes used by ProtoNet, a popular FSLtechnique. (2) We describe a novel Transformer model for Noisy Few-ShotLearning (TraNFS). TraNFS leverages a transformer's attention mechanism toweigh mislabeled versus correct samples. (3) Finally, we extensively test thesemethods on noisy versions of MiniImageNet and TieredImageNet. Our results showthat TraNFS is on-par with leading FSL methods on clean support sets, yetoutperforms them, by far, in the presence of label noise."
Impossible Triangle: What's Next for Pre-trained Language Models?,"['Chenguang Zhu', 'Michael Zeng']",http://arxiv.org/pdf/2204.06130v2.pdf,2022-04-13,['cs.cl'],"  Recent development of large-scale pre-trained language models (PLM) havesignificantly improved the capability of models in various NLP tasks, in termsof performance after task-specific fine-tuning and zero-shot / few-shotlearning. However, many of such models come with a dauntingly huge size thatfew institutions can afford to pre-train, fine-tune or even deploy, whilemoderate-sized models usually lack strong generalized few-shot learningcapabilities. In this paper, we first elaborate the current obstacles of usingPLM models in terms of the Impossible Triangle: 1) moderate model size, 2)state-of-the-art few-shot learning capability, and 3) state-of-the-artfine-tuning capability. We argue that all existing PLM models lack one or moreproperties from the Impossible Triangle. To remedy these missing properties ofPLMs, various techniques have been proposed, such as knowledge distillation,data augmentation and prompt learning, which inevitably brings additional workto the application of PLMs in real scenarios. We then offer insights intofuture research directions of PLMs to achieve the Impossible Triangle, andbreak down the task into several key phases."
A Study on Prompt-based Few-Shot Learning Methods for Belief State  Tracking in Task-oriented Dialog Systems,"['Debjoy Saha', 'Bishal Santra', 'Pawan Goyal']",http://arxiv.org/pdf/2204.08167v1.pdf,2022-04-18,"['cs.cl', 'cs.ai']","  We tackle the Dialogue Belief State Tracking(DST) problem of task-orientedconversational systems. Recent approaches to this problem leveragingTransformer-based models have yielded great results. However, training thesemodels is expensive, both in terms of computational resources and time.Additionally, collecting high quality annotated dialogue datasets remains achallenge for researchers because of the extensive annotation required fortraining these models. Driven by the recent success of pre-trained languagemodels and prompt-based learning, we explore prompt-based few-shot learning forDialogue Belief State Tracking. We formulate the DST problem as a 2-stageprompt-based language modelling task and train language models for both tasksand present a comprehensive empirical analysis of their separate and jointperformance. We demonstrate the potential of prompt-based methods in few-shotlearning for DST and provide directions for future improvement."
Meta-free few-shot learning via representation learning with weight  averaging,"['Kuilin Chen', 'Chi-Guhn Lee']",http://arxiv.org/pdf/2204.12466v2.pdf,2022-04-26,"['cs.lg', 'cs.cv']","  Recent studies on few-shot classification using transfer learning posechallenges to the effectiveness and efficiency of episodic meta-learningalgorithms. Transfer learning approaches are a natural alternative, but theyare restricted to few-shot classification. Moreover, little attention has beenon the development of probabilistic models with well-calibrated uncertaintyfrom few-shot samples, except for some Bayesian episodic learning algorithms.To tackle the aforementioned issues, we propose a new transfer learning methodto obtain accurate and reliable models for few-shot regression andclassification. The resulting method does not require episodic meta-learningand is called meta-free representation learning (MFRL). MFRL first findslow-rank representation generalizing well on meta-test tasks. Given the learnedrepresentation, probabilistic linear models are fine-tuned with few-shotsamples to obtain models with well-calibrated uncertainty. The proposed methodnot only achieves the highest accuracy on a wide range of few-shot learningbenchmark datasets but also correctly quantifies the prediction uncertainty. Inaddition, weight averaging and temperature scaling are effective in improvingthe accuracy and reliability of few-shot learning in existing meta-learningalgorithms with a wide range of learning paradigms and model architectures."
Single Morphing Attack Detection using Siamese Network and Few-shot  Learning,"['Juan Tapia', 'Daniel Schulz', 'Christoph Busch']",http://arxiv.org/pdf/2206.10969v1.pdf,2022-06-22,['cs.cv'],"  Face morphing attack detection is challenging and presents a concrete andsevere threat for face verification systems. Reliable detection mechanisms forsuch attacks, which have been tested with a robust cross-database protocol andunknown morphing tools still is a research challenge. This paper proposes aframework following the Few-Shot-Learning approach that shares imageinformation based on the siamese network using triplet-semi-hard-loss to tacklethe morphing attack detection and boost the clustering classification process.This network compares a bona fide or potentially morphed image with triplets ofmorphing and bona fide face images. Our results show that this new networkcluster the data points, and assigns them to classes in order to obtain a lowerequal error rate in a cross-database scenario sharing only small image numbersfrom an unknown database. Few-shot learning helps to boost the learningprocess. Experimental results using a cross-datasets trained with FRGCv2 andtested with FERET and the AMSL open-access databases reduced the BPCER10 from43% to 4.91% using ResNet50 and 5.50% for MobileNetV2."
Cross-Domain Cross-Set Few-Shot Learning via Learning Compact and  Aligned Representations,"['Wentao Chen', 'Zhang Zhang', 'Wei Wang', 'Liang Wang', 'Zilei Wang', 'Tieniu Tan']",http://arxiv.org/pdf/2207.07826v1.pdf,2022-07-16,['cs.cv'],"  Few-shot learning (FSL) aims to recognize novel queries with only a fewsupport samples through leveraging prior knowledge from a base dataset. In thispaper, we consider the domain shift problem in FSL and aim to address thedomain gap between the support set and the query set. Different from previouscross-domain FSL work (CD-FSL) that considers the domain shift between base andnovel classes, the new problem, termed cross-domain cross-set FSL (CDSC-FSL),requires few-shot learners not only to adapt to the new domain, but also to beconsistent between different domains within each novel class. To this end, wepropose a novel approach, namely stabPA, to learn prototypical compact andcross-domain aligned representations, so that the domain shift and few-shotlearning can be addressed simultaneously. We evaluate our approach on two newCDCS-FSL benchmarks built from the DomainNet and Office-Home datasetsrespectively. Remarkably, our approach outperforms multiple elaboratedbaselines by a large margin, e.g., improving 5-shot accuracy by 6.0 points onaverage on DomainNet. Code is available athttps://github.com/WentaoChen0813/CDCS-FSL"
Kernel Relative-prototype Spectral Filtering for Few-shot Learning,"['Tao Zhang', 'Wu Huang']",http://arxiv.org/pdf/2207.11685v1.pdf,2022-07-24,"['cs.cv', 'cs.ai']","  Few-shot learning performs classification tasks and regression tasks onscarce samples. As one of the most representative few-shot learning models,Prototypical Network represents each class as sample average, or a prototype,and measures the similarity of samples and prototypes by Euclidean distance. Inthis paper, we propose a framework of spectral filtering (shrinkage) formeasuring the difference between query samples and prototypes, or namely therelative prototypes, in a reproducing kernel Hilbert space (RKHS). In thisframework, we further propose a method utilizing Tikhonov regularization as thefilter function for few-shot classification. We conduct several experiments toverify our method utilizing different kernels based on the miniImageNetdataset, tiered-ImageNet dataset and CIFAR-FS dataset. The experimental resultsshow that the proposed model can perform the state-of-the-art. In addition, theexperimental results show that the proposed shrinkage method can boost theperformance. Source code is available at https://github.com/zhangtao2022/DSFN."
Continual Few-Shot Learning with Adversarial Class Storage,"['Kun Wu', 'Chengxiang Yin', 'Jian Tang', 'Zhiyuan Xu', 'Yanzhi Wang', 'Dejun Yang']",http://arxiv.org/pdf/2207.12303v1.pdf,2022-07-10,"['cs.lg', 'cs.ai', 'cs.cv']","  Humans have a remarkable ability to quickly and effectively learn newconcepts in a continuous manner without forgetting old knowledge. Though deeplearning has made tremendous successes on various computer vision tasks, itfaces challenges for achieving such human-level intelligence. In this paper, wedefine a new problem called continual few-shot learning, in which tasks arrivesequentially and each task is associated with a few training samples. Wepropose Continual Meta-Learner (CML) to solve this problem. CML integratesmetric-based classification and a memory-based mechanism along with adversariallearning into a meta-learning framework, which leads to the desirableproperties: 1) it can quickly and effectively learn to handle a new task; 2) itovercomes catastrophic forgetting; 3) it is model-agnostic. We conductextensive experiments on two image datasets, MiniImageNet and CIFAR100.Experimental results show that CML delivers state-of-the-art performance interms of classification accuracy on few-shot learning tasks withoutcatastrophic forgetting."
"Conservative Generator, Progressive Discriminator: Coordination of  Adversaries in Few-shot Incremental Image Synthesis","['Chaerin Kong', 'Nojun Kwak']",http://arxiv.org/pdf/2207.14491v1.pdf,2022-07-29,['cs.cv'],"  The capacity to learn incrementally from an online stream of data is anenvied trait of human learners, as deep neural networks typically suffer fromcatastrophic forgetting and stability-plasticity dilemma. Several works havepreviously explored incremental few-shot learning, a task with greaterchallenges due to data constraint, mostly in classification setting with mildsuccess. In this work, we study the underrepresented task of generativeincremental few-shot learning. To effectively handle the inherent challenges ofincremental learning and few-shot learning, we propose a novel framework namedConPro that leverages the two-player nature of GANs. Specifically, we design aconservative generator that preserves past knowledge in parameter and computeefficient manner, and a progressive discriminator that learns to reasonsemantic distances between past and present task samples, minimizingoverfitting with few data points and pursuing good forward transfer. We presentexperiments to validate the effectiveness of ConPro."
Constrained Few-Shot Learning: Human-Like Low Sample Complexity Learning  and Non-Episodic Text Classification,"['Jaron Mar', 'Jiamou Liu']",http://arxiv.org/pdf/2208.08089v2.pdf,2022-08-17,"['cs.lg', 'cs.cl']","  Few-shot learning (FSL) is an emergent paradigm of learning that attempts tolearn to reason with low sample complexity to mimic the way humans learn,generalise and extrapolate from only a few seen examples. While FSL attempts tomimic these human characteristics, fundamentally, the task of FSL asconventionally formulated using meta-learning with episodic-based training doesnot in actuality align with how humans acquire and reason with knowledge. FSLwith episodic training, while only requires $K$ instances of each test class,still requires a large number of labelled training instances from disjointclasses. In this paper, we introduce the novel task of constrained few-shotlearning (CFSL), a special case of FSL where $M$, the number of instances ofeach training class is constrained such that $M \leq K$ thus applying a similarrestriction during FSL training and test. We propose a method for CFSLleveraging Cat2Vec using a novel categorical contrastive loss inspired bycognitive theories such as fuzzy trace theory and prototype theory."
How to Prompt? Opportunities and Challenges of Zero- and Few-Shot  Learning for Human-AI Interaction in Creative Applications of Generative  Models,"['Hai Dang', 'Lukas Mecke', 'Florian Lehmann', 'Sven Goller', 'Daniel Buschek']",http://arxiv.org/pdf/2209.01390v1.pdf,2022-09-03,"['cs.hc', 'cs.cl', 'h.5.2; i.2.7']","  Deep generative models have the potential to fundamentally change the way wecreate high-fidelity digital content but are often hard to control. Prompting agenerative model is a promising recent development that in principle enablesend-users to creatively leverage zero-shot and few-shot learning to assign newtasks to an AI ad-hoc, simply by writing them down. However, for the majorityof end-users writing effective prompts is currently largely a trial and errorprocess. To address this, we discuss the key opportunities and challenges forinteractive creative applications that use prompting as a new paradigm forHuman-AI interaction. Based on our analysis, we propose four design goals foruser interfaces that support prompting. We illustrate these with concrete UIdesign sketches, focusing on the use case of creative writing. The researchcommunity in HCI and AI can take these as starting points to develop adequateuser interfaces for models capable of zero- and few-shot learning."
Less is More: Rethinking Few-Shot Learning and Recurrent Neural Nets,"['Deborah Pereg', 'Martin Villiger', 'Brett Bouma', 'Polina Golland']",http://arxiv.org/pdf/2209.14267v2.pdf,2022-09-28,"['cs.lg', 'cs.cv']","  The statistical supervised learning framework assumes an input-output setwith a joint probability distribution that is reliably represented by thetraining dataset. The learner is then required to output a prediction rulelearned from the training dataset's input-output pairs. In this work, weprovide meaningful insights into the asymptotic equipartition property (AEP)\citep{Shannon:1948} in the context of machine learning, and illuminate some ofits potential ramifications for few-shot learning. We provide theoreticalguarantees for reliable learning under the information-theoretic AEP, and forthe generalization error with respect to the sample size. We then focus on ahighly efficient recurrent neural net (RNN) framework and propose areduced-entropy algorithm for few-shot learning. We also propose a mathematicalintuition for the RNN as an approximation of a sparse coding solver. We verifythe applicability, robustness, and computational efficiency of the proposedapproach with image deblurring and optical coherence tomography (OCT) specklesuppression. Our experimental results demonstrate significant potential forimproving learning models' sample efficiency, generalization, and timecomplexity, that can therefore be leveraged for practical real-timeapplications."
Contrastive Graph Few-Shot Learning,"['Chunhui Zhang', 'Hongfu Liu', 'Jundong Li', 'Yanfang Ye', 'Chuxu Zhang']",http://arxiv.org/pdf/2210.00084v1.pdf,2022-09-30,['cs.lg'],"  Prevailing deep graph learning models often suffer from label sparsity issue.Although many graph few-shot learning (GFL) methods have been developed toavoid performance degradation in face of limited annotated data, theyexcessively rely on labeled data, where the distribution shift in the testphase might result in impaired generalization ability. Additionally, they lacka general purpose as their designs are coupled with task or data-specificcharacteristics. To this end, we propose a general and effective ContrastiveGraph Few-shot Learning framework (CGFL). CGFL leverages a self-distilledcontrastive learning procedure to boost GFL. Specifically, our model firstlypre-trains a graph encoder with contrastive learning using unlabeled data.Later, the trained encoder is frozen as a teacher model to distill a studentmodel with a contrastive loss. The distilled model is finally fed to GFL. CGFLlearns data representation in a self-supervised manner, thus mitigating thedistribution shift impact for better generalization and making model task anddata-independent for a general graph mining purpose. Furthermore, we introducean information-based method to quantitatively measure the capability of CGFL.Comprehensive experiments demonstrate that CGFL outperforms state-of-the-artbaselines on several graph mining tasks in the few-shot scenario. We alsoprovide quantitative measurement of CGFL's success."
A Unified Framework with Meta-dropout for Few-shot Learning,"['Shaobo Lin', 'Xingyu Zeng', 'Rui Zhao']",http://arxiv.org/pdf/2210.06409v1.pdf,2022-10-12,"['cs.cv', 'cs.ai']","  Conventional training of deep neural networks usually requires a substantialamount of data with expensive human annotations. In this paper, we utilize theidea of meta-learning to explain two very different streams of few-shotlearning, i.e., the episodic meta-learning-based and pre-train finetune-basedfew-shot learning, and form a unified meta-learning framework. In order toimprove the generalization power of our framework, we propose a simple yeteffective strategy named meta-dropout, which is applied to the transferableknowledge generalized from base categories to novel categories. The proposedstrategy can effectively prevent neural units from co-adapting excessively inthe meta-training stage. Extensive experiments on the few-shot object detectionand few-shot image classification datasets, i.e., Pascal VOC, MS COCO, CUB, andmini-ImageNet, validate the effectiveness of our method."
tSF: Transformer-based Semantic Filter for Few-Shot Learning,"['Jinxiang Lai', 'Siqian Yang', 'Wenlong Liu', 'Yi Zeng', 'Zhongyi Huang', 'Wenlong Wu', 'Jun Liu', 'Bin-Bin Gao', 'Chengjie Wang']",http://arxiv.org/pdf/2211.00868v1.pdf,2022-11-02,['cs.cv'],"  Few-Shot Learning (FSL) alleviates the data shortage challenge via embeddingdiscriminative target-aware features among plenty seen (base) and few unseen(novel) labeled samples. Most feature embedding modules in recent FSL methodsare specially designed for corresponding learning tasks (e.g., classification,segmentation, and object detection), which limits the utility of embeddingfeatures. To this end, we propose a light and universal module namedtransformer-based Semantic Filter (tSF), which can be applied for different FSLtasks. The proposed tSF redesigns the inputs of a transformer-based structureby a semantic filter, which not only embeds the knowledge from whole base setto novel set but also filters semantic features for target category.Furthermore, the parameters of tSF is equal to half of a standard transformerblock (less than 1M). In the experiments, our tSF is able to boost theperformances in different classic few-shot learning tasks (about 2%improvement), especially outperforms the state-of-the-arts on multiplebenchmark datasets in few-shot classification task."
Interpretable Few-shot Learning with Online Attribute Selection,"['Mohammad Reza Zarei', 'Majid Komeili']",http://arxiv.org/pdf/2211.09107v2.pdf,2022-11-16,"['cs.lg', 'cs.cv']","  Few-shot learning (FSL) is a challenging learning problem in which only a fewsamples are available for each class. Decision interpretation is more importantin few-shot classification since there is a greater chance of error than intraditional classification. However, most of the previous FSL methods areblack-box models. In this paper, we propose an inherently interpretable modelfor FSL based on human-friendly attributes. Moreover, we propose an onlineattribute selection mechanism that can effectively filter out irrelevantattributes in each episode. The attribute selection mechanism improves theaccuracy and helps with interpretability by reducing the number of participatedattributes in each episode. We propose a mechanism that automatically detectsthe episodes where the pool of human-friendly attributes are not adequate, andcompensates by engaging learned unknown attributes. We demonstrate that theproposed method achieves results on par with black-box few-shot-learning modelson four widely used datasets."
On Measuring the Intrinsic Few-Shot Hardness of Datasets,"['Xinran Zhao', 'Shikhar Murty', 'Christopher D. Manning']",http://arxiv.org/pdf/2211.09113v1.pdf,2022-11-16,['cs.cl'],"  While advances in pre-training have led to dramatic improvements in few-shotlearning of NLP tasks, there is limited understanding of what drives successfulfew-shot adaptation in datasets. In particular, given a new dataset and apre-trained model, what properties of the dataset make it \emph{few-shotlearnable} and are these properties independent of the specific adaptationtechniques used? We consider an extensive set of recent few-shot learningmethods, and show that their performance across a large number of datasets ishighly correlated, showing that few-shot hardness may be intrinsic to datasets,for a given pre-trained model. To estimate intrinsic few-shot hardness, we thenpropose a simple and lightweight metric called ""Spread"" that captures theintuition that few-shot learning is made possible by exploiting feature-spaceinvariances between training and test samples. Our metric better accounts forfew-shot hardness compared to existing notions of hardness, and is ~8-100xfaster to compute."
A Theory of Human-Like Few-Shot Learning,"['Zhiying Jiang', 'Rui Wang', 'Dongbo Bu', 'Ming Li']",http://arxiv.org/pdf/2301.01047v1.pdf,2023-01-03,['cs.lg'],"  We aim to bridge the gap between our common-sense few-sample human learningand large-data machine learning. We derive a theory of human-like few-shotlearning from von-Neuman-Landauer's principle. modelling human learning isdifficult as how people learn varies from one to another. Under commonlyaccepted definitions, we prove that all human or animal few-shot learning, andmajor models including Free Energy Principle and Bayesian Program Learning thatmodel such learning, approximate our theory, under Church-Turing thesis. Wefind that deep generative model like variational autoencoder (VAE) can be usedto approximate our theory and perform significantly better than baseline modelsincluding deep neural networks, for image recognition, low resource languageprocessing, and character recognition."
Differentiable Entailment for Parameter Efficient Few Shot Learning,"['Ethan Kim', 'Jerry Yang']",http://arxiv.org/pdf/2301.13345v1.pdf,2023-01-31,['cs.cl'],"  Few-shot learning allows pre-trained language models to adapt to downstreamtasks while using a limited number of training examples. However, practicalapplications are limited when all model parameters must be optimized. In thiswork we apply a new technique for parameter efficient few shot learning whileadopting a strict definition of parameter efficiency. Our training methodcombines 1) intermediate training by reformulating natural language tasks asentailment tasks \cite{wang_entailment_2021} and 2) differentiable optimizationof template and label tokens \cite{zhang_differentiable_2021}. We quantify thetradeoff between parameter efficiency and performance in the few-shot regimeand propose a simple model agnostic approach that can be extended to any taskBy achieving competitive performance while only optimizing 3\% of a model'sparameters and allowing for batched inference, we allow for more efficientpractical deployment of models."
Implicit Geometry and Interaction Embeddings Improve Few-Shot Molecular  Property Prediction,"['Christopher Fifty', 'Joseph M. Paggi', 'Ehsan Amid', 'Jure Leskovec', 'Ron Dror']",http://arxiv.org/pdf/2302.02055v2.pdf,2023-02-04,['cs.lg'],"  Few-shot learning is a promising approach to molecular property prediction assupervised data is often very limited. However, many important molecularproperties depend on complex molecular characteristics -- such as the various3D geometries a molecule may adopt or the types of chemical interactions it canform -- that are not explicitly encoded in the feature space and must beapproximated from low amounts of data. Learning these characteristics can bedifficult, especially for few-shot learning algorithms that are designed forfast adaptation to new tasks. In this work, we develop molecular embeddingsthat encode complex molecular characteristics to improve the performance offew-shot molecular property prediction. Our approach leverages large amounts ofsynthetic data, namely the results of molecular docking calculations, and amulti-task learning paradigm to structure the embedding space. On multiplemolecular property prediction benchmarks, training from the embedding spacesubstantially improves Multi-Task, MAML, and Prototypical Network few-shotlearning performance. Our code is available athttps://github.com/cfifty/IGNITE."
Transductive Few-shot Learning with Prototype-based Label Propagation by  Iterative Graph Refinement,"['Hao Zhu', 'Piotr Koniusz']",http://arxiv.org/pdf/2304.11598v1.pdf,2023-04-23,"['cs.cv', 'cs.ai', 'cs.lg']","  Few-shot learning (FSL) is popular due to its ability to adapt to novelclasses. Compared with inductive few-shot learning, transductive modelstypically perform better as they leverage all samples of the query set. The twoexisting classes of methods, prototype-based and graph-based, have thedisadvantages of inaccurate prototype estimation and sub-optimal graphconstruction with kernel functions, respectively. In this paper, we propose anovel prototype-based label propagation to solve these issues. Specifically,our graph construction is based on the relation between prototypes and samplesrather than between samples. As prototypes are being updated, the graphchanges. We also estimate the label of each prototype instead of considering aprototype be the class centre. On mini-ImageNet, tiered-ImageNet, CIFAR-FS andCUB datasets, we show the proposed method outperforms other state-of-the-artmethods in transductive FSL and semi-supervised FSL when some unlabeled dataaccompanies the novel few-shot task."
Adaptive manifold for imbalanced transductive few-shot learning,"['Michalis Lazarou', 'Yannis Avrithis', 'Tania Stathaki']",http://arxiv.org/pdf/2304.14281v1.pdf,2023-04-27,['cs.cv'],"  Transductive few-shot learning algorithms have showed substantially superiorperformance over their inductive counterparts by leveraging the unlabeledqueries. However, the vast majority of such methods are evaluated on perfectlyclass-balanced benchmarks. It has been shown that they undergo remarkable dropin performance under a more realistic, imbalanced setting. To this end, wepropose a novel algorithm to address imbalanced transductive few-shot learning,named Adaptive Manifold. Our method exploits the underlying manifold of thelabeled support examples and unlabeled queries by using manifold similarity topredict the class probability distribution per query. It is parameterized byone centroid per class as well as a set of graph-specific parameters thatdetermine the manifold. All parameters are optimized through a loss functionthat can be tuned towards class-balanced or imbalanced distributions. Themanifold similarity shows substantial improvement over Euclidean distance,especially in the 1-shot setting. Our algorithm outperforms or is on par withother state of the art methods in three benchmark datasets, namelyminiImageNet, tieredImageNet and CUB, and three different backbones, namelyResNet-18, WideResNet-28-10 and DenseNet-121. In certain cases, our algorithmoutperforms the previous state of the art by as much as 4.2%."
Learning More Discriminative Local Descriptors for Few-shot Learning,"['Qijun Song', 'Siyun Zhou', 'Liwei Xu']",http://arxiv.org/pdf/2305.08721v1.pdf,2023-05-15,['cs.cv'],"  Few-shot learning for image classification comes up as a hot topic incomputer vision, which aims at fast learning from a limited number of labeledimages and generalize over the new tasks. In this paper, motivated by the ideaof Fisher Score, we propose a Discriminative Local Descriptors Attention (DLDA)model that adaptively selects the representative local descriptors and does notintroduce any additional parameters, while most of the existing localdescriptors based methods utilize the neural networks that inevitably involvethe tedious parameter tuning. Moreover, we modify the traditional $k$-NNclassification model by adjusting the weights of the $k$ nearest neighborsaccording to their distances from the query point. Experiments on fourbenchmark datasets show that our method not only achieves higher accuracycompared with the state-of-art approaches for few-shot learning, but alsopossesses lower sensitivity to the choices of $k$."
Evaluating the Evaluators: Are Current Few-Shot Learning Benchmarks Fit  for Purpose?,"['Luísa Shimabucoro', 'Timothy Hospedales', 'Henry Gouk']",http://arxiv.org/pdf/2307.02732v1.pdf,2023-07-06,"['cs.lg', 'stat.ml']","  Numerous benchmarks for Few-Shot Learning have been proposed in the lastdecade. However all of these benchmarks focus on performance averaged over manytasks, and the question of how to reliably evaluate and tune models trained forindividual tasks in this regime has not been addressed. This paper presents thefirst investigation into task-level evaluation -- a fundamental step whendeploying a model. We measure the accuracy of performance estimators in thefew-shot setting, consider strategies for model selection, and examine thereasons for the failure of evaluators usually thought of as being robust. Weconclude that cross-validation with a low number of folds is the best choicefor directly estimating the performance of a model, whereas using bootstrappingor cross validation with a large number of folds is better for model selectionpurposes. Overall, we find that existing benchmarks for few-shot learning arenot designed in such a way that one can get a reliable picture of howeffectively methods can be used on individual tasks."
GenCo: An Auxiliary Generator from Contrastive Learning for Enhanced  Few-Shot Learning in Remote Sensing,"['Jing Wu', 'Naira Hovakimyan', 'Jennifer Hobbs']",http://arxiv.org/pdf/2307.14612v1.pdf,2023-07-27,['cs.cv'],"  Classifying and segmenting patterns from a limited number of examples is asignificant challenge in remote sensing and earth observation due to thedifficulty in acquiring accurately labeled data in large quantities. Previousstudies have shown that meta-learning, which involves episodic training onquery and support sets, is a promising approach. However, there has been littleattention paid to direct fine-tuning techniques. This paper repurposescontrastive learning as a pre-training method for few-shot learning forclassification and semantic segmentation tasks. Specifically, we introduce agenerator-based contrastive learning framework (GenCo) that pre-trainsbackbones and simultaneously explores variants of feature samples. Infine-tuning, the auxiliary generator can be used to enrich limited labeled datasamples in feature space. We demonstrate the effectiveness of our method inimproving few-shot learning performance on two key remote sensing datasets:Agriculture-Vision and EuroSAT. Empirically, our approach outperforms purelysupervised training on the nearly 95,000 images in Agriculture-Vision for bothclassification and semantic segmentation tasks. Similarly, the proposedfew-shot method achieves better results on the land-cover classification taskon EuroSAT compared to the results obtained from fully supervised modeltraining on the dataset."
MerA: Merging Pretrained Adapters For Few-Shot Learning,"['Shwai He', 'Run-Ze Fan', 'Liang Ding', 'Li Shen', 'Tianyi Zhou', 'Dacheng Tao']",http://arxiv.org/pdf/2308.15982v1.pdf,2023-08-30,['cs.cl'],"  Adapter tuning, which updates only a few parameters, has become a mainstreammethod for fine-tuning pretrained language models to downstream tasks. However,it often yields subpar results in few-shot learning. AdapterFusion, whichassembles pretrained adapters using composition layers tailored to specifictasks, is a possible solution but significantly increases trainable parametersand deployment costs. Despite this, our preliminary study reveals that evensingle adapters can outperform Adapterfusion in few-shot learning, urging us topropose \textbf{\texttt{Merging Pretrained Adapters}} (MerA) that efficientlyincorporates pretrained adapters to a single model through model fusion.Extensive experiments on two PLMs demonstrate that MerA achieves substantialimprovements compared to both single adapters and AdapterFusion. To furtherenhance the capacity of MerA, we also introduce a simple yet effectivetechnique, referred to as the ""\textit{same-track}"" setting, that mergesadapters from the same track of pretraining tasks. With the implementation ofthe ""\textit{same-track}"" setting, we observe even more impressive gains,surpassing the performance of both full fine-tuning and adapter tuning by asubstantial margin, e.g., 3.5\% in MRPC and 5.0\% in MNLI."
Few-shot Diagnosis of Chest x-rays Using an Ensemble of Random  Discriminative Subspaces,"[' Kshitiz', 'Garvit Garg', 'Angshuman Paul']",http://arxiv.org/pdf/2309.00081v1.pdf,2023-08-31,['cs.cv'],"  Due to the scarcity of annotated data in the medical domain, few-shotlearning may be useful for medical image analysis tasks. We design a few-shotlearning method using an ensemble of random subspaces for the diagnosis ofchest x-rays (CXRs). Our design is computationally efficient and almost 1.8times faster than method that uses the popular truncated singular valuedecomposition (t-SVD) for subspace decomposition. The proposed method istrained by minimizing a novel loss function that helps create well-separatedclusters of training data in discriminative subspaces. As a result, minimizingthe loss maximizes the distance between the subspaces, making themdiscriminative and assisting in better classification. Experiments onlarge-scale publicly available CXR datasets yield promising results. Code forthe project will be available athttps://github.com/Few-shot-Learning-on-chest-x-ray/fsl_subspace."
Text-to-feature diffusion for audio-visual few-shot learning,"['Otniel-Bogdan Mercea', 'Thomas Hummel', 'A. Sophia Koepke', 'Zeynep Akata']",http://arxiv.org/pdf/2309.03869v1.pdf,2023-09-07,['cs.cv'],"  Training deep learning models for video classification from audio-visual datacommonly requires immense amounts of labeled training data collected via acostly process. A challenging and underexplored, yet much cheaper, setup isfew-shot learning from video data. In particular, the inherently multi-modalnature of video data with sound and visual information has not been leveragedextensively for the few-shot video classification task. Therefore, we introducea unified audio-visual few-shot video classification benchmark on threedatasets, i.e. the VGGSound-FSL, UCF-FSL, ActivityNet-FSL datasets, where weadapt and compare ten methods. In addition, we propose AV-DIFF, atext-to-feature diffusion framework, which first fuses the temporal andaudio-visual features via cross-modal attention and then generates multi-modalfeatures for the novel classes. We show that AV-DIFF obtains state-of-the-artperformance on our proposed benchmark for audio-visual (generalised) few-shotlearning. Our benchmark paves the way for effective audio-visual classificationwhen only limited labeled data is available. Code and data are available athttps://github.com/ExplainableML/AVDIFF-GFSL."
Adaptive Anchor Label Propagation for Transductive Few-Shot Learning,"['Michalis Lazarou', 'Yannis Avrithis', 'Guangyu Ren', 'Tania Stathaki']",http://arxiv.org/pdf/2310.19996v1.pdf,2023-10-30,['cs.cv'],"  Few-shot learning addresses the issue of classifying images using limitedlabeled data. Exploiting unlabeled data through the use of transductiveinference methods such as label propagation has been shown to improve theperformance of few-shot learning significantly. Label propagation inferspseudo-labels for unlabeled data by utilizing a constructed graph that exploitsthe underlying manifold structure of the data. However, a limitation of theexisting label propagation approaches is that the positions of all data pointsare fixed and might be sub-optimal so that the algorithm is not as effective aspossible. In this work, we propose a novel algorithm that adapts the featureembeddings of the labeled data by minimizing a differentiable loss functionoptimizing their positions in the manifold in the process. Our novel algorithm,Adaptive Anchor Label Propagation}, outperforms the standard label propagationalgorithm by as much as 7% and 2% in the 1-shot and 5-shot settingsrespectively. We provide experimental results highlighting the merits of ouralgorithm on four widely used few-shot benchmark datasets, namely miniImageNet,tieredImageNet, CUB and CIFAR-FS and two commonly used backbones, ResNet12 andWideResNet-28-10. The source code can be found athttps://github.com/MichalisLazarou/A2LP."
Few-shot Learning using Data Augmentation and Time-Frequency  Transformation for Time Series Classification,"['Hao Zhang', 'Zhendong Pang', 'Jiangpeng Wang', 'Teng Li']",http://arxiv.org/pdf/2311.03194v1.pdf,2023-11-06,['cs.cv'],"  Deep neural networks (DNNs) that tackle the time series classification (TSC)task have provided a promising framework in signal processing. In real-worldapplications, as a data-driven model, DNNs are suffered from insufficient data.Few-shot learning has been studied to deal with this limitation. In this paper,we propose a novel few-shot learning framework through data augmentation, whichinvolves transformation through the time-frequency domain and the generation ofsynthetic images through random erasing. Additionally, we develop asequence-spectrogram neural network (SSNN). This neural network model composesof two sub-networks: one utilizing 1D residual blocks to extract features fromthe input sequence while the other one employing 2D residual blocks to extractfeatures from the spectrogram representation. In the experiments, comparisonstudies of different existing DNN models with/without data augmentation areconducted on an amyotrophic lateral sclerosis (ALS) dataset and a wind turbinefault (WTF) dataset. The experimental results manifest that our proposed methodachieves 93.75% F1 score and 93.33% accuracy on the ALS datasets while 95.48%F1 score and 95.59% accuracy on the WTF datasets. Our methodology demonstratesits applicability of addressing the few-shot problems for time seriesclassification."
Meta-Adapter: An Online Few-shot Learner for Vision-Language Model,"['Cheng Cheng', 'Lin Song', 'Ruoyi Xue', 'Hang Wang', 'Hongbin Sun', 'Yixiao Ge', 'Ying Shan']",http://arxiv.org/pdf/2311.03774v1.pdf,2023-11-07,['cs.cv'],"  The contrastive vision-language pre-training, known as CLIP, demonstratesremarkable potential in perceiving open-world visual concepts, enablingeffective zero-shot image recognition. Nevertheless, few-shot learning methodsbased on CLIP typically require offline fine-tuning of the parameters onfew-shot samples, resulting in longer inference time and the risk ofover-fitting in certain domains. To tackle these challenges, we propose theMeta-Adapter, a lightweight residual-style adapter, to refine the CLIP featuresguided by the few-shot samples in an online manner. With a few trainingsamples, our method can enable effective few-shot learning capabilities andgeneralize to unseen data or tasks without additional fine-tuning, achievingcompetitive performance and high efficiency. Without bells and whistles, ourapproach outperforms the state-of-the-art online few-shot learning method by anaverage of 3.6\% on eight image classification datasets with higher inferencespeed. Furthermore, our model is simple and flexible, serving as aplug-and-play module directly applicable to downstream tasks. Without furtherfine-tuning, Meta-Adapter obtains notable performance improvements inopen-vocabulary object detection and segmentation tasks."
Tabular Few-Shot Generalization Across Heterogeneous Feature Spaces,"['Max Zhu', 'Katarzyna Kobalczyk', 'Andrija Petrovic', 'Mladen Nikolic', 'Mihaela van der Schaar', 'Boris Delibasic', 'Petro Lio']",http://arxiv.org/pdf/2311.10051v1.pdf,2023-11-16,['cs.lg'],"  Despite the prevalence of tabular datasets, few-shot learning remainsunder-explored within this domain. Existing few-shot methods are not directlyapplicable to tabular datasets due to varying column relationships, meanings,and permutational invariance. To address these challenges, we propose FLAT-anovel approach to tabular few-shot learning, encompassing knowledge sharingbetween datasets with heterogeneous feature spaces. Utilizing an encoderinspired by Dataset2Vec, FLAT learns low-dimensional embeddings of datasets andtheir individual columns, which facilitate knowledge transfer andgeneralization to previously unseen datasets. A decoder network parametrizesthe predictive target network, implemented as a Graph Attention Network, toaccommodate the heterogeneous nature of tabular datasets. Experiments on adiverse collection of 118 UCI datasets demonstrate FLAT's successfulgeneralization to new tabular datasets and a considerable improvement over thebaselines."
Exploring Graph Classification Techniques Under Low Data Constraints: A  Comprehensive Study,"['Kush Kothari', 'Bhavya Mehta', 'Reshmika Nambiar', 'Seema Shrawne']",http://arxiv.org/pdf/2311.12737v1.pdf,2023-11-21,['cs.lg'],"  This survey paper presents a brief overview of recent research on graph dataaugmentation and few-shot learning. It covers various techniques for graph dataaugmentation, including node and edge perturbation, graph coarsening, and graphgeneration, as well as the latest developments in few-shot learning, such asmeta-learning and model-agnostic meta-learning. The paper explores these areasin depth and delves into further sub classifications. Rule based approaches andlearning based approaches are surveyed under graph augmentation techniques.Few-Shot Learning on graphs is also studied in terms of metric learningtechniques and optimization-based techniques. In all, this paper provides anextensive array of techniques that can be employed in solving graph processingproblems faced in low-data scenarios."
Benchmarking Toxic Molecule Classification using Graph Neural Networks  and Few Shot Learning,"['Bhavya Mehta', 'Kush Kothari', 'Reshmika Nambiar', 'Seema Shrawne']",http://arxiv.org/pdf/2311.13490v1.pdf,2023-11-22,"['q-bio.qm', 'cs.lg']","  Traditional methods like Graph Convolutional Networks (GCNs) face challengeswith limited data and class imbalance, leading to suboptimal performance ingraph classification tasks during toxicity prediction of molecules as a whole.To address these issues, we harness the power of Graph Isomorphic Networks,Multi Headed Attention and Free Large-scale Adversarial Augmentation separatelyon Graphs for precisely capturing the structural data of molecules and theirtoxicological properties. Additionally, we incorporate Few-Shot Learning toimprove the model's generalization with limited annotated samples. Extensiveexperiments on a diverse toxicology dataset demonstrate that our methodachieves an impressive state-of-art AUC-ROC value of 0.816, surpassing thebaseline GCN model by 11.4%. This highlights the significance of our proposedmethodology and Few Shot Learning in advancing Toxic Molecular Classification,with the potential to enhance drug discovery and environmental risk assessmentprocesses."
Target-Free Compound Activity Prediction via Few-Shot Learning,"['Peter Eckmann', 'Jake Anderson', 'Michael K. Gilson', 'Rose Yu']",http://arxiv.org/pdf/2311.16328v1.pdf,2023-11-27,"['cs.lg', 'q-bio.qm']","  Predicting the activities of compounds against protein-based or phenotypicassays using only a few known compounds and their activities is a common taskin target-free drug discovery. Existing few-shot learning approaches arelimited to predicting binary labels (active/inactive). However, in real-worlddrug discovery, degrees of compound activity are highly relevant. We studyFew-Shot Compound Activity Prediction (FS-CAP) and design a novel neuralarchitecture to meta-learn continuous compound activities across largebioactivity datasets. Our model aggregates encodings generated from the knowncompounds and their activities to capture assay information. We also introducea separate encoder for the unknown compound. We show that FS-CAP surpassestraditional similarity-based techniques as well as other state of the artfew-shot learning methods on a variety of target-free drug discovery settingsand datasets."
Simple Semantic-Aided Few-Shot Learning,"['Hai Zhang', 'Junzhe Xu', 'Shanlin Jiang', 'Zhenan He']",http://arxiv.org/pdf/2311.18649v1.pdf,2023-11-30,['cs.cv'],"  Learning from a limited amount of data, namely Few-Shot Learning, stands outas a challenging computer vision task. Several works exploit semantics anddesign complicated semantic fusion mechanisms to compensate for rarerepresentative features within restricted data. However, relying on naivesemantics such as class names introduces biases due to their brevity, whileacquiring extensive semantics from external knowledge takes a huge time andeffort. This limitation severely constrains the potential of semantics infew-shot learning. In this paper, we design an automatic way called SemanticEvolution to generate high-quality semantics. The incorporation of high-qualitysemantics alleviates the need for complex network structures and learningalgorithms used in previous works. Hence, we employ a simple two-layer networktermed Semantic Alignment Network to transform semantics and visual featuresinto robust class prototypes with rich discriminative features for few-shotclassification. The experimental results show our framework outperforms allprevious methods on five benchmarks, demonstrating a simple network withhigh-quality semantics can beat intricate multi-modal modules on few-shotclassification tasks."
A Hybrid Approach with Optimization and Metric-based Meta-Learner for  Few-Shot Learning,"['Duo Wang', 'Yu Cheng', 'Mo Yu', 'Xiaoxiao Guo', 'Tao Zhang']",http://arxiv.org/pdf/1904.03014v2.pdf,2019-04-04,"['cs.lg', 'cs.cv', 'stat.ml']","  Few-shot learning aims to learn classifiers for new classes with only a fewtraining examples per class. Most existing few-shot learning approaches belongto either metric-based meta-learning or optimization-based meta-learningcategory, both of which have achieved successes in the simplified ""$k$-shot$N$-way"" image classification settings. Specifically, the optimization-basedapproaches train a meta-learner to predict the parameters of the task-specificclassifiers. The task-specific classifiers are required to behomogeneous-structured to ease the parameter prediction, so the meta-learningapproaches could only handle few-shot learning problems where the tasks share auniform number of classes. The metric-based approaches learn one task-invariantmetric for all the tasks. Even though the metric-learning approaches allowdifferent numbers of classes, they require the tasks all coming from a similardomain such that there exists a uniform metric that could work across tasks. Inthis work, we propose a hybrid meta-learning model called Meta-Metric-Learnerwhich combines the merits of both optimization- and metric-based approaches.Our meta-metric-learning approach consists of two components, a task-specificmetric-based learner as a base model, and a meta-learner that learns andspecifies the base model. Thus our model is able to handle flexible numbers ofclasses as well as generate more generalized metrics for classification acrosstasks. We test our approach in the standard ""$k$-shot $N$-way"" few-shotlearning setting following previous works and a new realistic few-shot settingwith flexible class numbers in both single-source form and multi-source forms.Experiments show that our approach can obtain superior performance in allsettings."
Cross-Domain Few-Shot Learning by Representation Fusion,"['Thomas Adler', 'Johannes Brandstetter', 'Michael Widrich', 'Andreas Mayr', 'David Kreil', 'Michael Kopp', 'Günter Klambauer', 'Sepp Hochreiter']",http://arxiv.org/pdf/2010.06498v2.pdf,2020-10-13,['cs.lg'],"  In order to quickly adapt to new data, few-shot learning aims at learningfrom few examples, often by using already acquired knowledge. The new dataoften differs from the previously seen data due to a domain shift, that is, achange of the input-target distribution. While several methods perform well onsmall domain shifts like new target classes with similar inputs, larger domainshifts are still challenging. Large domain shifts may result in high-levelconcepts that are not shared between the original and the new domain, whereaslow-level concepts like edges in images might still be shared and useful. Forcross-domain few-shot learning, we suggest representation fusion to unifydifferent abstraction levels of a deep neural network into one representation.We propose Cross-domain Hebbian Ensemble Few-shot learning (CHEF), whichachieves representation fusion by an ensemble of Hebbian learners acting ondifferent layers of a deep neural network. Ablation studies show thatrepresentation fusion is a decisive factor to boost cross-domain few-shotlearning. On the few-shot datasets miniImagenet and tieredImagenet with smalldomain shifts, CHEF is competitive with state-of-the-art methods. Oncross-domain few-shot benchmark challenges with larger domain shifts, CHEFestablishes novel state-of-the-art results in all categories. We further applyCHEF on a real-world cross-domain application in drug discovery. We consider adomain shift from bioactive molecules to environmental chemicals and drugs withtwelve associated toxicity prediction tasks. On these tasks, that are highlyrelevant for computational drug discovery, CHEF significantly outperforms allits competitors. Github: https://github.com/ml-jku/chef"
Self-Promoted Supervision for Few-Shot Transformer,"['Bowen Dong', 'Pan Zhou', 'Shuicheng Yan', 'Wangmeng Zuo']",http://arxiv.org/pdf/2203.07057v2.pdf,2022-03-14,['cs.cv'],"  The few-shot learning ability of vision transformers (ViTs) is rarelyinvestigated though heavily desired. In this work, we empirically find thatwith the same few-shot learning frameworks, \eg~Meta-Baseline, replacing thewidely used CNN feature extractor with a ViT model often severely impairsfew-shot classification performance. Moreover, our empirical study shows thatin the absence of inductive bias, ViTs often learn the low-qualified tokendependencies under few-shot learning regime where only a few labeled trainingdata are available, which largely contributes to the above performancedegradation. To alleviate this issue, for the first time, we propose a simpleyet effective few-shot training framework for ViTs, namely Self-promotedsUpervisioN (SUN). Specifically, besides the conventional global supervisionfor global semantic learning SUN further pretrains the ViT on the few-shotlearning dataset and then uses it to generate individual location-specificsupervision for guiding each patch token. This location-specific supervisiontells the ViT which patch tokens are similar or dissimilar and thus acceleratestoken dependency learning. Moreover, it models the local semantics in eachpatch token to improve the object grounding and recognition capability whichhelps learn generalizable patterns. To improve the quality of location-specificsupervision, we further propose two techniques:~1) background patch filtrationto filtrate background patches out and assign them into an extra backgroundclass; and 2) spatial-consistent augmentation to introduce sufficient diversityfor data augmentation while keeping the accuracy of the generated localsupervisions. Experimental results show that SUN using ViTs significantlysurpasses other few-shot learning frameworks with ViTs and is the first onethat achieves higher performance than those CNN state-of-the-arts."
"A Unified approach for Conventional Zero-shot, Generalized Zero-shot and  Few-shot Learning","['Shafin Rahman', 'Salman H. Khan', 'Fatih Porikli']",http://arxiv.org/pdf/1706.08653v2.pdf,2017-06-27,['cs.cv'],"  Prevalent techniques in zero-shot learning do not generalize well to otherrelated problem scenarios. Here, we present a unified approach for conventionalzero-shot, generalized zero-shot and few-shot learning problems. Our approachis based on a novel Class Adapting Principal Directions (CAPD) concept thatallows multiple embeddings of image features into a semantic space. Given animage, our method produces one principal direction for each seen class. Then,it learns how to combine these directions to obtain the principal direction foreach unseen class such that the CAPD of the test image is aligned with thesemantic embedding of the true class, and opposite to the other classes. Thisallows efficient and class-adaptive information transfer from seen to unseenclasses. In addition, we propose an automatic process for selection of the mostuseful seen classes for each unseen class to achieve robustness in zero-shotlearning. Our method can update the unseen CAPD taking the advantages of fewunseen images to work in a few-shot learning scenario. Furthermore, our methodcan generalize the seen CAPDs by estimating seen-unseen diversity thatsignificantly improves the performance of generalized zero-shot learning. Ourextensive evaluations demonstrate that the proposed approach consistentlyachieves superior performance in zero-shot, generalized zero-shot andfew/one-shot learning problems."
Probabilistic Model-Agnostic Meta-Learning,"['Chelsea Finn', 'Kelvin Xu', 'Sergey Levine']",http://arxiv.org/pdf/1806.02817v2.pdf,2018-06-07,"['cs.lg', 'cs.ai', 'stat.ml']","  Meta-learning for few-shot learning entails acquiring a prior over previoustasks and experiences, such that new tasks be learned from small amounts ofdata. However, a critical challenge in few-shot learning is task ambiguity:even when a powerful prior can be meta-learned from a large number of priortasks, a small dataset for a new task can simply be too ambiguous to acquire asingle model (e.g., a classifier) for that task that is accurate. In thispaper, we propose a probabilistic meta-learning algorithm that can samplemodels for a new task from a model distribution. Our approach extendsmodel-agnostic meta-learning, which adapts to new tasks via gradient descent,to incorporate a parameter distribution that is trained via a variational lowerbound. At meta-test time, our algorithm adapts via a simple procedure thatinjects noise into gradient descent, and at meta-training time, the model istrained such that this stochastic adaptation procedure produces samples fromthe approximate model posterior. Our experimental results show that our methodcan sample plausible classifiers and regressors in ambiguous few-shot learningproblems. We also show how reasoning about ambiguity can also be used fordownstream active learning problems."
Prior-Knowledge and Attention-based Meta-Learning for Few-Shot Learning,"['Yunxiao Qin', 'Weiguo Zhang', 'Chenxu Zhao', 'Zezheng Wang', 'Xiangyu Zhu', 'Guojun Qi', 'Jingping Shi', 'Zhen Lei']",http://arxiv.org/pdf/1812.04955v5.pdf,2018-12-11,"['cs.cv', 'cs.lg']","  Recently, meta-learning has been shown as a promising way to solve few-shotlearning. In this paper, inspired by the human cognition process which utilizesboth prior-knowledge and vision attention in learning new knowledge, we presenta novel paradigm of meta-learning approach with three developments to introduceattention mechanism and prior-knowledge for meta-learning. In our approach,prior-knowledge is responsible for helping meta-learner expressing the inputdata into high-level representation space, and attention mechanism enablesmeta-learner focusing on key features of the data in the representation space.Compared with existing meta-learning approaches that pay little attention toprior-knowledge and vision attention, our approach alleviates themeta-learner's few-shot cognition burden. Furthermore, a Task-Over-Fitting(TOF) problem, which indicates that the meta-learner has poor generalization ondifferent K-shot learning tasks, is discovered and we propose a Cross-Entropyacross Tasks (CET) metric to model and solve the TOF problem. Extensiveexperiments demonstrate that we improve the meta-learner with state-of-the-artperformance on several few-shot learning benchmarks, and at the same time theTOF problem can also be released greatly."
Human few-shot learning of compositional instructions,"['Brenden M. Lake', 'Tal Linzen', 'Marco Baroni']",http://arxiv.org/pdf/1901.04587v2.pdf,2019-01-14,['cs.cl'],"  People learn in fast and flexible ways that have not been emulated bymachines. Once a person learns a new verb ""dax,"" he or she can effortlesslyunderstand how to ""dax twice,"" ""walk and dax,"" or ""dax vigorously."" There havebeen striking recent improvements in machine learning for natural languageprocessing, yet the best algorithms require vast amounts of experience andstruggle to generalize new concepts in compositional ways. To better understandthese distinctively human abilities, we study the compositional skills ofpeople through language-like instruction learning tasks. Our results show thatpeople can learn and use novel functional concepts from very few examples(few-shot learning), successfully applying familiar functions to novel inputs.People can also compose concepts in complex ways that go beyond the provideddemonstrations. Two additional experiments examined the assumptions andinductive biases that people make when solving these tasks, revealing threebiases: mutual exclusivity, one-to-one mappings, and iconic concatenation. Wediscuss the implications for cognitive modeling and the potential for buildingmachines with more human-like language learning capabilities."
Finding Task-Relevant Features for Few-Shot Learning by Category  Traversal,"['Hongyang Li', 'David Eigen', 'Samuel Dodge', 'Matthew Zeiler', 'Xiaogang Wang']",http://arxiv.org/pdf/1905.11116v1.pdf,2019-05-27,"['cs.cv', 'cs.ai']","  Few-shot learning is an important area of research. Conceptually, humans arereadily able to understand new concepts given just a few examples, while inmore pragmatic terms, limited-example training situations are common inpractice. Recent effective approaches to few-shot learning employ ametric-learning framework to learn a feature similarity comparison between aquery (test) example, and the few support (training) examples. However, theseapproaches treat each support class independently from one another, neverlooking at the entire task as a whole. Because of this, they are constrained touse a single set of features for all possible test-time tasks, which hindersthe ability to distinguish the most relevant dimensions for the task at hand.In this work, we introduce a Category Traversal Module that can be inserted asa plug-and-play module into most metric-learning based few-shot learners. Thiscomponent traverses across the entire support set at once, identifyingtask-relevant features based on both intra-class commonality and inter-classuniqueness in the feature space. Incorporating our module improves performanceconsiderably (5%-10% relative) over baseline systems on both mini-ImageNet andtieredImageNet benchmarks, with overall performance competitive with recentstate-of-the-art systems."
Few-Shot Learning with Per-Sample Rich Supervision,"['Roman Visotsky', 'Yuval Atzmon', 'Gal Chechik']",http://arxiv.org/pdf/1906.03859v1.pdf,2019-06-10,"['cs.lg', 'cs.cv', 'stat.ml']","  Learning with few samples is a major challenge for parameter-rich models likedeep networks. In contrast, people learn complex new concepts even from veryfew examples, suggesting that the sample complexity of learning can often bereduced. Many approaches to few-shot learning build on transferring arepresentation from well-sampled classes, or using meta learning to favorarchitectures that can learn with few samples. Unfortunately, such approachesoften struggle when learning in an online way or with non-stationary datastreams. Here we describe a new approach to learn with fewer samples, by usingadditional information that is provided per sample. Specifically, we show howthe sample complexity can be reduced by providing semantic information aboutthe relevance of features per sample, like information about the presence ofobjects in a scene or confidence of detecting attributes in an image. Weprovide an improved generalization error bound for this case. We cast theproblem of using per-sample feature relevance by using a new ellipsoid-marginloss, and develop an online algorithm that minimizes this loss effectively.Empirical evaluation on two machine vision benchmarks for scene classificationand fine-grain bird classification demonstrate the benefits of this approachfor few-shot learning."
Few-Shot Meta-Denoising,"['Leslie Casas', 'Attila Klimmek', 'Gustavo Carneiro', 'Nassir Navab', 'Vasileios Belagiannis']",http://arxiv.org/pdf/1908.00111v2.pdf,2019-07-31,"['cs.cv', 'cs.lg']","  We study the problem of few-shot learning-based denoising where the trainingset contains just a handful of clean and noisy samples. A solution to mitigatethe small training set issue is to pre-train a denoising model with smalltraining sets containing pairs of clean and synthesized noisy signals, producedfrom empirical noise priors, and fine-tune on the available small training set.While such transfer learning seems effective, it may not generalize wellbecause of the limited amount of training data. In this work, we propose a newmeta-learning training approach for few-shot learning-based denoising problems.Our model is meta-trained using known synthetic noise models, and thenfine-tuned with the small training set, with the real noise, as a few-shotlearning task. Meta-learning from small training sets of syntheticallygenerated data during meta-training enables us to not only generate an infinitenumber of training tasks, but also train a model to learn with small trainingsets -- both advantages have the potential to improve the generalisation of thedenoising model. Our approach is empirically shown to produce more accuratedenoising results than supervised learning and transfer learning in threedenoising evaluations for images and 1-D signals. Interestingly, our studyprovides strong indications that meta-learning has the potential to become themain learning algorithm for denoising."
A Baseline for Few-Shot Image Classification,"['Guneet S. Dhillon', 'Pratik Chaudhari', 'Avinash Ravichandran', 'Stefano Soatto']",http://arxiv.org/pdf/1909.02729v5.pdf,2019-09-06,"['cs.lg', 'cs.cv', 'stat.ml']","  Fine-tuning a deep network trained with the standard cross-entropy loss is astrong baseline for few-shot learning. When fine-tuned transductively, thisoutperforms the current state-of-the-art on standard datasets such asMini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the samehyper-parameters. The simplicity of this approach enables us to demonstrate thefirst few-shot learning results on the ImageNet-21k dataset. We find that usinga large number of meta-training classes results in high few-shot accuracieseven for a large number of few-shot classes. We do not advocate our approach asthe solution for few-shot learning, but simply use the results to highlightlimitations of current benchmarks and few-shot protocols. We perform extensivestudies on benchmark datasets to propose a metric that quantifies the""hardness"" of a few-shot episode. This metric can be used to report theperformance of few-shot algorithms in a more systematic way."
PARN: Position-Aware Relation Networks for Few-Shot Learning,"['Ziyang Wu', 'Yuwei Li', 'Lihua Guo', 'Kui Jia']",http://arxiv.org/pdf/1909.04332v1.pdf,2019-09-10,['cs.cv'],"  Few-shot learning presents a challenge that a classifier must quickly adaptto new classes that do not appear in the training set, given only a few labeledexamples of each new class. This paper proposes a position-aware relationnetwork (PARN) to learn a more flexible and robust metric ability for few-shotlearning. Relation networks (RNs), a kind of architectures for relationalreasoning, can acquire a deep metric ability for images by just being designedas a simple convolutional neural network (CNN) [23]. However, due to theinherent local connectivity of CNN, the CNN-based relation network (RN) can besensitive to the spatial position relationship of semantic objects in twocompared images. To address this problem, we introduce a deformable featureextractor (DFE) to extract more efficient features, and design a dualcorrelation attention mechanism (DCA) to deal with its inherent localconnectivity. Successfully, our proposed approach extents the potential of RNto be position-aware of semantic objects by introducing only a small number ofparameters. We evaluate our approach on two major benchmark datasets, i.e.,Omniglot and Mini-Imagenet, and on both of the datasets our approach achievesstate-of-the-art performance with the setting of using a shallow featureextraction network. It's worth noting that our 5-way 1-shot result on Omnigloteven outperforms the previous 5-way 5-shot results."
Transductive Episodic-Wise Adaptive Metric for Few-Shot Learning,"['Limeng Qiao', 'Yemin Shi', 'Jia Li', 'Yaowei Wang', 'Tiejun Huang', 'Yonghong Tian']",http://arxiv.org/pdf/1910.02224v1.pdf,2019-10-05,"['cs.lg', 'cs.cv']","  Few-shot learning, which aims at extracting new concepts rapidly fromextremely few examples of novel classes, has been featured into themeta-learning paradigm recently. Yet, the key challenge of how to learn ageneralizable classifier with the capability of adapting to specific tasks withseverely limited data still remains in this domain. To this end, we propose aTransductive Episodic-wise Adaptive Metric (TEAM) framework for few-shotlearning, by integrating the meta-learning paradigm with both deep metriclearning and transductive inference. With exploring the pairwise constraintsand regularization prior within each task, we explicitly formulate theadaptation procedure into a standard semi-definite programming problem. Bysolving the problem with its closed-form solution on the fly with the setup oftransduction, our approach efficiently tailors an episodic-wise metric for eachtask to adapt all features from a shared task-agnostic embedding space into amore discriminative task-specific metric space. Moreover, we further leveragean attention-based bi-directional similarity strategy for extracting the morerobust relationship between queries and prototypes. Extensive experiments onthree benchmark datasets show that our framework is superior to other existingapproaches and achieves the state-of-the-art performance in the few-shotliterature."
Generalized Few-Shot Video Classification with Video Retrieval and  Feature Generation,"['Yongqin Xian', 'Bruno Korbar', 'Matthijs Douze', 'Lorenzo Torresani', 'Bernt Schiele', 'Zeynep Akata']",http://arxiv.org/pdf/2007.04755v2.pdf,2020-07-09,['cs.cv'],"  Few-shot learning aims to recognize novel classes from a few examples.Although significant progress has been made in the image domain, few-shot videoclassification is relatively unexplored. We argue that previous methodsunderestimate the importance of video feature learning and propose to learnspatiotemporal features using a 3D CNN. Proposing a two-stage approach thatlearns video features on base classes followed by fine-tuning the classifierson novel classes, we show that this simple baseline approach outperforms priorfew-shot video classification methods by over 20 points on existing benchmarks.To circumvent the need of labeled examples, we present two novel approachesthat yield further improvement. First, we leverage tag-labeled videos from alarge dataset using tag retrieval followed by selecting the best clips withvisual similarities. Second, we learn generative adversarial networks thatgenerate video features of novel classes from their semantic embeddings.Moreover, we find existing benchmarks are limited because they only focus on 5novel classes in each testing episode and introduce more realistic benchmarksby involving more novel classes, i.e. few-shot learning, as well as a mixtureof novel and base classes, i.e. generalized few-shot learning. The experimentalresults show that our retrieval and feature generation approach significantlyoutperform the baseline approach on the new benchmarks."
FS-HGR: Few-shot Learning for Hand Gesture Recognition via  ElectroMyography,"['Elahe Rahimian', 'Soheil Zabihi', 'Amir Asif', 'Dario Farina', 'Seyed Farokh Atashzar', 'Arash Mohammadi']",http://arxiv.org/pdf/2011.06104v1.pdf,2020-11-11,"['cs.lg', 'eess.sp']","  This work is motivated by the recent advances in Deep Neural Networks (DNNs)and their widespread applications in human-machine interfaces. DNNs have beenrecently used for detecting the intended hand gesture through processing ofsurface electromyogram (sEMG) signals. The ultimate goal of these approaches isto realize high-performance controllers for prosthetic. However, although DNNshave shown superior accuracy than conventional methods when large amounts ofdata are available for training, their performance substantially decreases whendata are limited. Collecting large datasets for training may be feasible inresearch laboratories, but it is not a practical approach for real-lifeapplications. Therefore, there is an unmet need for the design of a moderngesture detection technique that relies on minimal training data whileproviding high accuracy. Here we propose an innovative and novel ""Few-ShotLearning"" framework based on the formulation of meta-learning, referred to asthe FS-HGR, to address this need. Few-shot learning is a variant of domainadaptation with the goal of inferring the required output based on just one ora few training examples. More specifically, the proposed FS-HGR quicklygeneralizes after seeing very few examples from each class. The proposedapproach led to 85.94% classification accuracy on new repetitions with few-shotobservation (5-way 5-shot), 81.29% accuracy on new subjects with few-shotobservation (5-way 5-shot), and 73.36% accuracy on new gestures with few-shotobservation (5-way 5-shot)."
In-Memory Nearest Neighbor Search with FeFET Multi-Bit  Content-Addressable Memories,"['Arman Kazemi', 'Mohammad Mehdi Sharifi', 'Ann Franchesca Laguna', 'Franz Müller', 'Ramin Rajaei', 'Ricardo Olivo', 'Thomas Kämpfe', 'Michael Niemier', 'X. Sharon Hu']",http://arxiv.org/pdf/2011.07095v1.pdf,2020-11-13,"['cs.et', 'cs.lg']","  Nearest neighbor (NN) search is an essential operation in many applications,such as one/few-shot learning and image classification. As such, fast andlow-energy hardware support for accurate NN search is highly desirable. Ternarycontent-addressable memories (TCAMs) have been proposed to accelerate NN searchfor few-shot learning tasks by implementing $L_\infty$ and Hamming distancemetrics, but they cannot achieve software-comparable accuracies. This paperproposes a novel distance function that can be natively evaluated withmulti-bit content-addressable memories (MCAMs) based on ferroelectric FETs(FeFETs) to perform a single-step, in-memory NN search. Moreover, this approachachieves accuracies comparable to floating-point precision implementations insoftware for NN classification and one/few-shot learning tasks. As an example,the proposed method achieves a 98.34% accuracy for a 5-way, 5-shotclassification task for the Omniglot dataset (only 0.8% lower thansoftware-based implementations) with a 3-bit MCAM. This represents a 13%accuracy improvement over state-of-the-art TCAM-based implementations atiso-energy and iso-delay. The presented distance function is resilient to theeffects of FeFET device-to-device variations. Furthermore, this workexperimentally demonstrates a 2-bit implementation of FeFET MCAM using ANDarrays from GLOBALFOUNDRIES to further validate proof of concept."
TLRM: Task-level Relation Module for GNN-based Few-Shot Learning,"['Yurong Guo', 'Zhanyu Ma', 'Xiaoxu Li', 'Yuan Dong']",http://arxiv.org/pdf/2101.09840v3.pdf,2021-01-25,['cs.cv'],"  Recently, graph neural networks (GNNs) have shown powerful ability to handlefew-shot classification problem, which aims at classifying unseen samples whentrained with limited labeled samples per class. GNN-based few-shot learningarchitectures mostly replace traditional metric with a learnable GNN. In theGNN, the nodes are set as the samples embedding, and the relationship betweentwo connected nodes can be obtained by a network, the input of which is thedifference of their embedding features. We consider this method of measuringrelation of samples only models the sample-to-sample relation, while neglectsthe specificity of different tasks. That is, this method of measuring relationdoes not take the task-level information into account. To this end, we proposea new relation measure method, namely the task-level relation module (TLRM), toexplicitly model the task-level relation of one sample to all the others. Theproposed module captures the relation representations between nodes byconsidering the sample-to-task instead of sample-to-sample embedding features.We conducted extensive experiments on four benchmark datasets: mini-ImageNet,tiered-ImageNet, CUB-$200$-$2011$, and CIFAR-FS. Experimental resultsdemonstrate that the proposed module is effective for GNN-based few-shotlearning."
Meta-learning with differentiable closed-form solvers,"['Luca Bertinetto', 'João F. Henriques', 'Philip H. S. Torr', 'Andrea Vedaldi']",http://arxiv.org/pdf/1805.08136v3.pdf,2018-05-21,"['cs.cv', 'cs.lg', 'stat.ml']","  Adapting deep networks to new concepts from a few examples is challenging,due to the high computational requirements of standard fine-tuning procedures.Most work on few-shot learning has thus focused on simple learning techniquesfor adaptation, such as nearest neighbours or gradient descent. Nonetheless,the machine learning literature contains a wealth of methods that learnnon-deep models very efficiently. In this paper, we propose to use these fastconvergent methods as the main adaptation mechanism for few-shot learning. Themain idea is to teach a deep network to use standard machine learning tools,such as ridge regression, as part of its own internal model, enabling it toquickly adapt to novel data. This requires back-propagating errors through thesolver steps. While normally the cost of the matrix operations involved in sucha process would be significant, by using the Woodbury identity we can make thesmall number of examples work to our advantage. We propose both closed-form anditerative solvers, based on ridge regression and logistic regressioncomponents. Our methods constitute a simple and novel approach to the problemof few-shot learning and achieve performance competitive with or superior tothe state of the art on three benchmarks."
Learning to Propagate Labels: Transductive Propagation Network for  Few-shot Learning,"['Yanbin Liu', 'Juho Lee', 'Minseop Park', 'Saehoon Kim', 'Eunho Yang', 'Sung Ju Hwang', 'Yi Yang']",http://arxiv.org/pdf/1805.10002v5.pdf,2018-05-25,"['cs.lg', 'cs.cv', 'cs.ne', 'stat.ml', '68t10, 97r40', 'i.5.1; i.2.6']","  The goal of few-shot learning is to learn a classifier that generalizes welleven when trained with a limited number of training instances per class. Therecently introduced meta-learning approaches tackle this problem by learning ageneric classifier across a large number of multiclass classification tasks andgeneralizing the model to a new task. Yet, even with such meta-learning, thelow-data problem in the novel classification task still remains. In this paper,we propose Transductive Propagation Network (TPN), a novel meta-learningframework for transductive inference that classifies the entire test set atonce to alleviate the low-data problem. Specifically, we propose to learn topropagate labels from labeled instances to unlabeled test instances, bylearning a graph construction module that exploits the manifold structure inthe data. TPN jointly learns both the parameters of feature embedding and thegraph construction in an end-to-end manner. We validate TPN on multiplebenchmark datasets, on which it largely outperforms existing few-shot learningapproaches and achieves the state-of-the-art results."
TADAM: Task dependent adaptive metric for improved few-shot learning,"['Boris N. Oreshkin', 'Pau Rodriguez', 'Alexandre Lacoste']",http://arxiv.org/pdf/1805.10123v4.pdf,2018-05-23,"['cs.lg', 'cs.ai', 'cs.cv', 'stat.ml']","  Few-shot learning has become essential for producing models that generalizefrom few examples. In this work, we identify that metric scaling and metrictask conditioning are important to improve the performance of few-shotalgorithms. Our analysis reveals that simple metric scaling completely changesthe nature of few-shot algorithm parameter updates. Metric scaling providesimprovements up to 14% in accuracy for certain metrics on the mini-Imagenet5-way 5-shot classification task. We further propose a simple and effective wayof conditioning a learner on the task sample set, resulting in learning atask-dependent metric space. Moreover, we propose and empirically test apractical end-to-end optimization procedure based on auxiliary task co-trainingto learn a task-dependent metric space. The resulting few-shot learning modelbased on the task-dependent scaled metric achieves state of the art onmini-Imagenet. We confirm these results on another few-shot dataset that weintroduce in this paper based on CIFAR100. Our code is publicly available athttps://github.com/ElementAI/TADAM."
Adaptive Cross-Modal Few-Shot Learning,"['Chen Xing', 'Negar Rostamzadeh', 'Boris N. Oreshkin', 'Pedro O. Pinheiro']",http://arxiv.org/pdf/1902.07104v3.pdf,2019-02-19,"['cs.lg', 'stat.ml']","  Metric-based meta-learning techniques have successfully been applied tofew-shot classification problems. In this paper, we propose to leveragecross-modal information to enhance metric-based few-shot learning methods.Visual and semantic feature spaces have different structures by definition. Forcertain concepts, visual features might be richer and more discriminative thantext ones. While for others, the inverse might be true. Moreover, when thesupport from visual information is limited in image classification, semanticrepresentations (learned from unsupervised text corpora) can provide strongprior knowledge and context to help learning. Based on these two intuitions, wepropose a mechanism that can adaptively combine information from bothmodalities according to new image categories to be learned. Through a series ofexperiments, we show that by this adaptive combination of the two modalities,our model outperforms current uni-modality few-shot learning methods andmodality-alignment methods by a large margin on all benchmarks and few-shotscenarios tested. Experiments also show that our model can effectively adjustits focus on the two modalities. The improvement in performance is particularlylarge when the number of shots is very small."
LaSO: Label-Set Operations networks for multi-label few-shot learning,"['Amit Alfassy', 'Leonid Karlinsky', 'Amit Aides', 'Joseph Shtok', 'Sivan Harary', 'Rogerio Feris', 'Raja Giryes', 'Alex M. Bronstein']",http://arxiv.org/pdf/1902.09811v1.pdf,2019-02-26,['cs.cv'],"  Example synthesis is one of the leading methods to tackle the problem offew-shot learning, where only a small number of samples per class areavailable. However, current synthesis approaches only address the scenario of asingle category label per image. In this work, we propose a novel technique forsynthesizing samples with multiple labels for the (yet unhandled) multi-labelfew-shot classification scenario. We propose to combine pairs of given examplesin feature space, so that the resulting synthesized feature vectors willcorrespond to examples whose label sets are obtained through certain setoperations on the label sets of the corresponding input pairs. Thus, our methodis capable of producing a sample containing the intersection, union orset-difference of labels present in two input samples. As we show, these setoperations generalize to labels unseen during training. This enables performingaugmentation on examples of novel categories, thus, facilitating multi-labelfew-shot classifier learning. We conduct numerous experiments showing promisingresults for the label-set manipulation capabilities of the proposed approach,both directly (using the classification and retrieval metrics), and in thecontext of performing data augmentation for multi-label few-shot learning. Wepropose a benchmark for this new and challenging task and show that our methodcompares favorably to all the common baselines."
L2AE-D: Learning to Aggregate Embeddings for Few-shot Learning with  Meta-level Dropout,"['Heda Song', 'Mercedes Torres Torres', 'Ender Özcan', 'Isaac Triguero']",http://arxiv.org/pdf/1904.04339v1.pdf,2019-04-08,"['cs.lg', 'cs.cv']","  Few-shot learning focuses on learning a new visual concept with very limitedlabelled examples. A successful approach to tackle this problem is to comparethe similarity between examples in a learned metric space based onconvolutional neural networks. However, existing methods typically suffer frommeta-level overfitting due to the limited amount of training tasks and do notnormally consider the importance of the convolutional features of differentexamples within the same channel. To address these limitations, we make thefollowing two contributions: (a) We propose a novel meta-learning approach foraggregating useful convolutional features and suppressing noisy ones based on achannel-wise attention mechanism to improve class representations. The proposedmodel does not require fine-tuning and can be trained in an end-to-end manner.The main novelty lies in incorporating a shared weight generation module thatlearns to assign different weights to the feature maps of different exampleswithin the same channel. (b) We also introduce a simple meta-level dropouttechnique that reduces meta-level overfitting in several few-shot learningapproaches. In our experiments, we find that this simple techniquesignificantly improves the performance of the proposed method as well asvarious state-of-the-art meta-learning algorithms. Applying our method tofew-shot image recognition using Omniglot and miniImageNet datasets shows thatit is capable of delivering a state-of-the-art classification performance."
Meta-Learning of Neural Architectures for Few-Shot Learning,"['Thomas Elsken', 'Benedikt Staffler', 'Jan Hendrik Metzen', 'Frank Hutter']",http://arxiv.org/pdf/1911.11090v3.pdf,2019-11-25,"['cs.lg', 'stat.ml']","  The recent progress in neural architecture search (NAS) has allowed scalingthe automated design of neural architectures to real-world domains, such asobject detection and semantic segmentation. However, one prerequisite for theapplication of NAS are large amounts of labeled data and compute resources.This renders its application challenging in few-shot learning scenarios, wheremany related tasks need to be learned, each with limited amounts of data andcompute time. Thus, few-shot learning is typically done with a fixed neuralarchitecture. To improve upon this, we propose MetaNAS, the first method whichfully integrates NAS with gradient-based meta-learning. MetaNAS optimizes ameta-architecture along with the meta-weights during meta-training. Duringmeta-testing, architectures can be adapted to a novel task with a few steps ofthe task optimizer, that is: task adaptation becomes computationally cheap andrequires only little data per task. Moreover, MetaNAS is agnostic in that itcan be used with arbitrary model-agnostic meta-learning algorithms andarbitrary gradient-based NAS methods. %We present encouraging results forMetaNAS with a combination of DARTS and REPTILE on few-shot classificationbenchmarks. Empirical results on standard few-shot classification benchmarksshow that MetaNAS with a combination of DARTS and REPTILE yieldsstate-of-the-art results."
Weakly-supervised Object Localization for Few-shot Learning and  Fine-grained Few-shot Learning,"['Xiaojian He', 'Jinfu Lin', 'Junming Shen']",http://arxiv.org/pdf/2003.00874v3.pdf,2020-03-02,"['cs.cv', 'cs.lg', 'stat.ml', '68t30, 68t10 (primary)', 'i.2.4; i.5.3']","  Few-shot learning (FSL) aims to learn novel visual categories from very fewsamples, which is a challenging problem in real-world applications. Manymethods of few-shot classification work well on general images to learn globalrepresentation. However, they can not deal with fine-grained categories well atthe same time due to a lack of subtle and local information. We argue thatlocalization is an efficient approach because it directly provides thediscriminative regions, which is critical for both general classification andfine-grained classification in a low data regime. In this paper, we propose aSelf-Attention Based Complementary Module (SAC Module) to fulfill theweakly-supervised object localization, and more importantly produce theactivated masks for selecting discriminative deep descriptors for few-shotclassification. Based on each selected deep descriptor, Semantic AlignmentModule (SAM) calculates the semantic alignment distance between the query andsupport images to boost classification performance. Extensive experiments showour method outperforms the state-of-the-art methods on benchmark datasets undervarious settings, especially on the fine-grained few-shot tasks. Besides, ourmethod achieves superior performance over previous methods when training themodel on miniImageNet and evaluating it on the different datasets,demonstrating its superior generalization capacity. Extra visualization showsthe proposed method can localize the key objects more interval."
Meta Cyclical Annealing Schedule: A Simple Approach to Avoiding  Meta-Amortization Error,"['Yusuke Hayashi', 'Taiji Suzuki']",http://arxiv.org/pdf/2003.01889v1.pdf,2020-03-04,"['stat.ml', 'cs.lg']","  The ability to learn new concepts with small amounts of data is a crucialaspect of intelligence that has proven challenging for deep learning methods.Meta-learning for few-shot learning offers a potential solution to thisproblem: by learning to learn across data from many previous tasks, few-shotlearning algorithms can discover the structure among tasks to enable fastlearning of new tasks. However, a critical challenge in few-shot learning istask ambiguity: even when a powerful prior can be meta-learned from a largenumber of prior tasks, a small dataset for a new task can simply be veryambiguous to acquire a single model for that task. The Bayesian meta-learningmodels can naturally resolve this problem by putting a sophisticated priordistribution and let the posterior well regularized through Bayesian decisiontheory. However, currently known Bayesian meta-learning procedures such asVERSA suffer from the so-called {\it information preference problem}, that is,the posterior distribution is degenerated to one point and is far from theexact one. To address this challenge, we design a novel meta-regularizationobjective using {\it cyclical annealing schedule} and {\it maximum meandiscrepancy} (MMD) criterion. The cyclical annealing schedule is quiteeffective at avoiding such degenerate solutions. This procedure includes adifficult KL-divergence estimation, but we resolve the issue by employing MMDinstead of KL-divergence. The experimental results show that our approachsubstantially outperforms standard meta-learning algorithms."
Semi-supervised few-shot learning for medical image segmentation,"['Abdur R Feyjie', 'Reza Azad', 'Marco Pedersoli', 'Claude Kauffman', 'Ismail Ben Ayed', 'Jose Dolz']",http://arxiv.org/pdf/2003.08462v2.pdf,2020-03-18,['cs.cv'],"  Recent years have witnessed the great progress of deep neural networks onsemantic segmentation, particularly in medical imaging. Nevertheless, traininghigh-performing models require large amounts of pixel-level ground truth masks,which can be prohibitive to obtain in the medical domain. Furthermore, trainingsuch models in a low-data regime highly increases the risk of overfitting.Recent attempts to alleviate the need for large annotated datasets havedeveloped training strategies under the few-shot learning paradigm, whichaddresses this shortcoming by learning a novel class from only a few labeledexamples. In this context, a segmentation model is trained on episodes, whichrepresent different segmentation problems, each of them trained with a verysmall labeled dataset. In this work, we propose a novel few-shot learningframework for semantic segmentation, where unlabeled images are also madeavailable at each episode. To handle this new learning paradigm, we propose toinclude surrogate tasks that can leverage very powerful supervisory signals--derived from the data itself-- for semantic feature learning. We show thatincluding unlabeled surrogate tasks in the episodic training leads to morepowerful feature representations, which ultimately results in bettergenerability to unseen tasks. We demonstrate the efficiency of our method inthe task of skin lesion segmentation in two publicly available datasets.Furthermore, our approach is general and model-agnostic, which can be combinedwith different deep architectures."
Domain-Adaptive Few-Shot Learning,"['An Zhao', 'Mingyu Ding', 'Zhiwu Lu', 'Tao Xiang', 'Yulei Niu', 'Jiechao Guan', 'Ji-Rong Wen', 'Ping Luo']",http://arxiv.org/pdf/2003.08626v1.pdf,2020-03-19,['cs.cv'],"  Existing few-shot learning (FSL) methods make the implicit assumption thatthe few target class samples are from the same domain as the source classsamples. However, in practice this assumption is often invalid -- the targetclasses could come from a different domain. This poses an additional challengeof domain adaptation (DA) with few training samples. In this paper, the problemof domain-adaptive few-shot learning (DA-FSL) is tackled, which requiressolving FSL and DA in a unified framework. To this end, we propose a noveldomain-adversarial prototypical network (DAPN) model. It is designed to addressa specific challenge in DA-FSL: the DA objective means that the source andtarget data distributions need to be aligned, typically through a shareddomain-adaptive feature embedding space; but the FSL objective dictates thatthe target domain per class distribution must be different from that of anysource domain class, meaning aligning the distributions across domains may harmthe FSL performance. How to achieve global domain distribution alignment whilstmaintaining source/target per-class discriminativeness thus becomes the key.Our solution is to explicitly enhance the source/target per-class separationbefore domain-adaptive feature embedding learning in the DAPN, in order toalleviate the negative effect of domain alignment on FSL. Extensive experimentsshow that our DAPN outperforms the state-of-the-art FSL and DA models, as wellas their na\""ive combinations. The code is available athttps://github.com/dingmyu/DAPN."
Additive Angular Margin for Few Shot Learning to Classify Clinical  Endoscopy Images,"['Sharib Ali', 'Binod Bhattarai', 'Tae-Kyun Kim', 'Jens Rittscher']",http://arxiv.org/pdf/2003.10033v2.pdf,2020-03-23,"['cs.cv', 'cs.lg', 'eess.iv']","  Endoscopy is a widely used imaging modality to diagnose and treat diseases inhollow organs as for example the gastrointestinal tract, the kidney and theliver. However, due to varied modalities and use of different imaging protocolsat various clinical centers impose significant challenges when generalisingdeep learning models. Moreover, the assembly of large datasets from differentclinical centers can introduce a huge label bias that renders any learnt modelunusable. Also, when using new modality or presence of images with rarepatterns, a bulk amount of similar image data and their corresponding labelsare required for training these models. In this work, we propose to use afew-shot learning approach that requires less training data and can be used topredict label classes of test samples from an unseen dataset. We propose anovel additive angular margin metric in the framework of prototypical networkin few-shot learning setting. We compare our approach to the severalestablished methods on a large cohort of multi-center, multi-organ, andmulti-modal endoscopy data. The proposed algorithm outperforms existingstate-of-the-art methods."
Cross-Domain Few-Shot Learning with Meta Fine-Tuning,"['John Cai', 'Sheng Mei Shen']",http://arxiv.org/pdf/2005.10544v4.pdf,2020-05-21,"['cs.cv', 'cs.lg']","  In this paper, we tackle the new Cross-Domain Few-Shot Learning benchmarkproposed by the CVPR 2020 Challenge. To this end, we build uponstate-of-the-art methods in domain adaptation and few-shot learning to create asystem that can be trained to perform both tasks. Inspired by the need tocreate models designed to be fine-tuned, we explore the integration oftransfer-learning (fine-tuning) with meta-learning algorithms, to train anetwork that has specific layers that are designed to be adapted at a laterfine-tuning stage. To do so, we modify the episodic training process to includea first-order MAML-based meta-learning algorithm, and use a Graph NeuralNetwork model as the subsequent meta-learning module. We find that our proposedmethod helps to boost accuracy significantly, especially when combined withdata augmentation. In our final results, we combine the novel method with thebaseline method in a simple ensemble, and achieve an average accuracy of 73.78%on the benchmark. This is a 6.51% improvement over existing benchmarks thatwere trained solely on miniImagenet."
One of these (Few) Things is Not Like the Others,"['Nat Roth', 'Justin Wagle']",http://arxiv.org/pdf/2005.11405v1.pdf,2020-05-22,"['cs.cv', 'cs.lg', 'eess.iv']","  To perform well, most deep learning based image classification systemsrequire large amounts of data and computing resources. These constraints makeit difficult to quickly personalize to individual users or train models outsideof fairly powerful machines. To deal with these problems, there has been alarge body of research into teaching machines to learn to classify images basedon only a handful of training examples, a field known as few-shot learning.Few-shot learning research traditionally makes the simplifying assumption thatall images belong to one of a fixed number of previously seen groups. However,many image datasets, such as a camera roll on a phone, will be noisy andcontain images that may not be relevant or fit into any clear group. We proposea model which can both classify new images based on a small number of examplesand recognize images which do not belong to any previously seen group. We adaptprevious few-shot learning work to include a simple mechanism for learning acutoff that determines whether an image should be excluded or classified. Weexamine how well our method performs in a realistic setting, benchmarking theapproach on a noisy and ambiguous dataset of images. We evaluate performanceover a spectrum of model architectures, including setups small enough to be runon low powered devices, such as mobile phones or web browsers. We find thatthis task of excluding irrelevant images poses significant extra difficultybeyond that of the traditional few-shot task. We decompose the sources of thiserror, and suggest future improvements that might alleviate this difficulty."
High-order structure preserving graph neural network for few-shot  learning,"['Guangfeng Lin', 'Ying Yang', 'Yindi Fan', 'Xiaobing Kang', 'Kaiyang Liao', 'Fan Zhao']",http://arxiv.org/pdf/2005.14415v1.pdf,2020-05-29,"['cs.cv', 'cs.lg']","  Few-shot learning can find the latent structure information between the priorknowledge and the queried data by the similarity metric of meta-learning toconstruct the discriminative model for recognizing the new categories with therare labeled samples. Most existing methods try to model the similarityrelationship of the samples in the intra tasks, and generalize the model toidentify the new categories. However, the relationship of samples between theseparated tasks is difficultly considered because of the different metriccriterion in the respective tasks. In contrast, the proposed high-orderstructure preserving graph neural network(HOSP-GNN) can further explore therich structure of the samples to predict the label of the queried data on graphthat enables the structure evolution to explicitly discriminate the categoriesby iteratively updating the high-order structure relationship (the relativemetric in multi-samples,instead of pairwise sample metric) with the manifoldstructure constraints. HOSP-GNN can not only mine the high-order structure forcomplementing the relevance between samples that may be divided into thedifferent task in meta-learning, and but also generate the rule of thestructure updating by manifold constraint. Furthermore, HOSP-GNN doesn't needretrain the learning model for recognizing the new classes, and HOSP-GNN hasthe well-generalizable high-order structure for model adaptability. Experimentsshow that HOSP-GNN outperforms the state-of-the-art methods on supervised andsemi-supervised few-shot learning in three benchmark datasets that areminiImageNet, tieredImageNet and FC100."
Improving Few-Shot Learning using Composite Rotation based Auxiliary  Task,"['Pratik Mazumder', 'Pravendra Singh', 'Vinay P. Namboodiri']",http://arxiv.org/pdf/2006.15919v2.pdf,2020-06-29,['cs.cv'],"  In this paper, we propose an approach to improve few-shot classificationperformance using a composite rotation based auxiliary task. Few-shotclassification methods aim to produce neural networks that perform well forclasses with a large number of training samples and classes with less number oftraining samples. They employ techniques to enable the network to producehighly discriminative features that are also very generic. Generally, thebetter the quality and generic-nature of the features produced by the network,the better is the performance of the network on few-shot learning. Our approachaims to train networks to produce such features by using a self-supervisedauxiliary task. Our proposed composite rotation based auxiliary task performsrotation at two levels, i.e., rotation of patches inside the image (innerrotation) and rotation of the whole image (outer rotation) and assigns one outof 16 rotation classes to the modified image. We then simultaneously train forthe composite rotation prediction task along with the original classificationtask, which forces the network to learn high-quality generic features that helpimprove the few-shot classification performance. We experimentally show thatour approach performs better than existing few-shot learning methods onmultiple benchmark datasets."
Few-shot Action Recognition with Implicit Temporal Alignment and Pair  Similarity Optimization,"['Congqi Cao', 'Yajuan Li', 'Qinyi Lv', 'Peng Wang', 'Yanning Zhang']",http://arxiv.org/pdf/2010.06215v1.pdf,2020-10-13,['cs.cv'],"  Few-shot learning aims to recognize instances from novel classes with fewlabeled samples, which has great value in research and application. Althoughthere has been a lot of work in this area recently, most of the existing workis based on image classification tasks. Video-based few-shot action recognitionhas not been explored well and remains challenging: 1) the differences ofimplementation details among different papers make a fair comparison difficult;2) the wide variations and misalignment of temporal sequences make thevideo-level similarity comparison difficult; 3) the scarcity of labeled datamakes the optimization difficult. To solve these problems, this paper presents1) a specific setting to evaluate the performance of few-shot actionrecognition algorithms; 2) an implicit sequence-alignment algorithm for bettervideo-level similarity comparison; 3) an advanced loss for few-shot learning tooptimize pair similarity with limited data. Specifically, we propose a novelfew-shot action recognition framework that uses long short-term memoryfollowing 3D convolutional layers for sequence modeling and alignment. Circleloss is introduced to maximize the within-class similarity and minimize thebetween-class similarity flexibly towards a more definite convergence target.Instead of using random or ambiguous experimental settings, we set a concretecriterion analogous to the standard image-based few-shot learning setting forfew-shot action recognition evaluation. Extensive experiments on two datasetsdemonstrate the effectiveness of our proposed method."
Partial Is Better Than All: Revisiting Fine-tuning Strategy for Few-shot  Learning,"['Zhiqiang Shen', 'Zechun Liu', 'Jie Qin', 'Marios Savvides', 'Kwang-Ting Cheng']",http://arxiv.org/pdf/2102.03983v1.pdf,2021-02-08,"['cs.cv', 'cs.ai', 'cs.lg']","  The goal of few-shot learning is to learn a classifier that can recognizeunseen classes from limited support data with labels. A common practice forthis task is to train a model on the base set first and then transfer to novelclasses through fine-tuning (Here fine-tuning procedure is defined astransferring knowledge from base to novel data, i.e. learning to transfer infew-shot scenario.) or meta-learning. However, as the base classes have nooverlap to the novel set, simply transferring whole knowledge from base data isnot an optimal solution since some knowledge in the base model may be biased oreven harmful to the novel class. In this paper, we propose to transfer partialknowledge by freezing or fine-tuning particular layer(s) in the base model.Specifically, layers will be imposed different learning rates if they arechosen to be fine-tuned, to control the extent of preserved transferability. Todetermine which layers to be recast and what values of learning rates for them,we introduce an evolutionary search based method that is efficient tosimultaneously locate the target layers and determine their individual learningrates. We conduct extensive experiments on CUB and mini-ImageNet to demonstratethe effectiveness of our proposed method. It achieves the state-of-the-artperformance on both meta-learning and non-meta based frameworks. Furthermore,we extend our method to the conventional pre-training + fine-tuning paradigmand obtain consistent improvement."
On Fast Adversarial Robustness Adaptation in Model-Agnostic  Meta-Learning,"['Ren Wang', 'Kaidi Xu', 'Sijia Liu', 'Pin-Yu Chen', 'Tsui-Wei Weng', 'Chuang Gan', 'Meng Wang']",http://arxiv.org/pdf/2102.10454v1.pdf,2021-02-20,"['cs.lg', 'cs.ai', 'cs.cv']","  Model-agnostic meta-learning (MAML) has emerged as one of the most successfulmeta-learning techniques in few-shot learning. It enables us to learn ameta-initialization} of model parameters (that we call meta-model) to rapidlyadapt to new tasks using a small amount of labeled training data. Despite thegeneralization power of the meta-model, it remains elusive that how adversarialrobustness can be maintained by MAML in few-shot learning. In addition togeneralization, robustness is also desired for a meta-model to defendadversarial examples (attacks). Toward promoting adversarial robustness inMAML, we first study WHEN a robustness-promoting regularization should beincorporated, given the fact that MAML adopts a bi-level (fine-tuning vs.meta-update) learning procedure. We show that robustifying the meta-updatestage is sufficient to make robustness adapted to the task-specific fine-tuningstage even if the latter uses a standard training protocol. We also makeadditional justification on the acquired robustness adaptation by peering intothe interpretability of neurons' activation maps. Furthermore, we investigateHOW robust regularization can efficiently be designed in MAML. We propose ageneral but easily-optimized robustness-regularized meta-learning framework,which allows the use of unlabeled data augmentation, fast adversarial attackgeneration, and computationally-light fine-tuning. In particular, we for thefirst time show that the auxiliary contrastive learning task can enhance theadversarial robustness of MAML. Finally, extensive experiments are conducted todemonstrate the effectiveness of our proposed methods in robust few-shotlearning."
Few-shot Continual Learning: a Brain-inspired Approach,"['Liyuan Wang', 'Qian Li', 'Yi Zhong', 'Jun Zhu']",http://arxiv.org/pdf/2104.09034v1.pdf,2021-04-19,['cs.lg'],"  It is an important yet challenging setting to continually learn new tasksfrom a few examples. Although numerous efforts have been devoted to eithercontinual learning or few-shot learning, little work has considered this newsetting of few-shot continual learning (FSCL), which needs to minimize thecatastrophic forgetting to the old tasks and gradually improve the ability offew-shot generalization. In this paper, we provide a first systematic study onFSCL and present an effective solution with deep neural networks. Our solutionis based on the observation that continual learning of a task sequenceinevitably interferes few-shot generalization, which makes it highly nontrivialto extend few-shot learning strategies to continual learning scenarios. We drawinspirations from the robust brain system and develop a method that (1)interdependently updates a pair of fast / slow weights for continual learningand few-shot learning to disentangle their divergent objectives, inspired bythe biological model of meta-plasticity and fast / slow synapse; and (2)applies a brain-inspired two-step consolidation strategy to learn a tasksequence without forgetting in the fast weights while improve generalizationwithout overfitting in the slow weights. Extensive results on variousbenchmarks show that our method achieves a better performance than jointtraining of all the tasks ever seen. The ability of few-shot generalization isalso substantially improved from incoming tasks and examples."
Demystification of Few-shot and One-shot Learning,"['Ivan Y. Tyukin', 'Alexander N. Gorban', 'Muhammad H. Alkhudaydi', 'Qinghua Zhou']",http://arxiv.org/pdf/2104.12174v2.pdf,2021-04-25,"['cs.lg', 'cs.ai', 'math.st', 'stat.th', '68t05, 68t07']","  Few-shot and one-shot learning have been the subject of active and intensiveresearch in recent years, with mounting evidence pointing to successfulimplementation and exploitation of few-shot learning algorithms in practice.Classical statistical learning theories do not fully explain why few- orone-shot learning is at all possible since traditional generalisation boundsnormally require large training and testing samples to be meaningful. Thissharply contrasts with numerous examples of successful one- and few-shotlearning systems and applications.  In this work we present mathematical foundations for a theory of one-shot andfew-shot learning and reveal conditions specifying when such learning schemesare likely to succeed. Our theory is based on intrinsic properties ofhigh-dimensional spaces. We show that if the ambient or latent decision spaceof a learning machine is sufficiently high-dimensional than a large class ofobjects in this space can indeed be easily learned from few examples providedthat certain data non-concentration conditions are met."
Bridging Few-Shot Learning and Adaptation: New Challenges of  Support-Query Shift,"['Etienne Bennequin', 'Victor Bouvier', 'Myriam Tami', 'Antoine Toubhans', 'Céline Hudelot']",http://arxiv.org/pdf/2105.11804v2.pdf,2021-05-25,"['cs.lg', 'cs.cv']","  Few-Shot Learning (FSL) algorithms have made substantial progress in learningnovel concepts with just a handful of labelled data. To classify queryinstances from novel classes encountered at test-time, they only require asupport set composed of a few labelled samples. FSL benchmarks commonly assumethat those queries come from the same distribution as instances in the supportset. However, in a realistic set-ting, data distribution is plausibly subjectto change, a situation referred to as Distribution Shift (DS). The present workaddresses the new and challenging problem of Few-Shot Learning underSupport/Query Shift (FSQS) i.e., when support and query instances are sampledfrom related but different distributions. Our contributions are the following.First, we release a testbed for FSQS, including datasets, relevant baselinesand a protocol for a rigorous and reproducible evaluation. Second, we observethat well-established FSL algorithms unsurprisingly suffer from a considerabledrop in accuracy when facing FSQS, stressing the significance of our study.Finally, we show that transductive algorithms can limit the inopportune effectof DS. In particular, we study both the role of Batch-Normalization and OptimalTransport (OT) in aligning distributions, bridging Unsupervised DomainAdaptation with FSL. This results in a new method that efficiently combines OTwith the celebrated Prototypical Networks. We bring compelling experimentsdemonstrating the advantage of our method. Our work opens an exciting line ofresearch by providing a testbed and strong baselines. Our code is available athttps://github.com/ebennequin/meta-domain-shift."
Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target  Data,"['Yuqian Fu', 'Yanwei Fu', 'Yu-Gang Jiang']",http://arxiv.org/pdf/2107.11978v1.pdf,2021-07-26,['cs.cv'],"  A recent study finds that existing few-shot learning methods, trained on thesource domain, fail to generalize to the novel target domain when a domain gapis observed. This motivates the task of Cross-Domain Few-Shot Learning(CD-FSL). In this paper, we realize that the labeled target data in CD-FSL hasnot been leveraged in any way to help the learning process. Thus, we advocateutilizing few labeled target data to guide the model learning. Technically, anovel meta-FDMixup network is proposed. We tackle this problem mainly from twoaspects. Firstly, to utilize the source and the newly introduced target data oftwo different class sets, a mixup module is re-proposed and integrated into themeta-learning mechanism. Secondly, a novel disentangle module together with adomain classifier is proposed to extract the disentangled domain-irrelevant anddomain-specific features. These two modules together enable our model to narrowthe domain gap thus generalizing well to the target datasets. Additionally, adetailed feasibility and pilot study is conducted to reflect the intuitiveunderstanding of CD-FSL under our new setting. Experimental results show theeffectiveness of our new setting and the proposed method. Codes and models areavailable at https://github.com/lovelyqian/Meta-FDMixup."
Prototype Completion for Few-Shot Learning,"['Baoquan Zhang', 'Xutao Li', 'Yunming Ye', 'Shanshan Feng']",http://arxiv.org/pdf/2108.05010v1.pdf,2021-08-11,['cs.cv'],"  Few-shot learning aims to recognize novel classes with few examples.Pre-training based methods effectively tackle the problem by pre-training afeature extractor and then fine-tuning it through the nearest centroid basedmeta-learning. However, results show that the fine-tuning step makes marginalimprovements. In this paper, 1) we figure out the reason, i.e., in thepre-trained feature space, the base classes already form compact clusters whilenovel classes spread as groups with large variances, which implies thatfine-tuning feature extractor is less meaningful; 2) instead of fine-tuningfeature extractor, we focus on estimating more representative prototypes.Consequently, we propose a novel prototype completion based meta-learningframework. This framework first introduces primitive knowledge (i.e.,class-level part or attribute annotations) and extracts representative featuresfor seen attributes as priors. Second, a part/attribute transfer network isdesigned to learn to infer the representative features for unseen attributes assupplementary priors. Finally, a prototype completion network is devised tolearn to complete prototypes with these priors. Moreover, to avoid theprototype completion error, we further develop a Gaussian based prototypefusion strategy that fuses the mean-based and completed prototypes byexploiting the unlabeled samples. Extensive experiments show that our method:(i) obtains more accurate prototypes; (ii) achieves superior performance onboth inductive and transductive FSL settings."
Boosting the Generalization Capability in Cross-Domain Few-shot Learning  via Noise-enhanced Supervised Autoencoder,"['Hanwen Liang', 'Qiong Zhang', 'Peng Dai', 'Juwei Lu']",http://arxiv.org/pdf/2108.05028v2.pdf,2021-08-11,"['cs.cv', 'cs.lg']","  State of the art (SOTA) few-shot learning (FSL) methods suffer significantperformance drop in the presence of domain differences between source andtarget datasets. The strong discrimination ability on the source dataset doesnot necessarily translate to high classification accuracy on the targetdataset. In this work, we address this cross-domain few-shot learning (CDFSL)problem by boosting the generalization capability of the model. Specifically,we teach the model to capture broader variations of the feature distributionswith a novel noise-enhanced supervised autoencoder (NSAE). NSAE trains themodel by jointly reconstructing inputs and predicting the labels of inputs aswell as their reconstructed pairs. Theoretical analysis based on intra-classcorrelation (ICC) shows that the feature embeddings learned from NSAE havestronger discrimination and generalization abilities in the target domain. Wealso take advantage of NSAE structure and propose a two-step fine-tuningprocedure that achieves better adaption and improves classification performancein the target domain. Extensive experiments and ablation studies are conductedto demonstrate the effectiveness of the proposed method. Experimental resultsshow that our proposed method consistently outperforms SOTA methods undervarious conditions."
Deep few-shot learning for bi-temporal building change detection,"['Mehdi Khoshboresh-Masouleh', 'Reza Shah-Hosseini']",http://arxiv.org/pdf/2108.11262v2.pdf,2021-08-25,"['cs.cv', 'cs.lg', 'eess.iv']","  In real-world applications (e.g., change detection), annotating images isvery expensive. To build effective deep learning models in these applications,deep few-shot learning methods have been developed and prove to be a robustapproach in small training data. The analysis of building change detection fromhigh spatial resolution remote sensing observations is important research inphotogrammetry, computer vision, and remote sensing nowadays, which can bewidely used in a variety of real-world applications, such as map updating. Asmanual high resolution image interpretation is expensive and time-consuming,building change detection methods are of high interest. The interest indeveloping building change detection approaches from optical remote sensingimages is rapidly increasing due to larger coverages, and lower costs ofoptical images. In this study, we focus on building change detection analysison a small set of building change from different regions that sit in severalcities. In this paper, a new deep few-shot learning method is proposed forbuilding change detection using Monte Carlo dropout and remote sensingobservations. The setup is based on a small dataset, including bitemporaloptical images labeled for building change detection."
Few-shot Learning in Emotion Recognition of Spontaneous Speech Using a  Siamese Neural Network with Adaptive Sample Pair Formation,"['Kexin Feng', 'Theodora Chaspari']",http://arxiv.org/pdf/2109.02915v1.pdf,2021-09-07,['cs.lg'],"  Speech-based machine learning (ML) has been heralded as a promising solutionfor tracking prosodic and spectrotemporal patterns in real-life that areindicative of emotional changes, providing a valuable window into one'scognitive and mental state. Yet, the scarcity of labelled data in ambulatorystudies prevents the reliable training of ML models, which usually rely on""data-hungry"" distribution-based learning. Leveraging the abundance of labelledspeech data from acted emotions, this paper proposes a few-shot learningapproach for automatically recognizing emotion in spontaneous speech from asmall number of labelled samples. Few-shot learning is implemented via a metriclearning approach through a siamese neural network, which models the relativedistance between samples rather than relying on learning absolute patterns ofthe corresponding distributions of each emotion. Results indicate thefeasibility of the proposed metric learning in recognizing emotions fromspontaneous speech in four datasets, even with a small amount of labelledsamples. They further demonstrate superior performance of the proposed metriclearning compared to commonly used adaptation methods, including networkfine-tuning and adversarial learning. Findings from this work provide afoundation for the ambulatory tracking of human emotion in spontaneous speechcontributing to the real-life assessment of mental health degradation."
Fine-Grained Few Shot Learning with Foreground Object Transformation,"['Chaofei Wang', 'Shiji Song', 'Qisen Yang', 'Xiang Li', 'Gao Huang']",http://arxiv.org/pdf/2109.05719v1.pdf,2021-09-13,['cs.cv'],"  Traditional fine-grained image classification generally requires abundantlabeled samples to deal with the low inter-class variance but high intra-classvariance problem. However, in many scenarios we may have limited samples forsome novel sub-categories, leading to the fine-grained few shot learning(FG-FSL) setting. To address this challenging task, we propose a novel methodnamed foreground object transformation (FOT), which is composed of a foregroundobject extractor and a posture transformation generator. The former aims toremove image background, which tends to increase the difficulty of fine-grainedimage classification as it amplifies the intra-class variance while reducesinter-class variance. The latter transforms the posture of the foregroundobject to generate additional samples for the novel sub-category. As a dataaugmentation method, FOT can be conveniently applied to any existing few shotlearning algorithm and greatly improve its performance on FG-FSL tasks. Inparticular, in combination with FOT, simple fine-tuning baseline methods can becompetitive with the state-of-the-art methods both in inductive setting andtransductive setting. Moreover, FOT can further boost the performances oflatest excellent methods and bring them up to the new state-of-the-art. Inaddition, we also show the effectiveness of FOT on general FSL tasks."
Disentangled Feature Representation for Few-shot Image Classification,"['Hao Cheng', 'Yufei Wang', 'Haoliang Li', 'Alex C. Kot', 'Bihan Wen']",http://arxiv.org/pdf/2109.12548v1.pdf,2021-09-26,['cs.cv'],"  Learning the generalizable feature representation is critical for few-shotimage classification. While recent works exploited task-specific featureembedding using meta-tasks for few-shot learning, they are limited in manychallenging tasks as being distracted by the excursive features such as thebackground, domain and style of the image samples. In this work, we propose anovel Disentangled Feature Representation framework, dubbed DFR, for few-shotlearning applications. DFR can adaptively decouple the discriminative featuresthat are modeled by the classification branch, from the class-irrelevantcomponent of the variation branch. In general, most of the popular deepfew-shot learning methods can be plugged in as the classification branch, thusDFR can boost their performance on various few-shot tasks. Furthermore, wepropose a novel FS-DomainNet dataset based on DomainNet, for benchmarking thefew-shot domain generalization tasks. We conducted extensive experiments toevaluate the proposed DFR on general and fine-grained few-shot classification,as well as few-shot domain generalization, using the corresponding fourbenchmarks, i.e., mini-ImageNet, tiered-ImageNet, CUB, as well as the proposedFS-DomainNet. Thanks to the effective feature disentangling, the DFR-basedfew-shot classifiers achieved the state-of-the-art results on all datasets."
Sparse Spatial Transformers for Few-Shot Learning,"['Haoxing Chen', 'Huaxiong Li', 'Yaohui Li', 'Chunlin Chen']",http://arxiv.org/pdf/2109.12932v3.pdf,2021-09-27,['cs.cv'],"  Learning from limited data is challenging because data scarcity leads to apoor generalization of the trained model. A classical global pooledrepresentation will probably lose useful local information. Many few-shotlearning methods have recently addressed this challenge using deep descriptorsand learning a pixel-level metric. However, using deep descriptors as featurerepresentations may lose image contextual information. Moreover, most of thesemethods independently address each class in the support set, which cannotsufficiently use discriminative information and task-specific embeddings. Inthis paper, we propose a novel transformer-based neural network architecturecalled sparse spatial transformers (SSFormers), which finds task-relevantfeatures and suppresses task-irrelevant features. Particularly, we first divideeach input image into several image patches of different sizes to obtain denselocal features. These features retain contextual information while expressinglocal information. Then, a sparse spatial transformer layer is proposed to findspatial correspondence between the query image and the full support set toselect task-relevant image patches and suppress task-irrelevant image patches.Finally, we propose using an image patch-matching module to calculate thedistance between dense local representations, thus determining which categorythe query image belongs to in the support set. Extensive experiments on popularfew-shot learning benchmarks demonstrate the superiority of our method overstate-of-the-art methods. Our source code is available at\url{https://github.com/chenhaoxing/ssformers}."
Omni-Training: Bridging Pre-Training and Meta-Training for Few-Shot  Learning,"['Yang Shu', 'Zhangjie Cao', 'Jinghan Gao', 'Jianmin Wang', 'Philip S. Yu', 'Mingsheng Long']",http://arxiv.org/pdf/2110.07510v3.pdf,2021-10-14,['cs.lg'],"  Few-shot learning aims to fast adapt a deep model from a few examples. Whilepre-training and meta-training can create deep models powerful for few-shotgeneralization, we find that pre-training and meta-training focusesrespectively on cross-domain transferability and cross-task transferability,which restricts their data efficiency in the entangled settings of domain shiftand task shift. We thus propose the Omni-Training framework to seamlesslybridge pre-training and meta-training for data-efficient few-shot learning. Ourfirst contribution is a tri-flow Omni-Net architecture. Besides the jointrepresentation flow, Omni-Net introduces two parallel flows for pre-trainingand meta-training, responsible for improving domain transferability and tasktransferability respectively. Omni-Net further coordinates the parallel flowsby routing their representations via the joint-flow, enabling knowledgetransfer across flows. Our second contribution is the Omni-Loss, whichintroduces a self-distillation strategy separately on the pre-training andmeta-training objectives for boosting knowledge transfer throughout differenttraining stages. Omni-Training is a general framework to accommodate manyexisting algorithms. Evaluations justify that our single framework consistentlyand clearly outperforms the individual state-of-the-art methods on bothcross-task and cross-domain settings in a variety of classification, regressionand reinforcement learning problems."
Ranking Distance Calibration for Cross-Domain Few-Shot Learning,"['Pan Li', 'Shaogang Gong', 'Chengjie Wang', 'Yanwei Fu']",http://arxiv.org/pdf/2112.00260v2.pdf,2021-12-01,"['cs.cv', 'cs.lg']","  Recent progress in few-shot learning promotes a more realistic cross-domainsetting, where the source and target datasets are from different domains. Dueto the domain gap and disjoint label spaces between source and target datasets,their shared knowledge is extremely limited. This encourages us to explore moreinformation in the target domain rather than to overly elaborate trainingstrategies on the source domain as in many existing methods. Hence, we startfrom a generic representation pre-trained by a cross-entropy loss and aconventional distance-based classifier, along with an image retrieval view, toemploy a re-ranking process for calibrating a target distance matrix bydiscovering the reciprocal k-nearest neighbours within the task. Assuming thepre-trained representation is biased towards the source, we construct anon-linear subspace to minimise task-irrelevant features therewithin while keepmore transferrable discriminative information by a hyperbolic tangenttransformation. The calibrated distance in this target-aware non-linearsubspace is complementary to that in the pre-trained representation. To imposesuch distance calibration information onto the pre-trained representation, aKullback-Leibler divergence loss is employed to gradually guide the modeltowards the calibrated distance-based distribution. Extensive evaluations oneight target domains show that this target ranking calibration process canimprove conventional distance-based classifiers in few-shot learning."
Adaptive Poincaré Point to Set Distance for Few-Shot Classification,"['Rongkai Ma', 'Pengfei Fang', 'Tom Drummond', 'Mehrtash Harandi']",http://arxiv.org/pdf/2112.01719v1.pdf,2021-12-03,"['cs.cv', 'cs.lg']","  Learning and generalizing from limited examples, i,e, few-shot learning, isof core importance to many real-world vision applications. A principal way ofachieving few-shot learning is to realize an embedding where samples fromdifferent classes are distinctive. Recent studies suggest that embedding viahyperbolic geometry enjoys low distortion for hierarchical and structured data,making it suitable for few-shot learning. In this paper, we propose to learn acontext-aware hyperbolic metric to characterize the distance between a pointand a set associated with a learned set to set distance. To this end, weformulate the metric as a weighted sum on the tangent bundle of the hyperbolicspace and develop a mechanism to obtain the weights adaptively and based on theconstellation of the points. This not only makes the metric local but alsodependent on the task in hand, meaning that the metric will adapt depending onthe samples that it compares. We empirically show that such metric yieldsrobustness in the presence of outliers and achieves a tangible improvement overbaseline models. This includes the state-of-the-art results on five popularfew-shot classification benchmarks, namely mini-ImageNet, tiered-ImageNet,Caltech-UCSD Birds-200-2011 (CUB), CIFAR-FS, and FC100."
The Curse of Zero Task Diversity: On the Failure of Transfer Learning to  Outperform MAML and their Empirical Equivalence,"['Brando Miranda', 'Yu-Xiong Wang', 'Sanmi Koyejo']",http://arxiv.org/pdf/2112.13121v4.pdf,2021-12-24,"['cs.lg', 'cs.ai', 'cs.cv', 'cs.ne']","  Recently, it has been observed that a transfer learning solution might be allwe need to solve many few-shot learning benchmarks -- thus raising importantquestions about when and how meta-learning algorithms should be deployed. Inthis paper, we seek to clarify these questions by proposing a novel metric --the diversity coefficient -- to measure the diversity of tasks in a few-shotlearning benchmark. We hypothesize that the diversity coefficient of thefew-shot learning benchmark is predictive of whether meta-learning solutionswill succeed or not. Using the diversity coefficient, we show that theMiniImagenet benchmark has zero diversity. This novel insight contextualizesclaims that transfer learning solutions are better than meta-learned solutions.Specifically, we empirically find that a diversity coefficient of zerocorrelates with a high similarity between transfer learning and Model-AgnosticMeta-Learning (MAML) learned solutions in terms of meta-accuracy (at meta-testtime). Therefore, we conjecture meta-learned solutions have the same meta-testperformance as transfer learning when the diversity coefficient is zero. Ourwork provides the first test of whether diversity correlates with meta-learningsuccess."
Multi-level Second-order Few-shot Learning,"['Hongguang Zhang', 'Hongdong Li', 'Piotr Koniusz']",http://arxiv.org/pdf/2201.05916v1.pdf,2022-01-15,['cs.cv'],"  We propose a Multi-level Second-order (MlSo) few-shot learning network forsupervised or unsupervised few-shot image classification and few-shot actionrecognition. We leverage so-called power-normalized second-order base learnerstreams combined with features that express multiple levels of visualabstraction, and we use self-supervised discriminating mechanisms. AsSecond-order Pooling (SoP) is popular in image recognition, we employ its basicelement-wise variant in our pipeline. The goal of multi-level feature design isto extract feature representations at different layer-wise levels of CNN,realizing several levels of visual abstraction to achieve robust few-shotlearning. As SoP can handle convolutional feature maps of varying spatialsizes, we also introduce image inputs at multiple spatial scales into MlSo. Toexploit the discriminative information from multi-level and multi-scalefeatures, we develop a Feature Matching (FM) module that reweights theirrespective branches. We also introduce a self-supervised step, which is adiscriminator of the spatial level and the scale of abstraction. Our pipelineis trained in an end-to-end manner. With a simple architecture, we demonstraterespectable results on standard datasets such as Omniglot, mini-ImageNet,tiered-ImageNet, Open MIC, fine-grained datasets such as CUB Birds, StanfordDogs and Cars, and action recognition datasets such as HMDB51, UCF101, andmini-MIT."
Wave-SAN: Wavelet based Style Augmentation Network for Cross-Domain  Few-Shot Learning,"['Yuqian Fu', 'Yu Xie', 'Yanwei Fu', 'Jingjing Chen', 'Yu-Gang Jiang']",http://arxiv.org/pdf/2203.07656v1.pdf,2022-03-15,['cs.cv'],"  Previous few-shot learning (FSL) works mostly are limited to natural imagesof general concepts and categories. These works assume very high visualsimilarity between the source and target classes. In contrast, the recentlyproposed cross-domain few-shot learning (CD-FSL) aims at transferring knowledgefrom general nature images of many labeled examples to novel domain-specifictarget categories of only a few labeled examples. The key challenge of CD-FSLlies in the huge data shift between source and target domains, which istypically in the form of totally different visual styles. This makes it verynontrivial to directly extend the classical FSL methods to address the CD-FSLtask. To this end, this paper studies the problem of CD-FSL by spanning thestyle distributions of the source dataset. Particularly, wavelet transform isintroduced to enable the decomposition of visual representations intolow-frequency components such as shape and style and high-frequency componentse.g., texture. To make our model robust to visual styles, the source images areaugmented by swapping the styles of their low-frequency components with eachother. We propose a novel Style Augmentation (StyleAug) module to implementthis idea. Furthermore, we present a Self-Supervised Learning (SSL) module toensure the predictions of style-augmented images are semantically similar tothe unchanged ones. This avoids the potential semantic drift problem inexchanging the styles. Extensive experiments on two CD-FSL benchmarks show theeffectiveness of our method. Our codes and models will be released."
Meta-Learning of NAS for Few-shot Learning in Medical Image Applications,"['Viet-Khoa Vo-Ho', 'Kashu Yamazaki', 'Hieu Hoang', 'Minh-Triet Tran', 'Ngan Le']",http://arxiv.org/pdf/2203.08951v1.pdf,2022-03-16,"['cs.lg', 'cs.cv', 'eess.iv']","  Deep learning methods have been successful in solving tasks in machinelearning and have made breakthroughs in many sectors owing to their ability toautomatically extract features from unstructured data. However, theirperformance relies on manual trial-and-error processes for selecting anappropriate network architecture, hyperparameters for training, andpre-/post-procedures. Even though it has been shown that network architectureplays a critical role in learning feature representation feature from data andthe final performance, searching for the best network architecture iscomputationally intensive and heavily relies on researchers' experience.Automated machine learning (AutoML) and its advanced techniques i.e. NeuralArchitecture Search (NAS) have been promoted to address those limitations. Notonly in general computer vision tasks, but NAS has also motivated variousapplications in multiple areas including medical imaging. In medical imaging,NAS has significant progress in improving the accuracy of image classification,segmentation, reconstruction, and more. However, NAS requires the availabilityof large annotated data, considerable computation resources, and pre-definedtasks. To address such limitations, meta-learning has been adopted in thescenarios of few-shot learning and multiple tasks. In this book chapter, wefirst present a brief review of NAS by discussing well-known approaches insearch space, search strategy, and evaluation strategy. We then introducevarious NAS approaches in medical imaging with different applications such asclassification, segmentation, detection, reconstruction, etc. Meta-learning inNAS for few-shot learning and multiple tasks is then explained. Finally, wedescribe several open problems in NAS."
Interval Bound Interpolation for Few-shot Learning with Few Tasks,"['Shounak Datta', 'Sankha Subhra Mullick', 'Anish Chakrabarty', 'Swagatam Das']",http://arxiv.org/pdf/2204.03511v4.pdf,2022-04-07,['cs.lg'],"  Few-shot learning aims to transfer the knowledge acquired from training on adiverse set of tasks to unseen tasks from the same task distribution with alimited amount of labeled data. The underlying requirement for effectivefew-shot generalization is to learn a good representation of the task manifold.This becomes more difficult when only a limited number of tasks are availablefor training. In such a few-task few-shot setting, it is beneficial toexplicitly preserve the local neighborhoods from the task manifold and exploitthis to generate artificial tasks for training. To this end, we introduce thenotion of interval bounds from the provably robust training literature tofew-shot learning. The interval bounds are used to characterize neighborhoodsaround the training tasks. These neighborhoods can then be preserved byminimizing the distance between a task and its respective bounds. We then use anovel strategy to artificially form new tasks for training by interpolatingbetween the available tasks and their respective interval bounds. We apply ourframework to both model-agnostic meta-learning as well as prototype-basedmetric-learning paradigms. The efficacy of our proposed approach is evidentfrom the improved performance on several datasets from diverse domains comparedto current methods."
Pushing the Limits of Simple Pipelines for Few-Shot Learning: External  Data and Fine-Tuning Make a Difference,"['Shell Xu Hu', 'Da Li', 'Jan Stühmer', 'Minyoung Kim', 'Timothy M. Hospedales']",http://arxiv.org/pdf/2204.07305v1.pdf,2022-04-15,"['cs.cv', 'cs.lg']","  Few-shot learning (FSL) is an important and topical problem in computervision that has motivated extensive research into numerous methods spanningfrom sophisticated meta-learning methods to simple transfer learning baselines.We seek to push the limits of a simple-but-effective pipeline for morerealistic and practical settings of few-shot image classification. To this end,we explore few-shot learning from the perspective of neural networkarchitecture, as well as a three stage pipeline of network updates underdifferent data supplies, where unsupervised external data is considered forpre-training, base categories are used to simulate few-shot tasks formeta-training, and the scarcely labelled data of an novel task is taken forfine-tuning. We investigate questions such as: (1) How pre-training on externaldata benefits FSL? (2) How state-of-the-art transformer architectures can beexploited? and (3) How fine-tuning mitigates domain shift? Ultimately, we showthat a simple transformer-based pipeline yields surprisingly good performanceon standard benchmarks such as Mini-ImageNet, CIFAR-FS, CDFSL and Meta-Dataset.Our code and demo are available at https://hushell.github.io/pmf."
"A Comprehensive Survey of Few-shot Learning: Evolution, Applications,  Challenges, and Opportunities","['Yisheng Song', 'Ting Wang', 'Subrota K Mondal', 'Jyoti Prakash Sahoo']",http://arxiv.org/pdf/2205.06743v2.pdf,2022-05-13,['cs.lg'],"  Few-shot learning (FSL) has emerged as an effective learning method and showsgreat potential. Despite the recent creative works in tackling FSL tasks,learning valid information rapidly from just a few or even zero samples stillremains a serious challenge. In this context, we extensively investigated 200+latest papers on FSL published in the past three years, aiming to present atimely and comprehensive overview of the most recent advances in FSL along withimpartial comparisons of the strengths and weaknesses of the existing works.For the sake of avoiding conceptual confusion, we first elaborate and compare aset of similar concepts including few-shot learning, transfer learning, andmeta-learning. Furthermore, we propose a novel taxonomy to classify theexisting work according to the level of abstraction of knowledge in accordancewith the challenges of FSL. To enrich this survey, in each subsection weprovide in-depth analysis and insightful discussion about recent advances onthese topics. Moreover, taking computer vision as an example, we highlight theimportant application of FSL, covering various research hotspots. Finally, weconclude the survey with unique insights into the technology evolution trendstogether with potential future research opportunities in the hope of providingguidance to follow-up research."
Adaptive Few-Shot Learning Algorithm for Rare Sound Event Detection,"['Chendong Zhao', 'Jianzong Wang', 'Leilai Li', 'Xiaoyang Qu', 'Jing Xiao']",http://arxiv.org/pdf/2205.11738v2.pdf,2022-05-24,"['cs.sd', 'cs.ai', 'eess.as']","  Sound event detection is to infer the event by understanding the surroundingenvironmental sounds. Due to the scarcity of rare sound events, it becomeschallenging for the well-trained detectors which have learned too much priorknowledge. Meanwhile, few-shot learning methods promise a good generalizationability when facing a new limited-data task. Recent approaches have achievedpromising results in this field. However, these approaches treat each supportexample independently, ignoring the information of other examples from thewhole task. Because of this, most of previous methods are constrained togenerate a same feature embedding for all test-time tasks, which is notadaptive to each inputted data. In this work, we propose a novel task-adaptivemodule which is easy to plant into any metric-based few-shot learningframeworks. The module could identify the task-relevant feature dimension.Incorporating our module improves the performance considerably on two datasetsover baseline methods, especially for the transductive propagation network.Such as +6.8% for 5-way 1-shot accuracy on ESC-50, and +5.9% on noiseESC-50. Weinvestigate our approach in the domain-mismatch setting and also achieve betterresults than previous methods."
Spatio-Temporal Graph Few-Shot Learning with Cross-City Knowledge  Transfer,"['Bin Lu', 'Xiaoying Gan', 'Weinan Zhang', 'Huaxiu Yao', 'Luoyi Fu', 'Xinbing Wang']",http://arxiv.org/pdf/2205.13947v2.pdf,2022-05-27,"['cs.lg', 'cs.ai']","  Spatio-temporal graph learning is a key method for urban computing tasks,such as traffic flow, taxi demand and air quality forecasting. Due to the highcost of data collection, some developing cities have few available data, whichmakes it infeasible to train a well-performed model. To address this challenge,cross-city knowledge transfer has shown its promise, where the model learnedfrom data-sufficient cities is leveraged to benefit the learning process ofdata-scarce cities. However, the spatio-temporal graphs among different citiesshow irregular structures and varied features, which limits the feasibility ofexisting Few-Shot Learning (\emph{FSL}) methods. Therefore, we propose amodel-agnostic few-shot learning framework for spatio-temporal graph calledST-GFSL. Specifically, to enhance feature extraction by transfering cross-cityknowledge, ST-GFSL proposes to generate non-shared parameters based onnode-level meta knowledge. The nodes in target city transfer the knowledge viaparameter matching, retrieving from similar spatio-temporal characteristics.Furthermore, we propose to reconstruct the graph structure duringmeta-learning. The graph reconstruction loss is defined to guidestructure-aware learning, avoiding structure deviation among differentdatasets. We conduct comprehensive experiments on four traffic speed predictionbenchmarks and the results demonstrate the effectiveness of ST-GFSL comparedwith state-of-the-art methods."
HyperMAML: Few-Shot Adaptation of Deep Models with Hypernetworks,"['M. Przewięźlikowski', 'P. Przybysz', 'J. Tabor', 'M. Zięba', 'P. Spurek']",http://arxiv.org/pdf/2205.15745v2.pdf,2022-05-31,"['cs.lg', 'cs.ai']","  The aim of Few-Shot learning methods is to train models which can easilyadapt to previously unseen tasks, based on small amounts of data. One of themost popular and elegant Few-Shot learning approaches is Model-AgnosticMeta-Learning (MAML). The main idea behind this method is to learn the generalweights of the meta-model, which are further adapted to specific problems in asmall number of gradient steps. However, the model's main limitation lies inthe fact that the update procedure is realized by gradient-based optimisation.In consequence, MAML cannot always modify weights to the essential level in oneor even a few gradient iterations. On the other hand, using many gradient stepsresults in a complex and time-consuming optimization procedure, which is hardto train in practice, and may lead to overfitting. In this paper, we proposeHyperMAML, a novel generalization of MAML, where the training of the updateprocedure is also part of the model. Namely, in HyperMAML, instead of updatingthe weights with gradient descent, we use for this purpose a trainableHypernetwork. Consequently, in this framework, the model can generatesignificant updates whose range is not limited to a fixed number of gradientsteps. Experiments show that HyperMAML consistently outperforms MAML andperforms comparably to other state-of-the-art techniques in a number ofstandard Few-Shot learning benchmarks."
"Multi-Level Fine-Tuning, Data Augmentation, and Few-Shot Learning for  Specialized Cyber Threat Intelligence","['Markus Bayer', 'Tobias Frey', 'Christian Reuter']",http://arxiv.org/pdf/2207.11076v1.pdf,2022-07-22,"['cs.cr', 'cs.cl']","  Gathering cyber threat intelligence from open sources is becomingincreasingly important for maintaining and achieving a high level of securityas systems become larger and more complex. However, these open sources areoften subject to information overload. It is therefore useful to apply machinelearning models that condense the amount of information to what is necessary.Yet, previous studies and applications have shown that existing classifiers arenot able to extract specific information about emerging cybersecurity eventsdue to their low generalization ability. Therefore, we propose a system toovercome this problem by training a new classifier for each new incident. Sincethis requires a lot of labelled data using standard training methods, wecombine three different low-data regime techniques - transfer learning, dataaugmentation, and few-shot learning - to train a high-quality classifier fromvery few labelled instances. We evaluated our approach using a novel datasetderived from the Microsoft Exchange Server data breach of 2021 which waslabelled by three experts. Our findings reveal an increase in F1 score of morethan 21 points compared to standard training methods and more than 18 pointscompared to a state-of-the-art method in few-shot learning. Furthermore, theclassifier trained with this method and 32 instances is only less than 5 F1score points worse than a classifier trained with 1800 instances."
The Curse of Low Task Diversity: On the Failure of Transfer Learning to  Outperform MAML and Their Empirical Equivalence,"['Brando Miranda', 'Patrick Yu', 'Yu-Xiong Wang', 'Sanmi Koyejo']",http://arxiv.org/pdf/2208.01545v1.pdf,2022-08-02,['cs.lg'],"  Recently, it has been observed that a transfer learning solution might be allwe need to solve many few-shot learning benchmarks -- thus raising importantquestions about when and how meta-learning algorithms should be deployed. Inthis paper, we seek to clarify these questions by 1. proposing a novel metric-- the diversity coefficient -- to measure the diversity of tasks in a few-shotlearning benchmark and 2. by comparing Model-Agnostic Meta-Learning (MAML) andtransfer learning under fair conditions (same architecture, same optimizer, andall models trained to convergence). Using the diversity coefficient, we showthat the popular MiniImageNet and CIFAR-FS few-shot learning benchmarks havelow diversity. This novel insight contextualizes claims that transfer learningsolutions are better than meta-learned solutions in the regime of low diversityunder a fair comparison. Specifically, we empirically find that a low diversitycoefficient correlates with a high similarity between transfer learning andMAML learned solutions in terms of accuracy at meta-test time andclassification layer similarity (using feature based distance metrics likeSVCCA, PWCCA, CKA, and OPD). To further support our claim, we find thismeta-test accuracy holds even as the model size changes. Therefore, we concludethat in the low diversity regime, MAML and transfer learning have equivalentmeta-test performance when both are compared fairly. We also hope our workinspires more thoughtful constructions and quantitative evaluations ofmeta-learning benchmarks in the future."
Domain-invariant Prototypes for Semantic Segmentation,"['Zhengeng Yang', 'Hongshan Yu', 'Wei Sun', ' Li-Cheng', 'Ajmal Mian']",http://arxiv.org/pdf/2208.06087v1.pdf,2022-08-12,['cs.cv'],"  Deep Learning has greatly advanced the performance of semantic segmentation,however, its success relies on the availability of large amounts of annotateddata for training. Hence, many efforts have been devoted to domain adaptivesemantic segmentation that focuses on transferring semantic knowledge from alabeled source domain to an unlabeled target domain. Existing self-trainingmethods typically require multiple rounds of training, while another popularframework based on adversarial training is known to be sensitive tohyper-parameters. In this paper, we present an easy-to-train framework thatlearns domain-invariant prototypes for domain adaptive semantic segmentation.In particular, we show that domain adaptation shares a common character withfew-shot learning in that both aim to recognize some types of unseen data withknowledge learned from large amounts of seen data. Thus, we propose a unifiedframework for domain adaptation and few-shot learning. The core idea is to usethe class prototypes extracted from few-shot annotated target images toclassify pixels of both source images and target images. Our method involvesonly one-stage training and does not need to be trained on large-scaleun-annotated target images. Moreover, our method can be extended to variants ofboth domain adaptation and few-shot learning. Experiments on adaptingGTA5-to-Cityscapes and SYNTHIA-to-Cityscapes show that our method achievescompetitive performance to state-of-the-art."
Gradient-Based Meta-Learning Using Uncertainty to Weigh Loss for  Few-Shot Learning,"['Lin Ding', 'Peng Liu', 'Wenfeng Shen', 'Weijia Lu', 'Shengbo Chen']",http://arxiv.org/pdf/2208.08135v1.pdf,2022-08-17,['cs.lg'],"  Model-Agnostic Meta-Learning (MAML) is one of the most successfulmeta-learning techniques for few-shot learning. It uses gradient descent tolearn commonalities between various tasks, enabling the model to learn themeta-initialization of its own parameters to quickly adapt to new tasks using asmall amount of labeled training data. A key challenge to few-shot learning istask uncertainty. Although a strong prior can be obtained from meta-learningwith a large number of tasks, a precision model of the new task cannot beguaranteed because the volume of the training dataset is normally too small. Inthis study, first,in the process of choosing initialization parameters, the newmethod is proposed for task-specific learner adaptively learn to selectinitialization parameters that minimize the loss of new tasks. Then, we proposetwo improved methods for the meta-loss part: Method 1 generates weights bycomparing meta-loss differences to improve the accuracy when there are fewclasses, and Method 2 introduces the homoscedastic uncertainty of each task toweigh multiple losses based on the original gradient descent,as a way toenhance the generalization ability to novel classes while ensuring accuracyimprovement. Compared with previous gradient-based meta-learning methods, ourmodel achieves better performance in regression tasks and few-shotclassification and improves the robustness of the model to the learning rateand query sets in the meta-test set."
Expanding continual few-shot learning benchmarks to include recognition  of specific instances,"['Gideon Kowadlo', 'Abdelrahman Ahmed', 'Amir Mayan', 'David Rawlinson']",http://arxiv.org/pdf/2209.07863v3.pdf,2022-08-26,"['cs.ne', 'cs.lg', 'i.2.6; i.5.0; i.5.1']","  Continual learning and few-shot learning are important frontiers in progresstowards broader Machine Learning (ML) capabilities. There is a growing body ofwork in both, but few works combining the two. One exception is the Continualfew-shot Learning (CFSL) framework of Antoniou et al. arXiv:2004.11967. In thisstudy, we extend CFSL in two ways that capture a broader range of challenges,important for intelligent agent behaviour in real-world conditions. First, wemodify CFSL to make it more comparable to standard continual learningexperiments, where usually a much larger number of classes are presented.Second, we introduce an 'instance test' which requires recognition of specificinstances of classes -- a capability of animal cognition that is usuallyneglected in ML. For an initial exploration of ML model performance under theseconditions, we selected representative baseline models from the original CFSLwork and added a model variant with replay. As expected, learning more classesis more difficult than the original CFSL experiments, and interestingly, theway in which image instances and classes are presented affects classificationperformance. Surprisingly, accuracy in the baseline instance test is comparableto other classification tasks, but poor given significant occlusion and noise.The use of replay for consolidation improves performance substantially for bothtypes of tasks, but particularly the instance test."
Multitask Pre-training of Modular Prompt for Chinese Few-Shot Learning,"['Tianxiang Sun', 'Zhengfu He', 'Qin Zhu', 'Xipeng Qiu', 'Xuanjing Huang']",http://arxiv.org/pdf/2210.07565v3.pdf,2022-10-14,['cs.cl'],"  Prompt tuning is a parameter-efficient approach to adapting pre-trainedlanguage models to downstream tasks. Although prompt tuning has been shown tomatch the performance of full model tuning when training data is sufficient, ittends to struggle in few-shot learning settings. In this paper, we presentMulti-task Pre-trained Modular Prompt (MP2) to boost prompt tuning for few-shotlearning. MP2 is a set of combinable prompts pre-trained on 38 Chinese tasks.On downstream tasks, the pre-trained prompts are selectively activated andcombined, leading to strong compositional generalization to unseen tasks. Tobridge the gap between pre-training and fine-tuning, we formulate upstream anddownstream tasks into a unified machine reading comprehension task. Extensiveexperiments under two learning paradigms, i.e., gradient descent and black-boxtuning, show that MP2 significantly outperforms prompt tuning, full modeltuning, and prior prompt pre-training methods in few-shot settings. Inaddition, we demonstrate that MP2 can achieve surprisingly fast and strongadaptation to downstream tasks by merely learning 8 parameters to combine thepre-trained modular prompts."
Few-Shot Learning of Compact Models via Task-Specific Meta Distillation,"['Yong Wu', 'Shekhor Chanda', 'Mehrdad Hosseinzadeh', 'Zhi Liu', 'Yang Wang']",http://arxiv.org/pdf/2210.09922v1.pdf,2022-10-18,['cs.lg'],"  We consider a new problem of few-shot learning of compact models.Meta-learning is a popular approach for few-shot learning. Previous work inmeta-learning typically assumes that the model architecture duringmeta-training is the same as the model architecture used for final deployment.In this paper, we challenge this basic assumption. For final deployment, weoften need the model to be small. But small models usually do not have enoughcapacity to effectively adapt to new tasks. In the mean time, we often haveaccess to the large dataset and extensive computing power during meta-trainingsince meta-training is typically performed on a server. In this paper, wepropose task-specific meta distillation that simultaneously learns two modelsin meta-learning: a large teacher model and a small student model. These twomodels are jointly learned during meta-training. Given a new task duringmeta-testing, the teacher model is first adapted to this task, then the adaptedteacher model is used to guide the adaptation of the student model. The adaptedstudent model is used for final deployment. We demonstrate the effectiveness ofour approach in few-shot image classification using model-agnosticmeta-learning (MAML). Our proposed method outperforms other alternatives onseveral benchmark datasets."
Visual-Semantic Contrastive Alignment for Few-Shot Image Classification,"['Mohamed Afham', 'Ranga Rodrigo']",http://arxiv.org/pdf/2210.11000v1.pdf,2022-10-20,['cs.cv'],"  Few-Shot learning aims to train and optimize a model that can adapt to unseenvisual classes with only a few labeled examples. The existing few-shot learning(FSL) methods, heavily rely only on visual data, thus fail to capture thesemantic attributes to learn a more generalized version of the visual conceptfrom very few examples. However, it is a known fact that human visual learningbenefits immensely from inputs from multiple modalities such as vision,language, and audio. Inspired by the human learning nature of encapsulating theexisting knowledge of a visual category which is in the form of language, weintroduce a contrastive alignment mechanism for visual and semantic featurevectors to learn much more generalized visual concepts for few-shot learning.Our method simply adds an auxiliary contrastive learning objective whichcaptures the contextual knowledge of a visual category from a strong textualencoder in addition to the existing training mechanism. Hence, the approach ismore generalized and can be plugged into any existing FSL method. Thepre-trained semantic feature extractor (learned from a large-scale textcorpora) we use in our approach provides a strong contextual prior knowledge toassist FSL. The experimental results done in popular FSL datasets show that ourapproach is generic in nature and provides a strong boost to the existing FSLbaselines."
Graph Few-shot Learning with Task-specific Structures,"['Song Wang', 'Chen Chen', 'Jundong Li']",http://arxiv.org/pdf/2210.12130v1.pdf,2022-10-21,"['cs.lg', 'cs.si']","  Graph few-shot learning is of great importance among various graph learningtasks. Under the few-shot scenario, models are often required to conductclassification given limited labeled samples. Existing graph few-shot learningmethods typically leverage Graph Neural Networks (GNNs) and performclassification across a series of meta-tasks. Nevertheless, these methodsgenerally rely on the original graph (i.e., the graph that the meta-task issampled from) to learn node representations. Consequently, the graph structureused in each meta-task is identical. Since the class sets are different acrossmeta-tasks, node representations should be learned in a task-specific manner topromote classification performance. Therefore, to adaptively learn noderepresentations across meta-tasks, we propose a novel framework that learns atask-specific structure for each meta-task. To handle the variety of nodesacross meta-tasks, we extract relevant nodes and learn task-specific structuresbased on node influence and mutual information. In this way, we can learn noderepresentations with the task-specific structure tailored for each meta-task.We further conduct extensive experiments on five node classification datasetsunder both single- and multiple-graph settings to validate the superiority ofour framework over the state-of-the-art baselines. Our code is provided athttps://github.com/SongW-SW/GLITTER."
Alleviating the Sample Selection Bias in Few-shot Learning by Removing  Projection to the Centroid,"['Jing Xu', 'Xu Luo', 'Xinglin Pan', 'Wenjie Pei', 'Yanan Li', 'Zenglin Xu']",http://arxiv.org/pdf/2210.16834v1.pdf,2022-10-30,['cs.cv'],"  Few-shot learning (FSL) targets at generalization of vision models towardsunseen tasks without sufficient annotations. Despite the emergence of a numberof few-shot learning methods, the sample selection bias problem, i.e., thesensitivity to the limited amount of support data, has not been wellunderstood. In this paper, we find that this problem usually occurs when thepositions of support samples are in the vicinity of task centroid -- the meanof all class centroids in the task. This motivates us to propose an extremelysimple feature transformation to alleviate this problem, dubbed Task CentroidProjection Removing (TCPR). TCPR is applied directly to all image features in agiven task, aiming at removing the dimension of features along the direction ofthe task centroid. While the exact task centroid cannot be accurately obtainedfrom limited data, we estimate it using base features that are each similar toone of the support features. Our method effectively prevents features frombeing too close to the task centroid. Extensive experiments over ten datasetsfrom different domains show that TCPR can reliably improve classificationaccuracy across various feature extractors, training algorithms and datasets.The code has been made available at https://github.com/KikimorMay/FSL-TCBR."
Robust Few-shot Learning Without Using any Adversarial Samples,"['Gaurav Kumar Nayak', 'Ruchit Rawal', 'Inder Khatri', 'Anirban Chakraborty']",http://arxiv.org/pdf/2211.01598v1.pdf,2022-11-03,"['cs.cv', 'cs.lg']","  The high cost of acquiring and annotating samples has made the `few-shot'learning problem of prime importance. Existing works mainly focus on improvingperformance on clean data and overlook robustness concerns on the dataperturbed with adversarial noise. Recently, a few efforts have been made tocombine the few-shot problem with the robustness objective using sophisticatedMeta-Learning techniques. These methods rely on the generation of adversarialsamples in every episode of training, which further adds a computationalburden. To avoid such time-consuming and complicated procedures, we propose asimple but effective alternative that does not require any adversarial samples.Inspired by the cognitive decision-making process in humans, we enforcehigh-level feature matching between the base class data and their correspondinglow-frequency samples in the pretraining stage via self distillation. The modelis then fine-tuned on the samples of novel classes where we additionallyimprove the discriminability of low-frequency query set features via cosinesimilarity. On a 1-shot setting of the CIFAR-FS dataset, our method yields amassive improvement of $60.55\%$ & $62.05\%$ in adversarial accuracy on the PGDand state-of-the-art Auto Attack, respectively, with a minor drop in cleanaccuracy compared to the baseline. Moreover, our method only takes $1.69\times$of the standard training time while being $\approx$ $5\times$ faster thanstate-of-the-art adversarial meta-learning methods. The code is available athttps://github.com/vcl-iisc/robust-few-shot-learning."
Few-shot Classification with Hypersphere Modeling of Prototypes,"['Ning Ding', 'Yulin Chen', 'Ganqu Cui', 'Xiaobin Wang', 'Hai-Tao Zheng', 'Zhiyuan Liu', 'Pengjun Xie']",http://arxiv.org/pdf/2211.05319v1.pdf,2022-11-10,"['cs.lg', 'cs.cl', 'cs.cv']","  Metric-based meta-learning is one of the de facto standards in few-shotlearning. It composes of representation learning and metrics calculationdesigns. Previous works construct class representations in different ways,varying from mean output embedding to covariance and distributions. However,using embeddings in space lacks expressivity and cannot capture classinformation robustly, while statistical complex modeling poses difficulty tometric designs. In this work, we use tensor fields (``areas'') to model classesfrom the geometrical perspective for few-shot learning. We present a simple andeffective method, dubbed hypersphere prototypes (HyperProto), where classinformation is represented by hyperspheres with dynamic sizes with two sets oflearnable parameters: the hypersphere's center and the radius. Extending frompoints to areas, hyperspheres are much more expressive than embeddings.Moreover, it is more convenient to perform metric-based classification withhypersphere prototypes than statistical modeling, as we only need to calculatethe distance from a data point to the surface of the hypersphere. Followingthis idea, we also develop two variants of prototypes under other measurements.Extensive experiments and analysis on few-shot learning tasks across NLP and CVand comparison with 20+ competitive baselines demonstrate the effectiveness ofour approach."
PatchMix Augmentation to Identify Causal Features in Few-shot Learning,"['Chengming Xu', 'Chen Liu', 'Xinwei Sun', 'Siqian Yang', 'Yabiao Wang', 'Chengjie Wang', 'Yanwei Fu']",http://arxiv.org/pdf/2211.16019v1.pdf,2022-11-29,['cs.cv'],"  The task of Few-shot learning (FSL) aims to transfer the knowledge learnedfrom base categories with sufficient labelled data to novel categories withscarce known information. It is currently an important research question andhas great practical values in the real-world applications. Despite extensiveprevious efforts are made on few-shot learning tasks, we emphasize that mostexisting methods did not take into account the distributional shift caused bysample selection bias in the FSL scenario. Such a selection bias can inducespurious correlation between the semantic causal features, that are causallyand semantically related to the class label, and the other non-causal features.Critically, the former ones should be invariant across changes indistributions, highly related to the classes of interest, and thus wellgeneralizable to novel classes, while the latter ones are not stable to changesin the distribution. To resolve this problem, we propose a novel dataaugmentation strategy dubbed as PatchMix that can break this spuriousdependency by replacing the patch-level information and supervision of thequery images with random gallery images from different classes from the queryones. We theoretically show that such an augmentation mechanism, different fromexisting ones, is able to identify the causal features. To further make thesefeatures to be discriminative enough for classification, we proposeCorrelation-guided Reconstruction (CGR) and Hardness-Aware module for instancediscrimination and easier discrimination between similar classes. Moreover,such a framework can be adapted to the unsupervised FSL scenario."
Better Generalized Few-Shot Learning Even Without Base Data,"['Seong-Woong Kim', 'Dong-Wan Choi']",http://arxiv.org/pdf/2211.16095v2.pdf,2022-11-29,"['cs.lg', 'cs.ai', 'cs.cv']","  This paper introduces and studies zero-base generalized few-shot learning(zero-base GFSL), which is an extreme yet practical version of few-shotlearning problem. Motivated by the cases where base data is not available dueto privacy or ethical issues, the goal of zero-base GFSL is to newlyincorporate the knowledge of few samples of novel classes into a pretrainedmodel without any samples of base classes. According to our analysis, wediscover the fact that both mean and variance of the weight distribution ofnovel classes are not properly established, compared to those of base classes.The existing GFSL methods attempt to make the weight norms balanced, which wefind helps only the variance part, but discard the importance of mean ofweights particularly for novel classes, leading to the limited performance inthe GFSL problem even with base data. In this paper, we overcome thislimitation by proposing a simple yet effective normalization method that caneffectively control both mean and variance of the weight distribution of novelclasses without using any base samples and thereby achieve a satisfactoryperformance on both novel and base classes. Our experimental results somewhatsurprisingly show that the proposed zero-base GFSL method that does not utilizeany base samples even outperforms the existing GFSL methods that make the bestuse of base data. Our implementation is available at:https://github.com/bigdata-inha/Zero-Base-GFSL."
StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot  Learning,"['Yuqian Fu', 'Yu Xie', 'Yanwei Fu', 'Yu-Gang Jiang']",http://arxiv.org/pdf/2302.09309v2.pdf,2023-02-18,['cs.cv'],"  Cross-Domain Few-Shot Learning (CD-FSL) is a recently emerging task thattackles few-shot learning across different domains. It aims at transferringprior knowledge learned on the source dataset to novel target datasets. TheCD-FSL task is especially challenged by the huge domain gap between differentdatasets. Critically, such a domain gap actually comes from the changes ofvisual styles, and wave-SAN empirically shows that spanning the styledistribution of the source data helps alleviate this issue. However, wave-SANsimply swaps styles of two images. Such a vanilla operation makes the generatedstyles ``real'' and ``easy'', which still fall into the original set of thesource styles. Thus, inspired by vanilla adversarial learning, a novelmodel-agnostic meta Style Adversarial training (StyleAdv) method together witha novel style adversarial attack method is proposed for CD-FSL. Particularly,our style attack method synthesizes both ``virtual'' and ``hard'' adversarialstyles for model training. This is achieved by perturbing the original stylewith the signed style gradients. By continually attacking styles and forcingthe model to recognize these challenging adversarial styles, our model isgradually robust to the visual styles, thus boosting the generalization abilityfor novel target datasets. Besides the typical CNN-based backbone, we alsoemploy our StyleAdv method on large-scale pretrained vision transformer.Extensive experiments conducted on eight various target datasets show theeffectiveness of our method. Whether built upon ResNet or ViT, we achieve thenew state of the art for CD-FSL. Code is available athttps://github.com/lovelyqian/StyleAdv-CDFSL."
Few-Shot Point Cloud Semantic Segmentation via Contrastive  Self-Supervision and Multi-Resolution Attention,"['Jiahui Wang', 'Haiyue Zhu', 'Haoren Guo', 'Abdullah Al Mamun', 'Cheng Xiang', 'Tong Heng Lee']",http://arxiv.org/pdf/2302.10501v1.pdf,2023-02-21,['cs.cv'],"  This paper presents an effective few-shot point cloud semantic segmentationapproach for real-world applications. Existing few-shot segmentation methods onpoint cloud heavily rely on the fully-supervised pretrain with large annotateddatasets, which causes the learned feature extraction bias to those pretrainedclasses. However, as the purpose of few-shot learning is to handleunknown/unseen classes, such class-specific feature extraction in pretrain isnot ideal to generalize into new classes for few-shot learning. Moreover, pointcloud datasets hardly have a large number of classes due to the annotationdifficulty. To address these issues, we propose a contrastive self-supervisionframework for few-shot learning pretrain, which aims to eliminate the featureextraction bias through class-agnostic contrastive supervision. Specifically,we implement a novel contrastive learning approach with a learnable augmentorfor a 3D point cloud to achieve point-wise differentiation, so that to enhancethe pretrain with managed overfitting through the self-supervision.Furthermore, we develop a multi-resolution attention module using both thenearest and farthest points to extract the local and global point informationmore effectively, and a center-concentrated multi-prototype is adopted tomitigate the intra-class sparsity. Comprehensive experiments are conducted toevaluate the proposed approach, which shows our approach achievesstate-of-the-art performance. Moreover, a case study on practical CAM/CADsegmentation is presented to demonstrate the effectiveness of our approach forreal-world applications."
Instance-level Few-shot Learning with Class Hierarchy Mining,"['Anh-Khoa Nguyen Vu', 'Thanh-Toan Do', 'Nhat-Duy Nguyen', 'Vinh-Tiep Nguyen', 'Thanh Duc Ngo', 'Tam V. Nguyen']",http://arxiv.org/pdf/2304.07459v1.pdf,2023-04-15,['cs.cv'],"  Few-shot learning is proposed to tackle the problem of scarce training datain novel classes. However, prior works in instance-level few-shot learning havepaid less attention to effectively utilizing the relationship betweencategories. In this paper, we exploit the hierarchical information to leveragediscriminative and relevant features of base classes to effectively classifynovel objects. These features are extracted from abundant data of base classes,which could be utilized to reasonably describe classes with scarce data.Specifically, we propose a novel superclass approach that automatically createsa hierarchy considering base and novel classes as fine-grained classes forfew-shot instance segmentation (FSIS). Based on the hierarchical information,we design a novel framework called Soft Multiple Superclass (SMS) to extractrelevant features or characteristics of classes in the same superclass. A newclass assigned to the superclass is easier to classify by leveraging theserelevant features. Besides, in order to effectively train thehierarchy-based-detector in FSIS, we apply the label refinement to furtherdescribe the associations between fine-grained classes. The extensiveexperiments demonstrate the effectiveness of our method on FSIS benchmarks.Code is available online."
ESPT: A Self-Supervised Episodic Spatial Pretext Task for Improving  Few-Shot Learning,"['Yi Rong', 'Xiongbo Lu', 'Zhaoyang Sun', 'Yaxiong Chen', 'Shengwu Xiong']",http://arxiv.org/pdf/2304.13287v1.pdf,2023-04-26,"['cs.cv', 'cs.ai']","  Self-supervised learning (SSL) techniques have recently been integrated intothe few-shot learning (FSL) framework and have shown promising results inimproving the few-shot image classification performance. However, existing SSLapproaches used in FSL typically seek the supervision signals from the globalembedding of every single image. Therefore, during the episodic training ofFSL, these methods cannot capture and fully utilize the local visualinformation in image samples and the data structure information of the wholeepisode, which are beneficial to FSL. To this end, we propose to augment thefew-shot learning objective with a novel self-supervised Episodic SpatialPretext Task (ESPT). Specifically, for each few-shot episode, we generate itscorresponding transformed episode by applying a random geometric transformationto all the images in it. Based on these, our ESPT objective is defined asmaximizing the local spatial relationship consistency between the originalepisode and the transformed one. With this definition, the ESPT-augmented FSLobjective promotes learning more transferable feature representations thatcapture the local spatial features of different images and theirinter-relational structural information in each input episode, thus enablingthe model to generalize better to new categories with only a few samples.Extensive experiments indicate that our ESPT method achieves newstate-of-the-art performance for few-shot image classification on threemainstay benchmark datasets. The source code will be available at:https://github.com/Whut-YiRong/ESPT."
Few-Shot Learning with Visual Distribution Calibration and Cross-Modal  Distribution Alignment,"['Runqi Wang', 'Hao Zheng', 'Xiaoyue Duan', 'Jianzhuang Liu', 'Yuning Lu', 'Tian Wang', 'Songcen Xu', 'Baochang Zhang']",http://arxiv.org/pdf/2305.11439v1.pdf,2023-05-19,['cs.cv'],"  Pre-trained vision-language models have inspired much research on few-shotlearning. However, with only a few training images, there exist two crucialproblems: (1) the visual feature distributions are easily distracted byclass-irrelevant information in images, and (2) the alignment between thevisual and language feature distributions is difficult. To deal with thedistraction problem, we propose a Selective Attack module, which consists oftrainable adapters that generate spatial attention maps of images to guide theattacks on class-irrelevant image areas. By messing up these areas, thecritical features are captured and the visual distributions of image featuresare calibrated. To better align the visual and language feature distributionsthat describe the same object class, we propose a cross-modal distributionalignment module, in which we introduce a vision-language prototype for eachclass to align the distributions, and adopt the Earth Mover's Distance (EMD) tooptimize the prototypes. For efficient computation, the upper bound of EMD isderived. In addition, we propose an augmentation strategy to increase thediversity of the images and the text prompts, which can reduce overfitting tothe few-shot training images. Extensive experiments on 11 datasets demonstratethat our method consistently outperforms prior arts in few-shot learning. Theimplementation code will be available at https://github.com/bhrqw/SADA."
Dual Adaptive Representation Alignment for Cross-domain Few-shot  Learning,"['Yifan Zhao', 'Tong Zhang', 'Jia Li', 'Yonghong Tian']",http://arxiv.org/pdf/2306.10511v1.pdf,2023-06-18,['cs.cv'],"  Few-shot learning aims to recognize novel queries with limited supportsamples by learning from base knowledge. Recent progress in this settingassumes that the base knowledge and novel query samples are distributed in thesame domains, which are usually infeasible for realistic applications. Towardthis issue, we propose to address the cross-domain few-shot learning problemwhere only extremely few samples are available in target domains. Under thisrealistic setting, we focus on the fast adaptation capability of meta-learnersby proposing an effective dual adaptive representation alignment approach. Inour approach, a prototypical feature alignment is first proposed to recalibratesupport instances as prototypes and reproject these prototypes with adifferentiable closed-form solution. Therefore feature spaces of learnedknowledge can be adaptively transformed to query spaces by the cross-instanceand cross-prototype relations. Besides the feature alignment, we furtherpresent a normalized distribution alignment module, which exploits priorstatistics of query samples for solving the covariant shifts among the supportand query samples. With these two modules, a progressive meta-learningframework is constructed to perform the fast adaptation with extremely few-shotsamples while maintaining its generalization capabilities. Experimentalevidence demonstrates our approach achieves new state-of-the-art results on 4CDFSL benchmarks and 4 fine-grained cross-domain benchmarks."
MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning,"['Baoquan Zhang', 'Demin Yu']",http://arxiv.org/pdf/2307.16424v1.pdf,2023-07-31,['cs.lg'],"  Equipping a deep model the abaility of few-shot learning, i.e., learningquickly from only few examples, is a core challenge for artificialintelligence. Gradient-based meta-learning approaches effectively address thechallenge by learning how to learn novel tasks. Its key idea is learning a deepmodel in a bi-level optimization manner, where the outer-loop process learns ashared gradient descent algorithm (i.e., its hyperparameters), while theinner-loop process leverage it to optimize a task-specific model by using onlyfew labeled data. Although these existing methods have shown superiorperformance, the outer-loop process requires calculating second-orderderivatives along the inner optimization path, which imposes considerablememory burdens and the risk of vanishing gradients. Drawing inspiration fromrecent progress of diffusion models, we find that the inner-loop gradientdescent process can be actually viewed as a reverse process (i.e., denoising)of diffusion where the target of denoising is model weights but the origindata. Based on this fact, in this paper, we propose to model the gradientdescent optimizer as a diffusion model and then present a noveltask-conditional diffusion-based meta-learning, called MetaDiff, thateffectively models the optimization process of model weights from Gaussionnoises to target weights in a denoising manner. Thanks to the trainingefficiency of diffusion models, our MetaDiff do not need to differentiatethrough the inner-loop path such that the memory burdens and the risk ofvanishing gradients can be effectvely alleviated. Experiment results show thatour MetaDiff outperforms the state-of-the-art gradient-based meta-learningfamily in few-shot learning tasks."
Cross-heterogeneity Graph Few-shot Learning,"['Pengfei Ding', 'Yan Wang', 'Guanfeng Liu']",http://arxiv.org/pdf/2308.05275v1.pdf,2023-08-10,"['cs.lg', 'cs.ai']","  In recent years, heterogeneous graph few-shot learning has been proposed toaddress the label sparsity issue in heterogeneous graphs (HGs), which containvarious types of nodes and edges. The existing methods have achieved goodperformance by transferring generalized knowledge extracted from rich-labeledclasses in source HG(s) to few-labeled classes in a target HG. However, thesemethods only consider the single-heterogeneity scenario where the source andtarget HGs share a fixed set of node/edge types, ignoring the more generalscenario of cross-heterogeneity, where each HG can have a different andnon-fixed set of node/edge types. To this end, we focus on the unexploredcross-heterogeneity scenario and propose a novel model for Cross-heterogeneityGraph Few-shot Learning, namely CGFL. In CGFL, we first extract meta-patternsto capture heterogeneous information and propose a multi-view heterogeneousgraph neural network (MHGN) to learn meta-patterns across HGs. Then, we proposea score module to measure the informativeness of labeled samples and determinethe transferability of each source HG. Finally, by integrating MHGN and thescore module into a meta-learning mechanism, CGFL can effectively transfergeneralized knowledge to predict new classes with few-labeled data. Extensiveexperiments on four real-world datasets have demonstrated the superiorperformance of CGFL over the state-of-the-art methods."
Federated Few-shot Learning for Cough Classification with Edge Devices,"['Ngan Dao Hoang', 'Dat Tran-Anh', 'Manh Luong', 'Cong Tran', 'Cuong Pham']",http://arxiv.org/pdf/2309.01076v1.pdf,2023-09-03,"['cs.lg', 'cs.sd', 'eess.as']","  Automatically classifying cough sounds is one of the most critical tasks forthe diagnosis and treatment of respiratory diseases. However, collecting a hugeamount of labeled cough dataset is challenging mainly due to high laboriousexpenses, data scarcity, and privacy concerns. In this work, our aim is todevelop a framework that can effectively perform cough classification even insituations when enormous cough data is not available, while also addressingprivacy concerns. Specifically, we formulate a new problem to tackle thesechallenges and adopt few-shot learning and federated learning to design a novelframework, termed F2LCough, for solving the newly formulated problem. Weillustrate the superiority of our method compared with other approaches onCOVID-19 Thermal Face & Cough dataset, in which F2LCough achieves an averageF1-Score of 86%. Our results show the feasibility of few-shot learning combinedwith federated learning to build a classification model of cough sounds. Thisnew methodology is able to classify cough sounds in data-scarce situations andmaintain privacy properties. The outcomes of this work can be a fundamentalframework for building support systems for the detection and diagnosis ofcough-related diseases."
Dual Adversarial Alignment for Realistic Support-Query Shift Few-shot  Learning,"['Siyang Jiang', 'Rui Fang', 'Hsi-Wen Chen', 'Wei Ding', 'Ming-Syan Chen']",http://arxiv.org/pdf/2309.02088v1.pdf,2023-09-05,"['cs.cv', 'cs.ai']","  Support-query shift few-shot learning aims to classify unseen examples (queryset) to labeled data (support set) based on the learned embedding in alow-dimensional space under a distribution shift between the support set andthe query set. However, in real-world scenarios the shifts are usually unknownand varied, making it difficult to estimate in advance. Therefore, in thispaper, we propose a novel but more difficult challenge, RSQS, focusing onRealistic Support-Query Shift few-shot learning. The key feature of RSQS isthat the individual samples in a meta-task are subjected to multipledistribution shifts in each meta-task. In addition, we propose a unifiedadversarial feature alignment method called DUal adversarial ALignmentframework (DuaL) to relieve RSQS from two aspects, i.e., inter-domain bias andintra-domain variance. On the one hand, for the inter-domain bias, we corruptthe original data in advance and use the synthesized perturbed inputs to trainthe repairer network by minimizing distance in the feature level. On the otherhand, for intra-domain variance, we proposed a generator network to synthesizehard, i.e., less similar, examples from the support set in a self-supervisedmanner and introduce regularized optimal transportation to derive a smoothoptimal transportation plan. Lastly, a benchmark of RSQS is built with severalstate-of-the-art baselines among three datasets (CIFAR100, mini-ImageNet, andTiered-Imagenet). Experiment results show that DuaL significantly outperformsthe state-of-the-art methods in our benchmark."
Multi-unit soft sensing permits few-shot learning,"['Bjarne Grimstad', 'Kristian Løvland', 'Lars S. Imsland']",http://arxiv.org/pdf/2309.15828v1.pdf,2023-09-27,"['stat.ml', 'cs.lg']","  Recent literature has explored various ways to improve soft sensors usinglearning algorithms with transferability. Broadly put, the performance of asoft sensor may be strengthened when it is learned by solving multiple tasks.The usefulness of transferability depends on how strongly related the devisedlearning tasks are. A particularly relevant case for transferability, is when asoft sensor is to be developed for a process of which there are manyrealizations, e.g. system or device with many implementations from which datais available. Then, each realization presents a soft sensor learning task, andit is reasonable to expect that the different tasks are strongly related.Applying transferability in this setting leads to what we call multi-unit softsensing, where a soft sensor models a process by learning from data from all ofits realizations.  This paper explores the learning abilities of a multi-unit soft sensor, whichis formulated as a hierarchical model and implemented using a deep neuralnetwork. In particular, we investigate how well the soft sensor generalizes asthe number of units increase. Using a large industrial dataset, we demonstratethat, when the soft sensor is learned from a sufficient number of tasks, itpermits few-shot learning on data from new units. Surprisingly, regarding thedifficulty of the task, few-shot learning on 1-3 data points often leads to ahigh performance on new units."
On the Role of Neural Collapse in Meta Learning Models for Few-shot  Learning,"['Saaketh Medepalli', 'Naren Doraiswamy']",http://arxiv.org/pdf/2310.00451v2.pdf,2023-09-30,"['cs.lg', 'cs.cv']","  Meta-learning frameworks for few-shot learning aims to learn models that canlearn new skills or adapt to new environments rapidly with a few trainingexamples. This has led to the generalizability of the developed model towardsnew classes with just a few labelled samples. However these networks are seenas black-box models and understanding the representations learnt underdifferent learning scenarios is crucial. Neural collapse ($\mathcal{NC}$) is arecently discovered phenomenon which showcases unique properties at the networkproceeds towards zero loss. The input features collapse to their respectiveclass means, the class means form a Simplex equiangular tight frame (ETF) wherethe class means are maximally distant and linearly separable, and theclassifier acts as a simple nearest neighbor classifier. While these phenomenahave been observed in simple classification networks, this study is the firstto explore and understand the properties of neural collapse in meta learningframeworks for few-shot learning. We perform studies on the Omniglot dataset inthe few-shot setting and study the neural collapse phenomenon. We observe thatthe learnt features indeed have the trend of neural collapse, especially asmodel size grows, but to do not necessarily showcase the complete collapse asmeasured by the $\mathcal{NC}$ properties."
Subspace Adaptation Prior for Few-Shot Learning,"['Mike Huisman', 'Aske Plaat', 'Jan N. van Rijn']",http://arxiv.org/pdf/2310.09028v1.pdf,2023-10-13,"['cs.lg', 'cs.ai', 'stat.ml']","  Gradient-based meta-learning techniques aim to distill useful prior knowledgefrom a set of training tasks such that new tasks can be learned moreefficiently with gradient descent. While these methods have achieved successesin various scenarios, they commonly adapt all parameters of trainable layerswhen learning new tasks. This neglects potentially more efficient learningstrategies for a given task distribution and may be susceptible to overfitting,especially in few-shot learning where tasks must be learned from a limitednumber of examples. To address these issues, we propose Subspace AdaptationPrior (SAP), a novel gradient-based meta-learning algorithm that jointly learnsgood initialization parameters (prior knowledge) and layer-wise parametersubspaces in the form of operation subsets that should be adaptable. In thisway, SAP can learn which operation subsets to adjust with gradient descentbased on the underlying task distribution, simultaneously decreasing the riskof overfitting when learning new tasks. We demonstrate that this ability ishelpful as SAP yields superior or competitive performance in few-shot imageclassification settings (gains between 0.1% and 3.9% in accuracy). Analysis ofthe learned subspaces demonstrates that low-dimensional operations often yieldhigh activation strengths, indicating that they may be important for achievinggood few-shot learning performance. For reproducibility purposes, we publishall our research code publicly."
MyriadAL: Active Few Shot Learning for Histopathology,"['Nico Schiavone', 'Jingyi Wang', 'Shuangzhi Li', 'Roger Zemp', 'Xingyu Li']",http://arxiv.org/pdf/2310.16161v1.pdf,2023-10-24,['cs.cv'],"  Active Learning (AL) and Few Shot Learning (FSL) are two label-efficientmethods which have achieved excellent results recently. However, most priorarts in both learning paradigms fail to explore the wealth of the vastunlabelled data. In this study, we address this issue in the scenario where theannotation budget is very limited, yet a large amount of unlabelled data forthe target task is available. We frame this work in the context ofhistopathology where labelling is prohibitively expensive. To this end, weintroduce an active few shot learning framework, Myriad Active Learning (MAL),including a contrastive-learning encoder, pseudo-label generation, and novelquery sample selection in the loop. Specifically, we propose to massageunlabelled data in a self-supervised manner, where the obtained datarepresentations and clustering knowledge form the basis to activate the ALloop. With feedback from the oracle in each AL cycle, the pseudo-labels of theunlabelled data are refined by optimizing a shallow task-specific net on top ofthe encoder. These updated pseudo-labels serve to inform and improve the activelearning query selection process. Furthermore, we introduce a novel recipe tocombine existing uncertainty measures and utilize the entire uncertainty listto reduce sample redundancy in AL. Extensive experiments on two publichistopathology datasets show that MAL has superior test accuracy, macroF1-score, and label efficiency compared to prior works, and can achieve acomparable test accuracy to a fully supervised algorithm while labelling only5% of the dataset."
Revisiting Local Descriptor based Image-to-Class Measure for Few-shot  Learning,"['Wenbin Li', 'Lei Wang', 'Jinglin Xu', 'Jing Huo', 'Yang Gao', 'Jiebo Luo']",http://arxiv.org/pdf/1903.12290v2.pdf,2019-03-28,['cs.cv'],"  Few-shot learning in image classification aims to learn a classifier toclassify images when only few training examples are available for each class.Recent work has achieved promising classification performance, where animage-level feature based measure is usually used. In this paper, we argue thata measure at such a level may not be effective enough in light of the scarcityof examples in few-shot learning. Instead, we think a local descriptor basedimage-to-class measure should be taken, inspired by its surprising success inthe heydays of local invariant features. Specifically, building upon the recentepisodic training mechanism, we propose a Deep Nearest Neighbor Neural Network(DN4 in short) and train it in an end-to-end manner. Its key difference fromthe literature is the replacement of the image-level feature based measure inthe final layer by a local descriptor based image-to-class measure. Thismeasure is conducted online via a $k$-nearest neighbor search over the deeplocal descriptors of convolutional feature maps. The proposed DN4 not onlylearns the optimal deep local descriptors for the image-to-class measure, butalso utilizes the higher efficiency of such a measure in the case of examplescarcity, thanks to the exchangeability of visual patterns across the images inthe same class. Our work leads to a simple, effective, and computationallyefficient framework for few-shot learning. Experimental study on benchmarkdatasets consistently shows its superiority over the related state-of-the-art,with the largest absolute improvement of $17\%$ over the next best. The sourcecode can be available from \UrlFont{https://github.com/WenbinLee/DN4.git}."
Automatic detection of rare pathologies in fundus photographs using  few-shot learning,"['Gwenolé Quellec', 'Mathieu Lamard', 'Pierre-Henri Conze', 'Pascale Massin', 'Béatrice Cochener']",http://arxiv.org/pdf/1907.09449v3.pdf,2019-07-22,['cs.cv'],"  In the last decades, large datasets of fundus photographs have been collectedin diabetic retinopathy (DR) screening networks. Through deep learning, thesedatasets were used to train automatic detectors for DR and a few other frequentpathologies, with the goal to automate screening. One challenge limits theadoption of such systems so far: automatic detectors ignore rare conditionsthat ophthalmologists currently detect, such as papilledema or anteriorischemic optic neuropathy. The reason is that standard deep learning requirestoo many examples of these conditions. However, this limitation can beaddressed with few-shot learning, a machine learning paradigm where aclassifier has to generalize to a new category not seen in training, given onlya few examples of this category. This paper presents a new few-shot learningframework that extends convolutional neural networks (CNNs), trained forfrequent conditions, with an unsupervised probabilistic model for rarecondition detection. It is based on the observation that CNNs often perceivephotographs containing the same anomalies as similar, even though these CNNswere trained to detect unrelated conditions. This observation was based on thet-SNE visualization tool, which we decided to incorporate in our probabilisticmodel. Experiments on a dataset of 164,660 screening examinations from theOPHDIAT screening network show that 37 conditions, out of 41, can be detectedwith an area under the ROC curve (AUC) greater than 0.8 (average AUC: 0.938).In particular, this framework significantly outperforms other frameworks fordetecting rare conditions, including multitask learning, transfer learning andSiamese networks, another few-shot learning solution. We expect these richerpredictions to trigger the adoption of automated eye pathology screening, whichwill revolutionize clinical practice in ophthalmology."
Dataset Bias in Few-shot Image Recognition,"['Shuqiang Jiang', 'Yaohui Zhu', 'Chenlong Liu', 'Xinhang Song', 'Xiangyang Li', 'Weiqing Min']",http://arxiv.org/pdf/2008.07960v3.pdf,2020-08-18,['cs.cv'],"  The goal of few-shot image recognition (FSIR) is to identify novel categorieswith a small number of annotated samples by exploiting transferable knowledgefrom training data (base categories). Most current studies assume that thetransferable knowledge can be well used to identify novel categories. However,such transferable capability may be impacted by the dataset bias, and thisproblem has rarely been investigated before. Besides, most of few-shot learningmethods are biased to different datasets, which is also an important issue thatneeds to be investigated deeply. In this paper, we first investigate the impactof transferable capabilities learned from base categories. Specifically, we usethe relevance to measure relationships between base categories and novelcategories. Distributions of base categories are depicted via the instancedensity and category diversity. The FSIR model learns better transferableknowledge from relevant training data. In the relevant data, dense instances ordiverse categories can further enrich the learned knowledge. Experimentalresults on different sub-datasets of ImagNet demonstrate category relevance,instance density and category diversity can depict transferable bias from basecategories. Second, we investigate performance differences on differentdatasets from dataset structures and different few-shot learning methods.Specifically, we introduce image complexity, intra-concept visual consistency,and inter-concept visual similarity to quantify characteristics of datasetstructures. We use these quantitative characteristics and four few-shotlearning methods to analyze performance differences on five different datasets.Based on the experimental analysis, some insightful observations are obtainedfrom the perspective of both dataset structures and few-shot learning methods.We hope these observations are useful to guide future FSIR research."
UVStyle-Net: Unsupervised Few-shot Learning of 3D Style Similarity  Measure for B-Reps,"['Peter Meltzer', 'Hooman Shayani', 'Amir Khasahmadi', 'Pradeep Kumar Jayaraman', 'Aditya Sanghi', 'Joseph Lambourne']",http://arxiv.org/pdf/2105.02961v3.pdf,2021-04-28,"['cs.cv', 'cs.lg', '68t07, 68t10', 'i.5.1; i.3.5']","  Boundary Representations (B-Reps) are the industry standard in 3D ComputerAided Design/Manufacturing (CAD/CAM) and industrial design due to theirfidelity in representing stylistic details. However, they have been ignored inthe 3D style research. Existing 3D style metrics typically operate on meshes orpointclouds, and fail to account for end-user subjectivity by adopting fixeddefinitions of style, either through crowd-sourcing for style labels orhand-crafted features. We propose UVStyle-Net, a style similarity measure forB-Reps that leverages the style signals in the second order statistics of theactivations in a pre-trained (unsupervised) 3D encoder, and learns theirrelative importance to a subjective end-user through few-shot learning. Ourapproach differs from all existing data-driven 3D style methods since it may beused in completely unsupervised settings, which is desirable given the lack ofpublicly available labelled B-Rep datasets. More importantly, the few-shotlearning accounts for the inherent subjectivity associated with style. We showquantitatively that our proposed method with B-Reps is able to capture strongerstyle signals than alternative methods on meshes and pointclouds despite itssignificantly greater computational efficiency. We also show it is able togenerate meaningful style gradients with respect to the input shape, and thatfew-shot learning with as few as two positive examples selected by an end-useris sufficient to significantly improve the style measure. Finally, wedemonstrate its efficacy on a large unlabeled public dataset of CAD models.Source code and data will be released in the future."
Few-Shot Bot: Prompt-Based Learning for Dialogue Systems,"['Andrea Madotto', 'Zhaojiang Lin', 'Genta Indra Winata', 'Pascale Fung']",http://arxiv.org/pdf/2110.08118v1.pdf,2021-10-15,"['cs.cl', 'cs.ai']","  Learning to converse using only a few examples is a great challenge inconversational AI. The current best conversational models, which are eithergood chit-chatters (e.g., BlenderBot) or goal-oriented systems (e.g., MinTL),are language models (LMs) fine-tuned on large conversational datasets. Trainingthese models is expensive, both in terms of computational resources and time,and it is hard to keep them up to date with new conversational skills. A simpleyet unexplored solution is prompt-based few-shot learning (Brown et al. 2020)which does not require gradient-based fine-tuning but instead uses a fewexamples in the LM context as the only source of learning. In this paper, weexplore prompt-based few-shot learning in dialogue tasks. We benchmark LMs ofdifferent sizes in nine response generation tasks, which include fourknowledge-grounded tasks, a task-oriented generations task, three open-chattasks, and controlled stylistic generation, and five conversational parsingtasks, which include dialogue state tracking, graph path generation, personainformation extraction, document retrieval, and internet query generation. Thecurrent largest released LM (GPT-J-6B) using prompt-based few-shot learning,and thus requiring no training, achieves competitive performance to fullytrained state-of-the-art models. Moreover, we propose a novel prompt-basedfew-shot classifier, that also does not require any fine-tuning, to select themost appropriate prompt given a dialogue history. Finally, by combining thepower of prompt-based few-shot learning and a Skill Selector, we create anend-to-end chatbot named the Few-Shot Bot (FSB), which automatically selectsthe most appropriate conversational skill, queries different knowledge bases orthe internet, and uses the retrieved knowledge to generate a human-likeresponse, all using only few dialogue examples per skill."
"A Neural Network Solves, Explains, and Generates University Math  Problems by Program Synthesis and Few-Shot Learning at Human Level","['Iddo Drori', 'Sarah Zhang', 'Reece Shuttleworth', 'Leonard Tang', 'Albert Lu', 'Elizabeth Ke', 'Kevin Liu', 'Linda Chen', 'Sunny Tran', 'Newman Cheng', 'Roman Wang', 'Nikhil Singh', 'Taylor L. Patti', 'Jayson Lynch', 'Avi Shporer', 'Nakul Verma', 'Eugene Wu', 'Gilbert Strang']",http://arxiv.org/pdf/2112.15594v4.pdf,2021-12-31,"['cs.lg', 'cs.ai']","  We demonstrate that a neural network pre-trained on text and fine-tuned oncode solves mathematics course problems, explains solutions, and generates newquestions at a human level. We automatically synthesize programs using few-shotlearning and OpenAI's Codex transformer and execute them to solve courseproblems at 81% automatic accuracy. We curate a new dataset of questions fromMIT's largest mathematics courses (Single Variable and Multivariable Calculus,Differential Equations, Introduction to Probability and Statistics, LinearAlgebra, and Mathematics for Computer Science) and Columbia University'sComputational Linear Algebra. We solve questions from a MATH dataset (onPrealgebra, Algebra, Counting and Probability, Intermediate Algebra, NumberTheory, and Precalculus), the latest benchmark of advanced mathematics problemsdesigned to assess mathematical reasoning. We randomly sample questions andgenerate solutions with multiple modalities, including numbers, equations, andplots. The latest GPT-3 language model pre-trained on text automatically solvesonly 18.8% of these university questions using zero-shot learning and 30.8%using few-shot learning and the most recent chain of thought prompting. Incontrast, program synthesis with few-shot learning using Codex fine-tuned oncode generates programs that automatically solve 81% of these questions. Ourapproach improves the previous state-of-the-art automatic solution accuracy onthe benchmark topics from 8.8% to 81.1%. We perform a survey to evaluate thequality and difficulty of generated questions. This work is the first toautomatically solve university-level mathematics course questions at a humanlevel and the first work to explain and generate university-level mathematicscourse questions at scale, a milestone for higher education."
Prototypical Networks for Few-shot Learning,"['Jake Snell', 'Kevin Swersky', 'Richard S. Zemel']",http://arxiv.org/pdf/1703.05175v2.pdf,2017-03-15,"['cs.lg', 'stat.ml']","  We propose prototypical networks for the problem of few-shot classification,where a classifier must generalize to new classes not seen in the training set,given only a small number of examples of each new class. Prototypical networkslearn a metric space in which classification can be performed by computingdistances to prototype representations of each class. Compared to recentapproaches for few-shot learning, they reflect a simpler inductive bias that isbeneficial in this limited-data regime, and achieve excellent results. Weprovide an analysis showing that some simple design decisions can yieldsubstantial improvements over recent approaches involving complicatedarchitectural choices and meta-learning. We further extend prototypicalnetworks to zero-shot learning and achieve state-of-the-art results on theCU-Birds dataset."
Few-Shot Learning Through an Information Retrieval Lens,"['Eleni Triantafillou', 'Richard Zemel', 'Raquel Urtasun']",http://arxiv.org/pdf/1707.02610v2.pdf,2017-07-09,['cs.lg'],"  Few-shot learning refers to understanding new concepts from only a fewexamples. We propose an information retrieval-inspired approach for thisproblem that is motivated by the increased importance of maximally leveragingall the available information in this low-data regime. We define a trainingobjective that aims to extract as much information as possible from eachtraining batch by effectively optimizing over all relative orderings of thebatch points simultaneously. In particular, we view each batch point as a`query' that ranks the remaining ones based on its predicted relevance to themand we define a model within the framework of structured prediction to optimizemean Average Precision over these rankings. Our method achieves impressiveresults on the standard few-shot classification benchmarks while is alsocapable of few-shot retrieval."
One-shot and few-shot learning of word embeddings,"['Andrew K. Lampinen', 'James L. McClelland']",http://arxiv.org/pdf/1710.10280v2.pdf,2017-10-27,"['cs.cl', 'cs.lg', 'stat.ml', 'i.2.7']","  Standard deep learning systems require thousands or millions of examples tolearn a concept, and cannot integrate new concepts easily. By contrast, humanshave an incredible ability to do one-shot or few-shot learning. For instance,from just hearing a word used in a sentence, humans can infer a great dealabout it, by leveraging what the syntax and semantics of the surrounding wordstells us. Here, we draw inspiration from this to highlight a simple techniqueby which deep recurrent networks can similarly exploit their prior knowledge tolearn a useful representation for a new word from little data. This could makenatural language processing systems much more flexible, by allowing them tolearn continually from the new words they encounter."
Deep Meta-Learning: Learning to Learn in the Concept Space,"['Fengwei Zhou', 'Bin Wu', 'Zhenguo Li']",http://arxiv.org/pdf/1802.03596v1.pdf,2018-02-10,['cs.lg'],"  Few-shot learning remains challenging for meta-learning that learns alearning algorithm (meta-learner) from many related tasks. In this work, weargue that this is due to the lack of a good representation for meta-learning,and propose deep meta-learning to integrate the representation power of deeplearning into meta-learning. The framework is composed of three modules, aconcept generator, a meta-learner, and a concept discriminator, which arelearned jointly. The concept generator, e.g. a deep residual net, extracts arepresentation for each instance that captures its high-level concept, on whichthe meta-learner performs few-shot learning, and the concept discriminatorrecognizes the concepts. By learning to learn in the concept space rather thanin the complicated instance space, deep meta-learning can substantially improvevanilla meta-learning, which is demonstrated on various few-shot imagerecognition problems. For example, on 5-way-1-shot image recognition onCIFAR-100 and CUB-200, it improves Matching Nets from 50.53% and 56.53% to58.18% and 63.47%, improves MAML from 49.28% and 50.45% to 56.65% and 64.63%,and improves Meta-SGD from 53.83% and 53.34% to 61.62% and 66.95%,respectively."
Few-Shot Learning with Metric-Agnostic Conditional Embeddings,"['Nathan Hilliard', 'Lawrence Phillips', 'Scott Howland', 'Artëm Yankov', 'Courtney D. Corley', 'Nathan O. Hodas']",http://arxiv.org/pdf/1802.04376v1.pdf,2018-02-12,"['cs.lg', 'stat.ml']","  Learning high quality class representations from few examples is a keyproblem in metric-learning approaches to few-shot learning. To accomplish this,we introduce a novel architecture where class representations are conditionedfor each few-shot trial based on a target image. We also deviate fromtraditional metric-learning approaches by training a network to performcomparisons between classes rather than relying on a static metric comparison.This allows the network to decide what aspects of each class are important forthe comparison at hand. We find that this flexible architecture works well inpractice, achieving state-of-the-art performance on the Caltech-UCSD birdsfine-grained classification task."
FewRel: A Large-Scale Supervised Few-Shot Relation Classification  Dataset with State-of-the-Art Evaluation,"['Xu Han', 'Hao Zhu', 'Pengfei Yu', 'Ziyun Wang', 'Yuan Yao', 'Zhiyuan Liu', 'Maosong Sun']",http://arxiv.org/pdf/1810.10147v2.pdf,2018-10-24,"['cs.lg', 'cs.ai', 'cs.cl', 'stat.ml']","  We present a Few-Shot Relation Classification Dataset (FewRel), consisting of70, 000 sentences on 100 relations derived from Wikipedia and annotated bycrowdworkers. The relation of each sentence is first recognized by distantsupervision methods, and then filtered by crowdworkers. We adapt the mostrecent state-of-the-art few-shot learning methods for relation classificationand conduct a thorough evaluation of these methods. Empirical results show thateven the most competitive few-shot learning models struggle on this task,especially as compared with humans. We also show that a range of differentreasoning skills are needed to solve our task. These results indicate thatfew-shot relation classification remains an open problem and still requiresfurther research. Our detailed analysis points multiple directions for futureresearch. All details and resources about the dataset and baselines arereleased on http://zhuhao.me/fewrel."
Cross-Modulation Networks for Few-Shot Learning,"['Hugo Prol', 'Vincent Dumoulin', 'Luis Herranz']",http://arxiv.org/pdf/1812.00273v1.pdf,2018-12-01,"['cs.lg', 'cs.cv', 'stat.ml']","  A family of recent successful approaches to few-shot learning relies onlearning an embedding space in which predictions are made by computingsimilarities between examples. This corresponds to combining informationbetween support and query examples at a very late stage of the predictionpipeline. Inspired by this observation, we hypothesize that there may bebenefits to combining the information at various levels of abstraction alongthe pipeline. We present an architecture called Cross-Modulation Networks whichallows support and query examples to interact throughout the feature extractionprocess via a feature-wise modulation mechanism. We adapt the Matching Networksarchitecture to take advantage of these interactions and show encouraginginitial results on miniImageNet in the 5-way, 1-shot setting, where we closethe gap with state-of-the-art."
Dense Classification and Implanting for Few-Shot Learning,"['Yann Lifchitz', 'Yannis Avrithis', 'Sylvaine Picard', 'Andrei Bursuc']",http://arxiv.org/pdf/1903.05050v1.pdf,2019-03-12,['cs.cv'],"  Training deep neural networks from few examples is a highly challenging andkey problem for many computer vision tasks. In this context, we are targetingknowledge transfer from a set with abundant data to other sets with fewavailable examples. We propose two simple and effective solutions: (i) denseclassification over feature maps, which for the first time studies localactivations in the domain of few-shot learning, and (ii) implanting, that is,attaching new neurons to a previously trained network to learn new,task-specific features. On miniImageNet, we improve the prior state-of-the-arton few-shot classification, i.e., we achieve 62.5%, 79.8% and 83.8% on 5-way1-shot, 5-shot and 10-shot settings respectively."
Few-Shot Learning with Embedded Class Models and Shot-Free Meta Training,"['Avinash Ravichandran', 'Rahul Bhotika', 'Stefano Soatto']",http://arxiv.org/pdf/1905.04398v2.pdf,2019-05-10,"['cs.lg', 'cs.cv', 'stat.ml']","  We propose a method for learning embeddings for few-shot learning that issuitable for use with any number of ways and any number of shots (shot-free).Rather than fixing the class prototypes to be the Euclidean average of sampleembeddings, we allow them to live in a higher-dimensional space (embedded classmodels) and learn the prototypes along with the model parameters. The classrepresentation function is defined implicitly, which allows us to deal with avariable number of shots per each class with a simple constant-sizearchitecture. The class embedding encompasses metric learning, that facilitatesadding new classes without crowding the class representation space. Despitebeing general and not tuned to the benchmark, our approach achievesstate-of-the-art performance on the standard few-shot benchmark datasets."
TapNet: Neural Network Augmented with Task-Adaptive Projection for  Few-Shot Learning,"['Sung Whan Yoon', 'Jun Seo', 'Jaekyun Moon']",http://arxiv.org/pdf/1905.06549v2.pdf,2019-05-16,"['cs.lg', 'stat.ml']","  Handling previously unseen tasks after given only a few training examplescontinues to be a tough challenge in machine learning. We propose TapNets,neural networks augmented with task-adaptive projection for improved few-shotlearning. Here, employing a meta-learning strategy with episode-based training,a network and a set of per-class reference vectors are learned across widelyvarying tasks. At the same time, for every episode, features in the embeddingspace are linearly projected into a new space as a form of quick task-specificconditioning. The training loss is obtained based on a distance metric betweenthe query and the reference vectors in the projection space. Excellentgeneralization results in this way. When tested on the Omniglot, miniImageNetand tieredImageNet datasets, we obtain state of the art classificationaccuracies under various few-shot scenarios."
Alpha MAML: Adaptive Model-Agnostic Meta-Learning,"['Harkirat Singh Behl', 'Atılım Güneş Baydin', 'Philip H. S. Torr']",http://arxiv.org/pdf/1905.07435v1.pdf,2019-05-17,"['cs.lg', 'cs.ai', 'stat.ml']","  Model-agnostic meta-learning (MAML) is a meta-learning technique to train amodel on a multitude of learning tasks in a way that primes the model forfew-shot learning of new tasks. The MAML algorithm performs well on few-shotlearning problems in classification, regression, and fine-tuning of policygradients in reinforcement learning, but comes with the need for costlyhyperparameter tuning for training stability. We address this shortcoming byintroducing an extension to MAML, called Alpha MAML, to incorporate an onlinehyperparameter adaptation scheme that eliminates the need to tune meta-learningand learning rates. Our results with the Omniglot database demonstrate asubstantial reduction in the need to tune MAML training hyperparameters andimprovement to training stability with less sensitivity to hyperparameterchoice."
Incremental Few-Shot Learning for Pedestrian Attribute Recognition,"['Liuyu Xiang', 'Xiaoming Jin', 'Guiguang Ding', 'Jungong Han', 'Leida Li']",http://arxiv.org/pdf/1906.00330v2.pdf,2019-06-02,['cs.cv'],"  Pedestrian attribute recognition has received increasing attention due to itsimportant role in video surveillance applications. However, most existingmethods are designed for a fixed set of attributes. They are unable to handlethe incremental few-shot learning scenario, i.e. adapting a well-trained modelto newly added attributes with scarce data, which commonly exists in the realworld. In this work, we present a meta learning based method to address thisissue. The core of our framework is a meta architecture capable ofdisentangling multiple attribute information and generalizing rapidly to newcoming attributes. By conducting extensive experiments on the benchmark datasetPETA and RAP under the incremental few-shot setting, we show that our method isable to perform the task with competitive performances and low resourcerequirements."
Boosting Supervision with Self-Supervision for Few-shot Learning,"['Jong-Chyi Su', 'Subhransu Maji', 'Bharath Hariharan']",http://arxiv.org/pdf/1906.07079v1.pdf,2019-06-17,"['cs.cv', 'cs.lg', 'stat.ml']","  We present a technique to improve the transferability of deep representationslearned on small labeled datasets by introducing self-supervised tasks asauxiliary loss functions. While recent approaches for self-supervised learninghave shown the benefits of training on large unlabeled datasets, we findimprovements in generalization even on small datasets and when combined withstrong supervision. Learning representations with self-supervised lossesreduces the relative error rate of a state-of-the-art meta-learner by 5-25% onseveral few-shot learning benchmarks, as well as off-the-shelf deep networks onstandard classification tasks when training from scratch. We find the benefitsof self-supervision increase with the difficulty of the task. Our approachutilizes the images within the dataset to construct self-supervised losses andhence is an effective way of learning transferable representations withoutrelying on any external training data."
Few-Shot Learning with Global Class Representations,"['Tiange Luo', 'Aoxue Li', 'Tao Xiang', 'Weiran Huang', 'Liwei Wang']",http://arxiv.org/pdf/1908.05257v1.pdf,2019-08-14,['cs.cv'],"  In this paper, we propose to tackle the challenging few-shot learning (FSL)problem by learning global class representations using both base and novelclass training samples. In each training episode, an episodic class meancomputed from a support set is registered with the global representation via aregistration module. This produces a registered global class representation forcomputing the classification loss using a query set. Though following a similarepisodic training pipeline as existing meta learning based approaches, ourmethod differs significantly in that novel class training samples are involvedin the training from the beginning. To compensate for the lack of novel classtraining samples, an effective sample synthesis strategy is developed to avoidoverfitting. Importantly, by joint base-novel class training, our approach canbe easily extended to a more practical yet challenging FSL setting, i.e.,generalized FSL, where the label space of test data is extended to both baseand novel classes. Extensive experiments show that our approach is effectivefor both of the two FSL settings."
Few-shot Learning with Deep Triplet Networks for Brain Imaging Modality  Recognition,"['Santi Puch', 'Irina Sánchez', 'Matt Rowe']",http://arxiv.org/pdf/1908.10266v1.pdf,2019-08-27,"['cs.cv', 'stat.ml']","  Image modality recognition is essential for efficient imaging workflows incurrent clinical environments, where multiple imaging modalities are used tobetter comprehend complex diseases. Emerging biomarkers from novel, raremodalities are being developed to aid in such understanding, however theavailability of these images is often limited. This scenario raises thenecessity of recognising new imaging modalities without them being collectedand annotated in large amounts. In this work, we present a few-shot learningmodel for limited training examples based on Deep Triplet Networks. We showthat the proposed model is more accurate in distinguishing different modalitiesthan a traditional Convolutional Neural Network classifier when limited samplesare available. Furthermore, we evaluate the performance of both classifierswhen presented with noisy samples and provide an initial inspection of how theproposed model can incorporate measures of uncertainty to be more robustagainst out-of-sample examples."
Metric-Based Few-Shot Learning for Video Action Recognition,"['Chris Careaga', 'Brian Hutchinson', 'Nathan Hodas', 'Lawrence Phillips']",http://arxiv.org/pdf/1909.09602v1.pdf,2019-09-14,"['cs.cv', 'cs.lg', 'stat.ml']","  In the few-shot scenario, a learner must effectively generalize to unseenclasses given a small support set of labeled examples. While a relatively largeamount of research has gone into few-shot learning for image classification,little work has been done on few-shot video classification. In this work, weaddress the task of few-shot video action recognition with a set of two-streammodels. We evaluate the performance of a set of convolutional and recurrentneural network video encoder architectures used in conjunction with threepopular metric-based few-shot algorithms. We train and evaluate using afew-shot split of the Kinetics 600 dataset. Our experiments confirm theimportance of the two-stream setup, and find prototypical networks and pooledlong short-term memory network embeddings to give the best performance asfew-shot method and video encoder, respectively. For a 5-shot 5-way task, thissetup obtains 84.2% accuracy on the test set and 59.4% on a special ""challenge""test set, composed of highly confusable classes."
Tackling Long-Tailed Relations and Uncommon Entities in Knowledge Graph  Completion,"['Zihao Wang', 'Kwun Ping Lai', 'Piji Li', 'Lidong Bing', 'Wai Lam']",http://arxiv.org/pdf/1909.11359v1.pdf,2019-09-25,['cs.cl'],"  For large-scale knowledge graphs (KGs), recent research has been focusing onthe large proportion of infrequent relations which have been ignored byprevious studies. For example few-shot learning paradigm for relations has beeninvestigated. In this work, we further advocate that handling uncommon entitiesis inevitable when dealing with infrequent relations. Therefore, we propose ameta-learning framework that aims at handling infrequent relations withfew-shot learning and uncommon entities by using textual descriptions. Wedesign a novel model to better extract key information from textualdescriptions. Besides, we also develop a novel generative model in ourframework to enhance the performance by generating extra triplets during thetraining stage. Experiments are conducted on two datasets from real-world KGs,and the results show that our framework outperforms previous methods whendealing with infrequent relations and their accompanying uncommon entities."
Adversarially Robust Few-Shot Learning: A Meta-Learning Approach,"['Micah Goldblum', 'Liam Fowl', 'Tom Goldstein']",http://arxiv.org/pdf/1910.00982v3.pdf,2019-10-02,"['cs.lg', 'stat.ml']","  Previous work on adversarially robust neural networks for imageclassification requires large training sets and computationally expensivetraining procedures. On the other hand, few-shot learning methods are highlyvulnerable to adversarial examples. The goal of our work is to produce networkswhich both perform well at few-shot classification tasks and are simultaneouslyrobust to adversarial examples. We develop an algorithm, called AdversarialQuerying (AQ), for producing adversarially robust meta-learners, and wethoroughly investigate the causes for adversarial vulnerability. Moreover, ourmethod achieves far superior robust performance on few-shot imageclassification tasks, such as Mini-ImageNet and CIFAR-FS, than robust transferlearning."
Graph Few-shot Learning via Knowledge Transfer,"['Huaxiu Yao', 'Chuxu Zhang', 'Ying Wei', 'Meng Jiang', 'Suhang Wang', 'Junzhou Huang', 'Nitesh V. Chawla', 'Zhenhui Li']",http://arxiv.org/pdf/1910.03053v3.pdf,2019-10-07,"['cs.lg', 'stat.ml']","  Towards the challenging problem of semi-supervised node classification, therehave been extensive studies. As a frontier, Graph Neural Networks (GNNs) havearoused great interest recently, which update the representation of each nodeby aggregating information of its neighbors. However, most GNNs have shallowlayers with a limited receptive field and may not achieve satisfactoryperformance especially when the number of labeled nodes is quite small. Toaddress this challenge, we innovatively propose a graph few-shot learning (GFL)algorithm that incorporates prior knowledge learned from auxiliary graphs toimprove classification accuracy on the target graph. Specifically, atransferable metric space characterized by a node embedding and agraph-specific prototype embedding function is shared between auxiliary graphsand the target, facilitating the transfer of structural knowledge. Extensiveexperiments and ablation studies on four real-world graph datasets demonstratethe effectiveness of our proposed model."
Revisiting Few-Shot Learning for Facial Expression Recognition,"['Anca-Nicoleta Ciubotaru', 'Arnout Devos', 'Behzad Bozorgtabar', 'Jean-Philippe Thiran', 'Maria Gabrani']",http://arxiv.org/pdf/1912.02751v2.pdf,2019-12-05,['cs.cv'],"  Most of the existing deep neural nets on automatic facial expressionrecognition focus on a set of predefined emotion classes, where the amount oftraining data has the biggest impact on performance. However, in the standardsetting over-parameterised neural networks are not amenable for learning fromfew samples as they can quickly over-fit. In addition, these approaches do nothave such a strong generalisation ability to identify a new category, where thedata of each category is too limited and significant variations exist in theexpression within the same semantic category. We embrace these challenges andformulate the problem as a low-shot learning, where once the base classifier isdeployed, it must rapidly adapt to recognise novel classes using a few samples.In this paper, we revisit and compare existing few-shot learning methods forthe low-shot facial expression recognition in terms of their generalisationability via episode-training. In particular, we extend our analysis on thecross-domain generalisation, where training and test tasks are not drawn fromthe same distribution. We demonstrate the efficacy of low-shot learning methodsthrough extensive experiments."
Few-shot link prediction via graph neural networks for Covid-19  drug-repurposing,"['Vassilis N. Ioannidis', 'Da Zheng', 'George Karypis']",http://arxiv.org/pdf/2007.10261v1.pdf,2020-07-20,"['cs.lg', 'stat.ml']","  Predicting interactions among heterogenous graph structured data has numerousapplications such as knowledge graph completion, recommendation systems anddrug discovery. Often times, the links to be predicted belong to rare typessuch as the case in repurposing drugs for novel diseases. This motivates thetask of few-shot link prediction. Typically, GCNs are ill-equipped in learningsuch rare link types since the relation embedding is not learned in aninductive fashion. This paper proposes an inductive RGCN for learninginformative relation embeddings even in the few-shot learning regime. Theproposed inductive model significantly outperforms the RGCN andstate-of-the-art KGE models in few-shot learning tasks. Furthermore, we applyour method on the drug-repurposing knowledge graph (DRKG) for discovering drugsfor Covid-19. We pose the drug discovery task as link prediction and learnembeddings for the biological entities that partake in the DRKG. Our initialresults corroborate that several drugs used in clinical trials were identifiedas possible drug candidates. The method in this paper are implemented using theefficient deep graph learning (DGL)"
Enhanced Few-shot Learning for Intrusion Detection in Railway Video  Surveillance,"['Xiao Gong', 'Xi Chen', 'Wei Chen']",http://arxiv.org/pdf/2011.04254v1.pdf,2020-11-09,"['cs.lg', 'cs.sy', 'eess.sp', 'eess.sy']","  Video surveillance is gaining increasing popularity to assist in railwayintrusion detection in recent years. However, efficient and accurate intrusiondetection remains a challenging issue due to: (a) limited sample number: onlysmall sample size (or portion) of intrusive video frames is available; (b) lowinter-scene dissimilarity: various railway track area scenes are captured bycameras installed in different landforms; (c) high intra-scene similarity: thevideo frames captured by an individual camera share a same backgound. In thispaper, an efficient few-shot learning solution is developed to address theabove issues. In particular, an enhanced model-agnostic meta-learner is trainedusing both the original video frames and segmented masks of track areaextracted from the video. Moreover, theoretical analysis and engineeringsolutions are provided to cope with the highly similar video frames in themeta-model training phase. The proposed method is tested on realistic railwayvideo dataset. Numerical results show that the enhanced meta-learnersuccessfully adapts unseen scene with only few newly collected video framesamples, and its intrusion detection accuracy outperforms that of the standardrandomly initialized supervised learning."
A Nested Bi-level Optimization Framework for Robust Few Shot Learning,"['Krishnateja Killamsetty', 'Changbin Li', 'Chen Zhao', 'Rishabh Iyer', 'Feng Chen']",http://arxiv.org/pdf/2011.06782v2.pdf,2020-11-13,['cs.lg'],"  Model-Agnostic Meta-Learning (MAML), a popular gradient-based meta-learningframework, assumes that the contribution of each task or instance to themeta-learner is equal. Hence, it fails to address the domain shift between baseand novel classes in few-shot learning. In this work, we propose a novel robustmeta-learning algorithm, NestedMAML, which learns to assign weights to trainingtasks or instances. We consider weights as hyper-parameters and iterativelyoptimize them using a small set of validation tasks set in a nested bi-leveloptimization approach (in contrast to the standard bi-level optimization inMAML). We then apply NestedMAML in the meta-training stage, which involves (1)several tasks sampled from a distribution different from the meta-test taskdistribution, or (2) some data samples with noisy labels. Extensive experimentson synthetic and real-world datasets demonstrate that NestedMAML efficientlymitigates the effects of ""unwanted"" tasks or instances, leading to significantimprovement over the state-of-the-art robust meta-learning methods."
Zero-shot Relation Classification from Side Information,"['Jiaying Gong', 'Hoda Eldardiry']",http://arxiv.org/pdf/2011.07126v2.pdf,2020-11-13,"['cs.cl', 'cs.lg']","  We propose a zero-shot learning relation classification (ZSLRC) frameworkthat improves on state-of-the-art by its ability to recognize novel relationsthat were not present in training data. The zero-shot learning approach mimicsthe way humans learn and recognize new concepts with no prior knowledge. Toachieve this, ZSLRC uses advanced prototypical networks that are modified toutilize weighted side (auxiliary) information. ZSLRC's side information isbuilt from keywords, hypernyms of name entities, and labels and their synonyms.ZSLRC also includes an automatic hypernym extraction framework that acquireshypernyms of various name entities directly from the web. ZSLRC improves onstate-of-the-art few-shot learning relation classification methods that rely onlabeled training data and is therefore applicable more widely even inreal-world scenarios where some relations have no corresponding labeledexamples for training. We present results using extensive experiments on twopublic datasets (NYT and FewRel) and show that ZSLRC significantly outperformsstate-of-the-art methods on supervised learning, few-shot learning, andzero-shot learning tasks. Our experimental results also demonstrate theeffectiveness and robustness of our proposed model."
Multimodal Prototypical Networks for Few-shot Learning,"['Frederik Pahde', 'Mihai Puscas', 'Tassilo Klein', 'Moin Nabi']",http://arxiv.org/pdf/2011.08899v1.pdf,2020-11-17,"['cs.cv', 'cs.lg']","  Although providing exceptional results for many computer vision tasks,state-of-the-art deep learning algorithms catastrophically struggle in low datascenarios. However, if data in additional modalities exist (e.g. text) this cancompensate for the lack of data and improve the classification results. Toovercome this data scarcity, we design a cross-modal feature generationframework capable of enriching the low populated embedding space in few-shotscenarios, leveraging data from the auxiliary modality. Specifically, we traina generative model that maps text data into the visual feature space to obtainmore reliable prototypes. This allows to exploit data from additionalmodalities (e.g. text) during training while the ultimate task at test timeremains classification with exclusively visual data. We show that in such casesnearest neighbor classification is a viable approach and outperformstate-of-the-art single-modal and multimodal few-shot learning methods on theCUB-200 and Oxford-102 datasets."
Hybrid Consistency Training with Prototype Adaptation for Few-Shot  Learning,"['Meng Ye', 'Xiao Lin', 'Giedrius Burachas', 'Ajay Divakaran', 'Yi Yao']",http://arxiv.org/pdf/2011.10082v1.pdf,2020-11-19,['cs.cv'],"  Few-Shot Learning (FSL) aims to improve a model's generalization capabilityin low data regimes. Recent FSL works have made steady progress via metriclearning, meta learning, representation learning, etc. However, FSL remainschallenging due to the following longstanding difficulties. 1) The seen andunseen classes are disjoint, resulting in a distribution shift between trainingand testing. 2) During testing, labeled data of previously unseen classes issparse, making it difficult to reliably extrapolate from labeled supportexamples to unlabeled query examples. To tackle the first challenge, weintroduce Hybrid Consistency Training to jointly leverage interpolationconsistency, including interpolating hidden features, that imposes linearbehavior locally and data augmentation consistency that learns robustembeddings against sample variations. As for the second challenge, we useunlabeled examples to iteratively normalize features and adapt prototypes, asopposed to commonly used one-time update, for more reliable prototype-basedtransductive inference. We show that our method generates a 2% to 5%improvement over the state-of-the-art methods with similar backbones on fiveFSL datasets and, more notably, a 7% to 8% improvement for more challengingcross-domain FSL."
Is Support Set Diversity Necessary for Meta-Learning?,"['Amrith Setlur', 'Oscar Li', 'Virginia Smith']",http://arxiv.org/pdf/2011.14048v2.pdf,2020-11-28,"['cs.lg', 'stat.ml']","  Meta-learning is a popular framework for learning with limited data in whichan algorithm is produced by training over multiple few-shot learning tasks. Forclassification problems, these tasks are typically constructed by sampling asmall number of support and query examples from a subset of the classes. Whileconventional wisdom is that task diversity should improve the performance ofmeta-learning, in this work we find evidence to the contrary: we propose amodification to traditional meta-learning approaches in which we keep thesupport sets fixed across tasks, thus reducing task diversity. Surprisingly, wefind that not only does this modification not result in adverse effects, italmost always improves the performance for a variety of datasets andmeta-learning methods. We also provide several initial analyses to understandthis phenomenon. Our work serves to: (i) more closely investigate the effect ofsupport set construction for the problem of meta-learning, and (ii) suggest asimple, general, and competitive baseline for few-shot learning."
Fixed-MAML for Few Shot Classification in Multilingual Speech Emotion  Recognition,"['Anugunj Naman', 'Chetan Sinha', 'Liliana Mancini']",http://arxiv.org/pdf/2101.01356v2.pdf,2021-01-05,"['cs.sd', 'cs.lg', 'eess.as', '68t10']","  In this paper, we analyze the feasibility of applying few-shot learning tospeech emotion recognition task (SER). The current speech emotion recognitionmodels work exceptionally well but fail when then input is multilingual.Moreover, when training such models, the models' performance is suitable onlywhen the training corpus is vast. This availability of a big training corpus isa significant problem when choosing a language that is not much popular orobscure. We attempt to solve this challenge of multilingualism and lack ofavailable data by turning this problem into a few-shot learning problem. Wesuggest relaxing the assumption that all N classes in an N-way K-shot problembe new and define an N+F way problem where N and F are the number of emotionclasses and predefined fixed classes, respectively. We propose thismodification to the Model-Agnostic MetaLearning (MAML) algorithm to solve theproblem and call this new model F-MAML. This modification performs better thanthe original MAML and outperforms on EmoFilm dataset."
Local Propagation for Few-Shot Learning,"['Yann Lifchitz', 'Yannis Avrithis', 'Sylvaine Picard']",http://arxiv.org/pdf/2101.01480v1.pdf,2021-01-05,['cs.cv'],"  The challenge in few-shot learning is that available data is not enough tocapture the underlying distribution. To mitigate this, two emerging directionsare (a) using local image representations, essentially multiplying the amountof data by a constant factor, and (b) using more unlabeled data, for instanceby transductive inference, jointly on a number of queries. In this work, webring these two ideas together, introducing \emph{local propagation}. We treatlocal image features as independent examples, we build a graph on them and weuse it to propagate both the features themselves and the labels, known andunknown. Interestingly, since there is a number of features per image, even asingle query gives rise to transductive inference. As a result, we provide auniversally safe choice for few-shot inference under both non-transductive andtransductive settings, improving accuracy over corresponding methods. This isin contrast to existing solutions, where one needs to choose the methoddepending on the quantity of available data."
Improving Few-Shot Learning with Auxiliary Self-Supervised Pretext Tasks,"['Nathaniel Simard', 'Guillaume Lagrange']",http://arxiv.org/pdf/2101.09825v1.pdf,2021-01-24,"['cs.lg', 'cs.cv']","  Recent work on few-shot learning \cite{tian2020rethinking} showed thatquality of learned representations plays an important role in few-shotclassification performance. On the other hand, the goal of self-supervisedlearning is to recover useful semantic information of the data without the useof class labels. In this work, we exploit the complementarity of both paradigmsvia a multi-task framework where we leverage recent self-supervised methods asauxiliary tasks. We found that combining multiple tasks is often beneficial,and that solving them simultaneously can be done efficiently. Our resultssuggest that self-supervised auxiliary tasks are effective data-dependentregularizers for representation learning. Our code is available at:\url{https://github.com/nathanielsimard/improving-fs-ssl}."
Supervised Momentum Contrastive Learning for Few-Shot Classification,"['Orchid Majumder', 'Avinash Ravichandran', 'Subhransu Maji', 'Alessandro Achille', 'Marzia Polito', 'Stefano Soatto']",http://arxiv.org/pdf/2101.11058v2.pdf,2021-01-26,"['cs.cv', 'cs.lg']","  Few-shot learning aims to transfer information from one task to enablegeneralization on novel tasks given a few examples. This information is presentboth in the domain and the class labels. In this work we investigate thecomplementary roles of these two sources of information by combininginstance-discriminative contrastive learning and supervised learning in asingle framework called Supervised Momentum Contrastive learning (SUPMOCO). Ourapproach avoids a problem observed in supervised learning where information inimages not relevant to the task is discarded, which hampers theirgeneralization to novel tasks. We show that (self-supervised) contrastivelearning and supervised learning are mutually beneficial, leading to a newstate-of-the-art on the META-DATASET - a recently introduced benchmark forfew-shot learning. Our method is based on a simple modification of MOCO andscales better than prior work on combining supervised and self-supervisedlearning. This allows us to easily combine data from multiple domains leadingto further improvements."
Diverse Few-Shot Text Classification with Multiple Metrics,"['Mo Yu', 'Xiaoxiao Guo', 'Jinfeng Yi', 'Shiyu Chang', 'Saloni Potdar', 'Yu Cheng', 'Gerald Tesauro', 'Haoyu Wang', 'Bowen Zhou']",http://arxiv.org/pdf/1805.07513v1.pdf,2018-05-19,"['cs.cl', 'cs.lg']","  We study few-shot learning in natural language domains. Compared to manyexisting works that apply either metric-based or optimization-basedmeta-learning to image domain with low inter-task variance, we consider a morerealistic setting, where tasks are diverse. However, it imposes tremendousdifficulties to existing state-of-the-art metric-based algorithms since asingle metric is insufficient to capture complex task variations in naturallanguage domain. To alleviate the problem, we propose an adaptive metriclearning approach that automatically determines the best weighted combinationfrom a set of metrics obtained from meta-training tasks for a newly seenfew-shot task. Extensive quantitative evaluations on real-world sentimentanalysis and dialog intent classification datasets demonstrate that theproposed method performs favorably against state-of-the-art few shot learningalgorithms in terms of predictive accuracy. We make our code and data availablefor further study."
Task-Agnostic Meta-Learning for Few-shot Learning,"['Muhammad Abdullah Jamal', 'Guo-Jun Qi', 'Mubarak Shah']",http://arxiv.org/pdf/1805.07722v1.pdf,2018-05-20,"['cs.lg', 'stat.ml']","  Meta-learning approaches have been proposed to tackle the few-shot learningproblem.Typically, a meta-learner is trained on a variety of tasks in the hopesof being generalizable to new tasks. However, the generalizability on new tasksof a meta-learner could be fragile when it is over-trained on existing tasksduring meta-training phase. In other words, the initial model of a meta-learnercould be too biased towards existing tasks to adapt to new tasks, especiallywhen only very few examples are available to update the model. To avoid abiased meta-learner and improve its generalizability, we propose a novelparadigm of Task-Agnostic Meta-Learning (TAML) algorithms. Specifically, wepresent an entropy-based approach that meta-learns an unbiased initial modelwith the largest uncertainty over the output labels by preventing it fromover-performing in classification tasks. Alternatively, a more generalinequality-minimization TAML is presented for more ubiquitous scenarios bydirectly minimizing the inequality of initial losses beyond the classificationtasks wherever a suitable loss can be defined.Experiments on benchmarkeddatasets demonstrate that the proposed approaches outperform comparedmeta-learning algorithms in both few-shot classification and reinforcementlearning tasks."
Object-Level Representation Learning for Few-Shot Image Classification,"['Liangqu Long', 'Wei Wang', 'Jun Wen', 'Meihui Zhang', 'Qian Lin', 'Beng Chin Ooi']",http://arxiv.org/pdf/1805.10777v1.pdf,2018-05-28,"['cs.cv', 'cs.ai', 'cs.lg']","  Few-shot learning that trains image classifiers over few labeled examples percategory is a challenging task. In this paper, we propose to exploit anadditional big dataset with different categories to improve the accuracy offew-shot learning over our target dataset. Our approach is based on theobservation that images can be decomposed into objects, which may appear inimages from both the additional dataset and our target dataset. We use theobject-level relation learned from the additional dataset to infer thesimilarity of images in our target dataset with unseen categories. Nearestneighbor search is applied to do image classification, which is anon-parametric model and thus does not need fine-tuning. We evaluate ouralgorithm on two popular datasets, namely Omniglot and MiniImagenet. We obtain8.5\% and 2.7\% absolute improvements for 5-way 1-shot and 5-way 5-shotexperiments on MiniImagenet, respectively. Source code will be published uponacceptance."
Infinite Mixture Prototypes for Few-Shot Learning,"['Kelsey R. Allen', 'Evan Shelhamer', 'Hanul Shin', 'Joshua B. Tenenbaum']",http://arxiv.org/pdf/1902.04552v1.pdf,2019-02-12,"['cs.lg', 'stat.ml']","  We propose infinite mixture prototypes to adaptively represent both simpleand complex data distributions for few-shot learning. Our infinite mixtureprototypes represent each class by a set of clusters, unlike existingprototypical methods that represent each class by a single cluster. Byinferring the number of clusters, infinite mixture prototypes interpolatebetween nearest neighbor and prototypical representations, which improvesaccuracy and robustness in the few-shot regime. We show the importance ofadaptive capacity for capturing complex data distributions such as alphabets,with 25% absolute accuracy improvements over prototypical networks, while stillmaintaining or improving accuracy on the standard Omniglot and mini-ImageNetbenchmarks. In clustering labeled and unlabeled data by the same clusteringrule, infinite mixture prototypes achieves state-of-the-art semi-supervisedaccuracy. As a further capability, we show that infinite mixture prototypes canperform purely unsupervised clustering, unlike existing prototypical methods."
Are Few-Shot Learning Benchmarks too Simple ? Solving them without Task  Supervision at Test-Time,"['Gabriel Huang', 'Hugo Larochelle', 'Simon Lacoste-Julien']",http://arxiv.org/pdf/1902.08605v3.pdf,2019-02-22,"['cs.lg', 'stat.ml']","  We show that several popular few-shot learning benchmarks can be solved withvarying degrees of success without using support set Labels at Test-time (LT).To this end, we introduce a new baseline called Centroid Networks, amodification of Prototypical Networks in which the support set labels arehidden from the method at test-time and have to be recovered throughclustering. A benchmark that can be solved perfectly without LT does notrequire proper task adaptation and is therefore inadequate for evaluatingfew-shot methods. In practice, most benchmarks cannot be solved perfectlywithout LT, but running our baseline on any new combinations of architecturesand datasets gives insights on the baseline performance to be expected fromleveraging a good representation, before any adaptation to the test-timelabels."
HoloDetect: Few-Shot Learning for Error Detection,"['Alireza Heidari', 'Joshua McGrath', 'Ihab F. Ilyas', 'Theodoros Rekatsinas']",http://arxiv.org/pdf/1904.02285v1.pdf,2019-04-04,['cs.db'],"  We introduce a few-shot learning framework for error detection. We show thatdata augmentation (a form of weak supervision) is key to training high-quality,ML-based error detection models that require minimal human involvement. Ourframework consists of two parts: (1) an expressive model to learn richrepresentations that capture the inherent syntactic and semantic heterogeneityof errors; and (2) a data augmentation model that, given a small seed of cleanrecords, uses dataset-specific transformations to automatically generateadditional training data. Our key insight is to learn data augmentationpolicies from the noisy input dataset in a weakly supervised manner. We showthat our framework detects errors with an average precision of ~94% and anaverage recall of ~93% across a diverse array of datasets that exhibitdifferent types and amounts of errors. We compare our approach to acomprehensive collection of error detection methods, ranging from traditionalrule-based methods to ensemble-based and active learning approaches. We showthat data augmentation yields an average improvement of 20 F1 points while itrequires access to 3x fewer labeled examples compared to other ML approaches."
Semi-Supervised Few-Shot Learning for Dual Question-Answer Extraction,"['Jue Wang', 'Ke Chen', 'Lidan Shou', 'Sai Wu', 'Sharad Mehrotra']",http://arxiv.org/pdf/1904.03898v1.pdf,2019-04-08,"['cs.cl', 'cs.ai', 'cs.lg']","  This paper addresses the problem of key phrase extraction from sentences.Existing state-of-the-art supervised methods require large amounts of annotateddata to achieve good performance and generalization. Collecting labeled datais, however, often expensive. In this paper, we redefine the problem asquestion-answer extraction, and present SAMIE: Self-Asking Model forInformation Ixtraction, a semi-supervised model which dually learns to ask andto answer questions by itself. Briefly, given a sentence $s$ and an answer $a$,the model needs to choose the most appropriate question $\hat q$; meanwhile,for the given sentence $s$ and same question $\hat q$ selected in the previousstep, the model will predict an answer $\hat a$. The model can support few-shotlearning with very limited supervision. It can also be used to performclustering analysis when no supervision is provided. Experimental results showthat the proposed method outperforms typical supervised methods especially whengiven little labeled data."
Few Shot Speaker Recognition using Deep Neural Networks,"['Prashant Anand', 'Ajeet Kumar Singh', 'Siddharth Srivastava', 'Brejesh Lall']",http://arxiv.org/pdf/1904.08775v1.pdf,2019-04-17,"['eess.as', 'cs.lg', 'cs.sd']","  The recent advances in deep learning are mostly driven by availability oflarge amount of training data. However, availability of such data is not alwayspossible for specific tasks such as speaker recognition where collection oflarge amount of data is not possible in practical scenarios. Therefore, in thispaper, we propose to identify speakers by learning from only a few trainingexamples. To achieve this, we use a deep neural network with prototypical losswhere the input to the network is a spectrogram. For output, we project theclass feature vectors into a common embedding space, followed byclassification. Further, we show the effectiveness of capsule net in a few shotlearning setting. To this end, we utilize an auto-encoder to learn generalizedfeature embeddings from class-specific embeddings obtained from capsulenetwork. We provide exhaustive experiments on publicly available datasets andcompetitive baselines, demonstrating the superiority and generalization abilityof the proposed few shot learning pipelines."
Hierarchical Meta Learning,"['Yingtian Zou', 'Jiashi Feng']",http://arxiv.org/pdf/1904.09081v1.pdf,2019-04-19,"['cs.lg', 'stat.ml']","  Meta learning is a promising solution to few-shot learning problems. However,existing meta learning methods are restricted to the scenarios where trainingand application tasks share the same out-put structure. To obtain a meta modelapplicable to the tasks with new structures, it is required to collect newtraining data and repeat the time-consuming meta training procedure. This makesthem inefficient or even inapplicable in learning to solve heterogeneousfew-shot learning tasks. We thus develop a novel and principledHierarchicalMeta Learning (HML) method. Different from existing methods thatonly focus on optimizing the adaptability of a meta model to similar tasks, HMLalso explicitly optimizes its generalizability across heterogeneous tasks. Tothis end, HML first factorizes a set of similar training tasks intoheterogeneous ones and trains the meta model over them at two levels tomaximize adaptation and generalization performance respectively. The resultantmodel can then directly generalize to new tasks. Extensive experiments onfew-shot classification and regression problems clearly demonstrate thesuperiority of HML over fine-tuning and state-of-the-art meta learningapproaches in terms of generalization across heterogeneous tasks."
Few-shot Learning for Domain-specific Fine-grained Image Classification,"['Xin Sun', 'Hongwei Xv', 'Junyu Dong', 'Qiong Li', 'Changrui Chen']",http://arxiv.org/pdf/1907.09647v3.pdf,2019-07-23,['cs.cv'],"  Learning to recognize novel visual categories from a few examples is achallenging task for machines in real-world industrial applications. Incontrast, humans have the ability to discriminate even similar objects withlittle supervision. This paper attempts to address the few shot fine-grainedimage classification problem. We propose a feature fusion model to explorediscriminative features by focusing on key regions. The model utilizes thefocus area location mechanism to discover the perceptually similar regionsamong objects. High-order integration is employed to capture the interactioninformation among intra-parts. We also design a Center Neighbor Loss to formrobust embedding space distributions. Furthermore, we build a typicalfine-grained and few-shot learning dataset miniPPlankton from the real-worldapplication in the area of marine ecological environments. Extensiveexperiments are carried out to validate the performance of our method. Theresults demonstrate that our model achieves competitive performance comparedwith state-of-the-art models. Our work is a valuable complement to the modeldomain-specific industrial applications."
SimpleShot: Revisiting Nearest-Neighbor Classification for Few-Shot  Learning,"['Yan Wang', 'Wei-Lun Chao', 'Kilian Q. Weinberger', 'Laurens van der Maaten']",http://arxiv.org/pdf/1911.04623v2.pdf,2019-11-12,['cs.cv'],"  Few-shot learners aim to recognize new object classes based on a small numberof labeled training examples. To prevent overfitting, state-of-the-art few-shotlearners use meta-learning on convolutional-network features and performclassification using a nearest-neighbor classifier. This paper studies theaccuracy of nearest-neighbor baselines without meta-learning. Surprisingly, wefind simple feature transformations suffice to obtain competitive few-shotlearning accuracies. For example, we find that a nearest-neighbor classifierused in combination with mean-subtraction and L2-normalization outperformsprior results in three out of five settings on the miniImageNet dataset."
Prototype Rectification for Few-Shot Learning,"['Jinlu Liu', 'Liang Song', 'Yongqiang Qin']",http://arxiv.org/pdf/1911.10713v4.pdf,2019-11-25,['cs.cv'],"  Few-shot learning requires to recognize novel classes with scarce labeleddata. Prototypical network is useful in existing researches, however, trainingon narrow-size distribution of scarce data usually tends to get biasedprototypes. In this paper, we figure out two key influencing factors of theprocess: the intra-class bias and the cross-class bias. We then propose asimple yet effective approach for prototype rectification in transductivesetting. The approach utilizes label propagation to diminish the intra-classbias and feature shifting to diminish the cross-class bias. We also conducttheoretical analysis to derive its rationality as well as the lower bound ofthe performance. Effectiveness is shown on three few-shot benchmarks. Notably,our approach achieves state-of-the-art performance on both miniImageNet (70.31%on 1-shot and 81.89% on 5-shot) and tieredImageNet (78.74% on 1-shot and 86.92%on 5-shot)."
Generalized Adaptation for Few-Shot Learning,"['Liang Song', 'Jinlu Liu', 'Yongqiang Qin']",http://arxiv.org/pdf/1911.10807v3.pdf,2019-11-25,['cs.cv'],"  Many Few-Shot Learning research works have two stages: pre-training basemodel and adapting to novel model. In this paper, we propose to use closed-formbase learner, which constrains the adapting stage with pre-trained base modelto get better generalized novel model. Following theoretical analysis provesits rationality as well as indication of how to train a well-generalized basemodel. We then conduct experiments on four benchmarks and achievestate-of-the-art performance in all cases. Notably, we achieve the accuracy of87.75% on 5-shot miniImageNet which approximately outperforms existing methodsby 10%."
Exploiting the Matching Information in the Support Set for Few Shot  Event Classification,"['Viet Dac Lai', 'Franck Dernoncourt', 'Thien Huu Nguyen']",http://arxiv.org/pdf/2002.05295v2.pdf,2020-02-13,"['cs.cl', 'cs.lg', 'stat.ml']","  The existing event classification (EC) work primarily focuseson thetraditional supervised learning setting in which models are unableto extractevent mentions of new/unseen event types. Few-shot learninghas not beeninvestigated in this area although it enables EC models toextend theiroperation to unobserved event types. To fill in this gap, inthis work, weinvestigate event classification under the few-shot learningsetting. We proposea novel training method for this problem that exten-sively exploit the supportset during the training process of a few-shotlearning model. In particular, inaddition to matching the query exam-ple with those in the support set fortraining, we seek to further matchthe examples within the support setthemselves. This method providesmore training signals for the models and can beapplied to every metric-learning-based few-shot learning methods. Our extensiveexperiments ontwo benchmark EC datasets show that the proposed method canimprovethe best reported few-shot learning models by up to 10% on accuracyforevent classification"
Few-shot acoustic event detection via meta-learning,"['Bowen Shi', 'Ming Sun', 'Krishna C. Puvvada', 'Chieh-Chi Kao', 'Spyros Matsoukas', 'Chao Wang']",http://arxiv.org/pdf/2002.09143v1.pdf,2020-02-21,"['cs.lg', 'cs.sd', 'eess.as', 'stat.ml']","  We study few-shot acoustic event detection (AED) in this paper. Few-shotlearning enables detection of new events with very limited labeled data.Compared to other research areas like computer vision, few-shot learning foraudio recognition has been under-studied. We formulate few-shot AED problem andexplore different ways of utilizing traditional supervised methods for thissetting as well as a variety of meta-learning approaches, which areconventionally used to solve few-shot classification problem. Compared tosupervised baselines, meta-learning models achieve superior performance, thusshowing its effectiveness on generalization to new audio events. Our analysisincluding impact of initialization and domain discrepancy further validate theadvantage of meta-learning approaches in few-shot AED."
Rethinking Few-Shot Image Classification: a Good Embedding Is All You  Need?,"['Yonglong Tian', 'Yue Wang', 'Dilip Krishnan', 'Joshua B. Tenenbaum', 'Phillip Isola']",http://arxiv.org/pdf/2003.11539v2.pdf,2020-03-25,"['cs.cv', 'cs.lg']","  The focus of recent meta-learning research has been on the development oflearning algorithms that can quickly adapt to test time tasks with limited dataand low computational cost. Few-shot learning is widely used as one of thestandard benchmarks in meta-learning. In this work, we show that a simplebaseline: learning a supervised or self-supervised representation on themeta-training set, followed by training a linear classifier on top of thisrepresentation, outperforms state-of-the-art few-shot learning methods. Anadditional boost can be achieved through the use of self-distillation. Thisdemonstrates that using a good learned embedding model can be more effectivethan sophisticated meta-learning algorithms. We believe that our findingsmotivate a rethinking of few-shot image classification benchmarks and theassociated role of meta-learning algorithms. Code is available at:http://github.com/WangYueFt/rfs/."
Learning What to Learn for Video Object Segmentation,"['Goutam Bhat', 'Felix Järemo Lawin', 'Martin Danelljan', 'Andreas Robinson', 'Michael Felsberg', 'Luc Van Gool', 'Radu Timofte']",http://arxiv.org/pdf/2003.11540v2.pdf,2020-03-25,['cs.cv'],"  Video object segmentation (VOS) is a highly challenging problem, since thetarget object is only defined during inference with a given first-framereference mask. The problem of how to capture and utilize this limited targetinformation remains a fundamental research question. We address this byintroducing an end-to-end trainable VOS architecture that integrates adifferentiable few-shot learning module. This internal learner is designed topredict a powerful parametric model of the target by minimizing a segmentationerror in the first frame. We further go beyond standard few-shot learningtechniques by learning what the few-shot learner should learn. This allows usto achieve a rich internal representation of the target in the current frame,significantly increasing the segmentation accuracy of our approach. We performextensive experiments on multiple benchmarks. Our approach sets a newstate-of-the-art on the large-scale YouTube-VOS 2018 dataset by achieving anoverall score of 81.5, corresponding to a 2.6% relative improvement over theprevious best result."
Adversarial Feature Hallucination Networks for Few-Shot Learning,"['Kai Li', 'Yulun Zhang', 'Kunpeng Li', 'Yun Fu']",http://arxiv.org/pdf/2003.13193v2.pdf,2020-03-30,['cs.cv'],"  The recent flourish of deep learning in various tasks is largely accreditedto the rich and accessible labeled data. Nonetheless, massive supervisionremains a luxury for many real applications, boosting great interest inlabel-scarce techniques such as few-shot learning (FSL), which aims to learnconcept of new classes with a few labeled samples. A natural approach to FSL isdata augmentation and many recent works have proved the feasibility byproposing various data synthesis models. However, these models fail to wellsecure the discriminability and diversity of the synthesized data and thusoften produce undesirable results. In this paper, we propose AdversarialFeature Hallucination Networks (AFHN) which is based on conditional WassersteinGenerative Adversarial networks (cWGAN) and hallucinates diverse anddiscriminative features conditioned on the few labeled samples. Two novelregularizers, i.e., the classification regularizer and the anti-collapseregularizer, are incorporated into AFHN to encourage discriminability anddiversity of the synthesized features, respectively. Ablation study verifiesthe effectiveness of the proposed cWGAN based feature hallucination frameworkand the proposed regularizers. Comparative results on three common benchmarkdatasets substantiate the superiority of AFHN to existing data augmentationbased FSL approaches and other state-of-the-art ones."
MA 3 : Model Agnostic Adversarial Augmentation for Few Shot learning,"['Rohit Jena', 'Shirsendu Sukanta Halder', 'Katia Sycara']",http://arxiv.org/pdf/2004.05100v1.pdf,2020-04-10,"['cs.cv', 'cs.lg']","  Despite the recent developments in vision-related problems using deep neuralnetworks, there still remains a wide scope in the improvement of generalizingthese models to unseen examples. In this paper, we explore the domain offew-shot learning with a novel augmentation technique. In contrast to othergenerative augmentation techniques, where the distribution over input imagesare learnt, we propose to learn the probability distribution over the imagetransformation parameters which are easier and quicker to learn. Our techniqueis fully differentiable which enables its extension to versatile data-sets andbase models. We evaluate our proposed method on multiple base-networks and 2data-sets to establish the robustness and efficiency of this method. We obtainan improvement of nearly 4% by adding our augmentation module without makingany change in network architectures. We also make the code readily availablefor usage by the community."
TAEN: Temporal Aware Embedding Network for Few-Shot Action Recognition,"['Rami Ben-Ari', 'Mor Shpigel', 'Ophir Azulai', 'Udi Barzelay', 'Daniel Rotman']",http://arxiv.org/pdf/2004.10141v2.pdf,2020-04-21,['cs.cv'],"  Classification of new class entities requires collecting and annotatinghundreds or thousands of samples that is often prohibitively costly. Few-shotlearning suggests learning to classify new classes using just a few examples.Only a small number of studies address the challenge of few-shot learning onspatio-temporal patterns such as videos. In this paper, we present the TemporalAware Embedding Network (TAEN) for few-shot action recognition, that learns torepresent actions, in a metric space as a trajectory, conveying both short termsemantics and longer term connectivity between action parts. We demonstrate theeffectiveness of TAEN on two few shot tasks, video classification and temporalaction detection and evaluate our method on the Kinetics-400 and on ActivityNet1.2 few-shot benchmarks. With training of just a few fully connected layers wereach comparable results to prior art on both few shot video classification andtemporal detection tasks, while reaching state-of-the-art in certain scenarios."
AdaDurIAN: Few-shot Adaptation for Neural Text-to-Speech with DurIAN,"['Zewang Zhang', 'Qiao Tian', 'Heng Lu', 'Ling-Hui Chen', 'Shan Liu']",http://arxiv.org/pdf/2005.05642v1.pdf,2020-05-12,"['cs.sd', 'cs.cl', 'eess.as']","  This paper investigates how to leverage a DurIAN-based average model toenable a new speaker to have both accurate pronunciation and fluentcross-lingual speaking with very limited monolingual data. A weakness of therecently proposed end-to-end text-to-speech (TTS) systems is that robustalignment is hard to achieve, which hinders it to scale well with very limiteddata. To cope with this issue, we introduce AdaDurIAN by training an improvedDurIAN-based average model and leverage it to few-shot learning with the sharedspeaker-independent content encoder across different speakers. Several few-shotlearning tasks in our experiments show AdaDurIAN can outperform the baselineend-to-end system by a large margin. Subjective evaluations also show thatAdaDurIAN yields higher mean opinion score (MOS) of naturalness and morepreferences of speaker similarity. In addition, we also apply AdaDurIAN toemotion transfer tasks and demonstrate its promising performance."
LFD-ProtoNet: Prototypical Network Based on Local Fisher Discriminant  Analysis for Few-shot Learning,"['Kei Mukaiyama', 'Issei Sato', 'Masashi Sugiyama']",http://arxiv.org/pdf/2006.08306v2.pdf,2020-06-15,"['cs.lg', 'stat.ml', '68t01(primary), 68t05(secondary)']","  The prototypical network (ProtoNet) is a few-shot learning framework thatperforms metric learning and classification using the distance to prototyperepresentations of each class. It has attracted a great deal of attentionrecently since it is simple to implement, highly extensible, and performs wellin experiments. However, it only takes into account the mean of the supportvectors as prototypes and thus it performs poorly when the support set has highvariance. In this paper, we propose to combine ProtoNet with local Fisherdiscriminant analysis to reduce the local within-class covariance and increasethe local between-class covariance of the support set. We show the usefulnessof the proposed method by theoretically providing an expected risk bound andempirically demonstrating its superior classification accuracy on miniImageNetand tieredImageNet."
Online Few-shot Gesture Learning on a Neuromorphic Processor,"['Kenneth Stewart', 'Garrick Orchard', 'Sumit Bam Shrestha', 'Emre Neftci']",http://arxiv.org/pdf/2008.01151v2.pdf,2020-08-03,['cs.ne'],"  We present the Surrogate-gradient Online Error-triggered Learning (SOEL)system for online few-shot learning on neuromorphic processors. The SOELlearning system uses a combination of transfer learning and principles ofcomputational neuroscience and deep learning. We show that partially traineddeep Spiking Neural Networks (SNNs) implemented on neuromorphic hardware canrapidly adapt online to new classes of data within a domain. SOEL updatestrigger when an error occurs, enabling faster learning with fewer updates.Using gesture recognition as a case study, we show SOEL can be used for onlinefew-shot learning of new classes of pre-recorded gesture data and rapid onlinelearning of new gestures from data streamed live from a Dynamic Active-pixelVision Sensor to an Intel Loihi neuromorphic research processor."
Few shot clustering for indoor occupancy detection with extremely  low-quality images from battery free cameras,"['Homagni Saha', 'Sin Yong Tan', 'Ali Saffari', 'Mohamad Katanbaf', 'Joshua R. Smith', 'Soumik Sarkar']",http://arxiv.org/pdf/2008.05654v1.pdf,2020-08-13,"['cs.cv', 'cs.lg']","  Reliable detection of human occupancy in indoor environments is critical forvarious energy efficiency, security, and safety applications. We consider thischallenge of occupancy detection using extremely low-quality,privacy-preserving images from low power image sensors. We propose a combinedfew shot learning and clustering algorithm to address this challenge that hasvery low commissioning and maintenance cost. While the few shot learningconcept enables us to commission our system with a few labeled examples, theclustering step serves the purpose of online adaptation to changing imagingenvironment over time. Apart from validating and comparing our algorithm onbenchmark datasets, we also demonstrate performance of our algorithm onstreaming images collected from real homes using our novel battery free camerahardware."
Proxy Network for Few Shot Learning,"['Bin Xiao', 'Chien-Liang Liu', 'Wen-Hoar Hsaio']",http://arxiv.org/pdf/2009.04292v1.pdf,2020-09-09,"['cs.lg', 'stat.ml']","  The use of a few examples for each class to train a predictive model that canbe generalized to novel classes is a crucial and valuable research direction inartificial intelligence. This work addresses this problem by proposing afew-shot learning (FSL) algorithm called proxy network under the architectureof meta-learning. Metric-learning based approaches assume that the data pointswithin the same class should be close, whereas the data points in the differentclasses should be separated as far as possible in the embedding space. Weconclude that the success of metric-learning based approaches lies in the dataembedding, the representative of each class, and the distance metric. In thiswork, we propose a simple but effective end-to-end model that directly learnsproxies for class representative and distance metric from data simultaneously.We conduct experiments on CUB and mini-ImageNet datasets in 1-shot-5-way and5-shot-5-way scenarios, and the experimental results demonstrate thesuperiority of our proposed method over state-of-the-art methods. Besides, weprovide a detailed analysis of our proposed method."
'Less Than One'-Shot Learning: Learning N Classes From M<N Samples,"['Ilia Sucholutsky', 'Matthias Schonlau']",http://arxiv.org/pdf/2009.08449v1.pdf,2020-09-17,"['cs.lg', 'stat.ml']","  Deep neural networks require large training sets but suffer from highcomputational cost and long training times. Training on much smaller trainingsets while maintaining nearly the same accuracy would be very beneficial. Inthe few-shot learning setting, a model must learn a new class given only asmall number of samples from that class. One-shot learning is an extreme formof few-shot learning where the model must learn a new class from a singleexample. We propose the `less than one'-shot learning task where models mustlearn $N$ new classes given only $M<N$ examples and we show that this isachievable with the help of soft labels. We use a soft-label generalization ofthe k-Nearest Neighbors classifier to explore the intricate decision landscapesthat can be created in the `less than one'-shot learning setting. We analyzethese decision landscapes to derive theoretical lower bounds for separating $N$classes using $M<N$ soft-label samples and investigate the robustness of theresulting systems."
Self-Supervised Few-Shot Learning on Point Clouds,"['Charu Sharma', 'Manohar Kaul']",http://arxiv.org/pdf/2009.14168v1.pdf,2020-09-29,"['cs.lg', 'stat.ml']","  The increased availability of massive point clouds coupled with their utilityin a wide variety of applications such as robotics, shape synthesis, andself-driving cars has attracted increased attention from both industry andacademia. Recently, deep neural networks operating on labeled point clouds haveshown promising results on supervised learning tasks like classification andsegmentation. However, supervised learning leads to the cumbersome task ofannotating the point clouds. To combat this problem, we propose two novelself-supervised pre-training tasks that encode a hierarchical partitioning ofthe point clouds using a cover-tree, where point cloud subsets lie within ballsof varying radii at each level of the cover-tree. Furthermore, ourself-supervised learning network is restricted to pre-train on the support set(comprising of scarce training examples) used to train the downstream networkin a few-shot learning (FSL) setting. Finally, the fully-trainedself-supervised network's point embeddings are input to the downstream task'snetwork. We present a comprehensive empirical evaluation of our method on bothdownstream classification and segmentation tasks and show that supervisedmethods pre-trained with our self-supervised learning method significantlyimprove the accuracy of state-of-the-art methods. Additionally, our method alsooutperforms previous unsupervised methods in downstream classification tasks."
Few-shot Learning for Time-series Forecasting,"['Tomoharu Iwata', 'Atsutoshi Kumagai']",http://arxiv.org/pdf/2009.14379v1.pdf,2020-09-30,"['stat.ml', 'cs.lg']","  Time-series forecasting is important for many applications. Forecastingmodels are usually trained using time-series data in a specific target task.However, sufficient data in the target task might be unavailable, which leadsto performance degradation. In this paper, we propose a few-shot learningmethod that forecasts a future value of a time-series in a target task given afew time-series in the target task. Our model is trained using time-series datain multiple training tasks that are different from target tasks. Our model usesa few time-series to build a forecasting function based on a recurrent neuralnetwork with an attention mechanism. With the attention mechanism, we canretrieve useful patterns in a small number of time-series for the currentsituation. Our model is trained by minimizing an expected test error offorecasting next timestep values. We demonstrate the effectiveness of theproposed method using 90 time-series datasets."
Few-shot Learning for Spatial Regression,"['Tomoharu Iwata', 'Yusuke Tanaka']",http://arxiv.org/pdf/2010.04360v1.pdf,2020-10-09,"['stat.ml', 'cs.lg']","  We propose a few-shot learning method for spatial regression. AlthoughGaussian processes (GPs) have been successfully used for spatial regression,they require many observations in the target task to achieve a high predictiveperformance. Our model is trained using spatial datasets on various attributesin various regions, and predicts values on unseen attributes in unseen regionsgiven a few observed data. With our model, a task representation is inferredfrom given small data using a neural network. Then, spatial values arepredicted by neural networks with a GP framework, in which task-specificproperties are controlled by the task representations. The GP framework allowsus to analytically obtain predictions that are adapted to small data. By usingthe adapted predictions in the objective function, we can train our modelefficiently and effectively so that the test predictive performance improveswhen adapted to newly given small data. In our experiments, we demonstrate thatthe proposed method achieves better predictive performance than existingmeta-learning methods using spatial datasets."
Training Data Generating Networks: Shape Reconstruction via Bi-level  Optimization,"['Biao Zhang', 'Peter Wonka']",http://arxiv.org/pdf/2010.08276v2.pdf,2020-10-16,"['cs.cv', 'cs.cg', 'cs.gr']","  We propose a novel 3d shape representation for 3d shape reconstruction from asingle image. Rather than predicting a shape directly, we train a network togenerate a training set which will be fed into another learning algorithm todefine the shape. The nested optimization problem can be modeled by bi-leveloptimization. Specifically, the algorithms for bi-level optimization are alsobeing used in meta learning approaches for few-shot learning. Our frameworkestablishes a link between 3D shape analysis and few-shot learning. We combinetraining data generating networks with bi-level optimization algorithms toobtain a complete framework for which all components can be jointly trained. Weimprove upon recent work on standard benchmarks for 3d shape reconstruction."
Task-Adaptive Feature Transformer for Few-Shot Segmentation,"['Jun Seo', 'Young-Hyun Park', 'Sung-Whan Yoon', 'Jaekyun Moon']",http://arxiv.org/pdf/2010.11437v1.pdf,2020-10-22,"['cs.cv', 'cs.ai']","  Few-shot learning allows machines to classify novel classes using only a fewlabeled samples. Recently, few-shot segmentation aiming at semanticsegmentation on low sample data has also seen great interest. In this paper, wepropose a learnable module for few-shot segmentation, the task-adaptive featuretransformer (TAFT). TAFT linearly transforms task-specific high-level featuresto a set of task-agnostic features well-suited to the segmentation job. Usingthis task-conditioned feature transformation, the model is shown to effectivelyutilize the semantic information in novel classes to generate tightsegmentation masks. The proposed TAFT module can be easily plugged intoexisting semantic segmentation algorithms to achieve few-shot segmentationcapability with only a few added parameters. We combine TAFT with Deeplab V3+,a well-known segmentation architecture; experiments on the PASCAL-$5^i$ datasetconfirm that this combination successfully adds few-shot learning capability tothe segmentation algorithm, achieving the state-of-the-art few-shotsegmentation performance in some key representative cases."
Few-shot Decoding of Brain Activation Maps,"['Myriam Bontonou', 'Giulia Lioi', 'Nicolas Farrugia', 'Vincent Gripon']",http://arxiv.org/pdf/2010.12500v3.pdf,2020-10-23,['cs.lg'],"  Few-shot learning addresses problems for which a limited number of trainingexamples are available. So far, the field has been mostly driven byapplications in computer vision. Here, we are interested in adapting recentlyintroduced few-shot methods to solve problems dealing with neuroimaging data, apromising application field. To this end, we create a neuroimaging benchmarkdataset for few-shot learning and compare multiple learning paradigms,including meta-learning, as well as various backbone networks. Our experimentsshow that few-shot methods are able to efficiently decode brain signals usingfew examples, which paves the way for a number of applications in clinical andcognitive neuroscience, such as identifying biomarkers from brain scans orunderstanding the generalization of brain representations across a wide rangeof cognitive tasks."
Iterative label cleaning for transductive and semi-supervised few-shot  learning,"['Michalis Lazarou', 'Tania Stathaki', 'Yannis Avrithis']",http://arxiv.org/pdf/2012.07962v3.pdf,2020-12-14,"['cs.lg', 'cs.ai', 'cs.cv']","  Few-shot learning amounts to learning representations and acquiring knowledgesuch that novel tasks may be solved with both supervision and data beinglimited. Improved performance is possible by transductive inference, where theentire test set is available concurrently, and semi-supervised learning, wheremore unlabeled data is available. Focusing on these two settings, we introducea new algorithm that leverages the manifold structure of the labeled andunlabeled data distribution to predict pseudo-labels, while balancing overclasses and using the loss value distribution of a limited-capacity classifierto select the cleanest labels, iteratively improving the quality ofpseudo-labels. Our solution surpasses or matches the state of the art resultson four benchmark datasets, namely miniImageNet, tieredImageNet, CUB andCIFAR-FS, while being robust over feature space pre-processing and the quantityof available data. The publicly available source code can be found inhttps://github.com/MichalisLazarou/iLPC."
Few-shot Sequence Learning with Transformers,"['Lajanugen Logeswaran', 'Ann Lee', 'Myle Ott', 'Honglak Lee', ""Marc'Aurelio Ranzato"", 'Arthur Szlam']",http://arxiv.org/pdf/2012.09543v1.pdf,2020-12-17,['cs.lg'],"  Few-shot algorithms aim at learning new tasks provided only a handful oftraining examples. In this work we investigate few-shot learning in the settingwhere the data points are sequences of tokens and propose an efficient learningalgorithm based on Transformers. In the simplest setting, we append a token toan input sequence which represents the particular task to be undertaken, andshow that the embedding of this token can be optimized on the fly given fewlabeled examples. Our approach does not require complicated changes to themodel architecture such as adapter layers nor computing second orderderivatives as is currently popular in the meta-learning and few-shot learningliterature. We demonstrate our approach on a variety of tasks, and analyze thegeneralization properties of several model variants and baseline approaches. Inparticular, we show that compositional task descriptors can improveperformance. Experiments show that our approach works at least as well as othermethods, while being more computationally efficient."
"On Episodes, Prototypical Networks, and Few-shot Learning","['Steinar Laenen', 'Luca Bertinetto']",http://arxiv.org/pdf/2012.09831v2.pdf,2020-12-17,"['cs.lg', 'cs.cv']","  Episodic learning is a popular practice among researchers and practitionersinterested in few-shot learning. It consists of organising training in a seriesof learning problems (or episodes), each divided into a small training andvalidation subset to mimic the circumstances encountered during evaluation. Butis this always necessary? In this paper, we investigate the usefulness ofepisodic learning in methods which use nonparametric approaches, such asnearest neighbours, at the level of the episode. For these methods, we not onlyshow how the constraints imposed by episodic learning are not necessary, butthat they in fact lead to a data-inefficient way of exploiting trainingbatches. We conduct a wide range of ablative experiments with Matching andPrototypical Networks, two of the most popular methods that use nonparametricapproaches at the level of the episode. Their ""non-episodic"" counterparts areconsiderably simpler, have less hyperparameters, and improve their performancein multiple few-shot classification datasets."
PTN: A Poisson Transfer Network for Semi-supervised Few-shot Learning,"['Huaxi Huang', 'Junjie Zhang', 'Jian Zhang', 'Qiang Wu', 'Chang Xu']",http://arxiv.org/pdf/2012.10844v3.pdf,2020-12-20,['cs.cv'],"  The predicament in semi-supervised few-shot learning (SSFSL) is to maximizethe value of the extra unlabeled data to boost the few-shot learner. In thispaper, we propose a Poisson Transfer Network (PTN) to mine the unlabeledinformation for SSFSL from two aspects. First, the Poisson Merriman Bence Osher(MBO) model builds a bridge for the communications between labeled andunlabeled examples. This model serves as a more stable and informativeclassifier than traditional graph-based SSFSL methods in the message-passingprocess of the labels. Second, the extra unlabeled samples are employed totransfer the knowledge from base classes to novel classes through contrastivelearning. Specifically, we force the augmented positive pairs close while pushthe negative ones distant. Our contrastive transfer scheme implicitly learnsthe novel-class embeddings to alleviate the over-fitting problem on the fewlabeled data. Thus, we can mitigate the degeneration of embedding generality innovel classes. Extensive experiments indicate that PTN outperforms thestate-of-the-art few-shot and SSFSL models on miniImageNet and tieredImageNetbenchmark datasets."
Few Shot Learning With No Labels,"['Aditya Bharti', 'N. B. Vineeth', 'C. V. Jawahar']",http://arxiv.org/pdf/2012.13751v1.pdf,2020-12-26,"['cs.cv', 'cs.lg']","  Few-shot learners aim to recognize new categories given only a small numberof training samples. The core challenge is to avoid overfitting to the limiteddata while ensuring good generalization to novel classes. Existing literaturemakes use of vast amounts of annotated data by simply shifting the labelrequirement from novel classes to base classes. Since data annotation istime-consuming and costly, reducing the label requirement even further is animportant goal. To that end, our paper presents a more challenging few-shotsetting where no label access is allowed during training or testing. Byleveraging self-supervision for learning image representations and imagesimilarity for classification at test time, we achieve competitive baselineswhile using \textbf{zero} labels, which is at least fewer labels thanstate-of-the-art. We hope that this work is a step towards developing few-shotlearning methods which do not depend on annotated data at all. Our code will bepublicly released."
End-to-end Generative Zero-shot Learning via Few-shot Learning,"['Georgios Chochlakis', 'Efthymios Georgiou', 'Alexandros Potamianos']",http://arxiv.org/pdf/2102.04379v1.pdf,2021-02-08,['cs.cv'],"  Contemporary state-of-the-art approaches to Zero-Shot Learning (ZSL) traingenerative nets to synthesize examples conditioned on the provided metadata.Thereafter, classifiers are trained on these synthetic data in a supervisedmanner. In this work, we introduce Z2FSL, an end-to-end generative ZSLframework that uses such an approach as a backbone and feeds its synthesizedoutput to a Few-Shot Learning (FSL) algorithm. The two modules are trainedjointly. Z2FSL solves the ZSL problem with a FSL algorithm, reducing, ineffect, ZSL to FSL. A wide class of algorithms can be integrated within ourframework. Our experimental results show consistent improvement over severalbaselines. The proposed method, evaluated across standard benchmarks, showsstate-of-the-art or competitive performance in ZSL and Generalized ZSL tasks."
Multi-Pretext Attention Network for Few-shot Learning with  Self-supervision,"['Hainan Li', 'Renshuai Tao', 'Jun Li', 'Haotong Qin', 'Yifu Ding', 'Shuo Wang', 'Xianglong Liu']",http://arxiv.org/pdf/2103.05985v1.pdf,2021-03-10,['cs.cv'],"  Few-shot learning is an interesting and challenging study, which enablesmachines to learn from few samples like humans. Existing studies rarely exploitauxiliary information from large amount of unlabeled data. Self-supervisedlearning is emerged as an efficient method to utilize unlabeled data. Existingself-supervised learning methods always rely on the combination of geometrictransformations for the single sample by augmentation, while seriously neglectthe endogenous correlation information among different samples that is the sameimportant for the task. In this work, we propose a Graph-driven Clustering(GC), a novel augmentation-free method for self-supervised learning, which doesnot rely on any auxiliary sample and utilizes the endogenous correlationinformation among input samples. Besides, we propose Multi-pretext AttentionNetwork (MAN), which exploits a specific attention mechanism to combine thetraditional augmentation-relied methods and our GC, adaptively learning theiroptimized weights to improve the performance and enabling the feature extractorto obtain more universal representations. We evaluate our MAN extensively onminiImageNet and tieredImageNet datasets and the results demonstrate that theproposed method outperforms the state-of-the-art (SOTA) relevant methods."
A Few-Shot Learning Approach for Accelerated MRI via Fusion of  Data-Driven and Subject-Driven Priors,"['Salman Ul Hassan Dar', 'Mahmut Yurt', 'Tolga Çukur']",http://arxiv.org/pdf/2103.07790v1.pdf,2021-03-13,"['cs.cv', 'eess.iv']","  Deep neural networks (DNNs) have recently found emerging use in acceleratedMRI reconstruction. DNNs typically learn data-driven priors from large datasetsconstituting pairs of undersampled and fully-sampled acquisitions. Acquiringsuch large datasets, however, might be impractical. To mitigate thislimitation, we propose a few-shot learning approach for accelerated MRI thatmerges subject-driven priors obtained via physical signal models withdata-driven priors obtained from a few training samples. Demonstrations onbrain MR images from the NYU fastMRI dataset indicate that the proposedapproach requires just a few samples to outperform traditional parallel imagingand DNN algorithms."
DMN4: Few-shot Learning via Discriminative Mutual Nearest Neighbor  Neural Network,"['Yang Liu', 'Tu Zheng', 'Jie Song', 'Deng Cai', 'Xiaofei He']",http://arxiv.org/pdf/2103.08160v3.pdf,2021-03-15,"['cs.cv', 'cs.lg']","  Few-shot learning (FSL) aims to classify images under low-data regimes, wherethe conventional pooled global feature is likely to lose useful localcharacteristics. Recent work has achieved promising performances by using deepdescriptors. They generally take all deep descriptors from neural networks intoconsideration while ignoring that some of them are useless in classificationdue to their limited receptive field, e.g., task-irrelevant descriptors couldbe misleading and multiple aggregative descriptors from background cluttercould even overwhelm the object's presence. In this paper, we argue that aMutual Nearest Neighbor (MNN) relation should be established to explicitlyselect the query descriptors that are most relevant to each task and discardless relevant ones from aggregative clutters in FSL. Specifically, we proposeDiscriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL. Extensiveexperiments demonstrate that our method outperforms the existingstate-of-the-arts on both fine-grained and generalized datasets."
Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning,"['Namyeong Kwon', 'Hwidong Na', 'Gabriel Huang', 'Simon Lacoste-Julien']",http://arxiv.org/pdf/2103.09027v1.pdf,2021-03-16,"['cs.lg', 'cs.cv']","  Model-agnostic meta-learning (MAML) is a popular method for few-shot learningbut assumes that we have access to the meta-training set. In practice, trainingon the meta-training set may not always be an option due to data privacyconcerns, intellectual property issues, or merely lack of computing resources.In this paper, we consider the novel problem of repurposing pretrained MAMLcheckpoints to solve new few-shot classification tasks. Because of thepotential distribution mismatch, the original MAML steps may no longer beoptimal. Therefore we propose an alternative meta-testing procedure and combineMAML gradient steps with adversarial training and uncertainty-based stepsizeadaptation. Our method outperforms ""vanilla"" MAML on same-domain andcross-domains benchmarks using both SGD and Adam optimizers and shows improvedrobustness to the choice of base stepsize."
Improving and Simplifying Pattern Exploiting Training,"['Derek Tam', 'Rakesh R Menon', 'Mohit Bansal', 'Shashank Srivastava', 'Colin Raffel']",http://arxiv.org/pdf/2103.11955v3.pdf,2021-03-22,"['cs.cl', 'cs.ai', 'cs.lg']","  Recently, pre-trained language models (LMs) have achieved strong performancewhen fine-tuned on difficult benchmarks like SuperGLUE. However, performancecan suffer when there are very few labeled examples available for fine-tuning.Pattern Exploiting Training (PET) is a recent approach that leverages patternsfor few-shot learning. However, PET uses task-specific unlabeled data. In thispaper, we focus on few-shot learning without any unlabeled data and introduceADAPET, which modifies PET's objective to provide denser supervision duringfine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without anytask-specific unlabeled data. Our code can be found athttps://github.com/rrmenon10/ADAPET."
Detecting Hate Speech with GPT-3,"['Ke-Li Chiu', 'Annie Collins', 'Rohan Alexander']",http://arxiv.org/pdf/2103.12407v4.pdf,2021-03-23,['cs.cl'],"  Sophisticated language models such as OpenAI's GPT-3 can generate hatefultext that targets marginalized groups. Given this capacity, we are interestedin whether large language models can be used to identify hate speech andclassify text as sexist or racist. We use GPT-3 to identify sexist and racisttext passages with zero-, one-, and few-shot learning. We find that with zero-and one-shot learning, GPT-3 can identify sexist or racist text with an averageaccuracy between 55 per cent and 67 per cent, depending on the category of textand type of learning. With few-shot learning, the model's accuracy can be ashigh as 85 per cent. Large language models have a role to play in hate speechdetection, and with further development they could eventually be used tocounter hate speech."
Head2HeadFS: Video-based Head Reenactment with Few-shot Learning,"['Michail Christos Doukas', 'Mohammad Rami Koujan', 'Viktoriia Sharmanska', 'Stefanos Zafeiriou']",http://arxiv.org/pdf/2103.16229v1.pdf,2021-03-30,['cs.cv'],"  Over the past years, a substantial amount of work has been done on theproblem of facial reenactment, with the solutions coming mainly from thegraphics community. Head reenactment is an even more challenging task, whichaims at transferring not only the facial expression, but also the entire headpose from a source person to a target. Current approaches either trainperson-specific systems, or use facial landmarks to model human heads, arepresentation that might transfer unwanted identity attributes from the sourceto the target. We propose head2headFS, a novel easily adaptable pipeline forhead reenactment. We condition synthesis of the target person on dense 3D faceshape information from the source, which enables high quality expression andpose transfer. Our video-based rendering network is fine-tuned under a few-shotlearning strategy, using only a few samples. This allows for fast adaptation ofa generic generator trained on a multiple-person dataset, into aperson-specific one."
Federated Few-Shot Learning with Adversarial Learning,"['Chenyou Fan', 'Jianwei Huang']",http://arxiv.org/pdf/2104.00365v1.pdf,2021-04-01,"['cs.lg', 'cs.cv']","  We are interested in developing a unified machine learning model over manymobile devices for practical learning tasks, where each device only has veryfew training data. This is a commonly encountered situation in mobile computingscenarios, where data is scarce and distributed while the tasks are distinct.In this paper, we propose a federated few-shot learning (FedFSL) framework tolearn a few-shot classification model that can classify unseen data classeswith only a few labeled samples. With the federated learning strategy, FedFSLcan utilize many data sources while keeping data privacy and communicationefficiency. There are two technical challenges: 1) directly using the existingfederated learning approach may lead to misaligned decision boundaries producedby client models, and 2) constraining the decision boundaries to be similarover clients would overfit to training tasks but not adapt well to unseentasks. To address these issues, we propose to regularize local updates byminimizing the divergence of client models. We also formulate the training inan adversarial fashion and optimize the client models to produce adiscriminative feature space that can better represent unseen data samples. Wedemonstrate the intuitions and conduct experiments to show our approachesoutperform baselines by more than 10% in learning vision tasks and 5% inlanguage tasks."
Pareto Self-Supervised Training for Few-Shot Learning,"['Zhengyu Chen', 'Jixie Ge', 'Heshen Zhan', 'Siteng Huang', 'Donglin Wang']",http://arxiv.org/pdf/2104.07841v2.pdf,2021-04-16,['cs.cv'],"  While few-shot learning (FSL) aims for rapid generalization to new conceptswith little supervision, self-supervised learning (SSL) constructs supervisorysignals directly computed from unlabeled data. Exploiting the complementarityof these two manners, few-shot auxiliary learning has recently drawn muchattention to deal with few labeled data. Previous works benefit from sharinginductive bias between the main task (FSL) and auxiliary tasks (SSL), where theshared parameters of tasks are optimized by minimizing a linear combination oftask losses. However, it is challenging to select a proper weight to balancetasks and reduce task conflict. To handle the problem as a whole, we propose anovel approach named as Pareto self-supervised training (PSST) for FSL. PSSTexplicitly decomposes the few-shot auxiliary problem into multiple constrainedmulti-objective subproblems with different trade-off preferences, and here apreference region in which the main task achieves the best performance isidentified. Then, an effective preferred Pareto exploration is proposed to finda set of optimal solutions in such a preference region. Extensive experimentson several public benchmark datasets validate the effectiveness of our approachby achieving state-of-the-art performance."
CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in  NLP,"['Qinyuan Ye', 'Bill Yuchen Lin', 'Xiang Ren']",http://arxiv.org/pdf/2104.08835v2.pdf,2021-04-18,"['cs.cl', 'cs.lg']","  Humans can learn a new language task efficiently with only few examples, byleveraging their knowledge obtained when learning prior tasks. In this paper,we explore whether and how such cross-task generalization ability can beacquired, and further applied to build better few-shot learners across diverseNLP tasks. We introduce CrossFit, a problem setup for studying cross-taskgeneralization ability, which standardizes seen/unseen task partitions, dataaccess during different learning stages, and the evaluation protocols. Toinstantiate different seen/unseen task partitions in CrossFit and facilitatein-depth analysis, we present the NLP Few-shot Gym, a repository of 160 diversefew-shot NLP tasks created from open-access NLP datasets and converted to aunified text-to-text format. Our analysis reveals that the few-shot learningability on unseen tasks can be improved via an upstream learning stage using aset of seen tasks. We also observe that the selection of upstream learningtasks can significantly influence few-shot performance on unseen tasks, askingfurther analysis on task similarity and transferability."
Few-shot Learning for Topic Modeling,['Tomoharu Iwata'],http://arxiv.org/pdf/2104.09011v1.pdf,2021-04-19,"['cs.cl', 'cs.lg', 'stat.ml']","  Topic models have been successfully used for analyzing text documents.However, with existing topic models, many documents are required for training.In this paper, we propose a neural network-based few-shot learning method thatcan learn a topic model from just a few documents. The neural networks in ourmodel take a small number of documents as inputs, and output topic modelpriors. The proposed method trains the neural networks such that the expectedtest likelihood is improved when topic model parameters are estimated bymaximizing the posterior probability using the priors based on the EMalgorithm. Since each step in the EM algorithm is differentiable, the proposedmethod can backpropagate the loss through the EM algorithm to train the neuralnetworks. The expected test likelihood is maximized by a stochastic gradientdescent method using a set of multiple text corpora with an episodic trainingframework. In our experiments, we demonstrate that the proposed method achievesbetter perplexity than existing methods using three real-world text documentsets."
Non-Parametric Few-Shot Learning for Word Sense Disambiguation,"['Howard Chen', 'Mengzhou Xia', 'Danqi Chen']",http://arxiv.org/pdf/2104.12677v2.pdf,2021-04-26,['cs.cl'],"  Word sense disambiguation (WSD) is a long-standing problem in naturallanguage processing. One significant challenge in supervised all-words WSD isto classify among senses for a majority of words that lie in the long-taildistribution. For instance, 84% of the annotated words have less than 10examples in the SemCor training data. This issue is more pronounced as theimbalance occurs in both word and sense distributions. In this work, we proposeMetricWSD, a non-parametric few-shot learning approach to mitigate this dataimbalance issue. By learning to compute distances among the senses of a givenword through episodic training, MetricWSD transfers knowledge (a learned metricspace) from high-frequency words to infrequent ones. MetricWSD constructs thetraining episodes tailored to word frequencies and explicitly addresses theproblem of the skewed distribution, as opposed to mixing all the words trainedwith parametric models in previous work. Without resorting to any lexicalresources, MetricWSD obtains strong performance against parametricalternatives, achieving a 75.1 F1 score on the unified WSD evaluation benchmark(Raganato et al., 2017b). Our analysis further validates that infrequent wordsand senses enjoy significant improvement."
Entailment as Few-Shot Learner,"['Sinong Wang', 'Han Fang', 'Madian Khabsa', 'Hanzi Mao', 'Hao Ma']",http://arxiv.org/pdf/2104.14690v1.pdf,2021-04-29,"['cs.cl', 'cs.ai']","  Large pre-trained language models (LMs) have demonstrated remarkable abilityas few-shot learners. However, their success hinges largely on scaling modelparameters to a degree that makes it challenging to train and serve. In thispaper, we propose a new approach, named as EFL, that can turn small LMs intobetter few-shot learners. The key idea of this approach is to reformulatepotential NLP task into an entailment one, and then fine-tune the model with aslittle as 8 examples. We further demonstrate our proposed method can be: (i)naturally combined with an unsupervised contrastive learning-based dataaugmentation method; (ii) easily extended to multilingual few-shot learning. Asystematic evaluation on 18 standard NLP tasks demonstrates that this approachimproves the various existing SOTA few-shot learning methods by 12\%, andyields competitive few-shot performance with 500 times larger models, such asGPT-3."
Exploring the Similarity of Representations in Model-Agnostic  Meta-Learning,"['Thomas Goerttler', 'Klaus Obermayer']",http://arxiv.org/pdf/2105.05757v1.pdf,2021-05-12,"['cs.lg', 'cs.ai', 'cs.ne', 'stat.ml']","  In past years model-agnostic meta-learning (MAML) has been one of the mostpromising approaches in meta-learning. It can be applied to different kinds ofproblems, e.g., reinforcement learning, but also shows good results on few-shotlearning tasks. Besides their tremendous success in these tasks, it has stillnot been fully revealed yet, why it works so well. Recent work proposes thatMAML rather reuses features than rapidly learns. In this paper, we want toinspire a deeper understanding of this question by analyzing MAML'srepresentation. We apply representation similarity analysis (RSA), awell-established method in neuroscience, to the few-shot learning instantiationof MAML. Although some part of our analysis supports their general results thatfeature reuse is predominant, we also reveal arguments against theirconclusion. The similarity-increase of layers closer to the input layers arisesfrom the learning task itself and not from the model. In addition, therepresentations after inner gradient steps make a broader change to therepresentation than the changes during meta-training."
Aligning Visual Prototypes with BERT Embeddings for Few-Shot Learning,"['Kun Yan', 'Zied Bouraoui', 'Ping Wang', 'Shoaib Jameel', 'Steven Schockaert']",http://arxiv.org/pdf/2105.10195v1.pdf,2021-05-21,['cs.cv'],"  Few-shot learning (FSL) is the task of learning to recognize previouslyunseen categories of images from a small number of training examples. This is achallenging task, as the available examples may not be enough to unambiguouslydetermine which visual features are most characteristic of the consideredcategories. To alleviate this issue, we propose a method that additionallytakes into account the names of the image classes. While the use of class nameshas already been explored in previous work, our approach differs in two keyaspects. First, while previous work has aimed to directly predict visualprototypes from word embeddings, we found that better results can be obtainedby treating visual and text-based prototypes separately. Second, we propose asimple strategy for learning class name embeddings using the BERT languagemodel, which we found to substantially outperform the GloVe vectors that wereused in previous work. We furthermore propose a strategy for dealing with thehigh dimensionality of these vectors, inspired by models for aligningcross-lingual word embeddings. We provide experiments on miniImageNet, CUB andtieredImageNet, showing that our approach consistently improves thestate-of-the-art in metric-based FSL."
True Few-Shot Learning with Language Models,"['Ethan Perez', 'Douwe Kiela', 'Kyunghyun Cho']",http://arxiv.org/pdf/2105.11447v1.pdf,2021-05-24,"['cs.cl', 'cs.lg', 'stat.ml']","  Pretrained language models (LMs) perform well on many tasks even whenlearning from a few examples, but prior work uses many held-out examples totune various aspects of learning, such as hyperparameters, training objectives,and natural language templates (""prompts""). Here, we evaluate the few-shotability of LMs when such held-out examples are unavailable, a setting we calltrue few-shot learning. We test two model selection criteria, cross-validationand minimum description length, for choosing LM prompts and hyperparameters inthe true few-shot setting. On average, both marginally outperform randomselection and greatly underperform selection based on held-out examples.Moreover, selection criteria often prefer models that perform significantlyworse than randomly-selected ones. We find similar results even when takinginto account our uncertainty in a model's true performance during selection, aswell as when varying the amount of computation and number of examples used forselection. Overall, our findings suggest that prior work significantlyoverestimated the true few-shot ability of LMs given the difficulty of few-shotmodel selection."
Improving Few-shot Learning with Weakly-supervised Object Localization,"['Inyong Koo', 'Minki Jeong', 'Changick Kim']",http://arxiv.org/pdf/2105.11715v1.pdf,2021-05-25,"['cs.cv', 'i.4.10']","  Few-shot learning often involves metric learning-based classifiers, whichpredict the image label by comparing the distance between the extracted featurevector and class representations. However, applying global pooling in thebackend of the feature extractor may not produce an embedding that correctlyfocuses on the class object. In this work, we propose a novel framework thatgenerates class representations by extracting features from class-relevantregions of the images. Given only a few exemplary images with image-levellabels, our framework first localizes the class objects by spatiallydecomposing the similarity between the images and their class prototypes. Then,enhanced class representations are achieved from the localization results. Wealso propose a loss function to enhance distinctions of the refined features.Our method outperforms the baseline few-shot model in miniImageNet andtieredImageNet benchmarks."
Few-Shot Learning with Part Discovery and Augmentation from Unlabeled  Images,"['Wentao Chen', 'Chenyang Si', 'Wei Wang', 'Liang Wang', 'Zilei Wang', 'Tieniu Tan']",http://arxiv.org/pdf/2105.11874v1.pdf,2021-05-25,['cs.cv'],"  Few-shot learning is a challenging task since only few instances are givenfor recognizing an unseen class. One way to alleviate this problem is toacquire a strong inductive bias via meta-learning on similar tasks. In thispaper, we show that such inductive bias can be learned from a flat collectionof unlabeled images, and instantiated as transferable representations amongseen and unseen classes. Specifically, we propose a novel part-basedself-supervised representation learning scheme to learn transferablerepresentations by maximizing the similarity of an image to its discriminativepart. To mitigate the overfitting in few-shot classification caused by datascarcity, we further propose a part augmentation strategy by retrieving extraimages from a base dataset. We conduct systematic studies on miniImageNet andtieredImageNet benchmarks. Remarkably, our method yields impressive results,outperforming the previous best unsupervised methods by 7.74% and 9.24% under5-way 1-shot and 5-way 5-shot settings, which are comparable withstate-of-the-art supervised methods."
Ensemble Making Few-Shot Learning Stronger,"['Qing Lin', 'Yongbin Liu', 'Wen Wen', 'Zhihua Tao']",http://arxiv.org/pdf/2105.11904v1.pdf,2021-05-12,"['cs.cl', 'cs.ai', 'cs.lg']","  Few-shot learning has been proposed and rapidly emerging as a viable meansfor completing various tasks. Many few-shot models have been widely used forrelation learning tasks. However, each of these models has a shortage ofcapturing a certain aspect of semantic features, for example, CNN on long-rangedependencies part, Transformer on local features. It is difficult for a singlemodel to adapt to various relation learning, which results in the high varianceproblem. Ensemble strategy could be competitive on improving the accuracy offew-shot relation extraction and mitigating high variance risks. This paperexplores an ensemble approach to reduce the variance and introduces fine-tuningand feature attention strategies to calibrate relation-level features. Resultson several few-shot relation learning tasks show that our model significantlyoutperforms the previous state-of-the-art models."
Bridging the Gap Between Practice and PAC-Bayes Theory in Few-Shot  Meta-Learning,"['Nan Ding', 'Xi Chen', 'Tomer Levinboim', 'Sebastian Goodman', 'Radu Soricut']",http://arxiv.org/pdf/2105.14099v2.pdf,2021-05-28,"['cs.lg', 'stat.ml']","  Despite recent advances in its theoretical understanding, there still remainsa significant gap in the ability of existing PAC-Bayesian theories onmeta-learning to explain performance improvements in the few-shot learningsetting, where the number of training examples in the target tasks is severelylimited. This gap originates from an assumption in the existing theories whichsupposes that the number of training examples in the observed tasks and thenumber of training examples in the target tasks follow the same distribution,an assumption that rarely holds in practice. By relaxing this assumption, wedevelop two PAC-Bayesian bounds tailored for the few-shot learning setting andshow that two existing meta-learning algorithms (MAML and Reptile) can bederived from our bounds, thereby bridging the gap between practice andPAC-Bayesian theories. Furthermore, we derive a new computationally-efficientPACMAML algorithm, and show it outperforms existing meta-learning algorithms onseveral few-shot benchmark datasets."
"Generate, Annotate, and Learn: NLP with Synthetic Text","['Xuanli He', 'Islam Nassar', 'Jamie Kiros', 'Gholamreza Haffari', 'Mohammad Norouzi']",http://arxiv.org/pdf/2106.06168v3.pdf,2021-06-11,['cs.lg'],"  This paper studies the use of language models as a source of syntheticunlabeled text for NLP. We formulate a general framework called ``generate,annotate, and learn (GAL)'' to take advantage of synthetic text withinknowledge distillation, self-training, and few-shot learning applications. Togenerate high-quality task-specific text, we either fine-tune LMs on inputsfrom the task of interest, or prompt large LMs with few examples. We use thebest available classifier to annotate synthetic text with soft pseudo labelsfor knowledge distillation and self-training, and use LMs to obtain hard labelsfor few-shot learning. We train new supervised models on the combination oflabeled and pseudo-labeled data, which results in significant gains acrossseveral applications. We investigate key components of GAL and presenttheoretical and empirical arguments against the use of class-conditional LMs togenerate synthetic labeled text instead of unlabeled text. GAL achieves newstate-of-the-art knowledge distillation results for 6-layer transformers on theGLUE leaderboard."
Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent  Detection and Slot Filling,"['Yutai Hou', 'Yongkui Lai', 'Cheng Chen', 'Wanxiang Che', 'Ting Liu']",http://arxiv.org/pdf/2106.07343v1.pdf,2021-05-25,"['cs.cl', 'cs.ai']","  In this paper, we investigate few-shot joint learning for dialogue languageunderstanding. Most existing few-shot models learn a single task each time withonly a few examples. However, dialogue language understanding contains twoclosely related tasks, i.e., intent detection and slot filling, and oftenbenefits from jointly learning the two tasks. This calls for new few-shotlearning techniques that are able to capture task relations from only a fewexamples and jointly learn multiple tasks. To achieve this, we propose asimilarity-based few-shot learning scheme, named Contrastive Prototype Mergingnetwork (ConProm), that learns to bridge metric spaces of intent and slot ondata-rich domains, and then adapt the bridged metric space to the specificfew-shot domain. Experiments on two public datasets, Snips and FewJoint, showthat our model significantly outperforms the strong baselines in one and fiveshots settings."
Task Attended Meta-Learning for Few-Shot Learning,"['Aroof Aimen', 'Sahil Sidheekh', 'Narayanan C. Krishnan']",http://arxiv.org/pdf/2106.10642v1.pdf,2021-06-20,"['cs.lg', 'cs.ai', 'cs.cv']","  Meta-learning (ML) has emerged as a promising direction in learning modelsunder constrained resource settings like few-shot learning. The popularapproaches for ML either learn a generalizable initial model or a genericparametric optimizer through episodic training. The former approaches leveragethe knowledge from a batch of tasks to learn an optimal prior. In this work, westudy the importance of a batch for ML. Specifically, we first incorporate abatch episodic training regimen to improve the learning of the genericparametric optimizer. We also hypothesize that the common assumption in batchepisodic training that each task in a batch has an equal contribution tolearning an optimal meta-model need not be true. We propose to weight the tasksin a batch according to their ""importance"" in improving the meta-model'slearning. To this end, we introduce a training curriculum motivated byselective focus in humans, called task attended meta-training, to weight thetasks in a batch. Task attention is a standalone module that can be integratedwith any batch episodic training regimen. The comparisons of the models withtheir non-task-attended counterparts on complex datasets like miniImageNet andtieredImageNet validate its effectiveness."
Multimodal Few-Shot Learning with Frozen Language Models,"['Maria Tsimpoukelli', 'Jacob Menick', 'Serkan Cabi', 'S. M. Ali Eslami', 'Oriol Vinyals', 'Felix Hill']",http://arxiv.org/pdf/2106.13884v2.pdf,2021-06-25,"['cs.cv', 'cs.cl', 'cs.lg']","  When trained at sufficient scale, auto-regressive language models exhibit thenotable ability to learn a new language task after being prompted with just afew examples. Here, we present a simple, yet effective, approach fortransferring this few-shot learning ability to a multimodal setting (vision andlanguage). Using aligned image and caption data, we train a vision encoder torepresent each image as a sequence of continuous embeddings, such that apre-trained, frozen language model prompted with this prefix generates theappropriate caption. The resulting system is a multimodal few-shot learner,with the surprising ability to learn a variety of new tasks when conditioned onexamples, represented as a sequence of multiple interleaved image and textembeddings. We demonstrate that it can rapidly learn words for new objects andnovel visual categories, do visual question-answering with only a handful ofexamples, and make use of outside knowledge, by measuring a single model on avariety of established and new benchmarks."
Few-shot Learning for Unsupervised Feature Selection,"['Atsutoshi Kumagai', 'Tomoharu Iwata', 'Yasuhiro Fujiwara']",http://arxiv.org/pdf/2107.00816v1.pdf,2021-07-02,"['cs.lg', 'stat.ml']","  We propose a few-shot learning method for unsupervised feature selection,which is a task to select a subset of relevant features in unlabeled data.Existing methods usually require many instances for feature selection. However,sufficient instances are often unavailable in practice. The proposed method canselect a subset of relevant features in a target task given a few unlabeledtarget instances by training with unlabeled instances in multiple source tasks.Our model consists of a feature selector and decoder. The feature selectoroutputs a subset of relevant features taking a few unlabeled instances as inputsuch that the decoder can reconstruct the original features of unseen instancesfrom the selected ones. The feature selector uses the Concrete random variablesto select features via gradient descent. To encode task-specific propertiesfrom a few unlabeled instances to the model, the Concrete random variables anddecoder are modeled using permutation-invariant neural networks that take a fewunlabeled instances as input. Our model is trained by minimizing the expectedtest reconstruction error given a few unlabeled instances that is calculatedwith datasets in source tasks. We experimentally demonstrate that the proposedmethod outperforms existing feature selection methods."
Learn from Anywhere: Rethinking Generalized Zero-Shot Learning with  Limited Supervision,"['Gaurav Bhatt', 'Shivam Chandhok', 'Vineeth N Balasubramanian']",http://arxiv.org/pdf/2107.04952v2.pdf,2021-07-11,"['cs.cv', 'cs.ai', 'cs.lg']","  A common problem with most zero and few-shot learning approaches is theysuffer from bias towards seen classes resulting in sub-optimal performance.Existing efforts aim to utilize unlabeled images from unseen classes (i.etransductive zero-shot) during training to enable generalization. However, thislimits their use in practical scenarios where data from target unseen classesis unavailable or infeasible to collect. In this work, we present a practicalsetting of inductive zero and few-shot learning, where unlabeled images fromother out-of-data classes, that do not belong to seen or unseen categories, canbe used to improve generalization in any-shot learning. We leverage aformulation based on product-of-experts and introduce a new AUD module thatenables us to use unlabeled samples from out-of-data classes which are usuallyeasily available and practically entail no annotation cost. In addition, wealso demonstrate the applicability of our model to address a more practical andchallenging, Generalized Zero-shot under a limited supervision setting, whereeven base seen classes do not have sufficient annotated samples."
Rectifying the Shortcut Learning of Background for Few-Shot Learning,"['Xu Luo', 'Longhui Wei', 'Liangjian Wen', 'Jinrong Yang', 'Lingxi Xie', 'Zenglin Xu', 'Qi Tian']",http://arxiv.org/pdf/2107.07746v3.pdf,2021-07-16,['cs.cv'],"  The category gap between training and evaluation has been characterised asone of the main obstacles to the success of Few-Shot Learning (FSL). In thispaper, we for the first time empirically identify image background, common inrealistic images, as a shortcut knowledge helpful for in-class classificationbut ungeneralizable beyond training categories in FSL. A novel framework,COSOC, is designed to tackle this problem by extracting foreground objects inimages at both training and evaluation without any extra supervision. Extensiveexperiments carried on inductive FSL tasks demonstrate the effectiveness of ourapproaches."
Deep Metric Learning for Open World Semantic Segmentation,"['Jun Cen', 'Peng Yun', 'Junhao Cai', 'Michael Yu Wang', 'Ming Liu']",http://arxiv.org/pdf/2108.04562v1.pdf,2021-08-10,['cs.cv'],"  Classical close-set semantic segmentation networks have limited ability todetect out-of-distribution (OOD) objects, which is important forsafety-critical applications such as autonomous driving. Incrementally learningthese OOD objects with few annotations is an ideal way to enlarge the knowledgebase of the deep learning models. In this paper, we propose an open worldsemantic segmentation system that includes two modules: (1) an open-setsemantic segmentation module to detect both in-distribution and OOD objects.(2) an incremental few-shot learning module to gradually incorporate those OODobjects into its existing knowledge base. This open world semantic segmentationsystem behaves like a human being, which is able to identify OOD objects andgradually learn them with corresponding supervision. We adopt the Deep MetricLearning Network (DMLNet) with contrastive clustering to implement open-setsemantic segmentation. Compared to other open-set semantic segmentationmethods, our DMLNet achieves state-of-the-art performance on three challengingopen-set semantic segmentation datasets without using additional data orgenerative models. On this basis, two incremental few-shot learning methods arefurther proposed to progressively improve the DMLNet with the annotations ofOOD objects."
Adaptive Few-Shot Learning PoC Ultrasound COVID-19 Diagnostic System,"['Michael Karnes', 'Shehan Perera', 'Srikar Adhikari', 'Alper Yilmaz']",http://arxiv.org/pdf/2109.03793v1.pdf,2021-09-08,"['eess.iv', 'cs.cv']","  This paper presents a novel ultrasound imaging point-of-care (PoC) COVID-19diagnostic system. The adaptive visual diagnostics utilize few-shot learning(FSL) to generate encoded disease state models that are stored and classifiedusing a dictionary of knowns. The novel vocabulary based feature processing ofthe pipeline adapts the knowledge of a pretrained deep neural network tocompress the ultrasound images into discrimative descriptions. Thecomputational efficiency of the FSL approach enables high diagnostic deeplearning performance in PoC settings, where training data is limited and theannotation process is not strictly controlled. The algorithm performance isevaluated on the open source COVID-19 POCUS Dataset to validate the system'sability to distinguish COVID-19, pneumonia, and healthy disease states. Theresults of the empirical analyses demonstrate the appropriate efficiency andaccuracy for scalable PoC use. The code for this work will be made publiclyavailable on GitHub upon acceptance."
Learning from Language Description: Low-shot Named Entity Recognition  via Decomposed Framework,"['Yaqing Wang', 'Haoda Chu', 'Chao Zhang', 'Jing Gao']",http://arxiv.org/pdf/2109.05357v1.pdf,2021-09-11,['cs.cl'],"  In this work, we study the problem of named entity recognition (NER) in a lowresource scenario, focusing on few-shot and zero-shot settings. Built uponlarge-scale pre-trained language models, we propose a novel NER framework,namely SpanNER, which learns from natural language supervision and enables theidentification of never-seen entity classes without using in-domain labeleddata. We perform extensive experiments on 5 benchmark datasets and evaluate theproposed method in the few-shot learning, domain transfer and zero-shotlearning settings. The experimental results show that the proposed method canbring 10%, 23% and 26% improvements in average over the best baselines infew-shot learning, domain transfer and zero-shot learning settingsrespectively."
Partner-Assisted Learning for Few-Shot Image Classification,"['Jiawei Ma', 'Hanchen Xie', 'Guangxing Han', 'Shih-Fu Chang', 'Aram Galstyan', 'Wael Abd-Almageed']",http://arxiv.org/pdf/2109.07607v1.pdf,2021-09-15,['cs.cv'],"  Few-shot Learning has been studied to mimic human visual capabilities andlearn effective models without the need of exhaustive human annotation. Eventhough the idea of meta-learning for adaptation has dominated the few-shotlearning methods, how to train a feature extractor is still a challenge. Inthis paper, we focus on the design of training strategy to obtain an elementalrepresentation such that the prototype of each novel class can be estimatedfrom a few labeled samples. We propose a two-stage training scheme,Partner-Assisted Learning (PAL), which first trains a partner encoder to modelpair-wise similarities and extract features serving as soft-anchors, and thentrains a main encoder by aligning its outputs with soft-anchors whileattempting to maximize classification performance. Two alignment constraintsfrom logit-level and feature-level are designed individually. For each few-shottask, we perform prototype classification. Our method consistently outperformsthe state-of-the-art method on four benchmarks. Detailed ablation studies ofPAL are provided to justify the selection of each component involved intraining."
Multi-Domain Few-Shot Learning and Dataset for Agricultural Applications,"['Sai Vidyaranya Nuthalapati', 'Anirudh Tunga']",http://arxiv.org/pdf/2109.09952v1.pdf,2021-09-21,['cs.cv'],"  Automatic classification of pests and plants (both healthy and diseased) isof paramount importance in agriculture to improve yield. Conventional deeplearning models based on convolutional neural networks require thousands oflabeled examples per category. In this work we propose a method to learn from afew samples to automatically classify different pests, plants, and theirdiseases, using Few-Shot Learning (FSL). We learn a feature extractor togenerate embeddings and then update the embeddings using Transformers. UsingMahalanobis distance, a class-covariance-based metric, we then calculate thesimilarity of the transformed embeddings with the embedding of the image to beclassified. Using our proposed architecture, we conduct extensive experimentson multiple datasets showing the effectiveness of our proposed model. Weconduct 42 experiments in total to comprehensively analyze the model and itachieves up to 14% and 24% performance gains on few-shot image classificationbenchmarks on two datasets.  We also compile a new FSL dataset containing images of healthy and diseasedplants taken in real-world settings. Using our proposed architecture which hasbeen shown to outperform several existing FSL architectures in agriculture, weprovide strong baselines on our newly proposed dataset."
Revisiting Self-Training for Few-Shot Learning of Language Model,"['Yiming Chen', 'Yan Zhang', 'Chen Zhang', 'Grandee Lee', 'Ran Cheng', 'Haizhou Li']",http://arxiv.org/pdf/2110.01256v1.pdf,2021-10-04,['cs.cl'],"  As unlabeled data carry rich task-relevant information, they are provenuseful for few-shot learning of language model. The question is how toeffectively make use of such data. In this work, we revisit the self-trainingtechnique for language model fine-tuning and present a state-of-the-artprompt-based few-shot learner, SFLM. Given two views of a text sample via weakand strong augmentation techniques, SFLM generates a pseudo label on the weaklyaugmented version. Then, the model predicts the same pseudo label whenfine-tuned with the strongly augmented version. This simple approach is shownto outperform other state-of-the-art supervised and semi-supervisedcounterparts on six sentence classification and six sentence-pairclassification benchmarking tasks. In addition, SFLM only relies on a fewin-domain unlabeled data. We conduct a comprehensive analysis to demonstratethe robustness of our proposed approach under various settings, includingaugmentation techniques, model scale, and few-shot knowledge transfer acrosstasks."
Task Affinity with Maximum Bipartite Matching in Few-Shot Learning,"['Cat P. Le', 'Juncheng Dong', 'Mohammadreza Soltani', 'Vahid Tarokh']",http://arxiv.org/pdf/2110.02399v2.pdf,2021-10-05,"['cs.lg', 'cs.cv']","  We propose an asymmetric affinity score for representing the complexity ofutilizing the knowledge of one task for learning another one. Our method isbased on the maximum bipartite matching algorithm and utilizes the FisherInformation matrix. We provide theoretical analyses demonstrating that theproposed score is mathematically well-defined, and subsequently use theaffinity score to propose a novel algorithm for the few-shot learning problem.In particular, using this score, we find relevant training data labels to thetest data and leverage the discovered relevant data for episodicallyfine-tuning a few-shot model. Results on various few-shot benchmark datasetsdemonstrate the efficacy of the proposed approach by improving theclassification accuracy over the state-of-the-art methods even when usingsmaller models."
Injecting Text and Cross-lingual Supervision in Few-shot Learning from  Self-Supervised Models,"['Matthew Wiesner', 'Desh Raj', 'Sanjeev Khudanpur']",http://arxiv.org/pdf/2110.04863v1.pdf,2021-10-10,"['eess.as', 'cs.cl']","  Self-supervised model pre-training has recently garnered significantinterest, but relatively few efforts have explored using additional resourcesin fine-tuning these models. We demonstrate how universal phoneset acousticmodels can leverage cross-lingual supervision to improve transfer of pretrainedself-supervised representations to new languages. We also show howtarget-language text can be used to enable and improve fine-tuning with thelattice-free maximum mutual information (LF-MMI) objective. In threelow-resource languages these techniques greatly improved few-shot learningperformance."
Ortho-Shot: Low Displacement Rank Regularization with Data Augmentation  for Few-Shot Learning,"['Uche Osahor', 'Nasser M. Nasrabadi']",http://arxiv.org/pdf/2110.09374v1.pdf,2021-10-18,"['cs.cv', 'cs.lg', 'eess.iv']","  In few-shot classification, the primary goal is to learn representations froma few samples that generalize well for novel classes. In this paper, we proposean efficient low displacement rank (LDR) regularization strategy termedOrtho-Shot; a technique that imposes orthogonal regularization on theconvolutional layers of a few-shot classifier, which is based on thedoubly-block toeplitz (DBT) matrix structure. The regularized convolutionallayers of the few-shot classifier enhances model generalization and intra-classfeature embeddings that are crucial for few-shot learning. Overfitting is atypical issue for few-shot models, the lack of data diversity inhibits propermodel inference which weakens the classification accuracy of few-shot learnersto novel classes. In this regard, we broke down the pipeline of the few-shotclassifier and established that the support, query and task data augmentationcollectively alleviates overfitting in networks. With compelling results, wedemonstrated that combining a DBT-based low-rank orthogonal regularizer withdata augmentation strategies, significantly boosts the performance of afew-shot classifier. We perform our experiments on the miniImagenet, CIFAR-FSand Stanford datasets with performance values of about 5\% when compared tostate-of-the-art"
Squeezing Backbone Feature Distributions to the Max for Efficient  Few-Shot Learning,"['Yuqing Hu', 'Vincent Gripon', 'Stéphane Pateux']",http://arxiv.org/pdf/2110.09446v1.pdf,2021-10-18,['cs.lg'],"  Few-shot classification is a challenging problem due to the uncertaintycaused by using few labelled samples. In the past few years, many methods havebeen proposed with the common aim of transferring knowledge acquired on apreviously solved task, what is often achieved by using a pretrained featureextractor. Following this vein, in this paper we propose a novel transfer-basedmethod which aims at processing the feature vectors so that they become closerto Gaussian-like distributions, resulting in increased accuracy. In the case oftransductive few-shot learning where unlabelled test samples are availableduring training, we also introduce an optimal-transport inspired algorithm toboost even further the achieved performance. Using standardized visionbenchmarks, we show the ability of the proposed methodology to achievestate-of-the-art accuracy with various datasets, backbone architectures andfew-shot settings."
GCCN: Global Context Convolutional Network,"['Ali Hamdi', 'Flora Salim', 'Du Yong Kim']",http://arxiv.org/pdf/2110.11664v1.pdf,2021-10-22,['cs.cv'],"  In this paper, we propose Global Context Convolutional Network (GCCN) forvisual recognition. GCCN computes global features representing contextualinformation across image patches. These global contextual features are definedas local maxima pixels with high visual sharpness in each patch. These featuresare then concatenated and utilised to augment the convolutional features. Thelearnt feature vector is normalised using the global context features usingFrobenius norm. This straightforward approach achieves high accuracy incompassion to the state-of-the-art methods with 94.6% and 95.41% on CIFAR-10and STL-10 datasets, respectively. To explore potential impact of GCCN on othervisual representation tasks, we implemented GCCN as a based model to few-shotimage classification. We learn metric distances between the augmented featurevectors and their prototypes representations, similar to Prototypical andMatching Networks. GCCN outperforms state-of-the-art few-shot learning methodsachieving 99.9%, 84.8% and 80.74% on Omniglot, MiniImageNet and CUB-200,respectively. GCCN has significantly improved on the accuracy ofstate-of-the-art prototypical and matching networks by up to 30% in differentfew-shot learning scenarios."
SCHA-VAE: Hierarchical Context Aggregation for Few-Shot Generation,"['Giorgio Giannone', 'Ole Winther']",http://arxiv.org/pdf/2110.12279v3.pdf,2021-10-23,"['cs.lg', 'cs.ai', 'stat.ml']","  A few-shot generative model should be able to generate data from a noveldistribution by only observing a limited set of examples. In few-shot learningthe model is trained on data from many sets from distributions sharing someunderlying properties such as sets of characters from different alphabets orobjects from different categories. We extend current latent variable models forsets to a fully hierarchical approach with an attention-based point toset-level aggregation and call our method SCHA-VAE forSet-Context-Hierarchical-Aggregation Variational Autoencoder. We explorelikelihood-based model comparison, iterative data sampling, and adaptation-freeout-of-distribution generalization. Our results show that the hierarchicalformulation better captures the intrinsic variability within the sets in thesmall data regime. This work generalizes deep latent variable approaches tofew-shot learning, taking a step toward large-scale few-shot generation with aformulation that readily works with current state-of-the-art deep generativemodels."
Few-shot learning with improved local representations via bias rectify  module,"['Chao Dong', 'Qi Ye', 'Wenchao Meng', 'Kaixiang Yang']",http://arxiv.org/pdf/2111.00754v1.pdf,2021-11-01,['cs.cv'],"  Recent approaches based on metric learning have achieved great progress infew-shot learning. However, most of them are limited to image-levelrepresentation manners, which fail to properly deal with the intra-classvariations and spatial knowledge and thus produce undesirable performance. Inthis paper we propose a Deep Bias Rectify Network (DBRN) to fully exploit thespatial information that exists in the structure of the featurerepresentations. We first employ a bias rectify module to alleviate the adverseimpact caused by the intra-class variations. bias rectify module is able tofocus on the features that are more discriminative for classification by givendifferent weights. To make full use of the training data, we design a prototypeaugment mechanism that can make the prototypes generated from the support setto be more representative. To validate the effectiveness of our method, weconducted extensive experiments on various popular few-shot classificationbenchmarks and our methods can outperform state-of-the-art methods."
Enhancing Prototypical Few-Shot Learning by Leveraging the Local-Level  Strategy,"['Junying Huang', 'Fan Chen', 'Keze Wang', 'Liang Lin', 'Dongyu Zhang']",http://arxiv.org/pdf/2111.04331v1.pdf,2021-11-08,['cs.cv'],"  Aiming at recognizing the samples from novel categories with few referencesamples, few-shot learning (FSL) is a challenging problem. We found that theexisting works often build their few-shot model based on the image-levelfeature by mixing all local-level features, which leads to the discriminativelocation bias and information loss in local details. To tackle the problem,this paper returns the perspective to the local-level feature and proposes aseries of local-level strategies. Specifically, we present (a) a local-agnostictraining strategy to avoid the discriminative location bias between the baseand novel categories, (b) a novel local-level similarity measure to capture theaccurate comparison between local-level features, and (c) a local-levelknowledge transfer that can synthesize different knowledge transfers from thebase category according to different location features. Extensive experimentsjustify that our proposed local-level strategies can significantly boost theperformance and achieve 2.8%-7.2% improvements over the baseline acrossdifferent benchmark datasets, which also achieves state-of-the-art accuracy."
Zero-Shot Learning in Named-Entity Recognition with External Knowledge,"['Nguyen Van Hoang', 'Soeren Hougaard Mulvad', 'Dexter Neo Yuan Rong', 'Yang Yue']",http://arxiv.org/pdf/2111.07734v1.pdf,2021-11-15,['cs.ai'],"  A significant shortcoming of current state-of-the-art (SOTA) named-entityrecognition (NER) systems is their lack of generalization to unseen domains,which poses a major problem since obtaining labeled data for NER in a newdomain is expensive and time-consuming. We propose ZERO, a model that performszero-shot and few-shot learning in NER to generalize to unseen domains byincorporating pre-existing knowledge in the form of semantic word embeddings.ZERO first obtains contextualized word representations of input sentences usingthe model LUKE, reduces their dimensionality, and compares them directly withthe embeddings of the external knowledge, allowing ZERO to be trained torecognize unseen output entities. We find that ZERO performs well on unseen NERdomains with an average macro F1 score of 0.23, outperforms LUKE in few-shotlearning, and even achieves competitive scores on an in-domain comparison. Theperformance across source-target domain pairs is shown to be inverselycorrelated with the pairs' KL divergence."
Few-shot Multi-hop Question Answering over Knowledge Base,"['Meihao Fan', 'Lei Zhang', 'Siyao Xiao', 'Yuru Liang']",http://arxiv.org/pdf/2112.11909v2.pdf,2021-12-14,"['cs.cl', 'cs.ai']","  KBQA is a task that requires to answer questions by using semantic structuredinformation in knowledge base. Previous work in this area has been restricteddue to the lack of large semantic parsing dataset and the exponential growth ofsearching space with the increasing hops of relation paths. In this paper, wepropose an efficient pipeline method equipped with a pre-trained languagemodel. By adopting Beam Search algorithm, the searching space will not berestricted in subgraph of 3 hops. Besides, we propose a data generationstrategy, which enables our model to generalize well from few training samples.We evaluate our model on an open-domain complex Chinese Question Answering taskCCKS2019 and achieve F1-score of 62.55% on the test dataset. In addition, inorder to test the few-shot learning capability of our model, we ramdomly select10% of the primary data to train our model, the result shows that our model canstill achieves F1-score of 58.54%, which verifies the capability of our modelto process KBQA task and the advantage in few-shot Learning."
Semantics-driven Attentive Few-shot Learning over Clean and Noisy  Samples,"['Orhun Buğra Baran', 'Ramazan Gökberk Cinbiş']",http://arxiv.org/pdf/2201.03043v2.pdf,2022-01-09,['cs.cv'],"  Over the last couple of years few-shot learning (FSL) has attracted greatattention towards minimizing the dependency on labeled training examples. Aninherent difficulty in FSL is the handling of ambiguities resulting from havingtoo few training samples per class. To tackle this fundamental challenge inFSL, we aim to train meta-learner models that can leverage prior semanticknowledge about novel classes to guide the classifier synthesis process. Inparticular, we propose semantically-conditioned feature attention and sampleattention mechanisms that estimate the importance of representation dimensionsand training instances. We also study the problem of sample noise in FSL,towards the utilization of meta-learners in more realistic and imperfectsettings. Our experimental results demonstrate the effectiveness of theproposed semantic FSL model with and without sample noise."
HyperTransformer: Model Generation for Supervised and Semi-Supervised  Few-Shot Learning,"['Andrey Zhmoginov', 'Mark Sandler', 'Max Vladymyrov']",http://arxiv.org/pdf/2201.04182v3.pdf,2022-01-11,"['cs.lg', 'cs.cv']","  In this work we propose a HyperTransformer, a Transformer-based model forsupervised and semi-supervised few-shot learning that generates weights of aconvolutional neural network (CNN) directly from support samples. Since thedependence of a small generated CNN model on a specific task is encoded by ahigh-capacity Transformer model, we effectively decouple the complexity of thelarge task space from the complexity of individual tasks. Our method isparticularly effective for small target CNN architectures where learning afixed universal task-independent embedding is not optimal and betterperformance is attained when the information about the task can modulate allmodel parameters. For larger models we discover that generating the last layeralone allows us to produce competitive or better results than those obtainedwith state-of-the-art methods while being end-to-end differentiable."
Semantically Proportional Patchmix for Few-Shot Learning,"['Jingquan Wang', 'Jing Xu', 'Yu Pan', 'Zenglin Xu']",http://arxiv.org/pdf/2202.08647v1.pdf,2022-02-17,['cs.cv'],"  Few-shot learning aims to classify unseen classes with only a limited numberof labeled data. Recent works have demonstrated that training models with asimple transfer learning strategy can achieve competitive results in few-shotclassification. Although excelling at distinguishing training data, thesemodels are not well generalized to unseen data, probably due to insufficientfeature representations on evaluation. To tackle this issue, we proposeSemantically Proportional Patchmix (SePPMix), in which patches are cut andpasted among training images and the ground truth labels are mixedproportionally to the semantic information of the patches. In this way, we canimprove the generalization ability of the model by regional dropout effectwithout introducing severe label noise. To learn more robust representations ofdata, we further take rotate transformation on the mixed images and predictrotations as a rule-based regularizer. Extensive experiments on prevalentfew-shot benchmarks have shown the effectiveness of our proposed method."
How Well Do Self-Supervised Methods Perform in Cross-Domain Few-Shot  Learning?,"['Yiyi Zhang', 'Ying Zheng', 'Xiaogang Xu', 'Jun Wang']",http://arxiv.org/pdf/2202.09014v1.pdf,2022-02-18,['cs.cv'],"  Cross-domain few-shot learning (CDFSL) remains a largely unsolved problem inthe area of computer vision, while self-supervised learning presents apromising solution. Both learning methods attempt to alleviate the dependencyof deep networks on the requirement of large-scale labeled data. Althoughself-supervised methods have recently advanced dramatically, their utility onCDFSL is relatively unexplored. In this paper, we investigate the role ofself-supervised representation learning in the context of CDFSL via a thoroughevaluation of existing methods. It comes as a surprise that even with shallowarchitectures or small training datasets, self-supervised methods can performfavorably compared to the existing SOTA methods. Nevertheless, no singleself-supervised approach dominates all datasets indicating that existingself-supervised methods are not universally applicable. In addition, we findthat representations extracted from self-supervised methods exhibit strongerrobustness than the supervised method. Intriguingly, whether self-supervisedrepresentations perform well on the source domain has little correlation withtheir applicability on the target domain. As part of our study, we conduct anobjective measurement of the performance for six kinds of representativeclassifiers. The results suggest Prototypical Classifier as the standardevaluation recipe for CDFSL."
Interpretable Concept-based Prototypical Networks for Few-Shot Learning,"['Mohammad Reza Zarei', 'Majid Komeili']",http://arxiv.org/pdf/2202.13474v1.pdf,2022-02-27,"['cs.lg', 'cs.cv']","  Few-shot learning aims at recognizing new instances from classes with limitedsamples. This challenging task is usually alleviated by performingmeta-learning on similar tasks. However, the resulting models are black-boxes.There has been growing concerns about deploying black-box machine learningmodels and FSL is not an exception in this regard. In this paper, we propose amethod for FSL based on a set of human-interpretable concepts. It constructs aset of metric spaces associated with the concepts and classifies samples ofnovel classes by aggregating concept-specific decisions. The proposed methoddoes not require concept annotations for query samples. This interpretablemethod achieved results on a par with six previously state-of-the-art black-boxFSL methods on the CUB fine-grained bird classification dataset."
In-Context Learning for Few-Shot Dialogue State Tracking,"['Yushi Hu', 'Chia-Hsuan Lee', 'Tianbao Xie', 'Tao Yu', 'Noah A. Smith', 'Mari Ostendorf']",http://arxiv.org/pdf/2203.08568v3.pdf,2022-03-16,['cs.cl'],"  Collecting and annotating task-oriented dialogues is time-consuming andcostly; thus, zero and few shot learning could greatly benefit dialogue statetracking (DST). In this work, we propose an in-context learning (ICL) frameworkfor zero-shot and few-shot learning DST, where a large pre-trained languagemodel (LM) takes a test instance and a few exemplars as input, and directlydecodes the dialogue state without any parameter updates. To better leverage atabular domain description in the LM prompt, we reformulate DST into atext-to-SQL problem. We also propose a novel approach to retrieve annotateddialogues as exemplars. Empirical results on MultiWOZ show that our methodIC-DST substantially outperforms previous fine-tuned state-of-the-art models infew-shot settings. In addition, we test IC-DST in zero-shot settings, in whichthe model only takes a fixed task instruction as input, finding that itoutperforms previous zero-shot methods by a large margin."
A Rationale-Centric Framework for Human-in-the-loop Machine Learning,"['Jinghui Lu', 'Linyi Yang', 'Brian Mac Namee', 'Yue Zhang']",http://arxiv.org/pdf/2203.12918v1.pdf,2022-03-24,"['cs.ai', 'cs.cl', 'cs.hc']","  We present a novel rationale-centric framework with human-in-the-loop --Rationales-centric Double-robustness Learning (RDL) -- to boost modelout-of-distribution performance in few-shot learning scenarios. By using staticsemi-factual generation and dynamic human-intervened correction, RDL exploitsrationales (i.e. phrases that cause the prediction), human interventions andsemi-factual augmentations to decouple spurious associations and bias modelstowards generally applicable underlying distributions, which enables fast andaccurate generalisation. Experimental results show that RDL leads tosignificant prediction benefits on both in-distribution and out-of-distributiontests compared to many state-of-the-art benchmarks -- especially for few-shotlearning scenarios. We also perform extensive ablation studies to supportin-depth analyses of each component in our framework."
Compare learning: bi-attention network for few-shot learning,"['Li Ke', 'Meng Pan', 'Weigao Wen', 'Dong Li']",http://arxiv.org/pdf/2203.13487v1.pdf,2022-03-25,['cs.cv'],"  Learning with few labeled data is a key challenge for visual recognition, asdeep neural networks tend to overfit using a few samples only. One of theFew-shot learning methods called metric learning addresses this challenge byfirst learning a deep distance metric to determine whether a pair of imagesbelong to the same category, then applying the trained metric to instances fromother test set with limited labels. This method makes the most of the fewsamples and limits the overfitting effectively. However, extant metric networksusually employ Linear classifiers or Convolutional neural networks (CNN) thatare not precise enough to globally capture the subtle differences betweenvectors. In this paper, we propose a novel approach named Bi-attention networkto compare the instances, which can measure the similarity between embeddingsof instances precisely, globally and efficiently. We verify the effectivenessof our model on two benchmarks. Experiments show that our approach achievedimproved accuracy and convergence speed over baseline models."
WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen  Language Models,"['Heting Gao', 'Junrui Ni', 'Kaizhi Qian', 'Yang Zhang', 'Shiyu Chang', 'Mark Hasegawa-Johnson']",http://arxiv.org/pdf/2203.15863v2.pdf,2022-03-29,"['eess.as', 'cs.ai', 'cs.cl']","  Large-scale auto-regressive language models pretrained on massive text havedemonstrated their impressive ability to perform new natural language taskswith only a few text examples, without the need for fine-tuning. Recent studiesfurther show that such a few-shot learning ability can be extended to thetext-image setting by training an encoder to encode the images into embeddingsfunctioning like the text embeddings of the language model. Interested inexploring the possibility of transferring the few-shot learning ability to theaudio-text setting, we propose a novel speech understanding framework,WavPrompt, where we finetune a wav2vec model to generate a sequence of audioembeddings understood by the language model. We show that WavPrompt is afew-shot learner that can perform speech understanding tasks better than anaive text baseline. We conduct detailed ablation studies on differentcomponents and hyperparameters to empirically identify the best modelconfiguration. In addition, we conduct a non-speech understanding experiment toshow WavPrompt can extract more information than just the transcriptions. Codeis available at https://github.com/Hertin/WavPrompt"
Universal Representations: A Unified Look at Multiple Task and Domain  Learning,"['Wei-Hong Li', 'Xialei Liu', 'Hakan Bilen']",http://arxiv.org/pdf/2204.02744v2.pdf,2022-04-06,['cs.cv'],"  We propose a unified look at jointly learning multiple vision tasks andvisual domains through universal representations, a single deep neural network.Learning multiple problems simultaneously involves minimizing a weighted sum ofmultiple loss functions with different magnitudes and characteristics and thusresults in unbalanced state of one loss dominating the optimization and poorresults compared to learning a separate model for each problem. To this end, wepropose distilling knowledge of multiple task/domain-specific networks into asingle deep neural network after aligning its representations with thetask/domain-specific ones through small capacity adapters. We rigorously showthat universal representations achieve state-of-the-art performances inlearning of multiple dense prediction problems in NYU-v2 and Cityscapes,multiple image classification problems from diverse domains in Visual DecathlonDataset and cross-domain few-shot learning in MetaDataset. Finally we alsoconduct multiple analysis through ablation and qualitative studies."
Powering Finetuning in Few-Shot Learning: Domain-Agnostic Bias Reduction  with Selected Sampling,"['Ran Tao', 'Han Zhang', 'Yutong Zheng', 'Marios Savvides']",http://arxiv.org/pdf/2204.03749v2.pdf,2022-04-07,['cs.cv'],"  In recent works, utilizing a deep network trained on meta-training set servesas a strong baseline in few-shot learning. In this paper, we move forward torefine novel-class features by finetuning a trained deep network. Finetuning isdesigned to focus on reducing biases in novel-class feature distributions,which we define as two aspects: class-agnostic and class-specific biases.Class-agnostic bias is defined as the distribution shifting introduced bydomain difference, which we propose Distribution Calibration Module(DCM) toreduce. DCM owes good property of eliminating domain difference and fastfeature adaptation during optimization. Class-specific bias is defined as thebiased estimation using a few samples in novel classes, which we proposeSelected Sampling(SS) to reduce. Without inferring the actual classdistribution, SS is designed by running sampling using proposal distributionsaround support-set samples. By powering finetuning with DCM and SS, we achievestate-of-the-art results on Meta-Dataset with consistent performance boostsover ten datasets from different domains. We believe our simple yet effectivemethod demonstrates its possibility to be applied on practical few-shotapplications."
Active Few-Shot Learning with FASL,"['Thomas Müller', 'Guillermo Pérez-Torró', 'Angelo Basile', 'Marc Franco-Salvador']",http://arxiv.org/pdf/2204.09347v2.pdf,2022-04-20,['cs.cl'],"  Recent advances in natural language processing (NLP) have led to strong textclassification models for many tasks. However, still often thousands ofexamples are needed to train models with good quality. This makes itchallenging to quickly develop and deploy new models for real world problemsand business needs. Few-shot learning and active learning are two lines ofresearch, aimed at tackling this problem. In this work, we combine both linesinto FASL, a platform that allows training text classification models using aniterative and fast process. We investigate which active learning methods workbest in our few-shot setup. Additionally, we develop a model to predict when tostop annotating. This is relevant as in a few-shot setup we do not have accessto a large validation set."
Few-Shot Speaker Identification Using Depthwise Separable Convolutional  Network with Channel Attention,"['Yanxiong Li', 'Wucheng Wang', 'Hao Chen', 'Wenchang Cao', 'Wei Li', 'Qianhua He']",http://arxiv.org/pdf/2204.11180v1.pdf,2022-04-24,"['eess.as', 'cs.sd']","  Although few-shot learning has attracted much attention from the fields ofimage and audio classification, few efforts have been made on few-shot speakeridentification. In the task of few-shot learning, overfitting is a toughproblem mainly due to the mismatch between training and testing conditions. Inthis paper, we propose a few-shot speaker identification method which canalleviate the overfitting problem. In the proposed method, the model of adepthwise separable convolutional network with channel attention is trainedwith a prototypical loss function. Experimental datasets are extracted fromthree public speech corpora: Aishell-2, VoxCeleb1 and TORGO. Experimentalresults show that the proposed method exceeds state-of-the-art methods forfew-shot speaker identification in terms of accuracy and F-score."
PGADA: Perturbation-Guided Adversarial Alignment for Few-shot Learning  Under the Support-Query Shift,"['Siyang Jiang', 'Wei Ding', 'Hsi-Wen Chen', 'Ming-Syan Chen']",http://arxiv.org/pdf/2205.03817v1.pdf,2022-05-08,"['cs.cv', 'cs.ai']","  Few-shot learning methods aim to embed the data to a low-dimensionalembedding space and then classify the unseen query data to the seen supportset. While these works assume that the support set and the query set lie in thesame embedding space, a distribution shift usually occurs between the supportset and the query set, i.e., the Support-Query Shift, in the real world. Thoughoptimal transportation has shown convincing results in aligning differentdistributions, we find that the small perturbations in the images wouldsignificantly misguide the optimal transportation and thus degrade the modelperformance. To relieve the misalignment, we first propose a novel adversarialdata augmentation method, namely Perturbation-Guided Adversarial Alignment(PGADA), which generates the hard examples in a self-supervised manner. Inaddition, we introduce Regularized Optimal Transportation to derive a smoothoptimal transportation plan. Extensive experiments on three benchmark datasetsmanifest that our framework significantly outperforms the elevenstate-of-the-art methods on three datasets."
ReFine: Re-randomization before Fine-tuning for Cross-domain Few-shot  Learning,"['Jaehoon Oh', 'Sungnyun Kim', 'Namgyu Ho', 'Jin-Hwa Kim', 'Hwanjun Song', 'Se-Young Yun']",http://arxiv.org/pdf/2205.05282v3.pdf,2022-05-11,['cs.cv'],"  Cross-domain few-shot learning (CD-FSL), where there are few target samplesunder extreme differences between source and target domains, has recentlyattracted huge attention. Recent studies on CD-FSL generally focus on transferlearning based approaches, where a neural network is pre-trained on popularlabeled source domain datasets and then transferred to target domain data.Although the labeled datasets may provide suitable initial parameters for thetarget data, the domain difference between the source and target might hinderfine-tuning on the target domain. This paper proposes a simple yet powerfulmethod that re-randomizes the parameters fitted on the source domain beforeadapting to the target data. The re-randomization resets source-specificparameters of the source pre-trained model and thus facilitates fine-tuning onthe target domain, improving few-shot performance."
Uncertainty-based Network for Few-shot Image Classification,"['Minglei Yuan', 'Qian Xu', 'Chunhao Cai', 'Yin-Dong Zheng', 'Tao Wang', 'Tong Lu']",http://arxiv.org/pdf/2205.08157v1.pdf,2022-05-17,"['cs.cv', 'cs.lg']","  The transductive inference is an effective technique in the few-shot learningtask, where query sets update prototypes to improve themselves. However, thesemethods optimize the model by considering only the classification scores of thequery instances as confidence while ignoring the uncertainty of theseclassification scores. In this paper, we propose a novel method calledUncertainty-Based Network, which models the uncertainty of classificationresults with the help of mutual information. Specifically, we first dataaugment and classify the query instance and calculate the mutual information ofthese classification scores. Then, mutual information is used as uncertainty toassign weights to classification scores, and the iterative update strategybased on classification scores and uncertainties assigns the optimal weights toquery instances in prototype optimization. Extensive results on four benchmarksshow that Uncertainty-Based Network achieves comparable performance inclassification accuracy compared to state-of-the-art method."
FLEURS: Few-shot Learning Evaluation of Universal Representations of  Speech,"['Alexis Conneau', 'Min Ma', 'Simran Khanuja', 'Yu Zhang', 'Vera Axelrod', 'Siddharth Dalmia', 'Jason Riesa', 'Clara Rivera', 'Ankur Bapna']",http://arxiv.org/pdf/2205.12446v1.pdf,2022-05-25,"['cs.cl', 'cs.lg', 'cs.sd', 'eess.as']","  We introduce FLEURS, the Few-shot Learning Evaluation of UniversalRepresentations of Speech benchmark. FLEURS is an n-way parallel speech datasetin 102 languages built on top of the machine translation FLoRes-101 benchmark,with approximately 12 hours of speech supervision per language. FLEURS can beused for a variety of speech tasks, including Automatic Speech Recognition(ASR), Speech Language Identification (Speech LangID), Translation andRetrieval. In this paper, we provide baselines for the tasks based onmultilingual pre-trained models like mSLAM. The goal of FLEURS is to enablespeech technology in more languages and catalyze research in low-resourcespeech understanding."
POODLE: Improving Few-shot Learning via Penalizing Out-of-Distribution  Samples,"['Duong H. Le', 'Khoi D. Nguyen', 'Khoi Nguyen', 'Quoc-Huy Tran', 'Rang Nguyen', 'Binh-Son Hua']",http://arxiv.org/pdf/2206.04679v1.pdf,2022-06-08,"['cs.lg', 'cs.cv']","  In this work, we propose to use out-of-distribution samples, i.e., unlabeledsamples coming from outside the target classes, to improve few-shot learning.Specifically, we exploit the easily available out-of-distribution samples todrive the classifier to avoid irrelevant features by maximizing the distancefrom prototypes to out-of-distribution samples while minimizing that ofin-distribution samples (i.e., support, query data). Our approach is simple toimplement, agnostic to feature extractors, lightweight without any additionalcost for pre-training, and applicable to both inductive and transductivesettings. Extensive experiments on various standard benchmarks demonstrate thatthe proposed method consistently improves the performance of pretrainednetworks with different architectures."
Few 'Zero Level Set'-Shot Learning of Shape Signed Distance Functions in  Feature Space,"['Amine Ouasfi', 'Adnane Boukhayma']",http://arxiv.org/pdf/2207.04161v1.pdf,2022-07-09,"['cs.cv', 'cs.ai', 'cs.lg']","  We explore a new idea for learning based shape reconstruction from a pointcloud, based on the recently popularized implicit neural shape representations.We cast the problem as a few-shot learning of implicit neural signed distancefunctions in feature space, that we approach using gradient basedmeta-learning. We use a convolutional encoder to build a feature space giventhe input point cloud. An implicit decoder learns to predict signed distancevalues given points represented in this feature space. Setting the input pointcloud, i.e. samples from the target shape function's zero level set, as thesupport (i.e. context) in few-shot learning terms, we train the decoder suchthat it can adapt its weights to the underlying shape of this context with afew (5) tuning steps. We thus combine two types of implicit neural networkconditioning mechanisms simultaneously for the first time, namely featureencoding and meta-learning. Our numerical and qualitative evaluation shows thatin the context of implicit reconstruction from a sparse point cloud, ourproposed strategy, i.e. meta-learning in feature space, outperforms existingalternatives, namely standard supervised learning in feature space, andmeta-learning in euclidean space, while still providing fast inference."
Contrastive Knowledge-Augmented Meta-Learning for Few-Shot  Classification,"['Rakshith Subramanyam', 'Mark Heimann', 'Jayram Thathachar', 'Rushil Anirudh', 'Jayaraman J. Thiagarajan']",http://arxiv.org/pdf/2207.12346v1.pdf,2022-07-25,['cs.lg'],"  Model agnostic meta-learning algorithms aim to infer priors from severalobserved tasks that can then be used to adapt to a new task with few examples.Given the inherent diversity of tasks arising in existing benchmarks, recentmethods use separate, learnable structure, such as hierarchies or graphs, forenabling task-specific adaptation of the prior. While these approaches haveproduced significantly better meta learners, our goal is to improve theirperformance when the heterogeneous task distribution contains challengingdistribution shifts and semantic disparities. To this end, we introduce CAML(Contrastive Knowledge-Augmented Meta Learning), a novel approach forknowledge-enhanced few-shot learning that evolves a knowledge graph toeffectively encode historical experience, and employs a contrastivedistillation strategy to leverage the encoded knowledge for task-awaremodulation of the base learner. Using standard benchmarks, we evaluate theperformance of CAML in different few-shot learning scenarios. In addition tothe standard few-shot task adaptation, we also consider the more challengingmulti-domain task adaptation and few-shot dataset generalization settings inour empirical studies. Our results shows that CAML consistently outperformsbest known approaches and achieves improved generalization."
Few-shot Single-view 3D Reconstruction with Memory Prior Contrastive  Network,"['Zhen Xing', 'Yijiang Chen', 'Zhixin Ling', 'Xiangdong Zhou', 'Yu Xiang']",http://arxiv.org/pdf/2208.00183v1.pdf,2022-07-30,['cs.cv'],"  3D reconstruction of novel categories based on few-shot learning is appealingin real-world applications and attracts increasing research interests. Previousapproaches mainly focus on how to design shape prior models for differentcategories. Their performance on unseen categories is not very competitive. Inthis paper, we present a Memory Prior Contrastive Network (MPCN) that can storeshape prior knowledge in a few-shot learning based 3D reconstruction framework.With the shape memory, a multi-head attention module is proposed to capturedifferent parts of a candidate shape prior and fuse these parts together toguide 3D reconstruction of novel categories. Besides, we introduce a 3D-awarecontrastive learning method, which can not only complement the retrievalaccuracy of memory network, but also better organize image features fordownstream tasks. Compared with previous few-shot 3D reconstruction methods,MPCN can handle the inter-class variability without category annotations.Experimental results on a benchmark synthetic dataset and the Pascal3D+real-world dataset show that our model outperforms the current state-of-the-artmethods significantly."
Transferable Multi-Agent Reinforcement Learning with Dynamic  Participating Agents,"['Xuting Tang', 'Jia Xu', 'Shusen Wang']",http://arxiv.org/pdf/2208.02424v1.pdf,2022-08-04,"['cs.lg', 'cs.ai', 'cs.ma']","  We study multi-agent reinforcement learning (MARL) with centralized trainingand decentralized execution. During the training, new agents may join, andexisting agents may unexpectedly leave the training. In such situations, astandard deep MARL model must be trained again from scratch, which is verytime-consuming. To tackle this problem, we propose a special networkarchitecture with a few-shot learning algorithm that allows the number ofagents to vary during centralized training. In particular, when a new agentjoins the centralized training, our few-shot learning algorithm trains itspolicy network and value network using a small number of samples; when an agentleaves the training, the training process of the remaining agents is notaffected. Our experiments show that using the proposed network architecture andalgorithm, model adaptation when new agents join can be 100+ times faster thanthe baseline. Our work is applicable to any setting, including cooperative,competitive, and mixed."
IPNET:Influential Prototypical Networks for Few Shot Learning,"['Ranjana Roy Chowdhury', 'Deepti R. Bathula']",http://arxiv.org/pdf/2208.09345v1.pdf,2022-08-19,['cs.cv'],"  Prototypical network (PN) is a simple yet effective few shot learningstrategy. It is a metric-based meta-learning technique where classification isperformed by computing Euclidean distances to prototypical representations ofeach class. Conventional PN attributes equal importance to all samples andgenerates prototypes by simply averaging the support sample embeddingsbelonging to each class. In this work, we propose a novel version of PN thatattributes weights to support samples corresponding to their influence on thesupport sample distribution. Influence weights of samples are calculated basedon maximum mean discrepancy (MMD) between the mean embeddings of sampledistributions including and excluding the sample. Further, the influence factorof a sample is measured using MMD based on the shift in the distribution in theabsence of that sample."
Adversarial Feature Augmentation for Cross-domain Few-shot  Classification,"['Yanxu Hu', 'Andy J. Ma']",http://arxiv.org/pdf/2208.11021v1.pdf,2022-08-23,['cs.cv'],"  Existing methods based on meta-learning predict novel-class labels for(target domain) testing tasks via meta knowledge learned from (source domain)training tasks of base classes. However, most existing works may fail togeneralize to novel classes due to the probably large domain discrepancy acrossdomains. To address this issue, we propose a novel adversarial featureaugmentation (AFA) method to bridge the domain gap in few-shot learning. Thefeature augmentation is designed to simulate distribution variations bymaximizing the domain discrepancy. During adversarial training, the domaindiscriminator is learned by distinguishing the augmented features (unseendomain) from the original ones (seen domain), while the domain discrepancy isminimized to obtain the optimal feature encoder. The proposed method is aplug-and-play module that can be easily integrated into existing few-shotlearning methods based on meta-learning. Extensive experiments on nine datasetsdemonstrate the superiority of our method for cross-domain few-shotclassification compared with the state of the art. Code is available athttps://github.com/youthhoo/AFA_For_Few_shot_learning"
Learn to Adapt to New Environment from Past Experience and Few Pilot,"['Ouya Wang', 'Jiabao Gao', 'Geoffrey Ye Li']",http://arxiv.org/pdf/2209.02649v1.pdf,2022-09-02,"['cs.it', 'cs.lg', 'math.it']","  In recent years, deep learning has been widely applied in communications andachieved remarkable performance improvement. Most of the existing works arebased on data-driven deep learning, which requires a significant amount oftraining data for the communication model to adapt to new environments andresults in huge computing resources for collecting data and retraining themodel. In this paper, we will significantly reduce the required amount oftraining data for new environments by leveraging the learning experience fromthe known environments. Therefore, we introduce few-shot learning to enable thecommunication model to generalize to new environments, which is realized by anattention-based method. With the attention network embedded into the deeplearning-based communication model, environments with different power delayprofiles can be learnt together in the training process, which is called thelearning experience. By exploiting the learning experience, the communicationmodel only requires few pilot blocks to perform well in the new environment.Through an example of deep-learning-based channel estimation, we demonstratethat this novel design method achieves better performance than the existingdata-driven approach designed for few-shot learning."
An Embarrassingly Simple Approach to Semi-Supervised Few-Shot Learning,"['Xiu-Shen Wei', 'He-Yang Xu', 'Faen Zhang', 'Yuxin Peng', 'Wei Zhou']",http://arxiv.org/pdf/2209.13777v1.pdf,2022-09-28,"['cs.cv', 'cs.lg']","  Semi-supervised few-shot learning consists in training a classifier to adaptto new tasks with limited labeled data and a fixed quantity of unlabeled data.Many sophisticated methods have been developed to address the challenges thisproblem comprises. In this paper, we propose a simple but quite effectiveapproach to predict accurate negative pseudo-labels of unlabeled data from anindirect learning perspective, and then augment the extremely label-constrainedsupport set in few-shot classification tasks. Our approach can be implementedin just few lines of code by only using off-the-shelf operations, yet it isable to outperform state-of-the-art methods on four benchmark datasets."
Hypernetwork approach to Bayesian MAML,"['Piotr Borycki', 'Piotr Kubacki', 'Marcin Przewięźlikowski', 'Tomasz Kuśmierczyk', 'Jacek Tabor', 'Przemysław Spurek']",http://arxiv.org/pdf/2210.02796v2.pdf,2022-10-06,"['cs.lg', 'cs.ai']","  The main goal of Few-Shot learning algorithms is to enable learning fromsmall amounts of data. One of the most popular and elegant Few-Shot learningapproaches is Model-Agnostic Meta-Learning (MAML). The main idea behind thismethod is to learn the shared universal weights of a meta-model, which are thenadapted for specific tasks. However, the method suffers from over-fitting andpoorly quantifies uncertainty due to limited data size. Bayesian approachescould, in principle, alleviate these shortcomings by learning weightdistributions in place of point-wise weights. Unfortunately, previousmodifications of MAML are limited due to the simplicity of Gaussian posteriors,MAML-like gradient-based weight updates, or by the same structure enforced foruniversal and adapted weights.  In this paper, we propose a novel framework for Bayesian MAML calledBayesianHMAML, which employs Hypernetworks for weight updates. It learns theuniversal weights point-wise, but a probabilistic structure is added whenadapted for specific tasks. In such a framework, we can use simple Gaussiandistributions or more complicated posteriors induced by Continuous NormalizingFlows."
Self-Attention Message Passing for Contrastive Few-Shot Learning,"['Ojas Kishorkumar Shirekar', 'Anuj Singh', 'Hadi Jamali-Rad']",http://arxiv.org/pdf/2210.06339v1.pdf,2022-10-12,['cs.cv'],"  Humans have a unique ability to learn new representations from just a handfulof examples with little to no supervision. Deep learning models, however,require an abundance of data and supervision to perform at a satisfactorylevel. Unsupervised few-shot learning (U-FSL) is the pursuit of bridging thisgap between machines and humans. Inspired by the capacity of graph neuralnetworks (GNNs) in discovering complex inter-sample relationships, we propose anovel self-attention based message passing contrastive learning approach(coined as SAMP-CLR) for U-FSL pre-training. We also propose an optimaltransport (OT) based fine-tuning strategy (we call OpT-Tune) to efficientlyinduce task awareness into our novel end-to-end unsupervised few-shotclassification framework (SAMPTransfer). Our extensive experimental resultscorroborate the efficacy of SAMPTransfer in a variety of downstream few-shotclassification scenarios, setting a new state-of-the-art for U-FSL on bothminiImagenet and tieredImagenet benchmarks, offering up to 7%+ and 5%+improvements, respectively. Our further investigations also confirm thatSAMPTransfer remains on-par with some supervised baselines on miniImagenet andoutperforms all existing U-FSL baselines in a challenging cross-domainscenario. Our code can be found in our GitHub repository athttps://github.com/ojss/SAMPTransfer/."
Enabling Classifiers to Make Judgements Explicitly Aligned with Human  Values,"['Yejin Bang', 'Tiezheng Yu', 'Andrea Madotto', 'Zhaojiang Lin', 'Mona Diab', 'Pascale Fung']",http://arxiv.org/pdf/2210.07652v1.pdf,2022-10-14,"['cs.cl', 'cs.ai']","  Many NLP classification tasks, such as sexism/racism detection or toxicitydetection, are based on human values. Yet, human values can vary under diversecultural conditions. Therefore, we introduce a framework for value-alignedclassification that performs prediction based on explicitly written humanvalues in the command. Along with the task, we propose a practical approachthat distills value-aligned knowledge from large-scale language models (LLMs)to construct value-aligned classifiers in two steps. First, we generatevalue-aligned training data from LLMs by prompt-based few-shot learning. Next,we fine-tune smaller classification models with the generated data for thetask. Empirical results show that our VA-Models surpass multiple baselines byat least 15.56% on the F1-score, including few-shot learning with OPT-175B andexisting text augmentation methods. We suggest that using classifiers withexplicit human value input improves both inclusivity & explainability in AI."
Learning Transferable Adversarial Robust Representations via Multi-view  Consistency,"['Minseon Kim', 'Hyeonjeong Ha', 'Dong Bok Lee', 'Sung Ju Hwang']",http://arxiv.org/pdf/2210.10485v2.pdf,2022-10-19,['cs.lg'],"  Despite the success on few-shot learning problems, most meta-learned modelsonly focus on achieving good performance on clean examples and thus easilybreak down when given adversarially perturbed samples. While some recent workshave shown that a combination of adversarial learning and meta-learning couldenhance the robustness of a meta-learner against adversarial attacks, they failto achieve generalizable adversarial robustness to unseen domains and tasks,which is the ultimate goal of meta-learning. To address this challenge, wepropose a novel meta-adversarial multi-view representation learning frameworkwith dual encoders. Specifically, we introduce the discrepancy across the twodifferently augmented samples of the same data instance by first updating theencoder parameters with them and further imposing a novel label-freeadversarial attack to maximize their discrepancy. Then, we maximize theconsistency across the views to learn transferable robust representationsacross domains and tasks. Through experimental validation on multiplebenchmarks, we demonstrate the effectiveness of our framework on few-shotlearning tasks from unseen domains, achieving over 10\% robust accuracyimprovements against previous adversarial meta-learning baselines."
A Task-aware Dual Similarity Network for Fine-grained Few-shot Learning,"['Yan Qi', 'Han Sun', 'Ningzhong Liu', 'Huiyu Zhou']",http://arxiv.org/pdf/2210.12348v1.pdf,2022-10-22,['cs.cv'],"  The goal of fine-grained few-shot learning is to recognize sub-categoriesunder the same super-category by learning few labeled samples. Most of therecent approaches adopt a single similarity measure, that is, global or localmeasure alone. However, for fine-grained images with high intra-class varianceand low inter-class variance, exploring global invariant features anddiscriminative local details is quite essential. In this paper, we propose aTask-aware Dual Similarity Network(TDSNet), which applies global features andlocal patches to achieve better performance. Specifically, a local featureenhancement module is adopted to activate the features with strongdiscriminability. Besides, task-aware attention exploits the important patchesamong the entire task. Finally, both the class prototypes obtained by globalfeatures and discriminative local patches are employed for prediction.Extensive experiments on three fine-grained datasets demonstrate that theproposed TDSNet achieves competitive performance by comparing with otherstate-of-the-art algorithms."
Aligning MAGMA by Few-Shot Learning and Finetuning,"['Jean-Charles Layoun', 'Alexis Roger', 'Irina Rish']",http://arxiv.org/pdf/2210.14161v1.pdf,2022-10-18,"['cs.cv', 'cs.ai']","  The goal of vision-language modeling is to allow models to tie languageunderstanding with visual inputs. The aim of this paper is to evaluate andalign the Visual Language Model (VLM) called Multimodal Augmentation ofGenerative Models through Adapter-based finetuning (MAGMA) with human values.MAGMA is a VLM that is capable of image captioning and visualquestion-answering. We will evaluate its alignment in three differentscenarios. To begin, we assess MAGMA's out-of-the-box alignment through thecheckpoint provided by Hugging Face. Then, we measure if few-shot learningmanages to improve the results. Finally, we finetune the model on alignedexamples and evaluate its behavior."
Efficient few-shot learning for pixel-precise handwritten document  layout analysis,"['Axel De Nardin', 'Silvia Zottin', 'Matteo Paier', 'Gian Luca Foresti', 'Emanuela Colombi', 'Claudio Piciarelli']",http://arxiv.org/pdf/2210.15570v1.pdf,2022-10-27,"['cs.cv', 'cs.ai']","  Layout analysis is a task of uttermost importance in ancient handwrittendocument analysis and represents a fundamental step toward the simplificationof subsequent tasks such as optical character recognition and automatictranscription. However, many of the approaches adopted to solve this problemrely on a fully supervised learning paradigm. While these systems achieve verygood performance on this task, the drawback is that pixel-precise text labelingof the entire training set is a very time-consuming process, which makes thistype of information rarely available in a real-world scenario. In the presentpaper, we address this problem by proposing an efficient few-shot learningframework that achieves performances comparable to current state-of-the-artfully supervised methods on the publicly available DIVA-HisDB dataset."
GPS: Genetic Prompt Search for Efficient Few-shot Learning,"['Hanwei Xu', 'Yujun Chen', 'Yulun Du', 'Nan Shao', 'Yanggang Wang', 'Haiyu Li', 'Zhilin Yang']",http://arxiv.org/pdf/2210.17041v1.pdf,2022-10-31,['cs.cl'],"  Prompt-based techniques have demostrated great potential for improving thefew-shot generalization of pretrained language models. However, theirperformance heavily relies on the manual design of prompts and thus requires alot of human efforts. In this paper, we introduce Genetic Prompt Search (GPS)to improve few-shot learning with prompts, which utilizes a genetic algorithmto automatically search for high-performing prompts. GPS is gradient-free andrequires no update of model parameters but only a small validation set.Experiments on diverse datasets proved the effectiveness of GPS, whichoutperforms manual prompts by a large margin of 2.6 points. Our method is alsobetter than other parameter-efficient tuning methods such as prompt tuning."
miCSE: Mutual Information Contrastive Learning for Low-shot Sentence  Embeddings,"['Tassilo Klein', 'Moin Nabi']",http://arxiv.org/pdf/2211.04928v2.pdf,2022-11-09,"['cs.cl', 'cs.lg']","  This paper presents miCSE, a mutual information-based contrastive learningframework that significantly advances the state-of-the-art in few-shot sentenceembedding. The proposed approach imposes alignment between the attentionpattern of different views during contrastive learning. Learning sentenceembeddings with miCSE entails enforcing the structural consistency acrossaugmented views for every sentence, making contrastive self-supervised learningmore sample efficient. As a result, the proposed approach shows strongperformance in the few-shot learning domain. While it achieves superior resultscompared to state-of-the-art methods on multiple benchmarks in few-shotlearning, it is comparable in the full-shot scenario. This study opens upavenues for efficient self-supervised learning methods that are more robustthan current contrastive methods for sentence embedding."
MEAL: Stable and Active Learning for Few-Shot Prompting,"['Abdullatif Köksal', 'Timo Schick', 'Hinrich Schütze']",http://arxiv.org/pdf/2211.08358v3.pdf,2022-11-15,['cs.cl'],"  Few-shot classification has made great strides due to foundation models that,through priming and prompting, are highly effective few-shot learners. However,this approach has high variance both across different sets of few shots (dataselection) and across different finetuning runs (run variability). This isproblematic not only because it impedes the fair comparison of differentapproaches, but especially because it makes few-shot learning too unreliablefor many real-world applications. To alleviate these issues, we make twocontributions for more stable and effective few-shot learning: First, wepropose novel ensembling methods and show that they substantially reduce runvariability. Second, we introduce a new active learning (AL) criterion for dataselection and present the first AL-based approach specifically tailored towardsprompt-based learning. In our experiments, we show that our combined method,MEAL (Multiprompt finetuning and prediction Ensembling with Active Learning),improves overall performance of prompt-based finetuning by 2.3 points on fivediverse tasks. We publicly share our code and data splits inhttps://github.com/akoksal/MEAL."
Adaptive Prototypical Networks,"['Manas Gogoi', 'Sambhavi Tiwari', 'Shekhar Verma']",http://arxiv.org/pdf/2211.12479v1.pdf,2022-11-22,"['cs.cv', 'cs.lg']","  Prototypical network for Few shot learning tries to learn an embeddingfunction in the encoder that embeds images with similar features close to oneanother in the embedding space. However, in this process, the support setsamples for a task are embedded independently of one other, and hence, theinter-class closeness is not taken into account. Thus, in the presence ofsimilar-looking classes in a task, the embeddings will tend to be close to eachother in the embedding space and even possibly overlap in some regions, whichis not desirable for classification. In this paper, we propose an approach thatintuitively pushes the embeddings of each of the classes away from the othersin the meta-testing phase, thereby grouping them closely based on the distinctclass labels rather than only the similarity of spatial features. This isachieved by training the encoder network for classification using the supportset samples and labels of the new task. Extensive experiments conducted onbenchmark data sets show improvements in meta-testing accuracy when comparedwith Prototypical Networks and also other standard few-shot learning models."
Revisiting Distance Metric Learning for Few-Shot Natural Language  Classification,"['Witold Sosnowski', 'Anna Wróblewska', 'Karolina Seweryn', 'Piotr Gawrysiak']",http://arxiv.org/pdf/2211.15202v1.pdf,2022-11-28,"['cs.cl', 'cs.ai', 'cs.lg']","  Distance Metric Learning (DML) has attracted much attention in imageprocessing in recent years. This paper analyzes its impact on supervisedfine-tuning language models for Natural Language Processing (NLP)classification tasks under few-shot learning settings. We investigated severalDML loss functions in training RoBERTa language models on known SentEvalTransfer Tasks datasets. We also analyzed the possibility of using proxy-basedDML losses during model inference.  Our systematic experiments have shown that under few-shot learning settings,particularly proxy-based DML losses can positively affect the fine-tuning andinference of a supervised language model. Models tuned with a combination ofCCE (categorical cross-entropy loss) and ProxyAnchor Loss have, on average, thebest performance and outperform models with only CCE by about 3.27 percentagepoints -- up to 10.38 percentage points depending on the training dataset."
Few-shot Query-Focused Summarization with Prefix-Merging,"['Ruifeng Yuan', 'Zili Wang', 'Ziqiang Cao', 'Wenjie Li']",http://arxiv.org/pdf/2211.16164v1.pdf,2022-11-29,"['cs.cl', 'cs.ai']","  Query-focused summarization has been considered as an important extension fortext summarization. It aims to generate a concise highlight for a given query.Different from text summarization, query-focused summarization has long beenplagued by the problem of lacking high-quality large-scale datasets. In thispaper, we investigate the idea that whether we can integrate and transfer theknowledge of text summarization and question answering to assist the few-shotlearning in query-focused summarization. Here, we propose prefix-merging, aprefix-based pretraining strategy for few-shot learning in query-focusedsummarization. Drawn inspiration from prefix-tuning, we are allowed tointegrate the task knowledge from text summarization and question answeringinto a properly designed prefix and apply the merged prefix to query-focusedsummarization. With only a small amount of trainable parameters, prefix-mergingoutperforms fine-tuning on query-focused summarization. We further discuss theinfluence of different prefix designs and propose a visualized explanation forhow prefix-merging works."
Disentangled Generation with Information Bottleneck for Few-Shot  Learning,"['Zhuohang Dang', 'Jihong Wang', 'Minnan Luo', 'Chengyou Jia', 'Caixia Yan', 'Qinghua Zheng']",http://arxiv.org/pdf/2211.16185v1.pdf,2022-11-29,['cs.cv'],"  Few-shot learning (FSL), which aims to classify unseen classes with fewsamples, is challenging due to data scarcity. Although various generativemethods have been explored for FSL, the entangled generation process of thesemethods exacerbates the distribution shift in FSL, thus greatly limiting thequality of generated samples. To these challenges, we propose a novelInformation Bottleneck (IB) based Disentangled Generation Framework for FSL,termed as DisGenIB, that can simultaneously guarantee the discrimination anddiversity of generated samples. Specifically, we formulate a novel frameworkwith information bottleneck that applies for both disentangled representationlearning and sample generation. Different from existing IB-based methods thatcan hardly exploit priors, we demonstrate our DisGenIB can effectively utilizepriors to further facilitate disentanglement. We further prove in theory thatsome previous generative and disentanglement methods are special cases of ourDisGenIB, which demonstrates the generality of the proposed DisGenIB. Extensiveexperiments on challenging FSL benchmarks confirm the effectiveness andsuperiority of DisGenIB, together with the validity of our theoreticalanalyses. Our codes will be open-source upon acceptance."
JASMINE: Arabic GPT Models for Few-Shot Learning,"['El Moatez Billah Nagoudi', 'Muhammad Abdul-Mageed', 'AbdelRahim Elmadany', 'Alcides Alcoba Inciarte', 'Md Tawkat Islam Khondaker']",http://arxiv.org/pdf/2212.10755v2.pdf,2022-12-21,['cs.cl'],"  Scholarship on generative pretraining (GPT) remains acutely Anglocentric,leaving serious gaps in our understanding of the whole class of autoregressivemodels. For example, we have little knowledge about the potential of thesemodels and their societal impacts in diverse linguistic and cultural settings.We alleviate this issue for Arabic, a wide collection of languages anddialectal varieties with more than 400 million population, by introducingJASMINE. JASMINE is a suite of powerful Arabic autoregressive Transformerlanguage models ranging in size between 300 million-6.7 billion parameterspretrained on a large and diverse dataset (~ 235 GB of text). We also carefullydesign and release a comprehensive benchmark for both automated and humanevaluation of Arabic autoregressive models, with coverage of potential socialbiases, harms, and toxicity. Using our novel benchmark, we evaluate JASMINEextensively showing powerful performance intrinsically as well as in few-shotlearning on a wide range of NLP tasks. We aim to responsibly release our modelsand evaluation benchmark with interested researchers, along with code forexperimenting with them."
Concept Discovery for Fast Adapatation,"['Shengyu Feng', 'Hanghang Tong']",http://arxiv.org/pdf/2301.07850v2.pdf,2023-01-19,['cs.lg'],"  The advances in deep learning have enabled machine learning methods tooutperform human beings in various areas, but it remains a great challenge fora well-trained model to quickly adapt to a new task. One promising solution torealize this goal is through meta-learning, also known as learning to learn,which has achieved promising results in few-shot learning. However, currentapproaches are still enormously different from human beings' learning process,especially in the ability to extract structural and transferable knowledge.This drawback makes current meta-learning frameworks non-interpretable and hardto extend to more complex tasks. We tackle this problem by introducing conceptdiscovery to the few-shot learning problem, where we achieve more effectiveadaptation by meta-learning the structure among the data features, leading to acomposite representation of the data. Our proposed method Concept-BasedModel-Agnostic Meta-Learning (COMAML) has been shown to achieve consistentimprovements in the structured data for both synthesized datasets andreal-world datasets."
Exploiting Style Transfer-based Task Augmentation for Cross-Domain  Few-Shot Learning,"['Shuzhen Rao', 'Jun Huang', 'Zengming Tang']",http://arxiv.org/pdf/2301.07927v2.pdf,2023-01-19,['cs.cv'],"  In cross-domain few-shot learning, the core issue is that the model trainedon source domains struggles to generalize to the target domain, especially whenthe domain shift is large. Motivated by the observation that the domain shiftbetween training tasks and target tasks usually can reflect in their stylevariation, we propose Task Augmented Meta-Learning (TAML) to conduct styletransfer-based task augmentation to improve the domain generalization ability.Firstly, Multi-task Interpolation (MTI) is introduced to fuse features frommultiple tasks with different styles, which makes more diverse stylesavailable. Furthermore, a novel task-augmentation strategy called Multi-TaskStyle Transfer (MTST) is proposed to perform style transfer on existing tasksto learn discriminative style-independent features. We also introduce a FeatureModulation module (FM) to add random styles and improve generalization of themodel. The proposed TAML increases the diversity of styles of training tasks,and contributes to training a model with better domain generalization ability.The effectiveness is demonstrated via theoretical analysis and thoroughexperiments on two popular cross-domain few-shot benchmarks."
Alignment with human representations supports robust few-shot learning,"['Ilia Sucholutsky', 'Thomas L. Griffiths']",http://arxiv.org/pdf/2301.11990v3.pdf,2023-01-27,"['cs.lg', 'cs.ai', 'cs.cv', 'cs.hc', 'stat.ml']","  Should we care whether AI systems have representations of the world that aresimilar to those of humans? We provide an information-theoretic analysis thatsuggests that there should be a U-shaped relationship between the degree ofrepresentational alignment with humans and performance on few-shot learningtasks. We confirm this prediction empirically, finding such a relationship inan analysis of the performance of 491 computer vision models. We also show thathighly-aligned models are more robust to both natural adversarial attacks anddomain shifts. Our results suggest that human-alignment is often a sufficient,but not necessary, condition for models to make effective use of limited data,be robust, and generalize well."
Contrastive Meta-Learning for Partially Observable Few-Shot Learning,"['Adam Jelley', 'Amos Storkey', 'Antreas Antoniou', 'Sam Devlin']",http://arxiv.org/pdf/2301.13136v1.pdf,2023-01-30,['cs.lg'],"  Many contrastive and meta-learning approaches learn representations byidentifying common features in multiple views. However, the formalism for theseapproaches generally assumes features to be shared across views to be capturedcoherently. We consider the problem of learning a unified representation frompartial observations, where useful features may be present in only some of theviews. We approach this through a probabilistic formalism enabling views to mapto representations with different levels of uncertainty in differentcomponents; these views can then be integrated with one another throughmarginalisation over that uncertainty. Our approach, Partial ObservationExperts Modelling (POEM), then enables us to meta-learn consistentrepresentations from partial observations. We evaluate our approach on anadaptation of a comprehensive few-shot learning benchmark, Meta-Dataset, anddemonstrate the benefits of POEM over other meta-learning methods atrepresentation learning from partial observations. We further demonstrate theutility of POEM by meta-learning to represent an environment from partial viewsobserved by an agent exploring the environment."
Few-shot learning approaches for classifying low resource domain  specific software requirements,"['Anmol Nayak', 'Hari Prasad Timmapathini', 'Vidhya Murali', 'Atul Anil Gohad']",http://arxiv.org/pdf/2302.06951v1.pdf,2023-02-14,['cs.cl'],"  With the advent of strong pre-trained natural language processing models likeBERT, DeBERTa, MiniLM, T5, the data requirement for industries to fine-tunethese models to their niche use cases has drastically reduced (typically to afew hundred annotated samples for achieving a reasonable performance). However,the availability of even a few hundred annotated samples may not always beguaranteed in low resource domains like automotive, which often limits theusage of such deep learning models in an industrial setting. In this paper weaim to address the challenge of fine-tuning such pre-trained models with only afew annotated samples, also known as Few-shot learning. Our experiments focuson evaluating the performance of a diverse set of algorithms and methodologiesto achieve the task of classifying BOSCH automotive domain textual softwarerequirements into 3 categories, while utilizing only 15 annotated samples percategory for fine-tuning. We find that while SciBERT and DeBERTa based modelstend to be the most accurate at 15 training samples, their performanceimprovement scales minimally as the number of annotated samples is increased to50 in comparison to Siamese and T5 based models."
Log Parsing with Prompt-based Few-shot Learning,"['Van-Hoang Le', 'Hongyu Zhang']",http://arxiv.org/pdf/2302.07435v1.pdf,2023-02-15,['cs.se'],"  Logs generated by large-scale software systems provide crucial informationfor engineers to understand the system status and diagnose problems of thesystems. Log parsing, which converts raw log messages into structured data, isthe first step to enabling automated log analytics. Existing log parsersextract the common part as log templates using statistical features. However,these log parsers often fail to identify the correct templates and parametersbecause: 1) they often overlook the semantic meaning of log messages, and 2)they require domain-specific knowledge for different log datasets. To addressthe limitations of existing methods, in this paper, we propose LogPPT tocapture the patterns of templates using prompt-based few-shot learning. LogPPTutilises a novel prompt tuning method to recognise keywords and parametersbased on a few labelled log data. In addition, an adaptive random samplingalgorithm is designed to select a small yet diverse training set. We haveconducted extensive experiments on 16 public log datasets. The experimentalresults show that LogPPT is effective and efficient for log parsing."
Conversation Style Transfer using Few-Shot Learning,"['Shamik Roy', 'Raphael Shu', 'Nikolaos Pappas', 'Elman Mansimov', 'Yi Zhang', 'Saab Mansour', 'Dan Roth']",http://arxiv.org/pdf/2302.08362v2.pdf,2023-02-16,['cs.cl'],"  Conventional text style transfer approaches focus on sentence-level styletransfer without considering contextual information, and the style is describedwith attributes (e.g., formality). When applying style transfer inconversations such as task-oriented dialogues, existing approaches suffer fromthese limitations as context can play an important role and the styleattributes are often difficult to define in conversations. In this paper, weintroduce conversation style transfer as a few-shot learning problem, where themodel learns to perform style transfer by observing only a few exampledialogues in the target style. We propose a novel in-context learning approachto solve the task with style-free dialogues as a pivot. Human evaluation showsthat by incorporating multi-turn context, the model is able to match the targetstyle while having better appropriateness and semantic correctness compared toutterance/sentence-level style transfer. Additionally, we show thatconversation style transfer can also benefit downstream tasks. For example, inmulti-domain intent classification tasks, the F1 scores improve aftertransferring the style of training data to match the style of the test data."
An Adaptive Plug-and-Play Network for Few-Shot Learning,"['Hao Li', 'Li Li', 'Yunmeng Huang', 'Ning Li', 'Yongtao Zhang']",http://arxiv.org/pdf/2302.09326v1.pdf,2023-02-18,['cs.cv'],"  Few-shot learning (FSL) requires a model to classify new samples afterlearning from only a few samples. While remarkable results are achieved inexisting methods, the performance of embedding and metrics determines the upperlimit of classification accuracy in FSL. The bottleneck is that deep networksand complex metrics tend to induce overfitting in FSL, making it difficult tofurther improve the performance. Towards this, we propose plug-and-playmodel-adaptive resizer (MAR) and adaptive similarity metric (ASM) without anyother losses. MAR retains high-resolution details to alleviate the overfittingproblem caused by data scarcity, and ASM decouples the relationship betweendifferent metrics and then fuses them into an advanced one. Extensiveexperiments show that the proposed method could boost existing methods on twostandard dataset and a fine-grained datasets, and achieve state-of-the-artresults on mini-ImageNet and tiered-ImageNet."
STUNT: Few-shot Tabular Learning with Self-generated Tasks from  Unlabeled Tables,"['Jaehyun Nam', 'Jihoon Tack', 'Kyungmin Lee', 'Hankook Lee', 'Jinwoo Shin']",http://arxiv.org/pdf/2303.00918v1.pdf,2023-03-02,"['cs.lg', 'cs.ai']","  Learning with few labeled tabular samples is often an essential requirementfor industrial machine learning applications as varieties of tabular datasuffer from high annotation costs or have difficulties in collecting newsamples for novel tasks. Despite the utter importance, such a problem is quiteunder-explored in the field of tabular learning, and existing few-shot learningschemes from other domains are not straightforward to apply, mainly due to theheterogeneous characteristics of tabular data. In this paper, we propose asimple yet effective framework for few-shot semi-supervised tabular learning,coined Self-generated Tasks from UNlabeled Tables (STUNT). Our key idea is toself-generate diverse few-shot tasks by treating randomly chosen columns as atarget label. We then employ a meta-learning scheme to learn generalizableknowledge with the constructed tasks. Moreover, we introduce an unsupervisedvalidation scheme for hyperparameter search (and early stopping) by generatinga pseudo-validation set using STUNT from unlabeled data. Our experimentalresults demonstrate that our simple framework brings significant performancegain under various tabular few-shot learning benchmarks, compared to priorsemi- and self-supervised baselines. Code is available athttps://github.com/jaehyun513/STUNT."
Exploring Data Augmentation Methods on Social Media Corpora,"['Isabel Garcia Pietri', 'Kineret Stanley']",http://arxiv.org/pdf/2303.02198v1.pdf,2023-03-03,['cs.cl'],"  Data augmentation has proven widely effective in computer vision. In NaturalLanguage Processing (NLP) data augmentation remains an area of active research.There is no widely accepted augmentation technique that works well across tasksand model architectures. In this paper we explore data augmentation techniquesin the context of text classification using two social media datasets. Weexplore popular varieties of data augmentation, starting with oversampling,Easy Data Augmentation (Wei and Zou, 2019) and Back-Translation (Sennrich etal., 2015). We also consider Greyscaling, a relatively unexplored dataaugmentation technique that seeks to mitigate the intensity of adjectives inexamples. Finally, we consider a few-shot learning approach: Pattern-ExploitingTraining (PET) (Schick et al., 2020). For the experiments we use a BERTtransformer architecture. Results show that augmentation techniques provideonly minimal and inconsistent improvements. Synonym replacement providedevidence of some performance improvement and adjective scales with Grayscalingis an area where further exploration would be valuable. Few-shot learningexperiments show consistent improvement over supervised training, and seem verypromising when classes are easily separable but further exploration would bevaluable."
Improving Few-Shot Learning for Talking Face System with TTS Data  Augmentation,"['Qi Chen', 'Ziyang Ma', 'Tao Liu', 'Xu Tan', 'Qu Lu', 'Xie Chen', 'Kai Yu']",http://arxiv.org/pdf/2303.05322v1.pdf,2023-03-09,"['cs.sd', 'cs.mm', 'eess.as']","  Audio-driven talking face has attracted broad interest from academia andindustry recently. However, data acquisition and labeling in audio-driventalking face are labor-intensive and costly. The lack of data resource resultsin poor synthesis effect. To alleviate this issue, we propose to use TTS(Text-To-Speech) for data augmentation to improve few-shot ability of thetalking face system. The misalignment problem brought by the TTS audio issolved with the introduction of soft-DTW, which is first adopted in the talkingface task. Moreover, features extracted by HuBERT are explored to utilizeunderlying information of audio, and found to be superior over other features.The proposed method achieves 17%, 14%, 38% dominance on MSE score, DTW scoreand user study preference repectively over the baseline model, which shows theeffectiveness of improving few-shot learning for talking face system with TTSaugmentation."
DETA: Denoised Task Adaptation for Few-Shot Learning,"['Ji Zhang', 'Lianli Gao', 'Xu Luo', 'Hengtao Shen', 'Jingkuan Song']",http://arxiv.org/pdf/2303.06315v2.pdf,2023-03-11,['cs.cv'],"  Test-time task adaptation in few-shot learning aims to adapt a pre-trainedtask-agnostic model for capturing taskspecific knowledge of the test task, relyonly on few-labeled support samples. Previous approaches generally focus ondeveloping advanced algorithms to achieve the goal, while neglecting theinherent problems of the given support samples. In fact, with only a handful ofsamples available, the adverse effect of either the image noise (a.k.a.X-noise) or the label noise (a.k.a. Y-noise) from support samples can beseverely amplified. To address this challenge, in this work we propose DEnoisedTask Adaptation (DETA), a first, unified image- and label-denoising frameworkorthogonal to existing task adaptation approaches. Without extra supervision,DETA filters out task-irrelevant, noisy representations by taking advantage ofboth global visual information and local region details of support samples. Onthe challenging Meta-Dataset, DETA consistently improves the performance of abroad spectrum of baseline methods applied on various pre-trained models.Notably, by tackling the overlooked image noise in Meta-Dataset, DETAestablishes new state-of-the-art results. Code is released athttps://github.com/JimZAI/DETA."
Hubs and Hyperspheres: Reducing Hubness and Improving Transductive  Few-shot Learning with Hyperspherical Embeddings,"['Daniel J. Trosten', 'Rwiddhi Chakraborty', 'Sigurd Løkse', 'Kristoffer Knutsen Wickstrøm', 'Robert Jenssen', 'Michael C. Kampffmeyer']",http://arxiv.org/pdf/2303.09352v1.pdf,2023-03-16,['cs.cv'],"  Distance-based classification is frequently used in transductive few-shotlearning (FSL). However, due to the high-dimensionality of imagerepresentations, FSL classifiers are prone to suffer from the hubness problem,where a few points (hubs) occur frequently in multiple nearest neighbour listsof other points. Hubness negatively impacts distance-based classification whenhubs from one class appear often among the nearest neighbors of points fromanother class, degrading the classifier's performance. To address the hubnessproblem in FSL, we first prove that hubness can be eliminated by distributingrepresentations uniformly on the hypersphere. We then propose two newapproaches to embed representations on the hypersphere, which we prove optimizea tradeoff between uniformity and local similarity preservation -- reducinghubness while retaining class structure. Our experiments show that the proposedmethods reduce hubness, and significantly improves transductive FSL accuracyfor a wide range of classifiers."
Remote Task-oriented Grasp Area Teaching By Non-Experts through  Interactive Segmentation and Few-Shot Learning,"['Furkan Kaynar', 'Sudarshan Rajagopalan', 'Shaobo Zhou', 'Eckehard Steinbach']",http://arxiv.org/pdf/2303.10195v1.pdf,2023-03-17,"['cs.cv', 'cs.ro']","  A robot operating in unstructured environments must be able to discriminatebetween different grasping styles depending on the prospective manipulationtask. Having a system that allows learning from remote non-expertdemonstrations can very feasibly extend the cognitive skills of a robot fortask-oriented grasping. We propose a novel two-step framework towards this aim.The first step involves grasp area estimation by segmentation. We receive grasparea demonstrations for a new task via interactive segmentation, and learn fromthese few demonstrations to estimate the required grasp area on an unseen scenefor the given task. The second step is autonomous grasp estimation in thesegmented region. To train the segmentation network for few-shot learning, webuilt a grasp area segmentation (GAS) dataset with 10089 images grouped into1121 segmentation tasks. We benefit from an efficient meta learning algorithmfor training for few-shot adaptation. Experimental evaluation showed that ourmethod successfully detects the correct grasp area on the respective objects inunseen test scenes and effectively allows remote teaching of new graspstrategies by non-experts."
Few-Shot 3D Point Cloud Semantic Segmentation via Stratified  Class-Specific Attention Based Transformer Network,"['Canyu Zhang', 'Zhenyao Wu', 'Xinyi Wu', 'Ziyu Zhao', 'Song Wang']",http://arxiv.org/pdf/2303.15654v1.pdf,2023-03-28,['cs.cv'],"  3D point cloud semantic segmentation aims to group all points into differentsemantic categories, which benefits important applications such as point cloudscene reconstruction and understanding. Existing supervised point cloudsemantic segmentation methods usually require large-scale annotated pointclouds for training and cannot handle new categories. While a few-shot learningmethod was proposed recently to address these two problems, it suffers fromhigh computational complexity caused by graph construction and inability tolearn fine-grained relationships among points due to the use of poolingoperations. In this paper, we further address these problems by developing anew multi-layer transformer network for few-shot point cloud semanticsegmentation. In the proposed network, the query point cloud features areaggregated based on the class-specific support features in different scales.Without using pooling operations, our method makes full use of all pixel-levelfeatures from the support samples. By better leveraging the support featuresfor few-shot learning, the proposed method achieves the new state-of-the-artperformance, with 15\% less inference time, over existing few-shot 3D pointcloud segmentation models on the S3DIS dataset and the ScanNet dataset."
LSFSL: Leveraging Shape Information in Few-shot Learning,"['Deepan Chakravarthi Padmanabhan', 'Shruthi Gowda', 'Elahe Arani', 'Bahram Zonooz']",http://arxiv.org/pdf/2304.06672v1.pdf,2023-04-13,"['cs.cv', 'cs.ai']","  Few-shot learning (FSL) techniques seek to learn the underlying patterns indata using fewer samples, analogous to how humans learn from limitedexperience. In this limited-data scenario, the challenges associated with deepneural networks, such as shortcut learning and texture bias behaviors, arefurther exacerbated. Moreover, the significance of addressing shortcut learningis not yet fully explored in the few-shot setup. To address these issues, wepropose LSFSL, which enforces the model to learn more generalizable featuresutilizing the implicit prior information present in the data. Throughcomprehensive analyses, we demonstrate that LSFSL-trained models are lessvulnerable to alteration in color schemes, statistical correlations, andadversarial perturbations leveraging the global semantics in the data. Ourfindings highlight the potential of incorporating relevant priors in few-shotapproaches to increase robustness and generalization."
The Art of Camouflage: Few-shot Learning for Animal Detection and  Segmentation,"['Thanh-Danh Nguyen', 'Anh-Khoa Nguyen Vu', 'Nhat-Duy Nguyen', 'Vinh-Tiep Nguyen', 'Thanh Duc Ngo', 'Thanh-Toan Do', 'Minh-Triet Tran', 'Tam V. Nguyen']",http://arxiv.org/pdf/2304.07444v2.pdf,2023-04-15,['cs.cv'],"  Camouflaged object detection and segmentation is a new and challengingresearch topic in computer vision. There is a serious issue of lacking data ofcamouflaged objects such as camouflaged animals in natural scenes. In thispaper, we address the problem of few-shot learning for camouflaged objectdetection and segmentation. To this end, we first collect a new dataset,CAMO-FS, for the benchmark. We then propose a novel method to efficientlydetect and segment the camouflaged objects in the images. In particular, weintroduce the instance triplet loss and the instance memory storage. Theextensive experiments demonstrated that our proposed method achievesstate-of-the-art performance on the newly collected dataset."
CancerGPT: Few-shot Drug Pair Synergy Prediction using Large Pre-trained  Language Models,"['Tianhao Li', 'Sandesh Shetty', 'Advaith Kamath', 'Ajay Jaiswal', 'Xianqian Jiang', 'Ying Ding', 'Yejin Kim']",http://arxiv.org/pdf/2304.10946v1.pdf,2023-04-18,"['cs.cl', 'cs.lg', 'q-bio.bm']","  Large pre-trained language models (LLMs) have been shown to have significantpotential in few-shot learning across various fields, even with minimaltraining data. However, their ability to generalize to unseen tasks in morecomplex fields, such as biology, has yet to be fully evaluated. LLMs can offera promising alternative approach for biological inference, particularly incases where structured data and sample size are limited, by extracting priorknowledge from text corpora. Our proposed few-shot learning approach uses LLMsto predict the synergy of drug pairs in rare tissues that lack structured dataand features. Our experiments, which involved seven rare tissues from differentcancer types, demonstrated that the LLM-based prediction model achievedsignificant accuracy with very few or zero samples. Our proposed model, theCancerGPT (with $\sim$ 124M parameters), was even comparable to the largerfine-tuned GPT-3 model (with $\sim$ 175B parameters). Our research is the firstto tackle drug pair synergy prediction in rare tissues with limited data. Weare also the first to utilize an LLM-based prediction model for biologicalreaction prediction tasks."
Accelerating Neural Self-Improvement via Bootstrapping,"['Kazuki Irie', 'Jürgen Schmidhuber']",http://arxiv.org/pdf/2305.01547v1.pdf,2023-05-02,['cs.lg'],"  Few-shot learning with sequence-processing neural networks (NNs) has recentlyattracted a new wave of attention in the context of large language models. Inthe standard N-way K-shot learning setting, an NN is explicitly optimised tolearn to classify unlabelled inputs by observing a sequence of NK labelledexamples. This pressures the NN to learn a learning algorithm that achievesoptimal performance, given the limited number of training examples. Here westudy an auxiliary loss that encourages further acceleration of few-shotlearning, by applying recently proposed bootstrapped meta-learning to NNfew-shot learners: we optimise the K-shot learner to match its own performanceachievable by observing more than NK examples, using only NK examples.Promising results are obtained on the standard Mini-ImageNet dataset. Our codeis public."
Meta-DM: Applications of Diffusion Models on Few-Shot Learning,"['Wentao Hu', 'Xiurong Jiang', 'Jiarun Liu', 'Yuqi Yang', 'Hui Tian']",http://arxiv.org/pdf/2305.08092v1.pdf,2023-05-14,"['cs.lg', 'cs.cv']","  In the field of few-shot learning (FSL), extensive research has focused onimproving network structures and training strategies. However, the role of dataprocessing modules has not been fully explored. Therefore, in this paper, wepropose Meta-DM, a generalized data processing module for FSL problems based ondiffusion models. Meta-DM is a simple yet effective module that can be easilyintegrated with existing FSL methods, leading to significant performanceimprovements in both supervised and unsupervised settings. We provide atheoretical analysis of Meta-DM and evaluate its performance on severalalgorithms. Our experiments show that combining Meta-DM with certain methodsachieves state-of-the-art results."
Automated Few-shot Classification with Instruction-Finetuned Language  Models,"['Rami Aly', 'Xingjian Shi', 'Kaixiang Lin', 'Aston Zhang', 'Andrew Gordon Wilson']",http://arxiv.org/pdf/2305.12576v2.pdf,2023-05-21,['cs.cl'],"  A particularly successful class of approaches for few-shot learning combineslanguage models with prompts -- hand-crafted task descriptions that complementdata samples. However, designing prompts by hand for each task commonlyrequires domain knowledge and substantial guesswork. We observe, in the contextof classification tasks, that instruction finetuned language models exhibitremarkable prompt robustness, and we subsequently propose a simple method toeliminate the need for handcrafted prompts, named AuT-Few. This approachconsists of (i) a prompt retrieval module that selects suitable taskinstructions from the instruction-tuning knowledge base, and (ii) thegeneration of two distinct, semantically meaningful, class descriptions and aselection mechanism via cross-validation. Over $12$ datasets, spanning $8$classification tasks, we show that AuT-Few outperforms current state-of-the-artfew-shot learning methods. Moreover, AuT-Few is the best ranking method acrossdatasets on the RAFT few-shot benchmark. Notably, these results are achievedwithout task-specific handcrafted prompts on unseen tasks."
Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts,"['Mohna Chakraborty', 'Adithya Kulkarni', 'Qi Li']",http://arxiv.org/pdf/2305.15689v2.pdf,2023-05-25,"['cs.cl', 'cs.ai']","  Recent studies have demonstrated that natural-language prompts can help toleverage the knowledge learned by pre-trained language models for the binarysentence-level sentiment classification task. Specifically, these methodsutilize few-shot learning settings to fine-tune the sentiment classificationmodel using manual or automatically generated prompts. However, the performanceof these methods is sensitive to the perturbations of the utilized prompts.Furthermore, these methods depend on a few labeled instances for automaticprompt generation and prompt ranking. This study aims to find high-qualityprompts for the given task in a zero-shot setting. Given a base prompt, ourproposed approach automatically generates multiple prompts similar to the baseprompt employing positional, reasoning, and paraphrasing techniques and thenranks the prompts using a novel metric. We empirically demonstrate that thetop-ranked prompts are high-quality and significantly outperform the baseprompt and the prompts generated using few-shot learning for the binarysentence-level sentiment classification task."
Instance-based Max-margin for Practical Few-shot Recognition,"['Minghao Fu', 'Ke Zhu', 'Jianxin Wu']",http://arxiv.org/pdf/2305.17368v1.pdf,2023-05-27,"['cs.cv', 'cs.ai']","  In order to mimic the human few-shot learning (FSL) ability better and tomake FSL closer to real-world applications, this paper proposes a practical FSL(pFSL) setting. pFSL is based on unsupervised pretrained models (analogous tohuman prior knowledge) and recognizes many novel classes simultaneously.Compared to traditional FSL, pFSL is simpler in its formulation, easier toevaluate, more challenging and more practical. To cope with the rarity oftraining examples, this paper proposes IbM2, an instance-based max-marginmethod not only for the new pFSL setting, but also works well in traditionalFSL scenarios. Based on the Gaussian Annulus Theorem, IbM2 converts randomnoise applied to the instances into a mechanism to achieve maximum margin inthe many-way pFSL (or traditional FSL) recognition task. Experiments withvarious self-supervised pretraining methods and diverse many- or few-way FSLtasks show that IbM2 almost always leads to improvements compared to itsrespective baseline methods, and in most cases the improvements aresignificant. With both the new pFSL setting and novel IbM2 method, this papershows that practical few-shot learning is both viable and promising."
FLamE: Few-shot Learning from Natural Language Explanations,"['Yangqiaoyu Zhou', 'Yiming Zhang', 'Chenhao Tan']",http://arxiv.org/pdf/2306.08042v1.pdf,2023-06-13,"['cs.cl', 'cs.ai']","  Natural language explanations have the potential to provide rich informationthat in principle guides model reasoning. Yet, recent work by Lampinen et al.(2022) has shown limited utility of natural language explanations in improvingclassification. To effectively learn from explanations, we present FLamE, atwo-stage few-shot learning framework that first generates explanations usingGPT-3, and then finetunes a smaller model (e.g., RoBERTa) with generatedexplanations. Our experiments on natural language inference demonstrateeffectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3Babbage and 5.7% over GPT-3 Davinci in e-SNLI. Despite improving classificationperformance, human evaluation surprisingly reveals that the majority ofgenerated explanations does not adequately justify classification decisions.Additional analyses point to the important role of label-specific cues (e.g.,""not know"" for the neutral label) in generated explanations."
Mutually Guided Few-shot Learning for Relational Triple Extraction,"['Chengmei Yang', 'Shuai Jiang', 'Bowei He', 'Chen Ma', 'Lianghua He']",http://arxiv.org/pdf/2306.13310v1.pdf,2023-06-23,"['cs.cl', 'cs.ai']","  Knowledge graphs (KGs), containing many entity-relation-entity triples,provide rich information for downstream applications. Although extractingtriples from unstructured texts has been widely explored, most of them requirea large number of labeled instances. The performance will drop dramaticallywhen only few labeled data are available. To tackle this problem, we proposethe Mutually Guided Few-shot learning framework for Relational TripleExtraction (MG-FTE). Specifically, our method consists of an entity-guidedrelation proto-decoder to classify the relations firstly and a relation-guidedentity proto-decoder to extract entities based on the classified relations. Todraw the connection between entity and relation, we design a proto-level fusionmodule to boost the performance of both entity extraction and relationclassification. Moreover, a new cross-domain few-shot triple extraction task isintroduced. Extensive experiments show that our method outperforms manystate-of-the-art methods by 12.6 F1 score on FewRel 1.0 (single-domain) and20.5 F1 score on FewRel 2.0 (cross-domain)."
Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners,"['Jihyeon Lee', 'Dain Kim', 'Doohae Jung', 'Boseop Kim', 'Kyoung-Woon On']",http://arxiv.org/pdf/2307.14856v1.pdf,2023-07-27,"['cs.cl', 'cs.ai']","  In-context learning, which offers substantial advantages over fine-tuning, ispredominantly observed in decoder-only models, while encoder-decoder (i.e.,seq2seq) models excel in methods that rely on weight updates. Recently, a fewstudies have demonstrated the feasibility of few-shot learning with seq2seqmodels; however, this has been limited to tasks that align well with theseq2seq architecture, such as summarization and translation. Inspired by theseinitial studies, we provide a first-ever extensive experiment comparing thein-context few-shot learning capabilities of decoder-only and encoder-decodermodels on a broad range of tasks. Furthermore, we propose two methods to moreeffectively elicit in-context learning ability in seq2seq models:objective-aligned prompting and a fusion-based approach. Remarkably, ourapproach outperforms a decoder-only model that is six times larger and exhibitssignificant performance improvements compared to conventional seq2seq modelsacross a variety of settings. We posit that, with the right configuration andprompt design, seq2seq models can be highly effective few-shot learners for awide spectrum of applications."
Prototypes-oriented Transductive Few-shot Learning with Conditional  Transport,"['Long Tian', 'Jingyi Feng', 'Wenchao Chen', 'Xiaoqiang Chai', 'Liming Wang', 'Xiyang Liu', 'Bo Chen']",http://arxiv.org/pdf/2308.03047v1.pdf,2023-08-06,['cs.cv'],"  Transductive Few-Shot Learning (TFSL) has recently attracted increasingattention since it typically outperforms its inductive peer by leveragingstatistics of query samples. However, previous TFSL methods usually encodeuniform prior that all the classes within query samples are equally likely,which is biased in imbalanced TFSL and causes severe performance degradation.  Given this pivotal issue, in this work, we propose a novel ConditionalTransport (CT) based imbalanced TFSL model called {\textbf P}rototypes-oriented{\textbf U}nbiased {\textbf T}ransfer {\textbf M}odel (PUTM) to fully exploitunbiased statistics of imbalanced query samples, which employs forward andbackward navigators as transport matrices to balance the prior of query samplesper class between uniform and adaptive data-driven distributions. Forefficiently transferring statistics learned by CT, we further derive a closedform solution to refine prototypes based on MAP given the learned navigators.The above two steps of discovering and transferring unbiased statistics followan iterative manner, formulating our EM-based solver.  Experimental results on four standard benchmarks including miniImageNet,tieredImageNet, CUB, and CIFAR-FS demonstrate superiority of our model inclass-imbalanced generalization."
Approximating Human-Like Few-shot Learning with GPT-based Compression,"['Cynthia Huang', 'Yuqing Xie', 'Zhiying Jiang', 'Jimmy Lin', 'Ming Li']",http://arxiv.org/pdf/2308.06942v1.pdf,2023-08-14,"['cs.ai', 'cs.cl', 'cs.it', 'math.it']","  In this work, we conceptualize the learning process as informationcompression. We seek to equip generative pre-trained models with human-likelearning capabilities that enable data compression during inference. We presenta novel approach that utilizes the Generative Pre-trained Transformer (GPT) toapproximate Kolmogorov complexity, with the aim of estimating the optimalInformation Distance for few-shot learning. We first propose using GPT as aprior for lossless text compression, achieving a noteworthy compression ratio.Experiment with LLAMA2-7B backbone achieves a compression ratio of 15.5 onenwik9. We justify the pre-training objective of GPT models by demonstratingits equivalence to the compression length, and, consequently, its ability toapproximate the information distance for texts. Leveraging the approximatedinformation distance, our method allows the direct application of GPT models inquantitative text similarity measurements. Experiment results show that ourmethod overall achieves superior performance compared to embedding and promptbaselines on challenging NLP tasks, including semantic similarity, zero andone-shot text classification, and zero-shot text ranking."
COCA: Classifier-Oriented Calibration for Source-Free Universal Domain  Adaptation via Textual Prototype,"['Xinghong Liu', 'Yi Zhou', 'Tao Zhou', 'Chun-Mei Feng', 'Ling Shao']",http://arxiv.org/pdf/2308.10450v1.pdf,2023-08-21,['cs.cv'],"  Universal Domain Adaptation (UniDA) aims to distinguish common and privateclasses between the source and target domains where domain shift exists.Recently, due to more stringent data restrictions, researchers have introducedSource-Free UniDA (SF-UniDA) in more realistic scenarios. SF-UniDA methodseliminate the need for direct access to source samples when performingadaptation to the target domain. However, existing SF-UniDA methods stillrequire an extensive quantity of labeled source samples to train a sourcemodel, resulting in significant labeling costs. To tackle this issue, wepresent a novel Classifier-Oriented Calibration (COCA) method. This method,which leverages textual prototypes, is formulated for the source model based onfew-shot learning. Specifically, we propose studying few-shot learning, usuallyexplored for closed-set scenarios, to identify common and domain-privateclasses despite a significant domain shift between source and target domains.Essentially, we present a novel paradigm based on the vision-language model tolearn SF-UniDA and hugely reduce the labeling costs on the source domain.Experimental results demonstrate that our approach outperforms state-of-the-artUniDA and SF-UniDA models."
VesselShot: Few-shot learning for cerebral blood vessel segmentation,"['Mumu Aktar', 'Hassan Rivaz', 'Marta Kersten-Oertel', 'Yiming Xiao']",http://arxiv.org/pdf/2308.14626v1.pdf,2023-08-28,"['eess.iv', 'cs.cv', 'cs.lg']","  Angiography is widely used to detect, diagnose, and treat cerebrovasculardiseases. While numerous techniques have been proposed to segment the vascularnetwork from different imaging modalities, deep learning (DL) has emerged as apromising approach. However, existing DL methods often depend on proprietarydatasets and extensive manual annotation. Moreover, the availability ofpre-trained networks specifically for medical domains and 3D volumes islimited. To overcome these challenges, we propose a few-shot learning approachcalled VesselShot for cerebrovascular segmentation. VesselShot leveragesknowledge from a few annotated support images and mitigates the scarcity oflabeled data and the need for extensive annotation in cerebral blood vesselsegmentation. We evaluated the performance of VesselShot using the publiclyavailable TubeTK dataset for the segmentation task, achieving a mean Dicecoefficient (DC) of 0.62(0.03)."
Generalized Cross-domain Multi-label Few-shot Learning for Chest X-rays,"['Aroof Aimen', 'Arsh Verma', 'Makarand Tapaswi', 'Narayanan C. Krishnan']",http://arxiv.org/pdf/2309.04462v1.pdf,2023-09-08,['cs.cv'],"  Real-world application of chest X-ray abnormality classification requiresdealing with several challenges: (i) limited training data; (ii) training andevaluation sets that are derived from different domains; and (iii) classes thatappear during training may have partial overlap with classes of interest duringevaluation. To address these challenges, we present an integrated frameworkcalled Generalized Cross-Domain Multi-Label Few-Shot Learning (GenCDML-FSL).The framework supports overlap in classes during training and evaluation,cross-domain transfer, adopts meta-learning to learn using few trainingsamples, and assumes each chest X-ray image is either normal or associated withone or more abnormalities. Furthermore, we propose Generalized EpisodicTraining (GenET), a training strategy that equips models to operate withmultiple challenges observed in the GenCDML-FSL scenario. Comparisons withwell-established methods such as transfer learning, hybrid transfer learning,and multi-label meta-learning on multiple datasets show the superiority of ourapproach."
Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories  of Articulated Objects,"['Chuanruo Ning', 'Ruihai Wu', 'Haoran Lu', 'Kaichun Mo', 'Hao Dong']",http://arxiv.org/pdf/2309.07473v2.pdf,2023-09-14,"['cs.ro', 'cs.ai']","  Articulated object manipulation is a fundamental yet challenging task inrobotics. Due to significant geometric and semantic variations across objectcategories, previous manipulation models struggle to generalize to novelcategories. Few-shot learning is a promising solution for alleviating thisissue by allowing robots to perform a few interactions with unseen objects.However, extant approaches often necessitate costly and inefficient test-timeinteractions with each unseen instance. Recognizing this limitation, we observethat despite their distinct shapes, different categories often share similarlocal geometries essential for manipulation, such as pullable handles andgraspable edges - a factor typically underutilized in previous few-shotlearning works. To harness this commonality, we introduce 'Where2Explore', anaffordance learning framework that effectively explores novel categories withminimal interactions on a limited number of instances. Our framework explicitlyestimates the geometric similarity across different categories, identifyinglocal areas that differ from shapes in the training categories for efficientexploration while concurrently transferring affordance knowledge to similarparts of the objects. Extensive experiments in simulated and real-worldenvironments demonstrate our framework's capacity for efficient few-shotexploration and generalization."
Evaluating the Decency and Consistency of Data Validation Tests  Generated by LLMs,"['Rohan Alexander', 'Lindsay Katz', 'Callandra Moore', 'Zane Schwartz']",http://arxiv.org/pdf/2310.01402v1.pdf,2023-10-02,['stat.me'],"  We investigated the potential of large language models (LLMs) in developingdataset validation tests. We carried out 96 experiments each for both GPT-3.5and GPT-4, examining different prompt scenarios, learning modes, temperaturesettings, and roles. The prompt scenarios were: 1) Asking for expectations, 2)Asking for expectations with a given context, 3) Asking for expectations afterrequesting a simulation, and 4) Asking for expectations with a provided datasample. For learning modes, we tested: 1) zero-shot, 2) one-shot, and 3)few-shot learning. We also tested four temperature settings: 0, 0.4, 0.6, and1. Furthermore, two distinct roles were considered: 1) ""helpful assistant"", 2)""expert data scientist"". To gauge consistency, every setup was tested fivetimes. The LLM-generated responses were benchmarked against a gold standardsuite, created by an experienced data scientist knowledgeable about the data inquestion. We find there are considerable returns to the use of few-shotlearning, and that the more explicit the data setting can be the better. Thebest LLM configurations complement, rather than substitute, the gold standardresults. This study underscores the value LLMs can bring to the data cleaningand preparation stages of the data science workflow."
PatchProto Networks for Few-shot Visual Anomaly Classification,"['Jian Wang', 'Yue Zhuo']",http://arxiv.org/pdf/2310.04688v1.pdf,2023-10-07,['cs.cv'],"  The visual anomaly diagnosis can automatically analyze the defectiveproducts, which has been widely applied in industrial quality inspection. Theanomaly classification can classify the defective products into differentcategories. However, the anomaly samples are hard to access in practice, whichimpedes the training of canonical machine learning models. This paper studies apractical issue that anomaly samples for training are extremely scarce, i.e.,few-shot learning (FSL). Utilizing the sufficient normal samples, we proposePatchProto networks for few-shot anomaly classification. Different fromclassical FSL methods, PatchProto networks only extract CNN features ofdefective regions of interest, which serves as the prototypes for few-shotlearning. Compared with basic few-shot classifier, the experiment results onMVTec-AD dataset show PatchProto networks significantly improve the few-shotanomaly classification accuracy."
Explainable Attention for Few-shot Learning and Beyond,"['Bahareh Nikpour', 'Narges Armanfard']",http://arxiv.org/pdf/2310.07800v1.pdf,2023-10-11,"['cs.ai', 'cs.lg']","  Attention mechanisms have exhibited promising potential in enhancing learningmodels by identifying salient portions of input data. This is particularlyvaluable in scenarios where limited training samples are accessible due tochallenges in data collection and labeling. Drawing inspiration from humanrecognition processes, we posit that an AI baseline's performance could be moreaccurate and dependable if it is exposed to essential segments of raw datarather than the entire input dataset, akin to human perception. However, thetask of selecting these informative data segments, referred to as hardattention finding, presents a formidable challenge. In situations with fewtraining samples, existing studies struggle to locate such informative regionsdue to the large number of training parameters that cannot be effectivelylearned from the available limited samples. In this study, we introduce a noveland practical framework for achieving explainable hard attention finding,specifically tailored for few-shot learning scenarios, called FewXAT. Ourapproach employs deep reinforcement learning to implement the concept of hardattention, directly impacting raw input data and thus rendering the processinterpretable for human understanding. Through extensive experimentation acrossvarious benchmark datasets, we demonstrate the efficacy of our proposed method."
CrisisMatch: Semi-Supervised Few-Shot Learning for Fine-Grained Disaster  Tweet Classification,"['Henry Peng Zou', 'Yue Zhou', 'Cornelia Caragea', 'Doina Caragea']",http://arxiv.org/pdf/2310.14627v1.pdf,2023-10-23,"['cs.cl', 'cs.lg']","  The shared real-time information about natural disasters on social mediaplatforms like Twitter and Facebook plays a critical role in informingvolunteers, emergency managers, and response organizations. However, supervisedlearning models for monitoring disaster events require large amounts ofannotated data, making them unrealistic for real-time use in disaster events.To address this challenge, we present a fine-grained disaster tweetclassification model under the semi-supervised, few-shot learning setting whereonly a small number of annotated data is required. Our model, CrisisMatch,effectively classifies tweets into fine-grained classes of interest using fewlabeled data and large amounts of unlabeled data, mimicking the early stage ofa disaster. Through integrating effective semi-supervised learning ideas andincorporating TextMixUp, CrisisMatch achieves performance improvement on twodisaster datasets of 11.2\% on average. Further analyses are also provided forthe influence of the number of labeled data and out-of-domain results."
Improving generalization in large language models by learning prefix  subspaces,"['Louis Falissard', 'Vincent Guigue', 'Laure Soulier']",http://arxiv.org/pdf/2310.15793v1.pdf,2023-10-24,"['cs.lg', 'cs.ai', 'cs.cl']","  This article focuses on large language models (LLMs) fine-tuning in thescarce data regime (also known as the ""few-shot"" learning setting). We proposea method to increase the generalization capabilities of LLMs based on neuralnetwork subspaces. This optimization method, recently introduced in computervision, aims to improve model generalization by identifying wider local optimathrough the joint optimization of an entire simplex of models in parameterspace. Its adaptation to massive, pretrained transformers, however, poses somechallenges. First, their considerable number of parameters makes it difficultto train several models jointly, and second, their deterministic parameterinitialization schemes make them unfit for the subspace method as originallyproposed. We show in this paper that ""Parameter Efficient Fine-Tuning"" (PEFT)methods, however, are perfectly compatible with this original approach, andpropose to learn entire simplex of continuous prefixes. We test our method on avariant of the GLUE benchmark adapted to the few-shot learning setting, andshow that both our contributions jointly lead to a gain in average performancescompared to sota methods. The implementation can be found at the followinglink: https://github.com/Liloulou/prefix_subspace"
SELF-EXPLAIN: Teaching Large Language Models to Reason Complex Questions  by Themselves,"['Jiachen Zhao', 'Zonghai Yao', 'Zhichao Yang', 'Hong Yu']",http://arxiv.org/pdf/2311.06985v1.pdf,2023-11-12,['cs.cl'],"  Large language models (LLMs) can generate intermediate reasoning steps. Toelicit the reliable reasoning, the common practice is to employ few-shotchain-of-thought prompting, where several in-context demonstrations forreasoning are prepended to the question. However, such chain-of-thoughtexamples are expensive to craft, especially for professional domains, and canhave high variance depending on human annotators. Therefore, this workinvestigates whether LLMs can teach themselves to reason without human-crafteddemonstrations. We propose SELF-EXPLAIN to generate CoT examples by LLMsinspired by ""encoding specificity"" in human memory retrieval. We find usingself-explanations makes LLMs more confident, more calibrated and less biasedwhen answering complex questions. Moreover, we find prompting withself-explanations can even significantly outperform using human-crafted CoTs onseveral complex question answering dataset."
Selecting Shots for Demographic Fairness in Few-Shot Learning with Large  Language Models,"['Carlos Aguirre', 'Kuleen Sasse', 'Isabel Cachola', 'Mark Dredze']",http://arxiv.org/pdf/2311.08472v1.pdf,2023-11-14,['cs.cl'],"  Recently, work in NLP has shifted to few-shot (in-context) learning, withlarge language models (LLMs) performing well across a range of tasks. However,while fairness evaluations have become a standard for supervised methods,little is known about the fairness of LLMs as prediction systems. Further,common standard methods for fairness involve access to models weights or areapplied during finetuning, which are not applicable in few-shot learning. DoLLMs exhibit prediction biases when used for standard NLP tasks? In this work,we explore the effect of shots, which directly affect the performance ofmodels, on the fairness of LLMs as NLP classification systems. We consider howdifferent shot selection strategies, both existing and new demographicallysensitive methods, affect model fairness across three standard fairnessdatasets. We discuss how future work can include LLM fairness evaluations."
When the Few Outweigh the Many: Illicit Content Recognition with  Few-Shot Learning,"['G. Cascavilla', 'G. Catolino', 'M. Conti', 'D. Mellios', 'D. A. Tamburri']",http://arxiv.org/pdf/2311.17026v1.pdf,2023-11-28,"['cs.cv', 'cs.ai', 'cs.cr', 'cs.cy', 'cs.lg']","  The anonymity and untraceability benefits of the Dark web account for theexponentially-increased potential of its popularity while creating a suitablewomb for many illicit activities, to date. Hence, in collaboration withcybersecurity and law enforcement agencies, research has provided approachesfor recognizing and classifying illicit activities with most exploiting textualdark web markets' content recognition; few such approaches use images thatoriginated from dark web content. This paper investigates this alternativetechnique for recognizing illegal activities from images. In particular, weinvestigate label-agnostic learning techniques like One-Shot and Few-Shotlearning featuring the use Siamese neural networks, a state-of-the-art approachin the field. Our solution manages to handle small-scale datasets withpromising accuracy. In particular, Siamese neural networks reach 90.9% on20-Shot experiments over a 10-class dataset; this leads us to conclude thatsuch models are a promising and cheaper alternative to the definition ofautomated law-enforcing machinery over the dark web."
A transductive few-shot learning approach for classification of digital  histopathological slides from liver cancer,"['Aymen Sadraoui', 'Ségolène Martin', 'Eliott Barbot', 'Astrid Laurent-Bellue', 'Jean-Christophe Pesquet', 'Catherine Guettier', 'Ismail Ben Ayed']",http://arxiv.org/pdf/2311.17740v1.pdf,2023-11-29,"['eess.iv', 'cs.lg', 'q-bio.to']","  This paper presents a new approach for classifying 2D histopathology patchesusing few-shot learning. The method is designed to tackle a significantchallenge in histopathology, which is the limited availability of labeled data.By applying a sliding window technique to histopathology slides, we illustratethe practical benefits of transductive learning (i.e., making joint predictionson patches) to achieve consistent and accurate classification. Our approachinvolves an optimization-based strategy that actively penalizes the predictionof a large number of distinct classes within each window. We conductedexperiments on histopathological data to classify tissue classes in digitalslides of liver cancer, specifically hepatocellular carcinoma. The initialresults show the effectiveness of our method and its potential to enhance theprocess of automated cancer diagnosis and treatment, all while reducing thetime and effort required for expert annotation."
Adaptive Weighted Co-Learning for Cross-Domain Few-Shot Learning,"['Abdullah Alchihabi', 'Marzi Heidari', 'Yuhong Guo']",http://arxiv.org/pdf/2312.03928v1.pdf,2023-12-06,['cs.lg'],"  Due to the availability of only a few labeled instances for the novel targetprediction task and the significant domain shift between the well annotatedsource domain and the target domain, cross-domain few-shot learning (CDFSL)induces a very challenging adaptation problem. In this paper, we propose asimple Adaptive Weighted Co-Learning (AWCoL) method to address the CDFSLchallenge by adapting two independently trained source prototypicalclassification models to the target task in a weighted co-learning manner. Theproposed method deploys a weighted moving average prediction strategy togenerate probabilistic predictions from each model, and then conducts adaptiveco-learning by jointly fine-tuning the two models in an alternating mannerbased on the pseudo-labels and instance weights produced from the predictions.Moreover, a negative pseudo-labeling regularizer is further deployed to improvethe fine-tuning process by penalizing false predictions. Comprehensiveexperiments are conducted on multiple benchmark datasets and the empiricalresults demonstrate that the proposed method produces state-of-the-art CDFSLperformance."
Representation based and Attention augmented Meta learning,"['Yunxiao Qin', 'Chenxu Zhao', 'Zezheng Wang', 'Junliang Xing', 'Jun Wan', 'Zhen Lei']",http://arxiv.org/pdf/1811.07545v3.pdf,2018-11-19,"['cs.lg', 'cs.ai']","  Deep learning based computer vision fails to work when labeled images arescarce. Recently, Meta learning algorithm has been confirmed as a promising wayto improve the ability of learning from few images for computer vision.However, previous Meta learning approaches expose problems:  1) they ignored the importance of attention mechanism for the Meta learner;  2) they didn't give the Meta learner the ability of well using the pastknowledge which can help to express images into high representations, resultingin that the Meta learner has to solve few shot learning task directly from theoriginal high dimensional RGB images.  In this paper, we argue that the attention mechanism and the past knowledgeare crucial for the Meta learner, and the Meta learner should be trained onhigh representations of the RGB images instead of directly on the originalones. Based on these arguments, we propose two methods: Attention augmentedMeta Learning (AML) and Representation based and Attention augmented MetaLearning(RAML). The method AML aims to improve the Meta learner's attentionability by explicitly embedding an attention model into its network. The methodRAML aims to give the Meta learner the ability of leveraging the past learnedknowledge to reduce the dimension of the original input data by expressing itinto high representations, and help the Meta learner to perform well. Extensiveexperiments demonstrate the effectiveness of the proposed models, withstate-of-the-art few shot learning performances on several few shot learningbenchmarks. The source code of our proposed methods will be released soon tofacilitate further studies on those aforementioned problem."
TGG: Transferable Graph Generation for Zero-shot and Few-shot Learning,"['Chenrui Zhang', 'Xiaoqing Lyu', 'Zhi Tang']",http://arxiv.org/pdf/1908.11503v1.pdf,2019-08-30,"['cs.lg', 'stat.ml']","  Zero-shot and few-shot learning aim to improve generalization to unseenconcepts, which are promising in many realistic scenarios. Due to the lack ofdata in unseen domain, relation modeling between seen and unseen domains isvital for knowledge transfer in these tasks. Most existing methods captureseen-unseen relation implicitly via semantic embedding or feature generation,resulting in inadequate use of relation and some issues remain (e.g. domainshift). To tackle these challenges, we propose a Transferable Graph Generation(TGG) approach, in which the relation is modeled and utilized explicitly viagraph generation. Specifically, our proposed TGG contains two main components:(1) Graph generation for relation modeling. An attention-based aggregatenetwork and a relation kernel are proposed, which generate instance-level graphbased on a class-level prototype graph and visual features. Proximityinformation aggregating is guided by a multi-head graph attention mechanism,where seen and unseen features synthesized by GAN are revised as nodeembeddings. The relation kernel further generates edges with GCN and graphkernel method, to capture instance-level topological structure while tacklingdata imbalance and noise. (2) Relation propagation for relation utilization. Adual relation propagation approach is proposed, where relations captured by thegenerated graph are separately propagated from the seen and unseen subgraphs.The two propagations learn from each other in a dual learning fashion, whichperforms as an adaptation way for mitigating domain shift. All components arejointly optimized with a meta-learning strategy, and our TGG acts as anend-to-end framework unifying conventional zero-shot, generalized zero-shot andfew-shot learning. Extensive experiments demonstrate that it consistentlysurpasses existing methods of the above three fields by a significant margin."
Meta-Transfer Learning through Hard Tasks,"['Qianru Sun', 'Yaoyao Liu', 'Zhaozheng Chen', 'Tat-Seng Chua', 'Bernt Schiele']",http://arxiv.org/pdf/1910.03648v1.pdf,2019-10-07,"['cs.cv', 'cs.lg', 'stat.ml']","  Meta-learning has been proposed as a framework to address the challengingfew-shot learning setting. The key idea is to leverage a large number ofsimilar few-shot tasks in order to learn how to adapt a base-learner to a newtask for which only a few labeled samples are available. As deep neuralnetworks (DNNs) tend to overfit using a few samples only, typical meta-learningmodels use shallow neural networks, thus limiting its effectiveness. In orderto achieve top performance, some recent works tried to use the DNNs pre-trainedon large-scale datasets but mostly in straight-forward manners, e.g., (1)taking their weights as a warm start of meta-training, and (2) freezing theirconvolutional layers as the feature extractor of base-learners. In this paper,we propose a novel approach called meta-transfer learning (MTL) which learns totransfer the weights of a deep NN for few-shot learning tasks. Specifically,meta refers to training multiple tasks, and transfer is achieved by learningscaling and shifting functions of DNN weights for each task. In addition, weintroduce the hard task (HT) meta-batch scheme as an effective learningcurriculum that further boosts the learning efficiency of MTL. We conductfew-shot learning experiments and report top performance for five-classfew-shot recognition tasks on three challenging benchmarks: miniImageNet,tieredImageNet and Fewshot-CIFAR100 (FC100). Extensive comparisons to relatedworks validate that our MTL approach trained with the proposed HT meta-batchscheme achieves top performance. An ablation study also shows that bothcomponents contribute to fast convergence and high accuracy."
Adapted Deep Embeddings: A Synthesis of Methods for $k$-Shot Inductive  Transfer Learning,"['Tyler R. Scott', 'Karl Ridgeway', 'Michael C. Mozer']",http://arxiv.org/pdf/1805.08402v4.pdf,2018-05-22,"['cs.lg', 'stat.ml']","  The focus in machine learning has branched beyond training classifiers on asingle task to investigating how previously acquired knowledge in a sourcedomain can be leveraged to facilitate learning in a related target domain,known as inductive transfer learning. Three active lines of research haveindependently explored transfer learning using neural networks. In weighttransfer, a model trained on the source domain is used as an initializationpoint for a network to be trained on the target domain. In deep metriclearning, the source domain is used to construct an embedding that capturesclass structure in both the source and target domains. In few-shot learning,the focus is on generalizing well in the target domain based on a limitednumber of labeled examples. We compare state-of-the-art methods from thesethree paradigms and also explore hybrid adapted-embedding methods that uselimited target-domain data to fine tune embeddings constructed fromsource-domain data. We conduct a systematic comparison of methods in a varietyof domains, varying the number of labeled instances available in the targetdomain ($k$), as well as the number of target-domain classes. We reach threeprincipal conclusions: (1) Deep embeddings are far superior, compared to weighttransfer, as a starting point for inter-domain transfer or model re-use (2) Ourhybrid methods robustly outperform every few-shot learning and every deepmetric learning method previously proposed, with a mean error reduction of 34%over state-of-the-art. (3) Among loss functions for discovering embeddings, thehistogram loss (Ustinova & Lempitsky, 2016) is most robust. We hope our resultswill motivate a unification of research in weight transfer, deep metriclearning, and few-shot learning."
Dynamic Distillation Network for Cross-Domain Few-Shot Recognition with  Unlabeled Data,"['Ashraful Islam', 'Chun-Fu Chen', 'Rameswar Panda', 'Leonid Karlinsky', 'Rogerio Feris', 'Richard J. Radke']",http://arxiv.org/pdf/2106.07807v3.pdf,2021-06-14,['cs.cv'],"  Most existing works in few-shot learning rely on meta-learning the network ona large base dataset which is typically from the same domain as the targetdataset. We tackle the problem of cross-domain few-shot learning where there isa large shift between the base and target domain. The problem of cross-domainfew-shot recognition with unlabeled target data is largely unaddressed in theliterature. STARTUP was the first method that tackles this problem usingself-training. However, it uses a fixed teacher pretrained on a labeled basedataset to create soft labels for the unlabeled target samples. As the basedataset and unlabeled dataset are from different domains, projecting the targetimages in the class-domain of the base dataset with a fixed pretrained modelmight be sub-optimal. We propose a simple dynamic distillation-based approachto facilitate unlabeled images from the novel/base dataset. We imposeconsistency regularization by calculating predictions from the weakly-augmentedversions of the unlabeled images from a teacher network and matching it withthe strongly augmented versions of the same images from a student network. Theparameters of the teacher network are updated as exponential moving average ofthe parameters of the student network. We show that the proposed network learnsrepresentation that can be easily adapted to the target domain even though ithas not been trained with target-specific classes during the pretraining phase.Our model outperforms the current state-of-the art method by 4.4% for 1-shotand 3.6% for 5-shot classification in the BSCD-FSL benchmark, and also showscompetitive performance on traditional in-domain few-shot learning task."
A Few-shot Learning Graph Multi-Trajectory Evolution Network for  Forecasting Multimodal Baby Connectivity Development from a Baseline  Timepoint,"['Alaa Bessadok', 'Ahmed Nebli', 'Mohamed Ali Mahjoub', 'Gang Li', 'Weili Lin', 'Dinggang Shen', 'Islem Rekik']",http://arxiv.org/pdf/2110.03535v1.pdf,2021-10-06,"['q-bio.nc', 'cs.cv', 'cs.lg']","  Charting the baby connectome evolution trajectory during the first year afterbirth plays a vital role in understanding dynamic connectivity development ofbaby brains. Such analysis requires acquisition of longitudinal connectomicdatasets. However, both neonatal and postnatal scans are rarely acquired due tovarious difficulties. A small body of works has focused on predicting babybrain evolution trajectory from a neonatal brain connectome derived from asingle modality. Although promising, large training datasets are essential toboost model learning and to generalize to a multi-trajectory prediction fromdifferent modalities (i.e., functional and morphological connectomes). Here, weunprecedentedly explore the question: Can we design a few-shot learning-basedframework for predicting brain graph trajectories across different modalities?To this aim, we propose a Graph Multi-Trajectory Evolution Network (GmTE-Net),which adopts a teacher-student paradigm where the teacher network learns onpure neonatal brain graphs and the student network learns on simulated braingraphs given a set of different timepoints. To the best of our knowledge, thisis the first teacher-student architecture tailored for brain graphmulti-trajectory growth prediction that is based on few-shot learning andgeneralized to graph neural networks (GNNs). To boost the performance of thestudent network, we introduce a local topology-aware distillation loss thatforces the predicted graph topology of the student network to be consistentwith the teacher network. Experimental results demonstrate substantialperformance gains over benchmark methods. Hence, our GmTE-Net can be leveragedto predict atypical brain connectivity trajectory evolution across variousmodalities. Our code is available at https: //github.com/basiralab/GmTE-Net."
Zero-shot and Few-shot Learning with Knowledge Graphs: A Comprehensive  Survey,"['Jiaoyan Chen', 'Yuxia Geng', 'Zhuo Chen', 'Jeff Z. Pan', 'Yuan He', 'Wen Zhang', 'Ian Horrocks', 'Huajun Chen']",http://arxiv.org/pdf/2112.10006v6.pdf,2021-12-18,"['cs.lg', 'cs.ai']","  Machine learning especially deep neural networks have achieved great successbut many of them often rely on a number of labeled samples for supervision. Assufficient labeled training data are not always ready due to e.g., continuouslyemerging prediction targets and costly sample annotation in real worldapplications, machine learning with sample shortage is now being widelyinvestigated. Among all these studies, many prefer to utilize auxiliaryinformation including those in the form of Knowledge Graph (KG) to reduce thereliance on labeled samples. In this survey, we have comprehensively reviewedover 90 papers about KG-aware research for two major sample shortage settings-- zero-shot learning (ZSL) where some classes to be predicted have no labeledsamples, and few-shot learning (FSL) where some classes to be predicted haveonly a small number of labeled samples that are available. We first introduceKGs used in ZSL and FSL as well as their construction methods, and thensystematically categorize and summarize KG-aware ZSL and FSL methods, dividingthem into different paradigms such as the mapping-based, the data augmentation,the propagation-based and the optimization-based. We next present differentapplications, including not only KG augmented prediction tasks such as imageclassification, question answering, text classification and knowledgeextraction, but also KG completion tasks, and some typical evaluation resourcesfor each task. We eventually discuss some challenges and open problems fromdifferent perspectives."
Few-shot Learning with Multilingual Language Models,"['Xi Victoria Lin', 'Todor Mihaylov', 'Mikel Artetxe', 'Tianlu Wang', 'Shuohui Chen', 'Daniel Simig', 'Myle Ott', 'Naman Goyal', 'Shruti Bhosale', 'Jingfei Du', 'Ramakanth Pasunuru', 'Sam Shleifer', 'Punit Singh Koura', 'Vishrav Chaudhary', ""Brian O'Horo"", 'Jeff Wang', 'Luke Zettlemoyer', 'Zornitsa Kozareva', 'Mona Diab', 'Veselin Stoyanov', 'Xian Li']",http://arxiv.org/pdf/2112.10668v3.pdf,2021-12-20,"['cs.cl', 'cs.ai']","  Large-scale generative language models such as GPT-3 are competitive few-shotlearners. While these models are known to be able to jointly represent manydifferent languages, their training data is dominated by English, potentiallylimiting their cross-lingual generalization. In this work, we trainmultilingual generative language models on a corpus covering a diverse set oflanguages, and study their few- and zero-shot learning capabilities in a widerange of tasks. Our largest model with 7.5 billion parameters sets new state ofthe art in few-shot learning in more than 20 representative languages,outperforming GPT-3 of comparable size in multilingual commonsense reasoning(with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in4-shot settings) and natural language inference (+5.4% in each of 0-shot and4-shot settings). On the FLORES-101 machine translation benchmark, our modeloutperforms GPT-3 on 171 out of 182 directions with 32 training examples, whilesurpassing the official supervised baseline in 45 directions. We conduct anin-depth analysis of different multilingual prompting approaches, showing inparticular that strong few-shot learning performance across languages can beachieved via cross-lingual transfer through both templates and demonstrationexamples. Finally, we evaluate our models in social value tasks such as hatespeech detection in five languages and find it has limitations similar tocomparable sized GPT-3 models."
Does MAML Only Work via Feature Re-use? A Data Centric Perspective,"['Brando Miranda', 'Yu-Xiong Wang', 'Sanmi Koyejo']",http://arxiv.org/pdf/2112.13137v1.pdf,2021-12-24,"['cs.lg', 'cs.ai', 'cs.cv', 'cs.ne']","  Recent work has suggested that a good embedding is all we need to solve manyfew-shot learning benchmarks. Furthermore, other work has strongly suggestedthat Model Agnostic Meta-Learning (MAML) also works via this same method - bylearning a good embedding. These observations highlight our lack ofunderstanding of what meta-learning algorithms are doing and when they work. Inthis work, we provide empirical results that shed some light on howmeta-learned MAML representations function. In particular, we identify threeinteresting properties: 1) In contrast to previous work, we show that it ispossible to define a family of synthetic benchmarks that result in a low degreeof feature re-use - suggesting that current few-shot learning benchmarks mightnot have the properties needed for the success of meta-learning algorithms; 2)meta-overfitting occurs when the number of classes (or concepts) are finite,and this issue disappears once the task has an unbounded number of concepts(e.g., online learning); 3) more adaptation at meta-test time with MAML doesnot necessarily result in a significant representation change or even animprovement in meta-test performance - even when training on our proposedsynthetic benchmarks. Finally, we suggest that to understand meta-learningalgorithms better, we must go beyond tracking only absolute performance and, inaddition, formally quantify the degree of meta-learning and track both metricstogether. Reporting results in future work this way will help us identify thesources of meta-overfitting more accurately and help us design more flexiblemeta-learning algorithms that learn beyond fixed feature re-use. Finally, weconjecture the core challenge of re-thinking meta-learning is in the design offew-shot learning data sets and benchmarks - rather than in the algorithms, assuggested by previous work."
GCT: Graph Co-Training for Semi-Supervised Few-Shot Learning,"['Shuai Shao', 'Lei Xing', 'Weifeng Liu', 'Yanjiang Wang', 'Baodi Liu']",http://arxiv.org/pdf/2203.07738v3.pdf,2022-03-15,['cs.cv'],"  Few-shot learning (FSL), purposing to resolve the problem of data-scarce, hasattracted considerable attention in recent years. A popular FSL frameworkcontains two phases: (i) the pre-train phase employs the base data to train aCNN-based feature extractor. (ii) the meta-test phase applies the frozenfeature extractor to novel data (novel data has different categories from basedata) and designs a classifier for recognition. To correct few-shot datadistribution, researchers propose Semi-Supervised Few-Shot Learning (SSFSL) byintroducing unlabeled data. Although SSFSL has been proved to achieveoutstanding performances in the FSL community, there still exists a fundamentalproblem: the pre-trained feature extractor can not adapt to the novel dataflawlessly due to the cross-category setting. Usually, large amounts of noisesare introduced to the novel feature. We dub it as Feature-Extractor-Maladaptive(FEM) problem. To tackle FEM, we make two efforts in this paper. First, wepropose a novel label prediction method, Isolated Graph Learning (IGL). IGLintroduces the Laplacian operator to encode the raw data to graph space, whichhelps reduce the dependence on features when classifying, and then projectgraph representation to label space for prediction. The key point is that: IGLcan weaken the negative influence of noise from the feature representationperspective, and is also flexible to independently complete training andtesting procedures, which is suitable for SSFSL. Second, we propose GraphCo-Training (GCT) to tackle this challenge from a multi-modal fusionperspective by extending the proposed IGL to the co-training framework. GCT isa semi-supervised method that exploits the unlabeled samples with two modalfeatures to crossly strengthen the IGL classifier."
Flamingo: a Visual Language Model for Few-Shot Learning,"['Jean-Baptiste Alayrac', 'Jeff Donahue', 'Pauline Luc', 'Antoine Miech', 'Iain Barr', 'Yana Hasson', 'Karel Lenc', 'Arthur Mensch', 'Katie Millican', 'Malcolm Reynolds', 'Roman Ring', 'Eliza Rutherford', 'Serkan Cabi', 'Tengda Han', 'Zhitao Gong', 'Sina Samangooei', 'Marianne Monteiro', 'Jacob Menick', 'Sebastian Borgeaud', 'Andrew Brock', 'Aida Nematzadeh', 'Sahand Sharifzadeh', 'Mikolaj Binkowski', 'Ricardo Barreira', 'Oriol Vinyals', 'Andrew Zisserman', 'Karen Simonyan']",http://arxiv.org/pdf/2204.14198v2.pdf,2022-04-29,"['cs.cv', 'cs.ai', 'cs.lg']","  Building models that can be rapidly adapted to novel tasks using only ahandful of annotated examples is an open challenge for multimodal machinelearning research. We introduce Flamingo, a family of Visual Language Models(VLM) with this ability. We propose key architectural innovations to: (i)bridge powerful pretrained vision-only and language-only models, (ii) handlesequences of arbitrarily interleaved visual and textual data, and (iii)seamlessly ingest images or videos as inputs. Thanks to their flexibility,Flamingo models can be trained on large-scale multimodal web corpora containingarbitrarily interleaved text and images, which is key to endow them within-context few-shot learning capabilities. We perform a thorough evaluation ofour models, exploring and measuring their ability to rapidly adapt to a varietyof image and video tasks. These include open-ended tasks such as visualquestion-answering, where the model is prompted with a question which it has toanswer; captioning tasks, which evaluate the ability to describe a scene or anevent; and close-ended tasks such as multiple-choice visual question-answering.For tasks lying anywhere on this spectrum, a single Flamingo model can achievea new state of the art with few-shot learning, simply by prompting the modelwith task-specific examples. On numerous benchmarks, Flamingo outperformsmodels fine-tuned on thousands of times more task-specific data."
"Code Generation Tools (Almost) for Free? A Study of Few-Shot,  Pre-Trained Language Models on Code","['Patrick Bareiß', 'Beatriz Souza', ""Marcelo d'Amorim"", 'Michael Pradel']",http://arxiv.org/pdf/2206.01335v2.pdf,2022-06-02,"['cs.se', 'cs.lg']","  Few-shot learning with large-scale, pre-trained language models is a powerfulway to answer questions about code, e.g., how to complete a given code example,or even generate code snippets from scratch. The success of these models raisesthe question whether they could serve as a basis for building a wide range codegeneration tools. Traditionally, such tools are built manually and separatelyfor each task. Instead, few-shot learning may allow to obtain different toolsfrom a single pre-trained language model by simply providing a few examples ora natural language description of the expected tool behavior. This paperstudies to what extent a state-of-the-art, pre-trained language model of code,Codex, may serve this purpose. We consider three code manipulation and codegeneration tasks targeted by a range of traditional tools: (i) code mutation;(ii) test oracle generation from natural language documentation; and (iii) testcase generation. For each task, we compare few-shot learning to a manuallybuilt tool. Our results show that the model-based tools complement (codemutation), are on par (test oracle generation), or even outperform theirrespective traditionally built tool (test case generation), while imposing farless effort to develop them. By comparing the effectiveness of differentvariants of the model-based tools, we provide insights on how to design anappropriate input (""prompt"") to the model and what influence the size of themodel has. For example, we find that providing a small natural languagedescription of the code generation task is an easy way to improve predictions.Overall, we conclude that few-shot language models are surprisingly effective,yet there is still more work to be done, such as exploring more diverse ways ofprompting and tackling even more involved tasks."
From Human Days to Machine Seconds: Automatically Answering and  Generating Machine Learning Final Exams,"['Iddo Drori', 'Sarah J. Zhang', 'Reece Shuttleworth', 'Sarah Zhang', 'Keith Tyser', 'Zad Chin', 'Pedro Lantigua', 'Saisamrit Surbehera', 'Gregory Hunter', 'Derek Austin', 'Leonard Tang', 'Yann Hicke', 'Sage Simhon', 'Sathwik Karnik', 'Darnell Granberry', 'Madeleine Udell']",http://arxiv.org/pdf/2206.05442v7.pdf,2022-06-11,['cs.lg'],"  A final exam in machine learning at a top institution such as MIT, Harvard,or Cornell typically takes faculty days to write, and students hours to solve.We demonstrate that large language models pass machine learning finals at ahuman level, on finals available online after the models were trained, andautomatically generate new human-quality final exam questions in seconds.Previous work has developed program synthesis and few-shot learning methods tosolve university-level problem set questions in mathematics and STEM courses.In this work, we develop and compare methods that solve final exams, whichdiffer from problem sets in several ways: the questions are longer, havemultiple parts, are more complicated, and span a broader set of topics. Wecurate a dataset and benchmark of questions from machine learning final examsavailable online and code for answering these questions and generating newquestions. We show how to generate new questions from other questions andcourse notes. For reproducibility and future research on this final exambenchmark, we use automatic checkers for multiple-choice, numeric, andquestions with expression answers. We perform ablation studies comparingzero-shot learning with few-shot learning and chain-of-thought prompting usingGPT-3, OPT, Codex, and ChatGPT across machine learning topics and find thatfew-shot learning methods perform best. We highlight the transformativepotential of language models to streamline the writing and solution oflarge-scale assessments, significantly reducing the workload from human days tomere machine seconds. Our results suggest that rather than banning largelanguage models such as ChatGPT in class, instructors should teach students toharness them by asking students meta-questions about correctness, completeness,and originality of the responses generated, encouraging critical thinking inacademic studies."
ME-D2N: Multi-Expert Domain Decompositional Network for Cross-Domain  Few-Shot Learning,"['Yuqian Fu', 'Yu Xie', 'Yanwei Fu', 'Jingjing Chen', 'Yu-Gang Jiang']",http://arxiv.org/pdf/2210.05280v1.pdf,2022-10-11,['cs.cv'],"  Recently, Cross-Domain Few-Shot Learning (CD-FSL) which aims at addressingthe Few-Shot Learning (FSL) problem across different domains has attractedrising attention. The core challenge of CD-FSL lies in the domain gap betweenthe source and novel target datasets. Though many attempts have been made forCD-FSL without any target data during model training, the huge domain gap makesit still hard for existing CD-FSL methods to achieve very satisfactory results.Alternatively, learning CD-FSL models with few labeled target domain data whichis more realistic and promising is advocated in previouswork~\cite{fu2021meta}. Thus, in this paper, we stick to this setting andtechnically contribute a novel Multi-Expert Domain Decompositional Network(ME-D2N). Concretely, to solve the data imbalance problem between the sourcedata with sufficient examples and the auxiliary target data with limitedexamples, we build our model under the umbrella of multi-expert learning. Twoteacher models which can be considered to be experts in their correspondingdomain are first trained on the source and the auxiliary target sets,respectively. Then, the knowledge distillation technique is introduced totransfer the knowledge from two teachers to a unified student model. Taking astep further, to help our student model learn knowledge from different domainteachers simultaneously, we further present a novel domain decomposition modulethat learns to decompose the student model into two domain-related sub parts.This is achieved by a novel domain-specific gate that learns to assign eachfilter to only one specific domain in a learnable way. Extensive experimentsdemonstrate the effectiveness of our method. Codes and models are available athttps://github.com/lovelyqian/ME-D2N_for_CDFSL."
Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in  Populus trichocarpa,"['John Lagergren', 'Mirko Pavicic', 'Hari B. Chhetri', 'Larry M. York', 'P. Doug Hyatt', 'David Kainer', 'Erica M. Rutter', 'Kevin Flores', 'Jack Bailey-Bale', 'Marie Klein', 'Gail Taylor', 'Daniel Jacobson', 'Jared Streich']",http://arxiv.org/pdf/2301.10351v3.pdf,2023-01-24,"['cs.cv', 'q-bio.qm']","  Plant phenotyping is typically a time-consuming and expensive endeavor,requiring large groups of researchers to meticulously measure biologicallyrelevant plant traits, and is the main bottleneck in understanding plantadaptation and the genetic architecture underlying complex traits at populationscale. In this work, we address these challenges by leveraging few-shotlearning with convolutional neural networks (CNNs) to segment the leaf body andvisible venation of 2,906 P. trichocarpa leaf images obtained in the field. Incontrast to previous methods, our approach (i) does not require experimental orimage pre-processing, (ii) uses the raw RGB images at full resolution, and(iii) requires very few samples for training (e.g., just eight images for veinsegmentation). Traits relating to leaf morphology and vein topology areextracted from the resulting segmentations using traditional open-sourceimage-processing tools, validated using real-world physical measurements, andused to conduct a genome-wide association study to identify genes controllingthe traits. In this way, the current work is designed to provide the plantphenotyping community with (i) methods for fast and accurate image-basedfeature extraction that require minimal training data, and (ii) a newpopulation-scale data set, including 68 different leaf phenotypes, for domainscientists and machine learning researchers. All of the few-shot learning code,data, and results are made publicly available."
Task-Equivariant Graph Few-shot Learning,"['Sungwon Kim', 'Junseok Lee', 'Namkyeong Lee', 'Wonjoong Kim', 'Seungyoon Choi', 'Chanyoung Park']",http://arxiv.org/pdf/2305.18758v4.pdf,2023-05-30,"['cs.lg', 'cs.ai']","  Although Graph Neural Networks (GNNs) have been successful in nodeclassification tasks, their performance heavily relies on the availability of asufficient number of labeled nodes per class. In real-world situations, not allclasses have many labeled nodes and there may be instances where the modelneeds to classify new classes, making manual labeling difficult. To solve thisproblem, it is important for GNNs to be able to classify nodes with a limitednumber of labeled nodes, known as few-shot node classification. Previousepisodic meta-learning based methods have demonstrated success in few-shot nodeclassification, but our findings suggest that optimal performance can only beachieved with a substantial amount of diverse training meta-tasks. To addressthis challenge of meta-learning based few-shot learning (FSL), we propose a newapproach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEGframework enables the model to learn transferable task-adaptation strategiesusing a limited number of training meta-tasks, allowing it to acquiremeta-knowledge for a wide range of meta-tasks. By incorporating equivariantneural networks, TEG can utilize their strong generalization abilities to learnhighly adaptable task-specific strategies. As a result, TEG achievesstate-of-the-art performance with limited training meta-tasks. Our experimentson various benchmark datasets demonstrate TEG's superiority in terms ofaccuracy and generalization ability, even when using minimal meta-trainingdata, highlighting the effectiveness of our proposed approach in addressing thechallenges of meta-learning based few-shot node classification. Our code isavailable at the following link: https://github.com/sung-won-kim/TEG"
Federated Few-shot Learning,"['Song Wang', 'Xingbo Fu', 'Kaize Ding', 'Chen Chen', 'Huiyuan Chen', 'Jundong Li']",http://arxiv.org/pdf/2306.10234v3.pdf,2023-06-17,"['cs.lg', 'cs.dc']","  Federated Learning (FL) enables multiple clients to collaboratively learn amachine learning model without exchanging their own local data. In this way,the server can exploit the computational power of all clients and train themodel on a larger set of data samples among all clients. Although such amechanism is proven to be effective in various fields, existing works generallyassume that each client preserves sufficient data for training. In practice,however, certain clients may only contain a limited number of samples (i.e.,few-shot samples). For example, the available photo data taken by a specificuser with a new mobile device is relatively rare. In this scenario, existing FLefforts typically encounter a significant performance drop on these clients.Therefore, it is urgent to develop a few-shot model that can generalize toclients with limited data under the FL scenario. In this paper, we refer tothis novel problem as federated few-shot learning. Nevertheless, the problemremains challenging due to two major reasons: the global data variance amongclients (i.e., the difference in data distributions among clients) and thelocal data insufficiency in each client (i.e., the lack of adequate local datafor training). To overcome these two challenges, we propose a novel federatedfew-shot learning framework with two separately updated models and dedicatedtraining strategies to reduce the adverse impact of global data variance andlocal data insufficiency. Extensive experiments on four prevalent datasets thatcover news articles and images validate the effectiveness of our frameworkcompared with the state-of-the-art baselines. Our code is provided athttps://github.com/SongW-SW/F2L."
Is Pre-training Truly Better Than Meta-Learning?,"['Brando Miranda', 'Patrick Yu', 'Saumya Goyal', 'Yu-Xiong Wang', 'Sanmi Koyejo']",http://arxiv.org/pdf/2306.13841v1.pdf,2023-06-24,"['cs.lg', 'cs.ai', 'cs.cl', 'cs.cv']","  In the context of few-shot learning, it is currently believed that a fixedpre-trained (PT) model, along with fine-tuning the final layer duringevaluation, outperforms standard meta-learning algorithms. We re-evaluate theseclaims under an in-depth empirical examination of an extensive set of formallydiverse datasets and compare PT to Model Agnostic Meta-Learning (MAML). Unlikeprevious work, we emphasize a fair comparison by using: the same architecture,the same optimizer, and all models trained to convergence. Crucially, we use amore rigorous statistical tool -- the effect size (Cohen's d) -- to determinethe practical significance of the difference between a model trained with PTvs. a MAML. We then use a previously proposed metric -- the diversitycoefficient -- to compute the average formal diversity of a dataset. Using thisanalysis, we demonstrate the following: 1. when the formal diversity of a dataset is low, PT beats MAML on average and 2. when the formal diversity is high,MAML beats PT on average. The caveat is that the magnitude of the averagedifference between a PT vs. MAML using the effect size is low (according toclassical statistical thresholds) -- less than 0.2. Nevertheless, thisobservation is contrary to the currently held belief that a pre-trained modelis always better than a meta-learning model. Our extensive experiments consider21 few-shot learning benchmarks, including the large-scale few-shot learningdataset Meta-Data set. We also show no significant difference between a MAMLmodel vs. a PT model with GPT-2 on Openwebtext. We, therefore, conclude that apre-trained model does not always beat a meta-learned model and that the formaldiversity of a dataset is a driving factor."
Model Tuning or Prompt Tuning? A Study of Large Language Models for  Clinical Concept and Relation Extraction,"['Cheng Peng', 'Xi Yang', 'Kaleb E Smith', 'Zehao Yu', 'Aokun Chen', 'Jiang Bian', 'Yonghui Wu']",http://arxiv.org/pdf/2310.06239v1.pdf,2023-10-10,"['cs.cl', 'cs.ai']","  Objective To develop soft prompt-based learning algorithms for large languagemodels (LLMs), examine the shape of prompts, prompt-tuning usingfrozen/unfrozen LLMs, transfer learning, and few-shot learning abilities.Methods We developed a soft prompt-based LLM model and compared 4 trainingstrategies including (1) fine-tuning without prompts; (2) hard-prompt withunfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt withfrozen LLMs. We evaluated 7 pretrained LLMs using the 4 training strategies forclinical concept and relation extraction on two benchmark datasets. Weevaluated the transfer learning ability of the prompt-based learning algorithmsin a cross-institution setting. We also assessed the few-shot learning ability.Results and Conclusion When LLMs are unfrozen, GatorTron-3.9B with softprompting achieves the best strict F1-scores of 0.9118 and 0.8604 for conceptextraction, outperforming the traditional fine-tuning and hard prompt-basedmodels by 0.6~3.1% and 1.2~2.9%, respectively; GatorTron-345M with softprompting achieves the best F1-scores of 0.8332 and 0.7488 for end-to-endrelation extraction, outperforming the other two models by 0.2~2% and0.6~11.7%, respectively. When LLMs are frozen, small (i.e., 345 millionparameters) LLMs have a big gap to be competitive with unfrozen models; scalingLLMs up to billions of parameters makes frozen LLMs competitive with unfrozenLLMs. For cross-institute evaluation, soft prompting with a frozenGatorTron-8.9B model achieved the best performance. This study demonstratesthat (1) machines can learn soft prompts better than humans, (2) frozen LLMshave better few-shot learning ability and transfer learning ability tofacilitate muti-institution applications, and (3) frozen LLMs require largemodels."
Pre-trained Recommender Systems: A Causal Debiasing Perspective,"['Ziqian Lin', 'Hao Ding', 'Nghia Hoang', 'Branislav Kveton', 'Anoop Deoras', 'Hao Wang']",http://arxiv.org/pdf/2310.19251v1.pdf,2023-10-30,"['cs.ir', 'cs.ai']","  Recent studies on pre-trained vision/language models have demonstrated thepractical benefit of a new, promising solution-building paradigm in AI wheremodels can be pre-trained on broad data describing a generic task space andthen adapted successfully to solve a wide range of downstream tasks, even whentraining data is severely limited (e.g., in zero- or few-shot learningscenarios). Inspired by such progress, we investigate in this paper thepossibilities and challenges of adapting such a paradigm to the context ofrecommender systems, which is less investigated from the perspective ofpre-trained model. In particular, we propose to develop a generic recommenderthat captures universal interaction patterns by training on generic user-iteminteraction data extracted from different domains, which can then be fastadapted to improve few-shot learning performance in unseen new domains (withlimited data).  However, unlike vision/language data which share strong conformity in thesemantic space, universal patterns underlying recommendation data collectedacross different domains (e.g., different countries or different E-commerceplatforms) are often occluded by both in-domain and cross-domain biasesimplicitly imposed by the cultural differences in their user and item bases, aswell as their uses of different e-commerce platforms. As shown in ourexperiments, such heterogeneous biases in the data tend to hinder theeffectiveness of the pre-trained model. To address this challenge, we furtherintroduce and formalize a causal debiasing perspective, which is substantiatedvia a hierarchical Bayesian deep learning model, named PreRec. Our empiricalstudies on real-world data show that the proposed model could significantlyimprove the recommendation performance in zero- and few-shot learning settingsunder both cross-market and cross-platform scenarios."
Semi-Supervised and Active Few-Shot Learning with Prototypical Networks,"['Rinu Boney', 'Alexander Ilin']",http://arxiv.org/pdf/1711.10856v2.pdf,2017-11-29,"['cs.lg', 'stat.ml']",  We consider the problem of semi-supervised few-shot classification where aclassifier needs to adapt to new tasks using a few labeled examples and(potentially many) unlabeled examples. We propose a clustering approach to theproblem. The features extracted with Prototypical Networks are clustered using$K$-means with the few labeled examples guiding the clustering process. We notethat in many real-world applications the adaptation performance can besignificantly improved by requesting the few labels through user feedback. Wedemonstrate good performance of the active adaptation strategy using imagedata.
Gotta Learn Fast: A New Benchmark for Generalization in RL,"['Alex Nichol', 'Vicki Pfau', 'Christopher Hesse', 'Oleg Klimov', 'John Schulman']",http://arxiv.org/pdf/1804.03720v2.pdf,2018-04-10,"['cs.lg', 'stat.ml']","  In this report, we present a new reinforcement learning (RL) benchmark basedon the Sonic the Hedgehog (TM) video game franchise. This benchmark is intendedto measure the performance of transfer learning and few-shot learningalgorithms in the RL domain. We also present and evaluate some baselinealgorithms on the new benchmark."
Coloring With Limited Data: Few-Shot Colorization via Memory-Augmented  Networks,"['Seungjoo Yoo', 'Hyojin Bahng', 'Sunghyo Chung', 'Junsoo Lee', 'Jaehyuk Chang', 'Jaegul Choo']",http://arxiv.org/pdf/1906.11888v1.pdf,2019-06-09,['cs.cv'],"  Despite recent advancements in deep learning-based automatic colorization,they are still limited when it comes to few-shot learning. Existing modelsrequire a significant amount of training data. To tackle this issue, we presenta novel memory-augmented colorization model MemoPainter that can producehigh-quality colorization with limited data. In particular, our model is ableto capture rare instances and successfully colorize them. We also propose anovel threshold triplet loss that enables unsupervised training of memorynetworks without the need of class labels. Experiments show that our model hassuperior quality in both few-shot and one-shot colorization tasks."
Texture Bias Of CNNs Limits Few-Shot Classification Performance,"['Sam Ringer', 'Will Williams', 'Tom Ash', 'Remi Francis', 'David MacLeod']",http://arxiv.org/pdf/1910.08519v1.pdf,2019-10-18,"['cs.lg', 'cs.cv', 'stat.ml']","  Accurate image classification given small amounts of labelled data (few-shotclassification) remains an open problem in computer vision. In this work weexamine how the known texture bias of Convolutional Neural Networks (CNNs)affects few-shot classification performance. Although texture bias can help instandard image classification, in this work we show it significantly harmsfew-shot classification performance. After correcting this bias we demonstratestate-of-the-art performance on the competitive miniImageNet task using amethod far simpler than the current best performing few-shot learningapproaches."
Online probabilistic label trees,"['Kalina Jasinska-Kobus', 'Marek Wydmuch', 'Devanathan Thiruvenkatachari', 'Krzysztof Dembczyński']",http://arxiv.org/pdf/2007.04451v2.pdf,2020-07-08,"['cs.lg', 'stat.ml']","  We introduce online probabilistic label trees (OPLTs), an algorithm thattrains a label tree classifier in a fully online manner without any priorknowledge about the number of training instances, their features and labels.OPLTs are characterized by low time and space complexity as well as strongtheoretical guarantees. They can be used for online multi-label and multi-classclassification, including the very challenging scenarios of one- or few-shotlearning. We demonstrate the attractiveness of OPLTs in a wide empirical studyon several instances of the tasks mentioned above."
Few-Shot Semantic Parsing for New Predicates,"['Zhuang Li', 'Lizhen Qu', 'Shuo Huang', 'Gholamreza Haffari']",http://arxiv.org/pdf/2101.10708v1.pdf,2021-01-26,"['cs.cl', 'cs.ai', 'cs.lg']","  In this work, we investigate the problems of semantic parsing in a few-shotlearning setting. In this setting, we are provided with utterance-logical formpairs per new predicate. The state-of-the-art neural semantic parsers achieveless than 25% accuracy on benchmark datasets when k= 1. To tackle this problem,we proposed to i) apply a designated meta-learning method to train the model;ii) regularize attention scores with alignment statistics; iii) apply asmoothing technique in pre-training. As a result, our method consistentlyoutperforms all the baselines in both one and two-shot settings."
Hyperbolic Image Embeddings,"['Valentin Khrulkov', 'Leyla Mirvakhabova', 'Evgeniya Ustinova', 'Ivan Oseledets', 'Victor Lempitsky']",http://arxiv.org/pdf/1904.02239v2.pdf,2019-04-03,"['cs.cv', 'cs.lg']","  Computer vision tasks such as image classification, image retrieval andfew-shot learning are currently dominated by Euclidean and sphericalembeddings, so that the final decisions about class belongings or the degree ofsimilarity are made using linear hyperplanes, Euclidean distances, or sphericalgeodesic distances (cosine similarity). In this work, we demonstrate that inmany practical scenarios hyperbolic embeddings provide a better alternative."
Cross-lingual Zero- and Few-shot Hate Speech Detection Utilising Frozen  Transformer Language Models and AXEL,"['Lukas Stappen', 'Fabian Brunn', 'Björn Schuller']",http://arxiv.org/pdf/2004.13850v1.pdf,2020-04-13,"['cs.cl', 'cs.lg', 'stat.ml']","  Detecting hate speech, especially in low-resource languages, is a non-trivialchallenge. To tackle this, we developed a tailored architecture based onfrozen, pre-trained Transformers to examine cross-lingual zero-shot andfew-shot learning, in addition to uni-lingual learning, on the HatEvalchallenge data set. With our novel attention-based classification block AXEL,we demonstrate highly competitive results on the English and Spanish subsets.We also re-sample the English subset, enabling additional, meaningfulcomparisons in the future."
Dynamic Memory Induction Networks for Few-Shot Text Classification,"['Ruiying Geng', 'Binhua Li', 'Yongbin Li', 'Jian Sun', 'Xiaodan Zhu']",http://arxiv.org/pdf/2005.05727v1.pdf,2020-05-12,"['cs.cl', 'cs.lg']","  This paper proposes Dynamic Memory Induction Networks (DMIN) for few-shottext classification. The model utilizes dynamic routing to provide moreflexibility to memory-based few-shot learning in order to better adapt thesupport sets, which is a critical capacity of few-shot classification models.Based on that, we further develop induction models with query information,aiming to enhance the generalization ability of meta-learning. The proposedmodel achieves new state-of-the-art results on the miniRCV1 and ODIC dataset,improving the best performance (accuracy) by 2~4%. Detailed analysis is furtherperformed to show the effectiveness of each component."
Example-Based Named Entity Recognition,"['Morteza Ziyadi', 'Yuting Sun', 'Abhishek Goswami', 'Jade Huang', 'Weizhu Chen']",http://arxiv.org/pdf/2008.10570v1.pdf,2020-08-24,"['cs.cl', 'cs.ir']","  We present a novel approach to named entity recognition (NER) in the presenceof scarce data that we call example-based NER. Our train-free few-shot learningapproach takes inspiration from question-answering to identify entity spans ina new and unseen domain. In comparison with the current state-of-the-art, theproposed method performs significantly better, especially when using a lownumber of support examples."
Structured Policy Representation: Imposing Stability in arbitrarily  conditioned dynamic systems,"['Julen Urain', 'Davide Tateo', 'Tianyu Ren', 'Jan Peters']",http://arxiv.org/pdf/2012.06224v1.pdf,2020-12-11,"['cs.ro', 'cs.lg']",  We present a new family of deep neural network-based dynamic systems. Thepresented dynamics are globally stable and can be conditioned with an arbitrarycontext state. We show how these dynamics can be used as structured robotpolicies. Global stability is one of the most important and straightforwardinductive biases as it allows us to impose reasonable behaviors outside theregion of the demonstrations.
Few Shot Learning for Information Verification,"['Usama Khalid', 'Mirza Omer Beg']",http://arxiv.org/pdf/2102.10956v1.pdf,2021-02-22,['cs.cl'],"  Information verification is quite a challenging task, this is because manytimes verifying a claim can require picking pieces of information from multiplepieces of evidence which can have a hierarchy of complex semantic relations.Previously a lot of researchers have mainly focused on simply concatenatingmultiple evidence sentences to accept or reject claims. These approaches arelimited as evidence can contain hierarchical information and dependencies. Inthis research, we aim to verify facts based on evidence selected from a list ofarticles taken from Wikipedia. Pretrained language models such as XLNET areused to generate meaningful representations and graph-based attention andconvolutions are used in such a way that the system requires little additionaltraining to learn to verify facts."
Inductive Relation Prediction by BERT,"['Hanwen Zha', 'Zhiyu Chen', 'Xifeng Yan']",http://arxiv.org/pdf/2103.07102v1.pdf,2021-03-12,['cs.cl'],"  Relation prediction in knowledge graphs is dominated by embedding basedmethods which mainly focus on the transductive setting. Unfortunately, they arenot able to handle inductive learning where unseen entities and relations arepresent and cannot take advantage of prior knowledge. Furthermore, theirinference process is not easily explainable. In this work, we propose anall-in-one solution, called BERTRL (BERT-based Relational Learning), whichleverages pre-trained language model and fine-tunes it by taking relationinstances and their possible reasoning paths as training samples. BERTRLoutperforms the SOTAs in 15 out of 18 cases in both inductive and transductivesettings. Meanwhile, it demonstrates strong generalization capability infew-shot learning and is explainable."
On Unifying Misinformation Detection,"['Nayeon Lee', 'Belinda Z. Li', 'Sinong Wang', 'Pascale Fung', 'Hao Ma', 'Wen-tau Yih', 'Madian Khabsa']",http://arxiv.org/pdf/2104.05243v1.pdf,2021-04-12,"['cs.ai', 'cs.cl']","  In this paper, we introduce UnifiedM2, a general-purpose misinformation modelthat jointly models multiple domains of misinformation with a single, unifiedsetup. The model is trained to handle four tasks: detecting news bias,clickbait, fake news, and verifying rumors. By grouping these tasks together,UnifiedM2learns a richer representation of misinformation, which leads tostate-of-the-art or comparable performance across all tasks. Furthermore, wedemonstrate that UnifiedM2's learned representation is helpful for few-shotlearning of unseen misinformation tasks/datasets and model's generalizabilityto unseen events."
SPeCiaL: Self-Supervised Pretraining for Continual Learning,"['Lucas Caccia', 'Joelle Pineau']",http://arxiv.org/pdf/2106.09065v1.pdf,2021-06-16,"['cs.cv', 'cs.lg']","  This paper presents SPeCiaL: a method for unsupervised pretraining ofrepresentations tailored for continual learning. Our approach devises ameta-learning objective that differentiates through a sequential learningprocess. Specifically, we train a linear model over the representations tomatch different augmented views of the same image together, each view presentedsequentially. The linear model is then evaluated on both its ability toclassify images it just saw, and also on images from previous iterations. Thisgives rise to representations that favor quick knowledge retention with minimalforgetting. We evaluate SPeCiaL in the Continual Few-Shot Learning setting, andshow that it can match or outperform other supervised pretraining approaches."
Discrete and Soft Prompting for Multilingual Models,"['Mengjie Zhao', 'Hinrich Schütze']",http://arxiv.org/pdf/2109.03630v1.pdf,2021-09-08,['cs.cl'],"  It has been shown for English that discrete and soft prompting performstrongly in few-shot learning with pretrained language models (PLMs). In thispaper, we show that discrete and soft prompting perform better than finetuningin multilingual cases: Crosslingual transfer and in-language training ofmultilingual natural language inference. For example, with 48 English trainingexamples, finetuning obtains 33.74% accuracy in crosslingual transfer, barelysurpassing the majority baseline (33.33%). In contrast, discrete and softprompting outperform finetuning, achieving 36.43% and 38.79%. We alsodemonstrate good performance of prompting with training data in multiplelanguages other than English."
Generalization Bounds For Meta-Learning: An Information-Theoretic  Analysis,"['Qi Chen', 'Changjian Shui', 'Mario Marchand']",http://arxiv.org/pdf/2109.14595v2.pdf,2021-09-29,"['cs.lg', 'stat.ml']","  We derive a novel information-theoretic analysis of the generalizationproperty of meta-learning algorithms. Concretely, our analysis proposes ageneric understanding of both the conventional learning-to-learn framework andthe modern model-agnostic meta-learning (MAML) algorithms. Moreover, we providea data-dependent generalization bound for a stochastic variant of MAML, whichis non-vacuous for deep few-shot learning. As compared to previous bounds thatdepend on the square norm of gradients, empirical validations on both simulateddata and a well-known few-shot benchmark show that our bound is orders ofmagnitude tighter in most situations."
Investigating the Effect of Natural Language Explanations on  Out-of-Distribution Generalization in Few-shot NLI,"['Yangqiaoyu Zhou', 'Chenhao Tan']",http://arxiv.org/pdf/2110.06223v1.pdf,2021-10-12,"['cs.cl', 'cs.ai']","  Although neural models have shown strong performance in datasets such asSNLI, they lack the ability to generalize out-of-distribution (OOD). In thiswork, we formulate a few-shot learning setup and examine the effects of naturallanguage explanations on OOD generalization. We leverage the templates in theHANS dataset and construct templated natural language explanations for eachtemplate. Although generated explanations show competitive BLEU scores againstgroundtruth explanations, they fail to improve prediction performance. Wefurther show that generated explanations often hallucinate information and misskey elements that indicate the label."
On sensitivity of meta-learning to support data,"['Mayank Agarwal', 'Mikhail Yurochkin', 'Yuekai Sun']",http://arxiv.org/pdf/2110.13953v1.pdf,2021-10-26,['cs.lg'],"  Meta-learning algorithms are widely used for few-shot learning. For example,image recognition systems that readily adapt to unseen classes after seeingonly a few labeled examples. Despite their success, we show that modernmeta-learning algorithms are extremely sensitive to the data used foradaptation, i.e. support data. In particular, we demonstrate the existence of(unaltered, in-distribution, natural) images that, when used for adaptation,yield accuracy as low as 4\% or as high as 95\% on standard few-shot imageclassification benchmarks. We explain our empirical findings in terms of classmargins, which in turn suggests that robust and safe meta-learning requireslarger margins than supervised learning."
Cedille: A large autoregressive French language model,"['Martin Müller', 'Florian Laurent']",http://arxiv.org/pdf/2202.03371v1.pdf,2022-02-07,"['cs.cl', '68t50', 'i.2.7']","  Scaling up the size and training of autoregressive language models hasenabled novel ways of solving Natural Language Processing tasks using zero-shotand few-shot learning. While extreme-scale language models such as GPT-3 offermultilingual capabilities, zero-shot learning for languages other than Englishremain largely unexplored. Here, we introduce Cedille, a large open sourceauto-regressive language model, specifically trained for the French language.Our results show that Cedille outperforms existing French language models andis competitive with GPT-3 on a range of French zero-shot benchmarks.Furthermore, we provide an in-depth comparison of the toxicity exhibited bythese models, showing that Cedille marks an improvement in language modelsafety thanks to dataset filtering."
On the Subspace Structure of Gradient-Based Meta-Learning,"['Gustaf Tegnér', 'Alfredo Reichlin', 'Hang Yin', 'Mårten Björkman', 'Danica Kragic']",http://arxiv.org/pdf/2207.03804v2.pdf,2022-07-08,['cs.lg'],"  In this work we provide an analysis of the distribution of thepost-adaptation parameters of Gradient-Based Meta-Learning (GBML) methods.Previous work has noticed how, for the case of image-classification, thisadaptation only takes place on the last layers of the network. We propose themore general notion that parameters are updated over a low-dimensional\emph{subspace} of the same dimensionality as the task-space and show that thisholds for regression as well. Furthermore, the induced subspace structureprovides a method to estimate the intrinsic dimension of the space of tasks ofcommon few-shot learning datasets."
Multi-task Learning for Cross-Lingual Sentiment Analysis,"['Gaurish Thakkar', 'Nives Mikelic Preradovic', 'Marko Tadic']",http://arxiv.org/pdf/2212.07160v1.pdf,2022-12-14,['cs.cl'],"  This paper presents a cross-lingual sentiment analysis of news articles usingzero-shot and few-shot learning. The study aims to classify the Croatian newsarticles with positive, negative, and neutral sentiments using the Slovenedataset. The system is based on a trilingual BERT-based model trained in threelanguages: English, Slovene, Croatian. The paper analyses different setupsusing datasets in two languages and proposes a simple multi-task model toperform sentiment classification. The evaluation is performed using thefew-shot and zero-shot scenarios in single-task and multi-task experiments forCroatian and Slovene."
Human in the loop: How to effectively create coherent topics by manually  labeling only a few documents per class,"['Anton Thielmann', 'Christoph Weisser', 'Benjamin Säfken']",http://arxiv.org/pdf/2212.09422v1.pdf,2022-12-19,['cs.cl'],"  Few-shot methods for accurate modeling under sparse label-settings haveimproved significantly. However, the applications of few-shot modeling innatural language processing remain solely in the field of documentclassification. With recent performance improvements, supervised few-shotmethods, combined with a simple topic extraction method pose a significantchallenge to unsupervised topic modeling methods. Our research shows thatsupervised few-shot learning, combined with a simple topic extraction method,can outperform unsupervised topic modeling techniques in terms of generatingcoherent topics, even when only a few labeled documents per class are used."
Sentence Simplification via Large Language Models,"['Yutao Feng', 'Jipeng Qiang', 'Yun Li', 'Yunhao Yuan', 'Yi Zhu']",http://arxiv.org/pdf/2302.11957v1.pdf,2023-02-23,"['cs.cl', 'cs.ai']","  Sentence Simplification aims to rephrase complex sentences into simplersentences while retaining original meaning. Large Language models (LLMs) havedemonstrated the ability to perform a variety of natural language processingtasks. However, it is not yet known whether LLMs can be served as ahigh-quality sentence simplification system. In this work, we empiricallyanalyze the zero-/few-shot learning ability of LLMs by evaluating them on anumber of benchmark test sets. Experimental results show LLMs outperformstate-of-the-art sentence simplification methods, and are judged to be on a parwith human annotators."
Meta-learning approaches for few-shot learning: A survey of recent  advances,"['Hassan Gharoun', 'Fereshteh Momenifar', 'Fang Chen', 'Amir H. Gandomi']",http://arxiv.org/pdf/2303.07502v1.pdf,2023-03-13,"['cs.lg', 'cs.ai']","  Despite its astounding success in learning deeper multi-dimensional data, theperformance of deep learning declines on new unseen tasks mainly due to itsfocus on same-distribution prediction. Moreover, deep learning is notorious forpoor generalization from few samples. Meta-learning is a promising approachthat addresses these issues by adapting to new tasks with few-shot datasets.This survey first briefly introduces meta-learning and then investigatesstate-of-the-art meta-learning methods and recent advances in: (I)metric-based, (II) memory-based, (III), and learning-based methods. Finally,current challenges and insights for future researches are discussed."
Few Shot Semantic Segmentation: a review of methodologies and open  challenges,"['Nico Catalano', 'Matteo Matteucci']",http://arxiv.org/pdf/2304.05832v1.pdf,2023-04-12,"['cs.cv', 'cs.ai']","  Semantic segmentation assigns category labels to each pixel in an image,enabling breakthroughs in fields such as autonomous driving and robotics. DeepNeural Networks have achieved high accuracies in semantic segmentation butrequire large training datasets. Some domains have difficulties building suchdatasets due to rarity, privacy concerns, and the need for skilled annotators.Few-Shot Learning (FSL) has emerged as a new research stream that allows modelsto learn new tasks from a few samples. This contribution provides an overviewof FSL in semantic segmentation (FSS), proposes a new taxonomy, and describescurrent limitations and outlooks."
Analyzing Text Representations by Measuring Task Alignment,"['Cesar Gonzalez-Gutierrez', 'Audi Primadhanty', 'Francesco Cazzaro', 'Ariadna Quattoni']",http://arxiv.org/pdf/2305.19747v1.pdf,2023-05-31,['cs.cl'],"  Textual representations based on pre-trained language models are key,especially in few-shot learning scenarios. What makes a representation good fortext classification? Is it due to the geometric properties of the space orbecause it is well aligned with the task? We hypothesize the second claim. Totest it, we develop a task alignment score based on hierarchical clusteringthat measures alignment at different levels of granularity. Our experiments ontext classification validate our hypothesis by showing that task alignment canexplain the classification performance of a given representation."
AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural  Language Processing,"['Asaad Alghamdi', 'Xinyu Duan', 'Wei Jiang', 'Zhenhai Wang', 'Yimeng Wu', 'Qingrong Xia', 'Zhefeng Wang', 'Yi Zheng', 'Mehdi Rezagholizadeh', 'Baoxing Huai', 'Peilun Cheng', 'Abbas Ghaddar']",http://arxiv.org/pdf/2306.06800v1.pdf,2023-06-11,['cs.cl'],"  Developing monolingual large Pre-trained Language Models (PLMs) is shown tobe very successful in handling different tasks in Natural Language Processing(NLP). In this work, we present AraMUS, the largest Arabic PLM with 11Bparameters trained on 529GB of high-quality Arabic textual data. AraMUSachieves state-of-the-art performances on a diverse set of Arabicclassification and generative tasks. Moreover, AraMUS shows impressive few-shotlearning abilities compared with the best existing Arabic PLMs."
NeuroCLIP: Neuromorphic Data Understanding by CLIP and SNN,"['Yufei Guo', 'Yuanpei Chen']",http://arxiv.org/pdf/2306.12073v1.pdf,2023-06-21,['cs.cv'],"  Recently, the neuromorphic vision sensor has received more and more interest.However, the neuromorphic data consists of asynchronous event spikes, which isnot natural and difficult to construct a benchmark, thus limiting theneuromorphic data understanding for ""unseen"" objects by deep learning.Zero-shot and few-shot learning via Contrastive Vision-Language Pre-training(CLIP) have shown inspirational performance in 2D frame image recognition. Tohandle ""unseen"" recognition for the neuromorphic data, in this paper, wepropose NeuroCLIP, which transfers the CLIP's 2D pre-trained knowledge to eventspikes. To improve the few-shot performance, we also provide an inter-timestepadapter based on a spiking neural network. Our code is open-sourced athttps://github.com/yfguo91/NeuroCLIP.git."
Leveraging Few-Shot Data Augmentation and Waterfall Prompting for  Response Generation,"['Lea Krause', 'Selene Báez Santamaría', 'Michiel van der Meer', 'Urja Khurana']",http://arxiv.org/pdf/2308.01080v1.pdf,2023-08-02,['cs.cl'],"  This paper discusses our approaches for task-oriented conversationalmodelling using subjective knowledge, with a particular emphasis on responsegeneration. Our methodology was shaped by an extensive data analysis thatevaluated key factors such as response length, sentiment, and dialogue actspresent in the provided dataset. We used few-shot learning to augment the datawith newly generated subjective knowledge items and present three approachesfor DSTC11: (1) task-specific model exploration, (2) incorporation of the mostfrequent question into all generated responses, and (3) a waterfall promptingtechnique using a combination of both GPT-3 and ChatGPT."
Logarithm-transform aided Gaussian Sampling for Few-Shot Learning,['Vaibhav Ganatra'],http://arxiv.org/pdf/2309.16337v1.pdf,2023-09-28,['cs.cv'],"  Few-shot image classification has recently witnessed the rise ofrepresentation learning being utilised for models to adapt to new classes usingonly a few training examples. Therefore, the properties of the representations,such as their underlying probability distributions, assume vital importance.Representations sampled from Gaussian distributions have been used in recentworks, [19] to train classifiers for few-shot classification. These methodsrely on transforming the distributions of experimental data to approximateGaussian distributions for their functioning. In this paper, I propose a novelGaussian transform, that outperforms existing methods on transformingexperimental data into Gaussian-like distributions. I then utilise this noveltransformation for few-shot image classification and show significant gains inperformance, while sampling lesser data."
Weakly-Supervised Surgical Phase Recognition,"['Roy Hirsch', 'Regev Cohen', 'Mathilde Caron', 'Tomer Golany', 'Daniel Freedman', 'Ehud Rivlin']",http://arxiv.org/pdf/2310.17209v1.pdf,2023-10-26,"['cs.cv', 'cs.lg']","  A key element of computer-assisted surgery systems is phase recognition ofsurgical videos. Existing phase recognition algorithms require frame-wiseannotation of a large number of videos, which is time and money consuming. Inthis work we join concepts of graph segmentation with self-supervised learningto derive a random-walk solution for per-frame phase prediction. Furthermore,we utilize within our method two forms of weak supervision: sparse timestampsor few-shot learning. The proposed algorithm enjoys low complexity and canoperate in lowdata regimes. We validate our method by running experiments withthe public Cholec80 dataset of laparoscopic cholecystectomy videos,demonstrating promising performance in multiple setups."
Robust Task Clustering for Deep Many-Task Learning,"['Mo Yu', 'Xiaoxiao Guo', 'Jinfeng Yi', 'Shiyu Chang', 'Saloni Potdar', 'Gerald Tesauro', 'Haoyu Wang', 'Bowen Zhou']",http://arxiv.org/pdf/1708.07918v2.pdf,2017-08-26,"['cs.lg', 'cs.ai', 'cs.cl', 'stat.ml']","  We investigate task clustering for deep-learning based multi-task andfew-shot learning in a many-task setting. We propose a new method to measuretask similarities with cross-task transfer performance matrix for the deeplearning scenario. Although this matrix provides us critical informationregarding similarity between tasks, its asymmetric property and unreliableperformance scores can affect conventional clustering methods adversely.Additionally, the uncertain task-pairs, i.e., the ones with extremelyasymmetric transfer scores, may collectively mislead clustering algorithms tooutput an inaccurate task-partition. To overcome these limitations, we proposea novel task-clustering algorithm by using the matrix completion technique. Theproposed algorithm constructs a partially-observed similarity matrix based onthe certainty of cluster membership of the task-pairs. We then use a matrixcompletion algorithm to complete the similarity matrix. Our theoreticalanalysis shows that under mild constraints, the proposed algorithm willperfectly recover the underlying ""true"" similarity matrix with a highprobability. Our results show that the new task clustering method can discovertask clusters for training flexible and superior neural network models in amulti-task learning setup for sentiment classification and dialog intentclassification tasks. Our task clustering approach also extends metric-basedfew-shot learning methods to adapt multiple metrics, which demonstratesempirical advantages when the tasks are diverse."
Dropping Networks for Transfer Learning,"[""James O' Neill"", 'Danushka Bollegala']",http://arxiv.org/pdf/1804.08501v3.pdf,2018-04-23,"['stat.ml', 'cs.lg']","  Many tasks in natural language understanding require learning relationshipsbetween two sequences for various tasks such as natural language inference,paraphrasing and entailment. These aforementioned tasks are similar in nature,yet they are often modeled individually. Knowledge transfer can be effectivefor closely related tasks. However, transferring all knowledge, some of whichirrelevant for a target task, can lead to sub-optimal results due to\textit{negative} transfer. Hence, this paper focuses on the transferability ofboth instances and parameters across natural language understanding tasks byproposing an ensemble-based transfer learning method. \newline The primarycontribution of this paper is the combination of both \textit{Dropout} and\textit{Bagging} for improved transferability in neural networks, referred toas \textit{Dropping} herein. We present a straightforward yet novel approachfor incorporating source \textit{Dropping} Networks to a target task forfew-shot learning that mitigates \textit{negative} transfer. This is achievedby using a decaying parameter chosen according to the slope changes of asmoothed spline error curve at sub-intervals during training. We compare theproposed approach against hard parameter sharing and soft parameter sharingtransfer methods in the few-shot learning case. We also compare against modelsthat are fully trained on the target task in the standard supervised learningsetup. The aforementioned adjustment leads to improved transfer learningperformance and comparable results to the current state of the art only using afraction of the data from the target task."
Incremental Few-Shot Learning with Attention Attractor Networks,"['Mengye Ren', 'Renjie Liao', 'Ethan Fetaya', 'Richard S. Zemel']",http://arxiv.org/pdf/1810.07218v3.pdf,2018-10-16,"['cs.lg', 'cs.cv', 'stat.ml']","  Machine learning classifiers are often trained to recognize a set ofpre-defined classes. However, in many applications, it is often desirable tohave the flexibility of learning additional concepts, with limited data andwithout re-training on the full training set. This paper addresses thisproblem, incremental few-shot learning, where a regular classification networkhas already been trained to recognize a set of base classes, and several extranovel classes are being considered, each with only a few labeled examples.After learning the novel classes, the model is then evaluated on the overallclassification performance on both base and novel classes. To this end, wepropose a meta-learning model, the Attention Attractor Network, whichregularizes the learning of novel classes. In each episode, we train a set ofnew weights to recognize novel classes until they converge, and we show thatthe technique of recurrent back-propagation can back-propagate through theoptimization process and facilitate the learning of these parameters. Wedemonstrate that the learned attractor network can help recognize novel classeswhile remembering old classes without the need to review the original trainingset, outperforming various baselines."
Zero and Few Shot Learning with Semantic Feature Synthesis and  Competitive Learning,"['Zhiwu Lu', 'Jiechao Guan', 'Aoxue Li', 'Tao Xiang', 'An Zhao', 'Ji-Rong Wen']",http://arxiv.org/pdf/1810.08332v1.pdf,2018-10-19,"['cs.cv', 'cs.lg']","  Zero-shot learning (ZSL) is made possible by learning a projection functionbetween a feature space and a semantic space (e.g.,~an attribute space). Key toZSL is thus to learn a projection that is robust against the often large domaingap between the seen and unseen class domains. In this work, this is achievedby unseen class data synthesis and robust projection function learning.Specifically, a novel semantic data synthesis strategy is proposed, by whichsemantic class prototypes (e.g., attribute vectors) are used to simply perturbseen class data for generating unseen class ones. As in any datasynthesis/hallucination approach, there are ambiguities and uncertainties onhow well the synthesised data can capture the targeted unseen class datadistribution. To cope with this, the second contribution of this work is anovel projection learning model termed competitive bidirectional projectionlearning (BPL) designed to best utilise the ambiguous synthesised data.Specifically, we assume that each synthesised data point can belong to anyunseen class; and the most likely two class candidates are exploited to learn arobust projection function in a competitive fashion. As a third contribution,we show that the proposed ZSL model can be easily extended to few-shot learning(FSL) by again exploiting semantic (class prototype guided) feature synthesisand competitive BPL. Extensive experiments show that our model achieves thestate-of-the-art results on both problems."
Generalized Zero- and Few-Shot Learning via Aligned Variational  Autoencoders,"['Edgar Schönfeld', 'Sayna Ebrahimi', 'Samarth Sinha', 'Trevor Darrell', 'Zeynep Akata']",http://arxiv.org/pdf/1812.01784v4.pdf,2018-12-05,"['cs.cv', 'cs.ai', 'cs.lg']","  Many approaches in generalized zero-shot learning rely on cross-modal mappingbetween the image feature space and the class embedding space. As labeledimages are expensive, one direction is to augment the dataset by generatingeither images or image features. However, the former misses fine-graineddetails and the latter requires learning a mapping associated with classembeddings. In this work, we take feature generation one step further andpropose a model where a shared latent space of image features and classembeddings is learned by modality-specific aligned variational autoencoders.This leaves us with the required discriminative information about the image andclasses in the latent features, on which we train a softmax classifier. The keyto our approach is that we align the distributions learned from images and fromside-information to construct latent features that contain the essentialmulti-modal information associated with unseen classes. We evaluate our learnedlatent features on several benchmark datasets, i.e. CUB, SUN, AWA1 and AWA2,and establish a new state of the art on generalized zero-shot as well as onfew-shot learning. Moreover, our results on ImageNet with various zero-shotsplits show that our latent features generalize well in large-scale settings."
Generating Classification Weights with GNN Denoising Autoencoders for  Few-Shot Learning,"['Spyros Gidaris', 'Nikos Komodakis']",http://arxiv.org/pdf/1905.01102v1.pdf,2019-05-03,"['cs.cv', 'cs.lg']","  Given an initial recognition model already trained on a set of base classes,the goal of this work is to develop a meta-model for few-shot learning. Themeta-model, given as input some novel classes with few training examples perclass, must properly adapt the existing recognition model into a new model thatcan correctly classify in a unified way both the novel and the base classes. Toaccomplish this goal it must learn to output the appropriate classificationweight vectors for those two types of classes. To build our meta-model we makeuse of two main innovations: we propose the use of a Denoising Autoencodernetwork (DAE) that (during training) takes as input a set of classificationweights corrupted with Gaussian noise and learns to reconstruct thetarget-discriminative classification weights. In this case, the injected noiseon the classification weights serves the role of regularizing the weightgenerating meta-model. Furthermore, in order to capture the co-dependenciesbetween different classes in a given task instance of our meta-model, wepropose to implement the DAE model as a Graph Neural Network (GNN). In order toverify the efficacy of our approach, we extensively evaluate it on ImageNetbased few-shot benchmarks and we report strong results that surpass priorapproaches. The code and models of our paper will be published on:https://github.com/gidariss/wDAE_GNN_FewShot"
Learning to learn via Self-Critique,"['Antreas Antoniou', 'Amos Storkey']",http://arxiv.org/pdf/1905.10295v6.pdf,2019-05-24,"['cs.lg', 'stat.ml']","  In few-shot learning, a machine learning system learns from a small set oflabelled examples relating to a specific task, such that it can generalize tonew examples of the same task. Given the limited availability of labelledexamples in such tasks, we wish to make use of all the information we can.Usually a model learns task-specific information from a small training-set(support-set) to predict on an unlabelled validation set (target-set). Thetarget-set contains additional task-specific information which is not utilizedby existing few-shot learning methods. Making use of the target-set examplesvia transductive learning requires approaches beyond the current methods; atinference time, the target-set contains only unlabelled input data-points, andso discriminative learning cannot be used. In this paper, we propose aframework called Self-Critique and Adapt or SCA, which learns to learn alabel-free loss function, parameterized as a neural network. A base-modellearns on a support-set using existing methods (e.g. stochastic gradientdescent combined with the cross-entropy loss), and then is updated for theincoming target-task using the learnt loss function. This label-free lossfunction is itself optimized such that the learnt model achieves highergeneralization performance. Experiments demonstrate that SCA offerssubstantially reduced error-rates compared to baselines which only adapt on thesupport-set, and results in state of the art benchmark performance onMini-ImageNet and Caltech-UCSD Birds 200."
Low-Rank Pairwise Alignment Bilinear Network For Few-Shot Fine-Grained  Image Classification,"['Huaxi Huang', 'Junjie Zhang', 'Jian Zhang', 'Jingsong Xu', 'Qiang Wu']",http://arxiv.org/pdf/1908.01313v3.pdf,2019-08-04,['cs.cv'],"  Deep neural networks have demonstrated advanced abilities on various visualclassification tasks, which heavily rely on the large-scale training sampleswith annotated ground-truth. However, it is unrealistic always to require suchannotation in real-world applications. Recently, Few-Shot learning (FS), as anattempt to address the shortage of training samples, has made significantprogress in generic classification tasks. Nonetheless, it is still challengingfor current FS models to distinguish the subtle differences betweenfine-grained categories given limited training data. To filling theclassification gap, in this paper, we address the Few-Shot Fine-Grained (FSFG)classification problem, which focuses on tackling the fine-grainedclassification under the challenging few-shot learning setting. A novellow-rank pairwise bilinear pooling operation is proposed to capture the nuanceddifferences between the support and query images for learning an effectivedistance metric. Moreover, a feature alignment layer is designed to match thesupport image features with query ones before the comparison. We name theproposed model Low-Rank Pairwise Alignment Bilinear Network (LRPABN), which istrained in an end-to-end fashion. Comprehensive experimental results on fourwidely used fine-grained classification datasets demonstrate that our LRPABNmodel achieves the superior performances compared to state-of-the-art methods."
ProtoGAN: Towards Few Shot Learning for Action Recognition,"['Sai Kumar Dwivedi', 'Vikram Gupta', 'Rahul Mitra', 'Shuaib Ahmed', 'Arjun Jain']",http://arxiv.org/pdf/1909.07945v1.pdf,2019-09-17,['cs.cv'],"  Few-shot learning (FSL) for action recognition is a challenging task ofrecognizing novel action categories which are represented by few instances inthe training data. In a more generalized FSL setting (G-FSL), both seen as wellas novel action categories need to be recognized. Conventional classifierssuffer due to inadequate data in FSL setting and inherent bias towards seenaction categories in G-FSL setting. In this paper, we address this problem byproposing a novel ProtoGAN framework which synthesizes additional examples fornovel categories by conditioning a conditional generative adversarial networkwith class prototype vectors. These class prototype vectors are learnt using aClass Prototype Transfer Network (CPTN) from examples of seen categories. Oursynthesized examples for a novel class are semantically similar to realexamples belonging to that class and is used to train a model exhibiting bettergeneralization towards novel classes. We support our claim by performingextensive experiments on three datasets: UCF101, HMDB51 and Olympic-Sports. Tothe best of our knowledge, we are the first to report the results for G-FSL andprovide a strong benchmark for future research. We also outperform thestate-of-the-art method in FSL for all the aforementioned datasets."
Meta R-CNN : Towards General Solver for Instance-level Few-shot Learning,"['Xiaopeng Yan', 'Ziliang Chen', 'Anni Xu', 'Xiaoxi Wang', 'Xiaodan Liang', 'Liang Lin']",http://arxiv.org/pdf/1909.13032v2.pdf,2019-09-28,"['cs.cv', 'cs.lg']","  Resembling the rapid learning capability of human, few-shot learning empowersvision systems to understand new concepts by training with few samples. Leadingapproaches derived from meta-learning on images with a single visual object.Obfuscated by a complex background and multiple objects in one image, they arehard to promote the research of few-shot object detection/segmentation. In thiswork, we present a flexible and general methodology to achieve these tasks. Ourwork extends Faster /Mask R-CNN by proposing meta-learning over RoI(Region-of-Interest) features instead of a full image feature. This simplespirit disentangles multi-object information merged with the background,without bells and whistles, enabling Faster /Mask R-CNN turn into ameta-learner to achieve the tasks. Specifically, we introduce a Predictor-headRemodeling Network (PRN) that shares its main backbone with Faster /Mask R-CNN.PRN receives images containing few-shot objects with their bounding boxes ormasks to infer their class attentive vectors. The vectors take channel-wisesoft-attention on RoI features, remodeling those R-CNN predictor heads todetect or segment the objects that are consistent with the classes thesevectors represent. In our experiments, Meta R-CNN yields the state of the artin few-shot object detection and improves few-shot object segmentation by MaskR-CNN."
On-chip Few-shot Learning with Surrogate Gradient Descent on a  Neuromorphic Processor,"['Kenneth Stewart', 'Garrick Orchard', 'Sumit Bam Shrestha', 'Emre Neftci']",http://arxiv.org/pdf/1910.04972v6.pdf,2019-10-11,['cs.ne'],"  Recent work suggests that synaptic plasticity dynamics in biological modelsof neurons and neuromorphic hardware are compatible with gradient-basedlearning (Neftci et al., 2019). Gradient-based learning requires iteratingseveral times over a dataset, which is both time-consuming and constrains thetraining samples to be independently and identically distributed. This isincompatible with learning systems that do not have boundaries between trainingand inference, such as in neuromorphic hardware. One approach to overcome theseconstraints is transfer learning, where a portion of the network is pre-trainedand mapped into hardware and the remaining portion is trained online. Transferlearning has the advantage that pre-training can be accelerated offline if thetask domain is known, and few samples of each class are sufficient for learningthe target task at reasonable accuracies. Here, we demonstrate on-linesurrogate gradient few-shot learning on Intel's Loihi neuromorphic researchprocessor using features pre-trained with spike-based gradientbackpropagation-through-time. Our experimental results show that the Loihi chipcan learn gestures online using a small number of shots and achieve resultsthat are comparable to the models simulated on a conventional processor."
A Two-Stage Approach to Few-Shot Learning for Image Recognition,"['Debasmit Das', 'C. S. George Lee']",http://arxiv.org/pdf/1912.04973v1.pdf,2019-12-10,"['cs.lg', 'cs.cv', 'eess.iv', 'stat.ml']","  This paper proposes a multi-layer neural network structure for few-shot imagerecognition of novel categories. The proposed multi-layer neural networkarchitecture encodes transferable knowledge extracted from a large annotateddataset of base categories. This architecture is then applied to novelcategories containing only a few samples. The transfer of knowledge is carriedout at the feature-extraction and the classification levels distributed acrossthe two training stages. In the first-training stage, we introduce the relativefeature to capture the structure of the data as well as obtain alow-dimensional discriminative space. Secondly, we account for the variablevariance of different categories by using a network to predict the variance ofeach class. Classification is then performed by computing the Mahalanobisdistance to the mean-class representation in contrast to previous approachesthat used the Euclidean distance. In the second-training stage, acategory-agnostic mapping is learned from the mean-sample representation to itscorresponding class-prototype representation. This is because the mean-samplerepresentation may not accurately represent the novel category prototype.Finally, we evaluate the proposed network structure on four standard few-shotimage recognition datasets, where our proposed few-shot learning systemproduces competitive performance compared to previous work. We also extensivelystudied and analyzed the contribution of each component of our proposedframework."
Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine  Pseudo-Labeling with Visual-Semantic Meta-Embedding,"['Jinhai Yang', 'Hua Yang', 'Lin Chen']",http://arxiv.org/pdf/2007.05675v3.pdf,2020-07-11,"['cs.cv', 'cs.lg', 'stat.ml']","  Few-shot learning aims at rapidly adapting to novel categories with only ahandful of samples at test time, which has been predominantly tackled with theidea of meta-learning. However, meta-learning approaches essentially learnacross a variety of few-shot tasks and thus still require large-scale trainingdata with fine-grained supervision to derive a generalized model, therebyinvolving prohibitive annotation cost. In this paper, we advance the few-shotclassification paradigm towards a more challenging scenario, i.e.,cross-granularity few-shot classification, where the model observes only coarselabels during training while is expected to perform fine-grained classificationduring testing. This task largely relieves the annotation cost sincefine-grained labeling usually requires strong domain-specific expertise. Tobridge the cross-granularity gap, we approximate the fine-grained datadistribution by greedy clustering of each coarse-class into pseudo-fine-classesaccording to the similarity of image embeddings. We then propose ameta-embedder that jointly optimizes the visual- and semantic-discrimination,in both instance-wise and coarse class-wise, to obtain a good feature space forthis coarse-to-fine pseudo-labeling process. Extensive experiments and ablationstudies are conducted to demonstrate the effectiveness and robustness of ourapproach on three representative datasets."
Augmented Bi-path Network for Few-shot Learning,"['Baoming Yan', 'Chen Zhou', 'Bo Zhao', 'Kan Guo', 'Jiang Yang', 'Xiaobo Li', 'Ming Zhang', 'Yizhou Wang']",http://arxiv.org/pdf/2007.07614v1.pdf,2020-07-15,['cs.cv'],"  Few-shot Learning (FSL) which aims to learn from few labeled training data isbecoming a popular research topic, due to the expensive labeling cost in manyreal-world applications. One kind of successful FSL method learns to comparethe testing (query) image and training (support) image by simply concatenatingthe features of two images and feeding it into the neural network. However,with few labeled data in each class, the neural network has difficulty inlearning or comparing the local features of two images. Such simple image-levelcomparison may cause serious mis-classification. To solve this problem, wepropose Augmented Bi-path Network (ABNet) for learning to compare both globaland local features on multi-scales. Specifically, the salient patches areextracted and embedded as the local features for every image. Then, the modellearns to augment the features for better robustness. Finally, the model learnsto compare global and local features separately, i.e., in two paths, beforemerging the similarities. Extensive experiments show that the proposed ABNetoutperforms the state-of-the-art methods. Both quantitative and visual ablationstudies are provided to verify that the proposed modules lead to more precisecomparison results."
Complementing Representation Deficiency in Few-shot Image  Classification: A Meta-Learning Approach,"['Xian Zhong', 'Cheng Gu', 'Wenxin Huang', 'Lin Li', 'Shuqin Chen', 'Chia-Wen Lin']",http://arxiv.org/pdf/2007.10778v1.pdf,2020-07-21,"['cs.cv', 'cs.lg']","  Few-shot learning is a challenging problem that has attracted more and moreattention recently since abundant training samples are difficult to obtain inpractical applications. Meta-learning has been proposed to address this issue,which focuses on quickly adapting a predictor as a base-learner to new tasks,given limited labeled samples. However, a critical challenge for meta-learningis the representation deficiency since it is hard to discover commoninformation from a small number of training samples or even one, as is therepresentation of key features from such little information. As a result, ameta-learner cannot be trained well in a high-dimensional parameter space togeneralize to new tasks. Existing methods mostly resort to extracting lessexpressive features so as to avoid the representation deficiency. Aiming atlearning better representations, we propose a meta-learning approach withcomplemented representations network (MCRNet) for few-shot imageclassification. In particular, we embed a latent space, where latent codes arereconstructed with extra representation information to complement therepresentation deficiency. Furthermore, the latent space is established withvariational inference, collaborating well with different base-learners, and canbe extended to other models. Finally, our end-to-end framework achieves thestate-of-the-art performance in image classification on three standard few-shotlearning datasets."
Few-Shot Bearing Fault Diagnosis Based on Model-Agnostic Meta-Learning,"['Shen Zhang', 'Fei Ye', 'Bingnan Wang', 'Thomas G. Habetler']",http://arxiv.org/pdf/2007.12851v4.pdf,2020-07-25,"['cs.lg', 'stat.ml']","  The rapid development of artificial intelligence and deep learning hasprovided many opportunities to further enhance the safety, stability, andaccuracy of industrial Cyber-Physical Systems (CPS). As indispensablecomponents to many mission-critical CPS assets and equipment, mechanicalbearings need to be monitored to identify any trace of abnormal conditions.Most of the data-driven approaches applied to bearing fault diagnosisup-to-date are trained using a large amount of fault data collected a priori.In many practical applications, however, it can be unsafe and time-consuming tocollect sufficient data samples for each fault category, making it challengingto train a robust classifier. In this paper, we propose a few-shot learningframework for bearing fault diagnosis based on model-agnostic meta-learning(MAML), which targets for training an effective fault classifier using limiteddata. In addition, it can leverage the training data and learn to identify newfault scenarios more efficiently. Case studies on the generalization to newartificial faults show that the proposed framework achieves an overall accuracyup to 25% higher than a Siamese network-based benchmark study. Finally, therobustness and the generalization capability of the proposed framework arefurther validated by applying it to identify real bearing damages using datafrom artificial damages, which compares favorably against 6 state-of-the-artfew-shot learning algorithms using consistent test environments."
BSNet: Bi-Similarity Network for Few-shot Fine-grained Image  Classification,"['Xiaoxu Li', 'Jijie Wu', 'Zhuo Sun', 'Zhanyu Ma', 'Jie Cao', 'Jing-Hao Xue']",http://arxiv.org/pdf/2011.14311v1.pdf,2020-11-29,"['cs.cv', '68u10', 'i.4']","  Few-shot learning for fine-grained image classification has gained recentattention in computer vision. Among the approaches for few-shot learning, dueto the simplicity and effectiveness, metric-based methods are favorablystate-of-the-art on many tasks. Most of the metric-based methods assume asingle similarity measure and thus obtain a single feature space. However, ifsamples can simultaneously be well classified via two distinct similaritymeasures, the samples within a class can distribute more compactly in a smallerfeature space, producing more discriminative feature maps. Motivated by this,we propose a so-called \textit{Bi-Similarity Network} (\textit{BSNet}) thatconsists of a single embedding module and a bi-similarity module of twosimilarity measures. After the support images and the query images pass throughthe convolution-based embedding module, the bi-similarity module learns featuremaps according to two similarity measures of diverse characteristics. In thisway, the model is enabled to learn more discriminative and lesssimilarity-biased features from few shots of fine-grained images, such that themodel generalization ability can be significantly improved. Through extensiveexperiments by slightly modifying established metric/similarity based networks,we show that the proposed approach produces a substantial improvement onseveral fine-grained image benchmark datasets. Codes are available at:https://github.com/spraise/BSNet"
Few-Shot Learning with Class Imbalance,"['Mateusz Ochal', 'Massimiliano Patacchiola', 'Amos Storkey', 'Jose Vazquez', 'Sen Wang']",http://arxiv.org/pdf/2101.02523v2.pdf,2021-01-07,"['cs.lg', 'cs.cv']","  Few-Shot Learning (FSL) algorithms are commonly trained through Meta-Learning(ML), which exposes models to batches of tasks sampled from a meta-dataset tomimic tasks seen during evaluation. However, the standard training proceduresoverlook the real-world dynamics where classes commonly occur at differentfrequencies. While it is generally understood that class imbalance harms theperformance of supervised methods, limited research examines the impact ofimbalance on the FSL evaluation task. Our analysis compares 10 state-of-the-artmeta-learning and FSL methods on different imbalance distributions andrebalancing techniques. Our results reveal that 1) some FSL methods display anatural disposition against imbalance while most other approaches produce aperformance drop by up to 17\% compared to the balanced task without theappropriate mitigation; 2) contrary to popular belief, many meta-learningalgorithms will not automatically learn to balance from exposure to imbalancedtraining tasks; 3) classical rebalancing strategies, such as randomoversampling, can still be very effective, leading to state-of-the-artperformances and should not be overlooked; 4) FSL methods are more robustagainst meta-dataset imbalance than imbalance at the task-level with a similarimbalance ratio ($\rho<20$), with the effect holding even in long-tail datasetsunder a larger imbalance ($\rho=65$)."
Learning to Focus: Cascaded Feature Matching Network for Few-shot Image  Recognition,"['Mengting Chen', 'Xinggang Wang', 'Heng Luo', 'Yifeng Geng', 'Wenyu Liu']",http://arxiv.org/pdf/2101.05018v1.pdf,2021-01-13,['cs.cv'],"  Deep networks can learn to accurately recognize objects of a category bytraining on a large number of annotated images. However, a meta-learningchallenge known as a low-shot image recognition task comes when only a fewimages with annotations are available for learning a recognition model for onecategory. The objects in testing/query and training/support images are likelyto be different in size, location, style, and so on. Our method, calledCascaded Feature Matching Network (CFMN), is proposed to solve this problem. Wetrain the meta-learner to learn a more fine-grained and adaptive deep distancemetric by focusing more on the features that have high correlations betweencompared images by the feature matching block which can align associatedfeatures together and naturally ignore those non-discriminative features. Byapplying the proposed feature matching block in different layers of thefew-shot recognition network, multi-scale information among the compared imagescan be incorporated into the final cascaded matching feature, which boosts therecognition performance further and generalizes better by learning onrelationships. The experiments for few-shot learning on two standard datasets,\emph{mini}ImageNet and Omniglot, have confirmed the effectiveness of ourmethod. Besides, the multi-label few-shot task is first studied on a new datasplit of COCO which further shows the superiority of the proposed featurematching network when performing few-shot learning in complex images. The codewill be made publicly available."
Contrastive Prototype Learning with Augmented Embeddings for Few-Shot  Learning,"['Yizhao Gao', 'Nanyi Fei', 'Guangzhen Liu', 'Zhiwu Lu', 'Tao Xiang', 'Songfang Huang']",http://arxiv.org/pdf/2101.09499v1.pdf,2021-01-23,['cs.cv'],"  Most recent few-shot learning (FSL) methods are based on meta-learning withepisodic training. In each meta-training episode, a discriminative featureembedding and/or classifier are first constructed from a support set in aninner loop, and then evaluated in an outer loop using a query set for modelupdating. This query set sample centered learning objective is howeverintrinsically limited in addressing the lack of training data problem in thesupport set. In this paper, a novel contrastive prototype learning withaugmented embeddings (CPLAE) model is proposed to overcome this limitation.First, data augmentations are introduced to both the support and query setswith each sample now being represented as an augmented embedding (AE) composedof concatenated embeddings of both the original and augmented versions. Second,a novel support set class prototype centered contrastive loss is proposed forcontrastive prototype learning (CPL). With a class prototype as an anchor, CPLaims to pull the query samples of the same class closer and those of differentclasses further away. This support set sample centered loss is highlycomplementary to the existing query centered loss, fully exploiting the limitedtraining data in each episode. Extensive experiments on several benchmarksdemonstrate that our proposed CPLAE achieves new state-of-the-art."
Machine learning with limited data,['Fupin Yao'],http://arxiv.org/pdf/2101.11461v1.pdf,2021-01-18,['cs.cv'],"  Thanks to the availability of powerful computing resources, big data and deeplearning algorithms, we have made great progress on computer vision in the lastfew years. Computer vision systems begin to surpass humans in some tasks, suchas object recognition, object detection, face recognition and pose estimation.Lots of computer vision algorithms have been deployed to real worldapplications and started to improve our life quality. However, big data andlabels are not always available. Sometimes we only have very limited labeleddata, such as medical images which requires experts to label them. In thispaper, we study few shot image classification, in which we only have very fewlabeled data. Machine learning with little data is a big challenge. To tacklethis challenge, we propose two methods and test their effectiveness thoroughly.One method is to augment image features by mixing the style of these images.The second method is applying spatial attention to explore the relationsbetween patches of images. We also find that domain shift is a critical issuein few shot learning when the training domain and testing domain are different.So we propose a more realistic cross-domain few-shot learning with unlabeleddata setting, in which some unlabeled data is available in the target domain.We propose two methods in this setting. Our first method transfers the styleinformation of the unlabeled target dataset to the samples in the sourcedataset and trains a model with stylized images and original images. Our secondmethod proposes a unified framework to fully utilize all the data. Both of ourmethods surpass the baseline method by a large margin."
AffinityNet: semi-supervised few-shot learning for disease type  prediction,"['Tianle Ma', 'Aidong Zhang']",http://arxiv.org/pdf/1805.08905v2.pdf,2018-05-22,"['cs.lg', 'stat.ml']","  While deep learning has achieved great success in computer vision and manyother fields, currently it does not work very well on patient genomic data withthe ""big p, small N"" problem (i.e., a relatively small number of samples withhigh-dimensional features). In order to make deep learning work with a smallamount of training data, we have to design new models that facilitate few-shotlearning. Here we present the Affinity Network Model (AffinityNet), a dataefficient deep learning model that can learn from a limited number of trainingexamples and generalize well. The backbone of the AffinityNet model consists ofstacked k-Nearest-Neighbor (kNN) attention pooling layers. The kNN attentionpooling layer is a generalization of the Graph Attention Model (GAM), and canbe applied to not only graphs but also any set of objects regardless of whethera graph is given or not. As a new deep learning module, kNN attention poolinglayers can be plugged into any neural network model just like convolutionallayers. As a simple special case of kNN attention pooling layer, featureattention layer can directly select important features that are useful forclassification tasks. Experiments on both synthetic data and cancer genomicdata from TCGA projects show that our AffinityNet model has bettergeneralization power than conventional neural network models with littletraining data. The code is freely available athttps://github.com/BeautyOfWeb/AffinityNet ."
Generalizing from a Few Examples: A Survey on Few-Shot Learning,"['Yaqing Wang', 'Quanming Yao', 'James Kwok', 'Lionel M. Ni']",http://arxiv.org/pdf/1904.05046v3.pdf,2019-04-10,"['cs.lg', 'cs.ai']","  Machine learning has been highly successful in data-intensive applicationsbut is often hampered when the data set is small. Recently, Few-Shot Learning(FSL) is proposed to tackle this problem. Using prior knowledge, FSL canrapidly generalize to new tasks containing only a few samples with supervisedinformation. In this paper, we conduct a thorough survey to fully understandFSL. Starting from a formal definition of FSL, we distinguish FSL from severalrelevant machine learning problems. We then point out that the core issue inFSL is that the empirical risk minimized is unreliable. Based on how priorknowledge can be used to handle this core issue, we categorize FSL methods fromthree perspectives: (i) data, which uses prior knowledge to augment thesupervised experience; (ii) model, which uses prior knowledge to reduce thesize of the hypothesis space; and (iii) algorithm, which uses prior knowledgeto alter the search for the best hypothesis in the given hypothesis space. Withthis taxonomy, we review and discuss the pros and cons of each category.Promising directions, in the aspects of the FSL problem setups, techniques,applications and theories, are also proposed to provide insights for futureresearch."
An Ensemble of Epoch-wise Empirical Bayes for Few-shot Learning,"['Yaoyao Liu', 'Bernt Schiele', 'Qianru Sun']",http://arxiv.org/pdf/1904.08479v6.pdf,2019-04-17,"['cs.cv', 'cs.lg', 'stat.ml']","  Few-shot learning aims to train efficient predictive models with a fewexamples. The lack of training data leads to poor models that performhigh-variance or low-confidence predictions. In this paper, we propose tometa-learn the ensemble of epoch-wise empirical Bayes models (E3BM) to achieverobust predictions. ""Epoch-wise"" means that each training epoch has a Bayesmodel whose parameters are specifically learned and deployed. ""Empirical"" meansthat the hyperparameters, e.g., used for learning and ensembling the epoch-wisemodels, are generated by hyperprior learners conditional on task-specific data.We introduce four kinds of hyperprior learners by considering inductive vs.transductive, and epoch-dependent vs. epoch-independent, in the paradigm ofmeta-learning. We conduct extensive experiments for five-class few-shot taskson three challenging benchmarks: miniImageNet, tieredImageNet, and FC100, andachieve top performance using the epoch-dependent transductive hyperpriorlearner, which captures the richest information. Our ablation study shows thatboth ""epoch-wise ensemble"" and ""empirical"" encourage high efficiency androbustness in the model performance."
Revisiting Metric Learning for Few-Shot Image Classification,"['Xiaomeng Li', 'Lequan Yu', 'Chi-Wing Fu', 'Meng Fang', 'Pheng-Ann Heng']",http://arxiv.org/pdf/1907.03123v2.pdf,2019-07-06,['cs.cv'],"  The goal of few-shot learning is to recognize new visual concepts with just afew amount of labeled samples in each class. Recent effective metric-basedfew-shot approaches employ neural networks to learn a feature similaritycomparison between query and support examples. However, the importance offeature embedding, i.e., exploring the relationship among training samples, isneglected. In this work, we present a simple yet powerful baseline for few-shotclassification by emphasizing the importance of feature embedding.Specifically, we revisit the classical triplet network from deep metriclearning, and extend it into a deep K-tuplet network for few-shot learning,utilizing the relationship among the input samples to learn a generalrepresentation learning via episode-training. Once trained, our network is ableto extract discriminative features for unseen novel categories and can beseamlessly incorporated with a non-linear distance metric function tofacilitate the few-shot classification. Our result on the miniImageNetbenchmark outperforms other metric-based few-shot classification methods. Moreimportantly, when evaluated on completely different datasets (Caltech-101,CUB-200, Stanford Dogs and Cars) using the model trained with miniImageNet, ourmethod significantly outperforms prior methods, demonstrating its superiorcapability to generalize to unseen classes."
Penalty Method for Inversion-Free Deep Bilevel Optimization,"['Akshay Mehra', 'Jihun Hamm']",http://arxiv.org/pdf/1911.03432v6.pdf,2019-11-08,"['cs.lg', 'math.oc', 'stat.ml']","  Solving a bilevel optimization problem is at the core of several machinelearning problems such as hyperparameter tuning, data denoising, meta- andfew-shot learning, and training-data poisoning. Different from simultaneous ormulti-objective optimization, the steepest descent direction for minimizing theupper-level cost in a bilevel problem requires the inverse of the Hessian ofthe lower-level cost. In this work, we propose a novel algorithm for solvingbilevel optimization problems based on the classical penalty function approach.Our method avoids computing the Hessian inverse and can handle constrainedbilevel problems easily. We prove the convergence of the method under mildconditions and show that the exact hypergradient is obtained asymptotically.Our method's simplicity and small space and time complexities enable us toeffectively solve large-scale bilevel problems involving deep neural networks.We present results on data denoising, few-shot learning, and training-datapoisoning problems in a large-scale setting. Our results show that our approachoutperforms or is comparable to previously proposed methods based on automaticdifferentiation and approximate inversion in terms of accuracy, run-time, andconvergence speed."
Learning to Few-Shot Learn Across Diverse Natural Language  Classification Tasks,"['Trapit Bansal', 'Rishikesh Jha', 'Andrew McCallum']",http://arxiv.org/pdf/1911.03863v3.pdf,2019-11-10,"['cs.cl', 'cs.lg']","  Self-supervised pre-training of transformer models has shown enormous successin improving performance on a number of downstream tasks. However, fine-tuningon a new task still requires large amounts of task-specific labelled data toachieve good performance. We consider this problem of learning to generalize tonew tasks with few examples as a meta-learning problem. While meta-learning hasshown tremendous progress in recent years, its application is still limited tosimulated problems or problems with limited diversity across tasks. We developa novel method, LEOPARD, which enables optimization-based meta-learning acrosstasks with different number of classes, and evaluate different methods ongeneralization to diverse NLP classification tasks. LEOPARD is trained with thestate-of-the-art transformer architecture and shows better generalization totasks not seen at all during training, with as few as 4 examples per label.Across 17 NLP tasks, including diverse domains of entity typing, naturallanguage inference, sentiment analysis, and several other text classificationtasks, we show that LEOPARD learns better initial parameters for few-shotlearning than self-supervised pre-training or multi-task training,outperforming many strong baselines, for example, yielding 14.5% averagerelative gain in accuracy on unseen tasks with only 4 examples per label."
Self-Supervised Learning For Few-Shot Image Classification,"['Da Chen', 'Yuefeng Chen', 'Yuhong Li', 'Feng Mao', 'Yuan He', 'Hui Xue']",http://arxiv.org/pdf/1911.06045v3.pdf,2019-11-14,['cs.cv'],"  Few-shot image classification aims to classify unseen classes with limitedlabelled samples. Recent works benefit from the meta-learning process withepisodic tasks and can fast adapt to class from training to testing. Due to thelimited number of samples for each task, the initial embedding network formeta-learning becomes an essential component and can largely affect theperformance in practice. To this end, most of the existing methods highly relyon the efficient embedding network. Due to the limited labelled data, the scaleof embedding network is constrained under a supervised learning(SL) mannerwhich becomes a bottleneck of the few-shot learning methods. In this paper, weproposed to train a more generalized embedding network with self-supervisedlearning (SSL) which can provide robust representation for downstream tasks bylearning from the data itself. We evaluate our work by extensive comparisonswith previous baseline methods on two few-shot classification datasets ({\emi.e.,} MiniImageNet and CUB) and achieve better performance over baselines.Tests on four datasets in cross-domain few-shot learning classification showthat the proposed method achieves state-of-the-art results and further provethe robustness of the proposed model. Our code is available at\hyperref[https://github.com/phecy/SSL-FEW-SHOT.]{https://github.com/phecy/SSL-FEW-SHOT.}"
Knowledge Graph Transfer Network for Few-Shot Recognition,"['Riquan Chen', 'Tianshui Chen', 'Xiaolu Hui', 'Hefeng Wu', 'Guanbin Li', 'Liang Lin']",http://arxiv.org/pdf/1911.09579v2.pdf,2019-11-21,['cs.cv'],"  Few-shot learning aims to learn novel categories from very few samples givensome base categories with sufficient training samples. The main challenge ofthis task is the novel categories are prone to dominated by color, texture,shape of the object or background context (namely specificity), which aredistinct for the given few training samples but not common for thecorresponding categories (see Figure 1). Fortunately, we find that transferringinformation of the correlated based categories can help learn the novelconcepts and thus avoid the novel concept being dominated by the specificity.Besides, incorporating semantic correlations among different categories caneffectively regularize this information transfer. In this work, we representthe semantic correlations in the form of structured knowledge graph andintegrate this graph into deep neural networks to promote few-shot learning bya novel Knowledge Graph Transfer Network (KGTN). Specifically, by initializingeach node with the classifier weight of the corresponding category, apropagation mechanism is learned to adaptively propagate node message throughthe graph to explore node interaction and transfer classifier information ofthe base categories to those of the novel ones. Extensive experiments on theImageNet dataset show significant performance improvement compared with currentleading competitors. Furthermore, we construct an ImageNet-6K dataset thatcovers larger scale categories, i.e, 6,000 categories, and experiments on thisdataset further demonstrate the effectiveness of our proposed model. Our codesand models are available at https://github.com/MyChocer/KGTN ."
Unlocking the Full Potential of Small Data with Diverse Supervision,"['Ziqi Pang', 'Zhiyuan Hu', 'Pavel Tokmakov', 'Yu-Xiong Wang', 'Martial Hebert']",http://arxiv.org/pdf/1911.12911v3.pdf,2019-11-29,['cs.cv'],"  Virtually all of deep learning literature relies on the assumption of largeamounts of available training data. Indeed, even the majority of few-shotlearning methods rely on a large set of ""base classes"" for pretraining. Thisassumption, however, does not always hold. For some tasks, annotating a largenumber of classes can be infeasible, and even collecting the images themselvescan be a challenge in some scenarios. In this paper, we study this problem andcall it ""Small Data"" setting, in contrast to ""Big Data"". To unlock the fullpotential of small data, we propose to augment the models with annotations forother related tasks, thus increasing their generalization abilities. Inparticular, we use the richly annotated scene parsing dataset ADE20K toconstruct our realistic Long-tail Recognition with Diverse Supervision (LRDS)benchmark by splitting the object categories into head and tail based on theirdistribution. Following the standard few-shot learning protocol, we use thehead classes for representation learning and the tail classes for evaluation.Moreover, we further subsample the head categories and images to generate twonovel settings which we call ""Scarce-Class"" and ""Scarce-Image"", respectivelycorresponding to the shortage of samples for rare classes and training images.Finally, we analyze the effect of applying various additional supervisionsources under the proposed settings. Our experiments demonstrate that denselylabeling a small set of images can indeed largely remedy the small dataconstraints."
Fine-grained Image-to-Image Transformation towards Visual Recognition,"['Wei Xiong', 'Yutong He', 'Yixuan Zhang', 'Wenhan Luo', 'Lin Ma', 'Jiebo Luo']",http://arxiv.org/pdf/2001.03856v2.pdf,2020-01-12,['cs.cv'],"  Existing image-to-image transformation approaches primarily focus onsynthesizing visually pleasing data. Generating images with correct identitylabels is challenging yet much less explored. It is even more challenging todeal with image transformation tasks with large deformation in poses,viewpoints, or scales while preserving the identity, such as face rotation andobject viewpoint morphing. In this paper, we aim at transforming an image witha fine-grained category to synthesize new images that preserve the identity ofthe input image, which can thereby benefit the subsequent fine-grained imagerecognition and few-shot learning tasks. The generated images, transformed withlarge geometric deformation, do not necessarily need to be of high visualquality but are required to maintain as much identity information as possible.To this end, we adopt a model based on generative adversarial networks todisentangle the identity related and unrelated factors of an image. In order topreserve the fine-grained contextual details of the input image during thedeformable transformation, a constrained nonalignment connection method isproposed to construct learnable highways between intermediate convolutionblocks in the generator. Moreover, an adaptive identity modulation mechanism isproposed to transfer the identity information into the output imageeffectively. Extensive experiments on the CompCars and Multi-PIE datasetsdemonstrate that our model preserves the identity of the generated images muchbetter than the state-of-the-art image-to-image transformation models, and as aresult significantly boosts the visual recognition performance in fine-grainedfew-shot learning."
Continual Local Replacement for Few-shot Learning,"['Canyu Le', 'Zhonggui Chen', 'Xihan Wei', 'Biao Wang', 'Lei Zhang']",http://arxiv.org/pdf/2001.08366v2.pdf,2020-01-23,"['cs.cv', 'cs.lg', 'eess.iv', 'stat.ml']","  The goal of few-shot learning is to learn a model that can recognize novelclasses based on one or few training data. It is challenging mainly due to twoaspects: (1) it lacks good feature representation of novel classes; (2) a fewof labeled data could not accurately represent the true data distribution andthus it's hard to learn a good decision function for classification. In thiswork, we use a sophisticated network architecture to learn better featurerepresentation and focus on the second issue. A novel continual localreplacement strategy is proposed to address the data deficiency problem. Ittakes advantage of the content in unlabeled images to continually enhancelabeled ones. Specifically, a pseudo labeling method is adopted to constantlyselect semantically similar images on the fly. Original labeled images will belocally replaced by the selected images for the next epoch training. In thisway, the model can directly learn new semantic information from unlabeledimages and the capacity of supervised signals in the embedding space can besignificantly enlarged. This allows the model to improve generalization andlearn a better decision boundary for classification. Our method is conceptuallysimple and easy to implement. Extensive experiments demonstrate that it canachieve state-of-the-art results on various few-shot image recognitionbenchmarks."
Few-Shot Learning as Domain Adaptation: Algorithm and Analysis,"['Jiechao Guan', 'Zhiwu Lu', 'Tao Xiang', 'Ji-Rong Wen']",http://arxiv.org/pdf/2002.02050v3.pdf,2020-02-06,"['cs.lg', 'stat.ml']","  To recognize the unseen classes with only few samples, few-shot learning(FSL) uses prior knowledge learned from the seen classes. A major challenge forFSL is that the distribution of the unseen classes is different from that ofthose seen, resulting in poor generalization even when a model is meta-trainedon the seen classes. This class-difference-caused distribution shift can beconsidered as a special case of domain shift. In this paper, for the firsttime, we propose a domain adaptation prototypical network with attention(DAPNA) to explicitly tackle such a domain shift problem in a meta-learningframework. Specifically, armed with a set transformer based attention module,we construct each episode with two sub-episodes without class overlap on theseen classes to simulate the domain shift between the seen and unseen classes.To align the feature distributions of the two sub-episodes with limitedtraining samples, a feature transfer network is employed together with a margindisparity discrepancy (MDD) loss. Importantly, theoretical analysis is providedto give the learning bound of our DAPNA. Extensive experiments show that ourDAPNA outperforms the state-of-the-art FSL alternatives, often by significantmargins."
Meta-Learning across Meta-Tasks for Few-Shot Learning,"['Nanyi Fei', 'Zhiwu Lu', 'Yizhao Gao', 'Jia Tian', 'Tao Xiang', 'Ji-Rong Wen']",http://arxiv.org/pdf/2002.04274v4.pdf,2020-02-11,"['cs.lg', 'stat.ml']","  Existing meta-learning based few-shot learning (FSL) methods typically adoptan episodic training strategy whereby each episode contains a meta-task. Acrossepisodes, these tasks are sampled randomly and their relationships are ignored.In this paper, we argue that the inter-meta-task relationships should beexploited and those tasks are sampled strategically to assist in meta-learning.Specifically, we consider the relationships defined over two types of meta-taskpairs and propose different strategies to exploit them. (1) Two meta-tasks withdisjoint sets of classes: this pair is interesting because it is reminiscent ofthe relationship between the source seen classes and target unseen classes,featured with domain gap caused by class differences. A novel learningobjective termed meta-domain adaptation (MDA) is proposed to make themeta-learned model more robust to the domain gap. (2) Two meta-tasks withidentical sets of classes: this pair is useful because it can be employed tolearn models that are robust against poorly sampled few-shots. To that end, anovel meta-knowledge distillation (MKD) objective is formulated. There are somemistakes in the experiments. We thus choose to withdraw this paper."
Few-Shot Few-Shot Learning and the role of Spatial Attention,"['Yann Lifchitz', 'Yannis Avrithis', 'Sylvaine Picard']",http://arxiv.org/pdf/2002.07522v1.pdf,2020-02-18,['cs.cv'],"  Few-shot learning is often motivated by the ability of humans to learn newtasks from few examples. However, standard few-shot classification benchmarksassume that the representation is learned on a limited amount of base classdata, ignoring the amount of prior knowledge that a human may have accumulatedbefore learning new tasks. At the same time, even if a powerful representationis available, it may happen in some domain that base class data are limited ornon-existent. This motivates us to study a problem where the representation isobtained from a classifier pre-trained on a large-scale dataset of a differentdomain, assuming no access to its training process, while the base class dataare limited to few examples per class and their role is to adapt therepresentation to the domain at hand rather than learn from scratch. We adaptthe representation in two stages, namely on the few base class data ifavailable and on the even fewer data of new tasks. In doing so, we obtain fromthe pre-trained classifier a spatial attention map that allows focusing onobjects and suppressing background clutter. This is important in the newproblem, because when base class data are few, the network cannot learn whereto focus implicitly. We also show that a pre-trained network may be easilyadapted to novel classes, without meta-learning."
"Few-Shot Learning via Learning the Representation, Provably","['Simon S. Du', 'Wei Hu', 'Sham M. Kakade', 'Jason D. Lee', 'Qi Lei']",http://arxiv.org/pdf/2002.09434v2.pdf,2020-02-21,"['cs.lg', 'math.oc', 'stat.ml']","  This paper studies few-shot learning via representation learning, where oneuses $T$ source tasks with $n_1$ data per task to learn a representation inorder to reduce the sample complexity of a target task for which there is only$n_2 (\ll n_1)$ data. Specifically, we focus on the setting where there existsa good \emph{common representation} between source and target, and our goal isto understand how much of a sample size reduction is possible. First, we studythe setting where this common representation is low-dimensional and provide afast rate of $O\left(\frac{\mathcal{C}\left(\Phi\right)}{n_1T} +\frac{k}{n_2}\right)$; here, $\Phi$ is the representation function class,$\mathcal{C}\left(\Phi\right)$ is its complexity measure, and $k$ is thedimension of the representation. When specialized to linear representationfunctions, this rate becomes $O\left(\frac{dk}{n_1T} + \frac{k}{n_2}\right)$where $d (\gg k)$ is the ambient input dimension, which is a substantialimprovement over the rate without using representation learning, i.e. over therate of $O\left(\frac{d}{n_2}\right)$. This result bypasses the$\Omega(\frac{1}{T})$ barrier under the i.i.d. task assumption, and can capturethe desired property that all $n_1T$ samples from source tasks can be\emph{pooled} together for representation learning. Next, we consider thesetting where the common representation may be high-dimensional but iscapacity-constrained (say in norm); here, we again demonstrate the advantage ofrepresentation learning in both high-dimensional linear regression and neuralnetwork learning. Our results demonstrate representation learning can fullyutilize all $n_1T$ samples from source tasks."
Is the Meta-Learning Idea Able to Improve the Generalization of Deep  Neural Networks on the Standard Supervised Learning?,"['Xiang Deng', 'Zhongfei Zhang']",http://arxiv.org/pdf/2002.12455v1.pdf,2020-02-27,"['cs.lg', 'cs.cv', 'stat.ml']","  Substantial efforts have been made on improving the generalization abilitiesof deep neural networks (DNNs) in order to obtain better performances withoutintroducing more parameters. On the other hand, meta-learning approachesexhibit powerful generalization on new tasks in few-shot learning. Intuitively,few-shot learning is more challenging than the standard supervised learning aseach target class only has a very few or no training samples. The naturalquestion that arises is whether the meta-learning idea can be used forimproving the generalization of DNNs on the standard supervised learning. Inthis paper, we propose a novel meta-learning based training procedure (MLTP)for DNNs and demonstrate that the meta-learning idea can indeed improve thegeneralization abilities of DNNs. MLTP simulates the meta-training process byconsidering a batch of training samples as a task. The key idea is that thegradient descent step for improving the current task performance should alsoimprove a new task performance, which is ignored by the current standardprocedure for training neural networks. MLTP also benefits from all theexisting training techniques such as dropout, weight decay, and batchnormalization. We evaluate MLTP by training a variety of small and large neuralnetworks on three benchmark datasets, i.e., CIFAR-10, CIFAR-100, and TinyImageNet. The experimental results show a consistently improved generalizationperformance on all the DNNs with different sizes, which verifies the promise ofMLTP and demonstrates that the meta-learning idea is indeed able to improve thegeneralization of DNNs on the standard supervised learning."
AdarGCN: Adaptive Aggregation GCN for Few-Shot Learning,"['Jianhong Zhang', 'Manli Zhang', 'Zhiwu Lu', 'Tao Xiang', 'Jirong Wen']",http://arxiv.org/pdf/2002.12641v2.pdf,2020-02-28,"['cs.lg', 'stat.ml']","  Existing few-shot learning (FSL) methods assume that there exist sufficienttraining samples from source classes for knowledge transfer to target classeswith few training samples. However, this assumption is often invalid,especially when it comes to fine-grained recognition. In this work, we define anew FSL setting termed few-shot fewshot learning (FSFSL), under which both thesource and target classes have limited training samples. To overcome the sourceclass data scarcity problem, a natural option is to crawl images from the webwith class names as search keywords. However, the crawled images are inevitablycorrupted by large amount of noise (irrelevant images) and thus may harm theperformance. To address this problem, we propose a graph convolutional network(GCN)-based label denoising (LDN) method to remove the irrelevant images.Further, with the cleaned web images as well as the original clean trainingimages, we propose a GCN-based FSL method. For both the LDN and FSL tasks, anovel adaptive aggregation GCN (AdarGCN) model is proposed, which differs fromexisting GCN models in that adaptive aggregation is performed based on amulti-head multi-level aggregation module. With AdarGCN, how much and how farinformation carried by each graph node is propagated in the graph structure canbe determined automatically, therefore alleviating the effects of both noisyand outlying training samples. Extensive experiments show the superiorperformance of our AdarGCN under both the new FSFSL and the conventional FSLsettings."
TAFSSL: Task-Adaptive Feature Sub-Space Learning for few-shot  classification,"['Moshe Lichtenstein', 'Prasanna Sattigeri', 'Rogerio Feris', 'Raja Giryes', 'Leonid Karlinsky']",http://arxiv.org/pdf/2003.06670v1.pdf,2020-03-14,['cs.cv'],"  The field of Few-Shot Learning (FSL), or learning from very few (typically$1$ or $5$) examples per novel class (unseen during training), has received alot of attention and significant performance advances in the recent literature.While number of techniques have been proposed for FSL, several factors haveemerged as most important for FSL performance, awarding SOTA even to thesimplest of techniques. These are: the backbone architecture (bigger isbetter), type of pre-training on the base classes (meta-training vs regularmulti-class, currently regular wins), quantity and diversity of the baseclasses set (the more the merrier, resulting in richer and better adaptivefeatures), and the use of self-supervised tasks during pre-training (serving asa proxy for increasing the diversity of the base set). In this paper we proposeyet another simple technique that is important for the few shot learningperformance - a search for a compact feature sub-space that is discriminativefor a given few-shot test task. We show that the Task-Adaptive FeatureSub-Space Learning (TAFSSL) can significantly boost the performance in FSLscenarios when some additional unlabeled data accompanies the novel few-shottask, be it either the set of unlabeled queries (transductive FSL) or someadditional set of unlabeled data samples (semi-supervised FSL). Specifically,we show that on the challenging miniImageNet and tieredImageNet benchmarks,TAFSSL can improve the current state-of-the-art in both transductive andsemi-supervised FSL settings by more than $5\%$, while increasing the benefitof using unlabeled data in FSL to above $10\%$ performance gain."
Interpretable Time-series Classification on Few-shot Samples,"['Wensi Tang', 'Lu Liu', 'Guodong Long']",http://arxiv.org/pdf/2006.02031v2.pdf,2020-06-03,"['cs.lg', 'stat.ml']","  Recent few-shot learning works focus on training a model with priormeta-knowledge to fast adapt to new tasks with unseen classes and samples.However, conventional time-series classification algorithms fail to tackle thefew-shot scenario. Existing few-shot learning methods are proposed to tackleimage or text data, and most of them are neural-based models that lackinterpretability. This paper proposes an interpretable neural-based framework,namely \textit{Dual Prototypical Shapelet Networks (DPSN)} for few-shottime-series classification, which not only trains a neural network-based modelbut also interprets the model from dual granularity: 1) global overview usingrepresentative time series samples, and 2) local highlights usingdiscriminative shapelets. In particular, the generated dual prototypicalshapelets consist of representative samples that can mostly demonstrate theoverall shapes of all samples in the class and discriminative partial-lengthshapelets that can be used to distinguish different classes. We have derived 18few-shot TSC datasets from public benchmark datasets and evaluated the proposedmethod by comparing with baselines. The DPSN framework outperformsstate-of-the-art time-series classification methods, especially when trainingwith limited amounts of data. Several case studies have been given todemonstrate the interpret ability of our model."
Graph Prototypical Networks for Few-shot Learning on Attributed Networks,"['Kaize Ding', 'Jianling Wang', 'Jundong Li', 'Kai Shu', 'Chenghao Liu', 'Huan Liu']",http://arxiv.org/pdf/2006.12739v3.pdf,2020-06-23,"['cs.lg', 'cs.si', 'stat.ml']","  Attributed networks nowadays are ubiquitous in a myriad of high-impactapplications, such as social network analysis, financial fraud detection, anddrug discovery. As a central analytical task on attributed networks, nodeclassification has received much attention in the research community. Inreal-world attributed networks, a large portion of node classes only containlimited labeled instances, rendering a long-tail node class distribution.Existing node classification algorithms are unequipped to handle the\textit{few-shot} node classes. As a remedy, few-shot learning has attracted asurge of attention in the research community. Yet, few-shot node classificationremains a challenging problem as we need to address the following questions:(i) How to extract meta-knowledge from an attributed network for few-shot nodeclassification? (ii) How to identify the informativeness of each labeledinstance for building a robust and effective model? To answer these questions,in this paper, we propose a graph meta-learning framework -- Graph PrototypicalNetworks (GPN). By constructing a pool of semi-supervised node classificationtasks to mimic the real test environment, GPN is able to perform\textit{meta-learning} on an attributed network and derive a highlygeneralizable model for handling the target classification task. Extensiveexperiments demonstrate the superior capability of GPN in few-shot nodeclassification."
Laplacian Regularized Few-Shot Learning,"['Imtiaz Masud Ziko', 'Jose Dolz', 'Eric Granger', 'Ismail Ben Ayed']",http://arxiv.org/pdf/2006.15486v3.pdf,2020-06-28,"['cs.lg', 'cs.cv', 'stat.ml']","  We propose a transductive Laplacian-regularized inference for few-shot tasks.Given any feature embedding learned from the base classes, we minimize aquadratic binary-assignment function containing two terms: (1) a unary termassigning query samples to the nearest class prototype, and (2) a pairwiseLaplacian term encouraging nearby query samples to have consistent labelassignments. Our transductive inference does not re-train the base model, andcan be viewed as a graph clustering of the query set, subject to supervisionconstraints from the support set. We derive a computationally efficient boundoptimizer of a relaxation of our function, which computes independent(parallel) updates for each query sample, while guaranteeing convergence.Following a simple cross-entropy training on the base classes, and withoutcomplex meta-learning strategies, we conducted comprehensive experiments overfive few-shot learning benchmarks. Our LaplacianShot consistently outperformsstate-of-the-art methods by significant margins across different models,settings, and data sets. Furthermore, our transductive inference is very fast,with computational times that are close to inductive inference, and can be usedfor large-scale few-shot tasks."
Few Shot Learning Framework to Reduce Inter-observer Variability in  Medical Images,['Sohini Roychowdhury'],http://arxiv.org/pdf/2008.02952v1.pdf,2020-08-07,"['cs.cv', 'cs.lg']","  Most computer aided pathology detection systems rely on large volumes ofquality annotated data to aid diagnostics and follow up procedures. However,quality assuring large volumes of annotated medical image data can besubjective and expensive. In this work we present a novel standardizationframework that implements three few-shot learning (FSL) models that can beiteratively trained by atmost 5 images per 3D stack to generate multipleregional proposals (RPs) per test image. These FSL models include a novelparallel echo state network (ParESN) framework and an augmented U-net model.Additionally, we propose a novel target label selection algorithm (TLSA) thatmeasures relative agreeability between RPs and the manually annotated targetlabels to detect the ""best"" quality annotation per image. Using the FSL models,our system achieves 0.28-0.64 Dice coefficient across vendor image stacks forintra-retinal cyst segmentation. Additionally, the TLSA is capable ofautomatically classifying high quality target labels from their noisycounterparts for 60-97% of the images while ensuring manual supervision onremaining images. Also, the proposed framework with ParESN model minimizesmanual annotation checking to 12-28% of the total number of images. The TLSAmetrics further provide confidence scores for the automated annotation qualityassurance. Thus, the proposed framework is flexible to extensions for qualityimage annotation curation of other image stacks as well."
BOIL: Towards Representation Change for Few-shot Learning,"['Jaehoon Oh', 'Hyungjun Yoo', 'ChangHwan Kim', 'Se-Young Yun']",http://arxiv.org/pdf/2008.08882v2.pdf,2020-08-20,"['cs.lg', 'cs.cv', 'stat.ml']","  Model Agnostic Meta-Learning (MAML) is one of the most representative ofgradient-based meta-learning algorithms. MAML learns new tasks with a few datasamples using inner updates from a meta-initialization point and learns themeta-initialization parameters with outer updates. It has recently beenhypothesized that representation reuse, which makes little change in efficientrepresentations, is the dominant factor in the performance of themeta-initialized model through MAML in contrast to representation change, whichcauses a significant change in representations. In this study, we investigatethe necessity of representation change for the ultimate goal of few-shotlearning, which is solving domain-agnostic tasks. To this aim, we propose anovel meta-learning algorithm, called BOIL (Body Only update in Inner Loop),which updates only the body (extractor) of the model and freezes the head(classifier) during inner loop updates. BOIL leverages representation changerather than representation reuse. This is because feature vectors(representations) have to move quickly to their corresponding frozen headvectors. We visualize this property using cosine similarity, CKA, and empiricalresults without the head. BOIL empirically shows significant performanceimprovement over MAML, particularly on cross-domain tasks. The results implythat representation change in gradient-based meta-learning approaches is acritical component."
Transductive Information Maximization For Few-Shot Learning,"['Malik Boudiaf', 'Ziko Imtiaz Masud', 'Jérôme Rony', 'José Dolz', 'Pablo Piantanida', 'Ismail Ben Ayed']",http://arxiv.org/pdf/2008.11297v3.pdf,2020-08-25,"['cs.lg', 'cs.cv', 'stat.ml']","  We introduce Transductive Infomation Maximization (TIM) for few-shotlearning. Our method maximizes the mutual information between the queryfeatures and their label predictions for a given few-shot task, in conjunctionwith a supervision loss based on the support set. Furthermore, we propose a newalternating-direction solver for our mutual-information loss, whichsubstantially speeds up transductive-inference convergence over gradient-basedoptimization, while yielding similar accuracy. TIM inference is modular: it canbe used on top of any base-training feature extractor. Following standardtransductive few-shot settings, our comprehensive experiments demonstrate thatTIM outperforms state-of-the-art methods significantly across various datasetsand networks, while used on top of a fixed feature extractor trained withsimple cross-entropy on the base classes, without resorting to complexmeta-learning schemes. It consistently brings between 2% and 5% improvement inaccuracy over the best performing method, not only on all the well-establishedfew-shot benchmarks but also on more challenging scenarios,with domain shiftsand larger numbers of classes."
Class Interference Regularization,"['Bharti Munjal', 'Sikandar Amin', 'Fabio Galasso']",http://arxiv.org/pdf/2009.02396v1.pdf,2020-09-04,['cs.cv'],"  Contrastive losses yield state-of-the-art performance for personre-identification, face verification and few shot learning. They have recentlyoutperformed the cross-entropy loss on classification at the ImageNet scale andoutperformed all self-supervision prior results by a large margin (SimCLR).Simple and effective regularization techniques such as label smoothing andself-distillation do not apply anymore, because they act on multinomial labeldistributions, adopted in cross-entropy losses, and not on tuple comparativeterms, which characterize the contrastive losses.  Here we propose a novel, simple and effective regularization technique, theClass Interference Regularization (CIR), which applies to cross-entropy lossesbut is especially effective on contrastive losses. CIR perturbs the outputfeatures by randomly moving them towards the average embeddings of the negativeclasses. To the best of our knowledge, CIR is the first regularizationtechnique to act on the output features.  In experimental evaluation, the combination of CIR and a plain Siamese-netwith triplet loss yields best few-shot learning performance on the challengingtieredImageNet. CIR also improves the state-of-the-art technique in personre-identification on the Market-1501 dataset, based on triplet loss, and thestate-of-the-art technique in person search on the CUHK-SYSU dataset, based ona cross-entropy loss. Finally, on the task of classification CIR performs onpar with the popular label smoothing, as demonstrated for CIFAR-10 and -100."
Prototype Completion with Primitive Knowledge for Few-Shot Learning,"['Baoquan Zhang', 'Xutao Li', 'Yunming Ye', 'Zhichao Huang', 'Lisai Zhang']",http://arxiv.org/pdf/2009.04960v6.pdf,2020-09-10,['cs.cv'],"  Few-shot learning is a challenging task, which aims to learn a classifier fornovel classes with few examples. Pre-training based meta-learning methodseffectively tackle the problem by pre-training a feature extractor and thenfine-tuning it through the nearest centroid based meta-learning. However,results show that the fine-tuning step makes very marginal improvements. Inthis paper, 1) we figure out the key reason, i.e., in the pre-trained featurespace, the base classes already form compact clusters while novel classesspread as groups with large variances, which implies that fine-tuning thefeature extractor is less meaningful; 2) instead of fine-tuning the featureextractor, we focus on estimating more representative prototypes duringmeta-learning. Consequently, we propose a novel prototype completion basedmeta-learning framework. This framework first introduces primitive knowledge(i.e., class-level part or attribute annotations) and extracts representativeattribute features as priors. Then, we design a prototype completion network tolearn to complete prototypes with these priors. To avoid the prototypecompletion error caused by primitive knowledge noises or class differences, wefurther develop a Gaussian based prototype fusion strategy that combines themean-based and completed prototypes by exploiting the unlabeled samples.Extensive experiments show that our method: (i) can obtain more accurateprototypes; (ii) outperforms state-of-the-art techniques by 2% - 9% in terms ofclassification accuracy. Our code is available online."
Knowledge-Guided Multi-Label Few-Shot Learning for General Image  Recognition,"['Tianshui Chen', 'Liang Lin', 'Riquan Chen', 'Xiaolu Hui', 'Hefeng Wu']",http://arxiv.org/pdf/2009.09450v1.pdf,2020-09-20,['cs.cv'],"  Recognizing multiple labels of an image is a practical yet challenging task,and remarkable progress has been achieved by searching for semantic regions andexploiting label dependencies. However, current works utilize RNN/LSTM toimplicitly capture sequential region/label dependencies, which cannot fullyexplore mutual interactions among the semantic regions/labels and do notexplicitly integrate label co-occurrences. In addition, these works requirelarge amounts of training samples for each category, and they are unable togeneralize to novel categories with limited samples. To address these issues,we propose a knowledge-guided graph routing (KGGR) framework, which unifiesprior knowledge of statistical label correlations with deep neural networks.The framework exploits prior knowledge to guide adaptive informationpropagation among different categories to facilitate multi-label analysis andreduce the dependency of training samples. Specifically, it first builds astructured knowledge graph to correlate different labels based on statisticallabel co-occurrence. Then, it introduces the label semantics to guide learningsemantic-specific features to initialize the graph, and it exploits a graphpropagation network to explore graph node interactions, enabling learningcontextualized image feature representations. Moreover, we initialize eachgraph node with the classifier weights for the corresponding label and applyanother propagation network to transfer node messages through the graph. Inthis way, it can facilitate exploiting the information of correlated labels tohelp train better classifiers. We conduct extensive experiments on thetraditional multi-label image recognition (MLR) and multi-label few-shotlearning (ML-FSL) tasks and show that our KGGR framework outperforms thecurrent state-of-the-art methods by sizable margins on the public benchmarks."
Unknown Presentation Attack Detection against Rational Attackers,"['Ali Khodabakhsh', 'Zahid Akhtar']",http://arxiv.org/pdf/2010.01592v2.pdf,2020-10-04,"['cs.cv', 'cs.cr', 'cs.gt', 'cs.lg']","  Despite the impressive progress in the field of presentation attack detectionand multimedia forensics over the last decade, these systems are stillvulnerable to attacks in real-life settings. Some of the challenges forexisting solutions are the detection of unknown attacks, the ability to performin adversarial settings, few-shot learning, and explainability. In this study,these limitations are approached by reliance on a game-theoretic view formodeling the interactions between the attacker and the detector. Consequently,a new optimization criterion is proposed and a set of requirements are definedfor improving the performance of these systems in real-life settings.Furthermore, a novel detection technique is proposed using generator-basedfeature sets that are not biased towards any specific attack species. Tofurther optimize the performance on known attacks, a new loss function coinedcategorical margin maximization loss (C-marmax) is proposed which graduallyimproves the performance against the most powerful attack. The proposedapproach provides a more balanced performance across known and unknown attacksand achieves state-of-the-art performance in known and unknown attack detectioncases against rational attackers. Lastly, the few-shot learning potential ofthe proposed approach is studied as well as its ability to provide pixel-levelexplainability."
Structural Supervision Improves Few-Shot Learning and Syntactic  Generalization in Neural Language Models,"['Ethan Wilcox', 'Peng Qian', 'Richard Futrell', 'Ryosuke Kohita', 'Roger Levy', 'Miguel Ballesteros']",http://arxiv.org/pdf/2010.05725v1.pdf,2020-10-12,['cs.cl'],"  Humans can learn structural properties about a word from minimal experience,and deploy their learned syntactic representations uniformly in differentgrammatical contexts. We assess the ability of modern neural language models toreproduce this behavior in English and evaluate the effect of structuralsupervision on learning outcomes. First, we assess few-shot learningcapabilities by developing controlled experiments that probe models' syntacticnominal number and verbal argument structure generalizations for tokens seen asfew as two times during training. Second, we assess invariance properties oflearned representation: the ability of a model to transfer syntacticgeneralizations from a base context (e.g., a simple declarative active-voicesentence) to a transformed context (e.g., an interrogative sentence). We testfour models trained on the same dataset: an n-gram baseline, an LSTM, and twoLSTM-variants trained with explicit structural supervision (Dyer et al.,2016;Charniak et al., 2016). We find that in most cases, the neural models are ableto induce the proper syntactic generalizations after minimal exposure, oftenfrom just two examples during training, and that the two structurallysupervised models generalize more accurately than the LSTM model. All neuralmodels are able to leverage information learned in base contexts to driveexpectations in transformed contexts, indicating that they have learned someinvariance properties of syntax."
Learning to Learn Variational Semantic Memory,"['Xiantong Zhen', 'Yingjun Du', 'Huan Xiong', 'Qiang Qiu', 'Cees G. M. Snoek', 'Ling Shao']",http://arxiv.org/pdf/2010.10341v3.pdf,2020-10-20,['cs.lg'],"  In this paper, we introduce variational semantic memory into meta-learning toacquire long-term knowledge for few-shot learning. The variational semanticmemory accrues and stores semantic information for the probabilistic inferenceof class prototypes in a hierarchical Bayesian framework. The semantic memoryis grown from scratch and gradually consolidated by absorbing information fromtasks it experiences. By doing so, it is able to accumulate long-term, generalknowledge that enables it to learn new concepts of objects. We formulate memoryrecall as the variational inference of a latent memory variable from addressedcontents, which offers a principled way to adapt the knowledge to individualtasks. Our variational semantic memory, as a new long-term memory module,confers principled recall and update mechanisms that enable semanticinformation to be efficiently accrued and adapted for few-shot learning.Experiments demonstrate that the probabilistic modelling of prototypes achievesa more informative representation of object classes compared to deterministicvectors. The consistent new state-of-the-art performance on four benchmarksshows the benefit of variational semantic memory in boosting few-shotrecognition."
Restoring Negative Information in Few-Shot Object Detection,"['Yukuan Yang', 'Fangyun Wei', 'Miaojing Shi', 'Guoqi Li']",http://arxiv.org/pdf/2010.11714v2.pdf,2020-10-22,"['cs.cv', 'cs.ai']","  Few-shot learning has recently emerged as a new challenge in the deeplearning field: unlike conventional methods that train the deep neural networks(DNNs) with a large number of labeled data, it asks for the generalization ofDNNs on new classes with few annotated samples. Recent advances in few-shotlearning mainly focus on image classification while in this paper we focus onobject detection. The initial explorations in few-shot object detection tend tosimulate a classification scenario by using the positive proposals in imageswith respect to certain object class while discarding the negative proposals ofthat class. Negatives, especially hard negatives, however, are essential to theembedding space learning in few-shot object detection. In this paper, werestore the negative information in few-shot object detection by introducing anew negative- and positive-representative based metric learning framework and anew inference scheme with negative and positive representatives. We build ourwork on a recent few-shot pipeline RepMet with several new modules to encodenegative information for both training and testing. Extensive experiments onImageNet-LOC and PASCAL VOC show our method substantially improves thestate-of-the-art few-shot object detection solutions. Our code is available athttps://github.com/yang-yk/NP-RepMet."
A Study of Few-Shot Audio Classification,"['Piper Wolters', 'Chris Careaga', 'Brian Hutchinson', 'Lauren Phillips']",http://arxiv.org/pdf/2012.01573v1.pdf,2020-12-02,"['eess.as', 'cs.ne', 'cs.sd']","  Advances in deep learning have resulted in state-of-the-art performance formany audio classification tasks but, unlike humans, these systems traditionallyrequire large amounts of data to make accurate predictions. Not every person ororganization has access to those resources, and the organizations that do, likeour field at large, do not reflect the demographics of our country. Enablingpeople to use machine learning without significant resource hurdles isimportant, because machine learning is an increasingly useful tool for solvingproblems, and can solve a broader set of problems when put in the hands of abroader set of people. Few-shot learning is a type of machine learning designedto enable the model to generalize to new classes with very few examples. Inthis research, we address two audio classification tasks (speakeridentification and activity classification) with the Prototypical Networkfew-shot learning algorithm, and assess performance of various encoderarchitectures. Our encoders include recurrent neural networks, as well as one-and two-dimensional convolutional neural networks. We evaluate our model forspeaker identification on the VoxCeleb dataset and ICSI Meeting Corpus,obtaining 5-shot 5-way accuracies of 93.5% and 54.0%, respectively. We alsoevaluate for activity classification from audio using few-shot subsets of theKinetics~600 dataset and AudioSet, both drawn from Youtube videos, obtaining51.5% and 35.2% accuracy, respectively."
RPT: Relational Pre-trained Transformer Is Almost All You Need towards  Democratizing Data Preparation,"['Nan Tang', 'Ju Fan', 'Fangyi Li', 'Jianhong Tu', 'Xiaoyong Du', 'Guoliang Li', 'Sam Madden', 'Mourad Ouzzani']",http://arxiv.org/pdf/2012.02469v2.pdf,2020-12-04,"['cs.lg', 'cs.db']","  Can AI help automate human-easy but computer-hard data preparation tasks thatburden data scientists, practitioners, and crowd workers? We answer thisquestion by presenting RPT, a denoising auto-encoder for tuple-to-X models (Xcould be tuple, token, label, JSON, and so on). RPT is pre-trained for atuple-to-tuple model by corrupting the input tuple and then learning a model toreconstruct the original tuple. It adopts a Transformer-based neuraltranslation architecture that consists of a bidirectional encoder (similar toBERT) and a left-to-right autoregressive decoder (similar to GPT), leading to ageneralization of both BERT and GPT. The pre-trained RPT can already supportseveral common data preparation tasks such as data cleaning, auto-completionand schema matching. Better still, RPT can be fine-tuned on a wide range ofdata preparation tasks, such as value normalization, data transformation, dataannotation, etc. To complement RPT, we also discuss several appealingtechniques such as collaborative training and few-shot learning for entityresolution, and few-shot learning and NLP question-answering for informationextraction. In addition, we identify a series of research opportunities toadvance the field of data preparation."
Making Pre-trained Language Models Better Few-shot Learners,"['Tianyu Gao', 'Adam Fisch', 'Danqi Chen']",http://arxiv.org/pdf/2012.15723v2.pdf,2020-12-31,"['cs.cl', 'cs.lg']","  The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shotperformance solely by leveraging a natural-language prompt and a few taskdemonstrations as input context. Inspired by their findings, we study few-shotlearning in a more practical scenario, where we use smaller language models forwhich fine-tuning is computationally efficient. We present LM-BFF--betterfew-shot fine-tuning of language models--a suite of simple and complementarytechniques for fine-tuning language models on a small number of annotatedexamples. Our approach includes (1) prompt-based fine-tuning together with anovel pipeline for automating prompt generation; and (2) a refined strategy fordynamically and selectively incorporating demonstrations into each context.Finally, we present a systematic evaluation for analyzing few-shot performanceon a range of NLP tasks, including classification and regression. Ourexperiments demonstrate that our methods combine to dramatically outperformstandard fine-tuning procedures in this low resource setting, achieving up to30% absolute improvement, and 11% on average across all tasks. Our approachmakes minimal assumptions on task resources and domain expertise, and henceconstitutes a strong task-agnostic method for few-shot learning."
Exploring Complementary Strengths of Invariant and Equivariant  Representations for Few-Shot Learning,"['Mamshad Nayeem Rizve', 'Salman Khan', 'Fahad Shahbaz Khan', 'Mubarak Shah']",http://arxiv.org/pdf/2103.01315v2.pdf,2021-03-01,"['cs.cv', 'cs.lg']","  In many real-world problems, collecting a large number of labeled samples isinfeasible. Few-shot learning (FSL) is the dominant approach to address thisissue, where the objective is to quickly adapt to novel categories in presenceof a limited number of samples. FSL tasks have been predominantly solved byleveraging the ideas from gradient-based meta-learning and metric learningapproaches. However, recent works have demonstrated the significance ofpowerful feature representations with a simple embedding network that canoutperform existing sophisticated FSL algorithms. In this work, we build onthis insight and propose a novel training mechanism that simultaneouslyenforces equivariance and invariance to a general set of geometrictransformations. Equivariance or invariance has been employed standalone in theprevious works; however, to the best of our knowledge, they have not been usedjointly. Simultaneous optimization for both of these contrasting objectivesallows the model to jointly learn features that are not only independent of theinput transformation but also the features that encode the structure ofgeometric transformations. These complementary sets of features help generalizewell to novel classes with only a few data samples. We achieve additionalimprovements by incorporating a novel self-supervised distillation objective.Our extensive experimentation shows that even without knowledge distillationour proposed method can outperform current state-of-the-art FSL methods on fivepopular benchmark datasets."
Distance Metric-Based Learning with Interpolated Latent Features for  Location Classification in Endoscopy Image and Video,"['Mohammad Reza Mohebbian', 'Khan A. Wahid', 'Anh Dinh', 'Paul Babyn']",http://arxiv.org/pdf/2103.08504v2.pdf,2021-03-15,"['cs.cv', 'cs.ai']","  Conventional Endoscopy (CE) and Wireless Capsule Endoscopy (WCE) are knowntools for diagnosing gastrointestinal (GI) tract disorders. Detecting theanatomical location of GI tract can help clinicians to determine a moreappropriate treatment plan, can reduce repetitive endoscopy and is important indrug-delivery. There are few research that address detecting anatomicallocation of WCE and CE images using classification, mainly because ofdifficulty in collecting data and anotating them. In this study, we present afew-shot learning method based on distance metric learning which combinestransfer-learning and manifold mixup scheme for localizing endoscopy frames andcan be trained on few samples. The manifold mixup process improves few-shotlearning by increasing the number of training epochs while reducingoverfitting, as well as providing more accurate decision boundaries. A datasetis collected from 10 different anatomical positions of human GI tract. Twomodels were trained using only 78 CE and 27 WCE annotated frames to predict thelocation of 25700 and 1825 video frames from CE and WCE, respectively. Inaddition, we performed subjective evaluation using nine gastroenterologists toshow the necessaity of having an AI system for localization. Various ablationstudies and interpretations are performed to show the importance of each step,such effect of transfer-learning approach, and impact of manifold mixup onperformance. The proposed method is also compared with various methods trainedon categorical cross-entropy loss and produced better results which show thatproposed method has potential to be used for endoscopy image classification."
Real-Time Visual Object Tracking via Few-Shot Learning,"['Jinghao Zhou', 'Bo Li', 'Peng Wang', 'Peixia Li', 'Weihao Gan', 'Wei Wu', 'Junjie Yan', 'Wanli Ouyang']",http://arxiv.org/pdf/2103.10130v1.pdf,2021-03-18,['cs.cv'],"  Visual Object Tracking (VOT) can be seen as an extended task of Few-ShotLearning (FSL). While the concept of FSL is not new in tracking and has beenpreviously applied by prior works, most of them are tailored to fit specifictypes of FSL algorithms and may sacrifice running speed. In this work, wepropose a generalized two-stage framework that is capable of employing a largevariety of FSL algorithms while presenting faster adaptation speed. The firststage uses a Siamese Regional Proposal Network to efficiently propose thepotential candidates and the second stage reformulates the task of classifyingthese candidates to a few-shot classification problem. Following such acoarse-to-fine pipeline, the first stage proposes informative sparse samplesfor the second stage, where a large variety of FSL algorithms can be conductedmore conveniently and efficiently. As substantiation of the second stage, wesystematically investigate several forms of optimization-based few-shotlearners from previous works with different objective functions, optimizationmethods, or solution space. Beyond that, our framework also entails a directapplication of the majority of other FSL algorithms to visual tracking,enabling mutual communication between researchers on these two topics.Extensive experiments on the major benchmarks, VOT2018, OTB2015, NFS, UAV123,TrackingNet, and GOT-10k are conducted, demonstrating a desirable performancegain and a real-time speed."
MetaNODE: Prototype Optimization as a Neural ODE for Few-Shot Learning,"['Baoquan Zhang', 'Xutao Li', 'Shanshan Feng', 'Yunming Ye', 'Rui Ye']",http://arxiv.org/pdf/2103.14341v2.pdf,2021-03-26,['cs.cv'],"  Few-Shot Learning (FSL) is a challenging task, \emph{i.e.}, how to recognizenovel classes with few examples? Pre-training based methods effectively tacklethe problem by pre-training a feature extractor and then predicting novelclasses via a cosine nearest neighbor classifier with mean-based prototypes.Nevertheless, due to the data scarcity, the mean-based prototypes are usuallybiased. In this paper, we attempt to diminish the prototype bias by regardingit as a prototype optimization problem. To this end, we propose a novelmeta-learning based prototype optimization framework to rectify prototypes,\emph{i.e.}, introducing a meta-optimizer to optimize prototypes. Although theexisting meta-optimizers can also be adapted to our framework, they alloverlook a crucial gradient bias issue, \emph{i.e.}, the mean-based gradientestimation is also biased on sparse data. To address the issue, we regard thegradient and its flow as meta-knowledge and then propose a novel NeuralOrdinary Differential Equation (ODE)-based meta-optimizer to polish prototypes,called MetaNODE. In this meta-optimizer, we first view the mean-basedprototypes as initial prototypes, and then model the process of prototypeoptimization as continuous-time dynamics specified by a Neural ODE. A gradientflow inference network is carefully designed to learn to estimate thecontinuous gradient flow for prototype dynamics. Finally, the optimalprototypes can be obtained by solving the Neural ODE. Extensive experiments onminiImagenet, tieredImagenet, and CUB-200-2011 show the effectiveness of ourmethod."
When Few-Shot Learning Meets Video Object Detection,"['Zhongjie Yu', 'Gaoang Wang', 'Lin Chen', 'Sebastian Raschka', 'Jiebo Luo']",http://arxiv.org/pdf/2103.14724v3.pdf,2021-03-26,['cs.cv'],"  Different from static images, videos contain additional temporal and spatialinformation for better object detection. However, it is costly to obtain alarge number of videos with bounding box annotations that are required forsupervised deep learning. Although humans can easily learn to recognize newobjects by watching only a few video clips, deep learning usually suffers fromoverfitting. This leads to an important question: how to effectively learn avideo object detector from only a few labeled video clips? In this paper, westudy the new problem of few-shot learning for video object detection. We firstdefine the few-shot setting and create a new benchmark dataset for few-shotvideo object detection derived from the widely used ImageNet VID dataset. Weemploy a transfer-learning framework to effectively train the video objectdetector on a large number of base-class objects and a few video clips ofnovel-class objects. By analyzing the results of two methods under thisframework (Joint and Freeze) on our designed weak and strong base datasets, wereveal insufficiency and overfitting problems. A simple but effective method,called Thaw, is naturally developed to trade off the two problems and validateour analysis. Extensive experiments on our proposed benchmark datasets withdifferent scenarios demonstrate the effectiveness of our novel analysis in thisnew few-shot video object detection problem."
Comparing Transfer and Meta Learning Approaches on a Unified Few-Shot  Classification Benchmark,"['Vincent Dumoulin', 'Neil Houlsby', 'Utku Evci', 'Xiaohua Zhai', 'Ross Goroshin', 'Sylvain Gelly', 'Hugo Larochelle']",http://arxiv.org/pdf/2104.02638v1.pdf,2021-04-06,"['cs.lg', 'cs.cv']","  Meta and transfer learning are two successful families of approaches tofew-shot learning. Despite highly related goals, state-of-the-art advances ineach family are measured largely in isolation of each other. As a result ofdiverging evaluation norms, a direct or thorough comparison of differentapproaches is challenging. To bridge this gap, we perform a cross-family studyof the best transfer and meta learners on both a large-scale meta-learningbenchmark (Meta-Dataset, MD), and a transfer learning benchmark (Visual TaskAdaptation Benchmark, VTAB). We find that, on average, large-scale transfermethods (Big Transfer, BiT) outperform competing approaches on MD, even whentrained only on ImageNet. In contrast, meta-learning approaches struggle tocompete on VTAB when trained and validated on MD. However, BiT is not withoutlimitations, and pushing for scale does not improve performance on highlyout-of-distribution MD tasks. In performing this study, we reveal a number ofdiscrepancies in evaluation norms and study some of these in light of theperformance gap. We hope that this work facilitates sharing of insights fromeach community, and accelerates progress on few-shot learning."
"Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation  for Few-shot Learning","['Xisen Jin', 'Bill Yuchen Lin', 'Mohammad Rostami', 'Xiang Ren']",http://arxiv.org/pdf/2104.08808v4.pdf,2021-04-18,['cs.cl'],"  The ability to continuously expand knowledge over time and utilize it torapidly generalize to new tasks is a key feature of human linguisticintelligence. Existing models that pursue rapid generalization to new tasks(e.g., few-shot learning methods), however, are mostly trained in a single shoton fixed datasets, unable to dynamically expand their knowledge; whilecontinual learning algorithms are not specifically designed for rapidgeneralization. We present a new learning setup, Continual Learning of Few-ShotLearners (CLIF), to address the challenges of both learning settings in aunified setup. CLIF assumes a model learns from a sequence of diverse NLP tasksarriving sequentially, accumulating knowledge for improved generalization tonew tasks, while also retaining performance on the tasks learned earlier. Weexamine how the generalization ability is affected in the continual learningsetup, evaluate a number of continual learning algorithms, and propose a novelregularized adapter generation approach. We find that catastrophic forgettingaffects generalization ability to a less degree than performance on seen tasks;while continual learning algorithms can still bring considerable benefit to thegeneralization ability."
Rich Semantics Improve Few-shot Learning,"['Mohamed Afham', 'Salman Khan', 'Muhammad Haris Khan', 'Muzammal Naseer', 'Fahad Shahbaz Khan']",http://arxiv.org/pdf/2104.12709v2.pdf,2021-04-26,['cs.cv'],"  Human learning benefits from multi-modal inputs that often appear as richsemantics (e.g., description of an object's attributes while learning aboutit). This enables us to learn generalizable concepts from very limited visualexamples. However, current few-shot learning (FSL) methods use numerical classlabels to denote object classes which do not provide rich semantic meaningsabout the learned concepts. In this work, we show that by using 'class-level'language descriptions, that can be acquired with minimal annotation cost, wecan improve the FSL performance. Given a support set and queries, our main ideais to create a bottleneck visual feature (hybrid prototype) which is then usedto generate language descriptions of the classes as an auxiliary task duringtraining. We develop a Transformer based forward and backward encodingmechanism to relate visual and semantic tokens that can encode intricaterelationships between the two modalities. Forcing the prototypes to retainsemantic information about class description acts as a regularizer on thevisual features, improving their generalization to novel classes at inference.Furthermore, this strategy imposes a human prior on the learnedrepresentations, ensuring that the model is faithfully relating visual andsemantic concepts, thereby improving model interpretability. Our experiments onfour datasets and ablation studies show the benefit of effectively modelingrich semantics for FSL."
HetMAML: Task-Heterogeneous Model-Agnostic Meta-Learning for Few-Shot  Learning Across Modalities,"['Jiayi Chen', 'Aidong Zhang']",http://arxiv.org/pdf/2105.07889v3.pdf,2021-05-17,['cs.ai'],"  Existing gradient-based meta-learning approaches to few-shot learning assumethat all tasks have the same input feature space. However, in the real worldscenarios, there are many cases that the input structures of tasks can bedifferent, that is, different tasks may vary in the number of input modalitiesor data types. Existing meta-learners cannot handle the heterogeneous taskdistribution (HTD) as there is not only global meta-knowledge shared acrosstasks but also type-specific knowledge that distinguishes each type of tasks.To deal with task heterogeneity and promote fast within-task adaptions for eachtype of tasks, in this paper, we propose HetMAML, a task-heterogeneousmodel-agnostic meta-learning framework, which can capture both thetype-specific and globally shared knowledge and can achieve the balance betweenknowledge customization and generalization. Specifically, we design amulti-channel backbone module that encodes the input of each type of tasks intothe same length sequence of modality-specific embeddings. Then, we propose atask-aware iterative feature aggregation network which can automatically takeinto account the context of task-specific input structures and adaptivelyproject the heterogeneous input spaces to the same lower-dimensional embeddingspace of concepts. Our experiments on six task-heterogeneous datasetsdemonstrate that HetMAML successfully leverages type-specific and globallyshared meta-parameters for heterogeneous tasks and achieves fast within-taskadaptions for each type of tasks."
Few-Shot Partial-Label Learning,"['Yunfeng Zhao', 'Guoxian Yu', 'Lei Liu', 'Zhongmin Yan', 'Lizhen Cui', 'Carlotta Domeniconi']",http://arxiv.org/pdf/2106.00984v1.pdf,2021-06-02,"['cs.cl', 'cs.ai', 'cs.lg']","  Partial-label learning (PLL) generally focuses on inducing a noise-tolerantmulti-class classifier by training on overly-annotated samples, each of whichis annotated with a set of labels, but only one is the valid label. A basicpromise of existing PLL solutions is that there are sufficient partial-label(PL) samples for training. However, it is more common than not to have just fewPL samples at hand when dealing with new tasks. Furthermore, existing few-shotlearning algorithms assume precise labels of the support set; as such,irrelevant labels may seriously mislead the meta-learner and thus lead to acompromised performance. How to enable PLL under a few-shot learning setting isan important problem, but not yet well studied. In this paper, we introduce anapproach called FsPLL (Few-shot PLL). FsPLL first performs adaptive distancemetric learning by an embedding network and rectifying prototypes on the taskspreviously encountered. Next, it calculates the prototype of each class of anew task in the embedding network. An unseen example can then be classified viaits distance to each prototype. Experimental results on widely-used few-shotdatasets (Omniglot and miniImageNet) demonstrate that our FsPLL can achieve asuperior performance than the state-of-the-art methods across differentsettings, and it needs fewer samples for quickly adapting to new tasks."
One Representation to Rule Them All: Identifying Out-of-Support Examples  in Few-shot Learning with Generic Representations,"['Henry Kvinge', 'Scott Howland', 'Nico Courts', 'Lauren A. Phillips', 'John Buckheit', 'Zachary New', 'Elliott Skomski', 'Jung H. Lee', 'Sandeep Tiwari', 'Jessica Hibler', 'Courtney D. Corley', 'Nathan O. Hodas']",http://arxiv.org/pdf/2106.01423v1.pdf,2021-06-02,"['cs.lg', 'cs.ai', 'cs.cv', 'math.mg']","  The field of few-shot learning has made remarkable strides in developingpowerful models that can operate in the small data regime. Nearly all of thesemethods assume every unlabeled instance encountered will belong to a handful ofknown classes for which one has examples. This can be problematic forreal-world use cases where one routinely finds 'none-of-the-above' examples. Inthis paper we describe this challenge of identifying what we term'out-of-support' (OOS) examples. We describe how this problem is subtlydifferent from out-of-distribution detection and describe a new method ofidentifying OOS examples within the Prototypical Networks framework using afixed point which we call the generic representation. We show that our methodoutperforms other existing approaches in the literature as well as otherapproaches that we propose in this paper. Finally, we investigate how the useof such a generic point affects the geometry of a model's feature space."
Tensor feature hallucination for few-shot learning,"['Michalis Lazarou', 'Tania Stathaki', 'Yannis Avrithis']",http://arxiv.org/pdf/2106.05321v2.pdf,2021-06-09,['cs.cv'],"  Few-shot learning addresses the challenge of learning how to address noveltasks given not just limited supervision but limited data as well. Anattractive solution is synthetic data generation. However, most such methodsare overly sophisticated, focusing on high-quality, realistic data in the inputspace. It is unclear whether adapting them to the few-shot regime and usingthem for the downstream task of classification is the right approach. Previousworks on synthetic data generation for few-shot classification focus onexploiting complex models, e.g. a Wasserstein GAN with multiple regularizers ora network that transfers latent diversities from known to novel classes.  We follow a different approach and investigate how a simple andstraightforward synthetic data generation method can be used effectively. Wemake two contributions, namely we show that: (1) using a simple loss functionis more than enough for training a feature generator in the few-shot setting;and (2) learning to generate tensor features instead of vector features issuperior. Extensive experiments on miniImagenet, CUB and CIFAR-FS datasets showthat our method sets a new state of the art, outperforming more sophisticatedfew-shot data augmentation methods. The source code can be found athttps://github.com/MichalisLazarou/TFH_fewshot."
Finding Significant Features for Few-Shot Learning using Dimensionality  Reduction,"['Mauricio Mendez-Ruiz', 'Ivan Garcia Jorge Gonzalez-Zapata', 'Gilberto Ochoa-Ruiz', 'Andres Mendez-Vazquez']",http://arxiv.org/pdf/2107.06992v1.pdf,2021-07-06,"['cs.lg', 'cs.cv']","  Few-shot learning is a relatively new technique that specializes in problemswhere we have little amounts of data. The goal of these methods is to classifycategories that have not been seen before with just a handful of samples.Recent approaches, such as metric learning, adopt the meta-learning strategy inwhich we have episodic tasks conformed by support (training) data and query(test) data. Metric learning methods have demonstrated that simple models canachieve good performance by learning a similarity function to compare thesupport and the query data. However, the feature space learned by a givenmetric learning approach may not exploit the information given by a specificfew-shot task. In this work, we explore the use of dimension reductiontechniques as a way to find task-significant features helping to make betterpredictions. We measure the performance of the reduced features by assigning ascore based on the intra-class and inter-class distance, and selecting afeature reduction method in which instances of different classes are far awayand instances of the same class are close. This module helps to improve theaccuracy performance by allowing the similarity function, given by the metriclearning method, to have more discriminative features for the classification.Our method outperforms the metric learning baselines in the miniImageNetdataset by around 2% in accuracy performance."
Few Shots Are All You Need: A Progressive Few Shot Learning Approach for  Low Resource Handwritten Text Recognition,"['Mohamed Ali Souibgui', 'Alicia Fornés', 'Yousri Kessentini', 'Beáta Megyesi']",http://arxiv.org/pdf/2107.10064v3.pdf,2021-07-21,['cs.cv'],"  Handwritten text recognition in low resource scenarios, such as manuscriptswith rare alphabets, is a challenging problem. The main difficulty comes fromthe very few annotated data and the limited linguistic information (e.g.dictionaries and language models). Thus, we propose a few-shot learning-basedhandwriting recognition approach that significantly reduces the human laborannotation process, requiring only few images of each alphabet symbol. Themethod consists in detecting all the symbols of a given alphabet in a textlineimage and decoding the obtained similarity scores to the final sequence oftranscribed symbols. Our model is first pretrained on synthetic line imagesgenerated from any alphabet, even though different from the target domain. Asecond training step is then applied to diminish the gap between the source andtarget data. Since this retraining would require annotation of thousands ofhandwritten symbols together with their bounding boxes, we propose to avoidsuch human effort through an unsupervised progressive learning approach thatautomatically assigns pseudo-labels to the non-annotated data. The evaluationon different manuscript datasets show that our model can lead to competitiveresults with a significant reduction in human effort. The code will be publiclyavailable in this repository: \url{https://github.com/dali92002/HTRbyMatching}"
Few-Shot and Continual Learning with Attentive Independent Mechanisms,"['Eugene Lee', 'Cheng-Han Huang', 'Chen-Yi Lee']",http://arxiv.org/pdf/2107.14053v1.pdf,2021-07-29,"['cs.lg', 'cs.ai', 'cs.cv']","  Deep neural networks (DNNs) are known to perform well when deployed to testdistributions that shares high similarity with the training distribution.Feeding DNNs with new data sequentially that were unseen in the trainingdistribution has two major challenges -- fast adaptation to new tasks andcatastrophic forgetting of old tasks. Such difficulties paved way for theon-going research on few-shot learning and continual learning. To tackle theseproblems, we introduce Attentive Independent Mechanisms (AIM). We incorporatethe idea of learning using fast and slow weights in conjunction with thedecoupling of the feature extraction and higher-order conceptual learning of aDNN. AIM is designed for higher-order conceptual learning, modeled by a mixtureof experts that compete to learn independent concepts to solve a new task. AIMis a modular component that can be inserted into existing deep learningframeworks. We demonstrate its capability for few-shot learning by adding it toSIB and trained on MiniImageNet and CIFAR-FS, showing significant improvement.AIM is also applied to ANML and OML trained on Omniglot, CIFAR-100 andMiniImageNet to demonstrate its capability in continual learning. Code madepublicly available at https://github.com/huang50213/AIM-Fewshot-Continual."
AdapterHub Playground: Simple and Flexible Few-Shot Learning with  Adapters,"['Tilman Beck', 'Bela Bohlender', 'Christina Viehmann', 'Vincent Hane', 'Yanik Adamson', 'Jaber Khuri', 'Jonas Brossmann', 'Jonas Pfeiffer', 'Iryna Gurevych']",http://arxiv.org/pdf/2108.08103v3.pdf,2021-08-18,['cs.cl'],"  The open-access dissemination of pretrained language models through onlinerepositories has led to a democratization of state-of-the-art natural languageprocessing (NLP) research. This also allows people outside of NLP to use suchmodels and adapt them to specific use-cases. However, a certain amount oftechnical proficiency is still required which is an entry barrier for users whowant to apply these models to a certain task but lack the necessary knowledgeor resources. In this work, we aim to overcome this gap by providing a toolwhich allows researchers to leverage pretrained models without writing a singleline of code. Built upon the parameter-efficient adapter modules for transferlearning, our AdapterHub Playground provides an intuitive interface, allowingthe usage of adapters for prediction, training and analysis of textual data fora variety of NLP tasks. We present the tool's architecture and demonstrate itsadvantages with prototypical use-cases, where we show that predictiveperformance can easily be increased in a few-shot learning scenario. Finally,we evaluate its usability in a user study. We provide the code and a liveinterface at https://adapter-hub.github.io/playground."
Few Shot Activity Recognition Using Variational Inference,"['Neeraj Kumar', 'Siddhansh Narang']",http://arxiv.org/pdf/2108.08990v1.pdf,2021-08-20,"['cs.cv', 'cs.lg']","  There has been a remarkable progress in learning a model which couldrecognise novel classes with only a few labeled examples in the last few years.Few-shot learning (FSL) for action recognition is a challenging task ofrecognising novel action categories which are represented by few instances inthe training data. We propose a novel variational inference based architecturalframework (HF-AR) for few shot activity recognition. Our framework leveragesvolume-preserving Householder Flow to learn a flexible posterior distributionof the novel classes. This results in better performance as compared tostate-of-the-art few shot approaches for human activity recognition. approachconsists of base model and an adapter model. Our architecture consists of abase model and an adapter model. The base model is trained on seen classes andit computes an embedding that represent the spatial and temporal insightsextracted from the input video, e.g. combination of Resnet-152 and LSTM basedencoder-decoder model. The adapter model applies a series of Householdertransformations to compute a flexible posterior distribution that lends higheraccuracy in the few shot approach. Extensive experiments on three well-knowndatasets: UCF101, HMDB51 and Something-Something-V2, demonstrate similar orbetter performance on 1-shot and 5-shot classification as compared tostate-of-the-art few shot approaches that use only RGB frame sequence as input.To the best of our knowledge, we are the first to explore variational inferencealong with householder transformations to capture the full rank covariancematrix of posterior distribution, for few shot learning in activityrecognition."
Information Symmetry Matters: A Modal-Alternating Propagation Network  for Few-Shot Learning,"['Zhong Ji', 'Zhishen Hou', 'Xiyao Liu', 'Yanwei Pang', 'Jungong Han']",http://arxiv.org/pdf/2109.01295v1.pdf,2021-09-03,"['cs.cv', 'cs.ai', 'cs.cl']","  Semantic information provides intra-class consistency and inter-classdiscriminability beyond visual concepts, which has been employed in Few-ShotLearning (FSL) to achieve further gains. However, semantic information is onlyavailable for labeled samples but absent for unlabeled samples, in which theembeddings are rectified unilaterally by guiding the few labeled samples withsemantics. Therefore, it is inevitable to bring a cross-modal bias betweensemantic-guided samples and nonsemantic-guided samples, which results in aninformation asymmetry problem. To address this problem, we propose aModal-Alternating Propagation Network (MAP-Net) to supplement the absentsemantic information of unlabeled samples, which builds information symmetryamong all samples in both visual and semantic modalities. Specifically, theMAP-Net transfers the neighbor information by the graph propagation to generatethe pseudo-semantics for unlabeled samples guided by the completed visualrelationships and rectify the feature embeddings. In addition, due to the largediscrepancy between visual and semantic modalities, we design a RelationGuidance (RG) strategy to guide the visual relation vectors via semantics sothat the propagated information is more beneficial. Extensive experimentalresults on three semantic-labeled datasets, i.e., Caltech-UCSD-Birds 200-2011,SUN Attribute Database, and Oxford 102 Flower, have demonstrated that ourproposed method achieves promising performance and outperforms thestate-of-the-art approaches, which indicates the necessity of informationsymmetry."
Self-Taught Cross-Domain Few-Shot Learning with Weakly Supervised Object  Localization and Task-Decomposition,"['Xiyao Liu', 'Zhong Ji', 'Yanwei Pang', 'Zhongfei Zhang']",http://arxiv.org/pdf/2109.01302v1.pdf,2021-09-03,"['cs.cv', 'cs.ai']","  The domain shift between the source and target domain is the main challengein Cross-Domain Few-Shot Learning (CD-FSL). However, the target domain isabsolutely unknown during the training on the source domain, which results inlacking directed guidance for target tasks. We observe that since there aresimilar backgrounds in target domains, it can apply self-labeled samples asprior tasks to transfer knowledge onto target tasks. To this end, we propose atask-expansion-decomposition framework for CD-FSL, called Self-Taught (ST)approach, which alleviates the problem of non-target guidance by constructingtask-oriented metric spaces. Specifically, Weakly Supervised ObjectLocalization (WSOL) and self-supervised technologies are employed to enrichtask-oriented samples by exchanging and rotating the discriminative regions,which generates a more abundant task set. Then these tasks are decomposed intoseveral tasks to finish the task of few-shot recognition and rotationclassification. It helps to transfer the source knowledge onto the target tasksand focus on discriminative regions. We conduct extensive experiments under thecross-domain setting including 8 target domains: CUB, Cars, Places, Plantae,CropDieases, EuroSAT, ISIC, and ChestX. Experimental results demonstrate thatthe proposed ST approach is applicable to various metric-based models, andprovides promising improvements in CD-FSL."
GPT-3 Models are Poor Few-Shot Learners in the Biomedical Domain,"['Milad Moradi', 'Kathrin Blagec', 'Florian Haberl', 'Matthias Samwald']",http://arxiv.org/pdf/2109.02555v2.pdf,2021-09-06,"['cs.cl', 'cs.ai', 'cs.lg']","  Deep neural language models have set new breakthroughs in many tasks ofNatural Language Processing (NLP). Recent work has shown that deep transformerlanguage models (pretrained on large amounts of texts) can achieve high levelsof task-specific few-shot performance comparable to state-of-the-art models.However, the ability of these large language models in few-shot transferlearning has not yet been explored in the biomedical domain. We investigatedthe performance of two powerful transformer language models, i.e. GPT-3 andBioBERT, in few-shot settings on various biomedical NLP tasks. The experimentalresults showed that, to a great extent, both the models underperform a languagemodel fine-tuned on the full training data. Although GPT-3 had already achievednear state-of-the-art results in few-shot knowledge transfer on open-domain NLPtasks, it could not perform as effectively as BioBERT, which is orders ofmagnitude smaller than GPT-3. Regarding that BioBERT was already pretrained onlarge biomedical text corpora, our study suggests that language models maylargely benefit from in-domain pretraining in task-specific few-shot learning.However, in-domain pretraining seems not to be sufficient; novel pretrainingand few-shot learning strategies are required in the biomedical NLP domain."
Few-shot Learning via Dependency Maximization and Instance Discriminant  Analysis,"['Zejiang Hou', 'Sun-Yuan Kung']",http://arxiv.org/pdf/2109.02820v1.pdf,2021-09-07,"['cs.cv', 'cs.ai', 'cs.lg']","  We study the few-shot learning (FSL) problem, where a model learns torecognize new objects with extremely few labeled training data per category.Most of previous FSL approaches resort to the meta-learning paradigm, where themodel accumulates inductive bias through learning many training tasks so as tosolve a new unseen few-shot task. In contrast, we propose a simple approach toexploit unlabeled data accompanying the few-shot task for improving few-shotperformance. Firstly, we propose a Dependency Maximization method based on theHilbert-Schmidt norm of the cross-covariance operator, which maximizes thestatistical dependency between the embedded feature of those unlabeled data andtheir label predictions, together with the supervised loss over the supportset. We then use the obtained model to infer the pseudo-labels for thoseunlabeled data. Furthermore, we propose anInstance Discriminant Analysis toevaluate the credibility of each pseudo-labeled example and select the mostfaithful ones into an augmented support set to retrain the model as in thefirst step. We iterate the above process until the pseudo-labels for theunlabeled data becomes stable. Following the standard transductive andsemi-supervised FSL setting, our experiments show that the proposed methodout-performs previous state-of-the-art methods on four widely used benchmarks,including mini-ImageNet, tiered-ImageNet, CUB, and CIFARFS."
PPT: Pre-trained Prompt Tuning for Few-shot Learning,"['Yuxian Gu', 'Xu Han', 'Zhiyuan Liu', 'Minlie Huang']",http://arxiv.org/pdf/2109.04332v3.pdf,2021-09-09,['cs.cl'],"  Prompts for pre-trained language models (PLMs) have shown remarkableperformance by bridging the gap between pre-training tasks and variousdownstream tasks. Among these methods, prompt tuning, which freezes PLMs andonly tunes soft prompts, provides an efficient and effective solution foradapting large-scale PLMs to downstream tasks. However, prompt tuning is yet tobe fully explored. In our pilot experiments, we find that prompt tuningperforms comparably with conventional full-model fine-tuning when downstreamdata are sufficient, whereas it performs much worse under few-shot learningsettings, which may hinder the application of prompt tuning in practice. Weattribute this low performance to the manner of initializing soft prompts.Therefore, in this work, we propose to pre-train prompts by adding soft promptsinto the pre-training stage to obtain a better initialization. We name thisPre-trained Prompt Tuning framework ""PPT"". To ensure the generalization of PPT,we formulate similar classification tasks into a unified task form andpre-train soft prompts for this unified task. Extensive experiments show thattuning pre-trained prompts for downstream tasks can reach or even outperformfull-model fine-tuning under both full-data and few-shot settings. Our approachis effective and efficient for using large-scale PLMs in practice."
One-Class Meta-Learning: Towards Generalizable Few-Shot Open-Set  Classification,"['Jedrzej Kozerawski', 'Matthew Turk']",http://arxiv.org/pdf/2109.06859v1.pdf,2021-09-14,"['cs.cv', 'cs.lg']","  Real-world classification tasks are frequently required to work in anopen-set setting. This is especially challenging for few-shot learning problemsdue to the small sample size for each known category, which prevents existingopen-set methods from working effectively; however, most multiclass few-shotmethods are limited to closed-set scenarios. In this work, we address theproblem of few-shot open-set classification by first proposing methods forfew-shot one-class classification and then extending them to few-shotmulticlass open-set classification. We introduce two independent few-shotone-class classification methods: Meta Binary Cross-Entropy (Meta-BCE), whichlearns a separate feature representation for one-class classification, andOne-Class Meta-Learning (OCML), which learns to generate one-class classifiersgiven standard multiclass feature representation. Both methods can augment anyexisting few-shot learning method without requiring retraining to work in afew-shot multiclass open-set setting without degrading its closed-setperformance. We demonstrate the benefits and drawbacks of both methods indifferent problem settings and evaluate them on three standard benchmarkdatasets, miniImageNet, tieredImageNet, and Caltech-UCSD-Birds-200-2011, wherethey surpass the state-of-the-art methods in the few-shot multiclass open-setand few-shot one-class tasks."
Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and  Few-Shot Learning,"['Shaohua Wu', 'Xudong Zhao', 'Tong Yu', 'Rongguo Zhang', 'Chong Shen', 'Hongli Liu', 'Feng Li', 'Hong Zhu', 'Jiangang Luo', 'Liang Xu', 'Xuanwei Zhang']",http://arxiv.org/pdf/2110.04725v2.pdf,2021-10-10,"['cs.cl', 'cs.ai']","  Recent work like GPT-3 has demonstrated excellent performance of Zero-Shotand Few-Shot learning on many natural language processing (NLP) tasks byscaling up model size, dataset size and the amount of computation. However,training a model like GPT-3 requires huge amount of computational resourceswhich makes it challengeable to researchers. In this work, we propose a methodthat incorporates large-scale distributed training performance into modelarchitecture design. With this method, Yuan 1.0, the current largest singletonlanguage model with 245B parameters, achieves excellent performance onthousands GPUs during training, and the state-of-the-art results on NLP tasks.A data processing method is designed to efficiently filter massive amount ofraw data. The current largest high-quality Chinese corpus with 5TB high qualitytexts is built based on this method. In addition, a calibration and labelexpansion method is proposed to improve the Zero-Shot and Few-Shot performance,and steady improvement is observed on the accuracy of various tasks. Yuan 1.0presents strong capacity of natural language generation, and the generatedarticles are difficult to distinguish from the human-written ones."
LiST: Lite Prompted Self-training Makes Parameter-Efficient Few-shot  Learners,"['Yaqing Wang', 'Subhabrata Mukherjee', 'Xiaodong Liu', 'Jing Gao', 'Ahmed Hassan Awadallah', 'Jianfeng Gao']",http://arxiv.org/pdf/2110.06274v2.pdf,2021-10-12,['cs.cl'],"  We present a new method LiST is short for Lite Prompted Self-Training forparameter-efficient fine-tuning of large pre-trained language models (PLMs) forfew-shot learning. LiST improves over recent methods that adopt prompt-basedfine-tuning (FN) using two key techniques. The first is the use ofself-training to leverage large amounts of unlabeled data for prompt-based FNin few-shot settings. We use self-training in conjunction with meta-learningfor re-weighting noisy pseudo-prompt labels. Self-training is expensive as itrequires updating all the model parameters repetitively. Therefore, we use asecond technique for light-weight fine-tuning where we introduce a small numberof task-specific parameters that are fine-tuned during self-training whilekeeping the PLM encoder frozen. Our experiments show that LiST can effectivelyleverage unlabeled data to improve the model performance for few-shot learning.Additionally, the fine-tuning is efficient as it only updates a smallpercentage of parameters and the overall model footprint is reduced sinceseveral tasks can share a common PLM encoder as backbone. A comprehensive studyon six NLU tasks demonstrate LiST to improve by 35% over classic fine-tuningand 6% over prompt-based FN with 96% reduction in number of trainableparameters when fine-tuned with no more than 30 labeled examples from eachtask. With only 14M tunable parameters, LiST outperforms GPT-3 in-contextlearning by 33% on few-shot NLU tasks."
A MIMO Radar-based Few-Shot Learning Approach for Human-ID,"['Pascal Weller', 'Fady Aziz', 'Sherif Abdulatif', 'Urs Schneider', 'Marco F. Huber']",http://arxiv.org/pdf/2110.08595v2.pdf,2021-10-16,"['eess.sp', 'cs.cv']","  Radar for deep learning-based human identification has become a research areaof increasing interest. It has been shown that micro-Doppler ($\mu$-D) canreflect the walking behavior through capturing the periodic limbs'micro-motions. One of the main aspects is maximizing the number of includedclasses while considering the real-time and training dataset size constraints.In this paper, a multiple-input-multiple-output (MIMO) radar is used toformulate micro-motion spectrograms of the elevation angular velocity($\mu$-$\omega$). The effectiveness of concatenating this newly-formulatedspectrogram with the commonly used $\mu$-D is investigated. To accommodate fornon-constrained real walking motion, an adaptive cycle segmentation frameworkis utilized and a metric learning network is trained on half gait cycles($\approx$ 0.5 s). Studies on the effects of various numbers of classes(5--20), different dataset sizes, and varying observation time windows 1--2 sare conducted. A non-constrained walking dataset of 22 subjects is collectedwith different aspect angles with respect to the radar. The proposed few-shotlearning (FSL) approach achieves a classification error of 11.3 % with only 2min of training data per subject."
Task-Aware Meta Learning-based Siamese Neural Network for Classifying  Obfuscated Malware,"['Jinting Zhu', 'Julian Jang-Jaccard', 'Amardeep Singh', 'Paul A. Watters', 'Seyit Camtepe']",http://arxiv.org/pdf/2110.13409v3.pdf,2021-10-26,"['cs.cr', 'cs.ai']","  Malware authors apply different techniques of control flow obfuscation, inorder to create new malware variants to avoid detection. Existing Siameseneural network (SNN)-based malware detection methods fail to correctly classifydifferent malware families when such obfuscated malware samples are present inthe training dataset, resulting in high false-positive rates. To address thisissue, we propose a novel task-aware few-shot-learning-based Siamese NeuralNetwork that is resilient against the presence of malware variants affected bysuch control flow obfuscation techniques. Using the average entropy features ofeach malware family as inputs, in addition to the image features, our modelgenerates the parameters for the feature layers, to more accurately adjust thefeature embedding for different malware families, each of which has obfuscatedmalware variants. In addition, our proposed method can classify malwareclasses, even if there are only one or a few training samples available. Ourmodel utilizes few-shot learning with the extracted features of a pre-trainednetwork (e.g., VGG-16), to avoid the bias typically associated with a modeltrained with a limited number of training samples. Our proposed approach ishighly effective in recognizing unique malware signatures, thus correctlyclassifying malware samples that belong to the same malware family, even in thepresence of obfuscated malware variants. Our experimental results, validated byN-way on N-shot learning, show that our model is highly effective inclassification accuracy, exceeding a rate \textgreater 91\%, compared to othersimilar methods."
Non-Gaussian Gaussian Processes for Few-Shot Regression,"['Marcin Sendera', 'Jacek Tabor', 'Aleksandra Nowak', 'Andrzej Bedychaj', 'Massimiliano Patacchiola', 'Tomasz Trzciński', 'Przemysław Spurek', 'Maciej Zięba']",http://arxiv.org/pdf/2110.13561v1.pdf,2021-10-26,['cs.lg'],"  Gaussian Processes (GPs) have been widely used in machine learning to modeldistributions over functions, with applications including multi-modalregression, time-series prediction, and few-shot learning. GPs are particularlyuseful in the last application since they rely on Normal distributions andenable closed-form computation of the posterior probability function.Unfortunately, because the resulting posterior is not flexible enough tocapture complex distributions, GPs assume high similarity between subsequenttasks - a requirement rarely met in real-world conditions. In this work, weaddress this limitation by leveraging the flexibility of Normalizing Flows tomodulate the posterior predictive distribution of the GP. This makes the GPposterior locally non-Gaussian, therefore we name our method Non-GaussianGaussian Processes (NGGPs). More precisely, we propose an invertible ODE-basedmapping that operates on each component of the random variable vectors andshares the parameters across all of them. We empirically tested the flexibilityof NGGPs on various few-shot learning regression datasets, showing that themapping can incorporate context embedding information to model different noiselevels for periodic functions. As a result, our method shares the structure ofthe problem between subsequent tasks, but the contextualization allows foradaptation to dissimilarities. NGGPs outperform the competing state-of-the-artapproaches on a diversified set of benchmarks and applications."
Influential Prototypical Networks for Few Shot Learning: A  Dermatological Case Study,"['Ranjana Roy Chowdhury', 'Deepti R. Bathula']",http://arxiv.org/pdf/2111.00698v5.pdf,2021-11-01,"['eess.iv', 'cs.cv', 'cs.lg']","  Prototypical network (PN) is a simple yet effective few shot learningstrategy. It is a metric-based meta-learning technique where classification isperformed by computing Euclidean distances to prototypical representations ofeach class. Conventional PN attributes equal importance to all samples andgenerates prototypes by simply averaging the support sample embeddingsbelonging to each class. In this work, we propose a novel version of PN thatattributes weights to support samples corresponding to their influence on thesupport sample distribution. Influence weights of samples are calculated basedon maximum mean discrepancy (MMD) between the mean embeddings of sampledistributions including and excluding the sample. Comprehensive evaluation ofour proposed influential PN (IPNet) is performed by comparing its performancewith other baseline PNs on three different benchmark dermatological datasets.IPNet outperforms all baseline models with compelling results across all threedatasets and various N-way, K-shot classification tasks. Findings fromcross-domain adaptation experiments further establish the robustness andgeneralizability of IPNet."
MDFM: Multi-Decision Fusing Model for Few-Shot Learning,"['Shuai Shao', 'Lei Xing', 'Rui Xu', 'Weifeng Liu', 'Yan-Jiang Wang', 'Bao-Di Liu']",http://arxiv.org/pdf/2112.00690v2.pdf,2021-12-01,"['cs.cv', 'cs.lg']","  In recent years, researchers pay growing attention to the few-shot learning(FSL) task to address the data-scarce problem. A standard FSL framework iscomposed of two components: i) Pre-train. Employ the base data to generate aCNN-based feature extraction model (FEM). ii) Meta-test. Apply the trained FEMto the novel data (category is different from base data) to acquire the featureembeddings and recognize them. Although researchers have made remarkablebreakthroughs in FSL, there still exists a fundamental problem. Since thetrained FEM with base data usually cannot adapt to the novel class flawlessly,the novel data's feature may lead to the distribution shift problem. To addressthis challenge, we hypothesize that even if most of the decisions based ondifferent FEMs are viewed as weak decisions, which are not available for allclasses, they still perform decently in some specific categories. Inspired bythis assumption, we propose a novel method Multi-Decision Fusing Model (MDFM),which comprehensively considers the decisions based on multiple FEMs to enhancethe efficacy and robustness of the model. MDFM is a simple, flexible,non-parametric method that can directly apply to the existing FEMs. Besides, weextend the proposed MDFM to two FSL settings (i.e., supervised andsemi-supervised settings). We evaluate the proposed method on five benchmarkdatasets and achieve significant improvements of 3.4%-7.3% compared withstate-of-the-arts."
Which images to label for few-shot medical landmark detection?,"['Quan Quan', 'Qingsong Yao', 'Jun Li', 'S. Kevin Zhou']",http://arxiv.org/pdf/2112.04386v2.pdf,2021-12-07,"['eess.iv', 'cs.cv', 'cs.lg']","  The success of deep learning methods relies on the availability ofwell-labeled large-scale datasets. However, for medical images, annotating suchabundant training data often requires experienced radiologists and consumestheir limited time. Few-shot learning is developed to alleviate this burden,which achieves competitive performances with only several labeled data.However, a crucial yet previously overlooked problem in few-shot learning isabout the selection of template images for annotation before learning, whichaffects the final performance. We herein propose a novel Sample Choosing Policy(SCP) to select ""the most worthy"" images for annotation, in the context offew-shot medical landmark detection. SCP consists of three parts: 1)Self-supervised training for building a pre-trained deep model to extractfeatures from radiological images, 2) Key Point Proposal for localizinginformative patches, and 3) Representative Score Estimation for searching themost representative samples or templates. The advantage of SCP is demonstratedby various experiments on three widely-used public datasets. For one-shotmedical landmark detection, its use reduces the mean radial errors onCephalometric and HandXray datasets by 14.2% (from 3.595mm to 3.083mm) and35.5% (4.114mm to 2.653mm), respectively."
Hybrid Graph Neural Networks for Few-Shot Learning,"['Tianyuan Yu', 'Sen He', 'Yi-Zhe Song', 'Tao Xiang']",http://arxiv.org/pdf/2112.06538v1.pdf,2021-12-13,['cs.cv'],"  Graph neural networks (GNNs) have been used to tackle the few-shot learning(FSL) problem and shown great potentials under the transductive setting.However under the inductive setting, existing GNN based methods are lesscompetitive. This is because they use an instance GNN as a labelpropagation/classification module, which is jointly meta-learned with a featureembedding network. This design is problematic because the classifier needs toadapt quickly to new tasks while the embedding does not. To overcome thisproblem, in this paper we propose a novel hybrid GNN (HGNN) model consisting oftwo GNNs, an instance GNN and a prototype GNN. Instead of label propagation,they act as feature embedding adaptation modules for quick adaptation of themeta-learned feature embedding to new tasks. Importantly they are designed todeal with a fundamental yet often neglected challenge in FSL, that is, withonly a handful of shots per class, any few-shot classifier would be sensitiveto badly sampled shots which are either outliers or can cause inter-classdistribution overlapping. %Our two GNNs are designed to address these two typesof poorly sampled few-shots respectively and their complementarity is exploitedin the hybrid GNN model. Extensive experiments show that our HGNN obtains newstate-of-the-art on three FSL benchmarks."
"Beyond Simple Meta-Learning: Multi-Purpose Models for Multi-Domain,  Active and Continual Few-Shot Learning","['Peyman Bateni', 'Jarred Barber', 'Raghav Goyal', 'Vaden Masrani', 'Jan-Willem van de Meent', 'Leonid Sigal', 'Frank Wood']",http://arxiv.org/pdf/2201.05151v2.pdf,2022-01-13,['cs.cv'],"  Modern deep learning requires large-scale extensively labelled datasets fortraining. Few-shot learning aims to alleviate this issue by learningeffectively from few labelled examples. In previously proposed few-shot visualclassifiers, it is assumed that the feature manifold, where classifierdecisions are made, has uncorrelated feature dimensions and uniform featurevariance. In this work, we focus on addressing the limitations arising fromthis assumption by proposing a variance-sensitive class of models that operatesin a low-label regime. The first method, Simple CNAPS, employs a hierarchicallyregularized Mahalanobis-distance based classifier combined with a state of theart neural adaptive feature extractor to achieve strong performance onMeta-Dataset, mini-ImageNet and tiered-ImageNet benchmarks. We further extendthis approach to a transductive learning setting, proposing Transductive CNAPS.This transductive method combines a soft k-means parameter refinement procedurewith a two-step task encoder to achieve improved test-time classificationaccuracy using unlabelled data. Transductive CNAPS achieves state of the artperformance on Meta-Dataset. Finally, we explore the use of our methods (Simpleand Transductive) for ""out of the box"" continual and active learning. Extensiveexperiments on large scale benchmarks illustrate robustness and versatility ofthis, relatively speaking, simple class of models. All trained modelcheckpoints and corresponding source codes have been made publicly available."
When Facial Expression Recognition Meets Few-Shot Learning: A Joint and  Alternate Learning Framework,"['Xinyi Zou', 'Yan Yan', 'Jing-Hao Xue', 'Si Chen', 'Hanzi Wang']",http://arxiv.org/pdf/2201.06781v1.pdf,2022-01-18,['cs.cv'],"  Human emotions involve basic and compound facial expressions. However,current research on facial expression recognition (FER) mainly focuses on basicexpressions, and thus fails to address the diversity of human emotions inpractical scenarios. Meanwhile, existing work on compound FER relies heavily onabundant labeled compound expression training data, which are often laboriouslycollected under the professional instruction of psychology. In this paper, westudy compound FER in the cross-domain few-shot learning setting, where only afew images of novel classes from the target domain are required as a reference.In particular, we aim to identify unseen compound expressions with the modeltrained on easily accessible basic expression datasets. To alleviate theproblem of limited base classes in our FER task, we propose a novel EmotionGuided Similarity Network (EGS-Net), consisting of an emotion branch and asimilarity branch, based on a two-stage learning framework. Specifically, inthe first stage, the similarity branch is jointly trained with the emotionbranch in a multi-task fashion. With the regularization of the emotion branch,we prevent the similarity branch from overfitting to sampled base classes thatare highly overlapped across different episodes. In the second stage, theemotion branch and the similarity branch play a ""two-student game"" toalternately learn from each other, thereby further improving the inferenceability of the similarity branch on unseen compound expressions. Experimentalresults on both in-the-lab and in-the-wild compound expression datasetsdemonstrate the superiority of our proposed method against severalstate-of-the-art methods."
"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and  Languages","['Emanuele Bugliarello', 'Fangyu Liu', 'Jonas Pfeiffer', 'Siva Reddy', 'Desmond Elliott', 'Edoardo Maria Ponti', 'Ivan Vulić']",http://arxiv.org/pdf/2201.11732v2.pdf,2022-01-27,"['cs.cl', 'cs.cv']","  Reliable evaluation benchmarks designed for replicability andcomprehensiveness have driven progress in machine learning. Due to the lack ofa multilingual benchmark, however, vision-and-language research has mostlyfocused on English language tasks. To fill this gap, we introduce theImage-Grounded Language Understanding Evaluation benchmark. IGLUE bringstogether - by both aggregating pre-existing datasets and creating new ones -visual question answering, cross-modal retrieval, grounded reasoning, andgrounded entailment tasks across 20 diverse languages. Our benchmark enablesthe evaluation of multilingual multimodal models for transfer learning, notonly in a zero-shot setting, but also in newly defined few-shot learningsetups. Based on the evaluation of the available state-of-the-art models, wefind that translate-test transfer is superior to zero-shot transfer and thatfew-shot learning is hard to harness for many tasks. Moreover, downstreamperformance is partially explained by the amount of available unlabelledtextual data for pretraining, and only weakly by the typological distance oftarget-source languages. We hope to encourage future research efforts in thisarea by releasing the benchmark to the community."
Understanding Cross-Domain Few-Shot Learning Based on Domain Similarity  and Few-Shot Difficulty,"['Jaehoon Oh', 'Sungnyun Kim', 'Namgyu Ho', 'Jin-Hwa Kim', 'Hwanjun Song', 'Se-Young Yun']",http://arxiv.org/pdf/2202.01339v3.pdf,2022-02-01,['cs.lg'],"  Cross-domain few-shot learning (CD-FSL) has drawn increasing attention forhandling large differences between the source and target domains--an importantconcern in real-world scenarios. To overcome these large differences, recentworks have considered exploiting small-scale unlabeled data from the targetdomain during the pre-training stage. This data enables self-supervisedpre-training on the target domain, in addition to supervised pre-training onthe source domain. In this paper, we empirically investigate which pre-trainingis preferred based on domain similarity and few-shot difficulty of the targetdomain. We discover that the performance gain of self-supervised pre-trainingover supervised pre-training becomes large when the target domain is dissimilarto the source domain, or the target domain itself has low few-shot difficulty.We further design two pre-training schemes, mixed-supervised and two-stagelearning, that improve performance. In this light, we present six findings forCD-FSL, which are supported by extensive experiments and analyses on threesource and eight target benchmark datasets with varying levels of domainsimilarity and few-shot difficulty. Our code is available athttps://github.com/sungnyun/understanding-cdfsl."
Few-shot Learning as Cluster-induced Voronoi Diagrams: A Geometric  Approach,"['Chunwei Ma', 'Ziyun Huang', 'Mingchen Gao', 'Jinhui Xu']",http://arxiv.org/pdf/2202.02471v1.pdf,2022-02-05,"['cs.lg', 'cs.cv']","  Few-shot learning (FSL) is the process of rapid generalization from abundantbase samples to inadequate novel samples. Despite extensive research in recentyears, FSL is still not yet able to generate satisfactory solutions for a widerange of real-world applications. To confront this challenge, we study the FSLproblem from a geometric point of view in this paper. One observation is thatthe widely embraced ProtoNet model is essentially a Voronoi Diagram (VD) in thefeature space. We retrofit it by making use of a recent advance incomputational geometry called Cluster-induced Voronoi Diagram (CIVD). Startingfrom the simplest nearest neighbor model, CIVD gradually incorporatescluster-to-point and then cluster-to-cluster relationships for spacesubdivision, which is used to improve the accuracy and robustness at multiplestages of FSL. Specifically, we use CIVD (1) to integrate parametric andnonparametric few-shot classifiers; (2) to combine feature representation andsurrogate representation; (3) and to leverage feature-level,transformation-level, and geometry-level heterogeneities for a better ensemble.Our CIVD-based workflow enables us to achieve new state-of-the-art results onmini-ImageNet, CUB, and tiered-ImagenNet datasets, with ${\sim}2\%{-}5\%$improvements upon the next best. To summarize, CIVD provides a mathematicallyelegant and geometrically interpretable framework that compensates for extremedata insufficiency, prevents overfitting, and allows for fast geometricensemble for thousands of individual VD. These together make FSL stronger."
Towards better understanding and better generalization of few-shot  classification in histology images with contrastive learning,"['Jiawei Yang', 'Hanbo Chen', 'Jiangpeng Yan', 'Xiaoyu Chen', 'Jianhua Yao']",http://arxiv.org/pdf/2202.09059v1.pdf,2022-02-18,"['eess.iv', 'cs.cv']","  Few-shot learning is an established topic in natural images for years, butfew work is attended to histology images, which is of high clinical value sincewell-labeled datasets and rare abnormal samples are expensive to collect. Here,we facilitate the study of few-shot learning in histology images by setting upthree cross-domain tasks that simulate real clinics problems. To enablelabel-efficient learning and better generalizability, we propose to incorporatecontrastive learning (CL) with latent augmentation (LA) to build a few-shotsystem. CL learns useful representations without manual labels, while LAtransfers semantic variations of the base dataset in an unsupervised way. Thesetwo components fully exploit unlabeled training data and can scale gracefullyto other label-hungry problems. In experiments, we find i) models learned by CLgeneralize better than supervised learning for histology images in unseenclasses, and ii) LA brings consistent gains over baselines. Prior studies ofself-supervised learning mainly focus on ImageNet-like images, which onlypresent a dominant object in their centers. Recent attention has been paid toimages with multi-objects and multi-textures. Histology images are a naturalchoice for such a study. We show the superiority of CL over supervised learningin terms of generalization for such data and provide our empiricalunderstanding for this observation. The findings in this work could contributeto understanding how the model generalizes in the context of bothrepresentation learning and histological image analysis. Code is available."
"FewSense, Towards a Scalable and Cross-Domain Wi-Fi Sensing System Using  Few-Shot Learning","['Guolin Yin', 'Junqing Zhang', 'Guanxiong Shen', 'Yingying Chen']",http://arxiv.org/pdf/2203.02014v1.pdf,2022-03-03,['eess.sp'],"  Wi-Fi sensing can classify human activities because each activity causesunique changes to the channel state information (CSI). Existing WiFi sensingsuffers from limited scalability as the system needs to be retrained whenevernew activities are added, which cause overheads of data collection andretraining. Cross-domain sensing may fail because the mapping betweenactivities and CSI variations is destroyed when a different environment or user(domain) is involved. This paper proposed a few-shot learning-based WiFisensing system, named FewSense, which can recognise novel classes in unseendomains with only few samples. Specifically, a feature extractor waspre-trained offline using the source domain data. When the system was appliedin the target domain, few samples were used to fine-tune the feature extractorfor domain adaptation. Inference was made by computing the cosine similarity.FewSense can further boost the classification accuracy by collaborativelyfusing inference from multiple receivers. We evaluated the performance usingthree public datasets, i.e., SignFi, Widar, and Wiar. The results show thatFewSense with five-shot learning recognised novel classes in unseen domainswith an accuracy of 90.3\%, 96.5\% ,82.7\% on SignFi, Widar, and Wiar datasets,respectively. Our collaborative sensing model improved system performance by anaverage of 30\%."
Automated Few-Shot Time Series Forecasting based on Bi-level Programming,"['Jiangjiao Xu', 'Ke Li']",http://arxiv.org/pdf/2203.03328v1.pdf,2022-03-07,"['cs.lg', 'cs.ne']","  New micro-grid design with renewable energy sources and battery storagesystems can help improve greenhouse gas emissions and reduce the operationalcost. To provide an effective short-/long-term forecasting of both energygeneration and load demand, time series predictive modeling has been one of thekey tools to guide the optimal decision-making for planning and operation. Oneof the critical challenges of time series renewable energy forecasting is thelack of historical data to train an adequate predictive model. Moreover, theperformance of a machine learning model is sensitive to the choice of itscorresponding hyperparameters. Bearing these considerations in mind, this paperdevelops a BiLO-Auto-TSF/ML framework that automates the optimal design of afew-shot learning pipeline from a bi-level programming perspective.Specifically, the lower-level meta-learning helps boost the base-learner tomitigate the small data challenge while the hyperparameter optimization at theupper level proactively searches for the optimal hyperparameter configurationsfor both base- and meta-learners. Note that the proposed framework is sogeneral that any off-the-shelf machine learning method can be used in a plug-inmanner. Comprehensive experiments fully demonstrate the effectiveness of ourproposed BiLO-Auto-TSF/ML framework to search for a high-performance few-shotlearning pipeline for various energy sources."
Attribute Surrogates Learning and Spectral Tokens Pooling in  Transformers for Few-shot Learning,"['Yangji He', 'Weihan Liang', 'Dongyang Zhao', 'Hong-Yu Zhou', 'Weifeng Ge', 'Yizhou Yu', 'Wenqiang Zhang']",http://arxiv.org/pdf/2203.09064v1.pdf,2022-03-17,"['cs.cv', 'cs.ai', 'cs.lg']","  This paper presents new hierarchically cascaded transformers that can improvedata efficiency through attribute surrogates learning and spectral tokenspooling. Vision transformers have recently been thought of as a promisingalternative to convolutional neural networks for visual recognition. But whenthere is no sufficient data, it gets stuck in overfitting and shows inferiorperformance. To improve data efficiency, we propose hierarchically cascadedtransformers that exploit intrinsic image structures through spectral tokenspooling and optimize the learnable parameters through latent attributesurrogates. The intrinsic image structure is utilized to reduce the ambiguitybetween foreground content and background noise by spectral tokens pooling. Andthe attribute surrogate learning scheme is designed to benefit from the richvisual information in image-label pairs instead of simple visual conceptsassigned by their labels. Our Hierarchically Cascaded Transformers, calledHCTransformers, is built upon a self-supervised learning framework DINO and istested on several popular few-shot learning benchmarks.  In the inductive setting, HCTransformers surpass the DINO baseline by a largemargin of 9.7% 5-way 1-shot accuracy and 9.17% 5-way 5-shot accuracy onminiImageNet, which demonstrates HCTransformers are efficient to extractdiscriminative features. Also, HCTransformers show clear advantages over SOTAfew-shot classification methods in both 5-way 1-shot and 5-way 5-shot settingson four popular benchmark datasets, including miniImageNet, tieredImageNet,FC100, and CIFAR-FS. The trained weights and codes are available athttps://github.com/StomachCold/HCTransformers."
Incremental Few-Shot Learning via Implanting and Compressing,"['Yiting Li', 'Haiyue Zhu', 'Xijia Feng', 'Zilong Cheng', 'Jun Ma', 'Cheng Xiang', 'Prahlad Vadakkepat', 'Tong Heng Lee']",http://arxiv.org/pdf/2203.10297v2.pdf,2022-03-19,['cs.cv'],"  This work focuses on tackling the challenging but realistic visual task ofIncremental Few-Shot Learning (IFSL), which requires a model to continuallylearn novel classes from only a few examples while not forgetting the baseclasses on which it was pre-trained. Our study reveals that the challenges ofIFSL lie in both inter-class separation and novel-class representation. Dur tointra-class variation, a novel class may implicitly leverage the knowledge frommultiple base classes to construct its feature representation. Hence, simplyreusing the pre-trained embedding space could lead to a scattered featuredistribution and result in category confusion. To address such issues, wepropose a two-step learning strategy referred to as \textbf{Im}planting and\textbf{Co}mpressing (\textbf{IMCO}), which optimizes both feature spacepartition and novel class reconstruction in a systematic manner. Specifically,in the \textbf{Implanting} step, we propose to mimic the data distribution ofnovel classes with the assistance of data-abundant base set, so that a modelcould learn semantically-rich features that are beneficial for discriminatingbetween the base and other unseen classes. In the \textbf{Compressing} step, weadapt the feature extractor to precisely represent each novel class forenhancing intra-class compactness, together with a regularized parameterupdating rule for preventing aggressive model updating. Finally, we demonstratethat IMCO outperforms competing baselines with a significant margin, both inimage classification task and more challenging object detection task."
Selecting task with optimal transport self-supervised learning for  few-shot classification,"['Renjie Xu', 'Xinghao Yang', 'Baodi Liu', 'Kai Zhang', 'Weifeng Liu']",http://arxiv.org/pdf/2204.00289v1.pdf,2022-04-01,"['cs.cv', 'cs.ai', 'cs.lg']","  Few-Shot classification aims at solving problems that only a few samples areavailable in the training process. Due to the lack of samples, researchersgenerally employ a set of training tasks from other domains to assist thetarget task, where the distribution between assistant tasks and the target taskis usually different. To reduce the distribution gap, several lines of methodshave been proposed, such as data augmentation and domain alignment. However,one common drawback of these algorithms is that they ignore the similarity taskselection before training. The fundamental problem is to push the auxiliarytasks close to the target task. In this paper, we propose a novel taskselecting algorithm, named Optimal Transport Task Selecting (OTTS), toconstruct a training set by selecting similar tasks for Few-Shot learning.Specifically, the OTTS measures the task similarity by calculating the optimaltransport distance and completes the model training via a self-supervisedstrategy. By utilizing the selected tasks with OTTS, the training process ofFew-Shot learning become more stable and effective. Other proposed methodsincluding data augmentation and domain alignment can be used in the meantimewith OTTS. We conduct extensive experiments on a variety of datasets, includingMiniImageNet, CIFAR, CUB, Cars, and Places, to evaluate the effectiveness ofOTTS. Experimental results validate that our OTTS outperforms the typicalbaselines, i.e., MAML, matchingnet, protonet, by a large margin (averagely1.72\% accuracy improvement)."
PERFECT: Prompt-free and Efficient Few-shot Learning with Language  Models,"['Rabeeh Karimi Mahabadi', 'Luke Zettlemoyer', 'James Henderson', 'Marzieh Saeidi', 'Lambert Mathias', 'Veselin Stoyanov', 'Majid Yazdani']",http://arxiv.org/pdf/2204.01172v2.pdf,2022-04-03,['cs.cl'],"  Current methods for few-shot fine-tuning of pretrained masked language models(PLMs) require carefully engineered prompts and verbalizers for each new taskto convert examples into a cloze-format that the PLM can score. In this work,we propose PERFECT, a simple and efficient method for few-shot fine-tuning ofPLMs without relying on any such handcrafting, which is highly effective givenas few as 32 data points. PERFECT makes two key design choices: First, we showthat manually engineered task prompts can be replaced with task-specificadapters that enable sample-efficient fine-tuning and reduce memory and storagecosts by roughly factors of 5 and 100, respectively. Second, instead of usinghandcrafted verbalizers, we learn new multi-token label embeddings duringfine-tuning, which are not tied to the model vocabulary and which allow us toavoid complex auto-regressive decoding. These embeddings are not only learnablefrom limited data but also enable nearly 100x faster training and inference.Experiments on a wide range of few-shot NLP tasks demonstrate that PERFECT,while being simple and efficient, also outperforms existing state-of-the-artfew-shot learning methods. Our code is publicly available athttps://github.com/facebookresearch/perfect.git."
Realistic Evaluation of Transductive Few-Shot Learning,"['Olivier Veilleux', 'Malik Boudiaf', 'Pablo Piantanida', 'Ismail Ben Ayed']",http://arxiv.org/pdf/2204.11181v1.pdf,2022-04-24,"['cs.lg', 'cs.cv']","  Transductive inference is widely used in few-shot learning, as it leveragesthe statistics of the unlabeled query set of a few-shot task, typicallyyielding substantially better performances than its inductive counterpart. Thecurrent few-shot benchmarks use perfectly class-balanced tasks at inference. Weargue that such an artificial regularity is unrealistic, as it assumes that themarginal label probability of the testing samples is known and fixed to theuniform distribution. In fact, in realistic scenarios, the unlabeled query setscome with arbitrary and unknown label marginals. We introduce and study theeffect of arbitrary class distributions within the query sets of few-shot tasksat inference, removing the class-balance artefact. Specifically, we model themarginal probabilities of the classes as Dirichlet-distributed randomvariables, which yields a principled and realistic sampling within the simplex.This leverages the current few-shot benchmarks, building testing tasks witharbitrary class distributions. We evaluate experimentally state-of-the-arttransductive methods over 3 widely used data sets, and observe, surprisingly,substantial performance drops, even below inductive methods in some cases.Furthermore, we propose a generalization of the mutual-information loss, basedon $\alpha$-divergences, which can handle effectively class-distributionvariations. Empirically, we show that our transductive $\alpha$-divergenceoptimization outperforms state-of-the-art methods across several data sets,models and few-shot settings. Our code is publicly available athttps://github.com/oveilleux/Realistic_Transductive_Few_Shot."
On the Effect of Pretraining Corpora on In-context Learning by a  Large-scale Language Model,"['Seongjin Shin', 'Sang-Woo Lee', 'Hwijeen Ahn', 'Sungdong Kim', 'HyoungSeok Kim', 'Boseop Kim', 'Kyunghyun Cho', 'Gichang Lee', 'Woomyoung Park', 'Jung-Woo Ha', 'Nako Sung']",http://arxiv.org/pdf/2204.13509v2.pdf,2022-04-28,['cs.cl'],"  Many recent studies on large-scale language models have reported successfulin-context zero- and few-shot learning ability. However, the in-depth analysisof when in-context learning occurs is still lacking. For example, it is unknownhow in-context learning performance changes as the training corpus varies.Here, we investigate the effects of the source and size of the pretrainingcorpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. Fromour in-depth investigation, we introduce the following observations: (1)in-context learning performance heavily depends on the corpus domain source,and the size of the pretraining corpus does not necessarily determine theemergence of in-context learning, (2) in-context learning ability can emergewhen a language model is trained on a combination of multiple corpora, evenwhen each corpus does not result in in-context learning on its own, (3)pretraining with a corpus related to a downstream task does not alwaysguarantee the competitive in-context learning performance of the downstreamtask, especially in the few-shot setting, and (4) the relationship betweenlanguage modeling (measured in perplexity) and in-context learning does notalways correlate: e.g., low perplexity does not always imply high in-contextfew-shot learning performance."
Few-shot learning for medical text: A systematic review,"['Yao Ge', 'Yuting Guo', 'Yuan-Chi Yang', 'Mohammed Ali Al-Garadi', 'Abeed Sarker']",http://arxiv.org/pdf/2204.14081v1.pdf,2022-04-21,"['cs.cl', 'cs.lg']","  Objective: Few-shot learning (FSL) methods require small numbers of labeledinstances for training. As many medical topics have limited annotated textualdata in practical settings, FSL-based natural language processing (NLP) methodshold substantial promise. We aimed to conduct a systematic review to explorethe state of FSL methods for medical NLP. Materials and Methods: We searchedfor articles published between January 2016 and August 2021 usingPubMed/Medline, Embase, ACL Anthology, and IEEE Xplore Digital Library. Toidentify the latest relevant methods, we also searched other sources such aspreprint servers (eg., medRxiv) via Google Scholar. We included all articlesthat involved FSL and any type of medical text. We abstracted articles based ondata source(s), aim(s), training set size(s), primary method(s)/approach(es),and evaluation method(s). Results: 31 studies met our inclusion criteria-allpublished after 2018; 22 (71%) since 2020. Concept extraction/named entityrecognition was the most frequently addressed task (13/31; 42%), followed bytext classification (10/31; 32%). Twenty-one (68%) studies reconstructedexisting datasets to create few-shot scenarios synthetically, and MIMIC-III wasthe most frequently used dataset (7/31; 23%). Common methods included FSL withattention mechanisms (12/31; 39%), prototypical networks (8/31; 26%), andmeta-learning (6/31; 19%). Discussion: Despite the potential for FSL inbiomedical NLP, progress has been limited compared to domain-independent FSL.This may be due to the paucity of standardized, public datasets, and therelative underperformance of FSL methods on biomedical topics. Creation andrelease of specialized datasets for biomedical FSL may aid method developmentby enabling comparative analyses."
FAITH: Few-Shot Graph Classification with Hierarchical Task Graphs,"['Song Wang', 'Yushun Dong', 'Xiao Huang', 'Chen Chen', 'Jundong Li']",http://arxiv.org/pdf/2205.02435v2.pdf,2022-05-05,"['cs.lg', 'cs.ai']","  Few-shot graph classification aims at predicting classes for graphs, givenlimited labeled graphs for each class. To tackle the bottleneck of labelscarcity, recent works propose to incorporate few-shot learning frameworks forfast adaptations to graph classes with limited labeled graphs. Specifically,these works propose to accumulate meta-knowledge across diverse meta-trainingtasks, and then generalize such meta-knowledge to the target task with adisjoint label set. However, existing methods generally ignore taskcorrelations among meta-training tasks while treating them independently.Nevertheless, such task correlations can advance the model generalization tothe target task for better classification performance. On the other hand, itremains non-trivial to utilize task correlations due to the complex componentsin a large number of meta-training tasks. To deal with this, we propose a novelfew-shot learning framework FAITH that captures task correlations viaconstructing a hierarchical task graph at different granularities. Then wefurther design a loss-based sampling strategy to select tasks with morecorrelated classes. Moreover, a task-specific classifier is proposed to utilizethe learned task correlations for few-shot classification. Extensiveexperiments on four prevalent few-shot graph classification datasetsdemonstrate the superiority of FAITH over other state-of-the-art baselines."
Feature Extractor Stacking for Cross-domain Few-shot Learning,"['Hongyu Wang', 'Eibe Frank', 'Bernhard Pfahringer', 'Michael Mayo', 'Geoffrey Holmes']",http://arxiv.org/pdf/2205.05831v4.pdf,2022-05-12,"['cs.cv', 'cs.lg']","  Cross-domain few-shot learning (CDFSL) addresses learning problems whereknowledge needs to be transferred from one or more source domains into aninstance-scarce target domain with an explicitly different distribution.Recently published CDFSL methods generally construct a universal model thatcombines knowledge of multiple source domains into one feature extractor. Thisenables efficient inference but necessitates re-computation of the extractorwhenever a new source domain is added. Some of these methods are alsoincompatible with heterogeneous source domain extractor architectures. Wepropose feature extractor stacking (FES), a new CDFSL method for combininginformation from a collection of extractors, that can utilise heterogeneouspretrained extractors out of the box and does not maintain a universal modelthat needs to be re-computed when its extractor collection is updated. Wepresent the basic FES algorithm, which is inspired by the classic stackedgeneralisation approach, and also introduce two variants: convolutional FES(ConFES) and regularised FES (ReFES). Given a target-domain task, thesealgorithms fine-tune each extractor independently, use cross-validation toextract training data for stacked generalisation from the support set, andlearn a simple linear stacking classifier from this data. We evaluate our FESmethods on the well-known Meta-Dataset benchmark, targeting imageclassification with convolutional neural networks, and show that they canachieve state-of-the-art performance."
Medical Coding with Biomedical Transformer Ensembles and Zero/Few-shot  Learning,"['Angelo Ziletti', 'Alan Akbik', 'Christoph Berns', 'Thomas Herold', 'Marion Legler', 'Martina Viell']",http://arxiv.org/pdf/2206.02662v1.pdf,2022-05-01,"['cs.ir', 'cs.cl', 'cs.lg']","  Medical coding (MC) is an essential pre-requisite for reliable data retrievaland reporting. Given a free-text reported term (RT) such as ""pain of rightthigh to the knee"", the task is to identify the matching lowest-level term(LLT) - in this case ""unilateral leg pain"" - from a very large and continuouslygrowing repository of standardized medical terms. However, automating this taskis challenging due to a large number of LLT codes (as of writing over 80,000),limited availability of training data for long tail/emerging classes, and thegeneral high accuracy demands of the medical domain. With this paper, weintroduce the MC task, discuss its challenges, and present a novel approachcalled xTARS that combines traditional BERT-based classification with a recentzero/few-shot learning approach (TARS). We present extensive experiments thatshow that our combined approach outperforms strong baselines, especially in thefew-shot regime. The approach is developed and deployed at Bayer, live sinceNovember 2021. As we believe our approach potentially promising beyond MC, andto ensure reproducibility, we release the code to the research community."
Few-Shot Stance Detection via Target-Aware Prompt Distillation,"['Yan Jiang', 'Jinhua Gao', 'Huawei Shen', 'Xueqi Cheng']",http://arxiv.org/pdf/2206.13214v1.pdf,2022-06-27,['cs.cl'],"  Stance detection aims to identify whether the author of a text is in favorof, against, or neutral to a given target. The main challenge of this taskcomes two-fold: few-shot learning resulting from the varying targets and thelack of contextual information of the targets. Existing works mainly focus onsolving the second issue by designing attention-based models or introducingnoisy external knowledge, while the first issue remains under-explored. In thispaper, inspired by the potential capability of pre-trained language models(PLMs) serving as knowledge bases and few-shot learners, we propose tointroduce prompt-based fine-tuning for stance detection. PLMs can provideessential contextual information for the targets and enable few-shot learningvia prompts. Considering the crucial role of the target in stance detectiontask, we design target-aware prompts and propose a novel verbalizer. Instead ofmapping each label to a concrete word, our verbalizer maps each label to avector and picks the label that best captures the correlation between thestance and the target. Moreover, to alleviate the possible defect of dealingwith varying targets with a single hand-crafted prompt, we propose to distillthe information learned from multiple prompts. Experimental results show thesuperior performance of our proposed model in both full-data and few-shotscenarios."
CAM/CAD Point Cloud Part Segmentation via Few-Shot Learning,"['Jiahui Wang', 'Haiyue Zhu', 'Haoren Guo', 'Abdullah Al Mamun', 'Vadakkepat Prahlad', 'Tong Heng Lee']",http://arxiv.org/pdf/2207.01218v2.pdf,2022-07-04,"['eess.iv', 'cs.ai', 'cs.cv']","  3D part segmentation is an essential step in advanced CAM/CAD workflow.Precise 3D segmentation contributes to lower defective rate of work-piecesproduced by the manufacturing equipment (such as computer controlled CNCs),thereby improving work efficiency and attaining the attendant economicbenefits. A large class of existing works on 3D model segmentation are mostlybased on fully-supervised learning, which trains the AI models with large,annotated datasets. However, the disadvantage is that the resulting models fromthe fully-supervised learning methodology are highly reliant on thecompleteness of the available dataset, and its generalization ability isrelatively poor to new unknown segmentation types (i.e. further additionalnovel classes). In this work, we propose and develop a noteworthy few-shotlearning-based approach for effective part segmentation in CAM/CAD; and this isdesigned to significantly enhance its generalization ability and flexibly adaptto new segmentation tasks by using only relatively rather few samples. As aresult, it not only reduces the requirements for the usually unattainable andexhaustive completeness of supervision datasets, but also improves theflexibility for real-world applications. As further improvement and innovation,we additionally adopt the transform net and the center loss block in thenetwork. These characteristics serve to improve the comprehension for 3Dfeatures of the various possible instances of the whole work-piece and ensurethe close distribution of the same class in feature space."
Pseudo-Labeling Based Practical Semi-Supervised Meta-Training for  Few-Shot Learning,"['Xingping Dong', 'Shengcai Liao', 'Bo Du', 'Ling Shao']",http://arxiv.org/pdf/2207.06817v2.pdf,2022-07-14,['cs.cv'],"  Most existing few-shot learning (FSL) methods require a large amount oflabeled data in meta-training, which is a major limit. To reduce therequirement of labels, a semi-supervised meta-training (SSMT) setting has beenproposed for FSL, which includes only a few labeled samples and numbers ofunlabeled samples in base classes. However, existing methods under this settingrequire class-aware sample selection from the unlabeled set, which violates theassumption of unlabeled set. In this paper, we propose a practicalsemi-supervised meta-training setting with truly unlabeled data to facilitatethe applications of FSL in realistic scenarios. To better utilize both thelabeled and truly unlabeled data, we propose a simple and effectivemeta-training framework, called pseudo-labeling based meta-learning (PLML).Firstly, we train a classifier via common semi-supervised learning (SSL) anduse it to obtain the pseudo-labels of unlabeled data. Then we build few-shottasks from labeled and pseudo-labeled data and design a novel finetuning methodwith feature smoothing and noise suppression to better learn the FSL model fromnoise labels. Surprisingly, through extensive experiments across two FSLdatasets, we find that this simple meta-training framework effectively preventsthe performance degradation of various FSL models under limited labeled data,and also significantly outperforms the state-of-the-art SSMT models. Besides,benefiting from meta-training, our method also improves two representative SSLalgorithms as well."
Bitwidth-Adaptive Quantization-Aware Neural Network Training: A  Meta-Learning Approach,"['Jiseok Youn', 'Jaehun Song', 'Hyung-Sin Kim', 'Saewoong Bahk']",http://arxiv.org/pdf/2207.10188v1.pdf,2022-07-20,"['cs.lg', 'cs.cv']","  Deep neural network quantization with adaptive bitwidths has gainedincreasing attention due to the ease of model deployment on various platformswith different resource budgets. In this paper, we propose a meta-learningapproach to achieve this goal. Specifically, we propose MEBQAT, a simple yeteffective way of bitwidth-adaptive quantization aware training (QAT) wheremeta-learning is effectively combined with QAT by redefining meta-learningtasks to incorporate bitwidths. After being deployed on a platform, MEBQATallows the (meta-)trained model to be quantized to any candidate bitwidth thenhelps to conduct inference without much accuracy drop from quantization.Moreover, with a few-shot learning scenario, MEBQAT can also adapt a model toany bitwidth as well as any unseen target classes by adding conventionaloptimization or metric-based meta-learning. We design variants of MEBQAT tosupport both (1) a bitwidth-adaptive quantization scenario and (2) a newfew-shot learning scenario where both quantization bitwidths and target classesare jointly adapted. We experimentally demonstrate their validity in multipleQAT schemes. By comparing their performance to (bitwidth-dedicated) QAT,existing bitwidth adaptive QAT and vanilla meta-learning, we find that mergingbitwidths into meta-learning tasks achieves a higher level of robustness."
Rethinking Few-Shot Object Detection on a Multi-Domain Benchmark,"['Kibok Lee', 'Hao Yang', 'Satyaki Chakraborty', 'Zhaowei Cai', 'Gurumurthy Swaminathan', 'Avinash Ravichandran', 'Onkar Dabeer']",http://arxiv.org/pdf/2207.11169v1.pdf,2022-07-22,['cs.cv'],"  Most existing works on few-shot object detection (FSOD) focus on a settingwhere both pre-training and few-shot learning datasets are from a similardomain. However, few-shot algorithms are important in multiple domains; henceevaluation needs to reflect the broad applications. We propose a Multi-dOmainFew-Shot Object Detection (MoFSOD) benchmark consisting of 10 datasets from awide range of domains to evaluate FSOD algorithms. We comprehensively analyzethe impacts of freezing layers, different architectures, and differentpre-training datasets on FSOD performance. Our empirical results show severalkey factors that have not been explored in previous works: 1) contrary toprevious belief, on a multi-domain benchmark, fine-tuning (FT) is a strongbaseline for FSOD, performing on par or better than the state-of-the-art (SOTA)algorithms; 2) utilizing FT as the baseline allows us to explore multiplearchitectures, and we found them to have a significant impact on down-streamfew-shot tasks, even with similar pre-training performances; 3) by decouplingpre-training and few-shot learning, MoFSOD allows us to explore the impact ofdifferent pre-training datasets, and the right choice can boost the performanceof the down-stream tasks significantly. Based on these findings, we listpossible avenues of investigation for improving FSOD performance and proposetwo simple modifications to existing algorithms that lead to SOTA performanceon the MoFSOD benchmark. The code is available athttps://github.com/amazon-research/few-shot-object-detection-benchmark."
Learning Primitive-aware Discriminative Representations for Few-shot  Learning,"['Jianpeng Yang', 'Yuhang Niu', 'Xuemei Xie', 'Guangming Shi']",http://arxiv.org/pdf/2208.09717v2.pdf,2022-08-20,['cs.cv'],"  Few-shot learning (FSL) aims to learn a classifier that can be easily adaptedto recognize novel classes with only a few labeled examples. Some recent workabout FSL has yielded promising classification performance, where theimage-level feature is used to calculate the similarity among samples forclassification. However, the image-level feature ignores abundant fine-grainedand structural in-formation of objects that may be transferable and consistentbetween seen and unseen classes. How can humans easily identify novel classeswith several sam-ples? Some study from cognitive science argues that humans canrecognize novel categories through primitives. Although base and novelcategories are non-overlapping, they can share some primitives in common.Inspired by above re-search, we propose a Primitive Mining and ReasoningNetwork (PMRN) to learn primitive-aware representations based on metric-basedFSL model. Concretely, we first add Self-supervision Jigsaw task (SSJ) forfeature extractor parallelly, guiding the model to encode visual patterncorresponding to object parts into fea-ture channels. To further minediscriminative representations, an Adaptive Chan-nel Grouping (ACG) method isapplied to cluster and weight spatially and se-mantically related visualpatterns to generate a group of visual primitives. To fur-ther enhance thediscriminability and transferability of primitives, we propose a visualprimitive Correlation Reasoning Network (CRN) based on graph convolu-tionalnetwork to learn abundant structural information and internal correlation amongprimitives. Finally, a primitive-level metric is conducted for classificationin a meta-task based on episodic training strategy. Extensive experiments showthat our method achieves state-of-the-art results on six standard benchmarks."
Few-Shot Learning Meets Transformer: Unified Query-Support Transformers  for Few-Shot Classification,"['Xixi Wang', 'Xiao Wang', 'Bo Jiang', 'Bin Luo']",http://arxiv.org/pdf/2208.12398v1.pdf,2022-08-26,['cs.cv'],"  Few-shot classification which aims to recognize unseen classes using verylimited samples has attracted more and more attention. Usually, it isformulated as a metric learning problem. The core issue of few-shotclassification is how to learn (1) consistent representations for images inboth support and query sets and (2) effective metric learning for imagesbetween support and query sets. In this paper, we show that the two challengescan be well modeled simultaneously via a unified Query-Support TransFormer(QSFormer) model. To be specific,the proposed QSFormer involves globalquery-support sample Transformer (sampleFormer) branch and local patchTransformer (patchFormer) learning branch. sampleFormer aims to capture thedependence of samples in support and query sets for image representation. Itadopts the Encoder, Decoder and Cross-Attention to respectively model theSupport, Query (image) representation and Metric learning for few-shotclassification task. Also, as a complementary to global learning branch, weadopt a local patch Transformer to extract structural representation for eachimage sample by capturing the long-range dependence of local image patches. Inaddition, a novel Cross-scale Interactive Feature Extractor (CIFE) is proposedto extract and fuse multi-scale CNN features as an effective backbone modulefor the proposed few-shot learning method. All modules are integrated into aunified framework and trained in an end-to-end manner. Extensive experiments onfour popular datasets demonstrate the effectiveness and superiority of theproposed QSFormer."
Predicting Flaky Tests Categories using Few-Shot Learning,"['Amal Akli', 'Guillaume Haben', 'Sarra Habchi', 'Mike Papadakis', 'Yves Le Traon']",http://arxiv.org/pdf/2208.14799v1.pdf,2022-08-31,['cs.se'],"  Flaky tests are tests that yield different outcomes when run on the sameversion of a program. This non-deterministic behaviour plagues continuousintegration with false signals, wasting developers' time and reducing theirtrust in test suites. Studies highlighted the importance of keeping testsflakiness-free. Recently, the research community has been pushing forward thedetection of flaky tests by suggesting many static and dynamic approaches.While promising, those approaches mainly focus on classifying tests as flaky ornot and, even when high performances are reported, it remains challenging tounderstand the cause of flakiness. This part is crucial for researchers anddevelopers that aim to fix it. To help with the comprehension of a given flakytest, we propose FlakyCat, the first approach for classifying flaky tests basedon their root cause category. FlakyCat relies on CodeBERT for coderepresentation and leverages a Siamese network-based Few-Shot learning methodto train a multi-class classifier with few data. We train and evaluate FlakyCaton a set of 343 flaky tests collected from open-source Java projects. Ourevaluation shows that FlakyCat categorises flaky tests accurately, with aweighted F1 score of 70%. Furthermore, we investigate the performance of ourapproach for each category, revealing that Async waits, Unordered collectionsand Time-related flaky tests are accurately classified, whileConcurrency-related flaky tests are more challenging to predict. Finally, tofacilitate the comprehension of FlakyCat's predictions, we present a newtechnique for CodeBERT-based model interpretability that highlights codestatements influencing the categorization."
Few-Shot Learning for Clinical Natural Language Processing Using Siamese  Neural Networks,"['David Oniani', 'Sonish Sivarajkumar', 'Yanshan Wang']",http://arxiv.org/pdf/2208.14923v2.pdf,2022-08-31,['cs.cl'],"  Clinical Natural Language Processing (NLP) has become an emerging technologyin healthcare that leverages a large amount of free-text data in electronichealth records (EHRs) to improve patient care, support clinical decisions, andfacilitate clinical and translational science research. Recently, deep learninghas achieved state-of-the-art performance in many clinical NLP tasks. However,training deep learning models usually requires large annotated datasets, whichare normally not publicly available and can be time-consuming to build inclinical domains. Working with smaller annotated datasets is typical inclinical NLP and therefore, ensuring that deep learning models perform well iscrucial for the models to be used in real-world applications. A widely adoptedapproach is fine-tuning existing Pre-trained Language Models (PLMs), but theseattempts fall short when the training dataset contains only a few annotatedsamples. Few-Shot Learning (FSL) has recently been investigated to tackle thisproblem. Siamese Neural Network (SNN) has been widely utilized as an FSLapproach in computer vision, but has not been studied well in NLP. Furthermore,the literature on its applications in clinical domains is scarce. In thispaper, we propose two SNN-based FSL approaches for clinical NLP, includingPre-Trained SNN (PT-SNN) and SNN with Second-Order Embeddings (SOE-SNN). Weevaluated the proposed approaches on two clinical tasks, namely clinical textclassification and clinical named entity recognition. We tested three few-shotsettings including 4-shot, 8-shot, and 16-shot learning. Both clinical NLPtasks were benchmarked using three PLMs, including BERT,BioBERT, andBioClinicalBERT. The experimental results verified the effectiveness of theproposed SNN-based FSL approaches in both NLP tasks."
Class-Specific Channel Attention for Few-Shot Learning,"['Ying-Yu Chen', 'Jun-Wei Hsieh', 'Ming-Ching Chang']",http://arxiv.org/pdf/2209.01332v2.pdf,2022-09-03,['cs.cv'],"  Few-Shot Learning (FSL) has attracted growing attention in computer visiondue to its capability in model training without the need for excessive data.FSL is challenging because the training and testing categories (the base vs.novel sets) can be largely diversified. Conventional transfer-based solutionsthat aim to transfer knowledge learned from large labeled training sets totarget testing sets are limited, as critical adverse impacts of the shift intask distribution are not adequately addressed. In this paper, we extend thesolution of transfer-based methods by incorporating the concept ofmetric-learning and channel attention. To better exploit the featurerepresentations extracted by the feature backbone, we propose Class-SpecificChannel Attention (CSCA) module, which learns to highlight the discriminativechannels in each class by assigning each class one CSCA weight vector. Unlikegeneral attention modules designed to learn global-class features, the CSCAmodule aims to learn local and class-specific features with very effectivecomputation. We evaluated the performance of the CSCA module on standardbenchmarks including miniImagenet, Tiered-ImageNet, CIFAR-FS, and CUB-200-2011.Experiments are performed in inductive and in/cross-domain settings. We achievenew state-of-the-art results."
TGDM: Target Guided Dynamic Mixup for Cross-Domain Few-Shot Learning,"['Linhai Zhuo', 'Yuqian Fu', 'Jingjing Chen', 'Yixin Cao', 'Yu-Gang Jiang']",http://arxiv.org/pdf/2210.05392v2.pdf,2022-10-11,['cs.cv'],"  Given sufficient training data on the source domain, cross-domain few-shotlearning (CD-FSL) aims at recognizing new classes with a small number oflabeled examples on the target domain. The key to addressing CD-FSL is tonarrow the domain gap and transferring knowledge of a network trained on thesource domain to the target domain. To help knowledge transfer, this paperintroduces an intermediate domain generated by mixing images in the source andthe target domain. Specifically, to generate the optimal intermediate domainfor different target data, we propose a novel target guided dynamic mixup(TGDM) framework that leverages the target data to guide the generation ofmixed images via dynamic mixup. The proposed TGDM framework contains a Mixup-3Tnetwork for learning classifiers and a dynamic ratio generation network (DRGN)for learning the optimal mix ratio. To better transfer the knowledge, theproposed Mixup-3T network contains three branches with shared parameters forclassifying classes in the source domain, target domain, and intermediatedomain. To generate the optimal intermediate domain, the DRGN learns togenerate an optimal mix ratio according to the performance on auxiliary targetdata. Then, the whole TGDM framework is trained via bi-level meta-learning sothat TGDM can rectify itself to achieve optimal performance on target data.Extensive experimental results on several benchmark datasets verify theeffectiveness of our method."
Knowledge-Driven New Drug Recommendation,"['Zhenbang Wu', 'Huaxiu Yao', 'Zhe Su', 'David M Liebovitz', 'Lucas M Glass', 'James Zou', 'Chelsea Finn', 'Jimeng Sun']",http://arxiv.org/pdf/2210.05572v1.pdf,2022-10-11,"['cs.lg', 'cs.ai', 'cs.ir']","  Drug recommendation assists doctors in prescribing personalized medicationsto patients based on their health conditions. Existing drug recommendationsolutions adopt the supervised multi-label classification setup and only workwith existing drugs with sufficient prescription data from many patients.However, newly approved drugs do not have much historical prescription data andcannot leverage existing drug recommendation methods. To address this, weformulate the new drug recommendation as a few-shot learning problem. Yet,directly applying existing few-shot learning algorithms faces two challenges:(1) complex relations among diseases and drugs and (2) numerous false-negativepatients who were eligible but did not yet use the new drugs. To tackle thesechallenges, we propose EDGE, which can quickly adapt to the recommendation fora new drug with limited prescription data from a few support patients. EDGEmaintains a drug-dependent multi-phenotype few-shot learner to bridge the gapbetween existing and new drugs. Specifically, EDGE leverages the drug ontologyto link new drugs to existing drugs with similar treatment effects and learnsontology-based drug representations. Such drug representations are used tocustomize the metric space of the phenotype-driven patient representations,which are composed of a set of phenotypes capturing complex patient healthstatus. Lastly, EDGE eliminates the false-negative supervision signal using anexternal drug-disease knowledge base. We evaluate EDGE on two real-worlddatasets: the public EHR data (MIMIC-IV) and private industrial claims data.Results show that EDGE achieves 7.3% improvement on the ROC-AUC score over thebest baseline."
Semantic Cross Attention for Few-shot Learning,"['Bin Xiao', 'Chien-Liang Liu', 'Wen-Hoar Hsaio']",http://arxiv.org/pdf/2210.06311v1.pdf,2022-10-12,"['cs.cv', 'cs.lg']","  Few-shot learning (FSL) has attracted considerable attention recently. Amongexisting approaches, the metric-based method aims to train an embedding networkthat can make similar samples close while dissimilar samples as far as possibleand achieves promising results. FSL is characterized by using only a few imagesto train a model that can generalize to novel classes in image classificationproblems, but this setting makes it difficult to learn the visual features thatcan identify the images' appearance variations. The model training is likely tomove in the wrong direction, as the images in an identical semantic class mayhave dissimilar appearances, whereas the images in different semantic classesmay share a similar appearance. We argue that FSL can benefit from additionalsemantic features to learn discriminative feature representations. Thus, thisstudy proposes a multi-task learning approach to view semantic features oflabel text as an auxiliary task to help boost the performance of the FSL task.Our proposed model uses word-embedding representations as semantic features tohelp train the embedding network and a semantic cross-attention module tobridge the semantic features into the typical visual modal. The proposedapproach is simple, but produces excellent results. We apply our proposedapproach to two previous metric-based FSL methods, all of which cansubstantially improve performance. The source code for our model is accessiblefrom github."
Prompting through Prototype: A Prototype-based Prompt Learning on  Pretrained Vision-Language Models,"['Yue Zhang', 'Hongliang Fei', 'Dingcheng Li', 'Tan Yu', 'Ping Li']",http://arxiv.org/pdf/2210.10841v1.pdf,2022-10-19,"['cs.cl', 'cs.cv']","  Prompt learning is a new learning paradigm which reformulates downstreamtasks as similar pretraining tasks on pretrained models by leveraging textualprompts. Recent works have demonstrated that prompt learning is particularlyuseful for few-shot learning, where there is limited training data. Dependingon the granularity of prompts, those methods can be roughly divided intotask-level prompting and instance-level prompting. Task-level prompting methodslearn one universal prompt for all input samples, which is efficient butineffective to capture subtle differences among different classes.Instance-level prompting methods learn a specific prompt for each input, thougheffective but inefficient. In this work, we develop a novel prototype-basedprompt learning method to overcome the above limitations. In particular, wefocus on few-shot image recognition tasks on pretrained vision-language models(PVLMs) and develop a method of prompting through prototype (PTP), where wedefine $K$ image prototypes and $K$ prompt prototypes. In PTP, the imageprototype represents a centroid of a certain image cluster in the latent spaceand a prompt prototype is defined as a soft prompt in the continuous space. Thesimilarity between a query image and an image prototype determines how muchthis prediction relies on the corresponding prompt prototype. Hence, in PTP,similar images will utilize similar prompting ways. Through extensiveexperiments on seven real-world benchmarks, we show that PTP is an effectivemethod to leverage the latent knowledge and adaptive to various PVLMs.Moreover, through detailed analysis, we discuss pros and cons for promptlearning and parameter-efficient fine-tuning under the context of few-shotlearning."
Fusion-based Few-Shot Morphing Attack Detection and Fingerprinting,"['Na Zhang', 'Shan Jia', 'Siwei Lyu', 'Xin Li']",http://arxiv.org/pdf/2210.15510v1.pdf,2022-10-27,['cs.cv'],"  The vulnerability of face recognition systems to morphing attacks has posed aserious security threat due to the wide adoption of face biometrics in the realworld. Most existing morphing attack detection (MAD) methods require a largeamount of training data and have only been tested on a few predefined attackmodels. The lack of good generalization properties, especially in view of thegrowing interest in developing novel morphing attacks, is a critical limitationwith existing MAD research. To address this issue, we propose to extend MADfrom supervised learning to few-shot learning and from binary detection tomulticlass fingerprinting in this paper. Our technical contributions include:1) We propose a fusion-based few-shot learning (FSL) method to learndiscriminative features that can generalize to unseen morphing attack typesfrom predefined presentation attacks; 2) The proposed FSL based on the fusionof the PRNU model and Noiseprint network is extended from binary MAD tomulticlass morphing attack fingerprinting (MAF). 3) We have collected alarge-scale database, which contains five face datasets and eight differentmorphing algorithms, to benchmark the proposed few-shot MAF (FS-MAF) method.Extensive experimental results show the outstanding performance of ourfusion-based FS-MAF. The code and data will be publicly available athttps://github.com/nz0001na/mad maf."
Few-Shot Classification of Skin Lesions from Dermoscopic Images by  Meta-Learning Representative Embeddings,"['Karthik Desingu', 'Mirunalini P.', 'Aravindan Chandrabose']",http://arxiv.org/pdf/2210.16954v1.pdf,2022-10-30,"['cs.cv', 'cs.ai', 'cs.lg']","  Annotated images and ground truth for the diagnosis of rare and noveldiseases are scarce. This is expected to prevail, considering the small numberof affected patient population and limited clinical expertise to annotateimages. Further, the frequently occurring long-tailed class distributions inskin lesion and other disease classification datasets cause conventionaltraining approaches to lead to poor generalization due to biased class priors.Few-shot learning, and meta-learning in general, aim to overcome these issuesby aiming to perform well in low data regimes. This paper focuses on improvingmeta-learning for the classification of dermoscopic images. Specifically, wepropose a baseline supervised method on the meta-training set that allows anetwork to learn highly representative and generalizable feature embeddings forimages, that are readily transferable to new few-shot learning tasks. We followsome of the previous work in literature that posit that a representativefeature embedding can be more effective than complex meta-learning algorithms.We empirically prove the efficacy of the proposed meta-training method ondermoscopic images for learning embeddings, and show that even simple linearclassifiers trained atop these representations suffice to outperform some ofthe usual meta-learning methods."
Rethinking the Metric in Few-shot Learning: From an Adaptive  Multi-Distance Perspective,"['Jinxiang Lai', 'Siqian Yang', 'Guannan Jiang', 'Xi Wang', 'Yuxi Li', 'Zihui Jia', 'Xiaochen Chen', 'Jun Liu', 'Bin-Bin Gao', 'Wei Zhang', 'Yuan Xie', 'Chengjie Wang']",http://arxiv.org/pdf/2211.00890v1.pdf,2022-11-02,['cs.cv'],"  Few-shot learning problem focuses on recognizing unseen classes given a fewlabeled images. In recent effort, more attention is paid to fine-grainedfeature embedding, ignoring the relationship among different distance metrics.In this paper, for the first time, we investigate the contributions ofdifferent distance metrics, and propose an adaptive fusion scheme, bringingsignificant improvements in few-shot classification. We start from a naivebaseline of confidence summation and demonstrate the necessity of exploitingthe complementary property of different distance metrics. By finding thecompetition problem among them, built upon the baseline, we propose an AdaptiveMetrics Module (AMM) to decouple metrics fusion into metric-prediction fusionand metric-losses fusion. The former encourages mutual complementary, while thelatter alleviates metric competition via multi-task collaborative learning.Based on AMM, we design a few-shot classification framework AMTNet, includingthe AMM and the Global Adaptive Loss (GAL), to jointly optimize the few-shottask and auxiliary self-supervised task, making the embedding features morerobust. In the experiment, the proposed AMM achieves 2% higher performance thanthe naive metrics fusion module, and our AMTNet outperforms thestate-of-the-arts on multiple benchmark datasets."
Neuromorphic Few-Shot Learning: Generalization in Multilayer Physical  Neural Networks,"['Kilian D. Stenning', 'Jack C. Gartside', 'Luca Manneschi', 'Christopher T. S. Cheung', 'Tony Chen', 'Alex Vanstone', 'Jake Love', 'Holly H. Holder', 'Francesco Caravelli', 'Karin Everschor-Sitte', 'Eleni Vasilaki', 'Will R. Branford']",http://arxiv.org/pdf/2211.06373v3.pdf,2022-11-11,['cond-mat.mes-hall'],"  Neuromorphic computing leverages the complex dynamics of physical systems forcomputation. The field has recently undergone an explosion in the range andsophistication of implementations, with rapidly improving performance.Neuromorphic schemes typically employ a single physical system, limiting thedimensionality and range of available dynamics - restricting strong performanceto a few specific tasks. This is a critical roadblock facing the field,inhibiting the power and versatility of neuromorphic schemes.  Here, we present a solution. We engineer a diverse suite of nanomagneticarrays and show how tuning microstate space and geometry enables a broad rangeof dynamics and computing performance. We interconnect arrays in parallel,series and multilayered neural network architectures, where each network nodeis a distinct physical system. This networked approach grants extremely highdimensionality and enriched dynamics enabling meta-learning to be implementedon small training sets and exhibiting strong performance across a broadtaskset. We showcase network performance via few-shot learning, rapidlyadapting on-the-fly to previously unseen tasks."
Technical Report on Neural Language Models and Few-Shot Learning for  Systematic Requirements Processing in MDSE,"['Vincent Bertram', 'Miriam Boß', 'Evgeny Kusmenko', 'Imke Helene Nachmann', 'Bernhard Rumpe', 'Danilo Trotta', 'Louis Wachtmeister']",http://arxiv.org/pdf/2211.09084v1.pdf,2022-11-16,"['cs.se', 'cs.ai', 'd.2.1; i.2.7']","  Systems engineering, in particular in the automotive domain, needs to copewith the massively increasing numbers of requirements that arise during thedevelopment process. To guarantee a high product quality and make sure thatfunctional safety standards such as ISO26262 are fulfilled, the exploitation ofpotentials of model-driven systems engineering in the form of automaticanalyses, consistency checks, and tracing mechanisms is indispensable. However,the language in which requirements are written, and the tools needed to operateon them, are highly individual and require domain-specific tailoring. Thishinders automated processing of requirements as well as the linking ofrequirements to models. Introducing formal requirement notations in existingprojects leads to the challenge of translating masses of requirements andprocess changes on the one hand and to the necessity of the correspondingtraining for the requirements engineers.  In this paper, based on the analysis of an open-source set of automotiverequirements, we derive domain-specific language constructs helping us to avoidambiguities in requirements and increase the level of formality. The maincontribution is the adoption and evaluation of few-shot learning with largepretrained language models for the automated translation of informalrequirements to structured languages such as a requirement DSL. We show thatsupport sets of less than ten translation examples can suffice to few-shottrain a language model to incorporate keywords and implement syntactic rulesinto informal natural language requirements."
DASECount: Domain-Agnostic Sample-Efficient Wireless Indoor Crowd  Counting via Few-shot Learning,"['Huawei Hou', 'Suzhi Bi', 'Lili Zheng', 'Xiaohui Lin', 'Yuan Wu', 'Zhi Quan']",http://arxiv.org/pdf/2211.10040v1.pdf,2022-11-18,['eess.sp'],"  Accurate indoor crowd counting (ICC) is a key enabler to many smarthome/office applications. In this paper, we propose a Domain-Agnostic andSample-Efficient wireless indoor crowd Counting (DASECount) framework thatsuffices to attain robust cross-domain detection accuracy given very limiteddata samples in new domains. DASECount leverages the wisdom of few-shotlearning (FSL) paradigm consisting of two major stages: source domain metatraining and target domain meta testing. Specifically, in the meta-trainingstage, we design and train two separate convolutional neural network (CNN)modules on the source domain dataset to fully capture the implicit amplitudeand phase features of CSI measurements related to human activities. Asubsequent knowledge distillation procedure is designed to iteratively updatethe CNN parameters for better generalization performance. In the meta-testingstage, we use the partial CNN modules to extract low-dimension features out ofthe high-dimension input target domain CSI data. With the obtainedlow-dimension CSI features, we can even use very few shots of target domaindata samples (e.g., 5-shot samples) to train a lightweight logistic regression(LR) classifier, and attain very high cross-domain ICC accuracy. Experimentresults show that the proposed DASECount method achieves over 92.68\%, and onaverage 96.37\% detection accuracy in a 0-8 people counting task under variousdomain setups, which significantly outperforms the other representativebenchmark methods considered."
Few-shot Learning for Multi-modal Social Media Event Filtering,"['José Nascimento', 'João Phillipe Cardenuto', 'Jing Yang', 'Anderson Rocha']",http://arxiv.org/pdf/2211.10340v1.pdf,2022-11-16,"['cs.lg', 'cs.si']","  Social media has become an important data source for event analysis. Whencollecting this type of data, most contain no useful information to a targetevent. Thus, it is essential to filter out those noisy data at the earliestopportunity for a human expert to perform further inspection. Most existingsolutions for event filtering rely on fully supervised methods for training.However, in many real-world scenarios, having access to large number of labeledsamples is not possible. To deal with a few labeled sample training problem forevent filtering, we propose a graph-based few-shot learning pipeline. We alsorelease the Brazilian Protest Dataset to test our method. To the best of ourknowledge, this dataset is the first of its kind in event filtering thatfocuses on protests in multi-modal social media data, with most of the text inPortuguese. Our experimental results show that our proposed pipeline hascomparable performance with only a few labeled samples (60) compared with afully labeled dataset (3100). To facilitate the research community, we make ourdataset and code available at https://github.com/jdnascim/7Set-AL."
A Maximum Log-Likelihood Method for Imbalanced Few-Shot Learning Tasks,"['Samuel Hess', 'Gregory Ditzler']",http://arxiv.org/pdf/2211.14668v2.pdf,2022-11-26,"['cs.cv', 'cs.lg']","  Few-shot learning is a rapidly evolving area of research in machine learningwhere the goal is to classify unlabeled data with only one or ""a few"" labeledexemplary samples. Neural networks are typically trained to minimize a distancemetric between labeled exemplary samples and a query set. Early few-shotapproaches use an episodic training process to sub-sample the training datainto few-shot batches. This training process matches the sub-sampling done onevaluation. Recently, conventional supervised training coupled with a cosinedistance has achieved superior performance for few-shot. Despite the diversityof few-shot approaches over the past decade, most methods still rely on thecosine or Euclidean distance layer between the latent features of the trainednetwork. In this work, we investigate the distributions of trained few-shotfeatures and demonstrate that they can be roughly approximated as exponentialdistributions. Under this assumption of an exponential distribution, we proposea new maximum log-likelihood metric for few-shot architectures. We demonstratethat the proposed metric achieves superior performance accuracy w.r.t.conventional similarity metrics (e.g., cosine, Euclidean, etc.), and achievestate-of-the-art inductive few-shot performance. Further, additional gains canbe achieved by carefully combining multiple metrics and neither of our methodsrequire post-processing feature transformations, which are common to manyalgorithms. Finally, we demonstrate a novel iterative algorithm designed aroundour maximum log-likelihood approach that achieves state-of-the-art transductivefew-shot performance when the evaluation data is imbalanced. We have made ourcode publicly available at https://github.com/samuelhess/MLL_FSL/."
SgVA-CLIP: Semantic-guided Visual Adapting of Vision-Language Models for  Few-shot Image Classification,"['Fang Peng', 'Xiaoshan Yang', 'Linhui Xiao', 'Yaowei Wang', 'Changsheng Xu']",http://arxiv.org/pdf/2211.16191v2.pdf,2022-11-28,"['cs.cv', 'cs.mm']","  Although significant progress has been made in few-shot learning, most ofexisting few-shot image classification methods require supervised pre-trainingon a large amount of samples of base classes, which limits their generalizationability in real world application. Recently, large-scale Vision-LanguagePre-trained models (VLPs) have been gaining increasing attention in few-shotlearning because they can provide a new paradigm for transferable visualrepresentation learning with easily available text on the Web. However, theVLPs may neglect detailed visual information that is difficult to describe bylanguage sentences, but important for learning an effective classifier todistinguish different images. To address the above problem, we propose a newframework, named Semantic-guided Visual Adapting (SgVA), which can effectivelyextend vision-language pre-trained models to produce discriminative adaptedvisual features by comprehensively using an implicit knowledge distillation, avision-specific contrastive loss, and a cross-modal contrastive loss. Theimplicit knowledge distillation is designed to transfer the fine-grainedcross-modal knowledge to guide the updating of the vision adapter.State-of-the-art results on 13 datasets demonstrate that the adapted visualfeatures can well complement the cross-modal features to improve few-shot imageclassification."
Finetune like you pretrain: Improved finetuning of zero-shot vision  models,"['Sachin Goyal', 'Ananya Kumar', 'Sankalp Garg', 'Zico Kolter', 'Aditi Raghunathan']",http://arxiv.org/pdf/2212.00638v1.pdf,2022-12-01,"['cs.cv', 'cs.lg']","  Finetuning image-text models such as CLIP achieves state-of-the-artaccuracies on a variety of benchmarks. However, recent works like WiseFT(Wortsman et al., 2021) and LP-FT (Kumar et al., 2022) have shown that evensubtle differences in the finetuning process can lead to surprisingly largedifferences in the final performance, both for in-distribution (ID) andout-of-distribution (OOD) data. In this work, we show that a natural and simpleapproach of mimicking contrastive pretraining consistently outperformsalternative finetuning approaches. Specifically, we cast downstream classlabels as text prompts and continue optimizing the contrastive loss betweenimage embeddings and class-descriptive prompt embeddings (contrastivefinetuning).  Our method consistently outperforms baselines across 7 distribution shifts, 6transfer learning, and 3 few-shot learning benchmarks. On WILDS-iWILDCam, ourproposed approach FLYP outperforms the top of the leaderboard by $2.3\%$ ID and$2.7\%$ OOD, giving the highest reported accuracy. Averaged across 7 OODdatasets (2 WILDS and 5 ImageNet associated shifts), FLYP gives gains of$4.2\%$ OOD over standard finetuning and outperforms the current state of theart (LP-FT) by more than $1\%$ both ID and OOD. Similarly, on 3 few-shotlearning benchmarks, our approach gives gains up to $4.6\%$ over standardfinetuning and $4.4\%$ over the state of the art. In total, these benchmarksestablish contrastive finetuning as a simple, intuitive, and state-of-the-artapproach for supervised finetuning of image-text models like CLIP. Code isavailable at https://github.com/locuslab/FLYP."
L-HYDRA: Multi-Head Physics-Informed Neural Networks,"['Zongren Zou', 'George Em Karniadakis']",http://arxiv.org/pdf/2301.02152v1.pdf,2023-01-05,"['cs.lg', 'physics.comp-ph', '34f05, 62m45, 65l99, 65m99, 65n99']","  We introduce multi-head neural networks (MH-NNs) to physics-informed machinelearning, which is a type of neural networks (NNs) with all nonlinear hiddenlayers as the body and multiple linear output layers as multi-head. Hence, weconstruct multi-head physics-informed neural networks (MH-PINNs) as a potenttool for multi-task learning (MTL), generative modeling, and few-shot learningfor diverse problems in scientific machine learning (SciML). MH-PINNs connectmultiple functions/tasks via a shared body as the basis functions as well as ashared distribution for the head. The former is accomplished by solvingmultiple tasks with MH-PINNs with each head independently corresponding to eachtask, while the latter by employing normalizing flows (NFs) for densityestimate and generative modeling. To this end, our method is a two-stagemethod, and both stages can be tackled with standard deep learning tools ofNNs, enabling easy implementation in practice. MH-PINNs can be used for variouspurposes, such as approximating stochastic processes, solving multiple taskssynergistically, providing informative prior knowledge for downstream few-shotlearning tasks such as meta-learning and transfer learning, learningrepresentative basis functions, and uncertainty quantification. We demonstratethe effectiveness of MH-PINNs in five benchmarks, investigating also thepossibility of synergistic learning in regression analysis. We name theopen-source code ""Lernaean Hydra"" (L-HYDRA), since this mythical creaturepossessed many heads for performing important multiple tasks, as in theproposed method."
Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with  Multimodal Models,"['Zhiqiu Lin', 'Samuel Yu', 'Zhiyi Kuang', 'Deepak Pathak', 'Deva Ramanan']",http://arxiv.org/pdf/2301.06267v4.pdf,2023-01-16,"['cs.cv', 'cs.ai', 'cs.lg', 'cs.sd', 'eess.as']","  The ability to quickly learn a new task with minimal instruction - known asfew-shot learning - is a central aspect of intelligent agents. Classicalfew-shot benchmarks make use of few-shot samples from a single modality, butsuch samples may not be sufficient to characterize an entire concept class. Incontrast, humans use cross-modal information to learn new concepts efficiently.In this work, we demonstrate that one can indeed build a better ${\bf visual}$dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to thembark. To do so, we exploit the fact that recent multimodal foundation modelssuch as CLIP are inherently cross-modal, mapping different modalities to thesame representation space. Specifically, we propose a simple cross-modaladaptation approach that learns from few-shot examples spanning differentmodalities. By repurposing class names as additional one-shot training samples,we achieve SOTA results with an embarrassingly simple linear classifier forvision-language adaptation. Furthermore, we show that our approach can benefitexisting methods such as prefix tuning, adapters, and classifier ensembling.Finally, to explore other modalities beyond vision and language, we constructthe first (to our knowledge) audiovisual few-shot benchmark and use cross-modaltraining to improve the performance of both image and audio classification."
Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification,"['Ihsan Ullah', 'Dustin Carrión-Ojeda', 'Sergio Escalera', 'Isabelle Guyon', 'Mike Huisman', 'Felix Mohr', 'Jan N van Rijn', 'Haozhe Sun', 'Joaquin Vanschoren', 'Phan Anh Vu']",http://arxiv.org/pdf/2302.08909v1.pdf,2023-02-16,['cs.cv'],"  We introduce Meta-Album, an image classification meta-dataset designed tofacilitate few-shot learning, transfer learning, meta-learning, among othertasks. It includes 40 open datasets, each having at least 20 classes with 40examples per class, with verified licences. They stem from diverse domains,such as ecology (fauna and flora), manufacturing (textures, vehicles), humanactions, and optical character recognition, featuring various image scales(microscopic, human scales, remote sensing). All datasets are preprocessed,annotated, and formatted uniformly, and come in 3 versions (Micro $\subset$Mini $\subset$ Extended) to match users' computational resources. We showcasethe utility of the first 30 datasets on few-shot learning problems. The other10 will be released shortly after. Meta-Album is already more diverse andlarger (in number of datasets) than similar efforts, and we are committed tokeep enlarging it via a series of competitions. As competitions terminate,their test data are released, thus creating a rolling benchmark, availablethrough OpenML.org. Our website https://meta-album.github.io/ contains thesource code of challenge winning methods, baseline methods, data loaders, andinstructions for contributing either new datasets or algorithms to ourexpandable meta-dataset."
AugGPT: Leveraging ChatGPT for Text Data Augmentation,"['Haixing Dai', 'Zhengliang Liu', 'Wenxiong Liao', 'Xiaoke Huang', 'Yihan Cao', 'Zihao Wu', 'Lin Zhao', 'Shaochen Xu', 'Wei Liu', 'Ninghao Liu', 'Sheng Li', 'Dajiang Zhu', 'Hongmin Cai', 'Lichao Sun', 'Quanzheng Li', 'Dinggang Shen', 'Tianming Liu', 'Xiang Li']",http://arxiv.org/pdf/2302.13007v3.pdf,2023-02-25,"['cs.cl', 'cs.ai', 'cs.lg']","  Text data augmentation is an effective strategy for overcoming the challengeof limited sample sizes in many natural language processing (NLP) tasks. Thischallenge is especially prominent in the few-shot learning scenario, where thedata in the target domain is generally much scarcer and of lowered quality. Anatural and widely-used strategy to mitigate such challenges is to perform dataaugmentation to better capture the data invariance and increase the samplesize. However, current text data augmentation methods either can't ensure thecorrect labeling of the generated data (lacking faithfulness) or can't ensuresufficient diversity in the generated data (lacking compactness), or both.Inspired by the recent success of large language models, especially thedevelopment of ChatGPT, which demonstrated improved language comprehensionabilities, in this work, we propose a text data augmentation approach based onChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samplesinto multiple conceptually similar but semantically different samples. Theaugmented samples can then be used in downstream model training. Experimentresults on few-shot learning text classification tasks show the superiorperformance of the proposed AugGPT approach over state-of-the-art text dataaugmentation methods in terms of testing accuracy and distribution of theaugmented samples."
Meta Learning to Bridge Vision and Language Models for Multimodal  Few-Shot Learning,"['Ivona Najdenkoska', 'Xiantong Zhen', 'Marcel Worring']",http://arxiv.org/pdf/2302.14794v1.pdf,2023-02-28,['cs.cv'],"  Multimodal few-shot learning is challenging due to the large domain gapbetween vision and language modalities. Existing methods are trying tocommunicate visual concepts as prompts to frozen language models, but rely onhand-engineered task induction to reduce the hypothesis space. To make thewhole process learnable, we introduce a multimodal meta-learning approach.Specifically, our approach decomposes the training of the model into a set ofrelated multimodal few-shot tasks. We define a meta-mapper network, acting as ameta-learner, to efficiently bridge frozen large-scale vision and languagemodels and leverage their already learned capacity. By updating the learnableparameters only of the meta-mapper, it learns to accrue shared meta-knowledgeamong these tasks. Thus, it can rapidly adapt to newly presented samples withonly a few gradient updates. Importantly, it induces the task in a completelydata-driven manner, with no need for a hand-engineered task induction. Weevaluate our approach on recently proposed multimodal few-shot benchmarks,measuring how rapidly the model can bind novel visual concepts to words andanswer visual questions by observing only a limited set of labeled examples.The experimental results show that our meta-learning approach outperforms thebaseline across multiple datasets and various training settings while beingcomputationally more efficient."
Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey,"['Huali Xu', 'Shuaifeng Zhi', 'Shuzhou Sun', 'Vishal M. Patel', 'Li Liu']",http://arxiv.org/pdf/2303.08557v1.pdf,2023-03-15,['cs.cv'],"  Deep learning has been highly successful in computer vision with largeamounts of labeled data, but struggles with limited labeled training data. Toaddress this, Few-shot learning (FSL) is proposed, but it assumes that allsamples (including source and target task data, where target tasks areperformed with prior knowledge from source ones) are from the same domain,which is a stringent assumption in the real world. To alleviate thislimitation, Cross-domain few-shot learning (CDFSL) has gained attention as itallows source and target data from different domains and label spaces. Thispaper provides a comprehensive review of CDFSL at the first time, which hasreceived far less attention than FSL due to its unique setup and difficulties.We expect this paper to serve as both a position paper and a tutorial for thosedoing research in CDFSL. This review first introduces the definition of CDFSLand the issues involved, followed by the core scientific question andchallenge. A comprehensive review of validated CDFSL approaches from theexisting literature is then presented, along with their detailed descriptionsbased on a rigorous taxonomy. Furthermore, this paper outlines and discussesseveral promising directions of CDFSL that deserve further scientificinvestigation, covering aspects of problem setups, applications and theories."
SpatialFormer: Semantic and Target Aware Attentions for Few-Shot  Learning,"['Jinxiang Lai', 'Siqian Yang', 'Wenlong Wu', 'Tao Wu', 'Guannan Jiang', 'Xi Wang', 'Jun Liu', 'Bin-Bin Gao', 'Wei Zhang', 'Yuan Xie', 'Chengjie Wang']",http://arxiv.org/pdf/2303.09281v1.pdf,2023-03-15,['cs.cv'],"  Recent Few-Shot Learning (FSL) methods put emphasis on generating adiscriminative embedding features to precisely measure the similarity betweensupport and query sets. Current CNN-based cross-attention approaches generatediscriminative representations via enhancing the mutually semantic similarregions of support and query pairs. However, it suffers from two problems: CNNstructure produces inaccurate attention map based on local features, andmutually similar backgrounds cause distraction. To alleviate these problems, wedesign a novel SpatialFormer structure to generate more accurate attentionregions based on global features. Different from the traditional Transformermodeling intrinsic instance-level similarity which causes accuracy degradationin FSL, our SpatialFormer explores the semantic-level similarity between pairinputs to boost the performance. Then we derive two specific attention modules,named SpatialFormer Semantic Attention (SFSA) and SpatialFormer TargetAttention (SFTA), to enhance the target object regions while reduce thebackground distraction. Particularly, SFSA highlights the regions with samesemantic information between pair features, and SFTA finds potential foregroundobject regions of novel feature that are similar to base categories. Extensiveexperiments show that our methods are effective and achieve newstate-of-the-art results on few-shot classification benchmarks."
Semantic Prompt for Few-Shot Image Recognition,"['Wentao Chen', 'Chenyang Si', 'Zhang Zhang', 'Liang Wang', 'Zilei Wang', 'Tieniu Tan']",http://arxiv.org/pdf/2303.14123v1.pdf,2023-03-24,['cs.cv'],"  Few-shot learning is a challenging problem since only a few examples areprovided to recognize a new class. Several recent studies exploit additionalsemantic information, e.g. text embeddings of class names, to address the issueof rare samples through combining semantic prototypes with visual prototypes.However, these methods still suffer from the spurious visual features learnedfrom the rare support samples, resulting in limited benefits. In this paper, wepropose a novel Semantic Prompt (SP) approach for few-shot learning. Instead ofthe naive exploitation of semantic information for remedying classifiers, weexplore leveraging semantic information as prompts to tune the visual featureextraction network adaptively. Specifically, we design two complementarymechanisms to insert semantic prompts into the feature extractor: one is toenable the interaction between semantic prompts and patch embeddings along thespatial dimension via self-attention, another is to supplement visual featureswith the transformed semantic prompts along the channel dimension. By combiningthese two mechanisms, the feature extractor presents a better ability to attendto the class-specific features and obtains more generalized imagerepresentations with merely a few support samples. Through extensiveexperiments on four datasets, the proposed approach achieves promising results,improving the 1-shot learning accuracy by 3.67% on average."
Dealing With Heterogeneous 3D MR Knee Images: A Federated Few-Shot  Learning Method With Dual Knowledge Distillation,"['Xiaoxiao He', 'Chaowei Tan', 'Bo Liu', 'Liping Si', 'Weiwu Yao', 'Liang Zhao', 'Di Liu', 'Qilong Zhangli', 'Qi Chang', 'Kang Li', 'Dimitris N. Metaxas']",http://arxiv.org/pdf/2303.14357v2.pdf,2023-03-25,"['eess.iv', 'cs.cv', 'cs.lg']","  Federated Learning has gained popularity among medical institutions since itenables collaborative training between clients (e.g., hospitals) withoutaggregating data. However, due to the high cost associated with creatingannotations, especially for large 3D image datasets, clinical institutions donot have enough supervised data for training locally. Thus, the performance ofthe collaborative model is subpar under limited supervision. On the other hand,large institutions have the resources to compile data repositories withhigh-resolution images and labels. Therefore, individual clients can utilizethe knowledge acquired in the public data repositories to mitigate the shortageof private annotated images. In this paper, we propose a federated few-shotlearning method with dual knowledge distillation. This method allows jointtraining with limited annotations across clients without jeopardizing privacy.The supervised learning of the proposed method extracts features from limitedlabeled data in each client, while the unsupervised data is used to distillboth feature and response-based knowledge from a national data repository tofurther improve the accuracy of the collaborative model and reduce thecommunication cost. Extensive evaluations are conducted on 3D magneticresonance knee images from a private clinical dataset. Our proposed methodshows superior performance and less training time than other semi-supervisedfederated learning methods. Codes and additional visualization results areavailable at https://github.com/hexiaoxiao-cs/fedml-knee."
What Makes for Effective Few-shot Point Cloud Classification?,"['Chuangguan Ye', 'Hongyuan Zhu', 'Yongbin Liao', 'Yanggang Zhang', 'Tao Chen', 'Jiayuan Fan']",http://arxiv.org/pdf/2304.00022v1.pdf,2023-03-31,['cs.cv'],"  Due to the emergence of powerful computing resources and large-scaleannotated datasets, deep learning has seen wide applications in our daily life.However, most current methods require extensive data collection and retrainingwhen dealing with novel classes never seen before. On the other hand, we humanscan quickly recognize new classes by looking at a few samples, which motivatesthe recent popularity of few-shot learning (FSL) in machine learningcommunities. Most current FSL approaches work on 2D image domain, however, itsimplication in 3D perception is relatively under-explored. Not only needs torecognize the unseen examples as in 2D domain, 3D few-shot learning is morechallenging with unordered structures, high intra-class variances, and subtleinter-class differences. Moreover, different architectures and learningalgorithms make it difficult to study the effectiveness of existing 2D methodswhen migrating to the 3D domain. In this work, for the first time, we performsystematic and extensive studies of recent 2D FSL and 3D backbone networks forbenchmarking few-shot point cloud classification, and we suggest a strongbaseline and learning architectures for 3D FSL. Then, we propose a novelplug-and-play component called Cross-Instance Adaptation (CIA) module, toaddress the high intra-class variances and subtle inter-class differencesissues, which can be easily inserted into current baselines with significantperformance improvement. Extensive experiments on two newly introducedbenchmark datasets, ModelNet40-FS and ShapeNet70-FS, demonstrate thesuperiority of our proposed network for 3D FSL."
Out-of-distribution Few-shot Learning For Edge Devices without Model  Fine-tuning,"['Xinyun Zhang', 'Lanqing Hong']",http://arxiv.org/pdf/2304.06309v1.pdf,2023-04-13,['cs.cv'],"  Few-shot learning (FSL) via customization of a deep learning network withlimited data has emerged as a promising technique to achieve personalized userexperiences on edge devices. However, existing FSL methods primarily assumeindependent and identically distributed (IID) data and utilize eithercomputational backpropagation updates for each task or a common model withtask-specific prototypes. Unfortunately, the former solution is infeasible foredge devices that lack on-device backpropagation capabilities, while the latteroften struggles with limited generalization ability, especially forout-of-distribution (OOD) data. This paper proposes a lightweight,plug-and-play FSL module called Task-aware Normalization (TANO) that enablesefficient and task-aware adaptation of a deep neural network withoutbackpropagation. TANO covers the properties of multiple user groups bycoordinating the updates of several groups of the normalization statisticsduring meta-training and automatically identifies the appropriate normalizationgroup for a downstream few-shot task. Consequently, TANO provides stable buttask-specific estimations of the normalization statistics to close thedistribution gaps and achieve efficient model adaptation. Results on bothintra-domain and out-of-domain generalization experiments demonstrate that TANOoutperforms recent methods in terms of accuracy, inference speed, and modelsize. Moreover, TANO achieves promising results on widely-used FSL benchmarksand data from real applications."
RPLKG: Robust Prompt Learning with Knowledge Graph,"['Yewon Kim', 'YongTaek Lim', 'Dokyung Yoon', 'KyungWoo Song']",http://arxiv.org/pdf/2304.10805v1.pdf,2023-04-21,"['cs.ai', 'cs.lg']","  Large-scale pre-trained models have been known that they are transferable,and they generalize well on the unseen dataset. Recently, multimodalpre-trained models such as CLIP show significant performance improvement indiverse experiments. However, when the labeled dataset is limited, thegeneralization of a new dataset or domain is still challenging. To improve thegeneralization performance on few-shot learning, there have been diverseefforts, such as prompt learning and adapter. However, the current few-shotadaptation methods are not interpretable, and they require a high computationcost for adaptation. In this study, we propose a new method, robust promptlearning with knowledge graph (RPLKG). Based on the knowledge graph, weautomatically design diverse interpretable and meaningful prompt sets. Ourmodel obtains cached embeddings of prompt sets after one forwarding from alarge pre-trained model. After that, model optimizes the prompt selectionprocesses with GumbelSoftmax. In this way, our model is trained usingrelatively little memory and learning time. Also, RPLKG selects the optimalinterpretable prompt automatically, depending on the dataset. In summary, RPLKGis i) interpretable, ii) requires small computation resources, and iii) easy toincorporate prior human knowledge. To validate the RPLKG, we providecomprehensive experimental results on few-shot learning, domain generalizationand new class generalization setting. RPLKG shows a significant performanceimprovement compared to zero-shot learning and competitive performance againstseveral prompt learning methods using much lower resources."
Context-enriched molecule representations improve few-shot drug  discovery,"['Johannes Schimunek', 'Philipp Seidl', 'Lukas Friedrich', 'Daniel Kuhn', 'Friedrich Rippmann', 'Sepp Hochreiter', 'Günter Klambauer']",http://arxiv.org/pdf/2305.09481v1.pdf,2023-04-24,"['q-bio.bm', 'cs.lg']","  A central task in computational drug discovery is to construct models fromknown active molecules to find further promising molecules for subsequentscreening. However, typically only very few active molecules are known.Therefore, few-shot learning methods have the potential to improve theeffectiveness of this critical phase of the drug discovery process. Weintroduce a new method for few-shot drug discovery. Its main idea is to enricha molecule representation by knowledge about known context or referencemolecules. Our novel concept for molecule representation enrichment is toassociate molecules from both the support set and the query set with a largeset of reference (context) molecules through a Modern Hopfield Network.Intuitively, this enrichment step is analogous to a human expert who wouldassociate a given molecule with familiar molecules whose properties are known.The enrichment step reinforces and amplifies the covariance structure of thedata, while simultaneously removing spurious correlations arising from thedecoration of molecules. Our approach is compared with other few-shot methodsfor drug discovery on the FS-Mol benchmark dataset. On FS-Mol, our approachoutperforms all compared methods and therefore sets a new state-of-the art forfew-shot learning in drug discovery. An ablation study shows that theenrichment step of our method is the key to improve the predictive quality. Ina domain shift experiment, we further demonstrate the robustness of our method.Code is available at https://github.com/ml-jku/MHNfs."
MetaModulation: Learning Variational Feature Hierarchies for Few-Shot  Learning with Fewer Tasks,"['Wenfang Sun', 'Yingjun Du', 'Xiantong Zhen', 'Fan Wang', 'Ling Wang', 'Cees G. M. Snoek']",http://arxiv.org/pdf/2305.10309v1.pdf,2023-05-17,['cs.lg'],"  Meta-learning algorithms are able to learn a new task using previouslylearned knowledge, but they often require a large number of meta-training taskswhich may not be readily available. To address this issue, we propose a methodfor few-shot learning with fewer tasks, which we call MetaModulation. The keyidea is to use a neural network to increase the density of the meta-trainingtasks by modulating batch normalization parameters during meta-training.Additionally, we modify parameters at various network levels, rather than justa single layer, to increase task diversity. To account for the uncertaintycaused by the limited training tasks, we propose a variational MetaModulationwhere the modulation parameters are treated as latent variables. We alsointroduce learning variational feature hierarchies by the variationalMetaModulation, which modulates features at all layers and can consider taskuncertainty and generate more diverse tasks. The ablation studies illustratethe advantages of utilizing a learnable task modulation at different levels anddemonstrate the benefit of incorporating probabilistic variants in few-taskmeta-learning. Our MetaModulation and its variational variants consistentlyoutperform state-of-the-art alternatives on four few-task meta-learningbenchmarks."
The CoT Collection: Improving Zero-shot and Few-shot Learning of  Language Models via Chain-of-Thought Fine-Tuning,"['Seungone Kim', 'Se June Joo', 'Doyoung Kim', 'Joel Jang', 'Seonghyeon Ye', 'Jamin Shin', 'Minjoon Seo']",http://arxiv.org/pdf/2305.14045v2.pdf,2023-05-23,"['cs.cl', 'cs.ai', 'cs.lg']","  Language models (LMs) with less than 100B parameters are known to performpoorly on chain-of-thought (CoT) reasoning in contrast to large LMs whensolving unseen tasks. In this work, we aim to equip smaller LMs with thestep-by-step reasoning capability by instruction tuning with CoT rationales. Inorder to achieve this goal, we first introduce a new instruction-tuning datasetcalled the CoT Collection, which augments the existing Flan Collection(including only 9 CoT tasks) with additional 1.84 million rationales across1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B & 11B) with CoTCollection enables smaller LMs to have better CoT capabilities on unseen tasks.On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of+4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot taskaccuracy. Furthermore, we show that instruction tuning with CoT Collectionallows LMs to possess stronger few-shot learning capabilities on 4domain-specific tasks, resulting in an improvement of +2.24% (Flan-T5 3B) and+2.37% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations untilthe max length by a +13.98% margin. Our code, the CoT Collection data, andmodel checkpoints are publicly available."
Active Learning Principles for In-Context Learning with Large Language  Models,"['Katerina Margatina', 'Timo Schick', 'Nikolaos Aletras', 'Jane Dwivedi-Yu']",http://arxiv.org/pdf/2305.14264v2.pdf,2023-05-23,"['cs.cl', 'cs.ai']","  The remarkable advancements in large language models (LLMs) havesignificantly enhanced the performance in few-shot learning settings. By usingonly a small number of labeled examples, referred to as demonstrations, LLMscan effectively grasp the task at hand through in-context learning. However,the process of selecting appropriate demonstrations has received limitedattention in prior work. This paper addresses the issue of identifying the mostinformative demonstrations for few-shot learning by approaching it as apool-based Active Learning (AL) problem over a single iteration. Our objectiveis to investigate how AL algorithms can serve as effective demonstrationselection methods for in-context learning. We compare various standard ALalgorithms based on uncertainty, diversity, and similarity, and consistentlyobserve that the latter outperforms all other methods, including randomsampling. Notably, uncertainty sampling, despite its success in conventionalsupervised learning scenarios, performs poorly in this context. Our extensiveexperimentation involving a diverse range of GPT and OPT models across $24$classification and multi-choice tasks, coupled with thorough analysis,unambiguously demonstrates that in-context example selection through ALprioritizes high-quality examples that exhibit low uncertainty and bearsimilarity to the test examples."
Improving few-shot learning-based protein engineering with evolutionary  sampling,"['M. Zaki Jawaid', 'Robin W. Yeo', 'Aayushma Gautam', 'T. Blair Gainous', 'Daniel O. Hart', 'Timothy P. Daley']",http://arxiv.org/pdf/2305.15441v2.pdf,2023-05-23,"['q-bio.qm', 'cs.lg']","  Designing novel functional proteins remains a slow and expensive process dueto a variety of protein engineering challenges; in particular, the number ofprotein variants that can be experimentally tested in a given assay pales incomparison to the vastness of the overall sequence space, resulting in low hitrates and expensive wet lab testing cycles. In this paper, we propose afew-shot learning approach to novel protein design that aims to accelerate theexpensive wet lab testing cycle and is capable of leveraging a training datasetthat is both small and skewed ($\approx 10^5$ datapoints, $< 1\%$ positivehits). Our approach is composed of two parts: a semi-supervised transferlearning approach to generate a discrete fitness landscape for a desiredprotein function and a novel evolutionary Monte Carlo Markov Chain samplingalgorithm to more efficiently explore the fitness landscape. We demonstrate theperformance of our approach by experimentally screening predicted high fitnessgene activators, resulting in a dramatically improved hit rate compared toexisting methods. Our method can be easily adapted to other protein engineeringand design problems, particularly where the cost associated with obtaininglabeled data is significantly high. We have provided open source code for ourmethod at https://github.com/SuperSecretBioTech/evolutionary_monte_carlo_search."
Epistemic Graph: A Plug-And-Play Module For Hybrid Representation  Learning,"['Jin Yuan', 'Yang Zhang', 'Yangzhou Du', 'Zhongchao Shi', 'Xin Geng', 'Jianping Fan', 'Yong Rui']",http://arxiv.org/pdf/2305.18731v3.pdf,2023-05-30,"['cs.cv', 'cs.ai']","  In recent years, deep models have achieved remarkable success in variousvision tasks. However, their performance heavily relies on large trainingdatasets. In contrast, humans exhibit hybrid learning, seamlessly integratingstructured knowledge for cross-domain recognition or relying on a smalleramount of data samples for few-shot learning. Motivated by this human-likeepistemic process, we aim to extend hybrid learning to computer vision tasks byintegrating structured knowledge with data samples for more effectiverepresentation learning. Nevertheless, this extension faces significantchallenges due to the substantial gap between structured knowledge and deepfeatures learned from data samples, encompassing both dimensions and knowledgegranularity. In this paper, a novel Epistemic Graph Layer (EGLayer) isintroduced to enable hybrid learning, enhancing the exchange of informationbetween deep features and a structured knowledge graph. Our EGLayer is composedof three major parts, including a local graph module, a query aggregationmodel, and a novel correlation alignment loss function to emulate humanepistemic ability. Serving as a plug-and-play module that can replace thestandard linear classifier, EGLayer significantly improves the performance ofdeep models. Extensive experiments demonstrates that EGLayer can greatlyenhance representation learning for the tasks of cross-domain recognition andfew-shot learning, and the visualization of knowledge graphs can aid in modelinterpretation."
Adversarial Robustness of Prompt-based Few-Shot Learning for Natural  Language Understanding,"['Venkata Prabhakara Sarath Nookala', 'Gaurav Verma', 'Subhabrata Mukherjee', 'Srijan Kumar']",http://arxiv.org/pdf/2306.11066v2.pdf,2023-06-19,"['cs.cl', 'cs.lg']","  State-of-the-art few-shot learning (FSL) methods leverage prompt-basedfine-tuning to obtain remarkable results for natural language understanding(NLU) tasks. While much of the prior FSL methods focus on improving downstreamtask performance, there is a limited understanding of the adversarialrobustness of such methods. In this work, we conduct an extensive study ofseveral state-of-the-art FSL methods to assess their robustness to adversarialperturbations. To better understand the impact of various factors towardsrobustness (or the lack of it), we evaluate prompt-based FSL methods againstfully fine-tuned models for aspects such as the use of unlabeled data, multipleprompts, number of few-shot examples, model size and type. Our results on sixGLUE tasks indicate that compared to fully fine-tuned models, vanilla FSLmethods lead to a notable relative drop in task performance (i.e., are lessrobust) in the face of adversarial perturbations. However, using (i) unlabeleddata for prompt-based FSL and (ii) multiple prompts flip the trend. We furtherdemonstrate that increasing the number of few-shot examples and model size leadto increased adversarial robustness of vanilla FSL methods. Broadly, our worksheds light on the adversarial robustness evaluation of prompt-based FSLmethods for NLU tasks."
Few-shot Learning for Inference in Medical Imaging with Subspace Feature  Representations,"['Jiahui Liu', 'Keqiang Fan', 'Xiaohao Cai', 'Mahesan Niranjan']",http://arxiv.org/pdf/2306.11152v1.pdf,2023-06-19,"['math.na', 'cs.na']","  Unlike the field of visual scene recognition where tremendous advances havetaken place due to the availability of very large datasets to train deep neuralnetworks, inference from medical images is often hampered by the fact that onlysmall amounts of data may be available. When working with very small datasetproblems, of the order of a few hundred items of data, the power of deeplearning may still be exploited by using a model pre-trained on natural imagesas a feature extractor and carrying out classic pattern recognition techniquesin this feature space, the so-called few-shot learning problem. In regimeswhere the dimension of this feature space is comparable to or even larger thanthe number of items of data, dimensionality reduction is a necessity and isoften achieved by principal component analysis, i.e., singular valuedecomposition (SVD). In this paper, noting the inappropriateness of using SVDfor this setting, we usher in and explore two alternatives based ondiscriminant analysis and non-negative matrix factorization (NMF). Using 14different datasets spanning $11$ distinct disease types, we demonstrate thatdiscriminant subspaces at low dimensions achieve significant improvements overSVD-based subspaces and the original feature space. We also show that NMF atmodest dimensions is a competitive alternative to SVD in this setting."
Visually grounded few-shot word learning in low-resource settings,"['Leanne Nortje', 'Dan Oneata', 'Herman Kamper']",http://arxiv.org/pdf/2306.11371v2.pdf,2023-06-20,"['eess.as', 'cs.cl']","  We propose a visually grounded speech model that learns new words and theirvisual depictions from just a few word-image example pairs. Given a set of testimages and a spoken query, we ask the model which image depicts the query word.Previous work has simplified this few-shot learning problem by either using anartificial setting with digit word-image pairs or by using a large number ofexamples per class. Moreover, all previous studies were performed using Englishspeech-image data. We propose an approach that can work on natural word-imagepairs but with less examples, i.e. fewer shots, and then illustrate how thisapproach can be applied for multimodal few-shot learning in a real low-resourcelanguage, Yoruba. Our approach involves using the given word-image examplepairs to mine new unsupervised word-image training pairs from large collectionsof unlabelledspeech and images. Additionally, we use a word-to-image attentionmechanism to determine word-image similarity. With this new model, we achievebetter performance with fewer shots than previous approaches on an existingEnglish benchmark. Many of the model's mistakes are due to confusion betweenvisual concepts co-occurring in similar contexts. The experiments on Yorubashow the benefit of transferring knowledge from a multimodal model trained on alarger set of English speech-image data."
Cross-Modal Concept Learning and Inference for Vision-Language Models,"['Yi Zhang', 'Ce Zhang', 'Yushun Tang', 'Zhihai He']",http://arxiv.org/pdf/2307.15460v1.pdf,2023-07-28,"['cs.cv', 'cs.cl']","  Large-scale pre-trained Vision-Language Models (VLMs), such as CLIP,establish the correlation between texts and images, achieving remarkablesuccess on various downstream tasks with fine-tuning. In existing fine-tuningmethods, the class-specific text description is matched against the wholeimage. We recognize that this whole image matching is not effective sinceimages from the same class often contain a set of different semantic objects,and an object further consists of a set of semantic parts or concepts.Individual semantic parts or concepts may appear in image samples fromdifferent classes. To address this issue, in this paper, we develop a newmethod called cross-model concept learning and inference (CCLI). Using thepowerful text-image correlation capability of CLIP, our method automaticallylearns a large set of distinctive visual concepts from images using a set ofsemantic text concepts. Based on these visual concepts, we construct adiscriminative representation of images and learn a concept inference networkto perform downstream image classification tasks, such as few-shot learning anddomain generalization. Extensive experimental results demonstrate that our CCLImethod is able to improve the performance upon the current state-of-the-artmethods by large margins, for example, by up to 8.0% improvement on few-shotlearning and by up to 1.3% for domain generalization."
Demonstration-based learning for few-shot biomedical named entity  recognition under machine reading comprehension,"['Leilei Su', 'Jian Chen', 'Yifan Peng', 'Cong Sun']",http://arxiv.org/pdf/2308.06454v1.pdf,2023-08-12,['cs.cl'],"  Although deep learning techniques have shown significant achievements, theyfrequently depend on extensive amounts of hand-labeled data and tend to performinadequately in few-shot scenarios. The objective of this study is to devise astrategy that can improve the model's capability to recognize biomedicalentities in scenarios of few-shot learning. By redefining biomedical namedentity recognition (BioNER) as a machine reading comprehension (MRC) problem,we propose a demonstration-based learning method to address few-shot BioNER,which involves constructing appropriate task demonstrations. In assessing ourproposed method, we compared the proposed method with existing advanced methodsusing six benchmark datasets, including BC4CHEMD, BC5CDR-Chemical,BC5CDR-Disease, NCBI-Disease, BC2GM, and JNLPBA. We examined the models'efficacy by reporting F1 scores from both the 25-shot and 50-shot learningexperiments. In 25-shot learning, we observed 1.1% improvements in the averageF1 scores compared to the baseline method, reaching 61.7%, 84.1%, 69.1%, 70.1%,50.6%, and 59.9% on six datasets, respectively. In 50-shot learning, we furtherimproved the average F1 scores by 1.0% compared to the baseline method,reaching 73.1%, 86.8%, 76.1%, 75.6%, 61.7%, and 65.4%, respectively. Wereported that in the realm of few-shot learning BioNER, MRC-based languagemodels are much more proficient in recognizing biomedical entities compared tothe sequence labeling approach. Furthermore, our MRC-language models cancompete successfully with fully-supervised learning methodologies that relyheavily on the availability of abundant annotated data. These results highlightpossible pathways for future advancements in few-shot BioNER methodologies."
Robustness Over Time: Understanding Adversarial Examples' Effectiveness  on Longitudinal Versions of Large Language Models,"['Yugeng Liu', 'Tianshuo Cong', 'Zhengyu Zhao', 'Michael Backes', 'Yun Shen', 'Yang Zhang']",http://arxiv.org/pdf/2308.07847v1.pdf,2023-08-15,['cs.cr'],"  Large Language Models (LLMs) have led to significant improvements in manytasks across various domains, such as code interpretation, response generation,and ambiguity handling. These LLMs, however, when upgrading, primarilyprioritize enhancing user experience while neglecting security, privacy, andsafety implications. Consequently, unintended vulnerabilities or biases can beintroduced. Previous studies have predominantly focused on specific versions ofthe models and disregard the potential emergence of new attack vectorstargeting the updated versions. Through the lens of adversarial examples withinthe in-context learning framework, this longitudinal study addresses this gapby conducting a comprehensive assessment of the robustness of successiveversions of LLMs, vis-\`a-vis GPT-3.5. We conduct extensive experiments toanalyze and understand the impact of the robustness in two distinct learningcategories: zero-shot learning and few-shot learning. Our findings indicatethat, in comparison to earlier versions of LLMs, the updated versions do notexhibit the anticipated level of robustness against adversarial attacks. Inaddition, our study emphasizes the increased effectiveness of synergizedadversarial queries in most zero-shot learning and few-shot learning cases. Wehope that our study can lead to a more refined assessment of the robustness ofLLMs over time and provide valuable insights of these models for bothdevelopers and users."
UniAP: Towards Universal Animal Perception in Vision via Few-shot  Learning,"['Meiqi Sun', 'Zhonghan Zhao', 'Wenhao Chai', 'Hanjun Luo', 'Shidong Cao', 'Yanting Zhang', 'Jenq-Neng Hwang', 'Gaoang Wang']",http://arxiv.org/pdf/2308.09953v1.pdf,2023-08-19,['cs.cv'],"  Animal visual perception is an important technique for automaticallymonitoring animal health, understanding animal behaviors, and assistinganimal-related research. However, it is challenging to design a deeplearning-based perception model that can freely adapt to different animalsacross various perception tasks, due to the varying poses of a large diversityof animals, lacking data on rare species, and the semantic inconsistency ofdifferent tasks. We introduce UniAP, a novel Universal Animal Perception modelthat leverages few-shot learning to enable cross-species perception amongvarious visual tasks. Our proposed model takes support images and labels asprompt guidance for a query image. Images and labels are processed through aTransformer-based encoder and a lightweight label encoder, respectively. Then amatching module is designed for aggregating information between prompt guidanceand the query image, followed by a multi-head label decoder to generate outputsfor various tasks. By capitalizing on the shared visual characteristics amongdifferent animals and tasks, UniAP enables the transfer of knowledge fromwell-studied species to those with limited labeled data or even unseen species.We demonstrate the effectiveness of UniAP through comprehensive experiments inpose estimation, segmentation, and classification tasks on diverse animalspecies, showcasing its ability to generalize and adapt to new classes withminimal labeled examples."
CDFSL-V: Cross-Domain Few-Shot Learning for Videos,"['Sarinda Samarasinghe', 'Mamshad Nayeem Rizve', 'Navid Kardan', 'Mubarak Shah']",http://arxiv.org/pdf/2309.03989v2.pdf,2023-09-07,['cs.cv'],"  Few-shot video action recognition is an effective approach to recognizing newcategories with only a few labeled examples, thereby reducing the challengesassociated with collecting and annotating large-scale video datasets. Existingmethods in video action recognition rely on large labeled datasets from thesame domain. However, this setup is not realistic as novel categories may comefrom different data domains that may have different spatial and temporalcharacteristics. This dissimilarity between the source and target domains canpose a significant challenge, rendering traditional few-shot action recognitiontechniques ineffective. To address this issue, in this work, we propose a novelcross-domain few-shot video action recognition method that leveragesself-supervised learning and curriculum learning to balance the informationfrom the source and target domains. To be particular, our method employs amasked autoencoder-based self-supervised training objective to learn from bothsource and target data in a self-supervised manner. Then a progressivecurriculum balances learning the discriminative information from the sourcedataset with the generic information learned from the target domain. Initially,our curriculum utilizes supervised learning to learn class discriminativefeatures from the source data. As the training progresses, we transition tolearning target-domain-specific features. We propose a progressive curriculumto encourage the emergence of rich features in the target domain based on classdiscriminative supervised features in the source domain. We evaluate our methodon several challenging benchmark datasets and demonstrate that our approachoutperforms existing cross-domain few-shot learning techniques. Our code isavailable at https://github.com/Sarinda251/CDFSL-V"
Few-Shot Learning Patterns in Financial Time-Series for Trend-Following  Strategies,"['Kieran Wood', 'Samuel Kessler', 'Stephen J. Roberts', 'Stefan Zohren']",http://arxiv.org/pdf/2310.10500v1.pdf,2023-10-16,"['q-fin.tr', 'cs.lg', 'q-fin.pm']","  Forecasting models for systematic trading strategies do not adapt quicklywhen financial market conditions change, as was seen in the advent of theCOVID-19 pandemic in 2020, when market conditions changed dramatically causingmany forecasting models to take loss-making positions. To deal with suchsituations, we propose a novel time-series trend-following forecaster that isable to quickly adapt to new market conditions, referred to as regimes. Weleverage recent developments from the deep learning community and use few-shotlearning. We propose the Cross Attentive Time-Series Trend Network - X-Trend -which takes positions attending over a context set of financial time-seriesregimes. X-Trend transfers trends from similar patterns in the context set tomake predictions and take positions for a new distinct target regime. X-Trendis able to quickly adapt to new financial regimes with a Sharpe ratio increaseof 18.9% over a neural forecaster and 10-fold over a conventional Time-seriesMomentum strategy during the turbulent market period from 2018 to 2023. Ourstrategy recovers twice as quickly from the COVID-19 drawdown compared to theneural-forecaster. X-Trend can also take zero-shot positions on novel unseenfinancial assets obtaining a 5-fold Sharpe ratio increase versus a neuraltime-series trend forecaster over the same period. X-Trend both forecastsnext-day prices and outputs a trading signal. Furthermore, the cross-attentionmechanism allows us to interpret the relationship between forecasts andpatterns in the context set."
Experimental Results of Underwater Sound Speed Profile Inversion by  Few-shot Multi-task Learning,"['Wei Huang', 'Fan Gao', 'Junting Wang', 'Hao Zhang']",http://arxiv.org/pdf/2310.11708v1.pdf,2023-10-18,"['eess.as', 'cs.sd']","  Underwater Sound Speed Profile (SSP) distribution has great influence on thepropagation mode of acoustic signal, thus the fast and accurate estimation ofSSP is of great importance in building underwater observation systems. Thestate-of-the-art SSP inversion methods include frameworks of matched fieldprocessing (MFP), compressive sensing (CS), and feedforeward neural networks(FNN), among which the FNN shows better real-time performance while maintainthe same level of accuracy. However, the training of FNN needs quite a lothistorical SSP samples, which is diffcult to be satisfied in many ocean areas.This situation is called few-shot learning. To tackle this issue, we propose amulti-task learning (MTL) model with partial parameter sharing among differenttraning tasks. By MTL, common features could be extracted, thus acceleratingthe learning process on given tasks, and reducing the demand for referencesamples, so as to enhance the generalization ability in few-shot learning. Toverify the feasibility and effectiveness of MTL, a deep-ocean experiment washeld in April 2023 at the South China Sea. Results shows that MTL outperformsthe state-of-the-art methods in terms of accuracy for SSP inversion, whileinherits the real-time advantage of FNN during the inversion stage."
Are LSTMs Good Few-Shot Learners?,"['Mike Huisman', 'Thomas M. Moerland', 'Aske Plaat', 'Jan N. van Rijn']",http://arxiv.org/pdf/2310.14139v1.pdf,2023-10-22,"['cs.lg', 'cs.ai', 'stat.ml']","  Deep learning requires large amounts of data to learn new tasks well,limiting its applicability to domains where such data is available.Meta-learning overcomes this limitation by learning how to learn. In 2001,Hochreiter et al. showed that an LSTM trained with backpropagation acrossdifferent tasks is capable of meta-learning. Despite promising results of thisapproach on small problems, and more recently, also on reinforcement learningproblems, the approach has received little attention in the supervised few-shotlearning setting. We revisit this approach and test it on modern few-shotlearning benchmarks. We find that LSTM, surprisingly, outperform the popularmeta-learning technique MAML on a simple few-shot sine wave regressionbenchmark, but that LSTM, expectedly, fall short on more complex few-shot imageclassification benchmarks. We identify two potential causes and propose a newmethod called Outer Product LSTM (OP-LSTM) that resolves these issues anddisplays substantial performance gains over the plain LSTM. Compared to popularmeta-learning baselines, OP-LSTM yields competitive performance onwithin-domain few-shot image classification, and performs better incross-domain settings by 0.5% to 1.9% in accuracy score. While these resultsalone do not set a new state-of-the-art, the advances of OP-LSTM are orthogonalto other advances in the field of meta-learning, yield new insights in how LSTMwork in image classification, allowing for a whole range of new researchdirections. For reproducibility purposes, we publish all our research codepublicly."
Few Shot Learning for the Classification of Confocal Laser  Endomicroscopy Images of Head and Neck Tumors,"['Marc Aubreville', 'Zhaoya Pan', 'Matti Sievert', 'Jonas Ammeling', 'Jonathan Ganz', 'Nicolai Oetter', 'Florian Stelzle', 'Ann-Kathrin Frenken', 'Katharina Breininger', 'Miguel Goncalves']",http://arxiv.org/pdf/2311.07216v1.pdf,2023-11-13,['cs.cv'],"  The surgical removal of head and neck tumors requires safe margins, which areusually confirmed intraoperatively by means of frozen sections. This method is,in itself, an oversampling procedure, which has a relatively low sensitivitycompared to the definitive tissue analysis on paraffin-embedded sections.Confocal laser endomicroscopy (CLE) is an in-vivo imaging technique that hasshown its potential in the live optical biopsy of tissue. An automated analysisof this notoriously difficult to interpret modality would help surgeons.However, the images of CLE show a wide variability of patterns, caused both byindividual factors but also, and most strongly, by the anatomical structures ofthe imaged tissue, making it a challenging pattern recognition task. In thiswork, we evaluate four popular few shot learning (FSL) methods towards theircapability of generalizing to unseen anatomical domains in CLE images. Weevaluate this on images of sinunasal tumors (SNT) from five patients and onimages of the vocal folds (VF) from 11 patients using a cross-validationscheme. The best respective approach reached a median accuracy of 79.6% on therather homogeneous VF dataset, but only of 61.6% for the highly diverse SNTdataset. Our results indicate that FSL on CLE images is viable, but stronglyaffected by the number of patients, as well as the diversity of anatomicalpatterns."
A Multi-solution Study on GDPR AI-enabled Completeness Checking of DPAs,"['Muhammad Ilyas Azeem', 'Sallam Abualhaija']",http://arxiv.org/pdf/2311.13881v1.pdf,2023-11-23,"['cs.se', 'cs.ai']","  Specifying legal requirements for software systems to ensure their compliancewith the applicable regulations is a major concern to requirements engineering(RE). Personal data which is collected by an organization is often shared withother organizations to perform certain processing activities. In such cases,the General Data Protection Regulation (GDPR) requires issuing a dataprocessing agreement (DPA) which regulates the processing and further ensuresthat personal data remains protected. Violating GDPR can lead to huge finesreaching to billions of Euros. Software systems involving personal dataprocessing must adhere to the legal obligations stipulated in GDPR and outlinedin DPAs. Requirements engineers can elicit from DPAs legal requirements forregulating the data processing activities in software systems. Checking thecompleteness of a DPA according to the GDPR provisions is therefore anessential prerequisite to ensure that the elicited requirements are complete.Analyzing DPAs entirely manually is time consuming and requires adequate legalexpertise. In this paper, we propose an automation strategy to address thecompleteness checking of DPAs against GDPR. Specifically, we pursue tenalternative solutions which are enabled by different technologies, namelytraditional machine learning, deep learning, language modeling, and few-shotlearning. The goal of our work is to empirically examine how these differenttechnologies fare in the legal domain. We computed F2 score on a set of 30 realDPAs. Our evaluation shows that best-performing solutions yield F2 score of86.7% and 89.7% are based on pre-trained BERT and RoBERTa language models. Ouranalysis further shows that other alternative solutions based on deep learning(e.g., BiLSTM) and few-shot learning (e.g., SetFit) can achieve comparableaccuracy, yet are more efficient to develop."
Italian Crossword Generator: Enhancing Education through Interactive  Word Puzzles,"['Kamyar Zeinalipour', 'Tommaso laquinta', 'Asya Zanollo', 'Giovanni Angelini', 'Leonardo Rigutini', 'Marco Maggini', 'Marco Gori']",http://arxiv.org/pdf/2311.15723v1.pdf,2023-11-27,"['cs.cl', 'cs.ai']","  Educational crosswords offer numerous benefits for students, includingincreased engagement, improved understanding, critical thinking, and memoryretention. Creating high-quality educational crosswords can be challenging, butrecent advances in natural language processing and machine learning have madeit possible to use language models to generate nice wordplays. The exploitationof cutting-edge language models like GPT3-DaVinci, GPT3-Curie, GPT3-Babbage,GPT3-Ada, and BERT-uncased has led to the development of a comprehensive systemfor generating and verifying crossword clues. A large dataset of clue-answerpairs was compiled to fine-tune the models in a supervised manner to generateoriginal and challenging clues from a given keyword. On the other hand, forgenerating crossword clues from a given text, Zero/Few-shot learning techniqueswere used to extract clues from the input text, adding variety and creativityto the puzzles. We employed the fine-tuned model to generate data and labeledthe acceptability of clue-answer parts with human supervision. To ensurequality, we developed a classifier by fine-tuning existing language models onthe labeled dataset. Conversely, to assess the quality of clues generated fromthe given text using zero/few-shot learning, we employed a zero-shot learningapproach to check the quality of generated clues. The results of the evaluationhave been very promising, demonstrating the effectiveness of the approach increating high-standard educational crosswords that offer students engaging andrewarding learning experiences."
Robust Transductive Few-shot Learning via Joint Message Passing and  Prototype-based Soft-label Propagation,"['Jiahui Wang', 'Qin Xu', 'Bo Jiang', 'Bin Luo']",http://arxiv.org/pdf/2311.17096v1.pdf,2023-11-28,['cs.cv'],"  Few-shot learning (FSL) aims to develop a learning model with the ability togeneralize to new classes using a few support samples. For transductive FSLtasks, prototype learning and label propagation methods are commonly employed.Prototype methods generally first learn the representative prototypes from thesupport set and then determine the labels of queries based on the metricbetween query samples and prototypes. Label propagation methods try topropagate the labels of support samples on the constructed graph encoding therelationships between both support and query samples. This paper aims tointegrate these two principles together and develop an efficient and robusttransductive FSL approach, termed Prototype-based Soft-label Propagation(PSLP). Specifically, we first estimate the soft-label presentation for eachquery sample by leveraging prototypes. Then, we conduct soft-label propagationon our learned query-support graph. Both steps are conducted progressively toboost their respective performance. Moreover, to learn effective prototypes forsoft-label estimation as well as the desirable query-support graph forsoft-label propagation, we design a new joint message passing scheme to learnsample presentation and relational graph jointly. Our PSLP method isparameter-free and can be implemented very efficiently. On four populardatasets, our method achieves competitive results on both balanced andimbalanced settings compared to the state-of-the-art methods. The code will bereleased upon acceptance."
D$^2$ST-Adapter: Disentangled-and-Deformable Spatio-Temporal Adapter for  Few-shot Action Recognition,"['Wenjie Pei', 'Qizhong Tan', 'Guangming Lu', 'Jiandong Tian']",http://arxiv.org/pdf/2312.01431v1.pdf,2023-12-03,['cs.cv'],"  Adapting large pre-trained image models to few-shot action recognition hasproven to be an effective and efficient strategy for learning robust featureextractors, which is essential for few-shot learning. Typical fine-tuning basedadaptation paradigm is prone to overfitting in the few-shot learning scenariosand offers little modeling flexibility for learning temporal features in videodata. In this work we present the Disentangled-and-Deformable Spatio-TemporalAdapter (D$^2$ST-Adapter), a novel adapter tuning framework for few-shot actionrecognition, which is designed in a dual-pathway architecture to encode spatialand temporal features in a disentangled manner. Furthermore, we devise theDeformable Spatio-Temporal Attention module as the core component ofD$^2$ST-Adapter, which can be tailored to model both spatial and temporalfeatures in corresponding pathways, allowing our D$^2$ST-Adapter to encodefeatures in a global view in 3D spatio-temporal space while maintaining alightweight design. Extensive experiments with instantiations of our method onboth pre-trained ResNet and ViT demonstrate the superiority of our method overstate-of-the-art methods for few-shot action recognition. Our method isparticularly well-suited to challenging scenarios where temporal dynamics arecritical for action recognition."
Efficient Few-Shot Clinical Task Adaptation with Large Language Models,"['Kaipeng Zheng', 'Weiran Huang', 'Lichao Sun']",http://arxiv.org/pdf/2312.07125v1.pdf,2023-12-12,['cs.cv'],"  Few-shot learning has been studied to adapt models to tasks with very fewsamples. It holds profound significance, particularly in clinical tasks, due tothe high annotation cost of medical images. Several works have exploredfew-shot learning on medical images, yet they still require a large number ofmedical images for pre-training models to gain domain-specific priors. Visionfoundation models recently have achieved remarkable success in natural images.Hence, adapting rapidly advancing vision foundation models from natural imagesto few-shot clinical tasks holds great promise. MedFMC has recently organized achallenge to shed more light on this topic at NeurIPS 2023. In this work, wepresent our challenge solution. We observe that a simple variant of fine-tuningwith partial freezing shows remarkable performance. Empirical evidencedemonstrates that this approach could outperform various common fine-tuningmethods under limited sample sizes. Additionally, we explore enhancedutilization of semantic supervision to boost performance. We propose a novelapproach that contextualizes labels via large language models (LLMs). Ourfindings reveal that the context generated by LLMs significantly enhances thediscrimination of semantic embeddings for similar categories, resulting in anotable performance improvement of 3%-5% in 1-shot settings compared tocommonly employed one-hot labels and other semantic supervision methods. Oursolution secures the 1st place in the MedFMC challenge."
Boosting LLM Reasoning: Push the Limits of Few-shot Learning with  Reinforced In-Context Pruning,"['Xijie Huang', 'Li Lyna Zhang', 'Kwang-Ting Cheng', 'Mao Yang']",http://arxiv.org/pdf/2312.08901v1.pdf,2023-12-14,"['cs.cl', 'cs.ai']","  Large language models (LLMs) have shown impressive capabilities in varioustasks, yet they still struggle with math reasoning. Despite efforts to optimizeChain-of-Thoughts (CoT) prompts and fine-tune LLMs, the potential of few-shotlearning remains unexplored. In this work, we propose CoT-Max, a novel approachpushing the boundaries of few-shot CoT learning to improve LLM math reasoningcapabilities. CoT-Max addresses the challenges of the selection of usefulexamples and limited number of examples due to restricted context windowlength. Inspired by our observation that natural language inputs contain manyredundancy, we propose a coarse-to-fine pruner as a plug-and-play module forLLMs, which first identifies crucial CoT examples from a large batch and thenfurther prunes unimportant tokens. To train the pruner, we collect a mathreasoning dataset with diverse difficulty and steps, introduce a reward tomeasure both the input's effectiveness for math reasoning and token lengthconstraints, and propose a novel training approach with reinforcement learning.As a result, CoT-Max significantly outperforms CoT and few-shot promptingbaselines across various LLMs (LLaMA2-7B, 13B, 70B) and 5 mathematicaldatasets, achieving up to 4.55% absolute improvements. Remarkably, without anyfine-tuning, LLaMA2-70B with CoT-Max surpasses GPT-3.5 and a wide range oflarger LLMs (PaLM, Minerva, etc.) on the GSM8K."
PaLM: Scaling Language Modeling with Pathways,"['Aakanksha Chowdhery', 'Sharan Narang', 'Jacob Devlin', 'Maarten Bosma', 'Gaurav Mishra', 'Adam Roberts', 'Paul Barham', 'Hyung Won Chung', 'Charles Sutton', 'Sebastian Gehrmann', 'Parker Schuh', 'Kensen Shi', 'Sasha Tsvyashchenko', 'Joshua Maynez', 'Abhishek Rao', 'Parker Barnes', 'Yi Tay', 'Noam Shazeer', 'Vinodkumar Prabhakaran', 'Emily Reif', 'Nan Du', 'Ben Hutchinson', 'Reiner Pope', 'James Bradbury', 'Jacob Austin', 'Michael Isard', 'Guy Gur-Ari', 'Pengcheng Yin', 'Toju Duke', 'Anselm Levskaya', 'Sanjay Ghemawat', 'Sunipa Dev', 'Henryk Michalewski', 'Xavier Garcia', 'Vedant Misra', 'Kevin Robinson', 'Liam Fedus', 'Denny Zhou', 'Daphne Ippolito', 'David Luan', 'Hyeontaek Lim', 'Barret Zoph', 'Alexander Spiridonov', 'Ryan Sepassi', 'David Dohan', 'Shivani Agrawal', 'Mark Omernick', 'Andrew M. Dai', 'Thanumalayan Sankaranarayana Pillai', 'Marie Pellat', 'Aitor Lewkowycz', 'Erica Moreira', 'Rewon Child', 'Oleksandr Polozov', 'Katherine Lee', 'Zongwei Zhou', 'Xuezhi Wang', 'Brennan Saeta', 'Mark Diaz', 'Orhan Firat', 'Michele Catasta', 'Jason Wei', 'Kathy Meier-Hellstern', 'Douglas Eck', 'Jeff Dean', 'Slav Petrov', 'Noah Fiedel']",http://arxiv.org/pdf/2204.02311v5.pdf,2022-04-05,['cs.cl'],"  Large language models have been shown to achieve remarkable performanceacross a variety of natural language tasks using few-shot learning, whichdrastically reduces the number of task-specific training examples needed toadapt the model to a particular application. To further our understanding ofthe impact of scale on few-shot learning, we trained a 540-billion parameter,densely activated, Transformer language model, which we call Pathways LanguageModel PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new MLsystem which enables highly efficient training across multiple TPU Pods. Wedemonstrate continued benefits of scaling by achieving state-of-the-artfew-shot learning results on hundreds of language understanding and generationbenchmarks. On a number of these tasks, PaLM 540B achieves breakthroughperformance, outperforming the finetuned state-of-the-art on a suite ofmulti-step reasoning tasks, and outperforming average human performance on therecently released BIG-bench benchmark. A significant number of BIG-bench tasksshowed discontinuous improvements from model scale, meaning that performancesteeply increased as we scaled to our largest model. PaLM also has strongcapabilities in multilingual tasks and source code generation, which wedemonstrate on a wide array of benchmarks. We additionally provide acomprehensive analysis on bias and toxicity, and study the extent of trainingdata memorization with respect to model scale. Finally, we discuss the ethicalconsiderations related to large language models and discuss potentialmitigation strategies."
Prototype Propagation Networks (PPN) for Weakly-supervised Few-shot  Learning on Category Graph,"['Lu Liu', 'Tianyi Zhou', 'Guodong Long', 'Jing Jiang', 'Lina Yao', 'Chengqi Zhang']",http://arxiv.org/pdf/1905.04042v2.pdf,2019-05-10,"['cs.lg', 'cs.cv', 'cs.ne', 'stat.ml']","  A variety of machine learning applications expect to achieve rapid learningfrom a limited number of labeled data. However, the success of most currentmodels is the result of heavy training on big data. Meta-learning addressesthis problem by extracting common knowledge across different tasks that can bequickly adapted to new tasks. However, they do not fully exploreweakly-supervised information, which is usually free or cheap to collect. Inthis paper, we show that weakly-labeled data can significantly improve theperformance of meta-learning on few-shot classification. We propose prototypepropagation network (PPN) trained on few-shot tasks together with dataannotated by coarse-label. Given a category graph of the targeted fine-classesand some weakly-labeled coarse-classes, PPN learns an attention mechanism whichpropagates the prototype of one class to another on the graph, so that theK-nearest neighbor (KNN) classifier defined on the propagated prototypesresults in high accuracy across different few-shot tasks. The training tasksare generated by subgraph sampling, and the training objective is obtained byaccumulating the level-wise classification loss on the subgraph. The resultinggraph of prototypes can be continually re-used and updated for new tasks andclasses. We also introduce two practical test/inference settings which differaccording to whether the test task can leverage any weakly-supervisedinformation as in training. On two benchmarks, PPN significantly outperformsmost recent few-shot learning methods in different settings, even when they arealso allowed to train on weakly-labeled data."
Learning to Propagate for Graph Meta-Learning,"['Lu Liu', 'Tianyi Zhou', 'Guodong Long', 'Jing Jiang', 'Chengqi Zhang']",http://arxiv.org/pdf/1909.05024v2.pdf,2019-09-11,"['cs.lg', 'cs.cv', 'stat.ml']","  Meta-learning extracts common knowledge from learning different tasks anduses it for unseen tasks. It can significantly improve tasks that suffer frominsufficient training data, e.g., few shot learning. In most meta-learningmethods, tasks are implicitly related by sharing parameters or optimizer. Inthis paper, we show that a meta-learner that explicitly relates tasks on agraph describing the relations of their output dimensions (e.g., classes) cansignificantly improve few shot learning. The graph's structure is usually freeor cheap to obtain but has rarely been explored in previous works. We develop anovel meta-learner of this type for prototype-based classification, in which aprototype is generated for each class, such that the nearest neighbor searchamong the prototypes produces an accurate classification. The meta-learner,called ""Gated Propagation Network (GPN)"", learns to propagate messages betweenprototypes of different classes on the graph, so that learning the prototype ofeach class benefits from the data of other related classes. In GPN, anattention mechanism aggregates messages from neighboring classes of each class,with a gate choosing between the aggregated message and the message from theclass itself. We train GPN on a sequence of tasks from many-shot to few shotgenerated by subgraph sampling. During training, it is able to reuse and updatepreviously achieved prototypes from the memory in a life-long learning cycle.In experiments, under different training-test discrepancy and test taskgeneration settings, GPN outperforms recent meta-learning methods on twobenchmark datasets. The code of GPN and dataset generation is available athttps://github.com/liulu112601/Gated-Propagation-Net."
How to trust unlabeled data? Instance Credibility Inference for Few-Shot  Learning,"['Yikai Wang', 'Li Zhang', 'Yuan Yao', 'Yanwei Fu']",http://arxiv.org/pdf/2007.08461v4.pdf,2020-07-15,['cs.cv'],"  Deep learning based models have excelled in many computer vision tasks andappear to surpass humans' performance. However, these models require anavalanche of expensive human labeled training data and many iterations to traintheir large number of parameters. This severely limits their scalability to thereal-world long-tail distributed categories, some of which are with a largenumber of instances, but with only a few manually annotated. Learning from suchextremely limited labeled examples is known as Few-shot learning (FSL).Different to prior arts that leverage meta-learning or data augmentationstrategies to alleviate this extremely data-scarce problem, this paper presentsa statistical approach, dubbed Instance Credibility Inference (ICI) to exploitthe support of unlabeled instances for few-shot visual recognition. Typically,we repurpose the self-taught learning paradigm to predict pseudo-labels ofunlabeled instances with an initial classifier trained from the few shot andthen select the most confident ones to augment the training set to re-train theclassifier. This is achieved by constructing a (Generalized) Linear Model(LM/GLM) with incidental parameters to model the mapping from (un-)labeledfeatures to their (pseudo-)labels, in which the sparsity of the incidentalparameters indicates the credibility of the corresponding pseudo-labeledinstance. We rank the credibility of pseudo-labeled instances along theregularization path of their corresponding incidental parameters, and the mosttrustworthy pseudo-labeled examples are preserved as the augmented labeledinstances. Theoretically, under mild conditions of restricted eigenvalue,irrepresentability, and large error, our approach is guaranteed to collect allthe correctly-predicted instances from the noisy pseudo-labeled set."
An Open-set Recognition and Few-Shot Learning Dataset for Audio Event  Classification in Domestic Environments,"['Javier Naranjo-Alcazar', 'Sergi Perez-Castanos', 'Pedro Zuccarrello', 'Ana M. Torres', 'Jose J. Lopez', 'Franscesc J. Ferri', 'Maximo Cobos']",http://arxiv.org/pdf/2002.11561v8.pdf,2020-02-26,"['cs.sd', 'cs.lg', 'eess.as']","  The problem of training with a small set of positive samples is known asfew-shot learning (FSL). It is widely known that traditional deep learning (DL)algorithms usually show very good performance when trained with large datasets.However, in many applications, it is not possible to obtain such a high numberof samples. In the image domain, typical FSL applications include those relatedto face recognition. In the audio domain, music fraud or speaker recognitioncan be clearly benefited from FSL methods. This paper deals with theapplication of FSL to the detection of specific and intentional acoustic eventsgiven by different types of sound alarms, such as door bells or fire alarms,using a limited number of samples. These sounds typically occur in domesticenvironments where many events corresponding to a wide variety of sound classestake place. Therefore, the detection of such alarms in a practical scenario canbe considered an open-set recognition (OSR) problem. To address the lack of adedicated public dataset for audio FSL, researchers usually make modificationson other available datasets. This paper is aimed at poviding the audiorecognition community with a carefully annotated dataset(https://zenodo.org/record/3689288) for FSL in an OSR context comprised of 1360clips from 34 classes divided into pattern sounds} and unwanted sounds. Tofacilitate and promote research on this area, results with state-of-the-artbaseline systems based on transfer learning are also presented."
Automatic Validation of Textual Attribute Values in E-commerce Catalog  by Learning with Limited Labeled Data,"['Yaqing Wang', 'Yifan Ethan Xu', 'Xian Li', 'Xin Luna Dong', 'Jing Gao']",http://arxiv.org/pdf/2006.08779v3.pdf,2020-06-15,"['cs.cl', 'cs.lg']","  Product catalogs are valuable resources for eCommerce website. In thecatalog, a product is associated with multiple attributes whose values areshort texts, such as product name, brand, functionality and flavor. Usuallyindividual retailers self-report these key values, and thus the cataloginformation unavoidably contains noisy facts. Although existing deep neuralnetwork models have shown success in conducting cross-checking between twopieces of texts, their success has to be dependent upon a large set of qualitylabeled data, which are hard to obtain in this validation task: products span avariety of categories. To address the aforementioned challenges, we propose anovel meta-learning latent variable approach, called MetaBridge, which canlearn transferable knowledge from a subset of categories with limited labeleddata and capture the uncertainty of never-seen categories with unlabeled data.More specifically, we make the following contributions. (1) We formalize theproblem of validating the textual attribute values of products from a varietyof categories as a natural language inference task in the few-shot learningsetting, and propose a meta-learning latent variable model to jointly processthe signals obtained from product profiles and textual attribute values. (2) Wepropose to integrate meta learning and latent variable in a unified model toeffectively capture the uncertainty of various categories. (3) We propose anovel objective function based on latent variable model in the few-shotlearning setting, which ensures distribution consistency between unlabeled andlabeled data and prevents overfitting by sampling from the learneddistribution. Extensive experiments on real eCommerce datasets from hundreds ofcategories demonstrate the effectiveness of MetaBridge on textual attributevalidation and its outstanding performance compared with state-of-the-artapproaches."
Deep Double-Side Learning Ensemble Model for Few-Shot Parkinson Speech  Recognition,"['Yongming Li', 'Lang Zhou', 'Lingyun Qin', 'Yuwei Zeng', 'Yuchuan Liu', 'Yan Lei', 'Pin Wang', 'Fan Li']",http://arxiv.org/pdf/2006.11593v1.pdf,2020-06-20,"['cs.cv', 'cs.lg']","  Diagnosis and therapeutic effect assessment of Parkinson disease based onvoice data are very important,but its few-shot learning problem ischallenging.Although deep learning is good at automatic feature extraction, itsuffers from few-shot learning problem. Therefore, the general effective methodis first conduct feature extraction based on prior knowledge, and then carryout feature reduction for subsequent classification. However, there are twomajor problems: 1) Structural information among speech features has not beenmined and new features of higher quality have not been reconstructed. 2)Structural information between data samples has not been mined and new sampleswith higher quality have not been reconstructed. To solve these two problems,based on the existing Parkinson speech feature data set, a deep double-sidelearning ensemble model is designed in this paper that can reconstruct speechfeatures and samples deeply and simultaneously. As to feature reconstruction,an embedded deep stacked group sparse auto-encoder is designed in this paper toconduct nonlinear feature transformation, so as to acquire new high-level deepfeatures, and then the deep features are fused with original speech features byL1 regularization feature selection method. As to speech sample reconstruction,a deep sample learning algorithm is designed in this paper based on iterativemean clustering to conduct samples transformation, so as to obtain newhigh-level deep samples. Finally, the bagging ensemble learning mode is adoptedto fuse the deep feature learning algorithm and the deep samples learningalgorithm together, thereby constructing a deep double-side learning ensemblemodel. At the end of this paper, two representative speech datasets ofParkinson's disease were used for verification. The experimental results showthat the proposed algorithm are effective."
Fuzzy Simplicial Networks: A Topology-Inspired Model to Improve Task  Generalization in Few-shot Learning,"['Henry Kvinge', 'Zachary New', 'Nico Courts', 'Jung H. Lee', 'Lauren A. Phillips', 'Courtney D. Corley', 'Aaron Tuor', 'Andrew Avila', 'Nathan O. Hodas']",http://arxiv.org/pdf/2009.11253v1.pdf,2020-09-23,"['cs.lg', 'cs.ai', 'cs.cv', 'math.gn', 'stat.ml']","  Deep learning has shown great success in settings with massive amounts ofdata but has struggled when data is limited. Few-shot learning algorithms,which seek to address this limitation, are designed to generalize well to newtasks with limited data. Typically, models are evaluated on unseen classes anddatasets that are defined by the same fundamental task as they are trained for(e.g. category membership). One can also ask how well a model can generalize tofundamentally different tasks within a fixed dataset (for example: moving fromcategory membership to tasks that involve detecting object orientation orquantity). To formalize this kind of shift we define a notion of ""independenceof tasks"" and identify three new sets of labels for established computer visiondatasets that test a model's ability to generalize to tasks which draw onorthogonal attributes in the data. We use these datasets to investigate thefailure modes of metric-based few-shot models. Based on our findings, weintroduce a new few-shot model called Fuzzy Simplicial Networks (FSN) whichleverages a construction from topology to more flexibly represent each classfrom limited data. In particular, FSN models can not only form multiplerepresentations for a given class but can also begin to capture thelow-dimensional structure which characterizes class manifolds in the encodedspace of deep networks. We show that FSN outperforms state-of-the-art models onthe challenging tasks we introduce in this paper while remaining competitive onstandard few-shot benchmarks."
Meta-learning with implicit gradients in a few-shot setting for medical  image segmentation,"['Rabindra Khadga', 'Debesh Jha', 'Steven Hicks', 'Vajira Thambawita', 'Michael A. Riegler', 'Sharib Ali', 'Pål Halvorsen']",http://arxiv.org/pdf/2106.03223v2.pdf,2021-06-06,['cs.cv'],"  Widely used traditional supervised deep learning methods require a largenumber of training samples but often fail to generalize on unseen datasets.Therefore, a more general application of any trained model is quite limited formedical imaging for clinical practice. Using separately trained models for eachunique lesion category or a unique patient population will require sufficientlylarge curated datasets, which is not practical to use in a real-world clinicalset-up. Few-shot learning approaches can not only minimize the need for anenormous number of reliable ground truth labels that are labour-intensive andexpensive but can also be used to model on a dataset coming from a newpopulation. To this end, we propose to exploit an optimization-based implicitmodel agnostic meta-learning (iMAML) algorithm under few-shot settings formedical image segmentation. Our approach can leverage the learned weights fromdiverse but small training samples to perform analysis on unseen datasets withhigh accuracy. We show that, unlike classical few-shot learning approaches, ourmethod improves generalization capability. To our knowledge, this is the firstwork that exploits iMAML for medical image segmentation and explores thestrength of the model on scenarios such as meta-training on unique and mixedinstances of lesion datasets. Our quantitative results on publicly availableskin and polyp datasets show that the proposed method outperforms the naivesupervised baseline model and two recent few-shot segmentation approaches bylarge margins. In addition, our iMAML approach shows an improvement of 2%-4% indice score compared to its counterpart MAML for most experiments."
Few-Shot Electronic Health Record Coding through Graph Contrastive  Learning,"['Shanshan Wang', 'Pengjie Ren', 'Zhumin Chen', 'Zhaochun Ren', 'Huasheng Liang', 'Qiang Yan', 'Evangelos Kanoulas', 'Maarten de Rijke']",http://arxiv.org/pdf/2106.15467v1.pdf,2021-06-29,"['cs.ai', 'cs.cl']","  Electronic health record (EHR) coding is the task of assigning ICD codes toeach EHR. Most previous studies either only focus on the frequent ICD codes ortreat rare and frequent ICD codes in the same way. These methods perform wellon frequent ICD codes but due to the extremely unbalanced distribution of ICDcodes, the performance on rare ones is far from satisfactory. We seek toimprove the performance for both frequent and rare ICD codes by using acontrastive graph-based EHR coding framework, CoGraph, which re-casts EHRcoding as a few-shot learning task. First, we construct a heterogeneous EHRword-entity (HEWE) graph for each EHR, where the words and entities extractedfrom an EHR serve as nodes and the relations between them serve as edges. Then,CoGraph learns similarities and dissimilarities between HEWE graphs fromdifferent ICD codes so that information can be transferred among them. In afew-shot learning scenario, the model only has access to frequent ICD codesduring training, which might force it to encode features that are useful forfrequent ICD codes only. To mitigate this risk, CoGraph devises two graphcontrastive learning schemes, GSCL and GECL, that exploit the HEWE graphstructures so as to encode transferable features. GSCL utilizes theintra-correlation of different sub-graphs sampled from HEWE graphs while GECLexploits the inter-correlation among HEWE graphs at different clinical stages.Experiments on the MIMIC-III benchmark dataset show that CoGraph significantlyoutperforms state-of-the-art methods on EHR coding, not only on frequent ICDcodes, but also on rare codes, in terms of several evaluation indicators. Onfrequent ICD codes, GSCL and GECL improve the classification accuracy and F1 by1.31% and 0.61%, respectively, and on rare ICD codes CoGraph has more obviousimprovements by 2.12% and 2.95%."
Few-Shot Learning with a Strong Teacher,"['Han-Jia Ye', 'Lu Ming', 'De-Chuan Zhan', 'Wei-Lun Chao']",http://arxiv.org/pdf/2107.00197v2.pdf,2021-07-01,"['cs.cv', 'cs.ai', 'cs.lg']","  Few-shot learning (FSL) aims to generate a classifier using limited labeledexamples. Many existing works take the meta-learning approach, constructing afew-shot learner that can learn from few-shot examples to generate aclassifier. Typically, the few-shot learner is constructed or meta-trained bysampling multiple few-shot tasks in turn and optimizing the few-shot learner'sperformance in generating classifiers for those tasks. The performance ismeasured by how well the resulting classifiers classify the test (i.e., query)examples of those tasks. In this paper, we point out two potential weaknessesof this approach. First, the sampled query examples may not provide sufficientsupervision for meta-training the few-shot learner. Second, the effectivenessof meta-learning diminishes sharply with the increasing number of shots. Toresolve these issues, we propose a novel meta-training objective for thefew-shot learner, which is to encourage the few-shot learner to generateclassifiers that perform like strong classifiers. Concretely, we associate eachsampled few-shot task with a strong classifier, which is trained with amplelabeled examples. The strong classifiers can be seen as the target classifiersthat we hope the few-shot learner to generate given few-shot examples, and weuse the strong classifiers to supervise the few-shot learner. We present anefficient way to construct the strong classifier, making our proposed objectivean easily plug-and-play term to existing meta-learning based FSL methods. Wevalidate our approach, LastShot, in combinations with many representativemeta-learning methods. On several benchmark datasets, our approach leads to anotable improvement across a variety of tasks. More importantly, with ourapproach, meta-learning based FSL methods can outperform non-meta-learningbased methods at different numbers of shots."
ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language  Understanding and Generation,"['Yu Sun', 'Shuohuan Wang', 'Shikun Feng', 'Siyu Ding', 'Chao Pang', 'Junyuan Shang', 'Jiaxiang Liu', 'Xuyi Chen', 'Yanbin Zhao', 'Yuxiang Lu', 'Weixin Liu', 'Zhihua Wu', 'Weibao Gong', 'Jianzhong Liang', 'Zhizhou Shang', 'Peng Sun', 'Wei Liu', 'Xuan Ouyang', 'Dianhai Yu', 'Hao Tian', 'Hua Wu', 'Haifeng Wang']",http://arxiv.org/pdf/2107.02137v1.pdf,2021-07-05,['cs.cl'],"  Pre-trained models have achieved state-of-the-art results in various NaturalLanguage Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shownthat scaling up pre-trained language models can improve their generalizationabilities. Particularly, the GPT-3 model with 175 billion parameters shows itsstrong task-agnostic zero-shot/few-shot learning capabilities. Despite theirsuccess, these large-scale models are trained on plain texts withoutintroducing knowledge such as linguistic knowledge and world knowledge. Inaddition, most large-scale models are trained in an auto-regressive way. As aresult, this kind of traditional fine-tuning approach demonstrates relativelyweak performance when solving downstream language understanding tasks. In orderto solve the above problems, we propose a unified framework named ERNIE 3.0 forpre-training large-scale knowledge enhanced models. It fuses auto-regressivenetwork and auto-encoding network, so that the trained model can be easilytailored for both natural language understanding and generation tasks withzero-shot learning, few-shot learning or fine-tuning. We trained the model with10 billion parameters on a 4TB corpus consisting of plain texts and alarge-scale knowledge graph. Empirical results show that the model outperformsthe state-of-the-art models on 54 Chinese NLP tasks, and its English versionachieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassingthe human performance by +0.8% (90.6% vs. 89.8%)."
Few-shot Learning with Global Relatedness Decoupled-Distillation,"['Yuan Zhou', 'Yanrong Guo', 'Shijie Hao', 'Richang Hong', 'Zhengjun Zha', 'Meng Wang']",http://arxiv.org/pdf/2107.05583v3.pdf,2021-07-12,['cs.cv'],"  Despite the success that metric learning based approaches have achieved infew-shot learning, recent works reveal the ineffectiveness of their episodictraining mode. In this paper, we point out two potential reasons for thisproblem: 1) the random episodic labels can only provide limited supervisioninformation, while the relatedness information between the query and supportsamples is not fully exploited; 2) the meta-learner is usually constrained bythe limited contextual information of the local episode. To overcome theseproblems, we propose a new Global Relatedness Decoupled-Distillation (GRDD)method using the global category knowledge and the RelatednessDecoupled-Distillation (RDD) strategy. Our GRDD learns new visual conceptsquickly by imitating the habit of humans, i.e. learning from the deep knowledgedistilled from the teacher. More specifically, we first train a global learneron the entire base subset using category labels as supervision to leverage theglobal context information of the categories. Then, the well-trained globallearner is used to simulate the query-support relatedness in globaldependencies. Finally, the distilled global query-support relatedness isexplicitly used to train the meta-learner using the RDD strategy, with the goalof making the meta-learner more discriminative. The RDD strategy aims todecouple the dense query-support relatedness into the groups of sparsedecoupled relatedness. Moreover, only the relatedness of a single supportsample with other query samples is considered in each group. By distilling thesparse decoupled relatedness group by group, sharper relatedness can beeffectively distilled to the meta-learner, thereby facilitating the learning ofa discriminative meta-learner. We conduct extensive experiments on theminiImagenet and CIFAR-FS datasets, which show the state-of-the-art performanceof our GRDD method."
MHFC: Multi-Head Feature Collaboration for Few-Shot Learning,"['Shuai Shao', 'Lei Xing', 'Yan Wang', 'Rui Xu', 'Chunyan Zhao', 'Yan-Jiang Wang', 'Bao-Di Liu']",http://arxiv.org/pdf/2109.07785v4.pdf,2021-09-16,['cs.cv'],"  Few-shot learning (FSL) aims to address the data-scarce problem. A standardFSL framework is composed of two components: (1) Pre-train. Employ the basedata to generate a CNN-based feature extraction model (FEM). (2) Meta-test.Apply the trained FEM to acquire the novel data's features and recognize them.FSL relies heavily on the design of the FEM. However, various FEMs havedistinct emphases. For example, several may focus more attention on the contourinformation, whereas others may lay particular emphasis on the textureinformation. The single-head feature is only a one-sided representation of thesample. Besides the negative influence of cross-domain (e.g., the trained FEMcan not adapt to the novel class flawlessly), the distribution of novel datamay have a certain degree of deviation compared with the ground truthdistribution, which is dubbed as distribution-shift-problem (DSP). To addressthe DSP, we propose Multi-Head Feature Collaboration (MHFC) algorithm, whichattempts to project the multi-head features (e.g., multiple features extractedfrom a variety of FEMs) to a unified space and fuse them to capture morediscriminative information. Typically, first, we introduce a subspace learningmethod to transform the multi-head features to aligned low-dimensionalrepresentations. It corrects the DSP via learning the feature with morepowerful discrimination and overcomes the problem of inconsistent measurementscales from different head features. Then, we design an attention block toupdate combination weights for each head feature automatically. Itcomprehensively considers the contribution of various perspectives and furtherimproves the discrimination of features. We evaluate the proposed method onfive benchmark datasets (including cross-domain experiments) and achievesignificant improvements of 2.1%-7.8% compared with state-of-the-arts."
UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding  with Text-to-Text Language Models,"['Tianbao Xie', 'Chen Henry Wu', 'Peng Shi', 'Ruiqi Zhong', 'Torsten Scholak', 'Michihiro Yasunaga', 'Chien-Sheng Wu', 'Ming Zhong', 'Pengcheng Yin', 'Sida I. Wang', 'Victor Zhong', 'Bailin Wang', 'Chengzu Li', 'Connor Boyle', 'Ansong Ni', 'Ziyu Yao', 'Dragomir Radev', 'Caiming Xiong', 'Lingpeng Kong', 'Rui Zhang', 'Noah A. Smith', 'Luke Zettlemoyer', 'Tao Yu']",http://arxiv.org/pdf/2201.05966v3.pdf,2022-01-16,['cs.cl'],"  Structured knowledge grounding (SKG) leverages structured knowledge tocomplete user requests, such as semantic parsing over databases and questionanswering over knowledge bases. Since the inputs and outputs of SKG tasks areheterogeneous, they have been studied separately by different communities,which limits systematic and compatible research on SKG. In this paper, weovercome this limitation by proposing the UnifiedSKG framework, which unifies21 SKG tasks into a text-to-text format, aiming to promote systematic SKGresearch, instead of being exclusive to a single task, domain, or dataset. Weuse UnifiedSKG to benchmark T5 with different sizes and show that T5, withsimple modifications when necessary, achieves state-of-the-art performance onalmost all of the 21 tasks. We further demonstrate that multi-taskprefix-tuning improves the performance on most tasks, largely improving theoverall performance. UnifiedSKG also facilitates the investigation of zero-shotand few-shot learning, and we show that T0, GPT-3, and Codex struggle inzero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct aseries of controlled experiments on structured knowledge encoding variantsacross SKG tasks. UnifiedSKG is easily extensible to more tasks, and it isopen-sourced at https://github.com/hkunlp/unifiedskg."
MetaDT: Meta Decision Tree with Class Hierarchy for Interpretable  Few-Shot Learning,"['Baoquan Zhang', 'Hao Jiang', 'Xutao Li', 'Shanshan Feng', 'Yunming Ye', 'Rui Ye']",http://arxiv.org/pdf/2203.01482v2.pdf,2022-03-03,"['cs.lg', 'cs.cv']","  Few-Shot Learning (FSL) is a challenging task, which aims to recognize novelclasses with few examples. Recently, lots of methods have been proposed fromthe perspective of meta-learning and representation learning. However, fewworks focus on the interpretability of FSL decision process. In this paper, wetake a step towards the interpretable FSL by proposing a novel meta-learningbased decision tree framework, namely, MetaDT. In particular, the FSLinterpretability is achieved from two aspects, i.e., a concept aspect and avisual aspect. On the concept aspect, we first introduce a tree-like concepthierarchy as FSL prior. Then, resorting to the prior, we split each few-shottask to a set of subtasks with different concept levels and then perform classprediction via a model of decision tree. The advantage of such design is that asequence of high-level concept decisions that lead up to a final classprediction can be obtained, which clarifies the FSL decision process. On thevisual aspect, a set of subtask-specific classifiers with visual attentionmechanism is designed to perform decision at each node of the decision tree. Asa result, a subtask-specific heatmap visualization can be obtained to achievethe decision interpretability of each tree node. At last, to alleviate the datascarcity issue of FSL, we regard the prior of concept hierarchy as anundirected graph, and then design a graph convolution-based decision treeinference network as our meta-learner to infer parameters of the decision tree.Extensive experiments on performance comparison and interpretability analysisshow superiority of our MetaDT."
Mask-guided Vision Transformer (MG-ViT) for Few-Shot Learning,"['Yuzhong Chen', 'Zhenxiang Xiao', 'Lin Zhao', 'Lu Zhang', 'Haixing Dai', 'David Weizhong Liu', 'Zihao Wu', 'Changhe Li', 'Tuo Zhang', 'Changying Li', 'Dajiang Zhu', 'Tianming Liu', 'Xi Jiang']",http://arxiv.org/pdf/2205.09995v1.pdf,2022-05-20,"['cs.cv', '68t10', 'i.4.0']","  Learning with little data is challenging but often inevitable in variousapplication scenarios where the labeled data is limited and costly. Recently,few-shot learning (FSL) gained increasing attention because of itsgeneralizability of prior knowledge to new tasks that contain only a fewsamples. However, for data-intensive models such as vision transformer (ViT),current fine-tuning based FSL approaches are inefficient in knowledgegeneralization and thus degenerate the downstream task performances. In thispaper, we propose a novel mask-guided vision transformer (MG-ViT) to achieve aneffective and efficient FSL on ViT model. The key idea is to apply a mask onimage patches to screen out the task-irrelevant ones and to guide the ViT tofocus on task-relevant and discriminative patches during FSL. Particularly,MG-ViT only introduces an additional mask operation and a residual connection,enabling the inheritance of parameters from pre-trained ViT without any othercost. To optimally select representative few-shot samples, we also include anactive learning based sample selection method to further improve thegeneralizability of MG-ViT based FSL. We evaluate the proposed MG-ViT on bothAgri-ImageNet classification task and ACFR apple detection task withgradient-weighted class activation mapping (Grad-CAM) as the mask. Theexperimental results show that the MG-ViT model significantly improves theperformance when compared with general fine-tuning based ViT models, providingnovel insights and a concrete approach towards generalizing data-intensive andlarge-scale deep learning models for FSL."
Flame-state monitoring based on very low number of visible or infrared  images via few-shot learning,"['Ruiyuan Kang', 'Panos Liatsis', 'Dimitrios C. Kyritsis']",http://arxiv.org/pdf/2210.07845v2.pdf,2022-10-14,"['cs.cv', 'physics.app-ph', '80a25, 68t30, 68t45', 'i.2.1; i.4.9; i.5.4']","  The current success of machine learning on image-based combustion monitoringis based on massive data, which is costly even impossible for industrialapplications. To address this conflict, we introduce few-shot learning in orderto achieve combustion monitoring and classification for the first time. Twoalgorithms, Siamese Network coupled with k Nearest Neighbors (SN-kNN) andPrototypical Network (PN), were tested. Rather than utilizing solely visibleimages as discussed in previous studies, we also used Infrared (IR) images. Weanalyzed the training process, test performance and inference speed of twoalgorithms on both image formats, and also used t-SNE to visualize learnedfeatures. The results demonstrated that both SN-kNN and PN were capable todistinguish flame states from learning with merely 20 images per flame state.The worst performance, which was realized by PN on IR images, still possessedprecision, accuracy, recall, and F1-score above 0.95. We showed that visibleimages demonstrated more substantial differences between classes and presentedmore consistent patterns inside the class, which made the training speed andmodel performance better compared to IR images. In contrast, the relatively lowquality of IR images made it difficult for PN to extract distinguishableprototypes, which caused relatively weak performance. With the entrire trainingset supporting classification, SN-kNN performed well with IR images. On theother hand, benefitting from the architecture design, PN has a much fasterspeed in training and inference than SN-kNN. The presented work analyzed thecharacteristics of both algorithms and image formats for the first time, thusproviding guidance for their future utilization in combustion monitoring tasks."
A Prompt-based Few-shot Learning Approach to Software Conflict Detection,"['Robert K. Helmeczi', 'Mucahit Cevik', 'Savas Yıldırım']",http://arxiv.org/pdf/2211.02709v1.pdf,2022-11-04,['cs.se'],"  A software requirement specification (SRS) document is an essential part ofthe software development life cycle which outlines the requirements that asoftware program in development must satisfy. This document is often specifiedby a diverse group of stakeholders and is subject to continual change, makingthe process of maintaining the document and detecting conflicts betweenrequirements an essential task in software development. Notably, projects thatdo not address conflicts in the SRS document early on face considerableproblems later in the development life cycle. These problems incur substantialcosts in terms of time and money, and these costs often become insurmountablebarriers that ultimately result in the termination of a software projectaltogether. As a result, early detection of SRS conflicts is critical toproject sustainability. The conflict detection task is approached in numerousways, many of which require a significant amount of manual intervention fromdevelopers, or require access to a large amount of labeled, task-specifictraining data. In this work, we propose using a prompt-based learning approachto perform few-shot learning for conflict detection. We compare our results tosupervised learning approaches that use pretrained language models, such asBERT and its variants. Our results show that prompting with just 32 labeledexamples can achieve a similar level of performance in many key metrics to thatof supervised learning on training sets that are magnitudes larger in size. Incontrast to many other conflict detection approaches, we make no assumptionsabout the type of underlying requirements, allowing us to analyze pairings ofboth functional and non-functional requirements. This allows us to omit thepotentially expensive task of filtering out non-functional requirements fromour dataset."
AuthentiSense: A Scalable Behavioral Biometrics Authentication Scheme  using Few-Shot Learning for Mobile Platforms,"['Hossein Fereidooni', 'Jan König', 'Phillip Rieger', 'Marco Chilese', 'Bora Gökbakan', 'Moritz Finke', 'Alexandra Dmitrienko', 'Ahmad-Reza Sadeghi']",http://arxiv.org/pdf/2302.02740v1.pdf,2023-02-06,['cs.cr'],"  Mobile applications are widely used for online services sharing a largeamount of personal data online. One-time authentication techniques such aspasswords and physiological biometrics (e.g., fingerprint, face, and iris) havetheir own advantages but also disadvantages since they can be stolen oremulated, and do not prevent access to the underlying device, once it isunlocked. To address these challenges, complementary authentication systemsbased on behavioural biometrics have emerged. The goal is to continuouslyprofile users based on their interaction with the mobile device. However,existing behavioural authentication schemes are not (i) user-agnostic meaningthat they cannot dynamically handle changes in the user-base without modelre-training, or (ii) do not scale well to authenticate millions of users.  In this paper, we present AuthentiSense, a user-agnostic, scalable, andefficient behavioural biometrics authentication system that enables continuousauthentication and utilizes only motion patterns (i.e., accelerometer,gyroscope and magnetometer data) while users interact with mobile apps. Ourapproach requires neither manually engineered features nor a significant amountof data for model training. We leverage a few-shot learning technique, calledSiamese network, to authenticate users at a large scale. We perform asystematic measurement study and report the impact of the parameters such asinteraction time needed for authentication and n-shot verification (comparisonwith enrollment samples) at the recognition stage. Remarkably, AuthentiSenseachieves high accuracy of up to 97% in terms of F1-score even when evaluated ina few-shot fashion that requires only a few behaviour samples per user (3shots). Our approach accurately authenticates users only after 1 second of userinteraction. For AuthentiSense, we report a FAR and FRR of 0.023 and 0.057,respectively."
SuSana Distancia is all you need: Enforcing class separability in metric  learning via two novel distance-based loss functions for few-shot image  classification,"['Mauricio Mendez-Ruiz', 'Jorge Gonzalez-Zapata', 'Ivan Reyes-Amezcua', 'Daniel Flores-Araiza', 'Francisco Lopez-Tiro', 'Andres Mendez-Vazquez', 'Gilberto Ochoa-Ruiz']",http://arxiv.org/pdf/2305.09062v3.pdf,2023-05-15,"['cs.cv', 'cs.ai']","  Few-shot learning is a challenging area of research that aims to learn newconcepts with only a few labeled samples of data. Recent works based onmetric-learning approaches leverage the meta-learning approach, which isencompassed by episodic tasks that make use a support (training) and query set(test) with the objective of learning a similarity comparison metric betweenthose sets. Due to the lack of data, the learning process of the embeddingnetwork becomes an important part of the few-shot task. Previous works haveaddressed this problem using metric learning approaches, but the properties ofthe underlying latent space and the separability of the difference classes onit was not entirely enforced. In this work, we propose two different lossfunctions which consider the importance of the embedding vectors by looking atthe intra-class and inter-class distance between the few data. The first lossfunction is the Proto-Triplet Loss, which is based on the original triplet losswith the modifications needed to better work on few-shot scenarios. The secondloss function, which we dub ICNN loss is based on an inter and intra classnearest neighbors score, which help us to assess the quality of embeddingsobtained from the trained network. Our results, obtained from a extensiveexperimental setup show a significant improvement in accuracy in theminiImagenNet benchmark compared to other metric-based few-shot learningmethods by a margin of 2%, demonstrating the capability of these loss functionsto allow the network to generalize better to previously unseen classes. In ourexperiments, we demonstrate competitive generalization capabilities to otherdomains, such as the Caltech CUB, Dogs and Cars datasets compared with thestate of the art."
Compact Bilinear Pooling,"['Yang Gao', 'Oscar Beijbom', 'Ning Zhang', 'Trevor Darrell']",http://arxiv.org/pdf/1511.06062v2.pdf,2015-11-19,['cs.cv'],"  Bilinear models has been shown to achieve impressive performance on a widerange of visual tasks, such as semantic segmentation, fine grained recognitionand face recognition. However, bilinear features are high dimensional,typically on the order of hundreds of thousands to a few million, which makesthem impractical for subsequent analysis. We propose two compact bilinearrepresentations with the same discriminative power as the full bilinearrepresentation but with only a few thousand dimensions. Our compactrepresentations allow back-propagation of classification errors enabling anend-to-end optimization of the visual recognition system. The compact bilinearrepresentations are derived through a novel kernelized analysis of bilinearpooling which provide insights into the discriminative power of bilinearpooling, and a platform for further research in compact pooling methods.Experimentation illustrate the utility of the proposed representations forimage classification and few-shot learning across several datasets."
LCNN: Lookup-based Convolutional Neural Network,"['Hessam Bagherinezhad', 'Mohammad Rastegari', 'Ali Farhadi']",http://arxiv.org/pdf/1611.06473v2.pdf,2016-11-20,['cs.cv'],"  Porting state of the art deep learning algorithms to resource constrainedcompute platforms (e.g. VR, AR, wearables) is extremely challenging. We proposea fast, compact, and accurate model for convolutional neural networks thatenables efficient learning and inference. We introduce LCNN, a lookup-basedconvolutional neural network that encodes convolutions by few lookups to adictionary that is trained to cover the space of weights in CNNs. Training LCNNinvolves jointly learning a dictionary and a small set of linear combinations.The size of the dictionary naturally traces a spectrum of trade-offs betweenefficiency and accuracy. Our experimental results on ImageNet challenge showthat LCNN can offer 3.2x speedup while achieving 55.1% top-1 accuracy usingAlexNet architecture. Our fastest LCNN offers 37.6x speed up over AlexNet whilemaintaining 44.3% top-1 accuracy. LCNN not only offers dramatic speed ups atinference, but it also enables efficient training. In this paper, we show thebenefits of LCNN in few-shot learning and few-iteration learning, two crucialaspects of on-device training of deep learning models."
Few-Shot Image Recognition by Predicting Parameters from Activations,"['Siyuan Qiao', 'Chenxi Liu', 'Wei Shen', 'Alan Yuille']",http://arxiv.org/pdf/1706.03466v3.pdf,2017-06-12,['cs.cv'],"  In this paper, we are interested in the few-shot learning problem. Inparticular, we focus on a challenging scenario where the number of categoriesis large and the number of examples per novel category is very limited, e.g. 1,2, or 3. Motivated by the close relationship between the parameters and theactivations in a neural network associated with the same category, we propose anovel method that can adapt a pre-trained neural network to novel categories bydirectly predicting the parameters from the activations. Zero training isrequired in adaptation to novel categories, and fast inference is realized by asingle forward pass. We evaluate our method by doing few-shot image recognitionon the ImageNet dataset, which achieves the state-of-the-art classificationaccuracy on novel categories by a significant margin while keeping comparableperformance on the large-scale categories. We also test our method on theMiniImageNet dataset and it strongly outperforms the previous state-of-the-artmethods."
Labeled Memory Networks for Online Model Adaptation,"['Shiv Shankar', 'Sunita Sarawagi']",http://arxiv.org/pdf/1707.01461v3.pdf,2017-07-05,"['cs.lg', 'stat.ml']","  Augmenting a neural network with memory that can grow without growing thenumber of trained parameters is a recent powerful concept with many excitingapplications. We propose a design of memory augmented neural networks (MANNs)called Labeled Memory Networks (LMNs) suited for tasks requiring onlineadaptation in classification models. LMNs organize the memory with classes asthe primary key.The memory acts as a second boosted stage following a regularneural network thereby allowing the memory and the primary network to playcomplementary roles. Unlike existing MANNs that write to memory for everyinstance and use LRU based memory replacement, LMNs write only for instanceswith non-zero loss and use label-based memory replacement. We demonstratesignificant accuracy gains on various tasks including word-modelling andfew-shot learning. In this paper, we establish their potential in onlineadapting a batch trained neural network to domain-relevant labeled data atdeployment time. We show that LMNs are better than other MANNs designed formeta-learning. We also found them to be more accurate and faster thanstate-of-the-art methods of retuning model parameters for adapting todomain-specific labeled data."
Gaussian Prototypical Networks for Few-Shot Learning on Omniglot,['Stanislav Fort'],http://arxiv.org/pdf/1708.02735v1.pdf,2017-08-09,"['cs.lg', 'cs.cv', 'cs.ne', 'stat.ml']","  We propose a novel architecture for $k$-shot classification on the Omniglotdataset. Building on prototypical networks, we extend their architecture towhat we call Gaussian prototypical networks. Prototypical networks learn a mapbetween images and embedding vectors, and use their clustering forclassification. In our model, a part of the encoder output is interpreted as aconfidence region estimate about the embedding point, and expressed as aGaussian covariance matrix. Our network then constructs a direction and classdependent distance metric on the embedding space, using uncertainties ofindividual data points as weights. We show that Gaussian prototypical networksare a preferred architecture over vanilla prototypical networks with anequivalent number of parameters. We report state-of-the-art performance in1-shot and 5-shot classification both in 5-way and 20-way regime (for 5-shot5-way, we are comparable to previous state-of-the-art) on the Omniglot dataset.We explore artificially down-sampling a fraction of images in the training set,which improves our performance even further. We therefore hypothesize thatGaussian prototypical networks might perform better in less homogeneous,noisier datasets, which are commonplace in real world applications."
Neural Task Programming: Learning to Generalize Across Hierarchical  Tasks,"['Danfei Xu', 'Suraj Nair', 'Yuke Zhu', 'Julian Gao', 'Animesh Garg', 'Li Fei-Fei', 'Silvio Savarese']",http://arxiv.org/pdf/1710.01813v2.pdf,2017-10-04,"['cs.ai', 'cs.lg', 'cs.ro']","  In this work, we propose a novel robot learning framework called Neural TaskProgramming (NTP), which bridges the idea of few-shot learning fromdemonstration and neural program induction. NTP takes as input a taskspecification (e.g., video demonstration of a task) and recursively decomposesit into finer sub-task specifications. These specifications are fed to ahierarchical neural program, where bottom-level programs are callablesubroutines that interact with the environment. We validate our method in threerobot manipulation tasks. NTP achieves strong generalization across sequentialtasks that exhibit hierarchal and compositional structures. The experimentalresults show that NTP learns to generalize well to- wards unseen tasks withincreasing lengths, variable topologies, and changing objectives."
Few-Shot Adversarial Domain Adaptation,"['Saeid Motiian', 'Quinn Jones', 'Seyed Mehdi Iranmanesh', 'Gianfranco Doretto']",http://arxiv.org/pdf/1711.02536v1.pdf,2017-11-05,['cs.cv'],"  This work provides a framework for addressing the problem of superviseddomain adaptation with deep models. The main idea is to exploit adversariallearning to learn an embedded subspace that simultaneously maximizes theconfusion between two domains while semantically aligning their embedding. Thesupervised setting becomes attractive especially when there are only a fewtarget data samples that need to be labeled. In this few-shot learningscenario, alignment and separation of semantic probability distributions isdifficult because of the lack of data. We found that by carefully designing atraining scheme whereby the typical binary adversarial discriminator isaugmented to distinguish between four different classes, it is possible toeffectively address the supervised adaptation problem. In addition, theapproach has a high speed of adaptation, i.e. it requires an extremely lownumber of labeled target training samples, even one per category can beeffective. We then extensively compare this approach to the state of the art indomain adaptation in two experiments: one using datasets for handwritten digitrecognition, and one using datasets for visual object recognition."
A Bridge Between Hyperparameter Optimization and Learning-to-learn,"['Luca Franceschi', 'Michele Donini', 'Paolo Frasconi', 'Massimiliano Pontil']",http://arxiv.org/pdf/1712.06283v3.pdf,2017-12-18,"['stat.ml', 'cs.lg']","  We consider a class of a nested optimization problems involving inner andouter objectives. We observe that by taking into explicit account theoptimization dynamics for the inner objective it is possible to derive ageneral framework that unifies gradient-based hyperparameter optimization andmeta-learning (or learning-to-learn). Depending on the specific setting, thevariables of the outer objective take either the meaning of hyperparameters ina supervised learning problem or parameters of a meta-learner. We show thatsome recently proposed methods in the latter setting can be instantiated in ourframework and tackled with the same gradient-based algorithms. Finally, wediscuss possible design patterns for learning-to-learn and present encouragingpreliminary experiments for few-shot learning."
Few-shot learning of neural networks from scratch by pseudo example  optimization,"['Akisato Kimura', 'Zoubin Ghahramani', 'Koh Takeuchi', 'Tomoharu Iwata', 'Naonori Ueda']",http://arxiv.org/pdf/1802.03039v3.pdf,2018-02-08,"['stat.ml', 'cs.lg', 'cs.ne']","  In this paper, we propose a simple but effective method for training neuralnetworks with a limited amount of training data. Our approach inherits the ideaof knowledge distillation that transfers knowledge from a deep or widereference model to a shallow or narrow target model. The proposed methodemploys this idea to mimic predictions of reference estimators that are morerobust against overfitting than the network we want to train. Different fromalmost all the previous work for knowledge distillation that requires a largeamount of labeled training data, the proposed method requires only a smallamount of training data. Instead, we introduce pseudo training examples thatare optimized as a part of model parameters. Experimental results for severalbenchmark datasets demonstrate that the proposed method outperformed all theother baselines, such as naive training of the target model and standardknowledge distillation."
BRUNO: A Deep Recurrent Model for Exchangeable Data,"['Iryna Korshunova', 'Jonas Degrave', 'Ferenc Huszár', 'Yarin Gal', 'Arthur Gretton', 'Joni Dambre']",http://arxiv.org/pdf/1802.07535v3.pdf,2018-02-21,['stat.ml'],"  We present a novel model architecture which leverages deep learning tools toperform exact Bayesian inference on sets of high dimensional, complexobservations. Our model is provably exchangeable, meaning that the jointdistribution over observations is invariant under permutation: this propertylies at the heart of Bayesian inference. The model does not require variationalapproximations to train, and new samples can be generated conditional onprevious samples, with cost linear in the size of the conditioning set. Theadvantages of our architecture are demonstrated on learning tasks that requiregeneralisation from short observed sequences while modelling sequencevariability, such as conditional image generation, few-shot learning, andanomaly detection."
Meta-Learner with Linear Nulling,"['Sung Whan Yoon', 'Jun Seo', 'Jaekyun Moon']",http://arxiv.org/pdf/1806.01010v3.pdf,2018-06-04,"['cs.lg', 'stat.ml']","  We propose a meta-learning algorithm utilizing a linear transformer thatcarries out null-space projection of neural network outputs. The main idea isto construct an alternative classification space such that the error signalsduring few-shot learning are quickly zero-forced on that space so that reliableclassification on low data is possible. The final decision on a query isobtained utilizing a null-space-projected distance measure between the networkoutput and reference vectors, both of which have been trained in the initiallearning phase. Among the known methods with a given model size, ourmeta-learner achieves the best or near-best image classification accuracieswith Omniglot and miniImageNet datasets."
Bilevel Programming for Hyperparameter Optimization and Meta-Learning,"['Luca Franceschi', 'Paolo Frasconi', 'Saverio Salzo', 'Riccardo Grazzi', 'Massimilano Pontil']",http://arxiv.org/pdf/1806.04910v2.pdf,2018-06-13,"['stat.ml', 'cs.lg']","  We introduce a framework based on bilevel programming that unifiesgradient-based hyperparameter optimization and meta-learning. We show that anapproximate version of the bilevel problem can be solved by taking intoexplicit account the optimization dynamics for the inner objective. Dependingon the specific setting, the outer variables take either the meaning ofhyperparameters in a supervised learning problem or parameters of ameta-learner. We provide sufficient conditions under which solutions of theapproximate problem converge to those of the exact problem. We instantiate ourapproach for meta-learning in the case of deep learning where representationlayers are treated as hyperparameters shared across a set of training episodes.In experiments, we confirm our theoretical findings, present encouragingresults for few-shot learning and contrast the bilevel approach againstclassical approaches for learning-to-learn."
Uncertainty in Multitask Transfer Learning,"['Alexandre Lacoste', 'Boris Oreshkin', 'Wonchang Chung', 'Thomas Boquet', 'Negar Rostamzadeh', 'David Krueger']",http://arxiv.org/pdf/1806.07528v3.pdf,2018-06-20,"['stat.ml', 'cs.lg']","  Using variational Bayes neural networks, we develop an algorithm capable ofaccumulating knowledge into a prior from multiple different tasks. The resultis a rich and meaningful prior capable of few-shot learning on new tasks. Theposterior can go beyond the mean field approximation and yields gooduncertainty on the performed experiments. Analysis on toy tasks shows that itcan learn from significantly different tasks while finding similarities amongthem. Experiments of Mini-Imagenet yields the new state of the art with 74.5%accuracy on 5 shot learning. Finally, we provide experiments showing that otherexisting methods can fail to perform well in different benchmarks."
One-shot Learning for iEEG Seizure Detection Using End-to-end Binary  Operations: Local Binary Patterns with Hyperdimensional Computing,"['Alessio Burrello', 'Kaspar Schindler', 'Luca Benini', 'Abbas Rahimi']",http://arxiv.org/pdf/1809.01926v1.pdf,2018-09-06,"['eess.sp', 'cs.lg', 'q-bio.nc', 'stat.ml']","  This paper presents an efficient binarized algorithm for both learning andclassification of human epileptic seizures from intracranialelectroencephalography (iEEG). The algorithm combines local binary patternswith brain-inspired hyperdimensional computing to enable end-to-end learningand inference with binary operations. The algorithm first transforms iEEG timeseries from each electrode into local binary pattern codes. Then atomichigh-dimensional binary vectors are used to construct composite representationsof seizures across all electrodes. For the majority of our patients (10 out of16), the algorithm quickly learns from one or two seizures (i.e., one-/few-shotlearning) and perfectly generalizes on 27 further seizures. For other patients,the algorithm requires three to six seizures for learning. Overall, ouralgorithm surpasses the state-of-the-art methods for detecting 65 novelseizures with higher specificity and sensitivity, and lower memory footprint."
Hierarchy-based Image Embeddings for Semantic Image Retrieval,"['Björn Barz', 'Joachim Denzler']",http://arxiv.org/pdf/1809.09924v4.pdf,2018-09-26,"['cs.cv', 'cs.ai', 'cs.ir', 'cs.lg']","  Deep neural networks trained for classification have been found to learnpowerful image representations, which are also often used for other tasks suchas comparing images w.r.t. their visual similarity. However, visual similaritydoes not imply semantic similarity. In order to learn semanticallydiscriminative features, we propose to map images onto class embeddings whosepair-wise dot products correspond to a measure of semantic similarity betweenclasses. Such an embedding does not only improve image retrieval results, butcould also facilitate integrating semantics for other tasks, e.g., noveltydetection or few-shot learning. We introduce a deterministic algorithm forcomputing the class centroids directly based on prior world-knowledge encodedin a hierarchy of classes such as WordNet. Experiments on CIFAR-100, NABirds,and ImageNet show that our learned semantic image embeddings improve thesemantic consistency of image retrieval results by a large margin."
Open-Ended Content-Style Recombination Via Leakage Filtering,"['Karl Ridgeway', 'Michael C. Mozer']",http://arxiv.org/pdf/1810.00110v1.pdf,2018-09-28,"['cs.lg', 'stat.ml']","  We consider visual domains in which a class label specifies the content of animage, and class-irrelevant properties that differentiate instances constitutethe style. We present a domain-independent method that permits the open-endedrecombination of style of one image with the content of another. Open endedsimply means that the method generalizes to style and content not present inthe training data. The method starts by constructing a content embedding usingan existing deep metric-learning technique. This trained content encoder isincorporated into a variational autoencoder (VAE), paired with a to-be-trainedstyle encoder. The VAE reconstruction loss alone is inadequate to ensure adecomposition of the latent representation into style and content. Our methodthus includes an auxiliary loss, leakage filtering, which ensures that no styleinformation remaining in the content representation is used for reconstructionand vice versa. We synthesize novel images by decoding the style representationobtained from one image with the content representation from another. Usingthis method for data-set augmentation, we obtain state-of-the-art performanceon few-shot learning tasks."
Gaussian Process Conditional Density Estimation,"['Vincent Dutordoir', 'Hugh Salimbeni', 'Marc Deisenroth', 'James Hensman']",http://arxiv.org/pdf/1810.12750v1.pdf,2018-10-30,"['stat.ml', 'cs.lg']","  Conditional Density Estimation (CDE) models deal with estimating conditionaldistributions. The conditions imposed on the distribution are the inputs of themodel. CDE is a challenging task as there is a fundamental trade-off betweenmodel complexity, representational capacity and overfitting. In this work, wepropose to extend the model's input with latent variables and use Gaussianprocesses (GP) to map this augmented input onto samples from the conditionaldistribution. Our Bayesian approach allows for the modeling of small datasets,but we also provide the machinery for it to be applied to big data usingstochastic variational inference. Our approach can be used to model densitieseven in sparse data regions, and allows for sharing learned structure betweenconditions. We illustrate the effectiveness and wide-reaching applicability ofour model on a variety of real-world problems, such as spatio-temporal densityestimation of taxi drop-offs, non-Gaussian noise modeling, and few-shotlearning on omniglot images."
Learning Cross-Lingual Sentence Representations via a Multi-task  Dual-Encoder Model,"['Muthuraman Chidambaram', 'Yinfei Yang', 'Daniel Cer', 'Steve Yuan', 'Yun-Hsuan Sung', 'Brian Strope', 'Ray Kurzweil']",http://arxiv.org/pdf/1810.12836v4.pdf,2018-10-30,['cs.cl'],"  A significant roadblock in multilingual neural language modeling is the lackof labeled non-English data. One potential method for overcoming this issue islearning cross-lingual text representations that can be used to transfer theperformance from training on English tasks to non-English tasks, despite littleto no task-specific non-English data. In this paper, we explore a natural setupfor learning cross-lingual sentence representations: the dual-encoder. Weprovide a comprehensive evaluation of our cross-lingual representations on anumber of monolingual, cross-lingual, and zero-shot/few-shot learning tasks,and also give an analysis of different learned cross-lingual embedding spaces."
Few-shot Learning for Named Entity Recognition in Medical Text,"['Maximilian Hofer', 'Andrey Kormilitzin', 'Paul Goldberg', 'Alejo Nevado-Holgado']",http://arxiv.org/pdf/1811.05468v1.pdf,2018-11-13,"['cs.cl', 'cs.lg', 'stat.ml']","  Deep neural network models have recently achieved state-of-the-artperformance gains in a variety of natural language processing (NLP) tasks(Young, Hazarika, Poria, & Cambria, 2017). However, these gains rely on theavailability of large amounts of annotated examples, without whichstate-of-the-art performance is rarely achievable. This is especiallyinconvenient for the many NLP fields where annotated examples are scarce, suchas medical text. To improve NLP models in this situation, we evaluate fiveimprovements on named entity recognition (NER) tasks when only ten annotatedexamples are available: (1) layer-wise initialization with pre-trained weights,(2) hyperparameter tuning, (3) combining pre-training data, (4) custom wordembeddings, and (5) optimizing out-of-vocabulary (OOV) words. Experimentalresults show that the F1 score of 69.3% achievable by state-of-the-art modelscan be improved to 78.87%."
RelationNet2: Deep Comparison Columns for Few-Shot Learning,"['Xueting Zhang', 'Yuting Qiang', 'Flood Sung', 'Yongxin Yang', 'Timothy M. Hospedales']",http://arxiv.org/pdf/1811.07100v3.pdf,2018-11-17,['cs.cv'],"  Few-shot deep learning is a topical challenge area for scaling visualrecognition to open ended growth of unseen new classes with limited labeledexamples. A promising approach is based on metric learning, which trains a deepembedding to support image similarity matching. Our insight is that effectivegeneral purpose matching requires non-linear comparison of features at multipleabstraction levels. We thus propose a new deep comparison network comprised ofembedding and relation modules that learn multiple non-linear distance metricsbased on different levels of features simultaneously. Furthermore, to reduceover-fitting and enable the use of deeper embeddings, we represent images asdistributions rather than vectors via learning parameterized Gaussian noiseregularization. The resulting network achieves excellent performance on bothminiImageNet and tieredImageNet."
Unsupervised Meta-Learning For Few-Shot Image Classification,"['Siavash Khodadadeh', 'Ladislau Bölöni', 'Mubarak Shah']",http://arxiv.org/pdf/1811.11819v2.pdf,2018-11-28,"['cs.cv', 'cs.lg']","  Few-shot or one-shot learning of classifiers requires a significant inductivebias towards the type of task to be learned. One way to acquire this is bymeta-learning on tasks similar to the target task. In this paper, we proposeUMTRA, an algorithm that performs unsupervised, model-agnostic meta-learningfor classification tasks. The meta-learning step of UMTRA is performed on aflat collection of unlabeled images. While we assume that these images can begrouped into a diverse set of classes and are relevant to the target task, noexplicit information about the classes or any labels are needed. UMTRA usesrandom sampling and augmentation to create synthetic training tasks formeta-learning phase. Labels are only needed at the final target task learningstep, and they can be as little as one sample per class. On the Omniglot andMini-Imagenet few-shot learning benchmarks, UMTRA outperforms every testedapproach based on unsupervised learning of representations, while alternatingfor the best performance with the recent CACTUs algorithm. Compared tosupervised model-agnostic meta-learning approaches, UMTRA trades off someclassification accuracy for a reduction in the required labels of severalorders of magnitude."
Learning to match transient sound events using attentional similarity  for few-shot sound recognition,"['Szu-Yu Chou', 'Kai-Hsiang Cheng', 'Jyh-Shing Roger Jang', 'Yi-Hsuan Yang']",http://arxiv.org/pdf/1812.01269v2.pdf,2018-12-04,"['cs.sd', 'eess.as']","  In this paper, we introduce a novel attentional similarity module for theproblem of few-shot sound recognition. Given a few examples of an unseen soundevent, a classifier must be quickly adapted to recognize the new sound eventwithout much fine-tuning. The proposed attentional similarity module can beplugged into any metric-based learning method for few-shot learning, allowingthe resulting model to especially match related short sound events. Extensiveexperiments on two datasets shows that the proposed module consistentlyimproves the performance of five different metric-based learning methods forfew-shot sound recognition. The relative improvement ranges from +4.1% to +7.7%for 5-shot 5-way accuracy for the ESC-50 dataset, and from +2.1% to +6.5% fornoiseESC-50. Qualitative results demonstrate that our method contributes inparticular to the recognition of transient sound events."
The effects of negative adaptation in Model-Agnostic Meta-Learning,"['Tristan Deleu', 'Yoshua Bengio']",http://arxiv.org/pdf/1812.02159v1.pdf,2018-12-05,"['cs.lg', 'stat.ml']","  The capacity of meta-learning algorithms to quickly adapt to a variety oftasks, including ones they did not experience during meta-training, has been akey factor in the recent success of these methods on few-shot learningproblems. This particular advantage of using meta-learning over standardsupervised or reinforcement learning is only well founded under the assumptionthat the adaptation phase does improve the performance of our model on the taskof interest. However, in the classical framework of meta-learning, thisconstraint is only mildly enforced, if not at all, and we only see animprovement on average over a distribution of tasks. In this paper, we showthat the adaptation in an algorithm like MAML can significantly decrease theperformance of an agent in a meta-reinforcement learning setting, even on arange of meta-training tasks."
Reconciling meta-learning and continual learning with online mixtures of  tasks,"['Ghassen Jerfel', 'Erin Grant', 'Thomas L. Griffiths', 'Katherine Heller']",http://arxiv.org/pdf/1812.06080v3.pdf,2018-12-14,"['cs.lg', 'stat.ml']","  Learning-to-learn or meta-learning leverages data-driven inductive bias toincrease the efficiency of learning on a novel task. This approach encountersdifficulty when transfer is not advantageous, for instance, when tasks areconsiderably dissimilar or change over time. We use the connection betweengradient-based meta-learning and hierarchical Bayes to propose a Dirichletprocess mixture of hierarchical Bayesian models over the parameters of anarbitrary parametric model such as a neural network. In contrast toconsolidating inductive biases into a single set of hyperparameters, ourapproach of task-dependent hyperparameter selection better handles latentdistribution shift, as demonstrated on a set of evolving, image-based, few-shotlearning benchmarks."
Low-Shot Learning from Imaginary 3D Model,"['Frederik Pahde', 'Mihai Puscas', 'Jannik Wolff', 'Tassilo Klein', 'Nicu Sebe', 'Moin Nabi']",http://arxiv.org/pdf/1901.01868v1.pdf,2019-01-04,"['cs.cv', 'cs.lg']","  Since the advent of deep learning, neural networks have demonstratedremarkable results in many visual recognition tasks, constantly pushing thelimits. However, the state-of-the-art approaches are largely unsuitable inscarce data regimes. To address this shortcoming, this paper proposes employinga 3D model, which is derived from training images. Such a model can then beused to hallucinate novel viewpoints and poses for the scarce samples of thefew-shot learning scenario. A self-paced learning approach allows for theselection of a diverse set of high-quality images, which facilitates thetraining of a classifier. The performance of the proposed approach is showcasedon the fine-grained CUB-200-2011 dataset in a few-shot setting andsignificantly improves our baseline accuracy."
FIGR: Few-shot Image Generation with Reptile,"['Louis Clouâtre', 'Marc Demers']",http://arxiv.org/pdf/1901.02199v1.pdf,2019-01-08,"['cs.lg', 'cs.cv', 'stat.ml']","  Generative Adversarial Networks (GAN) boast impressive capacity to generaterealistic images. However, like much of the field of deep learning, theyrequire an inordinate amount of data to produce results, thereby limiting theirusefulness in generating novelty. In the same vein, recent advances inmeta-learning have opened the door to many few-shot learning applications. Inthe present work, we propose Few-shot Image Generation using Reptile (FIGR), aGAN meta-trained with Reptile. Our model successfully generates novel images onboth MNIST and Omniglot with as little as 4 images from an unseen class. Wefurther contribute FIGR-8, a new dataset for few-shot image generation, whichcontains 1,548,944 icons categorized in over 18,409 classes. Trained on FIGR-8,initial results show that our model can generalize to more advanced concepts(such as ""bird"" and ""knife"") from as few as 8 samples from a previously unseenclass of images and as little as 10 training steps through those 8 images. Thiswork demonstrates the potential of training a GAN for few-shot image generationand aims to set a new benchmark for future work in the domain."
"Learning Classifiers for Domain Adaptation, Zero and Few-Shot  Recognition Based on Learning Latent Semantic Parts","['Pengkai Zhu', 'Hanxiao Wang', 'Venkatesh Saligrama']",http://arxiv.org/pdf/1901.09079v3.pdf,2019-01-25,['cs.cv'],"  In computer vision applications, such as domain adaptation (DA), few shotlearning (FSL) and zero-shot learning (ZSL), we encounter new objects andenvironments, for which insufficient examples exist to allow for training""models from scratch,"" and methods that adapt existing models, trained on thepresented training environment, to the new scenario are required. We propose anovel visual attribute encoding method that encodes each image as alow-dimensional probability vector composed of prototypical part-typeprobabilities. The prototypes are learnt to be representative of all trainingdata. At test-time we utilize this encoding as an input to a classifier. Attest-time we freeze the encoder and only learn/adapt the classifier componentto limited annotated labels in FSL; new semantic attributes in ZSL. We conductextensive experiments on benchmark datasets. Our method outperformsstate-of-art methods trained for the specific contexts (ZSL, FSL, DA)."
CANet: Class-Agnostic Segmentation Networks with Iterative Refinement  and Attentive Few-Shot Learning,"['Chi Zhang', 'Guosheng Lin', 'Fayao Liu', 'Rui Yao', 'Chunhua Shen']",http://arxiv.org/pdf/1903.02351v1.pdf,2019-03-06,['cs.cv'],"  Recent progress in semantic segmentation is driven by deep ConvolutionalNeural Networks and large-scale labeled image datasets. However, data labelingfor pixel-wise segmentation is tedious and costly. Moreover, a trained modelcan only make predictions within a set of pre-defined classes. In this paper,we present CANet, a class-agnostic segmentation network that performs few-shotsegmentation on new classes with only a few annotated images available. Ournetwork consists of a two-branch dense comparison module which performsmulti-level feature comparison between the support image and the query image,and an iterative optimization module which iteratively refines the predictedresults. Furthermore, we introduce an attention mechanism to effectively fuseinformation from multiple support examples under the setting of k-shotlearning. Experiments on PASCAL VOC 2012 show that our method achieves a meanIntersection-over-Union score of 55.4% for 1-shot segmentation and 57.1% for5-shot segmentation, outperforming state-of-the-art methods by a large marginof 14.6% and 13.2%, respectively."
f-VAEGAN-D2: A Feature Generating Framework for Any-Shot Learning,"['Yongqin Xian', 'Saurabh Sharma', 'Bernt Schiele', 'Zeynep Akata']",http://arxiv.org/pdf/1903.10132v1.pdf,2019-03-25,['cs.cv'],"  When labeled training data is scarce, a promising data augmentation approachis to generate visual features of unknown classes using their attributes. Tolearn the class conditional distribution of CNN features, these models rely onpairs of image features and class attributes. Hence, they can not make use ofthe abundance of unlabeled data samples. In this paper, we tackle any-shotlearning problems i.e. zero-shot and few-shot, in a unified feature generatingframework that operates in both inductive and transductive learning settings.We develop a conditional generative model that combines the strength of VAE andGANs and in addition, via an unconditional discriminator, learns the marginalfeature distribution of unlabeled images. We empirically show that our modellearns highly discriminative CNN features for five datasets, i.e. CUB, SUN, AWAand ImageNet, and establish a new state-of-the-art in any-shot learning, i.e.inductive and transductive (generalized) zero- and few-shot learning settings.We also demonstrate that our learned features are interpretable: we visualizethem by inverting them back to the pixel space and we explain them bygenerating textual arguments of why they are associated with a certain label."
Learning from Adversarial Features for Few-Shot Classification,"['Wei Shen', 'Ziqiang Shi', 'Jun Sun']",http://arxiv.org/pdf/1903.10225v1.pdf,2019-03-25,['cs.cv'],"  Many recent few-shot learning methods concentrate on designing novel modelarchitectures. In this paper, we instead show that with a simple backboneconvolutional network we can even surpass state-of-the-art classificationaccuracy. The essential part that contributes to this superior performance isan adversarial feature learning strategy that improves the generalizationcapability of our model. In this work, adversarial features are those featuresthat can cause the classifier uncertain about its prediction. In order togenerate adversarial features, we firstly locate adversarial regions based onthe derivative of the entropy with respect to an averaging mask. Then we usethe adversarial region attention to aggregate the feature maps to obtain theadversarial features. In this way, we can explore and exploit the entirespatial area of the feature maps to mine more diverse discriminative knowledge.We perform extensive model evaluations and analyses on miniImageNet andtieredImageNet datasets demonstrating the effectiveness of the proposed method."
Diversity with Cooperation: Ensemble Methods for Few-Shot Classification,"['Nikita Dvornik', 'Cordelia Schmid', 'Julien Mairal']",http://arxiv.org/pdf/1903.11341v2.pdf,2019-03-27,"['cs.cv', 'cs.ai']","  Few-shot classification consists of learning a predictive model that is ableto effectively adapt to a new class, given only a few annotated samples. Tosolve this challenging problem, meta-learning has become a popular paradigmthat advocates the ability to ""learn to adapt"". Recent works have shown,however, that simple learning strategies without meta-learning could becompetitive. In this paper, we go a step further and show that by addressingthe fundamental high-variance issue of few-shot learning classifiers, it ispossible to significantly outperform current meta-learning techniques. Ourapproach consists of designing an ensemble of deep networks to leverage thevariance of the classifiers, and introducing new strategies to encourage thenetworks to cooperate, while encouraging prediction diversity. Evaluation isconducted on the mini-ImageNet and CUB datasets, where we show that even asingle network obtained by distillation yields state-of-the-art results."
LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning,"['Huaiyu Li', 'Weiming Dong', 'Xing Mei', 'Chongyang Ma', 'Feiyue Huang', 'Bao-Gang Hu']",http://arxiv.org/pdf/1905.06331v1.pdf,2019-05-15,"['cs.lg', 'stat.ml']","  In this work, we propose a novel meta-learning approach for few-shotclassification, which learns transferable prior knowledge across tasks anddirectly produces network parameters for similar unseen tasks with trainingsamples. Our approach, called LGM-Net, includes two key modules, namely,TargetNet and MetaNet. The TargetNet module is a neural network for solving aspecific task and the MetaNet module aims at learning to generate functionalweights for TargetNet by observing training samples. We also present anintertask normalization strategy for the training process to leverage commoninformation shared across different tasks. The experimental results on Omniglotand miniImageNet datasets demonstrate that LGM-Net can effectively adapt tosimilar unseen tasks and achieve competitive performance, and the results onsynthetic datasets show that transferable prior knowledge is learned by theMetaNet module via mapping training data to functional weights. LGM-Net enablesfast learning and adaptation since no further tuning steps are requiredcompared to other meta-learning approaches."
A Neural-Symbolic Architecture for Inverse Graphics Improved by Lifelong  Meta-Learning,"['Michael Kissner', 'Helmut Mayer']",http://arxiv.org/pdf/1905.08910v2.pdf,2019-05-22,['cs.cv'],"  We follow the idea of formulating vision as inverse graphics and propose anew type of element for this task, a neural-symbolic capsule. It is capable ofde-rendering a scene into semantic information feed-forward, as well asrendering it feed-backward. An initial set of capsules for graphical primitivesis obtained from a generative grammar and connected into a full capsulenetwork. Lifelong meta-learning continuously improves this network's detectioncapabilities by adding capsules for new and more complex objects it detects ina scene using few-shot learning. Preliminary results demonstrate the potentialof our novel approach."
Constellation Loss: Improving the efficiency of deep metric learning  loss functions for optimal embedding,"['Alfonso Medela', 'Artzai Picon']",http://arxiv.org/pdf/1905.10675v1.pdf,2019-05-25,"['cs.lg', 'cs.cv']","  Metric learning has become an attractive field for research on the latestyears. Loss functions like contrastive loss, triplet loss or multi-class N-pairloss have made possible generating models capable of tackling complex scenarioswith the presence of many classes and scarcity on the number of images perclass not only work to build classifiers, but to many other applications wheremeasuring similarity is the key. Deep Neural Networks trained via metriclearning also offer the possibility to solve few-shot learning problems.Currently used state of the art loss functions such as triplet and contrastiveloss functions, still suffer from slow convergence due to the selection ofeffective training samples that has been partially solved by the multi-classN-pair loss by simultaneously adding additional samples from the differentclasses. In this work, we extend triplet and multiclass-N-pair loss function byproposing the constellation loss metric where the distances among all classcombinations are simultaneously learned. We have compared our constellationloss for visual class embedding showing that our loss function over-performsthe other methods by obtaining more compact clusters while achieving betterclassification results."
A Plug-in Method for Representation Factorization in Connectionist  Models,"['Jee Seok Yoon', 'Myung-Cheol Roh', 'Heung-Il Suk']",http://arxiv.org/pdf/1905.11088v4.pdf,2019-05-27,"['cs.lg', 'stat.ml']","  In this article, we focus on decomposing latent representations in generativeadversarial networks or learned feature representations in deep autoencodersinto semantically controllable factors in a semisupervised manner, withoutmodifying the original trained models. Particularly, we propose factors'decomposer-entangler network (FDEN) that learns to decompose a latentrepresentation into mutually independent factors. Given a latentrepresentation, the proposed framework draws a set of interpretable factors,each aligned to independent factors of variations by minimizing their totalcorrelation in an information-theoretic means. As a plug-in method, we haveapplied our proposed FDEN to the existing networks of adversarially learnedinference and pioneer network and performed computer vision tasks ofimage-to-image translation in semantic ways, e.g., changing styles, whilekeeping the identity of a subject, and object classification in a few-shotlearning scheme. We have also validated the effectiveness of the proposedmethod with various ablation studies in the qualitative, quantitative, andstatistical examination."
Adaptive Deep Kernel Learning,"['Prudencio Tossou', 'Basile Dura', 'Francois Laviolette', 'Mario Marchand', 'Alexandre Lacoste']",http://arxiv.org/pdf/1905.12131v2.pdf,2019-05-28,"['cs.lg', 'stat.ml']","  Deep kernel learning provides an elegant and principled framework forcombining the structural properties of deep learning algorithms with theflexibility of kernel methods. By means of a deep neural network, we learn aparametrized kernel operator that can be combined with a differentiable kernelalgorithm during inference. While previous work within this framework hasfocused on learning a single kernel for large datasets, we learn a kernelfamily for a variety of few-shot regression tasks. Compared to single deepkernel learning, our algorithm enables the identification of the appropriatekernel for each task during inference. As such, it is well adapted for complextask distributions in a few-shot learning setting, which we demonstrate bycomparing against existing state-of-the-art algorithms using real-world,few-shot regression tasks related to the field of drug discovery."
Regression Networks for Meta-Learning Few-Shot Classification,"['Arnout Devos', 'Matthias Grossglauser']",http://arxiv.org/pdf/1905.13613v2.pdf,2019-05-31,"['cs.lg', 'cs.cv', 'stat.ml']","  We propose regression networks for the problem of few-shot classification,where a classifier must generalize to new classes not seen in the training set,given only a small number of examples of each class. In high dimensionalembedding spaces the direction of data generally contains richer informationthan magnitude. Next to this, state-of-the-art few-shot metric methods thatcompare distances with aggregated class representations, have shown superiorperformance. Combining these two insights, we propose to meta-learnclassification of embedded points by regressing the closest approximation inevery class subspace while using the regression error as a distance metric.Similarly to recent approaches for few-shot learning, regression networksreflect a simple inductive bias that is beneficial in this limited-data regimeand they achieve excellent results, especially when more aggregate classrepresentations can be formed with multiple shots."
Sequential Scenario-Specific Meta Learner for Online Recommendation,"['Zhengxiao Du', 'Xiaowei Wang', 'Hongxia Yang', 'Jingren Zhou', 'Jie Tang']",http://arxiv.org/pdf/1906.00391v1.pdf,2019-06-02,"['cs.ir', 'cs.lg', 'stat.ml']","  Cold-start problems are long-standing challenges for practicalrecommendations. Most existing recommendation algorithms rely on extensiveobserved data and are brittle to recommendation scenarios with fewinteractions. This paper addresses such problems using few-shot learning andmeta learning. Our approach is based on the insight that having a goodgeneralization from a few examples relies on both a generic modelinitialization and an effective strategy for adapting this model to newlyarising tasks. To accomplish this, we combine the scenario-specific learningwith a model-agnostic sequential meta-learning and unify them into anintegrated end-to-end framework, namely Scenario-specific Sequential Metalearner (or s^2 meta). By doing so, our meta-learner produces a generic initialmodel through aggregating contextual information from a variety of predictiontasks while effectively adapting to specific tasks by leveraginglearning-to-learn knowledge. Extensive experiments on various real-worlddatasets demonstrate that our proposed model can achieve significant gains overthe state-of-the-arts for cold-start problems in online recommendation.Deployment is at the Guess You Like session, the front page of the MobileTaobao."
Active Object Manipulation Facilitates Visual Object Learning: An  Egocentric Vision Study,"['Satoshi Tsutsui', 'Dian Zhi', 'Md Alimoor Reza', 'David Crandall', 'Chen Yu']",http://arxiv.org/pdf/1906.01415v1.pdf,2019-06-04,['cs.cv'],"  Inspired by the remarkable ability of the infant visual learning system, arecent study collected first-person images from children to analyze the`training data' that they receive. We conduct a follow-up study thatinvestigates two additional directions. First, given that infants can quicklylearn to recognize a new object without much supervision (i.e. few-shotlearning), we limit the number of training images. Second, we investigate howchildren control the supervision signals they receive during learning based onhand manipulation of objects. Our experimental results suggest that supervisionwith hand manipulation is better than without hands, and the trend isconsistent even when a small number of images is available."
Adaptive Gradient-Based Meta-Learning Methods,"['Mikhail Khodak', 'Maria-Florina Balcan', 'Ameet Talwalkar']",http://arxiv.org/pdf/1906.02717v3.pdf,2019-06-06,"['cs.lg', 'cs.ai', 'stat.ml']","  We build a theoretical framework for designing and understanding practicalmeta-learning methods that integrates sophisticated formalizations oftask-similarity with the extensive literature on online convex optimization andsequential prediction algorithms. Our approach enables the task-similarity tobe learned adaptively, provides sharper transfer-risk bounds in the setting ofstatistical learning-to-learn, and leads to straightforward derivations ofaverage-case regret bounds for efficient algorithms in settings where thetask-environment changes dynamically or the tasks share a certain geometricstructure. We use our theory to modify several popular meta-learning algorithmsand improve their meta-test-time performance on standard problems in few-shotlearning and federated learning."
Evolving Losses for Unlabeled Video Representation Learning,"['AJ Piergiovanni', 'Anelia Angelova', 'Michael S. Ryoo']",http://arxiv.org/pdf/1906.03248v1.pdf,2019-06-07,['cs.cv'],"  We present a new method to learn video representations from unlabeled data.Given large-scale unlabeled video data, the objective is to benefit from suchdata by learning a generic and transferable representation space that can bedirectly used for a new task such as zero/few-shot learning. We formulate ourunsupervised representation learning as a multi-modal, multi-task learningproblem, where the representations are also shared across different modalitiesvia distillation. Further, we also introduce the concept of finding a betterloss function to train such multi-task multi-modal representation space usingan evolutionary algorithm; our method automatically searches over differentcombinations of loss functions capturing multiple (self-supervised) tasks andmodalities. Our formulation allows for the distillation of audio, optical flowand temporal information into a single, RGB-based convolutional neural network.We also compare the effects of using additional unlabeled video data andevaluate our representation learning on standard public video datasets."
Few-Shot Point Cloud Region Annotation with Human in the Loop,"['Siddhant Jain', 'Sowmya Munukutla', 'David Held']",http://arxiv.org/pdf/1906.04409v1.pdf,2019-06-11,"['cs.cv', 'cs.lg']","  We propose a point cloud annotation framework that employs human-in-looplearning to enable the creation of large point cloud datasets with per-pointannotations. Sparse labels from a human annotator are iteratively propagated togenerate a full segmentation of the network by fine-tuning a pre-trained modelof an allied task via a few-shot learning paradigm. We show that the proposedframework significantly reduces the amount of human interaction needed inannotating point clouds, without sacrificing on the quality of the annotations.Our experiments also suggest the suitability of the framework in annotatinglarge datasets by noting a reduction in human interaction as the number of fullannotations completed by the system increases. Finally, we demonstrate theflexibility of the framework to support multiple different annotations of thesame point cloud enabling the creation of datasets with different granularitiesof annotation."
Learning to Forget for Meta-Learning,"['Sungyong Baik', 'Seokil Hong', 'Kyoung Mu Lee']",http://arxiv.org/pdf/1906.05895v2.pdf,2019-06-13,"['cs.lg', 'cs.cv', 'stat.ml']","  Few-shot learning is a challenging problem where the goal is to achievegeneralization from only few examples. Model-agnostic meta-learning (MAML)tackles the problem by formulating prior knowledge as a common initializationacross tasks, which is then used to quickly adapt to unseen tasks. However,forcibly sharing an initialization can lead to conflicts among tasks and thecompromised (undesired by tasks) location on optimization landscape, therebyhindering the task adaptation. Further, we observe that the degree of conflictdiffers among not only tasks but also layers of a neural network. Thus, wepropose task-and-layer-wise attenuation on the compromised initialization toreduce its influence. As the attenuation dynamically controls (or selectivelyforgets) the influence of prior knowledge for a given task and each layer, wename our method as L2F (Learn to Forget). The experimental results demonstratethat the proposed method provides faster adaptation and greatly improves theperformance. Furthermore, L2F can be easily applied and improve otherstate-of-the-art MAML-based frameworks, illustrating its simplicity andgeneralizability."
Fast and Flexible Multi-Task Classification Using Conditional Neural  Adaptive Processes,"['James Requeima', 'Jonathan Gordon', 'John Bronskill', 'Sebastian Nowozin', 'Richard E. Turner']",http://arxiv.org/pdf/1906.07697v2.pdf,2019-06-18,"['stat.ml', 'cs.lg']","  The goal of this paper is to design image classification systems that, afteran initial multi-task training phase, can automatically adapt to new tasksencountered at test time. We introduce a conditional neural process basedapproach to the multi-task classification setting for this purpose, andestablish connections to the meta-learning and few-shot learning literature.The resulting approach, called CNAPs, comprises a classifier whose parametersare modulated by an adaptation network that takes the current task's dataset asinput. We demonstrate that CNAPs achieves state-of-the-art results on thechallenging Meta-Dataset benchmark indicating high-quality transfer-learning.We show that the approach is robust, avoiding both over-fitting in low-shotregimes and under-fitting in high-shot regimes. Timing experiments reveal thatCNAPs is computationally efficient at test-time as it does not involve gradientbased adaptation. Finally, we show that trained models are immediatelydeployable to continual learning and active learning where they can outperformexisting approaches that do not leverage transfer learning."
Neural Stored-program Memory,"['Hung Le', 'Truyen Tran', 'Svetha Venkatesh']",http://arxiv.org/pdf/1906.08862v2.pdf,2019-05-25,"['cs.ne', 'cs.lg', 'stat.ml']","  Neural networks powered with external memory simulate computer behaviors.These models, which use the memory to store data for a neural controller, canlearn algorithms and other complex tasks. In this paper, we introduce a newmemory to store weights for the controller, analogous to the stored-programmemory in modern computer architectures. The proposed model, dubbed NeuralStored-program Memory, augments current memory-augmented neural networks,creating differentiable machines that can switch programs through time, adaptto variable contexts and thus resemble the Universal Turing Machine. A widerange of experiments demonstrate that the resulting machines not only excel inclassical algorithmic problems, but also have potential for compositional,continual, few-shot learning and question-answering tasks."
An Empirical Study of Batch Normalization and Group Normalization in  Conditional Computation,"['Vincent Michalski', 'Vikram Voleti', 'Samira Ebrahimi Kahou', 'Anthony Ortiz', 'Pascal Vincent', 'Chris Pal', 'Doina Precup']",http://arxiv.org/pdf/1908.00061v1.pdf,2019-07-31,"['cs.cv', 'cs.lg']","  Batch normalization has been widely used to improve optimization in deepneural networks. While the uncertainty in batch statistics can act as aregularizer, using these dataset statistics specific to the training setimpairs generalization in certain tasks. Recently, alternative methods fornormalizing feature activations in neural networks have been proposed. Amongthem, group normalization has been shown to yield similar, in some domains evensuperior performance to batch normalization. All these methods utilize alearned affine transformation after the normalization operation to increaserepresentational power. Methods used in conditional computation define theparameters of these transformations as learnable functions of conditioninginformation. In this work, we study whether and where the conditionalformulation of group normalization can improve generalization compared toconditional batch normalization. We evaluate performances on the tasks ofvisual question answering, few-shot learning, and conditional image generation."
Meta Reasoning over Knowledge Graphs,"['Hong Wang', 'Wenhan Xiong', 'Mo Yu', 'Xiaoxiao Guo', 'Shiyu Chang', 'William Yang Wang']",http://arxiv.org/pdf/1908.04877v1.pdf,2019-08-13,['cs.cl'],"  The ability to reason over learned knowledge is an innate ability for humansand humans can easily master new reasoning rules with only a fewdemonstrations. While most existing studies on knowledge graph (KG) reasoningassume enough training examples, we study the challenging and practical problemof few-shot knowledge graph reasoning under the paradigm of meta-learning. Wepropose a new meta learning framework that effectively utilizes thetask-specific meta information such as local graph neighbors and reasoningpaths in KGs. Specifically, we design a meta-encoder that encodes the metainformation into task-specific initialization parameters for different tasks.This allows our reasoning module to have diverse starting points when learningto reason over different relations, which is expected to better fit the targettask. On two few-shot knowledge base completion benchmarks, we show that theaugmented task-specific meta-encoder yields much better initial point than MAMLand outperforms several few-shot learning baselines."
Memory-Based Neighbourhood Embedding for Visual Recognition,"['Suichan Li', 'Dapeng Chen', 'Bin Liu', 'Nenghai Yu', 'Rui Zhao']",http://arxiv.org/pdf/1908.04992v1.pdf,2019-08-14,"['cs.cv', 'cs.lg']","  Learning discriminative image feature embeddings is of great importance tovisual recognition. To achieve better feature embeddings, most current methodsfocus on designing different network structures or loss functions, and theestimated feature embeddings are usually only related to the input images. Inthis paper, we propose Memory-based Neighbourhood Embedding (MNE) to enhance ageneral CNN feature by considering its neighbourhood. The method aims to solvetwo critical problems, i.e., how to acquire more relevant neighbours in thenetwork training and how to aggregate the neighbourhood information for a morediscriminative embedding. We first augment an episodic memory module into thenetwork, which can provide more relevant neighbours for both training andtesting. Then the neighbours are organized in a tree graph with the targetinstance as the root node. The neighbourhood information is graduallyaggregated to the root node in a bottom-up manner, and aggregation weights aresupervised by the class relationships between the nodes. We apply MNE on imagesearch and few shot learning tasks. Extensive ablation studies demonstrate theeffectiveness of each component, and our method significantly outperforms thestate-of-the-art approaches."
When Low Resource NLP Meets Unsupervised Language Model:  Meta-pretraining Then Meta-learning for Few-shot Text Classification,"['Shumin Deng', 'Ningyu Zhang', 'Zhanlin Sun', 'Jiaoyan Chen', 'Huajun Chen']",http://arxiv.org/pdf/1908.08788v2.pdf,2019-08-22,"['cs.ir', 'cs.cl']","  Text classification tends to be difficult when data are deficient or when itis required to adapt to unseen classes. In such challenging scenarios, recentstudies have often used meta-learning to simulate the few-shot task, thusnegating implicit common linguistic features across tasks. This paper addressessuch problems using meta-learning and unsupervised language models. Ourapproach is based on the insight that having a good generalization from a fewexamples relies on both a generic model initialization and an effectivestrategy for adapting this model to newly arising tasks. We show that ourapproach is not only simple but also produces a state-of-the-art performance ona well-studied sentiment classification dataset. It can thus be furthersuggested that pretraining could be a promising solution for few-shot learningof many other NLP tasks. The code and the dataset to replicate the experimentsare made available at https://github.com/zxlzr/FewShotNLP."
Domain-Agnostic Few-Shot Classification by Learning Disparate Modulators,"['Yongseok Choi', 'Junyoung Park', 'Subin Yi', 'Dong-Yeon Cho']",http://arxiv.org/pdf/1909.04999v2.pdf,2019-09-11,"['cs.lg', 'cs.cv', 'stat.ml']","  Although few-shot learning research has advanced rapidly with the help ofmeta-learning, its practical usefulness is still limited because most of themassumed that all meta-training and meta-testing examples came from a singledomain. We propose a simple but effective way for few-shot classification inwhich a task distribution spans multiple domains including ones never seenduring meta-training. The key idea is to build a pool of models to cover thiswide task distribution and learn to select the best one for a particular taskthrough cross-domain meta-learning. All models in the pool share a base networkwhile each model has a separate modulator to refine the base network in its ownway. This framework allows the pool to have representational diversity withoutlosing beneficial domain-invariant features. We verify the effectiveness of theproposed algorithm through experiments on various datasets across diversedomains."
Differentially Private Meta-Learning,"['Jeffrey Li', 'Mikhail Khodak', 'Sebastian Caldas', 'Ameet Talwalkar']",http://arxiv.org/pdf/1909.05830v2.pdf,2019-09-12,"['cs.lg', 'cs.ai', 'cs.cr', 'stat.ml']","  Parameter-transfer is a well-known and versatile approach for meta-learning,with applications including few-shot learning, federated learning, andreinforcement learning. However, parameter-transfer algorithms often requiresharing models that have been trained on the samples from specific tasks, thusleaving the task-owners susceptible to breaches of privacy. We conduct thefirst formal study of privacy in this setting and formalize the notion oftask-global differential privacy as a practical relaxation of more commonlystudied threat models. We then propose a new differentially private algorithmfor gradient-based parameter transfer that not only satisfies this privacyrequirement but also retains provable transfer learning guarantees in convexsettings. Empirically, we apply our analysis to the problems of federatedlearning with personalization and few-shot classification, showing thatallowing the relaxation to task-global privacy from the more commonly studiednotion of local privacy leads to dramatically increased performance inrecurrent neural language modeling and image classification."
Teaching Pretrained Models with Commonsense Reasoning: A Preliminary  KB-Based Approach,"['Shiyang Li', 'Jianshu Chen', 'Dian Yu']",http://arxiv.org/pdf/1909.09743v2.pdf,2019-09-20,"['cs.ai', 'cs.lg']","  Recently, pretrained language models (e.g., BERT) have achieved great successon many downstream natural language understanding tasks and exhibit a certainlevel of commonsense reasoning ability. However, their performance oncommonsense tasks is still far from that of humans. As a preliminary attempt,we propose a simple yet effective method to teach pretrained models withcommonsense reasoning by leveraging the structured knowledge in ConceptNet, thelargest commonsense knowledge base (KB). Specifically, the structured knowledgein KB allows us to construct various logical forms, and then generatemultiple-choice questions requiring commonsense logical reasoning. Experimentalresults demonstrate that, when refined on these training examples, thepretrained models consistently improve their performance on tasks that requirecommonsense reasoning, especially in the few-shot learning setting. Besides, wealso perform analysis to understand which logical relations are more relevantto commonsense reasoning."
Decoder Choice Network for Meta-Learning,"['Jialin Liu', 'Fei Chao', 'Longzhi Yang', 'Chih-Min Lin', 'Qiang Shen']",http://arxiv.org/pdf/1909.11446v2.pdf,2019-09-25,"['cs.lg', 'stat.ml']","  Meta-learning has been widely used for implementing few-shot learning andfast model adaptation. One kind of meta-learning methods attempt to learn howto control the gradient descent process in order to make the gradient-basedlearning have high speed and generalization. This work proposes a method thatcontrols the gradient descent process of the model parameters of a neuralnetwork by limiting the model parameters in a low-dimensional latent space. Themain challenge of this idea is that a decoder with too many parameters isrequired. This work designs a decoder with typical structure and shares a partof weights in the decoder to reduce the number of the required parameters.Besides, this work has introduced ensemble learning to work with the proposedapproach for improving performance. The results show that the proposed approachis witnessed by the superior performance over the Omniglot classification andthe miniImageNet classification tasks."
Stochastic Prototype Embeddings,"['Tyler R. Scott', 'Karl Ridgeway', 'Michael C. Mozer']",http://arxiv.org/pdf/1909.11702v1.pdf,2019-09-25,"['stat.ml', 'cs.lg']","  Supervised deep-embedding methods project inputs of a domain to arepresentational space in which same-class instances lie near one another anddifferent-class instances lie far apart. We propose a probabilistic method thattreats embeddings as random variables. Extending a state-of-the-artdeterministic method, Prototypical Networks (Snell et al., 2017), our approachsupposes the existence of a class prototype around which class instances areGaussian distributed. The prototype posterior is a product distribution overlabeled instances, and query instances are classified by marginalizing relativeprototype proximity over embedding uncertainty. We describe an efficientsampler for approximate inference that allows us to train the model at roughlythe same space and time cost as its deterministic sibling. Incorporatinguncertainty improves performance on few-shot learning and gracefully handleslabel noise and out-of-distribution inputs. Compared to the state-of-the-artstochastic method, Hedged Instance Embeddings (Oh et al., 2019), we achievesuperior large- and open-set classification accuracy. Our method also alignsclass-discriminating features with the axes of the embedding space, yielding aninterpretable, disentangled representation."
Meta-learning algorithms for Few-Shot Computer Vision,['Etienne Bennequin'],http://arxiv.org/pdf/1909.13579v1.pdf,2019-09-30,"['cs.cv', 'cs.lg']","  Few-Shot Learning is the challenge of training a model with only a smallamount of data. Many solutions to this problem use meta-learning algorithms,i.e. algorithms that learn to learn. By sampling few-shot tasks from a largerdataset, we can teach these algorithms to solve new, unseen tasks. Thisdocument reports my work on meta-learning algorithms for Few-Shot ComputerVision. This work was done during my internship at Sicara, a French companybuilding image recognition solutions for businesses. It contains: 1. anextensive review of the state-of-the-art in few-shot computer vision; 2. abenchmark of meta-learning algorithms for few-shot image classification; 3. theintroduction to a novel meta-learning algorithm for few-shot object detection,which is still in development."
Bad Form: Comparing Context-Based and Form-Based Few-Shot Learning in  Distributional Semantic Models,"['Jeroen Van Hautte', 'Guy Emerson', 'Marek Rei']",http://arxiv.org/pdf/1910.00275v1.pdf,2019-10-01,"['cs.cl', 'cs.lg']","  Word embeddings are an essential component in a wide range of naturallanguage processing applications. However, distributional semantic models areknown to struggle when only a small number of context sentences are available.Several methods have been proposed to obtain higher-quality vectors for thesewords, leveraging both this context information and sometimes the word formsthemselves through a hybrid approach. We show that the current tasks do notsuffice to evaluate models that use word-form information, as such models caneasily leverage word forms in the training data that are related to word formsin the test data. We introduce 3 new tasks, allowing for a more balancedcomparison between models. Furthermore, we show that hyperparameters that havelargely been ignored in previous work can consistently improve the performanceof both baseline and advanced models, achieving a new state of the art on 4 outof 6 tasks."
Graph convolutional networks for learning with few clean and many noisy  labels,"['Ahmet Iscen', 'Giorgos Tolias', 'Yannis Avrithis', 'Ondrej Chum', 'Cordelia Schmid']",http://arxiv.org/pdf/1910.00324v3.pdf,2019-10-01,"['cs.cv', 'cs.lg']","  In this work we consider the problem of learning a classifier from noisylabels when a few clean labeled examples are given. The structure of clean andnoisy data is modeled by a graph per class and Graph Convolutional Networks(GCN) are used to predict class relevance of noisy examples. For each class,the GCN is treated as a binary classifier, which learns to discriminate cleanfrom noisy examples using a weighted binary cross-entropy loss function. TheGCN-inferred ""clean"" probability is then exploited as a relevance measure. Eachnoisy example is weighted by its relevance when learning a classifier for theend task. We evaluate our method on an extended version of a few-shot learningproblem, where the few clean examples of novel classes are supplemented withadditional noisy data. Experimental results show that our GCN-based cleaningprocess significantly improves the classification accuracy over not cleaningthe noisy data, as well as standard few-shot classification where only fewclean examples are used."
Few-Shot Abstract Visual Reasoning With Spectral Features,"['Tanner Bohn', 'Yining Hu', 'Charles X. Ling']",http://arxiv.org/pdf/1910.01833v1.pdf,2019-10-04,"['cs.lg', 'cs.cv', 'stat.ml']","  We present an image preprocessing technique capable of improving theperformance of few-shot classifiers on abstract visual reasoning tasks. Manyvisual reasoning tasks with abstract features are easy for humans to learn withfew examples but very difficult for computer vision approaches with the samenumber of samples, despite the ability for deep learning models to learnabstract features. Same-different (SD) problems represent a type of visualreasoning task requiring knowledge of pattern repetition within individualimages, and modern computer vision approaches have largely faltered on theseclassification problems, even when provided with vast amounts of training data.We propose a simple method for solving these problems based on the insight thatremoving peaks from the amplitude spectrum of an image is capable ofemphasizing the unique parts of the image. When combined with severalclassifiers, our method performs well on the SD SVRT tasks with few-shotlearning, improving upon the best comparable results on all tasks, with averageabsolute accuracy increases nearly 40% for some classifiers. In particular, wefind that combining Relational Networks with this image preprocessing approachimproves their performance from chance-level to over 90% accuracy on several SDtasks."
Compositional Generalization for Primitive Substitutions,"['Yuanpeng Li', 'Liang Zhao', 'Jianyu Wang', 'Joel Hestness']",http://arxiv.org/pdf/1910.02612v1.pdf,2019-10-07,"['cs.cl', 'cs.lg']","  Compositional generalization is a basic mechanism in human language learning,but current neural networks lack such ability. In this paper, we conductfundamental research for encoding compositionality in neural networks.Conventional methods use a single representation for the input sentence, makingit hard to apply prior knowledge of compositionality. In contrast, our approachleverages such knowledge with two representations, one generating attentionmaps, and the other mapping attended input words to output symbols. We reducethe entropy in each representation to improve generalization. Our experimentsdemonstrate significant improvements over the conventional methods in five NLPtasks including instruction learning and machine translation. In the SCANdomain, it boosts accuracies from 14.0% to 98.8% in Jump task, and from 92.0%to 99.7% in TurnLeft task. It also beats human performance on a few-shotlearning task. We hope the proposed approach can help ease future researchtowards human-level compositional language learning."
Semi Few-Shot Attribute Translation,"['Ricard Durall', 'Franz-Josef Pfreundt', 'Janis Keuper']",http://arxiv.org/pdf/1910.03240v2.pdf,2019-10-08,['cs.cv'],"  Recent studies have shown remarkable success in image-to-image translationfor attribute transfer applications. However, most of existing approaches arebased on deep learning and require an abundant amount of labeled data toproduce good results, therefore limiting their applicability. In the same vein,recent advances in meta-learning have led to successful implementations withlimited available data, allowing so-called few-shot learning.  In this paper, we address this limitation of supervised methods, by proposinga novel approach based on GANs. These are trained in a meta-training manner,which allows them to perform image-to-image translations using just a fewlabeled samples from a new target class. This work empirically demonstrates thepotential of training a GAN for few shot image-to-image translation on haircolor attribute synthesis tasks, opening the door to further research ongenerative transfer learning."
Neural Similarity Learning,"['Weiyang Liu', 'Zhen Liu', 'James M. Rehg', 'Le Song']",http://arxiv.org/pdf/1910.13003v3.pdf,2019-10-28,"['cs.lg', 'cs.cv', 'stat.ml']","  Inner product-based convolution has been the founding stone of convolutionalneural networks (CNNs), enabling end-to-end learning of visual representation.By generalizing inner product with a bilinear matrix, we propose the neuralsimilarity which serves as a learnable parametric similarity measure for CNNs.Neural similarity naturally generalizes the convolution and enhancesflexibility. Further, we consider the neural similarity learning (NSL) in orderto learn the neural similarity adaptively from training data. Specifically, wepropose two different ways of learning the neural similarity: static NSL anddynamic NSL. Interestingly, dynamic neural similarity makes the CNN become adynamic inference network. By regularizing the bilinear matrix, NSL can beviewed as learning the shape of kernel and the similarity measuresimultaneously. We further justify the effectiveness of NSL with a theoreticalviewpoint. Most importantly, NSL shows promising performance in visualrecognition and few-shot learning, validating the superiority of NSL over theinner product-based convolution counterparts."
Identity Document to Selfie Face Matching Across Adolescence,"['Vítor Albiero', 'Nisha Srinivas', 'Esteban Villalobos', 'Jorge Perez-Facuse', 'Roberto Rosenthal', 'Domingo Mery', 'Karl Ricanek', 'Kevin W. Bowyer']",http://arxiv.org/pdf/1912.10021v1.pdf,2019-12-20,['cs.cv'],"  Matching live images (``selfies'') to images from ID documents is a problemthat can arise in various applications. A challenging instance of the problemarises when the face image on the ID document is from early adolescence and thelive image is from later adolescence. We explore this problem using a privatedataset called Chilean Young Adult (CHIYA) dataset, where we match live faceimages taken at age 18-19 to face images on ID documents created at ages 9 to18. State-of-the-art deep learning face matchers (e.g., ArcFace) haverelatively poor accuracy for document-to-selfie face matching. To achievehigher accuracy, we fine-tune the best available open-source model with tripletloss for a few-shot learning. Experiments show that our approach achieveshigher accuracy than the DocFace+ model recently developed for this problem.Our fine-tuned model was able to improve the true acceptance rate for the mostdifficult (largest age span) subset from 62.92% to 96.67% at a false acceptancerate of 0.01%. Our fine-tuned model is available for use by other researchers."
Measuring Dataset Granularity,"['Yin Cui', 'Zeqi Gu', 'Dhruv Mahajan', 'Laurens van der Maaten', 'Serge Belongie', 'Ser-Nam Lim']",http://arxiv.org/pdf/1912.10154v1.pdf,2019-12-21,"['cs.cv', 'cs.lg']","  Despite the increasing visibility of fine-grained recognition in our field,""fine-grained'' has thus far lacked a precise definition. In this work,building upon clustering theory, we pursue a framework for measuring datasetgranularity. We argue that dataset granularity should depend not only on thedata samples and their labels, but also on the distance function we choose. Wepropose an axiomatic framework to capture desired properties for a datasetgranularity measure and provide examples of measures that satisfy theseproperties. We assess each measure via experiments on datasets withhierarchical labels of varying granularity. When measuring granularity incommonly used datasets with our measure, we find that certain datasets that arewidely considered fine-grained in fact contain subsets of considerable sizethat are substantially more coarse-grained than datasets generally regarded ascoarse-grained. We also investigate the interplay between dataset granularitywith a variety of factors and find that fine-grained datasets are moredifficult to learn from, more difficult to transfer to, more difficult toperform few-shot learning with, and more vulnerable to adversarial attacks."
Covariate Distribution Aware Meta-learning,"['Amrith Setlur', 'Saket Dingliwal', 'Barnabas Poczos']",http://arxiv.org/pdf/2007.02523v3.pdf,2020-07-06,"['cs.lg', 'stat.ml']","  Meta-learning has proven to be successful for few-shot learning across theregression, classification, and reinforcement learning paradigms. Recentapproaches have adopted Bayesian interpretations to improve gradient-basedmeta-learners by quantifying the uncertainty of the post-adaptation estimates.Most of these works almost completely ignore the latent relationship betweenthe covariate distribution $(p(x))$ of a task and the corresponding conditionaldistribution $p(y|x)$. In this paper, we identify the need to explicitly modelthe meta-distribution over the task covariates in a hierarchical Bayesianframework. We begin by introducing a graphical model that leverages the samplesfrom the marginal $p(x)$ to better infer the posterior over the optimalparameters of the conditional distribution $(p(y|x))$ for each task. Based onthis model we propose a computationally feasible meta-learning algorithm byintroducing meaningful relaxations in our final objective. We demonstrate thegains of our algorithm over initialization based meta-learning baselines onpopular classification benchmarks. Finally, to understand the potential benefitof modeling task covariates we further evaluate our method on a syntheticregression dataset."
Meta-Learning Divergences of Variational Inference,"['Ruqi Zhang', 'Yingzhen Li', 'Christopher De Sa', 'Sam Devlin', 'Cheng Zhang']",http://arxiv.org/pdf/2007.02912v2.pdf,2020-07-06,"['cs.lg', 'stat.ml']","  Variational inference (VI) plays an essential role in approximate Bayesianinference due to its computational efficiency and broad applicability. Crucialto the performance of VI is the selection of the associated divergence measure,as VI approximates the intractable distribution by minimizing this divergence.In this paper we propose a meta-learning algorithm to learn the divergencemetric suited for the task of interest, automating the design of VI methods. Inaddition, we learn the initialization of the variational parameters withoutadditional cost when our method is deployed in the few-shot learning scenarios.We demonstrate our approach outperforms standard VI on Gaussian mixturedistribution approximation, Bayesian neural network regression, imagegeneration with variational autoencoders and recommender systems with a partialvariational autoencoder."
Learning to learn generative programs with Memoised Wake-Sleep,"['Luke B. Hewitt', 'Tuan Anh Le', 'Joshua B. Tenenbaum']",http://arxiv.org/pdf/2007.03132v2.pdf,2020-07-06,"['cs.ai', 'cs.lg']","  We study a class of neuro-symbolic generative models in which neural networksare used both for inference and as priors over symbolic, data-generatingprograms. As generative models, these programs capture compositional structuresin a naturally explainable form. To tackle the challenge of performing programinduction as an 'inner-loop' to learning, we propose the Memoised Wake-Sleep(MWS) algorithm, which extends Wake Sleep by explicitly storing and reusing thebest programs discovered by the inference network throughout training. We useMWS to learn accurate, explainable models in three challenging domains:stroke-based character modelling, cellular automata, and few-shot learning in anovel dataset of real-world string concepts."
Meta-Learning with Network Pruning,"['Hongduan Tian', 'Bo Liu', 'Xiao-Tong Yuan', 'Qingshan Liu']",http://arxiv.org/pdf/2007.03219v2.pdf,2020-07-07,"['cs.lg', 'stat.ml']","  Meta-learning is a powerful paradigm for few-shot learning. Although withremarkable success witnessed in many applications, the existing optimizationbased meta-learning models with over-parameterized neural networks have beenevidenced to ovetfit on training tasks. To remedy this deficiency, we propose anetwork pruning based meta-learning approach for overfitting reduction viaexplicitly controlling the capacity of network. A uniform concentrationanalysis reveals the benefit of network capacity constraint for reducinggeneralization gap of the proposed meta-learner. We have implemented ourapproach on top of Reptile assembled with two network pruning routines:Dense-Sparse-Dense (DSD) and Iterative Hard Thresholding (IHT). Extensiveexperimental results on benchmark datasets with different over-parameterizeddeep networks demonstrate that our method not only effectively alleviatesmeta-overfitting but also in many cases improves the overall generalizationperformance when applied to few-shot classification tasks."
Predicting the Accuracy of a Few-Shot Classifier,"['Myriam Bontonou', 'Louis Béthune', 'Vincent Gripon']",http://arxiv.org/pdf/2007.04238v1.pdf,2020-07-08,"['cs.lg', 'cs.cv', 'stat.ml']","  In the context of few-shot learning, one cannot measure the generalizationability of a trained classifier using validation sets, due to the small numberof labeled samples. In this paper, we are interested in finding alternatives toanswer the question: is my classifier generalizing well to previously unseendata? We first analyze the reasons for the variability of generalizationperformances. We then investigate the case of using transfer-based solutions,and consider three settings: i) supervised where we only have access to a fewlabeled samples, ii) semi-supervised where we have access to both a few labeledsamples and a set of unlabeled samples and iii) unsupervised where we only haveaccess to unlabeled samples. For each setting, we propose reasonable measuresthat we empirically demonstrate to be correlated with the generalizationability of considered classifiers. We also show that these simple measures canbe used to predict generalization up to a certain confidence. We conduct ourexperiments on standard few-shot vision datasets."
Concept Learners for Few-Shot Learning,"['Kaidi Cao', 'Maria Brbic', 'Jure Leskovec']",http://arxiv.org/pdf/2007.07375v3.pdf,2020-07-14,"['cs.lg', 'cs.cv', 'stat.ml']","  Developing algorithms that are able to generalize to a novel task given onlya few labeled examples represents a fundamental challenge in closing the gapbetween machine- and human-level performance. The core of human cognition liesin the structured, reusable concepts that help us to rapidly adapt to new tasksand provide reasoning behind our decisions. However, existing meta-learningmethods learn complex representations across prior labeled tasks withoutimposing any structure on the learned representations. Here we propose COMET, ameta-learning method that improves generalization ability by learning to learnalong human-interpretable concept dimensions. Instead of learning a jointunstructured metric space, COMET learns mappings of high-level concepts intosemi-structured metric spaces, and effectively combines the outputs ofindependent concept learners. We evaluate our model on few-shot tasks fromdiverse domains, including fine-grained image classification, documentcategorization and cell type annotation on a novel dataset from a biologicaldomain developed in our work. COMET significantly outperforms strongmeta-learning baselines, achieving 6-15% relative improvement on the mostchallenging 1-shot learning tasks, while unlike existing methods providinginterpretations behind the model's predictions."
Contextualizing Enhances Gradient Based Meta Learning,"['Evan Vogelbaum', 'Rumen Dangovski', 'Li Jing', 'Marin Soljačić']",http://arxiv.org/pdf/2007.10143v1.pdf,2020-07-17,"['cs.lg', 'cs.cv', 'cs.ne', 'stat.ml']","  Meta learning methods have found success when applied to few shotclassification problems, in which they quickly adapt to a small number oflabeled examples. Prototypical representations, each representing a particularclass, have been of particular importance in this setting, as they provide acompact form to convey information learned from the labeled examples. However,these prototypes are just one method of representing this information, and theyare narrow in their scope and ability to classify unseen examples. We proposethe implementation of contextualizers, which are generalizable prototypes thatadapt to given examples and play a larger role in classification forgradient-based models. We demonstrate how to equip meta learning methods withcontextualizers and show that their use can significantly boost performance ona range of few shot learning datasets. We also present figures of meritdemonstrating the potential benefits of contextualizers, along with analysis ofhow models make use of them. Our approach is particularly apt for low-dataenvironments where it is difficult to update parameters without overfitting.Our implementation and instructions to reproduce the experiments are availableat https://github.com/naveace/proto-context."
A Comprehensive Evaluation of Multi-task Learning and Multi-task  Pre-training on EHR Time-series Data,"['Matthew B. A. McDermott', 'Bret Nestor', 'Evan Kim', 'Wancong Zhang', 'Anna Goldenberg', 'Peter Szolovits', 'Marzyeh Ghassemi']",http://arxiv.org/pdf/2007.10185v1.pdf,2020-07-20,"['cs.lg', 'stat.ml']","  Multi-task learning (MTL) is a machine learning technique aiming to improvemodel performance by leveraging information across many tasks. It has been usedextensively on various data modalities, including electronic health record(EHR) data. However, despite significant use on EHR data, there has been littlesystematic investigation of the utility of MTL across the diverse set ofpossible tasks and training schemes of interest in healthcare. In this work, weexamine MTL across a battery of tasks on EHR time-series data. We find thatwhile MTL does suffer from common negative transfer, we can realize significantgains via MTL pre-training combined with single-task fine-tuning. Wedemonstrate that these gains can be achieved in a task-independent manner andoffer not only minor improvements under traditional learning, but also notablegains in a few-shot learning context, thereby suggesting this could be ascalable vehicle to offer improved performance in important healthcarecontexts."
Leveraging Bottom-Up and Top-Down Attention for Few-Shot Object  Detection,"['Xianyu Chen', 'Ming Jiang', 'Qi Zhao']",http://arxiv.org/pdf/2007.12104v1.pdf,2020-07-23,['cs.cv'],"  Few-shot object detection aims at detecting objects with few annotatedexamples, which remains a challenging research problem yet to be explored.Recent studies have shown the effectiveness of self-learned top-down attentionmechanisms in object detection and other vision tasks. The top-down attention,however, is less effective at improving the performance of few-shot detectors.Due to the insufficient training data, object detectors cannot effectivelygenerate attention maps for few-shot examples. To improve the performance andinterpretability of few-shot object detectors, we propose an attentive few-shotobject detection network (AttFDNet) that takes the advantages of both top-downand bottom-up attention. Being task-agnostic, the bottom-up attention serves asa prior that helps detect and localize naturally salient objects. We furtheraddress specific challenges in few-shot object detection by introducing twonovel loss terms and a hybrid few-shot learning strategy. Experimental resultsand visualization demonstrate the complementary nature of the two types ofattention and their roles in few-shot object detection. Codes are available athttps://github.com/chenxy99/AttFDNet."
Towards Accuracy-Fairness Paradox: Adversarial Example-based Data  Augmentation for Visual Debiasing,"['Yi Zhang', 'Jitao Sang']",http://arxiv.org/pdf/2007.13632v2.pdf,2020-07-27,"['cs.cv', 'cs.lg']","  Machine learning fairness concerns about the biases towards certain protectedor sensitive group of people when addressing the target tasks. This paperstudies the debiasing problem in the context of image classification tasks. Ourdata analysis on facial attribute recognition demonstrates (1) the attributionof model bias from imbalanced training data distribution and (2) the potentialof adversarial examples in balancing data distribution. We are thus motivatedto employ adversarial example to augment the training data for visualdebiasing. Specifically, to ensure the adversarial generalization as well ascross-task transferability, we propose to couple the operations of target taskclassifier training, bias task classifier training, and adversarial examplegeneration. The generated adversarial examples supplement the target tasktraining dataset via balancing the distribution over bias variables in anonline fashion. Results on simulated and real-world debiasing experimentsdemonstrate the effectiveness of the proposed solution in simultaneouslyimproving model accuracy and fairness. Preliminary experiment on few-shotlearning further shows the potential of adversarial attack-based pseudo samplegeneration as alternative solution to make up for the training data lackage."
Learning from Few Samples: A Survey,"['Nihar Bendre', 'Hugo Terashima Marín', 'Peyman Najafirad']",http://arxiv.org/pdf/2007.15484v1.pdf,2020-07-30,['cs.cv'],"  Deep neural networks have been able to outperform humans in some cases likeimage recognition and image classification. However, with the emergence ofvarious novel categories, the ability to continuously widen the learningcapability of such networks from limited samples, still remains a challenge.Techniques like Meta-Learning and/or few-shot learning showed promisingresults, where they can learn or generalize to a novel category/task based onprior knowledge. In this paper, we perform a study of the existing few-shotmeta-learning techniques in the computer vision domain based on their methodand evaluation metrics. We provide a taxonomy for the techniques and categorizethem as data-augmentation, embedding, optimization and semantics based learningfor few-shot, one-shot and zero-shot settings. We then describe the seminalwork done in each category and discuss their approach towards solving thepredicament of learning from few samples. Lastly we provide a comparison ofthese techniques on the commonly used benchmark datasets: Omniglot, andMiniImagenet, along with a discussion towards the future direction of improvingthe performance of these techniques towards the final goal of outperforminghumans."
Meta-Learning with Adaptive Hyperparameters,"['Sungyong Baik', 'Myungsub Choi', 'Janghoon Choi', 'Heewon Kim', 'Kyoung Mu Lee']",http://arxiv.org/pdf/2011.00209v2.pdf,2020-10-31,"['cs.lg', 'cs.cv']","  Despite its popularity, several recent works question the effectiveness ofMAML when test tasks are different from training tasks, thus suggesting varioustask-conditioned methodology to improve the initialization. Instead ofsearching for better task-aware initialization, we focus on a complementaryfactor in MAML framework, inner-loop optimization (or fast adaptation).Consequently, we propose a new weight update rule that greatly enhances thefast adaptation process. Specifically, we introduce a small meta-network thatcan adaptively generate per-step hyperparameters: learning rate and weightdecay coefficients. The experimental results validate that the AdaptiveLearning of hyperparameters for Fast Adaptation (ALFA) is the equally importantingredient that was often neglected in the recent few-shot learning approaches.Surprisingly, fast adaptation from random initialization with ALFA can alreadyoutperform MAML."
Supervised Contrastive Learning for Pre-trained Language Model  Fine-tuning,"['Beliz Gunel', 'Jingfei Du', 'Alexis Conneau', 'Ves Stoyanov']",http://arxiv.org/pdf/2011.01403v3.pdf,2020-11-03,"['cs.cl', 'cs.lg']","  State-of-the-art natural language understanding classification models followtwo-stages: pre-training a large language model on an auxiliary task, and thenfine-tuning the model on a task-specific labeled dataset using cross-entropyloss. However, the cross-entropy loss has several shortcomings that can lead tosub-optimal generalization and instability. Driven by the intuition that goodgeneralization requires capturing the similarity between examples in one classand contrasting them with examples in other classes, we propose a supervisedcontrastive learning (SCL) objective for the fine-tuning stage. Combined withcross-entropy, our proposed SCL loss obtains significant improvements over astrong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark infew-shot learning settings, without requiring specialized architecture, dataaugmentations, memory banks, or additional unsupervised data. Our proposedfine-tuning objective leads to models that are more robust to different levelsof noise in the fine-tuning training data, and can generalize better to relatedtasks with limited labeled data."
CMT in TREC-COVID Round 2: Mitigating the Generalization Gaps from Web  to Special Domain Search,"['Chenyan Xiong', 'Zhenghao Liu', 'Si Sun', 'Zhuyun Dai', 'Kaitao Zhang', 'Shi Yu', 'Zhiyuan Liu', 'Hoifung Poon', 'Jianfeng Gao', 'Paul Bennett']",http://arxiv.org/pdf/2011.01580v1.pdf,2020-11-03,"['cs.ir', 'cs.cl']","  Neural rankers based on deep pretrained language models (LMs) have been shownto improve many information retrieval benchmarks. However, these methods areaffected by their the correlation between pretraining domain and target domainand rely on massive fine-tuning relevance labels. Directly applying pretrainingmethods to specific domains may result in suboptimal search quality becausespecific domains may have domain adaption problems, such as the COVID domain.This paper presents a search system to alleviate the special domain adaptionproblem. The system utilizes the domain-adaptive pretraining and few-shotlearning technologies to help neural rankers mitigate the domain discrepancyand label scarcity problems. Besides, we also integrate dense retrieval toalleviate traditional sparse retrieval's vocabulary mismatch obstacle. Oursystem performs the best among the non-manual runs in Round 2 of the TREC-COVIDtask, which aims to retrieve useful information from scientific literaturerelated to COVID-19. Our code is publicly available athttps://github.com/thunlp/OpenMatch."
Investigating Novel Verb Learning in BERT: Selectional Preference  Classes and Alternation-Based Syntactic Generalization,"['Tristan Thrush', 'Ethan Wilcox', 'Roger Levy']",http://arxiv.org/pdf/2011.02417v1.pdf,2020-11-04,"['cs.cl', 'cs.lg']","  Previous studies investigating the syntactic abilities of deep learningmodels have not targeted the relationship between the strength of thegrammatical generalization and the amount of evidence to which the model isexposed during training. We address this issue by deploying a novelword-learning paradigm to test BERT's few-shot learning capabilities for twoaspects of English verbs: alternations and classes of selectional preferences.For the former, we fine-tune BERT on a single frame in a verbal-alternationpair and ask whether the model expects the novel verb to occur in its sisterframe. For the latter, we fine-tune BERT on an incomplete selectional networkof verbal objects and ask whether it expects unattested but plausibleverb/object pairs. We find that BERT makes robust grammatical generalizationsafter just one or two instances of a novel word in fine-tuning. For the verbalalternation tests, we find that the model displays behavior that is consistentwith a transitivity bias: verbs seen few times are expected to take directobjects, but verbs seen with direct objects are not expected to occurintransitively."
Self-Supervised Learning from Contrastive Mixtures for Personalized  Speech Enhancement,"['Aswin Sivaraman', 'Minje Kim']",http://arxiv.org/pdf/2011.03426v2.pdf,2020-11-06,"['eess.as', 'cs.lg', 'cs.sd']","  This work explores how self-supervised learning can be universally used todiscover speaker-specific features towards enabling personalized speechenhancement models. We specifically address the few-shot learning scenariowhere access to cleaning recordings of a test-time speaker is limited to a fewseconds, but noisy recordings of the speaker are abundant. We develop a simplecontrastive learning procedure which treats the abundant noisy data asmakeshift training targets through pairwise noise injection: the model ispretrained to maximize agreement between pairs of differently deformedidentical utterances and to minimize agreement between pairs of similarlydeformed nonidentical utterances. Our experiments compare the proposedpretraining approach with two baseline alternatives: speaker-agnosticfully-supervised pretraining, and speaker-specific self-supervised pretrainingwithout contrastive loss terms. Of all three approaches, the proposed methodusing contrastive mixtures is found to be most robust to model compression(using 85% fewer parameters) and reduced clean speech (requiring only 3seconds)."
A Broad Dataset is All You Need for One-Shot Object Detection,"['Claudio Michaelis', 'Matthias Bethge', 'Alexander S. Ecker']",http://arxiv.org/pdf/2011.04267v2.pdf,2020-11-09,"['cs.cv', 'cs.lg']","  Is it possible to detect arbitrary objects from a single example? A centralproblem of all existing attempts at one-shot object detection is thegeneralization gap: Object categories used during training are detected muchmore reliably than novel ones. We here show that this generalization gap can benearly closed by increasing the number of object categories used duringtraining. Doing so allows us to improve generalization from seen to unseenclasses from 45% to 89% and improve the state-of-the-art on COCO by 5.4 %AP50(from 22.0 to 27.5). We verify that the effect is caused by the number ofcategories and not the number of training samples, and that it holds fordifferent models, backbones and datasets. This result suggests that the key tostrong few-shot detection models may not lie in sophisticated metric learningapproaches, but instead simply in scaling the number of categories. We hopethat our findings will help to better understand the challenges of few-shotlearning and encourage future data annotation efforts to focus on widerdatasets with a broader set of categories rather than gathering more samplesper category."
Bidirectional RNN-based Few Shot Learning for 3D Medical Image  Segmentation,"['Soopil Kim', 'Sion An', 'Philip Chikontwe', 'Sang Hyun Park']",http://arxiv.org/pdf/2011.09608v1.pdf,2020-11-19,"['cs.cv', 'cs.ai']","  Segmentation of organs of interest in 3D medical images is necessary foraccurate diagnosis and longitudinal studies. Though recent advances using deeplearning have shown success for many segmentation tasks, large datasets arerequired for high performance and the annotation process is both time consumingand labor intensive. In this paper, we propose a 3D few shot segmentationframework for accurate organ segmentation using limited training samples of thetarget organ annotation. To achieve this, a U-Net like network is designed topredict segmentation by learning the relationship between 2D slices of supportdata and a query image, including a bidirectional gated recurrent unit (GRU)that learns consistency of encoded features between adjacent slices. Also, weintroduce a transfer learning method to adapt the characteristics of the targetimage and organ by updating the model before testing with arbitrary support andquery data sampled from the support data. We evaluate our proposed model usingthree 3D CT datasets with annotations of different organs. Our model yieldedsignificantly improved performance over state-of-the-art few shot segmentationmodels and was comparable to a fully supervised model trained with more targettraining data."
An Effective Anti-Aliasing Approach for Residual Networks,"['Cristina Vasconcelos', 'Hugo Larochelle', 'Vincent Dumoulin', 'Nicolas Le Roux', 'Ross Goroshin']",http://arxiv.org/pdf/2011.10675v1.pdf,2020-11-20,['cs.cv'],"  Image pre-processing in the frequency domain has traditionally played a vitalrole in computer vision and was even part of the standard pipeline in the earlydays of deep learning. However, with the advent of large datasets, manypractitioners concluded that this was unnecessary due to the belief that thesepriors can be learned from the data itself. Frequency aliasing is a phenomenonthat may occur when sub-sampling any signal, such as an image or feature map,causing distortion in the sub-sampled output. We show that we can mitigate thiseffect by placing non-trainable blur filters and using smooth activationfunctions at key locations, particularly where networks lack the capacity tolearn them. These simple architectural changes lead to substantial improvementsin out-of-distribution generalization on both image classification undernatural corruptions on ImageNet-C [10] and few-shot learning on Meta-Dataset[17], without introducing additional trainable parameters and using the defaulthyper-parameters of open source codebases."
Match Them Up: Visually Explainable Few-shot Image Classification,"['Bowen Wang', 'Liangzhi Li', 'Manisha Verma', 'Yuta Nakashima', 'Ryo Kawasaki', 'Hajime Nagahara']",http://arxiv.org/pdf/2011.12527v1.pdf,2020-11-25,['cs.cv'],"  Few-shot learning (FSL) approaches are usually based on an assumption thatthe pre-trained knowledge can be obtained from base (seen) categories and canbe well transferred to novel (unseen) categories. However, there is noguarantee, especially for the latter part. This issue leads to the unknownnature of the inference process in most FSL methods, which hampers itsapplication in some risk-sensitive areas. In this paper, we reveal a new way toperform FSL for image classification, using visual representations from thebackbone model and weights generated by a newly-emerged explainable classifier.The weighted representations only include a minimum number of distinguishablefeatures and the visualized weights can serve as an informative hint for theFSL process. Finally, a discriminator will compare the representations of eachpair of the images in the support set and the query set. Pairs with the highestscores will decide the classification results. Experimental results prove thatthe proposed method can achieve both good accuracy and satisfactoryexplainability on three mainstream datasets."
Data-Efficient Classification of Radio Galaxies,"['Ashwin Samudre', 'Lijo George', 'Mahak Bansal', 'Yogesh Wadadekar']",http://arxiv.org/pdf/2011.13311v2.pdf,2020-11-26,"['astro-ph.im', 'cs.lg']","  The continuum emission from radio galaxies can be generally classified intodifferent morphological classes such as FRI, FRII, Bent, or Compact. In thispaper, we explore the task of radio galaxy classification based on morphologyusing deep learning methods with a focus on using a small scale dataset ($\sim2000$ samples). We apply few-shot learning techniques based on Twin Networksand transfer learning techniques using a pre-trained DenseNet model withadvanced techniques like cyclical learning rate and discriminative learning totrain the model rapidly. We achieve a classification accuracy of over 92\%using our best performing model with the biggest source of confusion beingbetween Bent and FRII type galaxies. Our results show that focusing on a smallbut curated dataset along with the use of best practices to train the neuralnetwork can lead to good results. Automated classification techniques will becrucial for upcoming surveys with next generation radio telescopes which areexpected to detect hundreds of thousands of new radio galaxies in the nearfuture."
Shallow Bayesian Meta Learning for Real-World Few-Shot Recognition,"['Xueting Zhang', 'Debin Meng', 'Henry Gouk', 'Timothy Hospedales']",http://arxiv.org/pdf/2101.02833v2.pdf,2021-01-08,"['cs.lg', '68t07, 62f15', 'i.4.10; i.5.1']","  Current state-of-the-art few-shot learners focus on developing effectivetraining procedures for feature representations, before using simple, e.g.nearest centroid, classifiers. In this paper, we take an orthogonal approachthat is agnostic to the features used and focus exclusively on meta-learningthe actual classifier layer. Specifically, we introduce MetaQDA, a Bayesianmeta-learning generalization of the classic quadratic discriminant analysis.This setup has several benefits of interest to practitioners: meta-learning isfast and memory-efficient, without the need to fine-tune features. It isagnostic to the off-the-shelf features chosen and thus will continue to benefitfrom advances in feature representations. Empirically, it leads to robustperformance in cross-domain few-shot learning and, crucially for real-worldapplications, it leads to better uncertainty calibration in predictions."
Free Lunch for Few-shot Learning: Distribution Calibration,"['Shuo Yang', 'Lu Liu', 'Min Xu']",http://arxiv.org/pdf/2101.06395v3.pdf,2021-01-16,"['cs.lg', 'cs.cv']","  Learning from a limited number of samples is challenging since the learnedmodel can easily become overfitted based on the biased distribution formed byonly a few training examples. In this paper, we calibrate the distribution ofthese few-sample classes by transferring statistics from the classes withsufficient examples, then an adequate number of examples can be sampled fromthe calibrated distribution to expand the inputs to the classifier. We assumeevery dimension in the feature representation follows a Gaussian distributionso that the mean and the variance of the distribution can borrow from that ofsimilar classes whose statistics are better estimated with an adequate numberof samples. Our method can be built on top of off-the-shelf pretrained featureextractors and classification models without extra parameters. We show that asimple logistic regression classifier trained using the features sampled fromour calibrated distribution can outperform the state-of-the-art accuracy on twodatasets (~5% improvement on miniImageNet compared to the next best). Thevisualization of these generated features demonstrates that our calibrateddistribution is an accurate estimation."
Few-Shot Bayesian Optimization with Deep Kernel Surrogates,"['Martin Wistuba', 'Josif Grabocka']",http://arxiv.org/pdf/2101.07667v1.pdf,2021-01-19,['cs.lg'],"  Hyperparameter optimization (HPO) is a central pillar in the automation ofmachine learning solutions and is mainly performed via Bayesian optimization,where a parametric surrogate is learned to approximate the black box responsefunction (e.g. validation error). Unfortunately, evaluating the responsefunction is computationally intensive. As a remedy, earlier work emphasizes theneed for transfer learning surrogates which learn to optimize hyperparametersfor an algorithm from other tasks. In contrast to previous work, we propose torethink HPO as a few-shot learning problem in which we train a shared deepsurrogate model to quickly adapt (with few response evaluations) to theresponse function of a new task. We propose the use of a deep kernel networkfor a Gaussian process surrogate that is meta-learned in an end-to-end fashionin order to jointly approximate the response functions of a collection oftraining data sets. As a result, the novel few-shot optimization of our deepkernel surrogate leads to new state-of-the-art results at HPO compared toseveral recent methods on diverse metadata sets."
The Ikshana Hypothesis of Human Scene Understanding,['Venkata Satya Sai Ajay Daliparthi'],http://arxiv.org/pdf/2101.10837v4.pdf,2021-01-21,['cs.cv'],"  In recent years, deep neural networks (DNNs) achieved state-of-the-artperformance on several computer vision tasks. However, the one typical drawbackof these DNNs is the requirement of massive labeled data. Even though few-shotlearning methods address this problem, they often use techniques such asmeta-learning and metric-learning on top of the existing methods. In this work,we address this problem from a neuroscience perspective by proposing ahypothesis named Ikshana, which is supported by several findings inneuroscience. Our hypothesis approximates the refining process of conceptualgist in the human brain while understanding a natural scene/image. While ourhypothesis holds no particular novelty in neuroscience, it provides a novelperspective for designing DNNs for vision tasks. By following the Ikshanahypothesis, we design a novel neural-inspired CNN architecture namedIkshanaNet. The empirical results demonstrate the effectiveness of our methodby outperforming several baselines on the entire and subsets of the Cityscapesand the CamVid semantic segmentation benchmarks."
Similarity of Classification Tasks,"['Cuong Nguyen', 'Thanh-Toan Do', 'Gustavo Carneiro']",http://arxiv.org/pdf/2101.11201v1.pdf,2021-01-27,"['cs.lg', 'stat.ml']","  Recent advances in meta-learning has led to remarkable performances onseveral few-shot learning benchmarks. However, such success often ignores thesimilarity between training and testing tasks, resulting in a potential biasevaluation. We, therefore, propose a generative approach based on a variant ofLatent Dirichlet Allocation to analyse task similarity to optimise and betterunderstand the performance of meta-learning. We demonstrate that the proposedmethod can provide an insightful evaluation for meta-learning algorithms on twofew-shot classification benchmarks that matches common intuition: the moresimilar the higher performance. Based on this similarity measure, we propose atask-selection strategy for meta-learning and show that it can produce moreaccurate classification results than methods that randomly select trainingtasks."
Siamese Capsule Networks,"[""James O' Neill""]",http://arxiv.org/pdf/1805.07242v1.pdf,2018-05-18,"['stat.ml', 'cs.lg']","  Capsule Networks have shown encouraging results on \textit{defacto} benchmarkcomputer vision datasets such as MNIST, CIFAR and smallNORB. Although, they areyet to be tested on tasks where (1) the entities detected inherently have morecomplex internal representations and (2) there are very few instances per classto learn from and (3) where point-wise classification is not suitable. Hence,this paper carries out experiments on face verification in both controlled anduncontrolled settings that together address these points. In doing so weintroduce \textit{Siamese Capsule Networks}, a new variant that can be used forpairwise learning tasks. The model is trained using contrastive loss with$\ell_2$-normalized capsule encoded pose features. We find that \textit{SiameseCapsule Networks} perform well against strong baselines on both pairwiselearning datasets, yielding best results in the few-shot learning setting whereimage pairs in the test set contain unseen subjects."
Affinity Network Fusion and Semi-supervised Learning for Cancer Patient  Clustering,"['Tianle Ma', 'Aidong Zhang']",http://arxiv.org/pdf/1805.09673v1.pdf,2018-05-22,['q-bio.qm'],"  Defining subtypes of complex diseases such as cancer and stratifying patientgroups with the same disease but different subtypes for targeted treatments isimportant for personalized and precision medicine. Approaches that incorporatemulti-omic data are more advantageous to those using only one data type forpatient clustering and disease subtype discovery. However, it is challenging tointegrate multi-omic data as they are heterogeneous and noisy. In this paper,we present Affinity Network Fusion (ANF) to integrate multi-omic data forpatient clustering. ANF first constructs patient affinity networks for eachomic data type, and then calculates a fused network for spectral clustering. Weapplied ANF to a processed harmonized cancer dataset downloaded from GDC dataportal consisting of 2193 patients, and generated promising results onclustering patients into correct disease types. Moreover, we developed asemi-supervised model combining ANF and neural network for few-shot learning.In several cases, the model can achieve greater than 90% acccuracy on test setwith training less than 1% of the data. This demonstrates the power of ANF inlearning a good representation of patients, and shows the great potential ofsemi-supervised learning in cancer patient clustering."
Meta-Learning Probabilistic Inference For Prediction,"['Jonathan Gordon', 'John Bronskill', 'Matthias Bauer', 'Sebastian Nowozin', 'Richard E. Turner']",http://arxiv.org/pdf/1805.09921v4.pdf,2018-05-24,"['stat.ml', 'cs.lg']","  This paper introduces a new framework for data efficient and versatilelearning. Specifically: 1) We develop ML-PIP, a general framework forMeta-Learning approximate Probabilistic Inference for Prediction. ML-PIPextends existing probabilistic interpretations of meta-learning to cover abroad class of methods. 2) We introduce VERSA, an instance of the frameworkemploying a flexible and versatile amortization network that takes few-shotlearning datasets as inputs, with arbitrary numbers of shots, and outputs adistribution over task-specific parameters in a single forward pass. VERSAsubstitutes optimization at test time with forward passes through inferencenetworks, amortizing the cost of inference and relieving the need for secondderivatives during training. 3) We evaluate VERSA on benchmark datasets wherethe method sets new state-of-the-art results, handles arbitrary numbers ofshots, and for classification, arbitrary numbers of classes at train and testtime. The power of the approach is then demonstrated through a challengingfew-shot ShapeNet view reconstruction task."
Meta-Learning with Latent Embedding Optimization,"['Andrei A. Rusu', 'Dushyant Rao', 'Jakub Sygnowski', 'Oriol Vinyals', 'Razvan Pascanu', 'Simon Osindero', 'Raia Hadsell']",http://arxiv.org/pdf/1807.05960v3.pdf,2018-07-16,"['cs.lg', 'cs.cv', 'stat.ml']","  Gradient-based meta-learning techniques are both widely applicable andproficient at solving challenging few-shot learning and fast adaptationproblems. However, they have practical difficulties when operating onhigh-dimensional parameter spaces in extreme low-data regimes. We show that itis possible to bypass these limitations by learning a data-dependent latentgenerative representation of model parameters, and performing gradient-basedmeta-learning in this low-dimensional latent space. The resulting approach,latent embedding optimization (LEO), decouples the gradient-based adaptationprocedure from the underlying high-dimensional space of model parameters. Ourevaluation shows that LEO can achieve state-of-the-art performance on thecompetitive miniImageNet and tieredImageNet few-shot classification tasks.Further analysis indicates LEO is able to capture uncertainty in the data, andcan perform adaptation more effectively by optimizing in latent space."
Few-Shot Adaptation for Multimedia Semantic Indexing,"['Nakamasa Inoue', 'Koichi Shinoda']",http://arxiv.org/pdf/1807.07203v1.pdf,2018-07-19,"['cs.mm', 'cs.cv']","  We propose a few-shot adaptation framework, which bridges zero-shot learningand supervised many-shot learning, for semantic indexing of image and videodata. Few-shot adaptation provides robust parameter estimation with fewtraining examples, by optimizing the parameters of zero-shot learning andsupervised many-shot learning simultaneously. In this method, first we build azero-shot detector, and then update it by using the few examples. Ourexperiments show the effectiveness of the proposed framework on three datasets:TRECVID Semantic Indexing 2010, 2014, and ImageNET. On the ImageNET dataset, weshow that our method outperforms recent few-shot learning methods. On theTRECVID 2014 dataset, we achieve 15.19% and 35.98% in Mean Average Precisionunder the zero-shot condition and the supervised condition, respectively. Tothe best of our knowledge, these are the best results on this dataset."
Massively Multilingual Transfer for NER,"['Afshin Rahimi', 'Yuan Li', 'Trevor Cohn']",http://arxiv.org/pdf/1902.00193v4.pdf,2019-02-01,['cs.cl'],"  In cross-lingual transfer, NLP models over one or more source languages areapplied to a low-resource target language. While most prior work has used asingle source model or a few carefully selected models, here we consider a`massive' setting with many such models. This setting raises the problem ofpoor transfer, particularly from distant languages. We propose two techniquesfor modulating the transfer, suitable for zero-shot or few-shot learning,respectively. Evaluating on named entity recognition, we show that ourtechniques are much more effective than strong baselines, including standardensembling, and our unsupervised method rivals oracle selection of the singlebest individual model."
tax2vec: Constructing Interpretable Features from Taxonomies for Short  Text Classification,"['Blaž Škrlj', 'Matej Martinc', 'Jan Kralj', 'Nada Lavrač', 'Senja Pollak']",http://arxiv.org/pdf/1902.00438v3.pdf,2019-02-01,['cs.cl'],"  The use of background knowledge is largely unexploited in text classificationtasks. This paper explores word taxonomies as means for constructing newsemantic features, which may improve the performance and robustness of thelearned classifiers. We propose tax2vec, a parallel algorithm for constructingtaxonomy-based features, and demonstrate its use on six short textclassification problems: prediction of gender, personality type, age, newstopics, drug side effects and drug effectiveness. The constructed semanticfeatures, in combination with fast linear classifiers, tested against strongbaselines such as hierarchical attention neural networks, achieves comparableclassification results on short text documents. The algorithm's performance isalso tested in a few-shot learning setting, indicating that the inclusion ofsemantic features can improve the performance in data-scarce situations. Thetax2vec capability to extract corpus-specific semantic keywords is alsodemonstrated. Finally, we investigate the semantic space of potential features,where we observe a similarity with the well known Zipf's law."
Centroid-based deep metric learning for speaker recognition,"['Jixuan Wang', 'Kuan-Chieh Wang', 'Marc Law', 'Frank Rudzicz', 'Michael Brudno']",http://arxiv.org/pdf/1902.02375v1.pdf,2019-02-06,"['cs.lg', 'cs.sd', 'eess.as', 'stat.ml']","  Speaker embedding models that utilize neural networks to map utterances to aspace where distances reflect similarity between speakers have driven recentprogress in the speaker recognition task. However, there is still a significantperformance gap between recognizing speakers in the training set and unseenspeakers. The latter case corresponds to the few-shot learning task, where atrained model is evaluated on unseen classes. Here, we optimize a speakerembedding model with prototypical network loss (PNL), a state-of-the-artapproach for the few-shot image classification task. The resulting embeddingmodel outperforms the state-of-the-art triplet loss based models in bothspeaker verification and identification tasks, for both seen and unseenspeakers."
Adaptive Posterior Learning: few-shot learning with a surprise-based  memory module,"['Tiago Ramalho', 'Marta Garnelo']",http://arxiv.org/pdf/1902.02527v1.pdf,2019-02-07,"['cs.lg', 'stat.ml']","  The ability to generalize quickly from few observations is crucial forintelligent systems. In this paper we introduce APL, an algorithm thatapproximates probability distributions by remembering the most surprisingobservations it has encountered. These past observations are recalled from anexternal memory module and processed by a decoder network that can combineinformation from different memory slots to generalize beyond direct recall. Weshow this algorithm can perform as well as state of the art baselines onfew-shot classification benchmarks with a smaller memory footprint. Inaddition, its memory compression allows it to scale to thousands of unknownlabels. Finally, we introduce a meta-learning reasoning task which is morechallenging than direct classification. In this setting, APL is able togeneralize with fewer than one example per class via deductive reasoning."
Meta-Curvature,"['Eunbyung Park', 'Junier B. Oliva']",http://arxiv.org/pdf/1902.03356v3.pdf,2019-02-09,"['cs.lg', 'stat.ml']","  We propose meta-curvature (MC), a framework to learn curvature informationfor better generalization and fast model adaptation. MC expands on themodel-agnostic meta-learner (MAML) by learning to transform the gradients inthe inner optimization such that the transformed gradients achieve bettergeneralization performance to a new task. For training large scale neuralnetworks, we decompose the curvature matrix into smaller matrices in a novelscheme where we capture the dependencies of the model's parameters with aseries of tensor products. We demonstrate the effects of our proposed method onseveral few-shot learning tasks and datasets. Without any task specifictechniques and architectures, the proposed method achieves substantialimprovement upon previous MAML variants and outperforms the recentstate-of-the-art methods. Furthermore, we observe faster convergence rates ofthe meta-training process. Finally, we present an analysis that explains bettergeneralization performance with the meta-trained curvature."
"Cross-Lingual Alignment of Contextual Word Embeddings, with Applications  to Zero-shot Dependency Parsing","['Tal Schuster', 'Ori Ram', 'Regina Barzilay', 'Amir Globerson']",http://arxiv.org/pdf/1902.09492v2.pdf,2019-02-25,"['cs.cl', 'cs.lg']","  We introduce a novel method for multilingual transfer that utilizes deepcontextual embeddings, pretrained in an unsupervised fashion. While contextualembeddings have been shown to yield richer representations of meaning comparedto their static counterparts, aligning them poses a challenge due to theirdynamic nature. To this end, we construct context-independent variants of theoriginal monolingual spaces and utilize their mapping to derive an alignmentfor the context-dependent spaces. This mapping readily supports processing of atarget language, improving transfer by context-aware embeddings. Ourexperimental results demonstrate the effectiveness of this approach forzero-shot and few-shot learning of dependency parsing. Specifically, our methodconsistently outperforms the previous state-of-the-art on 6 tested languages,yielding an improvement of 6.8 LAS points on average."
Learning to Find Common Objects Across Few Image Collections,"['Amirreza Shaban', 'Amir Rahimi', 'Shray Bansal', 'Stephen Gould', 'Byron Boots', 'Richard Hartley']",http://arxiv.org/pdf/1904.12936v2.pdf,2019-04-29,['cs.cv'],"  Given a collection of bags where each bag is a set of images, our goal is toselect one image from each bag such that the selected images are from the sameobject class. We model the selection as an energy minimization problem withunary and pairwise potential functions. Inspired by recent few-shot learningalgorithms, we propose an approach to learn the potential functions directlyfrom the data. Furthermore, we propose a fast greedy inference algorithm forenergy minimization. We evaluate our approach on few-shot common objectrecognition as well as object co-localization tasks. Our experiments show thatlearning the pairwise and unary terms greatly improves the performance of themodel over several well-known methods for these tasks. The proposed greedyoptimization algorithm achieves performance comparable to state-of-the-artstructured inference algorithms while being ~10 times faster. The code ispublicly available on https://github.com/haamoon/finding_common_object."
Curvature: A signature for Action Recognition in Video Sequences,"['He Chen', 'Gregory S. Chirikjian']",http://arxiv.org/pdf/1904.13003v2.pdf,2019-04-30,['cs.cv'],"  In this paper, a novel signature of human action recognition, namely thecurvature of a video sequence, is introduced. In this way, the distribution ofsequential data is modeled, which enables few-shot learning. Instead ofdepending on recognizing features within images, our algorithm views actions assequences on the universal time scale across a whole sequence of images. Thevideo sequence, viewed as a curve in pixel space, is aligned byreparameterization using the arclength of the curve in pixel space. Once suchcurvatures are obtained, statistical indexes are extracted and fed into alearning-based classifier. Overall, our method is simple but powerful.Preliminary experimental results show that our method is effective and achievesstate-of-the-art performance in video-based human action recognition. Moreover,we see latent capacity in transferring this idea into other sequence-basedrecognition applications such as speech recognition, machine translation, andtext generation."
Unsupervised Task Design to Meta-Train Medical Image Classifiers,"['Gabriel Maicas', 'Cuong Nguyen', 'Farbod Motlagh', 'Jacinto C. Nascimento', 'Gustavo Carneiro']",http://arxiv.org/pdf/1907.07816v1.pdf,2019-07-17,['cs.cv'],"  Meta-training has been empirically demonstrated to be the most effectivepre-training method for few-shot learning of medical image classifiers (i.e.,classifiers modeled with small training sets). However, the effectiveness ofmeta-training relies on the availability of a reasonable number ofhand-designed classification tasks, which are costly to obtain, andconsequently rarely available. In this paper, we propose a new method tounsupervisedly design a large number of classification tasks to meta-trainmedical image classifiers. We evaluate our method on a breast dynamicallycontrast enhanced magnetic resonance imaging (DCE-MRI) data set that has beenused to benchmark few-shot training methods of medical image classifiers. Ourresults show that the proposed unsupervised task design to meta-train medicalimage classifiers builds a pre-trained model that, after fine-tuning, producesbetter classification results than other unsupervised and supervisedpre-training methods, and competitive results with respect to meta-trainingthat relies on hand-designed classification tasks."
Uncertainty in Model-Agnostic Meta-Learning using Variational Inference,"['Cuong Nguyen', 'Thanh-Toan Do', 'Gustavo Carneiro']",http://arxiv.org/pdf/1907.11864v2.pdf,2019-07-27,"['cs.lg', 'stat.ml']","  We introduce a new, rigorously-formulated Bayesian meta-learning algorithmthat learns a probability distribution of model parameter prior for few-shotlearning. The proposed algorithm employs a gradient-based variational inferenceto infer the posterior of model parameters to a new task. Our algorithm can beapplied to any model architecture and can be implemented in various machinelearning paradigms, including regression and classification. We show that themodels trained with our proposed meta-learning algorithm are well calibratedand accurate, with state-of-the-art calibration and classification results ontwo few-shot classification benchmarks (Omniglot and Mini-ImageNet), andcompetitive results in a multi-modal task-distribution regression."
Learning from the Past: Continual Meta-Learning via Bayesian Graph  Modeling,"['Yadan Luo', 'Zi Huang', 'Zheng Zhang', 'Ziwei Wang', 'Mahsa Baktashmotlagh', 'Yang Yang']",http://arxiv.org/pdf/1911.04695v1.pdf,2019-11-12,"['cs.lg', 'stat.ml']","  Meta-learning for few-shot learning allows a machine to leverage previouslyacquired knowledge as a prior, thus improving the performance on novel taskswith only small amounts of data. However, most mainstream models suffer fromcatastrophic forgetting and insufficient robustness issues, thereby failing tofully retain or exploit long-term knowledge while being prone to cause severeerror accumulation. In this paper, we propose a novel Continual Meta-Learningapproach with Bayesian Graph Neural Networks (CML-BGNN) that mathematicallyformulates meta-learning as continual learning of a sequence of tasks. Witheach task forming as a graph, the intra- and inter-task correlations can bewell preserved via message-passing and history transition. To remedytopological uncertainty from graph initialization, we utilize Bayes by Backpropstrategy that approximates the posterior distribution of task-specificparameters with amortized inference networks, which are seamlessly integratedinto the end-to-end edge learning. Extensive experiments conducted on theminiImageNet and tieredImageNet datasets demonstrate the effectiveness andefficiency of the proposed method, improving the performance by 42.8% comparedwith state-of-the-art on the miniImageNet 5-way 1-shot classification task."
A Conceptual Framework for Lifelong Learning,"['Charles X. Ling', 'Tanner Bohn']",http://arxiv.org/pdf/1911.09704v4.pdf,2019-11-21,"['cs.lg', 'stat.ml']","  Humans can learn a variety of concepts and skills incrementally over thecourse of their lives while exhibiting many desirable properties, such ascontinual learning without forgetting, forward transfer and backward transferof knowledge, and learning a new concept or task with only a few examples.Several lines of machine learning research, such as lifelong learning, few-shotlearning, and transfer learning, attempt to capture these properties. However,most previous approaches can only demonstrate subsets of these properties,often by different complex mechanisms. In this work, we propose a simple yetpowerful unified framework that supports almost all of these properties andapproaches through one central mechanism. We also draw connections between manypeculiarities of human learning (such as memory loss and ""rain man"") and ourframework. While we do not present any state-of-the-art results, we hope thatthis conceptual framework provides a novel perspective on existing work andproposes many new research directions."
Meta Adaptation using Importance Weighted Demonstrations,"['Kiran Lekkala', 'Sami Abu-El-Haija', 'Laurent Itti']",http://arxiv.org/pdf/1911.10322v2.pdf,2019-11-23,"['cs.lg', 'cs.ai', 'stat.ml']","  Imitation learning has gained immense popularity because of its highsample-efficiency. However, in real-world scenarios, where the trajectorydistribution of most of the tasks dynamically shifts, model fitting oncontinuously aggregated data alone would be futile. In some cases, thedistribution shifts, so much, that it is difficult for an agent to infer thenew task. We propose a novel algorithm to generalize on any related task byleveraging prior knowledge on a set of specific tasks, which involves assigningimportance weights to each past demonstration. We show experiments where therobot is trained from a diversity of environmental tasks and is also able toadapt to an unseen environment, using few-shot learning. We also developed aprototype robot system to test our approach on the task of visual navigation,and experimental results obtained were able to confirm these suppositions."
Facial Landmark Correlation Analysis,"['Yongzhe Yan', 'Stefan Duffner', 'Priyanka Phutane', 'Anthony Berthelier', 'Christophe Blanc', 'Christophe Garcia', 'Thierry Chateau']",http://arxiv.org/pdf/1911.10576v2.pdf,2019-11-24,['cs.cv'],"  We present a facial landmark position correlation analysis as well as itsapplications. Although numerous facial landmark detection methods have beenpresented in the literature, few of them explicitly take into account theinherent relationship among landmarks. To reveal and interpret thisrelationship, we propose to analyze landmark correlation by using CanonicalCorrelation Analysis~(CCA). We experimentally show that the dense faciallandmark annotations in current benchmarks are strongly correlated. We proposetwo applications based on this analysis. First, by analyzing the landmarkcorrelation, we gain some interesting insights into the predictions ofdifferent landmark detection models (including random forests model and CNNmodels). We also demonstrate how CNNs progressively learn to predict faciallandmarks. Second, we propose a few-shot learning method that allows toconsiderably reduce the manual effort for dense landmark annotation."
VIABLE: Fast Adaptation via Backpropagating Learned Loss,"['Leo Feng', 'Luisa Zintgraf', 'Bei Peng', 'Shimon Whiteson']",http://arxiv.org/pdf/1911.13159v1.pdf,2019-11-29,"['cs.lg', 'stat.ml']","  In few-shot learning, typically, the loss function which is applied at testtime is the one we are ultimately interested in minimising, such as themean-squared-error loss for a regression problem. However, given that we havefew samples at test time, we argue that the loss function that we areinterested in minimising is not necessarily the loss function most suitable forcomputing gradients in a few-shot setting. We propose VIABLE, a genericmeta-learning extension that builds on existing meta-gradient-based methods bylearning a differentiable loss function, replacing the pre-defined inner-looploss function in performing task-specific updates. We show that learning a lossfunction capable of leveraging relational information between samples reducesunderfitting, and significantly improves performance and sample efficiency on asimple regression task. Furthermore, we show VIABLE is scalable by evaluatingon the Mini-Imagenet dataset."
Modelling Latent Skills for Multitask Language Generation,"['Kris Cao', 'Dani Yogatama']",http://arxiv.org/pdf/2002.09543v1.pdf,2020-02-21,['cs.cl'],"  We present a generative model for multitask conditional language generation.Our guiding hypothesis is that a shared set of latent skills underlies manydisparate language generation tasks, and that explicitly modelling these skillsin a task embedding space can help with both positive transfer across tasks andwith efficient adaptation to new tasks. We instantiate this task embeddingspace as a latent variable in a latent variable sequence-to-sequence model. Weevaluate this hypothesis by curating a series of monolingual text-to-textlanguage generation datasets - covering a broad range of tasks and domains -and comparing the performance of models both in the multitask and few-shotregimes. We show that our latent task variable model outperforms othersequence-to-sequence baselines on average across tasks in the multitasksetting. In the few-shot learning setting on an unseen test dataset (i.e., anew task), we demonstrate that model adaptation based on inference in thelatent task space is more robust than standard fine-tuning based parameteradaptation and performs comparably in terms of overall performance. Finally, weexamine the latent task representations learnt by our model and show that theycluster tasks in a natural way."
The Sample Complexity of Meta Sparse Regression,"['Zhanyu Wang', 'Jean Honorio']",http://arxiv.org/pdf/2002.09587v2.pdf,2020-02-22,"['cs.lg', 'stat.ml']","  This paper addresses the meta-learning problem in sparse linear regressionwith infinite tasks. We assume that the learner can access several similartasks. The goal of the learner is to transfer knowledge from the prior tasks toa similar but novel task. For p parameters, size of the support set k , and lsamples per task, we show that T \in O (( k log(p) ) /l ) tasks are sufficientin order to recover the common support of all tasks. With the recoveredsupport, we can greatly reduce the sample complexity for estimating theparameter of the novel task, i.e., l \in O (1) with respect to T and p . Wealso prove that our rates are minimax optimal. A key difference betweenmeta-learning and the classical multi-task learning, is that meta-learningfocuses only on the recovery of the parameters of the novel task, whilemulti-task learning estimates the parameter of all tasks, which requires l togrow with T . Instead, our efficient meta-learning estimator allows for l to beconstant with respect to T (i.e., few-shot learning)."
Evolving Losses for Unsupervised Video Representation Learning,"['AJ Piergiovanni', 'Anelia Angelova', 'Michael S. Ryoo']",http://arxiv.org/pdf/2002.12177v1.pdf,2020-02-26,"['cs.cv', 'cs.lg']","  We present a new method to learn video representations from large-scaleunlabeled video data. Ideally, this representation will be generic andtransferable, directly usable for new tasks such as action recognition and zeroor few-shot learning. We formulate unsupervised representation learning as amulti-modal, multi-task learning problem, where the representations are sharedacross different modalities via distillation. Further, we introduce the conceptof loss function evolution by using an evolutionary search algorithm toautomatically find optimal combination of loss functions capturing many(self-supervised) tasks and modalities. Thirdly, we propose an unsupervisedrepresentation evaluation metric using distribution matching to a largeunlabeled dataset as a prior constraint, based on Zipf's law. This unsupervisedconstraint, which is not guided by any labeling, produces similar results toweakly-supervised, task-specific ones. The proposed unsupervised representationlearning results in a single RGB network and outperforms previous methods.Notably, it is also more effective than several label-based methods (e.g.,ImageNet), with the exception of large, fully labeled video datasets."
Few-shot Natural Language Generation for Task-Oriented Dialog,"['Baolin Peng', 'Chenguang Zhu', 'Chunyuan Li', 'Xiujun Li', 'Jinchao Li', 'Michael Zeng', 'Jianfeng Gao']",http://arxiv.org/pdf/2002.12328v1.pdf,2020-02-27,['cs.cl'],"  As a crucial component in task-oriented dialog systems, the Natural LanguageGeneration (NLG) module converts a dialog act represented in a semantic forminto a response in natural language. The success of traditional template-basedor statistical models typically relies on heavily annotated data, which isinfeasible for new domains. Therefore, it is pivotal for an NLG system togeneralize well with limited labelled data in real applications. To this end,we present FewShotWoz, the first NLG benchmark to simulate the few-shotlearning setting in task-oriented dialog systems. Further, we develop theSC-GPT model. It is pre-trained on a large set of annotated NLG corpus toacquire the controllable generation ability, and fine-tuned with only a fewdomain-specific labels to adapt to new domains. Experiments on FewShotWoz andthe large Multi-Domain-WOZ datasets show that the proposed SC-GPT significantlyoutperforms existing methods, measured by various automatic metrics and humanevaluations."
Task Augmentation by Rotating for Meta-Learning,"['Jialin Liu', 'Fei Chao', 'Chih-Min Lin']",http://arxiv.org/pdf/2003.00804v1.pdf,2020-02-08,['cs.cv'],"  Data augmentation is one of the most effective approaches for improving theaccuracy of modern machine learning models, and it is also indispensable totrain a deep model for meta-learning. In this paper, we introduce a taskaugmentation method by rotating, which increases the number of classes byrotating the original images 90, 180 and 270 degrees, different fromtraditional augmentation methods which increase the number of images. With alarger amount of classes, we can sample more diverse task instances duringtraining. Therefore, task augmentation by rotating allows us to train a deepnetwork by meta-learning methods with little over-fitting. Experimental resultsshow that our approach is better than the rotation for increasing the number ofimages and achieves state-of-the-art performance on miniImageNet, CIFAR-FS, andFC100 few-shot learning benchmarks. The code is available on\url{www.github.com/AceChuse/TaskLevelAug}."
PAC-Bayes meta-learning with implicit task-specific posteriors,"['Cuong Nguyen', 'Thanh-Toan Do', 'Gustavo Carneiro']",http://arxiv.org/pdf/2003.02455v3.pdf,2020-03-05,"['cs.lg', 'stat.ml']","  We introduce a new and rigorously-formulated PAC-Bayes meta-learningalgorithm that solves few-shot learning. Our proposed method extends thePAC-Bayes framework from a single task setting to the meta-learning multipletask setting to upper-bound the error evaluated on any, even unseen, tasks andsamples. We also propose a generative-based approach to estimate the posteriorof task-specific model parameters more expressively compared to the usualassumption based on a multivariate normal distribution with a diagonalcovariance matrix. We show that the models trained with our proposedmeta-learning algorithm are well calibrated and accurate, with state-of-the-artcalibration and classification results on few-shot classification(mini-ImageNet and tiered-ImageNet) and regression (multi-modaltask-distribution regression) benchmarks."
Meta-Learning GNN Initializations for Low-Resource Molecular Property  Prediction,"['Cuong Q. Nguyen', 'Constantine Kreatsoulas', 'Kim M. Branson']",http://arxiv.org/pdf/2003.05996v2.pdf,2020-03-12,"['cs.lg', 'physics.chem-ph', 'stat.ml']","  Building in silico models to predict chemical properties and activities is acrucial step in drug discovery. However, limited labeled data often hinders theapplication of deep learning in this setting. Meanwhile advances inmeta-learning have enabled state-of-the-art performances in few-shot learningbenchmarks, naturally prompting the question: Can meta-learning improve deeplearning performance in low-resource drug discovery projects? In this work, weassess the transferability of graph neural networks initializations learned bythe Model-Agnostic Meta-Learning (MAML) algorithm - and its variants FO-MAMLand ANIL - for chemical properties and activities tasks. Using the ChEMBL20dataset to emulate low-resource settings, our benchmark shows thatmeta-initializations perform comparably to or outperform multi-taskpre-training baselines on 16 out of 20 in-distribution tasks and on allout-of-distribution tasks, providing an average improvement in AUPRC of 11.2%and 26.9% respectively. Finally, we observe that meta-initializationsconsistently result in the best performing models across fine-tuning sets with$k \in \{16, 32, 64, 128, 256\}$ instances."
CAFENet: Class-Agnostic Few-Shot Edge Detection Network,"['Young-Hyun Park', 'Jun Seo', 'Jaekyun Moon']",http://arxiv.org/pdf/2003.08235v1.pdf,2020-03-18,"['cs.cv', 'cs.lg', 'eess.iv']","  We tackle a novel few-shot learning challenge, which we call few-shotsemantic edge detection, aiming to localize crisp boundaries of novelcategories using only a few labeled samples. We also present a Class-AgnosticFew-shot Edge detection Network (CAFENet) based on meta-learning strategy.CAFENet employs a semantic segmentation module in small-scale to compensate forlack of semantic information in edge labels. The predicted segmentation mask isused to generate an attention map to highlight the target object region, andmake the decoder module concentrate on that region. We also propose a newregularization method based on multi-split matching. In meta-training, themetric-learning problem with high-dimensional vectors are divided into smallsubproblems with low-dimensional sub-vectors. Since there is no existingdataset for few-shot semantic edge detection, we construct two new datasets,FSE-1000 and SBD-$5^i$, and evaluate the performance of the proposed CAFENet onthem. Extensive simulation results confirm the performance merits of thetechniques adopted in CAFENet."
Selecting Relevant Features from a Multi-domain Representation for  Few-shot Classification,"['Nikita Dvornik', 'Cordelia Schmid', 'Julien Mairal']",http://arxiv.org/pdf/2003.09338v2.pdf,2020-03-20,['cs.cv'],"  Popular approaches for few-shot classification consist of first learning ageneric data representation based on a large annotated dataset, before adaptingthe representation to new classes given only a few labeled samples. In thiswork, we propose a new strategy based on feature selection, which is bothsimpler and more effective than previous feature adaptation approaches. First,we obtain a multi-domain representation by training a set of semanticallydifferent feature extractors. Then, given a few-shot learning task, we use ourmulti-domain feature bank to automatically select the most relevantrepresentations. We show that a simple non-parametric classifier built on topof such features produces high accuracy and generalizes to domains never seenduring training, which leads to state-of-the-art results on MetaDataset andimproved accuracy on mini-ImageNet."
Auto-Ensemble: An Adaptive Learning Rate Scheduling based Deep Learning  Model Ensembling,"['Jun Yang', 'Fei Wang']",http://arxiv.org/pdf/2003.11266v2.pdf,2020-03-25,"['cs.lg', 'stat.ml']","  Ensembling deep learning models is a shortcut to promote its implementationin new scenarios, which can avoid tuning neural networks, losses and trainingalgorithms from scratch. However, it is difficult to collect sufficientaccurate and diverse models through once training. This paper proposesAuto-Ensemble (AE) to collect checkpoints of deep learning model and ensemblethem automatically by adaptive learning rate scheduling algorithm. Theadvantage of this method is to make the model converge to various local optimaby scheduling the learning rate in once training. When the number of lo-caloptimal solutions tends to be saturated, all the collected checkpoints are usedfor ensemble. Our method is universal, it can be applied to various scenarios.Experiment results on multiple datasets and neural networks demonstrate it iseffective and competitive, especially on few-shot learning. Besides, weproposed a method to measure the distance among models. Then we can ensure theaccuracy and diversity of collected models."
Negative Margin Matters: Understanding Margin in Few-shot Classification,"['Bin Liu', 'Yue Cao', 'Yutong Lin', 'Qi Li', 'Zheng Zhang', 'Mingsheng Long', 'Han Hu']",http://arxiv.org/pdf/2003.12060v1.pdf,2020-03-26,"['cs.cv', 'cs.lg', 'stat.ml']","  This paper introduces a negative margin loss to metric learning basedfew-shot learning methods. The negative margin loss significantly outperformsregular softmax loss, and achieves state-of-the-art accuracy on three standardfew-shot classification benchmarks with few bells and whistles. These resultsare contrary to the common practice in the metric learning field, that themargin is zero or positive. To understand why the negative margin loss performswell for the few-shot classification, we analyze the discriminability oflearned features w.r.t different margins for training and novel classes, bothempirically and theoretically. We find that although negative margin reducesthe feature discriminability for training classes, it may also avoid falselymapping samples of the same novel class to multiple peaks or clusters, and thusbenefit the discrimination of novel classes. Code is available athttps://github.com/bl0/negative-margin.few-shot."
Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining,"['Chengyu Wang', 'Minghui Qiu', 'Jun Huang', 'Xiaofeng He']",http://arxiv.org/pdf/2003.13003v2.pdf,2020-03-29,"['cs.cl', 'cs.lg']","  Pre-trained neural language models bring significant improvement for variousNLP tasks, by fine-tuning the models on task-specific training sets. Duringfine-tuning, the parameters are initialized from pre-trained models directly,which ignores how the learning process of similar NLP tasks in differentdomains is correlated and mutually reinforced. In this paper, we propose aneffective learning procedure named Meta Fine-Tuning (MFT), served as ameta-learner to solve a group of similar NLP tasks for neural language models.Instead of simply multi-task training over all the datasets, MFT only learnsfrom typical instances of various domains to acquire highly transferableknowledge. It further encourages the language model to encode domain-invariantrepresentations by optimizing a series of novel domain corruption lossfunctions. After MFT, the model can be fine-tuned for each domain with betterparameter initializations and higher generalization ability. We implement MFTupon BERT to solve several multi-domain text mining tasks. Experimental resultsconfirm the effectiveness of MFT and its usefulness for few-shot learning."
Improving out-of-distribution generalization via multi-task  self-supervised pretraining,"['Isabela Albuquerque', 'Nikhil Naik', 'Junnan Li', 'Nitish Keskar', 'Richard Socher']",http://arxiv.org/pdf/2003.13525v1.pdf,2020-03-30,"['cs.cv', 'cs.lg']","  Self-supervised feature representations have been shown to be useful forsupervised classification, few-shot learning, and adversarial robustness. Weshow that features obtained using self-supervised learning are comparable to,or better than, supervised learning for domain generalization in computervision. We introduce a new self-supervised pretext task of predicting responsesto Gabor filter banks and demonstrate that multi-task learning of compatiblepretext tasks improves domain generalization performance as compared totraining individual tasks alone. Features learnt through self-supervisionobtain better generalization to unseen domains when compared to theirsupervised counterpart when there is a larger domain shift between training andtest distributions and even show better localization ability for objects ofinterest. Self-supervised feature representations can also be combined withother domain generalization methods to further boost performance."
FGN: Fully Guided Network for Few-Shot Instance Segmentation,"['Zhibo Fan', 'Jin-Gang Yu', 'Zhihao Liang', 'Jiarong Ou', 'Changxin Gao', 'Gui-Song Xia', 'Yuanqing Li']",http://arxiv.org/pdf/2003.13954v1.pdf,2020-03-31,['cs.cv'],"  Few-shot instance segmentation (FSIS) conjoins the few-shot learning paradigmwith general instance segmentation, which provides a possible way of tacklinginstance segmentation in the lack of abundant labeled data for training. Thispaper presents a Fully Guided Network (FGN) for few-shot instance segmentation.FGN perceives FSIS as a guided model where a so-called support set is encodedand utilized to guide the predictions of a base instance segmentation network(i.e., Mask R-CNN), critical to which is the guidance mechanism. In this view,FGN introduces different guidance mechanisms into the various key components inMask R-CNN, including Attention-Guided RPN, Relation-Guided Detector, andAttention-Guided FCN, in order to make full use of the guidance effect from thesupport set and adapt better to the inter-class generalization. Experiments onpublic datasets demonstrate that our proposed FGN can outperform thestate-of-the-art methods."
Learning to Select Base Classes for Few-shot Classification,"['Linjun Zhou', 'Peng Cui', 'Xu Jia', 'Shiqiang Yang', 'Qi Tian']",http://arxiv.org/pdf/2004.00315v1.pdf,2020-04-01,"['cs.cv', 'cs.lg', 'stat.ml']","  Few-shot learning has attracted intensive research attention in recent years.Many methods have been proposed to generalize a model learned from providedbase classes to novel classes, but no previous work studies how to select baseclasses, or even whether different base classes will result in differentgeneralization performance of the learned model. In this paper, we utilize asimple yet effective measure, the Similarity Ratio, as an indicator for thegeneralization performance of a few-shot model. We then formulate the baseclass selection problem as a submodular optimization problem over SimilarityRatio. We further provide theoretical analysis on the optimization lower boundof different optimization methods, which could be used to identify the mostappropriate algorithm for different experimental settings. The extensiveexperiments on ImageNet, Caltech256 and CUB-200-2011 demonstrate that ourproposed method is effective in selecting a better base dataset."
Knowledge Guided Metric Learning for Few-Shot Text Classification,"['Dianbo Sui', 'Yubo Chen', 'Binjie Mao', 'Delai Qiu', 'Kang Liu', 'Jun Zhao']",http://arxiv.org/pdf/2004.01907v1.pdf,2020-04-04,"['cs.cl', 'cs.ir', 'cs.lg']","  The training of deep-learning-based text classification models relies heavilyon a huge amount of annotation data, which is difficult to obtain. When thelabeled data is scarce, models tend to struggle to achieve satisfactoryperformance. However, human beings can distinguish new categories veryefficiently with few examples. This is mainly due to the fact that human beingscan leverage knowledge obtained from relevant tasks. Inspired by humanintelligence, we propose to introduce external knowledge into few-shot learningto imitate human knowledge. A novel parameter generator network is investigatedto this end, which is able to use the external knowledge to generate relationnetwork parameters. Metrics can be transferred among tasks when equipped withthese generated parameters, so that similar tasks use similar metrics whiledifferent tasks use different metrics. Through experiments, we demonstrate thatour method outperforms the state-of-the-art few-shot text classificationmodels."
Meta-Learning in Neural Networks: A Survey,"['Timothy Hospedales', 'Antreas Antoniou', 'Paul Micaelli', 'Amos Storkey']",http://arxiv.org/pdf/2004.05439v2.pdf,2020-04-11,"['cs.lg', 'stat.ml']","  The field of meta-learning, or learning-to-learn, has seen a dramatic rise ininterest in recent years. Contrary to conventional approaches to AI where tasksare solved from scratch using a fixed learning algorithm, meta-learning aims toimprove the learning algorithm itself, given the experience of multiplelearning episodes. This paradigm provides an opportunity to tackle manyconventional challenges of deep learning, including data and computationbottlenecks, as well as generalization. This survey describes the contemporarymeta-learning landscape. We first discuss definitions of meta-learning andposition it with respect to related fields, such as transfer learning andhyperparameter optimization. We then propose a new taxonomy that provides amore comprehensive breakdown of the space of meta-learning methods today. Wesurvey promising applications and successes of meta-learning such as few-shotlearning and reinforcement learning. Finally, we discuss outstanding challengesand promising areas for future research."
Meta-Meta Classification for One-Shot Learning,"['Arkabandhu Chowdhury', 'Dipak Chaudhari', 'Swarat Chaudhuri', 'Chris Jermaine']",http://arxiv.org/pdf/2004.08083v4.pdf,2020-04-17,"['cs.lg', 'cs.cv', 'stat.ml']","  We present a new approach, called meta-meta classification, to learning insmall-data settings. In this approach, one uses a large set of learningproblems to design an ensemble of learners, where each learner has high biasand low variance and is skilled at solving a specific type of learning problem.The meta-meta classifier learns how to examine a given learning problem andcombine the various learners to solve the problem. The meta-meta learningapproach is especially suited to solving few-shot learning tasks, as it iseasier to learn to classify a new learning problem with little data than it isto apply a learning algorithm to a small data set. We evaluate the approach ona one-shot, one-class-versus-all classification task and show that it is ableto outperform traditional meta-learning as well as ensembling approaches."
Alleviating the Incompatibility between Cross Entropy Loss and Episode  Training for Few-shot Skin Disease Classification,"['Wei Zhu', 'Haofu Liao', 'Wenbin Li', 'Weijian Li', 'Jiebo Luo']",http://arxiv.org/pdf/2004.09694v1.pdf,2020-04-21,"['eess.iv', 'cs.cv', 'cs.lg']","  Skin disease classification from images is crucial to dermatologicaldiagnosis. However, identifying skin lesions involves a variety of aspects interms of size, color, shape, and texture. To make matters worse, manycategories only contain very few samples, posing great challenges toconventional machine learning algorithms and even human experts. Inspired bythe recent success of Few-Shot Learning (FSL) in natural image classification,we propose to apply FSL to skin disease identification to address the extremescarcity of training sample problem. However, directly applying FSL to thistask does not work well in practice, and we find that the problem can belargely attributed to the incompatibility between Cross Entropy (CE) andepisode training, which are both commonly used in FSL. Based on a detailedanalysis, we propose the Query-Relative (QR) loss, which proves superior to CEunder episode training and is closely related to recently proposed mutualinformation estimation. Moreover, we further strengthen the proposed QR losswith a novel adaptive hard margin strategy. Comprehensive experiments validatethe effectiveness of the proposed FSL scheme and the possibility to diagnosisrare skin disease with a few labeled samples."
Supervised Domain Adaptation: A Graph Embedding Perspective and a  Rectified Experimental Protocol,"['Lukas Hedegaard', 'Omar Ali Sheikh-Omar', 'Alexandros Iosifidis']",http://arxiv.org/pdf/2004.11262v4.pdf,2020-04-23,"['cs.lg', 'stat.ml']","  Domain Adaptation is the process of alleviating distribution gaps betweendata from different domains. In this paper, we show that Domain Adaptationmethods using pair-wise relationships between source and target domain data canbe formulated as a Graph Embedding in which the domain labels are incorporatedinto the structure of the intrinsic and penalty graphs. Specifically, weanalyse the loss functions of three existing state-of-the-art Supervised DomainAdaptation methods and demonstrate that they perform Graph Embedding. Moreover,we highlight some generalisation and reproducibility issues related to theexperimental setup commonly used to demonstrate the few-shot learningcapabilities of these methods. To assess and compare Supervised DomainAdaptation methods accurately, we propose a rectified evaluation protocol, andreport updated benchmarks on the standard datasets Office31 (Amazon, DSLR, andWebcam), Digits (MNIST, USPS, SVHN, and MNIST-M) and VisDA (Synthetic, Real)."
Generalized Reinforcement Meta Learning for Few-Shot Optimization,"['Raviteja Anantha', 'Stephen Pulman', 'Srinivas Chappidi']",http://arxiv.org/pdf/2005.01246v1.pdf,2020-05-04,"['cs.lg', 'cs.ai', 'stat.ml']","  We present a generic and flexible Reinforcement Learning (RL) basedmeta-learning framework for the problem of few-shot learning. During training,it learns the best optimization algorithm to produce a learner(ranker/classifier, etc) by exploiting stable patterns in loss surfaces. Ourmethod implicitly estimates the gradients of a scaled loss function whileretaining the general properties intact for parameter updates. Besidesproviding improved performance on few-shot tasks, our framework could be easilyextended to do network architecture search. We further propose a novel dualencoder, affinity-score based decoder topology that achieves additionalimprovements to performance. Experiments on an internal dataset, MQ2007, andAwA2 show our approach outperforms existing alternative approaches by 21%, 8%,and 4% respectively on accuracy and NDCG metrics. On Mini-ImageNet dataset ourapproach achieves comparable results with Prototypical Networks. Empiricalevaluations demonstrate that our approach provides a unified and effectiveframework."
Harvesting and Refining Question-Answer Pairs for Unsupervised QA,"['Zhongli Li', 'Wenhui Wang', 'Li Dong', 'Furu Wei', 'Ke Xu']",http://arxiv.org/pdf/2005.02925v1.pdf,2020-05-06,['cs.cl'],"  Question Answering (QA) has shown great success thanks to the availability oflarge-scale datasets and the effectiveness of neural models. Recent researchworks have attempted to extend these successes to the settings with few or nolabeled data available. In this work, we introduce two approaches to improveunsupervised QA. First, we harvest lexically and syntactically divergentquestions from Wikipedia to automatically construct a corpus of question-answerpairs (named as RefQA). Second, we take advantage of the QA model to extractmore appropriate answers, which iteratively refines data over RefQA. We conductexperiments on SQuAD 1.1, and NewsQA by fine-tuning BERT without access tomanually annotated data. Our approach outperforms previous unsupervisedapproaches by a large margin and is competitive with early supervised models.We also show the effectiveness of our approach in the few-shot learningsetting."
DenoiSeg: Joint Denoising and Segmentation,"['Tim-Oliver Buchholz', 'Mangal Prakash', 'Alexander Krull', 'Florian Jug']",http://arxiv.org/pdf/2005.02987v2.pdf,2020-05-06,"['cs.cv', 'cs.lg']","  Microscopy image analysis often requires the segmentation of objects, buttraining data for this task is typically scarce and hard to obtain. Here wepropose DenoiSeg, a new method that can be trained end-to-end on only a fewannotated ground truth segmentations. We achieve this by extending Noise2Void,a self-supervised denoising scheme that can be trained on noisy images alone,to also predict dense 3-class segmentations. The reason for the success of ourmethod is that segmentation can profit from denoising, especially whenperformed jointly within the same network. The network becomes a denoisingexpert by seeing all available raw data, while co-learning to segment, even ifonly a few segmentation labels are available. This hypothesis is additionallyfueled by our observation that the best segmentation results on high quality(very low noise) raw data are obtained when moderate amounts of synthetic noiseare added. This renders the denoising-task non-trivial and unleashes thedesired co-learning effect. We believe that DenoiSeg offers a viable way tocircumvent the tremendous hunger for high quality training data and effectivelyenables few-shot learning of dense segmentations."
Feature Transformation Ensemble Model with Batch Spectral Regularization  for Cross-Domain Few-Shot Classification,"['Bingyu Liu', 'Zhen Zhao', 'Zhenpeng Li', 'Jianan Jiang', 'Yuhong Guo', 'Jieping Ye']",http://arxiv.org/pdf/2005.08463v3.pdf,2020-05-18,['cs.cv'],"  In this paper, we propose a feature transformation ensemble model with batchspectral regularization for the Cross-domain few-shot learning (CD-FSL)challenge. Specifically, we proposes to construct an ensemble prediction modelby performing diverse feature transformations after a feature extractionnetwork. On each branch prediction network of the model we use a batch spectralregularization term to suppress the singular values of the feature matrixduring pre-training to improve the generalization ability of the model. Theproposed model can then be fine tuned in the target domain to address few-shotclassification. We also further apply label propagation, entropy minimizationand data augmentation to mitigate the shortage of labeled data in targetdomains. Experiments are conducted on a number of CD-FSL benchmark tasks withfour target domains and the results demonstrate the superiority of our proposedmodel."
Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained  Conversational Representations,"['Sam Coope', 'Tyler Farghly', 'Daniela Gerz', 'Ivan Vulić', 'Matthew Henderson']",http://arxiv.org/pdf/2005.08866v2.pdf,2020-05-18,['cs.cl'],"  We introduce Span-ConveRT, a light-weight model for dialog slot-filling whichframes the task as a turn-based span extraction task. This formulation allowsfor a simple integration of conversational knowledge coded in large pretrainedconversational models such as ConveRT (Henderson et al., 2019). We show thatleveraging such knowledge in Span-ConveRT is especially useful for few-shotlearning scenarios: we report consistent gains over 1) a span extractor thattrains representations from scratch in the target domain, and 2) a BERT-basedspan extractor. In order to inspire more work on span extraction for theslot-filling task, we also release RESTAURANTS-8K, a new challenging data setof 8,198 utterances, compiled from actual conversations in the restaurantbooking domain."
Fine-Grain Few-Shot Vision via Domain Knowledge as Hyperspherical Priors,"['Bijan Haney', 'Alexander Lavin']",http://arxiv.org/pdf/2005.11450v1.pdf,2020-05-23,"['cs.cv', 'cs.lg', 'eess.iv']","  Prototypical networks have been shown to perform well at few-shot learningtasks in computer vision. Yet these networks struggle when classes are verysimilar to each other (fine-grain classification) and currently have no way oftaking into account prior knowledge (through the use of tabular data). Using aspherical latent space to encode prototypes, we can achieve few-shot fine-grainclassification by maximally separating the classes while incorporating domainknowledge as informative priors. We describe how to construct a hypersphere ofprototypes that embed a-priori domain information, and demonstrate theeffectiveness of the approach on challenging benchmark datasets for fine-grainclassification, with top results for one-shot classification and 5x speedups intraining time."
Pay Attention to What You Read: Non-recurrent Handwritten Text-Line  Recognition,"['Lei Kang', 'Pau Riba', 'Marçal Rusiñol', 'Alicia Fornés', 'Mauricio Villegas']",http://arxiv.org/pdf/2005.13044v1.pdf,2020-05-26,['cs.cv'],"  The advent of recurrent neural networks for handwriting recognition marked animportant milestone reaching impressive recognition accuracies despite thegreat variability that we observe across different writing styles. Sequentialarchitectures are a perfect fit to model text lines, not only because of theinherent temporal aspect of text, but also to learn probability distributionsover sequences of characters and words. However, using such recurrent paradigmscomes at a cost at training stage, since their sequential pipelines preventparallelization. In this work, we introduce a non-recurrent approach torecognize handwritten text by the use of transformer models. We propose a novelmethod that bypasses any recurrence. By using multi-head self-attention layersboth at the visual and textual stages, we are able to tackle characterrecognition as well as to learn language-related dependencies of the charactersequences to be decoded. Our model is unconstrained to any predefinedvocabulary, being able to recognize out-of-vocabulary words, i.e. words that donot appear in the training vocabulary. We significantly advance over prior artand demonstrate that satisfactory recognition accuracies are yielded even infew-shot learning scenarios."
Leveraging the Feature Distribution in Transfer-based Few-Shot Learning,"['Yuqing Hu', 'Vincent Gripon', 'Stéphane Pateux']",http://arxiv.org/pdf/2006.03806v3.pdf,2020-06-06,"['cs.lg', 'stat.ml']","  Few-shot classification is a challenging problem due to the uncertaintycaused by using few labelled samples. In the past few years, many methods havebeen proposed to solve few-shot classification, among which transfer-basedmethods have proved to achieve the best performance. Following this vein, inthis paper we propose a novel transfer-based method that builds on twosuccessive steps: 1) preprocessing the feature vectors so that they becomecloser to Gaussian-like distributions, and 2) leveraging this preprocessingusing an optimal-transport inspired algorithm (in the case of transductivesettings). Using standardized vision benchmarks, we prove the ability of theproposed methodology to achieve state-of-the-art accuracy with variousdatasets, backbone architectures and few-shot settings."
Unsupervised Transfer Learning with Self-Supervised Remedy,"['Jiabo Huang', 'Shaogang Gong']",http://arxiv.org/pdf/2006.04737v1.pdf,2020-06-08,['cs.cv'],"  Generalising deep networks to novel domains without manual labels ischallenging to deep learning. This problem is intrinsically difficult due tounpredictable changing nature of imagery data distributions in novel domains.Pre-learned knowledge does not transfer well without making strong assumptionsabout the learned and the novel domains. Different methods have been studied toaddress the underlying problem based on different assumptions, e.g. from domainadaptation to zero-shot and few-shot learning. In this work, we address thisproblem by transfer clustering that aims to learn a discriminative latent spaceof the unlabelled target data in a novel domain by knowledge transfer fromlabelled related domains. Specifically, we want to leverage relative (pairwise)imagery information, which is freely available and intrinsic to a targetdomain, to model the target domain image distribution characteristics as wellas the prior-knowledge learned from related labelled domains to enable morediscriminative clustering of unlabelled target data. Our method mitigatesnontransferrable prior-knowledge by self-supervision, benefiting from bothtransfer and self-supervised learning. Extensive experiments on four datasetsfor image clustering tasks reveal the superiority of our model over thestate-of-the-art transfer clustering techniques. We further demonstrate itscompetitive transferability on four zero-shot learning benchmarks."
Calibrated neighborhood aware confidence measure for deep metric  learning,"['Maryna Karpusha', 'Sunghee Yun', 'Istvan Fehervari']",http://arxiv.org/pdf/2006.04935v1.pdf,2020-06-08,"['cs.lg', 'stat.ml']","  Deep metric learning has gained promising improvement in recent yearsfollowing the success of deep learning. It has been successfully applied toproblems in few-shot learning, image retrieval, and open-set classifications.However, measuring the confidence of a deep metric learning model andidentifying unreliable predictions is still an open challenge. This paperfocuses on defining a calibrated and interpretable confidence metric thatclosely reflects its classification accuracy. While performing similaritycomparison directly in the latent space using the learned distance metric, ourapproach approximates the distribution of data points for each class using aGaussian kernel smoothing function. The post-processing calibration algorithmwith proposed confidence metric on the held-out validation dataset improvesgeneralization and robustness of state-of-the-art deep metric learning modelswhile provides an interpretable estimation of the confidence. Extensive testson four popular benchmark datasets (Caltech-UCSD Birds, Stanford OnlineProduct, Stanford Car-196, and In-shop Clothes Retrieval) show consistentimprovements even at the presence of distribution shifts in test data relatedto additional noise or adversarial examples."
Few-shot Slot Tagging with Collapsed Dependency Transfer and  Label-enhanced Task-adaptive Projection Network,"['Yutai Hou', 'Wanxiang Che', 'Yongkui Lai', 'Zhihan Zhou', 'Yijia Liu', 'Han Liu', 'Ting Liu']",http://arxiv.org/pdf/2006.05702v1.pdf,2020-06-10,"['cs.cl', 'cs.lg']","  In this paper, we explore the slot tagging with only a few labeled supportsentences (a.k.a. few-shot). Few-shot slot tagging faces a unique challengecompared to the other few-shot classification problems as it calls for modelingthe dependencies between labels. But it is hard to apply previously learnedlabel dependencies to an unseen domain, due to the discrepancy of label sets.To tackle this, we introduce a collapsed dependency transfer mechanism into theconditional random field (CRF) to transfer abstract label dependency patternsas transition scores. In the few-shot setting, the emission score of CRF can becalculated as a word's similarity to the representation of each label. Tocalculate such similarity, we propose a Label-enhanced Task-Adaptive ProjectionNetwork (L-TapNet) based on the state-of-the-art few-shot classification model-- TapNet, by leveraging label name semantics in representing labels.Experimental results show that our model significantly outperforms thestrongest few-shot learning baseline by 14.64 F1 scores in the one-shotsetting."
Learning to Learn Kernels with Variational Random Features,"['Xiantong Zhen', 'Haoliang Sun', 'Yingjun Du', 'Jun Xu', 'Yilong Yin', 'Ling Shao', 'Cees Snoek']",http://arxiv.org/pdf/2006.06707v2.pdf,2020-06-11,"['cs.lg', 'stat.ml']","  In this work, we introduce kernels with random Fourier features in themeta-learning framework to leverage their strong few-shot learning ability. Wepropose meta variational random features (MetaVRF) to learn adaptive kernelsfor the base-learner, which is developed in a latent variable model by treatingthe random feature basis as the latent variable. We formulate the optimizationof MetaVRF as a variational inference problem by deriving an evidence lowerbound under the meta-learning framework. To incorporate shared knowledge fromrelated tasks, we propose a context inference of the posterior, which isestablished by an LSTM architecture. The LSTM-based inference network caneffectively integrate the context information of previous tasks withtask-specific information, generating informative and adaptive features. Thelearned MetaVRF can produce kernels of high representational power with arelatively low spectral sampling rate and also enables fast adaptation to newtasks. Experimental results on a variety of few-shot regression andclassification tasks demonstrate that MetaVRF delivers much better, or at leastcompetitive, performance compared to existing meta-learning alternatives."
Predictive Complexity Priors,"['Eric Nalisnick', 'Jonathan Gordon', 'José Miguel Hernández-Lobato']",http://arxiv.org/pdf/2006.10801v3.pdf,2020-06-18,"['stat.ml', 'cs.lg']","  Specifying a Bayesian prior is notoriously difficult for complex models suchas neural networks. Reasoning about parameters is made challenging by thehigh-dimensionality and over-parameterization of the space. Priors that seembenign and uninformative can have unintuitive and detrimental effects on amodel's predictions. For this reason, we propose predictive complexity priors:a functional prior that is defined by comparing the model's predictions tothose of a reference model. Although originally defined on the model outputs,we transfer the prior to the model parameters via a change of variables. Thetraditional Bayesian workflow can then proceed as usual. We apply ourpredictive complexity prior to high-dimensional regression, reasoning overneural network depth, and sharing of statistical strength for few-shotlearning."
Self-Supervised Prototypical Transfer Learning for Few-Shot  Classification,"['Carlos Medina', 'Arnout Devos', 'Matthias Grossglauser']",http://arxiv.org/pdf/2006.11325v1.pdf,2020-06-19,"['cs.lg', 'cs.cv', 'stat.ml']","  Most approaches in few-shot learning rely on costly annotated data related tothe goal task domain during (pre-)training. Recently, unsupervisedmeta-learning methods have exchanged the annotation requirement for a reductionin few-shot classification performance. Simultaneously, in settings withrealistic domain shift, common transfer learning has been shown to outperformsupervised meta-learning. Building on these insights and on advances inself-supervised learning, we propose a transfer learning approach whichconstructs a metric embedding that clusters unlabeled prototypical samples andtheir augmentations closely together. This pre-trained embedding is a startingpoint for few-shot classification by summarizing class clusters andfine-tuning. We demonstrate that our self-supervised prototypical transferlearning approach ProtoTransfer outperforms state-of-the-art unsupervisedmeta-learning methods on few-shot tasks from the mini-ImageNet dataset. Infew-shot experiments with domain shift, our approach even has comparableperformance to supervised methods, but requires orders of magnitude fewerlabels."
Enhancing Few-Shot Image Classification with Unlabelled Examples,"['Peyman Bateni', 'Jarred Barber', 'Jan-Willem van de Meent', 'Frank Wood']",http://arxiv.org/pdf/2006.12245v6.pdf,2020-06-17,"['cs.cv', 'cs.lg', 'stat.ml']","  We develop a transductive meta-learning method that uses unlabelled instancesto improve few-shot image classification performance. Our approach combines aregularized Mahalanobis-distance-based soft k-means clustering procedure with amodified state of the art neural adaptive feature extractor to achieve improvedtest-time classification accuracy using unlabelled data. We evaluate our methodon transductive few-shot learning tasks, in which the goal is to jointlypredict labels for query (test) examples given a set of support (training)examples. We achieve state of the art performance on the Meta-Dataset,mini-ImageNet and tiered-ImageNet benchmarks. All trained models and code havebeen made publicly available at github.com/plai-group/simple-cnaps."
"Deep Learning of Unified Region, Edge, and Contour Models for Automated  Image Segmentation",['Ali Hatamizadeh'],http://arxiv.org/pdf/2006.12706v1.pdf,2020-06-23,['cs.cv'],"  Image segmentation is a fundamental and challenging problem in computervision with applications spanning multiple areas, such as medical imaging,remote sensing, and autonomous vehicles. Recently, convolutional neuralnetworks (CNNs) have gained traction in the design of automated segmentationpipelines. Although CNN-based models are adept at learning abstract featuresfrom raw image data, their performance is dependent on the availability andsize of suitable training datasets. Additionally, these models are often unableto capture the details of object boundaries and generalize poorly to unseenclasses. In this thesis, we devise novel methodologies that address theseissues and establish robust representation learning frameworks forfully-automatic semantic segmentation in medical imaging and mainstreamcomputer vision. In particular, our contributions include (1) state-of-the-art2D and 3D image segmentation networks for computer vision and medical imageanalysis, (2) an end-to-end trainable image segmentation framework that unifiesCNNs and active contour models with learnable parameters for fast and robustobject delineation, (3) a novel approach for disentangling edge and textureprocessing in segmentation networks, and (4) a novel few-shot learning model inboth supervised settings and semi-supervised settings where synergies betweenlatent and image spaces are leveraged to learn to segment images given limitedtraining data."
Global Convergence and Generalization Bound of Gradient-Based  Meta-Learning with Deep Neural Nets,"['Haoxiang Wang', 'Ruoyu Sun', 'Bo Li']",http://arxiv.org/pdf/2006.14606v2.pdf,2020-06-25,"['cs.lg', 'stat.ml']","  Gradient-based meta-learning (GBML) with deep neural nets (DNNs) has become apopular approach for few-shot learning. However, due to the non-convexity ofDNNs and the bi-level optimization in GBML, the theoretical properties of GBMLwith DNNs remain largely unknown. In this paper, we first aim to answer thefollowing question: Does GBML with DNNs have global convergence guarantees? Weprovide a positive answer to this question by proving that GBML withover-parameterized DNNs is guaranteed to converge to global optima at a linearrate. The second question we aim to address is: How does GBML achieve fastadaption to new tasks with prior experience on past tasks? To answer it, wetheoretically show that GBML is equivalent to a functional gradient descentoperation that explicitly propagates experience from the past tasks to newones, and then we prove a generalization error bound of GBML withover-parameterized DNNs."
Meta-DRN: Meta-Learning for 1-Shot Image Segmentation,['Atmadeep Banerjee'],http://arxiv.org/pdf/2008.00247v1.pdf,2020-08-01,"['cs.cv', 'cs.lg', 'eess.iv']","  Modern deep learning models have revolutionized the field of computer vision.But, a significant drawback of most of these models is that they require alarge number of labelled examples to generalize properly. Recent developmentsin few-shot learning aim to alleviate this requirement. In this paper, wepropose a novel lightweight CNN architecture for 1-shot image segmentation. Theproposed model is created by taking inspiration from well-performingarchitectures for semantic segmentation and adapting it to the 1-shot domain.We train our model using 4 meta-learning algorithms that have worked well forimage classification and compare the results. For the chosen dataset, ourproposed model has a 70% lower parameter count than the benchmark, while havingbetter or comparable mean IoU scores using all 4 of the meta-learningalgorithms."
Few-Shot Drum Transcription in Polyphonic Music,"['Yu Wang', 'Justin Salamon', 'Mark Cartwright', 'Nicholas J. Bryan', 'Juan Pablo Bello']",http://arxiv.org/pdf/2008.02791v1.pdf,2020-08-06,"['cs.sd', 'eess.as']","  Data-driven approaches to automatic drum transcription (ADT) are oftenlimited to a predefined, small vocabulary of percussion instrument classes.Such models cannot recognize out-of-vocabulary classes nor are they able toadapt to finer-grained vocabularies. In this work, we address open vocabularyADT by introducing few-shot learning to the task. We train a PrototypicalNetwork on a synthetic dataset and evaluate the model on multiple real-worldADT datasets with polyphonic accompaniment. We show that, given just a handfulof selected examples at inference time, we can match and in some casesoutperform a state-of-the-art supervised ADT approach under a fixed vocabularysetting. At the same time, we show that our model can successfully generalizeto finer-grained or extended vocabularies unseen during training, a scenariowhere supervised approaches cannot operate at all. We provide a detailedanalysis of our experimental results, including a breakdown of performance bysound class and by polyphony."
Cooperative Bi-path Metric for Few-shot Learning,"['Zeyuan Wang', 'Yifan Zhao', 'Jia Li', 'Yonghong Tian']",http://arxiv.org/pdf/2008.04031v1.pdf,2020-08-10,['cs.cv'],"  Given base classes with sufficient labeled samples, the target of few-shotclassification is to recognize unlabeled samples of novel classes with only afew labeled samples. Most existing methods only pay attention to therelationship between labeled and unlabeled samples of novel classes, which donot make full use of information within base classes. In this paper, we maketwo contributions to investigate the few-shot classification problem. First, wereport a simple and effective baseline trained on base classes in the way oftraditional supervised learning, which can achieve comparable results to thestate of the art. Second, based on the baseline, we propose a cooperativebi-path metric for classification, which leverages the correlations betweenbase classes and novel classes to further improve the accuracy. Experiments ontwo widely used benchmarks show that our method is a simple and effectiveframework, and a new state of the art is established in the few-shotclassification field."
Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems,"['Andrea Madotto', 'Zihan Liu', 'Zhaojiang Lin', 'Pascale Fung']",http://arxiv.org/pdf/2008.06239v2.pdf,2020-08-14,"['cs.cl', 'cs.lg']","  Task-oriented dialogue systems use four connected modules, namely, NaturalLanguage Understanding (NLU), a Dialogue State Tracking (DST), Dialogue Policy(DP) and Natural Language Generation (NLG). A research challenge is to learneach module with the least amount of samples (i.e., few-shots) given the highcost related to the data collection. The most common and effective technique tosolve this problem is transfer learning, where large language models, eitherpre-trained on text or task-specific data, are fine-tuned on the few samples.These methods require fine-tuning steps and a set of parameters for each task.Differently, language models, such as GPT-2 (Radford et al., 2019) and GPT-3(Brown et al., 2020), allow few-shot learning by priming the model with fewexamples. In this paper, we evaluate the priming few-shot ability of languagemodels in the NLU, DST, DP and NLG tasks. Importantly, we highlight the currentlimitations of this approach, and we discuss the possible implication forfuture work."
Compositional Generalization via Neural-Symbolic Stack Machines,"['Xinyun Chen', 'Chen Liang', 'Adams Wei Yu', 'Dawn Song', 'Denny Zhou']",http://arxiv.org/pdf/2008.06662v2.pdf,2020-08-15,"['cs.lg', 'cs.ai', 'stat.ml']","  Despite achieving tremendous success, existing deep learning models haveexposed limitations in compositional generalization, the capability to learncompositional rules and apply them to unseen cases in a systematic manner. Totackle this issue, we propose the Neural-Symbolic Stack Machine (NeSS). Itcontains a neural network to generate traces, which are then executed by asymbolic stack machine enhanced with sequence manipulation operations. NeSScombines the expressive power of neural sequence models with the recursionsupported by the symbolic stack machine. Without training supervision onexecution traces, NeSS achieves 100% generalization performance in fourdomains: the SCAN benchmark of language-driven navigation tasks, the task offew-shot learning of compositional instructions, the compositional machinetranslation benchmark, and context-free grammar parsing tasks."
Cross-Modality Multi-Atlas Segmentation Using Deep Neural Networks,"['Wangbin Ding', 'Lei Li', 'Xiahai Zhuang', 'Liqin Huang']",http://arxiv.org/pdf/2008.08946v1.pdf,2020-08-15,['cs.cv'],"  Both image registration and label fusion in the multi-atlas segmentation(MAS) rely on the intensity similarity between target and atlas images.However, such similarity can be problematic when target and atlas images areacquired using different imaging protocols. High-level structure informationcan provide reliable similarity measurement for cross-modality images whencooperating with deep neural networks (DNNs). This work presents a new MASframework for cross-modality images, where both image registration and labelfusion are achieved by DNNs. For image registration, we propose a consistentregistration network, which can jointly estimate forward and backward densedisplacement fields (DDFs). Additionally, an invertible constraint is employedin the network to reduce the correspondence ambiguity of the estimated DDFs.For label fusion, we adapt a few-shot learning network to measure thesimilarity of atlas and target patches. Moreover, the network can be seamlesslyintegrated into the patch-based label fusion. The proposed framework isevaluated on the MM-WHS dataset of MICCAI 2017. Results show that the frameworkis effective in both cross-modality registration and segmentation."
learn2learn: A Library for Meta-Learning Research,"['Sébastien M. R. Arnold', 'Praateek Mahajan', 'Debajyoti Datta', 'Ian Bunner', 'Konstantinos Saitas Zarkias']",http://arxiv.org/pdf/2008.12284v2.pdf,2020-08-27,"['cs.lg', 'cs.cv', 'cs.ro', 'stat.ml']","  Meta-learning researchers face two fundamental issues in their empiricalwork: prototyping and reproducibility. Researchers are prone to make mistakeswhen prototyping new algorithms and tasks because modern meta-learning methodsrely on unconventional functionalities of machine learning frameworks. In turn,reproducing existing results becomes a tedious endeavour -- a situationexacerbated by the lack of standardized implementations and benchmarks. As aresult, researchers spend inordinate amounts of time on implementing softwarerather than understanding and developing new ideas.  This manuscript introduces learn2learn, a library for meta-learning researchfocused on solving those prototyping and reproducibility issues. learn2learnprovides low-level routines common across a wide-range of meta-learningtechniques (e.g. meta-descent, meta-reinforcement learning, few-shot learning),and builds standardized interfaces to algorithms and benchmarks on top of them.In releasing learn2learn under a free and open source license, we hope tofoster a community around standardized software for meta-learning research."
All About Knowledge Graphs for Actions,"['Pallabi Ghosh', 'Nirat Saini', 'Larry S. Davis', 'Abhinav Shrivastava']",http://arxiv.org/pdf/2008.12432v1.pdf,2020-08-28,['cs.cv'],"  Current action recognition systems require large amounts of training data forrecognizing an action. Recent works have explored the paradigm of zero-shot andfew-shot learning to learn classifiers for unseen categories or categories withfew labels. Following similar paradigms in object recognition, these approachesutilize external sources of knowledge (eg. knowledge graphs from languagedomains). However, unlike objects, it is unclear what is the best knowledgerepresentation for actions. In this paper, we intend to gain a betterunderstanding of knowledge graphs (KGs) that can be utilized for zero-shot andfew-shot action recognition. In particular, we study three differentconstruction mechanisms for KGs: action embeddings, action-object embeddings,visual embeddings. We present extensive analysis of the impact of different KGsin different experimental setups. Finally, to enable a systematic study ofzero-shot and few-shot approaches, we propose an improved evaluation paradigmbased on UCF101, HMDB51, and Charades datasets for knowledge transfer frommodels trained on Kinetics."
Vector Projection Network for Few-shot Slot Tagging in Natural Language  Understanding,"['Su Zhu', 'Ruisheng Cao', 'Lu Chen', 'Kai Yu']",http://arxiv.org/pdf/2009.09568v2.pdf,2020-09-21,['cs.cl'],"  Few-shot slot tagging becomes appealing for rapid domain transfer andadaptation, motivated by the tremendous development of conversational dialoguesystems. In this paper, we propose a vector projection network for few-shotslot tagging, which exploits projections of contextual word embeddings on eachtarget label vector as word-label similarities. Essentially, this approach isequivalent to a normalized linear model with an adaptive bias. The contrastiveexperiment demonstrates that our proposed vector projection based similaritymetric can significantly surpass other variants. Specifically, in the five-shotsetting on benchmarks SNIPS and NER, our method outperforms the strongestfew-shot learning baseline by $6.30$ and $13.79$ points on F$_1$ score,respectively. Our code will be released athttps://github.com/sz128/few_shot_slot_tagging_and_NER."
A Few-shot Learning Approach for Historical Ciphered Manuscript  Recognition,"['Mohamed Ali Souibgui', 'Alicia Fornés', 'Yousri Kessentini', 'Crina Tudor']",http://arxiv.org/pdf/2009.12577v1.pdf,2020-09-26,['cs.cv'],"  Encoded (or ciphered) manuscripts are a special type of historical documentsthat contain encrypted text. The automatic recognition of this kind ofdocuments is challenging because: 1) the cipher alphabet changes from onedocument to another, 2) there is a lack of annotated corpus for training and 3)touching symbols make the symbol segmentation difficult and complex. Toovercome these difficulties, we propose a novel method for handwritten ciphersrecognition based on few-shot object detection. Our method first detects allsymbols of a given alphabet in a line image, and then a decoding step maps thesymbol similarity scores to the final sequence of transcribed symbols. Bytraining on synthetic data, we show that the proposed architecture is able torecognize handwritten ciphers with unseen alphabets. In addition, if fewlabeled pages with the same alphabet are used for fine tuning, our methodsurpasses existing unsupervised and supervised HTR methods for ciphersrecognition."
Message Passing Neural Processes,"['Ben Day', 'Cătălina Cangea', 'Arian R. Jamasb', 'Pietro Liò']",http://arxiv.org/pdf/2009.13895v1.pdf,2020-09-29,"['cs.lg', 'cs.ai', 'stat.ml']","  Neural Processes (NPs) are powerful and flexible models able to incorporateuncertainty when representing stochastic processes, while maintaining a lineartime complexity. However, NPs produce a latent description by aggregatingindependent representations of context points and lack the ability to exploitrelational information present in many datasets. This renders NPs ineffectivein settings where the stochastic process is primarily governed by neighbourhoodrules, such as cellular automata (CA), and limits performance for any taskwhere relational information remains unused. We address this shortcoming byintroducing Message Passing Neural Processes (MPNPs), the first class of NPsthat explicitly makes use of relational structure within the model. Ourevaluation shows that MPNPs thrive at lower sampling rates, on existingbenchmarks and newly-proposed CA and Cora-Branched tasks. We further reportstrong generalisation over density-based CA rule-sets and significant gains inchallenging arbitrary-labelling and few-shot learning setups."
Cross-Lingual Transfer Learning for Complex Word Identification,"['George-Eduard Zaharia', 'Dumitru-Clementin Cercel', 'Mihai Dascalu']",http://arxiv.org/pdf/2010.01108v1.pdf,2020-10-02,['cs.cl'],"  Complex Word Identification (CWI) is a task centered on detectinghard-to-understand words, or groups of words, in texts from different areas ofexpertise. The purpose of CWI is to highlight problematic structures thatnon-native speakers would usually find difficult to understand. Our approachuses zero-shot, one-shot, and few-shot learning techniques, alongsidestate-of-the-art solutions for Natural Language Processing (NLP) tasks (i.e.,Transformers). Our aim is to provide evidence that the proposed models canlearn the characteristics of complex words in a multilingual environment byrelying on the CWI shared task 2018 dataset available for four differentlanguages (i.e., English, German, Spanish, and also French). Our approachsurpasses state-of-the-art cross-lingual results in terms of macro F1-score onEnglish (0.774), German (0.782), and Spanish (0.734) languages, for thezero-shot learning scenario. At the same time, our model also outperforms thestate-of-the-art monolingual result for German (0.795 macro F1-score)."
Improving Few-Shot Learning through Multi-task Representation Learning  Theory,"['Quentin Bouniot', 'Ievgen Redko', 'Romaric Audigier', 'Angélique Loesch', 'Amaury Habrard']",http://arxiv.org/pdf/2010.01992v3.pdf,2020-10-05,"['cs.lg', 'cs.ai', 'cs.cv', 'stat.ml']","  In this paper, we consider the framework of multi-task representation (MTR)learning where the goal is to use source tasks to learn a representation thatreduces the sample complexity of solving a target task. We start by reviewingrecent advances in MTR theory and show that they can provide novel insights forpopular meta-learning algorithms when analyzed within this framework. Inparticular, we highlight a fundamental difference between gradient-based andmetric-based algorithms in practice and put forward a theoretical analysis toexplain it. Finally, we use the derived insights to improve the performance ofmeta-learning methods via a new spectral-based regularization term and confirmits efficiency through experimental studies on few-shot classificationbenchmarks. To the best of our knowledge, this is the first contribution thatputs the most recent learning bounds of MTR theory into practice for the taskof few-shot classification."
Self-training Improves Pre-training for Natural Language Understanding,"['Jingfei Du', 'Edouard Grave', 'Beliz Gunel', 'Vishrav Chaudhary', 'Onur Celebi', 'Michael Auli', 'Ves Stoyanov', 'Alexis Conneau']",http://arxiv.org/pdf/2010.02194v1.pdf,2020-10-05,['cs.cl'],"  Unsupervised pre-training has led to much recent progress in natural languageunderstanding. In this paper, we study self-training as another way to leverageunlabeled data through semi-supervised learning. To obtain additional data fora specific task, we introduce SentAugment, a data augmentation method whichcomputes task-specific query embeddings from labeled data to retrieve sentencesfrom a bank of billions of unlabeled sentences crawled from the web. Unlikeprevious semi-supervised methods, our approach does not require in-domainunlabeled data and is therefore more generally applicable. Experiments showthat self-training is complementary to strong RoBERTa baselines on a variety oftasks. Our augmentation approach leads to scalable and effective self-trainingwith improvements of up to 2.6% on standard text classification benchmarks.Finally, we also show strong gains on knowledge-distillation and few-shotlearning."
Dynamic Semantic Matching and Aggregation Network for Few-shot Intent  Detection,"['Hoang Nguyen', 'Chenwei Zhang', 'Congying Xia', 'Philip S. Yu']",http://arxiv.org/pdf/2010.02481v2.pdf,2020-10-06,"['cs.cl', 'cs.lg']","  Few-shot Intent Detection is challenging due to the scarcity of availableannotated utterances. Although recent works demonstrate that multi-levelmatching plays an important role in transferring learned knowledge from seentraining classes to novel testing classes, they rely on a static similaritymeasure and overly fine-grained matching components. These limitations inhibitgeneralizing capability towards Generalized Few-shot Learning settings whereboth seen and novel classes are co-existent. In this paper, we propose a novelSemantic Matching and Aggregation Network where semantic components aredistilled from utterances via multi-head self-attention with additional dynamicregularization constraints. These semantic components capture high-levelinformation, resulting in more effective matching between instances. Ourmulti-perspective matching method provides a comprehensive matching measure toenhance representations of both labeled and unlabeled instances. We alsopropose a more challenging evaluation setting that considers classification onthe joint all-class label space. Extensive experimental results demonstrate theeffectiveness of our method. Our code and data are publicly available."
Representation learning from videos in-the-wild: An object-centric  approach,"['Rob Romijnders', 'Aravindh Mahendran', 'Michael Tschannen', 'Josip Djolonga', 'Marvin Ritter', 'Neil Houlsby', 'Mario Lucic']",http://arxiv.org/pdf/2010.02808v2.pdf,2020-10-06,['cs.cv'],"  We propose a method to learn image representations from uncurated videos. Wecombine a supervised loss from off-the-shelf object detectors andself-supervised losses which naturally arise from the video-shot-frame-objecthierarchy present in each video. We report competitive results on 19 transferlearning tasks of the Visual Task Adaptation Benchmark (VTAB), and on 8out-of-distribution-generalization tasks, and discuss the benefits andshortcomings of the proposed approach. In particular, it improves over thebaseline on all 18/19 few-shot learning tasks and 8/8 out-of-distributiongeneralization tasks. Finally, we perform several ablation studies and analyzethe impact of the pretrained object detector on the performance across thissuite of tasks."
Learning Clusterable Visual Features for Zero-Shot Recognition,"['Jingyi Xu', 'Zhixin Shu', 'Dimitris Samaras']",http://arxiv.org/pdf/2010.03245v2.pdf,2020-10-07,['cs.cv'],"  In zero-shot learning (ZSL), conditional generators have been widely used togenerate additional training features. These features can then be used to trainthe classifiers for testing data. However, some testing data are considered""hard"" as they lie close to the decision boundaries and are prone tomisclassification, leading to performance degradation for ZSL. In this paper,we propose to learn clusterable features for ZSL problems. Using a ConditionalVariational Autoencoder (CVAE) as the feature generator, we project theoriginal features to a new feature space supervised by an auxiliaryclassification loss. To further increase clusterability, we fine-tune thefeatures using Gaussian similarity loss. The clusterable visual features arenot only more suitable for CVAE reconstruction but are also more separablewhich improves classification accuracy. Moreover, we introduce Gaussian noiseto enlarge the intra-class variance of the generated features, which helps toimprove the classifier's robustness. Our experiments on SUN,CUB, and AWA2datasets show consistent improvement over previous state-of-the-art ZSL resultsby a large margin. In addition to its effectiveness on zero-shotclassification, experiments show that our method to increase featureclusterability benefits few-shot learning algorithms as well."
Addressing the Real-world Class Imbalance Problem in Dermatology,"['Wei-Hung Weng', 'Jonathan Deaton', 'Vivek Natarajan', 'Gamaleldin F. Elsayed', 'Yuan Liu']",http://arxiv.org/pdf/2010.04308v2.pdf,2020-10-09,"['cs.cv', 'cs.lg']","  Class imbalance is a common problem in medical diagnosis, causing a standardclassifier to be biased towards the common classes and perform poorly on therare classes. This is especially true for dermatology, a specialty withthousands of skin conditions but many of which have low prevalence in the realworld. Motivated by recent advances, we explore few-shot learning methods aswell as conventional class imbalance techniques for the skin conditionrecognition problem and propose an evaluation setup to fairly assess thereal-world utility of such approaches. We find the performance of few-showlearning methods does not reach that of conventional class imbalancetechniques, but combining the two approaches using a novel ensemble improvesmodel performance, especially for rare classes. We conclude that ensembling canbe useful to address the class imbalance problem, yet progress can further beaccelerated by real-world evaluation setups for benchmarking new methods."
Few-shot Learning for Multi-label Intent Detection,"['Yutai Hou', 'Yongkui Lai', 'Yushan Wu', 'Wanxiang Che', 'Ting Liu']",http://arxiv.org/pdf/2010.05256v1.pdf,2020-10-11,"['cs.cl', 'cs.ai']","  In this paper, we study the few-shot multi-label classification for userintent detection. For multi-label intent detection, state-of-the-art workestimates label-instance relevance scores and uses a threshold to selectmultiple associated intent labels. To determine appropriate thresholds withonly a few examples, we first learn universal thresholding experience ondata-rich domains, and then adapt the thresholds to certain few-shot domainswith a calibration based on nonparametric learning. For better calculation oflabel-instance relevance score, we introduce label name embedding as anchorpoints in representation space, which refines representations of differentclasses to be well-separated from each other. Experiments on two datasets showthat the proposed model significantly outperforms strong baselines in bothone-shot and five-shot settings."
The Tatoeba Translation Challenge -- Realistic Data Sets for Low  Resource and Multilingual MT,['Jörg Tiedemann'],http://arxiv.org/pdf/2010.06354v1.pdf,2020-10-13,['cs.cl'],"  This paper describes the development of a new benchmark for machinetranslation that provides training and test data for thousands of languagepairs covering over 500 languages and tools for creating state-of-the-arttranslation models from that collection. The main goal is to trigger thedevelopment of open translation tools and models with a much broader coverageof the World's languages. Using the package it is possible to work on realisticlow-resource scenarios avoiding artificially reduced setups that are commonwhen demonstrating zero-shot or few-shot learning. For the first time, thispackage provides a comprehensive collection of diverse data sets in hundreds oflanguages with systematic language and script annotation and data splits toextend the narrow coverage of existing benchmarks. Together with the datarelease, we also provide a growing number of pre-trained baseline models forindividual language pairs and selected language groups."
Theoretical bounds on estimation error for meta-learning,"['James Lucas', 'Mengye Ren', 'Irene Kameni', 'Toniann Pitassi', 'Richard Zemel']",http://arxiv.org/pdf/2010.07140v1.pdf,2020-10-14,"['stat.ml', 'cs.lg', 'math.st', 'stat.th']","  Machine learning models have traditionally been developed under theassumption that the training and test distributions match exactly. However,recent success in few-shot learning and related problems are encouraging signsthat these models can be adapted to more realistic settings where train andtest distributions differ. Unfortunately, there is severely limited theoreticalsupport for these algorithms and little is known about the difficulty of theseproblems. In this work, we provide novel information-theoretic lower-bounds onminimax rates of convergence for algorithms that are trained on data frommultiple sources and tested on novel data. Our bounds depend intuitively on theinformation shared between sources of data, and characterize the difficulty oflearning in this setting for arbitrary algorithms. We demonstrate these boundson a hierarchical Bayesian model of meta-learning, computing both upper andlower bounds on parameter estimation via maximum-a-posteriori inference."
Self-training for Few-shot Transfer Across Extreme Task Differences,"['Cheng Perng Phoo', 'Bharath Hariharan']",http://arxiv.org/pdf/2010.07734v2.pdf,2020-10-15,"['cs.cv', 'cs.ai', 'cs.lg']","  Most few-shot learning techniques are pre-trained on a large, labeled ""basedataset"". In problem domains where such large labeled datasets are notavailable for pre-training (e.g., X-ray, satellite images), one must resort topre-training in a different ""source"" problem domain (e.g., ImageNet), which canbe very different from the desired target task. Traditional few-shot andtransfer learning techniques fail in the presence of such extreme differencesbetween the source and target tasks. In this paper, we present a simple andeffective solution to tackle this extreme domain gap: self-training a sourcedomain representation on unlabeled data from the target domain. We show thatthis improves one-shot performance on the target domain by 2.9 points onaverage on the challenging BSCD-FSL benchmark consisting of datasets frommultiple domains. Our code is available at https://github.com/cpphoo/STARTUP."
ALPaCA vs. GP-based Prior Learning: A Comparison between two Bayesian  Meta-Learning Algorithms,['Yilun Wu'],http://arxiv.org/pdf/2010.07994v1.pdf,2020-10-15,['cs.lg'],"  Meta-learning or few-shot learning, has been successfully applied in a widerange of domains from computer vision to reinforcement learning. Among the manyframeworks proposed for meta-learning, bayesian methods are particularlyfavoured when accurate and calibrated uncertainty estimate is required. In thispaper, we investigate the similarities and disparities among two recentlypublished bayesian meta-learning methods: ALPaCA (Harrison et al. [2018]) andPACOH (Rothfuss et al. [2020]). We provide theoretical analysis as well asempirical benchmarks across synthetic and real-world dataset. While ALPaCAholds advantage in computation time by the usage of a linear kernel, generalGP-based methods provide much more flexibility and achieves better resultacross datasets when using a common kernel such as SE (Squared Exponential)kernel. The influence of different loss function choice is also discussed."
Directed Variational Cross-encoder Network for Few-shot Multi-image  Co-segmentation,"['Sayan Banerjee', 'S Divakar Bhat', 'Subhasis Chaudhuri', 'Rajbabu Velmurugan']",http://arxiv.org/pdf/2010.08800v1.pdf,2020-10-17,['cs.cv'],"  In this paper, we propose a novel framework for multi-image co-segmentationusing class agnostic meta-learning strategy by generalizing to new classesgiven only a small number of training samples for each new class. We havedeveloped a novel encoder-decoder network termed as DVICE (Directed VariationalInference Cross Encoder), which learns a continuous embedding space to ensurebetter similarity learning. We employ a combination of the proposed DVICEnetwork and a novel few-shot learning approach to tackle the small sample sizeproblem encountered in co-segmentation with small datasets like iCoseg andMSRC. Furthermore, the proposed framework does not use any semantic classlabels and is entirely class agnostic. Through exhaustive experimentation overmultiple datasets using only a small volume of training data, we havedemonstrated that our approach outperforms all existing state-of-the-arttechniques."
Word Frequency Does Not Predict Grammatical Knowledge in Language Models,"['Charles Yu', 'Ryan Sie', 'Nico Tedeschi', 'Leon Bergen']",http://arxiv.org/pdf/2010.13870v1.pdf,2020-10-26,['cs.cl'],"  Neural language models learn, to varying degrees of accuracy, the grammaticalproperties of natural languages. In this work, we investigate whether there aresystematic sources of variation in the language models' accuracy. Focusing onsubject-verb agreement and reflexive anaphora, we find that certain nouns aresystematically understood better than others, an effect which is robust acrossgrammatical tasks and different language models. Surprisingly, we find thatacross four orders of magnitude, corpus frequency is unrelated to a noun'sperformance on grammatical tasks. Finally, we find that a novel noun'sgrammatical properties can be few-shot learned from various types of trainingdata. The results present a paradox: there should be less variation ingrammatical performance than is actually observed."
Spatial Contrastive Learning for Few-Shot Classification,"['Yassine Ouali', 'Céline Hudelot', 'Myriam Tami']",http://arxiv.org/pdf/2012.13831v3.pdf,2020-12-26,"['cs.cv', 'cs.lg']","  In this paper, we explore contrastive learning for few-shot classification,in which we propose to use it as an additional auxiliary training objectiveacting as a data-dependent regularizer to promote more general and transferablefeatures. In particular, we present a novel attention-based spatial contrastiveobjective to learn locally discriminative and class-agnostic features. As aresult, our approach overcomes some of the limitations of the cross-entropyloss, such as its excessive discrimination towards seen classes, which reducesthe transferability of features to unseen classes. With extensive experiments,we show that the proposed method outperforms state-of-the-art approaches,confirming the importance of learning good and transferable embeddings forfew-shot learning."
Compositional Prototype Network with Multi-view Comparision for Few-Shot  Point Cloud Semantic Segmentation,"['Xiaoyu Chen', 'Chi Zhang', 'Guosheng Lin', 'Jing Han']",http://arxiv.org/pdf/2012.14255v1.pdf,2020-12-28,['cs.cv'],"  Point cloud segmentation is a fundamental visual understanding task in 3Dvision. A fully supervised point cloud segmentation network often requires alarge amount of data with point-wise annotations, which is expensive to obtain.In this work, we present the Compositional Prototype Network that can undertakepoint cloud segmentation with only a few labeled training data. Inspired by thefew-shot learning literature in images, our network directly transfers labelinformation from the limited training data to unlabeled test data forprediction. The network decomposes the representations of complex point clouddata into a set of local regional representations and utilizes them tocalculate the compositional prototypes of a visual concept. Our networkincludes a key Multi-View Comparison Component that exploits the redundantviews of the support set. To evaluate the proposed method, we create a newsegmentation benchmark dataset, ScanNet-$6^i$, which is built upon ScanNetdataset. Extensive experiments show that our method outperforms baselines witha significant advantage. Moreover, when we use our network to handle thelong-tail problem in a fully supervised point cloud segmentation dataset, itcan also effectively boost the performance of the few-shot classes."
Few-Shot Named Entity Recognition: A Comprehensive Study,"['Jiaxin Huang', 'Chunyuan Li', 'Krishan Subudhi', 'Damien Jose', 'Shobana Balakrishnan', 'Weizhu Chen', 'Baolin Peng', 'Jianfeng Gao', 'Jiawei Han']",http://arxiv.org/pdf/2012.14978v1.pdf,2020-12-29,"['cs.cl', 'cs.ir', 'cs.lg']","  This paper presents a comprehensive study to efficiently build named entityrecognition (NER) systems when a small number of in-domain labeled data isavailable. Based upon recent Transformer-based self-supervised pre-trainedlanguage models (PLMs), we investigate three orthogonal schemes to improve themodel generalization ability for few-shot settings: (1) meta-learning toconstruct prototypes for different entity types, (2) supervised pre-training onnoisy web data to extract entity-related generic representations and (3)self-training to leverage unlabeled in-domain data. Different combinations ofthese schemes are also considered. We perform extensive empirical comparisonson 10 public NER datasets with various proportions of labeled data, suggestinguseful insights for future research. Our experiments show that (i) in thefew-shot learning setting, the proposed NER schemes significantly improve oroutperform the commonly used baseline, a PLM-based linear classifier fine-tunedon domain labels; (ii) We create new state-of-the-art results on both few-shotand training-free settings compared with existing methods. We will release ourcode and pre-trained models for reproducible research."
Few-shot Image Classification with Multi-Facet Prototypes,"['Kun Yan', 'Zied Bouraoui', 'Ping Wang', 'Shoaib Jameel', 'Steven Schockaert']",http://arxiv.org/pdf/2102.00801v1.pdf,2021-02-01,"['cs.cv', 'cs.ai']","  The aim of few-shot learning (FSL) is to learn how to recognize imagecategories from a small number of training examples. A central challenge isthat the available training examples are normally insufficient to determinewhich visual features are most characteristic of the considered categories. Toaddress this challenge, we organize these visual features into facets, whichintuitively group features of the same kind (e.g. features that are relevant toshape, color, or texture). This is motivated from the assumption that (i) theimportance of each facet differs from category to category and (ii) it ispossible to predict facet importance from a pre-trained embedding of thecategory names. In particular, we propose an adaptive similarity measure,relying on predicted facet importance weights for a given set of categories.This measure can be used in combination with a wide array of existingmetric-based methods. Experiments on miniImageNet and CUB show that ourapproach improves the state-of-the-art in metric-based FSL."
Neural Data Augmentation via Example Extrapolation,"['Kenton Lee', 'Kelvin Guu', 'Luheng He', 'Tim Dozat', 'Hyung Won Chung']",http://arxiv.org/pdf/2102.01335v1.pdf,2021-02-02,"['cs.cl', 'cs.ai']","  In many applications of machine learning, certain categories of examples maybe underrepresented in the training data, causing systems to underperform onsuch ""few-shot"" cases at test time. A common remedy is to perform dataaugmentation, such as by duplicating underrepresented examples, orheuristically synthesizing new examples. But these remedies often fail to coverthe full diversity and complexity of real examples.  We propose a data augmentation approach that performs neural ExampleExtrapolation (Ex2). Given a handful of exemplars sampled from somedistribution, Ex2 synthesizes new examples that also belong to the samedistribution. The Ex2 model is learned by simulating the example generationprocedure on data-rich slices of the data, and it is applied tounderrepresented, few-shot slices.  We apply Ex2 to a range of language understanding tasks and significantlyimprove over state-of-the-art methods on multiple few-shot learning benchmarks,including for relation extraction (FewRel) and intent classification + slotfilling (SNIPS)."
Few-shot time series segmentation using prototype-defined infinite  hidden Markov models,"['Yazan Qarout', 'Yordan P. Raykov', 'Max A. Little']",http://arxiv.org/pdf/2102.03885v1.pdf,2021-02-07,"['cs.lg', 'eess.sp', 'math.st', 'stat.th']","  We propose a robust framework for interpretable, few-shot analysis ofnon-stationary sequential data based on flexible graphical models to expressthe structured distribution of sequential events, using prototype radial basisfunction (RBF) neural network emissions. A motivational link is demonstratedbetween prototypical neural network architectures for few-shot learning and theproposed RBF network infinite hidden Markov model (RBF-iHMM). We show that RBFnetworks can be efficiently specified via prototypes allowing us to expresscomplex nonstationary patterns, while hidden Markov models are used to inferprincipled high-level Markov dynamics. The utility of the framework isdemonstrated on biomedical signal processing applications such as automatedseizure detection from EEG data where RBF networks achieve state-of-the-artperformance using a fraction of the data needed to train long-short-term memoryvariational autoencoders."
Multi-Objective Meta Learning,"['Feiyang Ye', 'Baijiong Lin', 'Zhixiong Yue', 'Pengxin Guo', 'Qiao Xiao', 'Yu Zhang']",http://arxiv.org/pdf/2102.07121v1.pdf,2021-02-14,"['cs.lg', 'cs.ai']","  Meta learning with multiple objectives can be formulated as a Multi-ObjectiveBi-Level optimization Problem (MOBLP) where the upper-level subproblem is tosolve several possible conflicting targets for the meta learner. However,existing studies either apply an inefficient evolutionary algorithm or linearlycombine multiple objectives as a single-objective problem with the need to tunecombination weights. In this paper, we propose a unified gradient-basedMulti-Objective Meta Learning (MOML) framework and devise the firstgradient-based optimization algorithm to solve the MOBLP by alternativelysolving the lower-level and upper-level subproblems via the gradient descentmethod and the gradient-based multi-objective optimization method,respectively. Theoretically, we prove the convergence properties of theproposed gradient-based optimization algorithm. Empirically, we show theeffectiveness of the proposed MOML framework in several meta learning problems,including few-shot learning, neural architecture search, domain adaptation, andmulti-task learning."
One-shot learning for the long term: consolidation with an artificial  hippocampal algorithm,"['Gideon Kowadlo', 'Abdelrahman Ahmed', 'David Rawlinson']",http://arxiv.org/pdf/2102.07503v2.pdf,2021-02-15,"['cs.lg', 'cs.ai', 'cs.ne', 'i.2.6; i.5.0; i.5.1']","  Standard few-shot experiments involve learning to efficiently matchpreviously unseen samples by class. We claim that few-shot learning should belong term, assimilating knowledge for the future, without forgetting previousconcepts. In the mammalian brain, the hippocampus is understood to play asignificant role in this process, by learning rapidly and consolidatingknowledge to the neocortex incrementally over a short period. In this researchwe tested whether an artificial hippocampal algorithm (AHA), could be used witha conventional Machine Learning (ML) model that learns incrementally analogousto the neocortex, to achieve one-shot learning both short and long term. Theresults demonstrated that with the addition of AHA, the system could learn inone-shot and consolidate the knowledge for the long term without catastrophicforgetting. This study is one of the first examples of using a CLS model ofhippocampus to consolidate memories, and it constitutes a step toward few-shotcontinual learning."
GP-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning,"['Idan Achituve', 'Aviv Navon', 'Yochai Yemini', 'Gal Chechik', 'Ethan Fetaya']",http://arxiv.org/pdf/2102.07868v4.pdf,2021-02-15,['cs.lg'],"  Gaussian processes (GPs) are non-parametric, flexible, models that work wellin many tasks. Combining GPs with deep learning methods via deep kernellearning (DKL) is especially compelling due to the strong representationalpower induced by the network. However, inference in GPs, whether with orwithout DKL, can be computationally challenging on large datasets. Here, wepropose GP-Tree, a novel method for multi-class classification with Gaussianprocesses and DKL. We develop a tree-based hierarchical model in which eachinternal node of the tree fits a GP to the data using the P\'olya Gammaaugmentation scheme. As a result, our method scales well with both the numberof classes and data size. We demonstrate the effectiveness of our methodagainst other Gaussian process training baselines, and we show how our generalGP approach achieves improved accuracy on standard incremental few-shotlearning benchmarks."
Semi Supervised Learning For Few-shot Audio Classification By Episodic  Triplet Mining,"['Swapnil Bhosale', 'Rupayan Chakraborty', 'Sunil Kumar Kopparapu']",http://arxiv.org/pdf/2102.08074v1.pdf,2021-02-16,"['cs.sd', 'cs.lg', 'eess.as']","  Few-shot learning aims to generalize unseen classes that appear duringtesting but are unavailable during training. Prototypical networks incorporatefew-shot metric learning, by constructing a class prototype in the form of amean vector of the embedded support points within a class. The performance ofprototypical networks in extreme few-shot scenarios (like one-shot) degradesdrastically, mainly due to the desuetude of variations within the clusterswhile constructing prototypes. In this paper, we propose to replace the typicalprototypical loss function with an Episodic Triplet Mining (ETM) technique. Theconventional triplet selection leads to overfitting, because of all possiblecombinations being used during training. We incorporate episodic training formining the semi hard positive and the semi hard negative triplets to overcomethe overfitting. We also propose an adaptation to make use of unlabeledtraining samples for better modeling. Experimenting on two different audioprocessing tasks, namely speaker recognition and audio event detection; showimproved performances and hence the efficacy of ETM over the prototypical lossfunction and other meta-learning frameworks. Further, we show improvedperformances when unlabeled training samples are used."
Towards Faithfulness in Open Domain Table-to-text Generation from an  Entity-centric View,"['Tianyu Liu', 'Xin Zheng', 'Baobao Chang', 'Zhifang Sui']",http://arxiv.org/pdf/2102.08585v1.pdf,2021-02-17,"['cs.cl', 'cs.ai']","  In open domain table-to-text generation, we notice that the unfaithfulgeneration usually contains hallucinated content which can not be aligned toany input table record. We thus try to evaluate the generation faithfulnesswith two entity-centric metrics: table record coverage and the ratio ofhallucinated entities in text, both of which are shown to have strong agreementwith human judgements. Then based on these metrics, we quantitatively analyzethe correlation between training data quality and generation fidelity whichindicates the potential usage of entity information in faithful generation.Motivated by these findings, we propose two methods for faithful generation: 1)augmented training by incorporating the auxiliary entity information, includingboth an augmented plan-based model and an unsupervised model and 2) traininginstance selection based on faithfulness ranking. We show these approachesimprove generation fidelity in both full dataset setting and few shot learningsettings by both automatic and human evaluations."
Calibrate Before Use: Improving Few-Shot Performance of Language Models,"['Tony Z. Zhao', 'Eric Wallace', 'Shi Feng', 'Dan Klein', 'Sameer Singh']",http://arxiv.org/pdf/2102.09690v2.pdf,2021-02-19,"['cs.cl', 'cs.lg']","  GPT-3 can perform numerous tasks when provided a natural language prompt thatcontains a few training examples. We show that this type of few-shot learningcan be unstable: the choice of prompt format, training examples, and even theorder of the training examples can cause accuracy to vary from near chance tonear state-of-the-art. We demonstrate that this instability arises from thebias of language models towards predicting certain answers, e.g., those thatare placed near the end of the prompt or are common in the pre-training data.To mitigate this, we first estimate the model's bias towards each answer byasking for its prediction when given the training prompt and a content-freetest input such as ""N/A"". We then fit calibration parameters that cause theprediction for this input to be uniform across answers. On a diverse set oftasks, this contextual calibration procedure substantially improves GPT-3 andGPT-2's average accuracy (up to 30.0% absolute) and reduces variance acrossdifferent choices of the prompt."
MetaDelta: A Meta-Learning System for Few-shot Image Classification,"['Yudong Chen', 'Chaoyu Guan', 'Zhikun Wei', 'Xin Wang', 'Wenwu Zhu']",http://arxiv.org/pdf/2102.10744v1.pdf,2021-02-22,"['cs.cv', 'cs.ai']","  Meta-learning aims at learning quickly on novel tasks with limited data bytransferring generic experience learned from previous tasks. Naturally,few-shot learning has been one of the most popular applications formeta-learning. However, existing meta-learning algorithms rarely consider thetime and resource efficiency or the generalization capacity for unknowndatasets, which limits their applicability in real-world scenarios. In thispaper, we propose MetaDelta, a novel practical meta-learning system for thefew-shot image classification. MetaDelta consists of two core components: i)multiple meta-learners supervised by a central controller to ensure efficiency,and ii) a meta-ensemble module in charge of integrated inference and bettergeneralization. In particular, each meta-learner in MetaDelta is composed of aunique pretrained encoder fine-tuned by batch training and parameter-freedecoder used for prediction. MetaDelta ranks first in the final phase in theAAAI 2021 MetaDLChallenge\footnote{https://competitions.codalab.org/competitions/26638},demonstrating the advantages of our proposed system. The codes are publiclyavailable at https://github.com/Frozenmad/MetaDelta."
Enabling the Network to Surf the Internet,"['Zhuoling Li', 'Haohan Wang', 'Tymoteusz Swistek', 'Weixin Chen', 'Yuanzheng Li', 'Haoqian Wang']",http://arxiv.org/pdf/2102.12205v1.pdf,2021-02-24,['cs.cv'],"  Few-shot learning is challenging due to the limited data and labels. Existingalgorithms usually resolve this problem by pre-training the model with aconsiderable amount of annotated data which shares knowledge with the targetdomain. Nevertheless, large quantities of homogenous data samples are notalways available. To tackle this issue, we develop a framework that enables themodel to surf the Internet, which implies that the model can collect andannotate data without manual effort. Since the online data is virtuallylimitless and continues to be generated, the model can thus be empowered toconstantly obtain up-to-date knowledge from the Internet. Additionally, weobserve that the generalization ability of the learned representation iscrucial for self-supervised learning. To present its importance, a naive yetefficient normalization strategy is proposed. Consequentially, this strategyboosts the accuracy of the model significantly (20.46% at most). We demonstratethe superiority of the proposed framework with experiments on miniImageNet,tieredImageNet and Omniglot. The results indicate that our method has surpassedprevious unsupervised counterparts by a large margin (more than 10%) andobtained performance comparable with the supervised ones."
Image Augmentation for Multitask Few-Shot Learning: Agricultural Domain  Use-Case,"['Sergey Nesteruk', 'Dmitrii Shadrin', 'Mariia Pukalchik']",http://arxiv.org/pdf/2102.12295v1.pdf,2021-02-24,"['cs.cv', 'cs.ai']","  Large datasets' availability is catalyzing a rapid expansion of deep learningin general and computer vision in particular. At the same time, in manydomains, a sufficient amount of training data is lacking, which may become anobstacle to the practical application of computer vision techniques. This paperchallenges small and imbalanced datasets based on the example of a plantphenomics domain. We introduce an image augmentation framework, which enablesus to extremely enlarge the number of training samples while providing the datafor such tasks as object detection, semantic segmentation, instancesegmentation, object counting, image denoising, and classification. We provethat our augmentation method increases model performance when only a fewtraining samples are available. In our experiment, we use the DeepLabV3 modelon semantic segmentation tasks with Arabidopsis and Nicotiana tabacum imagedataset. The obtained result shows a 9% relative increase in model performancecompared to the basic image augmentation techniques."
Domain Adaptation for Learning Generator from Paired Few-Shot Data,"['Chun-Chih Teng', 'Pin-Yu Chen', 'Wei-Chen Chiu']",http://arxiv.org/pdf/2102.12765v1.pdf,2021-02-25,['cs.cv'],"  We propose a Paired Few-shot GAN (PFS-GAN) model for learning generators withsufficient source data and a few target data. While generative model learningtypically needs large-scale training data, our PFS-GAN not only uses theconcept of few-shot learning but also domain shift to transfer the knowledgeacross domains, which alleviates the issue of obtaining low-quality generatorwhen only trained with target domain data. The cross-domain datasets areassumed to have two properties: (1) each target-domain sample has itssource-domain correspondence and (2) two domains share similar contentinformation but different appearance. Our PFS-GAN aims to learn thedisentangled representation from images, which composed of domain-invariantcontent features and domain-specific appearance features. Furthermore, arelation loss is introduced on the content features while shifting theappearance features to increase the structural diversity. Extensive experimentsshow that our method has better quantitative and qualitative results on thegenerated target-domain data with higher diversity in comparison to severalbaselines."
Meta-learning One-class Classifiers with Eigenvalue Solvers for  Supervised Anomaly Detection,"['Tomoharu Iwata', 'Atsutoshi Kumagai']",http://arxiv.org/pdf/2103.00684v1.pdf,2021-03-01,"['stat.ml', 'cs.lg']","  Neural network-based anomaly detection methods have shown to achieve highperformance. However, they require a large amount of training data for eachtask. We propose a neural network-based meta-learning method for supervisedanomaly detection. The proposed method improves the anomaly detectionperformance on unseen tasks, which contains a few labeled normal and anomalousinstances, by meta-training with various datasets. With a meta-learningframework, quick adaptation to each task and its effective backpropagation areimportant since the model is trained by the adaptation for each epoch. Ourmodel enables them by formulating adaptation as a generalized eigenvalueproblem with one-class classification; its global optimum solution is obtained,and the solver is differentiable. We experimentally demonstrate that theproposed method achieves better performance than existing anomaly detection andfew-shot learning methods on various datasets."
On Few Shot Learning of Dynamical Systems: A Koopman Operator Theoretic  Approach,"['Subhrajit Sinha', 'Umesh Vaidya', 'Enoch Yeung']",http://arxiv.org/pdf/2103.04221v1.pdf,2021-03-07,['math.ds'],"  In this paper, we propose a novel algorithm for learning the Koopman operatorof a dynamical system from a \textit{small} amount of training data. In manyapplications of data-driven modeling, e.g. biological network modeling,cybersecurity, modeling the Internet of Things, or smart grid monitoring, it isimpossible to obtain regularly sampled time-series data with a sufficientlyhigh sampling frequency. In such situations the existing Dynamic ModeDecomposition (DMD) or Extended Dynamic Mode Decomposition (EDMD) algorithmsfor Koopman operator computation often leads to a low fidelity approximateKoopman operator. To this end, this paper proposes an algorithm which cancompute the Koopman operator efficiently when the training data-set is sparselysampled across time. In particular, the proposed algorithm enriches the smalltraining data-set by appending artificial data points, which are treated asnoisy observations. The larger, albeit noisy data-set is then used to computethe Koopman operator, using techniques from Robust Optimization. The efficacyof the proposed algorithm is also demonstrated on three different dynamicalsystems, namely a linear network of oscillators, a nonlinear system and adynamical system governed by a Partial Differential Equation (PDE)."
Few-Shot Learning of an Interleaved Text Summarization Model by  Pretraining with Synthetic Data,"['Sanjeev Kumar Karn', 'Francine Chen', 'Yan-Ying Chen', 'Ulli Waltinger', 'Hinrich Schuetze']",http://arxiv.org/pdf/2103.05131v1.pdf,2021-03-08,['cs.cl'],"  Interleaved texts, where posts belonging to different threads occur in asequence, commonly occur in online chat posts, so that it can be time-consumingto quickly obtain an overview of the discussions. Existing systems firstdisentangle the posts by threads and then extract summaries from those threads.A major issue with such systems is error propagation from the disentanglementcomponent. While end-to-end trainable summarization system could obviateexplicit disentanglement, such systems require a large amount of labeled data.To address this, we propose to pretrain an end-to-end trainable hierarchicalencoder-decoder system using synthetic interleaved texts. We show that byfine-tuning on a real-world meeting dataset (AMI), such a system out-performs atraditional two-step system by 22%. We also compare against transformer modelsand observed that pretraining with synthetic data both the encoder and decoderoutperforms the BertSumExtAbs transformer model which pretrains only theencoder on a large dataset."
FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding,"['Bo Sun', 'Banghuai Li', 'Shengcai Cai', 'Ye Yuan', 'Chi Zhang']",http://arxiv.org/pdf/2103.05950v2.pdf,2021-03-10,['cs.cv'],"  Emerging interests have been brought to recognize previously unseen objectsgiven very few training examples, known as few-shot object detection (FSOD).Recent researches demonstrate that good feature embedding is the key to reachfavorable few-shot learning performance. We observe object proposals withdifferent Intersection-of-Union (IoU) scores are analogous to the intra-imageaugmentation used in contrastive approaches. And we exploit this analogy andincorporate supervised contrastive learning to achieve more robust objectsrepresentations in FSOD. We present Few-Shot object detection via Contrastiveproposals Encoding (FSCE), a simple yet effective approach to learningcontrastive-aware object proposal encodings that facilitate the classificationof detected objects. We notice the degradation of average precision (AP) forrare objects mainly comes from misclassifying novel instances as confusableclasses. And we ease the misclassification issues by promoting instance levelintra-class compactness and inter-class variance via our contrastive proposalencoding loss (CPE loss). Our design outperforms current state-of-the-art worksin any shot and all data splits, with up to +8.8% on standard benchmark PASCALVOC and +2.7% on challenging COCO benchmark. Code is available at: https://github.com/MegviiDetection/FSCE"
A New Method for Features Normalization in Motor Imagery Few-Shot  Learning using Resting-State,"['M. Amin. Ghasemi', 'Sadjaad Ozgoli', 'Ali. M. NasrAbadi']",http://arxiv.org/pdf/2103.09507v1.pdf,2021-03-17,"['eess.sp', 'q-bio.nc']","  Brain-computer interface (BCI) systems are usually designed specifically foreach subject based on motor imagery. Therefore, the usability of these networkshas become a significant challenge. The network has to be designed separatelyfor each user, which is time-consuming for the user. Therefore, this studyproposes a method by which the calibration time is significantly reduced whilethe classification accuracy is increased. In this method, we calibrated thefeatures extracted from the motor imagery task by dividing the featuresextracted from the resting-state into both open-eye and closed-eye modes andthe state in which the subject moves his eyes. The best classification accuracywas obtained using the SVM classifier using the resting-state signal in theopen eye, which increased by 3.64% to 74.04%. In this paper, we alsoinvestigated the effect of recording time of the resting-state signal and theimpact of eye state on the classification accuracy."
von Mises-Fisher Loss: An Exploration of Embedding Geometries for  Supervised Learning,"['Tyler R. Scott', 'Andrew C. Gallagher', 'Michael C. Mozer']",http://arxiv.org/pdf/2103.15718v4.pdf,2021-03-29,"['cs.lg', 'cs.cv']","  Recent work has argued that classification losses utilizing softmaxcross-entropy are superior not only for fixed-set classification tasks, butalso by outperforming losses developed specifically for open-set tasksincluding few-shot learning and retrieval. Softmax classifiers have beenstudied using different embedding geometries -- Euclidean, hyperbolic, andspherical -- and claims have been made about the superiority of one or another,but they have not been systematically compared with careful controls. Weconduct an empirical investigation of embedding geometry on softmax losses fora variety of fixed-set classification and image retrieval tasks. An interestingproperty observed for the spherical losses lead us to propose a probabilisticclassifier based on the von Mises-Fisher distribution, and we show that it iscompetitive with state-of-the-art methods while producing improvedout-of-the-box calibration. We provide guidance regarding the trade-offsbetween losses and how to choose among them."
Few-shot learning through contextual data augmentation,"['Farid Arthaud', 'Rachel Bawden', 'Alexandra Birch']",http://arxiv.org/pdf/2103.16911v1.pdf,2021-03-31,['cs.cl'],"  Machine translation (MT) models used in industries with constantly changingtopics, such as translation or news agencies, need to adapt to new data tomaintain their performance over time. Our aim is to teach a pre-trained MTmodel to translate previously unseen words accurately, based on very fewexamples. We propose (i) an experimental setup allowing us to simulate novelvocabulary appearing in human-submitted translations, and (ii) correspondingevaluation metrics to compare our approaches. We extend a data augmentationapproach using a pre-trained language model to create training examples withsimilar contexts for novel words. We compare different fine-tuning and dataaugmentation approaches and show that adaptation on the scale of one to fiveexamples is possible. Combining data augmentation with randomly selectedtraining sentences leads to the highest BLEU score and accuracy improvements.Impressively, with only 1 to 5 examples, our model reports better accuracyscores than a reference system trained with on average 313 parallel examples."
FAPIS: A Few-shot Anchor-free Part-based Instance Segmenter,"['Khoi Nguyen', 'Sinisa Todorovic']",http://arxiv.org/pdf/2104.00073v1.pdf,2021-03-31,['cs.cv'],"  This paper is about few-shot instance segmentation, where training and testimage sets do not share the same object classes. We specify and evaluate a newfew-shot anchor-free part-based instance segmenter FAPIS. Our key novelty is inexplicit modeling of latent object parts shared across training object classes,which is expected to facilitate our few-shot learning on new classes intesting. We specify a new anchor-free object detector aimed at scoring andregressing locations of foreground bounding boxes, as well as estimatingrelative importance of latent parts within each box. Also, we specify a newnetwork for delineating and weighting latent parts for the final instancesegmentation within every detected bounding box. Our evaluation on thebenchmark COCO-20i dataset demonstrates that we significantly outperform thestate of the art."
Modular Adaptation for Cross-Domain Few-Shot Learning,"['Xiao Lin', 'Meng Ye', 'Yunye Gong', 'Giedrius Buracas', 'Nikoletta Basiou', 'Ajay Divakaran', 'Yi Yao']",http://arxiv.org/pdf/2104.00619v1.pdf,2021-04-01,['cs.cv'],"  Adapting pre-trained representations has become the go-to recipe for learningnew downstream tasks with limited examples. While literature has demonstratedgreat successes via representation learning, in this work, we show thatsubstantial performance improvement of downstream tasks can also be achieved byappropriate designs of the adaptation process. Specifically, we propose amodular adaptation method that selectively performs multiple state-of-the-art(SOTA) adaptation methods in sequence. As different downstream tasks mayrequire different types of adaptation, our modular adaptation enables thedynamic configuration of the most suitable modules based on the downstreamtask. Moreover, as an extension to existing cross-domain 5-way k-shotbenchmarks (e.g., miniImageNet -> CUB), we create a new high-way (~100) k-shotbenchmark with data from 10 different datasets. This benchmark provides adiverse set of domains and allows the use of stronger representations learnedfrom ImageNet. Experimental results show that by customizing adaptation processtowards downstream tasks, our modular adaptation pipeline (MAP) improves 3.1%in 5-shot classification accuracy over baselines of finetuning and PrototypicalNetworks."
Meta-Learning for Fast Cross-Lingual Adaptation in Dependency Parsing,"['Anna Langedijk', 'Verna Dankers', 'Phillip Lippe', 'Sander Bos', 'Bryan Cardenas Guevara', 'Helen Yannakoudakis', 'Ekaterina Shutova']",http://arxiv.org/pdf/2104.04736v3.pdf,2021-04-10,"['cs.cl', 'cs.ai']","  Meta-learning, or learning to learn, is a technique that can help to overcomeresource scarcity in cross-lingual NLP problems, by enabling fast adaptation tonew tasks. We apply model-agnostic meta-learning (MAML) to the task ofcross-lingual dependency parsing. We train our model on a diverse set oflanguages to learn a parameter initialization that can adapt quickly to newlanguages. We find that meta-learning with pre-training can significantlyimprove upon the performance of language transfer and standard supervisedlearning baselines for a variety of unseen, typologically diverse, andlow-resource languages, in a few-shot learning setup."
How Sensitive are Meta-Learners to Dataset Imbalance?,"['Mateusz Ochal', 'Massimiliano Patacchiola', 'Amos Storkey', 'Jose Vazquez', 'Sen Wang']",http://arxiv.org/pdf/2104.05344v1.pdf,2021-04-12,"['cs.lg', 'cs.cv']","  Meta-Learning (ML) has proven to be a useful tool for training Few-ShotLearning (FSL) algorithms by exposure to batches of tasks sampled from ameta-dataset. However, the standard training procedure overlooks the dynamicnature of the real-world where object classes are likely to occur at differentfrequencies. While it is generally understood that imbalanced tasks harm theperformance of supervised methods, there is no significant research examiningthe impact of imbalanced meta-datasets on the FSL evaluation task. This studyexposes the magnitude and extent of this problem. Our results show that MLmethods are more robust against meta-dataset imbalance than imbalance at thetask-level with a similar imbalance ratio ($\rho<20$), with the effect holdingeven in long-tail datasets under a larger imbalance ($\rho=65$). Overall, theseresults highlight an implicit strength of ML algorithms, capable of learninggeneralizable features under dataset imbalance and domain-shift. The code toreproduce the experiments is released under an open-source license."
Few-shot Intent Classification and Slot Filling with Retrieved Examples,"['Dian Yu', 'Luheng He', 'Yuan Zhang', 'Xinya Du', 'Panupong Pasupat', 'Qi Li']",http://arxiv.org/pdf/2104.05763v1.pdf,2021-04-12,['cs.cl'],"  Few-shot learning arises in important practical scenarios, such as when anatural language understanding system needs to learn new semantic labels for anemerging, resource-scarce domain. In this paper, we explore retrieval-basedmethods for intent classification and slot filling tasks in few-shot settings.Retrieval-based methods make predictions based on labeled examples in theretrieval index that are similar to the input, and thus can adapt to newdomains simply by changing the index without having to retrain the model.However, it is non-trivial to apply such methods on tasks with a complex labelspace like slot filling. To this end, we propose a span-level retrieval methodthat learns similar contextualized representations for spans with the samelabel via a novel batch-softmax objective. At inference time, we use the labelsof the retrieved spans to construct the final structure with the highestaggregated score. Our method outperforms previous systems in various few-shotsettings on the CLINC and SNIPS benchmarks."
Embedding Adaptation is Still Needed for Few-Shot Learning,"['Sébastien M. R. Arnold', 'Fei Sha']",http://arxiv.org/pdf/2104.07255v1.pdf,2021-04-15,"['cs.lg', 'cs.cv']","  Constructing new and more challenging tasksets is a fruitful methodology toanalyse and understand few-shot classification methods. Unfortunately, existingapproaches to building those tasksets are somewhat unsatisfactory: they eitherassume train and test task distributions to be identical -- which leads tooverly optimistic evaluations -- or take a ""worst-case"" philosophy -- whichtypically requires additional human labor such as obtaining semantic classrelationships. We propose ATG, a principled clustering method to defining trainand test tasksets without additional human knowledge. ATG models train and testtask distributions while requiring them to share a predefined amount ofinformation. We empirically demonstrate the effectiveness of ATG in generatingtasksets that are easier, in-between, or harder than existing benchmarks,including those that rely on semantic information. Finally, we leverage ourgenerated tasksets to shed a new light on few-shot classification:gradient-based methods -- previously believed to underperform -- can outperformmetric-based ones when transfer is most challenging."
Does language help generalization in vision models?,"['Benjamin Devillers', 'Bhavin Choksi', 'Romain Bielawski', 'Rufin VanRullen']",http://arxiv.org/pdf/2104.08313v3.pdf,2021-04-16,"['cs.ai', 'cs.cl', 'cs.cv']","  Vision models trained on multimodal datasets can benefit from the wideavailability of large image-caption datasets. A recent model (CLIP) was foundto generalize well in zero-shot and transfer learning settings. This couldimply that linguistic or ""semantic grounding"" confers additional generalizationabilities to the visual feature space. Here, we systematically evaluate variousmultimodal architectures and vision-only models in terms of unsupervisedclustering, few-shot learning, transfer learning and adversarial robustness. Ineach setting, multimodal training produced no additional generalizationcapability compared to standard supervised visual training. We conclude thatwork is still required for semantic grounding to help improve vision models."
Revisiting Few-shot Relation Classification: Evaluation Data and  Classification Schemes,"['Ofer Sabo', 'Yanai Elazar', 'Yoav Goldberg', 'Ido Dagan']",http://arxiv.org/pdf/2104.08481v1.pdf,2021-04-17,['cs.cl'],"  We explore Few-Shot Learning (FSL) for Relation Classification (RC). Focusingon the realistic scenario of FSL, in which a test instance might not belong toany of the target categories (none-of-the-above, aka NOTA), we first revisitthe recent popular dataset structure for FSL, pointing out its unrealistic datadistribution. To remedy this, we propose a novel methodology for deriving morerealistic few-shot test data from available datasets for supervised RC, andapply it to the TACRED dataset. This yields a new challenging benchmark for FSLRC, on which state of the art models show poor performance. Next, we analyzeclassification schemes within the popular embedding-based nearest-neighborapproach for FSL, with respect to constraints they impose on the embeddingspace. Triggered by this analysis we propose a novel classification scheme, inwhich the NOTA category is represented as learned vectors, shown empirically tobe an appealing option for FSL."
Multilingual and Cross-Lingual Intent Detection from Spoken Data,"['Daniela Gerz', 'Pei-Hao Su', 'Razvan Kusztos', 'Avishek Mondal', 'Michał Lis', 'Eshan Singhal', 'Nikola Mrkšić', 'Tsung-Hsien Wen', 'Ivan Vulić']",http://arxiv.org/pdf/2104.08524v1.pdf,2021-04-17,['cs.cl'],"  We present a systematic study on multilingual and cross-lingual intentdetection from spoken data. The study leverages a new resource put forth inthis work, termed MInDS-14, a first training and evaluation resource for theintent detection task with spoken data. It covers 14 intents extracted from acommercial system in the e-banking domain, associated with spoken examples in14 diverse language varieties. Our key results indicate that combining machinetranslation models with state-of-the-art multilingual sentence encoders (e.g.,LaBSE) can yield strong intent detectors in the majority of target languagescovered in MInDS-14, and offer comparative analyses across different axes:e.g., zero-shot versus few-shot learning, translation direction, and impact ofspeech recognition. We see this work as an important step towards moreinclusive development and evaluation of multilingual intent detectors fromspoken data, in a much wider spectrum of languages compared to prior work."
The Power of Scale for Parameter-Efficient Prompt Tuning,"['Brian Lester', 'Rami Al-Rfou', 'Noah Constant']",http://arxiv.org/pdf/2104.08691v2.pdf,2021-04-18,['cs.cl'],"  In this work, we explore ""prompt tuning"", a simple yet effective mechanismfor learning ""soft prompts"" to condition frozen language models to performspecific downstream tasks. Unlike the discrete text prompts used by GPT-3, softprompts are learned through backpropagation and can be tuned to incorporatesignal from any number of labeled examples. Our end-to-end learned approachoutperforms GPT-3's ""few-shot"" learning by a large margin. More remarkably,through ablations on model size using T5, we show that prompt tuning becomesmore competitive with scale: as models exceed billions of parameters, ourmethod ""closes the gap"" and matches the strong performance of model tuning(where all model weights are tuned). This finding is especially relevant inthat large models are costly to share and serve, and the ability to reuse onefrozen model for multiple downstream tasks can ease this burden. Our method canbe seen as a simplification of the recently proposed ""prefix tuning"" of Li andLiang (2021), and we provide a comparison to this and other similar approaches.Finally, we show that conditioning a frozen model with soft prompts confersbenefits in robustness to domain transfer, as compared to full model tuning."
Self-Supervised WiFi-Based Activity Recognition,"['Hok-Shing Lau', 'Ryan McConville', 'Mohammud J. Bocus', 'Robert J. Piechocki', 'Raul Santos-Rodriguez']",http://arxiv.org/pdf/2104.09072v1.pdf,2021-04-19,"['cs.ni', 'cs.lg']","  Traditional approaches to activity recognition involve the use of wearablesensors or cameras in order to recognise human activities. In this work, weextract fine-grained physical layer information from WiFi devices for thepurpose of passive activity recognition in indoor environments. While such datais ubiquitous, few approaches are designed to utilise large amounts ofunlabelled WiFi data. We propose the use of self-supervised contrastivelearning to improve activity recognition performance when using multiple viewsof the transmitted WiFi signal captured by different synchronised receivers. Weconduct experiments where the transmitters and receivers are arranged indifferent physical layouts so as to cover both Line-of-Sight (LoS) and non LoS(NLoS) conditions. We compare the proposed contrastive learning system withnon-contrastive systems and observe a 17.7% increase in macro averaged F1 scoreon the task of WiFi based activity recognition, as well as significantimprovements in one- and few-shot learning scenarios."
Few-shot learning via tensor hallucination,"['Michalis Lazarou', 'Yannis Avrithis', 'Tania Stathaki']",http://arxiv.org/pdf/2104.09467v1.pdf,2021-04-19,['cs.cv'],"  Few-shot classification addresses the challenge of classifying examples givenonly limited labeled data. A powerful approach is to go beyond dataaugmentation, towards data synthesis. However, most of dataaugmentation/synthesis methods for few-shot classification are overly complexand sophisticated, e.g. training a wGAN with multiple regularizers or traininga network to transfer latent diversities from known to novel classes. We maketwo contributions, namely we show that: (1) using a simple loss function ismore than enough for training a feature generator in the few-shot setting; and(2) learning to generate tensor features instead of vector features issuperior. Extensive experiments on miniImagenet, CUB and CIFAR-FS datasets showthat our method sets a new state of the art, outperforming more sophisticatedfew-shot data augmentation methods."
Few-Shot Video Object Detection,"['Qi Fan', 'Chi-Keung Tang', 'Yu-Wing Tai']",http://arxiv.org/pdf/2104.14805v3.pdf,2021-04-30,['cs.cv'],  We introduce Few-Shot Video Object Detection (FSVOD) with three contributionsto real-world visual learning challenge in our highly diverse and dynamicworld: 1) a large-scale video dataset FSVOD-500 comprising of 500 classes withclass-balanced videos in each category for few-shot learning; 2) a novel TubeProposal Network (TPN) to generate high-quality video tube proposals foraggregating feature representation for the target video object which can behighly dynamic; 3) a strategically improved Temporal Matching Network (TMN+)for matching representative query tube features with better discriminativeability thus achieving higher diversity. Our TPN and TMN+ are jointly andend-to-end trained. Extensive experiments demonstrate that our method producessignificantly better detection results on two few-shot video object detectiondatasets compared to image-based methods and other naive video-basedextensions. Codes and datasets are released at\url{https://github.com/fanq15/FewX}.
One Model to Rule them All: Towards Zero-Shot Learning for Databases,"['Benjamin Hilprecht', 'Carsten Binnig']",http://arxiv.org/pdf/2105.00642v4.pdf,2021-05-03,"['cs.db', 'cs.ai']","  In this paper, we present our vision of so called zero-shot learning fordatabases which is a new learning approach for database components. Zero-shotlearning for databases is inspired by recent advances in transfer learning ofmodels such as GPT-3 and can support a new database out-of-the box without theneed to train a new model. Furthermore, it can easily be extended to few-shotlearning by further retraining the model on the unseen database. As a firstconcrete contribution in this paper, we show the feasibility of zero-shotlearning for the task of physical cost estimation and present very promisinginitial results. Moreover, as a second contribution we discuss the corechallenges related to zero-shot learning for databases and present a roadmap toextend zero-shot learning towards many other tasks beyond cost estimation oreven beyond classical database systems and workloads."
Memorisation versus Generalisation in Pre-trained Language Models,"['Michael Tänzer', 'Sebastian Ruder', 'Marek Rei']",http://arxiv.org/pdf/2105.00828v2.pdf,2021-04-16,"['cs.cl', 'cs.lg']","  State-of-the-art pre-trained language models have been shown to memorisefacts and perform well with limited amounts of training data. To gain a betterunderstanding of how these models learn, we study their generalisation andmemorisation capabilities in noisy and low-resource scenarios. We find that thetraining of these models is almost unaffected by label noise and that it ispossible to reach near-optimal results even on extremely noisy datasets.However, our experiments also show that they mainly learn from high-frequencypatterns and largely fail when tested on low-resource tasks such as few-shotlearning and rare entity recognition. To mitigate such limitations, we proposean extension based on prototypical networks that improves performance inlow-resource named entity recognition tasks."
How Fine-Tuning Allows for Effective Meta-Learning,"['Kurtland Chua', 'Qi Lei', 'Jason D. Lee']",http://arxiv.org/pdf/2105.02221v1.pdf,2021-05-05,"['cs.lg', 'stat.ml']","  Representation learning has been widely studied in the context ofmeta-learning, enabling rapid learning of new tasks through sharedrepresentations. Recent works such as MAML have explored usingfine-tuning-based metrics, which measure the ease by which fine-tuning canachieve good performance, as proxies for obtaining representations. We presenta theoretical framework for analyzing representations derived from a MAML-likealgorithm, assuming the available tasks use approximately the same underlyingrepresentation. We then provide risk bounds on the best predictor found byfine-tuning via gradient descent, demonstrating that the algorithm can provablyleverage the shared structure. The upper bound applies to general functionclasses, which we demonstrate by instantiating the guarantees of our frameworkin the logistic regression and neural network settings. In contrast, weestablish the existence of settings where any algorithm, using a representationtrained with no consideration for task-specific fine-tuning, performs as wellas a learner with no access to source tasks in the worst case. This separationresult underscores the benefit of fine-tuning-based methods, such as MAML, overmethods with ""frozen representation"" objectives in few-shot learning."
Few-Shot Learning for Image Classification of Common Flora,['Joshua Ball'],http://arxiv.org/pdf/2105.03056v1.pdf,2021-05-07,['cs.cv'],"  The use of meta-learning and transfer learning in the task of few-shot imageclassification is a well researched area with many papers showcasing theadvantages of transfer learning over meta-learning in cases where data isplentiful and there is no major limitations to computational resources. In thispaper we will showcase our experimental results from testing variousstate-of-the-art transfer learning weights and architectures versus similarstate-of-the-art works in the meta-learning field for image classificationutilizing Model-Agnostic Meta Learning (MAML). Our results show that bothpractices provide adequate performance when the dataset is sufficiently large,but that they both also struggle when data sparsity is introduced to maintainsufficient performance. This problem is moderately reduced with the use ofimage augmentation and the fine-tuning of hyperparameters. In this paper wewill discuss: (1) our process of developing a robust multi-class convolutionalneural network (CNN) for the task of few-shot image classification, (2)demonstrate that transfer learning is the superior method of helping create animage classification model when the dataset is large and (3) that MAMLoutperforms transfer learning in the case where data is very limited. The codeis available here: github.com/JBall1/Few-Shot-Limited-Data"
Comprehensive Study: How the Context Information of Different  Granularity Affects Dialogue State Tracking?,"['Puhai Yang', 'Heyan Huang', 'Xian-Ling Mao']",http://arxiv.org/pdf/2105.03571v2.pdf,2021-05-08,['cs.cl'],"  Dialogue state tracking (DST) plays a key role in task-oriented dialoguesystems to monitor the user's goal. In general, there are two strategies totrack a dialogue state: predicting it from scratch and updating it fromprevious state. The scratch-based strategy obtains each slot value by inquiringall the dialogue history, and the previous-based strategy relies on the currentturn dialogue to update the previous dialogue state. However, it is hard forthe scratch-based strategy to correctly track short-dependency dialogue statebecause of noise; meanwhile, the previous-based strategy is not very useful forlong-dependency dialogue state tracking. Obviously, it plays different rolesfor the context information of different granularity to track different kindsof dialogue states. Thus, in this paper, we will study and discuss how thecontext information of different granularity affects dialogue state tracking.First, we explore how greatly different granularities affect dialogue statetracking. Then, we further discuss how to combine multiple granularities fordialogue state tracking. Finally, we apply the findings about contextgranularity to few-shot learning scenario. Besides, we have publicly releasedall codes."
Learning Implicit Temporal Alignment for Few-shot Video Classification,"['Songyang Zhang', 'Jiale Zhou', 'Xuming He']",http://arxiv.org/pdf/2105.04823v1.pdf,2021-05-11,"['cs.cv', 'cs.ai', 'cs.lg']","  Few-shot video classification aims to learn new video categories with only afew labeled examples, alleviating the burden of costly annotation in real-worldapplications. However, it is particularly challenging to learn aclass-invariant spatial-temporal representation in such a setting. To addressthis, we propose a novel matching-based few-shot learning strategy for videosequences in this work. Our main idea is to introduce an implicit temporalalignment for a video pair, capable of estimating the similarity between themin an accurate and robust manner. Moreover, we design an effective contextencoding module to incorporate spatial and feature channel context, resultingin better modeling of intra-class variations. To train our model, we develop amulti-task loss for learning video matching, leading to video features withbetter generalization. Extensive experimental results on two challengingbenchmarks, show that our method outperforms the prior arts with a sizablemargin on SomethingSomething-V2 and competitive results on Kinetics."
Representation Learning via Global Temporal Alignment and  Cycle-Consistency,"['Isma Hadji', 'Konstantinos G. Derpanis', 'Allan D. Jepson']",http://arxiv.org/pdf/2105.05217v1.pdf,2021-05-11,['cs.cv'],"  We introduce a weakly supervised method for representation learning based onaligning temporal sequences (e.g., videos) of the same process (e.g., humanaction). The main idea is to use the global temporal ordering of latentcorrespondences across sequence pairs as a supervisory signal. In particular,we propose a loss based on scoring the optimal sequence alignment to train anembedding network. Our loss is based on a novel probabilistic path finding viewof dynamic time warping (DTW) that contains the following three key features:(i) the local path routing decisions are contrastive and differentiable, (ii)pairwise distances are cast as probabilities that are contrastive as well, and(iii) our formulation naturally admits a global cycle consistency loss thatverifies correspondences. For evaluation, we consider the tasks of fine-grainedaction classification, few shot learning, and video synchronization. We reportsignificant performance increases over previous methods. In addition, we reporttwo applications of our temporal alignment framework, namely 3D posereconstruction and fine-grained audio/visual retrieval."
Livewired Neural Networks: Making Neurons That Fire Together Wire  Together,['Thomas Schumacher'],http://arxiv.org/pdf/2105.08111v1.pdf,2021-05-17,"['cs.ne', 'cs.lg', 'q-bio.nc']","  Until recently, artificial neural networks were typically designed with afixed network structure. Here, I argue that network structure is highlyrelevant to function, and therefore neural networks should be livewired(Eagleman 2020): dynamically rewired to reflect relationships between higherorder representations of the external environment identified by coincidentactivations in individual neurons. I discuss how this approach may enable suchnetworks to build compositional world models that operate on symbols and thatachieve few-shot learning, capabilities thought by many to be critical tohuman-level cognition. Here, I also 1) discuss how such livewired neuralnetworks maximize the information the environment provides to a model, 2)explore evidence indicating that livewiring is implemented in the brain, guidedby glial cells, 3) discuss how livewiring may give rise to the associativeemergent behaviors of brains, and 4) suggest paths for future research usinglivewired networks to understand and create human-like reasoning."
Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event  Detection,"['Shirong Shen', 'Tongtong Wu', 'Guilin Qi', 'Yuan-Fang Li', 'Gholamreza Haffari', 'Sheng Bi']",http://arxiv.org/pdf/2105.09509v2.pdf,2021-05-20,"['cs.cl', 'cs.ai']","  Event detection (ED) aims at detecting event trigger words in sentences andclassifying them into specific event types. In real-world applications, EDtypically does not have sufficient labelled data, thus can be formulated as afew-shot learning problem. To tackle the issue of low sample diversity infew-shot ED, we propose a novel knowledge-based few-shot event detection methodwhich uses a definition-based encoder to introduce external event knowledge asthe knowledge prior of event types. Furthermore, as external knowledgetypically provides limited and imperfect coverage of event types, we introducean adaptive knowledge-enhanced Bayesian meta-learning method to dynamicallyadjust the knowledge prior of event types. Experiments show our methodconsistently and substantially outperforms a number of baselines by at least 15absolute F1 points under the same few-shot settings."
Compositional Fine-Grained Low-Shot Learning,"['Dat Huynh', 'Ehsan Elhamifar']",http://arxiv.org/pdf/2105.10438v1.pdf,2021-05-21,['cs.cv'],"  We develop a novel compositional generative model for zero- and few-shotlearning to recognize fine-grained classes with a few or no training samples.Our key observation is that generating holistic features for fine-grainedclasses fails to capture small attribute differences between classes.Therefore, we propose a feature composition framework that learns to extractattribute features from training samples and combines them to constructfine-grained features for rare and unseen classes. Feature composition allowsus to not only selectively compose features of every class from only relevanttraining samples, but also obtain diversity among composed features viachanging samples used for the composition. In addition, instead of buildingholistic features for classes, we use our attribute features to form denserepresentations capable of capturing fine-grained attribute details of classes.We propose a training scheme that uses a discriminative model to constructfeatures that are subsequently used to train the model itself. Therefore, wedirectly train the discriminative model on the composed features withoutlearning a separate generative model. We conduct experiments on four populardatasets of DeepFashion, AWA2, CUB, and SUN, showing the effectiveness of ourmethod."
Anti-aliasing Semantic Reconstruction for Few-Shot Semantic Segmentation,"['Binghao Liu', 'Yao Ding', 'Jianbin Jiao', 'Xiangyang Ji', 'Qixiang Ye']",http://arxiv.org/pdf/2106.00184v1.pdf,2021-06-01,['cs.cv'],"  Encouraging progress in few-shot semantic segmentation has been made byleveraging features learned upon base classes with sufficient training data torepresent novel classes with few-shot examples. However, this feature sharingmechanism inevitably causes semantic aliasing between novel classes when theyhave similar compositions of semantic concepts. In this paper, we reformulatefew-shot segmentation as a semantic reconstruction problem, and convert baseclass features into a series of basis vectors which span a class-level semanticspace for novel class reconstruction. By introducing contrastive loss, wemaximize the orthogonality of basis vectors while minimizing semantic aliasingbetween classes. Within the reconstructed representation space, we furthersuppress interference from other classes by projecting query features to thesupport vector for precise semantic activation. Our proposed approach, referredto as anti-aliasing semantic reconstruction (ASR), provides a systematic yetinterpretable solution for few-shot learning problems. Extensive experiments onPASCAL VOC and MS COCO datasets show that ASR achieves strong results comparedwith the prior works."
Ethical-Advice Taker: Do Language Models Understand Natural Language  Interventions?,"['Jieyu Zhao', 'Daniel Khashabi', 'Tushar Khot', 'Ashish Sabharwal', 'Kai-Wei Chang']",http://arxiv.org/pdf/2106.01465v1.pdf,2021-06-02,"['cs.cl', 'cs.ai', 'cs.lg']","  Is it possible to use natural language to intervene in a model's behavior andalter its prediction in a desired way? We investigate the effectiveness ofnatural language interventions for reading-comprehension systems, studying thisin the context of social stereotypes. Specifically, we propose a new languageunderstanding task, Linguistic Ethical Interventions (LEI), where the goal isto amend a question-answering (QA) model's unethical behavior by communicatingcontext-specific principles of ethics and equity to it. To this end, we buildupon recent methods for quantifying a system's social stereotypes, augmentingthem with different kinds of ethical interventions and the desired modelbehavior under such interventions. Our zero-shot evaluation finds that eventoday's powerful neural language models are extremely poor ethical-advicetakers, that is, they respond surprisingly little to ethical interventions eventhough these interventions are stated as simple sentences. Few-shot learningimproves model behavior but remains far from the desired outcome, especiallywhen evaluated for various types of generalization. Our new task thus poses anovel language understanding challenge for the community."
Personalizing Pre-trained Models,"['Mina Khan', 'P Srivatsa', 'Advait Rane', 'Shriram Chenniappa', 'Asadali Hazariwala', 'Pattie Maes']",http://arxiv.org/pdf/2106.01499v1.pdf,2021-06-02,['cs.cv'],"  Self-supervised or weakly supervised models trained on large-scale datasetshave shown sample-efficient transfer to diverse datasets in few-shot settings.We consider how upstream pretrained models can be leveraged for downstreamfew-shot, multilabel, and continual learning tasks. Our model CLIPPER (CLIPPERsonalized) uses image representations from CLIP, a large-scale imagerepresentation learning model trained using weak natural language supervision.We developed a technique, called Multi-label Weight Imprinting (MWI), formulti-label, continual, and few-shot learning, and CLIPPER uses MWI with imagerepresentations from CLIP. We evaluated CLIPPER on 10 single-label and 5multi-label datasets. Our model shows robust and competitive performance, andwe set new benchmarks for few-shot, multi-label, and continual learning. Ourlightweight technique is also compute-efficient and enables privacy-preservingapplications as the data is not sent to the upstream model for fine-tuning."
DAMSL: Domain Agnostic Meta Score-based Learning,"['John Cai', 'Bill Cai', 'Shengmei Shen']",http://arxiv.org/pdf/2106.03041v1.pdf,2021-06-06,"['cs.lg', 'cs.ai', 'cs.cv']","  In this paper, we propose Domain Agnostic Meta Score-based Learning (DAMSL),a novel, versatile and highly effective solution that delivers significantout-performance over state-of-the-art methods for cross-domain few-shotlearning. We identify key problems in previous meta-learning methodsover-fitting to the source domain, and previous transfer-learning methodsunder-utilizing the structure of the support set. The core idea behind ourmethod is that instead of directly using the scores from a fine-tuned featureencoder, we use these scores to create input coordinates for a domain agnosticmetric space. A graph neural network is applied to learn an embedding andrelation function over these coordinates to process all information containedin the score distribution of the support set. We test our model on bothestablished CD-FSL benchmarks and new domains and show that our methodovercomes the limitations of previous meta-learning and transfer-learningmethods to deliver substantial improvements in accuracy across both smaller andlarger domain shifts."
DETReg: Unsupervised Pretraining with Region Priors for Object Detection,"['Amir Bar', 'Xin Wang', 'Vadim Kantorov', 'Colorado J Reed', 'Roei Herzig', 'Gal Chechik', 'Anna Rohrbach', 'Trevor Darrell', 'Amir Globerson']",http://arxiv.org/pdf/2106.04550v5.pdf,2021-06-08,['cs.cv'],"  Recent self-supervised pretraining methods for object detection largely focuson pretraining the backbone of the object detector, neglecting key parts ofdetection architecture. Instead, we introduce DETReg, a new self-supervisedmethod that pretrains the entire object detection network, including the objectlocalization and embedding components. During pretraining, DETReg predictsobject localizations to match the localizations from an unsupervised regionproposal generator and simultaneously aligns the corresponding featureembeddings with embeddings from a self-supervised image encoder. We implementDETReg using the DETR family of detectors and show that it improves overcompetitive baselines when finetuned on COCO, PASCAL VOC, and Airbus Shipbenchmarks. In low-data regimes DETReg achieves improved performance, e.g.,when training with only 1% of the labels and in the few-shot learning settings."
Attentional Meta-learners for Few-shot Polythetic Classification,"['Ben Day', 'Ramon Viñas', 'Nikola Simidjievski', 'Pietro Liò']",http://arxiv.org/pdf/2106.05317v2.pdf,2021-06-09,['cs.lg'],"  Polythetic classifications, based on shared patterns of features that needneither be universal nor constant among members of a class, are common in thenatural world and greatly outnumber monothetic classifications over a set offeatures. We show that threshold meta-learners, such as Prototypical Networks,require an embedding dimension that is exponential in the number oftask-relevant features to emulate these functions. In contrast, attentionalclassifiers, such as Matching Networks, are polythetic by default and able tosolve these problems with a linear embedding dimension. However, we find thatin the presence of task-irrelevant features, inherent to meta-learningproblems, attentional models are susceptible to misclassification. To addressthis challenge, we propose a self-attention feature-selection mechanism thatadaptively dilutes non-discriminative features. We demonstrate theeffectiveness of our approach in meta-learning Boolean functions, and syntheticand real-world few-shot learning tasks."
AUGNLG: Few-shot Natural Language Generation using Self-trained Data  Augmentation,"['Xinnuo Xu', 'Guoyin Wang', 'Young-Bum Kim', 'Sungjin Lee']",http://arxiv.org/pdf/2106.05589v1.pdf,2021-06-10,['cs.cl'],"  Natural Language Generation (NLG) is a key component in a task-orienteddialogue system, which converts the structured meaning representation (MR) tothe natural language. For large-scale conversational systems, where it iscommon to have over hundreds of intents and thousands of slots, neithertemplate-based approaches nor model-based approaches are scalable. Recently,neural NLGs started leveraging transfer learning and showed promising resultsin few-shot settings. This paper proposes AUGNLG, a novel data augmentationapproach that combines a self-trained neural retrieval model with a few-shotlearned NLU model, to automatically create MR-to-Text data from open-domaintexts. The proposed system mostly outperforms the state-of-the-art methods onthe FewShotWOZ data in both BLEU and Slot Error Rate. We further confirmimproved results on the FewShotSGD data and provide comprehensive analysisresults on key components of our system. Our code and data are available athttps://github.com/XinnuoXu/AugNLG."
Interpretable Self-supervised Multi-task Learning for COVID-19  Information Retrieval and Extraction,"['Nima Ebadi', 'Peyman Najafirad']",http://arxiv.org/pdf/2106.08252v1.pdf,2021-06-15,"['cs.ir', 'cs.cl']","  The rapidly evolving literature of COVID-19 related articles makes itchallenging for NLP models to be effectively trained for information retrievaland extraction with the corresponding labeled data that follows the currentdistribution of the pandemic. On the other hand, due to the uncertainty of thesituation, human experts' supervision would always be required to double checkthe decision making of these models highlighting the importance ofinterpretability. In the light of these challenges, this study proposes aninterpretable self-supervised multi-task learning model to jointly andeffectively tackle the tasks of information retrieval (IR) and extraction (IE)during the current emergency health crisis situation. Our results show that ourmodel effectively leverage the multi-task and self-supervised learning toimprove generalization, data efficiency and robustness to the ongoing datasetshift problem. Our model outperforms baselines in IE and IR tasks, respectivelyby micro-f score of 0.08 (LCA-F score of 0.05), and MAP of 0.05 on average. InIE the zero- and few-shot learning performances are on average 0.32 and 0.19micro-f score higher than those of the baselines."
ECKPN: Explicit Class Knowledge Propagation Network for Transductive  Few-shot Learning,"['Chaofan Chen', 'Xiaoshan Yang', 'Changsheng Xu', 'Xuhui Huang', 'Zhe Ma']",http://arxiv.org/pdf/2106.08523v1.pdf,2021-06-16,"['cs.cv', 'cs.ai']","  Recently, the transductive graph-based methods have achieved great success inthe few-shot classification task. However, most existing methods ignoreexploring the class-level knowledge that can be easily learned by humans fromjust a handful of samples. In this paper, we propose an Explicit ClassKnowledge Propagation Network (ECKPN), which is composed of the comparison,squeeze and calibration modules, to address this problem. Specifically, wefirst employ the comparison module to explore the pairwise sample relations tolearn rich sample representations in the instance-level graph. Then, we squeezethe instance-level graph to generate the class-level graph, which can helpobtain the class-level visual knowledge and facilitate modeling the relationsof different classes. Next, the calibration module is adopted to characterizethe relations of the classes explicitly to obtain the more discriminativeclass-level knowledge representations. Finally, we combine the class-levelknowledge with the instance-level sample representations to guide the inferenceof the query samples. We conduct extensive experiments on four few-shotclassification benchmarks, and the experimental results show that the proposedECKPN significantly outperforms the state-of-the-art methods."
EvoGrad: Efficient Gradient-Based Meta-Learning and Hyperparameter  Optimization,"['Ondrej Bohdal', 'Yongxin Yang', 'Timothy Hospedales']",http://arxiv.org/pdf/2106.10575v2.pdf,2021-06-19,"['cs.lg', 'cs.ne', 'stat.ml']","  Gradient-based meta-learning and hyperparameter optimization have seensignificant progress recently, enabling practical end-to-end training of neuralnetworks together with many hyperparameters. Nevertheless, existing approachesare relatively expensive as they need to compute second-order derivatives andstore a longer computational graph. This cost prevents scaling them to largernetwork architectures. We present EvoGrad, a new approach to meta-learning thatdraws upon evolutionary techniques to more efficiently compute hypergradients.EvoGrad estimates hypergradient with respect to hyperparameters withoutcalculating second-order gradients, or storing a longer computational graph,leading to significant improvements in efficiency. We evaluate EvoGrad on threesubstantial recent meta-learning applications, namely cross-domain few-shotlearning with feature-wise transformations, noisy label learning withMeta-Weight-Net and low-resource cross-lingual learning with metarepresentation transformation. The results show that EvoGrad significantlyimproves efficiency and enables scaling meta-learning to bigger architecturessuch as from ResNet10 to ResNet34."
TNT: Text-Conditioned Network with Transductive Inference for Few-Shot  Video Classification,"['Andrés Villa', 'Juan-Manuel Perez-Rua', 'Victor Escorcia', 'Vladimir Araujo', 'Juan Carlos Niebles', 'Alvaro Soto']",http://arxiv.org/pdf/2106.11173v2.pdf,2021-06-21,['cs.cv'],"  Recently, few-shot video classification has received an increasing interest.Current approaches mostly focus on effectively exploiting the temporaldimension in videos to improve learning under low data regimes. However, mostworks have largely ignored that videos are often accompanied by rich textualdescriptions that can also be an essential source of information to handlefew-shot recognition cases. In this paper, we propose to leverage thesehuman-provided textual descriptions as privileged information when training afew-shot video classification model. Specifically, we formulate a text-basedtask conditioner to adapt video features to the few-shot learning task.Furthermore, our model follows a transductive setting to improve thetask-adaptation ability of the model by using the support textual descriptionsand query instances to update a set of class prototypes. Our model achievesstate-of-the-art performance on four challenging benchmarks commonly used toevaluate few-shot video action classification models."
Generalized Zero-Shot Learning using Multimodal Variational Auto-Encoder  with Semantic Concepts,"['Nihar Bendre', 'Kevin Desai', 'Peyman Najafirad']",http://arxiv.org/pdf/2106.14082v1.pdf,2021-06-26,"['cs.cv', 'cs.ai']","  With the ever-increasing amount of data, the central challenge in multimodallearning involves limitations of labelled samples. For the task ofclassification, techniques such as meta-learning, zero-shot learning, andfew-shot learning showcase the ability to learn information about novel classesbased on prior knowledge. Recent techniques try to learn a cross-modal mappingbetween the semantic space and the image space. However, they tend to ignorethe local and global semantic knowledge. To overcome this problem, we propose aMultimodal Variational Auto-Encoder (M-VAE) which can learn the shared latentspace of image features and the semantic space. In our approach we concatenatemultimodal data to a single embedding before passing it to the VAE for learningthe latent space. We propose the use of a multi-modal loss during thereconstruction of the feature embedding through the decoder. Our approach iscapable to correlating modalities and exploit the local and global semanticknowledge for novel sample predictions. Our experimental results using a MLPclassifier on four benchmark datasets show that our proposed model outperformsthe current state-of-the-art approaches for generalized zero-shot learning."
What's in a Measurement? Using GPT-3 on SemEval 2021 Task 8 -- MeasEval,"['Curt Kohler', 'Ron Daniel Jr']",http://arxiv.org/pdf/2106.14720v1.pdf,2021-06-28,['cs.cl'],"  In the summer of 2020 OpenAI released its GPT-3 autoregressive language modelto much fanfare. While the model has shown promise on tasks in several areas,it has not always been clear when the results were cherry-picked or when theywere the unvarnished output. We were particularly interested in what benefitsGPT-3 could bring to the SemEval 2021 MeasEval task - identifying measurementsand their associated attributes in scientific literature. We had alreadyexperimented with multi-turn questions answering as a solution to this task. Wewanted to see if we could use GPT-3's few-shot learning capabilities to moreeasily develop a solution that would have better performance than our priorwork. Unfortunately, we have not been successful in that effort. This paperdiscusses the approach we used, challenges we encountered, and results weobserved. Some of the problems we encountered were simply due to the state ofthe art. For example, the limits on the size of the prompt and answer limitedthe amount of the training signal that could be offered. Others are morefundamental. We are unaware of generative models that excel in retainingfactual information. Also, the impact of changes in the prompts isunpredictable, making it hard to reliably improve performance."
SIMPL: Generating Synthetic Overhead Imagery to Address Zero-shot and  Few-Shot Detection Problems,"['Yang Xu', 'Bohao Huang', 'Xiong Luo', 'Kyle Bradbury', 'Jordan M. Malof']",http://arxiv.org/pdf/2106.15681v1.pdf,2021-06-29,['cs.cv'],"  Recently deep neural networks (DNNs) have achieved tremendous success forobject detection in overhead (e.g., satellite) imagery. One ongoing challengehowever is the acquisition of training data, due to high costs of obtainingsatellite imagery and annotating objects in it. In this work we present asimple approach - termed Synthetic object IMPLantation (SIMPL) - to easily andrapidly generate large quantities of synthetic overhead training data forcustom target objects. We demonstrate the effectiveness of using SIMPLsynthetic imagery for training DNNs in zero-shot scenarios where no realimagery is available; and few-shot learning scenarios, where limited real-worldimagery is available. We also conduct experiments to study the sensitivity ofSIMPL's effectiveness to some key design parameters, providing users forinsights when designing synthetic imagery for custom objects. We release asoftware implementation of our SIMPL approach so that others can build upon it,or use it for their own custom problems."
Cross-domain Few-shot Learning with Task-specific Adapters,"['Wei-Hong Li', 'Xialei Liu', 'Hakan Bilen']",http://arxiv.org/pdf/2107.00358v4.pdf,2021-07-01,['cs.cv'],"  In this paper, we look at the problem of cross-domain few-shot classificationthat aims to learn a classifier from previously unseen classes and domains withfew labeled samples. Recent approaches broadly solve this problem byparameterizing their few-shot classifiers with task-agnostic and task-specificweights where the former is typically learned on a large training set and thelatter is dynamically predicted through an auxiliary network conditioned on asmall support set. In this work, we focus on the estimation of the latter, andpropose to learn task-specific weights from scratch directly on a small supportset, in contrast to dynamically estimating them. In particular, throughsystematic analysis, we show that task-specific weights through parametricadapters in matrix form with residual connections to multiple intermediatelayers of a backbone network significantly improves the performance of thestate-of-the-art models in the Meta-Dataset benchmark with minor additionalcost."
Sequential Recommendation for Cold-start Users with Meta Transitional  Learning,"['Jianling Wang', 'Kaize Ding', 'James Caverlee']",http://arxiv.org/pdf/2107.06427v1.pdf,2021-07-13,['cs.ir'],"  A fundamental challenge for sequential recommenders is to capture thesequential patterns of users toward modeling how users transit among items. Inmany practical scenarios, however, there are a great number of cold-start userswith only minimal logged interactions. As a result, existing sequentialrecommendation models will lose their predictive power due to the difficultiesin learning sequential patterns over users with only limited interactions. Inthis work, we aim to improve sequential recommendation for cold-start userswith a novel framework named MetaTL, which learns to model the transitionpatterns of users through meta-learning. Specifically, the proposed MetaTL: (i)formulates sequential recommendation for cold-start users as a few-shotlearning problem; (ii) extracts the dynamic transition patterns among userswith a translation-based architecture; and (iii) adopts meta transitionallearning to enable fast learning for cold-start users with only limitedinteractions, leading to accurate inference of sequential interactions."
Leveraging Hierarchical Structures for Few-Shot Musical Instrument  Recognition,"['Hugo Flores Garcia', 'Aldo Aguilar', 'Ethan Manilow', 'Bryan Pardo']",http://arxiv.org/pdf/2107.07029v2.pdf,2021-07-14,"['cs.sd', 'cs.lg', 'eess.as']","  Deep learning work on musical instrument recognition has generally focused oninstrument classes for which we have abundant data. In this work, we exploithierarchical relationships between instruments in a few-shot learning setup toenable classification of a wider set of musical instruments, given a fewexamples at inference. We apply a hierarchical loss function to the training ofprototypical networks, combined with a method to aggregate prototypeshierarchically, mirroring the structure of a predefined musical instrumenthierarchy. These extensions require no changes to the network architecture andnew levels can be easily added or removed. Compared to a non-hierarchicalfew-shot baseline, our method leads to a significant increase in classificationaccuracy and significant decrease mistake severity on instrument classes unseenin training."
FLEX: Unifying Evaluation for Few-Shot NLP,"['Jonathan Bragg', 'Arman Cohan', 'Kyle Lo', 'Iz Beltagy']",http://arxiv.org/pdf/2107.07170v2.pdf,2021-07-15,"['cs.cl', 'cs.lg', 'i.2.7']","  Few-shot NLP research is highly active, yet conducted in disjoint researchthreads with evaluation suites that lack challenging-yet-realistic testingsetups and fail to employ careful experimental design. Consequently, thecommunity does not know which techniques perform best or even if theyoutperform simple baselines. In response, we formulate the FLEX Principles, aset of requirements and best practices for unified, rigorous, valid, andcost-sensitive few-shot NLP evaluation. These principles include Sample SizeDesign, a novel approach to benchmark design that optimizes statisticalaccuracy and precision while keeping evaluation costs manageable. Following theprinciples, we release the FLEX benchmark, which includes four few-shottransfer settings, zero-shot evaluation, and a public leaderboard that coversdiverse NLP tasks. In addition, we present UniFew, a prompt-based model forfew-shot learning that unifies pretraining and finetuning prompt formats,eschewing complex machinery of recent prompt-based approaches in adaptingdownstream task formats to language model pretraining objectives. Wedemonstrate that despite simplicity, UniFew achieves results competitive withboth popular meta-learning and prompt-based approaches."
Wordcraft: a Human-AI Collaborative Editor for Story Writing,"['Andy Coenen', 'Luke Davis', 'Daphne Ippolito', 'Emily Reif', 'Ann Yuan']",http://arxiv.org/pdf/2107.07430v1.pdf,2021-07-15,['cs.cl'],"  As neural language models grow in effectiveness, they are increasingly beingapplied in real-world settings. However these applications tend to be limitedin the modes of interaction they support. In this extended abstract, we proposeWordcraft, an AI-assisted editor for story writing in which a writer and adialog system collaborate to write a story. Our novel interface uses few-shotlearning and the natural affordances of conversation to support a variety ofinteractions. Our editor provides a sandbox for writers to probe the boundariesof transformer-based language models and paves the way for futurehuman-in-the-loop training pipelines and novel evaluation methods."
Multi-Level Contrastive Learning for Few-Shot Problems,"['Qing Chen', 'Jian Zhang']",http://arxiv.org/pdf/2107.07608v1.pdf,2021-07-15,['cs.cv'],"  Contrastive learning is a discriminative approach that aims at groupingsimilar samples closer and diverse samples far from each other. It it anefficient technique to train an encoder generating distinguishable andinformative representations, and it may even increase the encoder'stransferability. Most current applications of contrastive learning benefit onlya single representation from the last layer of an encoder.In this paper, wepropose a multi-level contrasitive learning approach which applies contrastivelosses at different layers of an encoder to learn multiple representations fromthe encoder. Afterward, an ensemble can be constructed to take advantage of themultiple representations for the downstream tasks. We evaluated the proposedmethod on few-shot learning problems and conducted experiments using themini-ImageNet and the tiered-ImageNet datasets. Our model achieved the newstate-of-the-art results for both datasets, comparing to previous regular,ensemble, and contrastive learing (single-level) based approaches."
Boosting Few-Shot Classification with View-Learnable Contrastive  Learning,"['Xu Luo', 'Yuxuan Chen', 'Liangjian Wen', 'Lili Pan', 'Zenglin Xu']",http://arxiv.org/pdf/2107.09242v2.pdf,2021-07-20,['cs.cv'],"  The goal of few-shot classification is to classify new categories with fewlabeled examples within each class. Nowadays, the excellent performance inhandling few-shot classification problems is shown by metric-basedmeta-learning methods. However, it is very hard for previous methods todiscriminate the fine-grained sub-categories in the embedding space withoutfine-grained labels. This may lead to unsatisfactory generalization tofine-grained subcategories, and thus affects model interpretation. To tacklethis problem, we introduce the contrastive loss into few-shot classificationfor learning latent fine-grained structure in the embedding space. Furthermore,to overcome the drawbacks of random image transformation used in currentcontrastive learning in producing noisy and inaccurate image pairs (i.e.,views), we develop a learning-to-learn algorithm to automatically generatedifferent views of the same image. Extensive experiments on standard few-shotlearning benchmarks demonstrate the superiority of our method."
Design of a Graphical User Interface for Few-Shot Machine Learning  Classification of Electron Microscopy Data,"['Christina Doty', 'Shaun Gallagher', 'Wenqi Cui', 'Wenya Chen', 'Shweta Bhushan', 'Marjolein Oostrom', 'Sarah Akers', 'Steven R. Spurgeon']",http://arxiv.org/pdf/2107.10387v1.pdf,2021-07-21,"['cond-mat.mtrl-sci', 'cs.lg']","  The recent growth in data volumes produced by modern electron microscopesrequires rapid, scalable, and flexible approaches to image segmentation andanalysis. Few-shot machine learning, which can richly classify images from ahandful of user-provided examples, is a promising route to high-throughputanalysis. However, current command-line implementations of such approaches canbe slow and unintuitive to use, lacking the real-time feedback necessary toperform effective classification. Here we report on the development of aPython-based graphical user interface that enables end users to easily conductand visualize the output of few-shot learning models. This interface islightweight and can be hosted locally or on the web, providing the opportunityto reproducibly conduct, share, and crowd-source few-shot analyses."
External-Memory Networks for Low-Shot Learning of Targets in  Forward-Looking-Sonar Imagery,"['Isaac J. Sledge', 'Christopher D. Toole', 'Joseph A. Maestri', 'Jose C. Principe']",http://arxiv.org/pdf/2107.10504v1.pdf,2021-07-22,"['cs.cv', 'cs.lg']","  We propose a memory-based framework for real-time, data-efficient targetanalysis in forward-looking-sonar (FLS) imagery. Our framework relies on firstremoving non-discriminative details from the imagery using a small-scaleDenseNet-inspired network. Doing so simplifies ensuing analyses and permitsgeneralizing from few labeled examples. We then cascade the filtered imageryinto a novel NeuralRAM-based convolutional matching network, NRMN, for low-shottarget recognition. We employ a small-scale FlowNet, LFN to align and registerFLS imagery across local temporal scales. LFN enables target label consensusvoting across images and generally improves target detection and recognitionrates.  We evaluate our framework using real-world FLS imagery with multiple broadtarget classes that have high intra-class variability and rich sub-classstructure. We show that few-shot learning, with anywhere from ten to thirtyclass-specific exemplars, performs similarly to supervised deep networkstrained on hundreds of samples per class. Effective zero-shot learning is alsopossible. High performance is realized from the inductive-transfer propertiesof NRMNs when distractor elements are removed."
Improving Social Meaning Detection with Pragmatic Masking and Surrogate  Fine-Tuning,"['Chiyu Zhang', 'Muhammad Abdul-Mageed']",http://arxiv.org/pdf/2108.00356v4.pdf,2021-08-01,"['cs.cl', 'cs.ai']","  Masked language models (MLMs) are pre-trained with a denoising objective thatis in a mismatch with the objective of downstream fine-tuning. We proposepragmatic masking and surrogate fine-tuning as two complementing strategiesthat exploit social cues to drive pre-trained representations toward a broadset of concepts useful for a wide class of social meaning tasks. We test ourmodels on $15$ different Twitter datasets for social meaning detection. Ourmethods achieve $2.34\%$ $F_1$ over a competitive baseline, while outperformingdomain-specific language models pre-trained on large datasets. Our methods alsoexcel in few-shot learning: with only $5\%$ of training data (severelyfew-shot), our methods enable an impressive $68.54\%$ average $F_1$. Themethods are also language agnostic, as we show in a zero-shot setting involvingsix datasets from three different languages."
Recurrent Mask Refinement for Few-Shot Medical Image Segmentation,"['Hao Tang', 'Xingwei Liu', 'Shanlin Sun', 'Xiangyi Yan', 'Xiaohui Xie']",http://arxiv.org/pdf/2108.00622v2.pdf,2021-08-02,['cs.cv'],"  Although having achieved great success in medical image segmentation, deepconvolutional neural networks usually require a large dataset with manualannotations for training and are difficult to generalize to unseen classes.Few-shot learning has the potential to address these challenges by learning newclasses from only a few labeled examples. In this work, we propose a newframework for few-shot medical image segmentation based on prototypicalnetworks. Our innovation lies in the design of two key modules: 1) a contextrelation encoder (CRE) that uses correlation to capture local relation featuresbetween foreground and background regions; and 2) a recurrent mask refinementmodule that repeatedly uses the CRE and a prototypical network to recapture thechange of context relationship and refine the segmentation mask iteratively.Experiments on two abdomen CT datasets and an abdomen MRI dataset show theproposed method obtains substantial improvement over the state-of-the-artmethods by an average of 16.32%, 8.45% and 6.24% in terms of DSC, respectively.Code is publicly available."
From LSAT: The Progress and Challenges of Complex Reasoning,"['Siyuan Wang', 'Zhongkun Liu', 'Wanjun Zhong', 'Ming Zhou', 'Zhongyu Wei', 'Zhumin Chen', 'Nan Duan']",http://arxiv.org/pdf/2108.00648v1.pdf,2021-08-02,['cs.cl'],"  Complex reasoning aims to draw a correct inference based on complex rules. Asa hallmark of human intelligence, it involves a degree of explicit readingcomprehension, interpretation of logical knowledge and complex ruleapplication. In this paper, we take a step forward in complex reasoning bysystematically studying the three challenging and domain-general tasks of theLaw School Admission Test (LSAT), including analytical reasoning, logicalreasoning and reading comprehension. We propose a hybrid reasoning system tointegrate these three tasks and achieve impressive overall performance on theLSAT tests. The experimental results demonstrate that our system endows itselfa certain complex reasoning ability, especially the fundamental readingcomprehension and challenging logical reasoning capacities. Further analysisalso shows the effectiveness of combining the pre-trained models with thetask-specific reasoning module, and integrating symbolic knowledge intodiscrete interpretable reasoning steps in complex reasoning. We further shed alight on the potential future directions, like unsupervised symbolic knowledgeextraction, model interpretability, few-shot learning and comprehensivebenchmark for complex reasoning."
Impact of Aliasing on Generalization in Deep Convolutional Networks,"['Cristina Vasconcelos', 'Hugo Larochelle', 'Vincent Dumoulin', 'Rob Romijnders', 'Nicolas Le Roux', 'Ross Goroshin']",http://arxiv.org/pdf/2108.03489v1.pdf,2021-08-07,"['cs.cv', 'cs.lg']","  We investigate the impact of aliasing on generalization in Deep ConvolutionalNetworks and show that data augmentation schemes alone are unable to prevent itdue to structural limitations in widely used architectures. Drawing insightsfrom frequency analysis theory, we take a closer look at ResNet andEfficientNet architectures and review the trade-off between aliasing andinformation loss in each of their major components. We show how to mitigatealiasing by inserting non-trainable low-pass filters at key locations,particularly where networks lack the capacity to learn them. These simplearchitectural changes lead to substantial improvements in generalization oni.i.d. and even more on out-of-distribution conditions, such as imageclassification under natural corruptions on ImageNet-C [11] and few-shotlearning on Meta-Dataset [26]. State-of-the art results are achieved on bothdatasets without introducing additional trainable parameters and using thedefault hyper-parameters of open source codebases."
Transductive Few-Shot Classification on the Oblique Manifold,"['Guodong Qi', 'Huimin Yu', 'Zhaohui Lu', 'Shuzhao Li']",http://arxiv.org/pdf/2108.04009v1.pdf,2021-08-09,['cs.cv'],"  Few-shot learning (FSL) attempts to learn with limited data. In this work, weperform the feature extraction in the Euclidean space and the geodesic distancemetric on the Oblique Manifold (OM). Specially, for better feature extraction,we propose a non-parametric Region Self-attention with Spatial Pyramid Pooling(RSSPP), which realizes a trade-off between the generalization and thediscriminative ability of the single image feature. Then, we embed the featureto OM as a point. Furthermore, we design an Oblique Distance-based Classifier(ODC) that achieves classification in the tangent spaces which betterapproximate OM locally by learnable tangency points. Finally, we introduce anew method for parameters initialization and a novel loss function in thetransductive settings. Extensive experiments demonstrate the effectiveness ofour algorithm and it outperforms state-of-the-art methods on the popularbenchmarks: mini-ImageNet, tiered-ImageNet, and Caltech-UCSD Birds-200-2011(CUB)."
The Role of Global Labels in Few-Shot Classification and How to Infer  Them,"['Ruohan Wang', 'Massimiliano Pontil', 'Carlo Ciliberto']",http://arxiv.org/pdf/2108.04055v2.pdf,2021-08-09,"['cs.lg', 'stat.ml']","  Few-shot learning is a central problem in meta-learning, where learners mustquickly adapt to new tasks given limited training data. Recently, featurepre-training has become a ubiquitous component in state-of-the-artmeta-learning methods and is shown to provide significant performanceimprovement. However, there is limited theoretical understanding of theconnection between pre-training and meta-learning. Further, pre-trainingrequires global labels shared across tasks, which may be unavailable inpractice. In this paper, we show why exploiting pre-training is theoreticallyadvantageous for meta-learning, and in particular the critical role of globallabels. This motivates us to propose Meta Label Learning (MeLa), a novelmeta-learning framework that automatically infers global labels to obtainsrobust few-shot models. Empirically, we demonstrate that MeLa is competitivewith existing methods and provide extensive ablation experiments to highlightits key properties."
Noisy Channel Language Model Prompting for Few-Shot Text Classification,"['Sewon Min', 'Mike Lewis', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer']",http://arxiv.org/pdf/2108.04106v3.pdf,2021-08-09,"['cs.cl', 'cs.ai']","  We introduce a noisy channel approach for language model prompting infew-shot text classification. Instead of computing the likelihood of the labelgiven the input (referred as direct models), channel models compute theconditional probability of the input given the label, and are thereby requiredto explain every word in the input. We use channel models for recently proposedfew-shot learning methods with no or very limited updates to the language modelparameters, via either in-context demonstration or prompt tuning. Ourexperiments show that, for both methods, channel models significantlyoutperform their direct counterparts, which we attribute to their stability,i.e., lower variance and higher worst-case accuracy. We also present extensiveablations that provide recommendations for when to use channel prompt tuninginstead of other competitive methods (e.g., direct head tuning): channel prompttuning is preferred when the number of training examples is small, labels inthe training data are imbalanced, or generalization to unseen labels isrequired."
FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning,"['Jing Zhou', 'Yanan Zheng', 'Jie Tang', 'Jian Li', 'Zhilin Yang']",http://arxiv.org/pdf/2108.06332v2.pdf,2021-08-13,['cs.cl'],"  Most previous methods for text data augmentation are limited to simple tasksand weak baselines. We explore data augmentation on hard tasks (i.e., few-shotnatural language understanding) and strong baselines (i.e., pretrained modelswith over one billion parameters). Under this setting, we reproduced a largenumber of previous augmentation methods and found that these methods bringmarginal gains at best and sometimes degrade the performance much. To addressthis challenge, we propose a novel data augmentation method FlipDA that jointlyuses a generative model and a classifier to generate label-flipped data.Central to the idea of FlipDA is the discovery that generating label-flippeddata is more crucial to the performance than generating label-preserved data.Experiments show that FlipDA achieves a good tradeoff between effectiveness androbustness -- it substantially improves many tasks while not negativelyaffecting the others."
Few-Shot Batch Incremental Road Object Detection via Detector Fusion,"['Anuj Tambwekar', 'Kshitij Agrawal', 'Anay Majee', 'Anbumani Subramanian']",http://arxiv.org/pdf/2108.08048v1.pdf,2021-08-18,"['cs.cv', 'cs.ai']","  Incremental few-shot learning has emerged as a new and challenging area indeep learning, whose objective is to train deep learning models using very fewsamples of new class data, and none of the old class data. In this work wetackle the problem of batch incremental few-shot road object detection usingdata from the India Driving Dataset (IDD). Our approach, DualFusion, combinesobject detectors in a manner that allows us to learn to detect rare objectswith very limited data, all without severely degrading the performance of thedetector on the abundant classes. In the IDD OpenSet incremental few-shotdetection task, we achieve a mAP50 score of 40.0 on the base classes and anoverall mAP50 score of 38.8, both of which are the highest to date. In the COCObatch incremental few-shot detection task, we achieve a novel AP score of 9.9,surpassing the state-of-the-art novel class performance on the same by over 6.6times."
Neural TMDlayer: Modeling Instantaneous flow of features via SDE  Generators,"['Zihang Meng', 'Vikas Singh', 'Sathya N. Ravi']",http://arxiv.org/pdf/2108.08891v1.pdf,2021-08-19,"['cs.cv', 'cs.lg']","  We study how stochastic differential equation (SDE) based ideas can inspirenew modifications to existing algorithms for a set of problems in computervision. Loosely speaking, our formulation is related to both explicit andimplicit strategies for data augmentation and group equivariance, but isderived from new results in the SDE literature on estimating infinitesimalgenerators of a class of stochastic processes. If and when there is nominalagreement between the needs of an application/task and the inherent propertiesand behavior of the types of processes that we can efficiently handle, weobtain a very simple and efficient plug-in layer that can be incorporatedwithin any existing network architecture, with minimal modification and only afew additional parameters. We show promising experiments on a number of visiontasks including few shot learning, point cloud transformers and deepvariational segmentation obtaining efficiency or performance improvements."
On the Multilingual Capabilities of Very Large-Scale English Language  Models,"['Jordi Armengol-Estapé', 'Ona de Gibert Bonet', 'Maite Melero']",http://arxiv.org/pdf/2108.13349v1.pdf,2021-08-30,"['cs.cl', 'cs.ai']","  Generative Pre-trained Transformers (GPTs) have recently been scaled tounprecedented sizes in the history of machine learning. These models, solelytrained on the language modeling objective, have been shown to exhibitoutstanding few-shot learning capabilities in a number of different tasks.Nevertheless, aside from anecdotal experiences, little is known regarding theirmultilingual capabilities, given the fact that the pre-training corpus isalmost entirely composed of English text. In this work, we investigate themultilingual skills of GPT-3, focusing on one language that barely appears inthe pre-training corpus, Catalan, which makes the results especiallymeaningful; we assume that our results may be relevant for other languages aswell. We find that the model shows an outstanding performance, particularly ingenerative tasks, with predictable limitations mostly in language understandingtasks but still with remarkable results given the zero-shot scenario. Weinvestigate its potential and limits in extractive question-answering andnatural language generation, as well as the effect of scale in terms of modelsize."
Want To Reduce Labeling Cost? GPT-3 Can Help,"['Shuohang Wang', 'Yang Liu', 'Yichong Xu', 'Chenguang Zhu', 'Michael Zeng']",http://arxiv.org/pdf/2108.13487v1.pdf,2021-08-30,"['cs.cl', 'cs.ai']","  Data annotation is a time-consuming and labor-intensive process for many NLPtasks. Although there exist various methods to produce pseudo data labels, theyare often task-specific and require a decent amount of labeled data to startwith. Recently, the immense language model GPT-3 with 175 billion parametershas achieved tremendous improvement across many few-shot learning tasks. Inthis paper, we explore ways to leverage GPT-3 as a low-cost data labeler totrain other models. We find that, to make the downstream model achieve the sameperformance on a variety of NLU and NLG tasks, it costs 50% to 96% less to uselabels from GPT-3 than using labels from humans. Furthermore, we propose anovel framework of combining pseudo labels from GPT-3 with human labels, whichleads to even better performance with limited labeling budget. These resultspresent a cost-effective data labeling methodology that is generalizable tomany practical applications."
Semi-Supervised Exaggeration Detection of Health Science Press Releases,"['Dustin Wright', 'Isabelle Augenstein']",http://arxiv.org/pdf/2108.13493v1.pdf,2021-08-30,"['cs.cl', 'cs.lg']","  Public trust in science depends on honest and factual communication ofscientific papers. However, recent studies have demonstrated a tendency of newsmedia to misrepresent scientific papers by exaggerating their findings. Giventhis, we present a formalization of and study into the problem of exaggerationdetection in science communication. While there are an abundance of scientificpapers and popular media articles written about them, very rarely do thearticles include a direct link to the original paper, making data collectionchallenging. We address this by curating a set of labeled pressrelease/abstract pairs from existing expert annotated studies on exaggerationin press releases of scientific papers suitable for benchmarking theperformance of machine learning models on the task. Using limited data fromthis and previous studies on exaggeration detection in science, we introduceMT-PET, a multi-task version of Pattern Exploiting Training (PET), whichleverages knowledge from complementary cloze-style QA tasks to improve few-shotlearning. We demonstrate that MT-PET outperforms PET and supervised learningboth when data is limited, as well as when there is an abundance of data forthe main task."
ConQX: Semantic Expansion of Spoken Queries for Intent Detection based  on Conditioned Text Generation,"['Eyup Halit Yilmaz', 'Cagri Toraman']",http://arxiv.org/pdf/2109.00729v1.pdf,2021-09-02,"['cs.cl', 'cs.ai']","  Intent detection of spoken queries is a challenging task due to their noisystructure and short length. To provide additional information regarding thequery and enhance the performance of intent detection, we propose a method forsemantic expansion of spoken queries, called ConQX, which utilizes the textgeneration ability of an auto-regressive language model, GPT-2. To avoidoff-topic text generation, we condition the input query to a structured contextwith prompt mining. We then apply zero-shot, one-shot, and few-shot learning.We lastly use the expanded queries to fine-tune BERT and RoBERTa for intentdetection. The experimental results show that the performance of intentdetection can be improved by our semantic expansion method."
Do Prompt-Based Models Really Understand the Meaning of their Prompts?,"['Albert Webson', 'Ellie Pavlick']",http://arxiv.org/pdf/2109.01247v2.pdf,2021-09-02,['cs.cl'],"  Recently, a boom of papers has shown extraordinary progress in zero-shot andfew-shot learning with various prompt-based models. It is commonly argued thatprompts help models to learn faster in the same way that humans learn fasterwhen provided with task instructions expressed in natural language. In thisstudy, we experiment with over 30 prompt templates manually written for naturallanguage inference (NLI). We find that models learn just as fast with manyprompts that are intentionally irrelevant or even pathologically misleading asthey do with instructively ""good"" prompts. Further, such patterns hold even formodels as large as 175 billion parameters (Brown et al., 2020) as well as therecently proposed instruction-tuned models which are trained on hundreds ofprompts (Sanh et al., 2022). That is, instruction-tuned models often producegood predictions with irrelevant and misleading prompts even at zero shots. Insum, notwithstanding prompt-based models' impressive improvement, we findevidence of serious limitations that question the degree to which suchimprovement is derived from models understanding task instructions in waysanalogous to humans' use of task instructions."
Nearest Neighbour Few-Shot Learning for Cross-lingual Classification,"['M Saiful Bari', 'Batool Haider', 'Saab Mansour']",http://arxiv.org/pdf/2109.02221v1.pdf,2021-09-06,['cs.cl'],"  Even though large pre-trained multilingual models (e.g. mBERT, XLM-R) haveled to significant performance gains on a wide range of cross-lingual NLPtasks, success on many downstream tasks still relies on the availability ofsufficient annotated data. Traditional fine-tuning of pre-trained models usingonly a few target samples can cause over-fitting. This can be quite limiting asmost languages in the world are under-resourced. In this work, we investigatecross-lingual adaptation using a simple nearest neighbor few-shot (<15 samples)inference technique for classification tasks. We experiment using a total of 16distinct languages across two NLP tasks- XNLI and PAWS-X. Our approachconsistently improves traditional fine-tuning using only a handful of labeledsamples in target locales. We also demonstrate its generalization capabilityacross tasks."
MapRE: An Effective Semantic Mapping Approach for Low-resource Relation  Extraction,"['Manqing Dong', 'Chunguang Pan', 'Zhipeng Luo']",http://arxiv.org/pdf/2109.04108v1.pdf,2021-09-09,"['cs.cl', 'cs.ai']","  Neural relation extraction models have shown promising results in recentyears; however, the model performance drops dramatically given only a fewtraining samples. Recent works try leveraging the advance in few-shot learningto solve the low resource problem, where they train label-agnostic models todirectly compare the semantic similarities among context sentences in theembedding space. However, the label-aware information, i.e., the relation labelthat contains the semantic knowledge of the relation itself, is often neglectedfor prediction. In this work, we propose a framework considering bothlabel-agnostic and label-aware semantic mapping information for low resourcerelation extraction. We show that incorporating the above two types of mappinginformation in both pretraining and fine-tuning can significantly improve themodel performance on low-resource relation extraction tasks."
Compositional Clustering: Applications to Multi-Label Object Recognition  and Speaker Identification,"['Zeqian Li', 'Xinlu He', 'Jacob Whitehill']",http://arxiv.org/pdf/2109.04160v4.pdf,2021-09-09,['cs.lg'],"  We consider a novel clustering task in which clusters can have compositionalrelationships, e.g., one cluster contains images of rectangles, one containsimages of circles, and a third (compositional) cluster contains images withboth objects. In contrast to hierarchical clustering in which a parent clusterrepresents the intersection of properties of the child clusters, our problem isabout finding compositional clusters that represent the union of the propertiesof the constituent clusters. This task is motivated by recently developedfew-shot learning and embedding models can distinguish the label sets, not justthe individual labels, assigned to the examples. We propose three newalgorithms -- Compositional Affinity Propagation (CAP), Compositional k-means(CKM), and Greedy Compositional Reassignment (GCR) -- that can partitionexamples into coherent groups and infer the compositional structure among them.We show promising results, compared to popular algorithms such as Gaussianmixtures, Fuzzy c-means, and Agglomerative Clustering, on the OmniGlot andLibriSpeech datasets. Our work has applications to open-world multi-labelobject recognition and speaker identification & diarization with simultaneousspeech from multiple speakers."
Learning Opinion Summarizers by Selecting Informative Reviews,"['Arthur Bražinskas', 'Mirella Lapata', 'Ivan Titov']",http://arxiv.org/pdf/2109.04325v1.pdf,2021-09-09,"['cs.cl', 'cs.ai', 'cs.lg']","  Opinion summarization has been traditionally approached with unsupervised,weakly-supervised and few-shot learning techniques. In this work, we collect alarge dataset of summaries paired with user reviews for over 31,000 products,enabling supervised training. However, the number of reviews per product islarge (320 on average), making summarization - and especially training asummarizer - impractical. Moreover, the content of many reviews is notreflected in the human-written summaries, and, thus, the summarizer trained onrandom review subsets hallucinates. In order to deal with both of thesechallenges, we formulate the task as jointly learning to select informativesubsets of reviews and summarizing the opinions expressed in these subsets. Thechoice of the review subset is treated as a latent variable, predicted by asmall and simple selector. The subset is then fed into a more powerfulsummarizer. For joint training, we use amortized variational inference andpolicy gradient methods. Our experiments demonstrate the importance ofselecting informative reviews resulting in improved quality of summaries andreduced hallucinations."
STraTA: Self-Training with Task Augmentation for Better Few-shot  Learning,"['Tu Vu', 'Minh-Thang Luong', 'Quoc V. Le', 'Grady Simon', 'Mohit Iyyer']",http://arxiv.org/pdf/2109.06270v2.pdf,2021-09-13,['cs.cl'],"  Despite their recent successes in tackling many NLP tasks, large-scalepre-trained language models do not perform as well in few-shot settings whereonly a handful of training examples are available. To address this shortcoming,we propose STraTA, which stands for Self-Training with Task Augmentation, anapproach that builds on two key ideas for effective leverage of unlabeled data.First, STraTA uses task augmentation, a novel technique that synthesizes alarge amount of data for auxiliary-task fine-tuning from target-task unlabeledtexts. Second, STraTA performs self-training by further fine-tuning the strongbase model created by task augmentation on a broad distribution ofpseudo-labeled data. Our experiments demonstrate that STraTA can substantiallyimprove sample efficiency across 12 few-shot benchmarks. Remarkably, on theSST-2 sentiment dataset, STraTA, with only 8 training examples per class,achieves comparable results to standard fine-tuning with 67K training examples.Our analyses reveal that task augmentation and self-training are bothcomplementary and independently effective."
Few-Shot Emotion Recognition in Conversation with Sequential  Prototypical Networks,"['Gaël Guibon', 'Matthieu Labeau', 'Hélène Flamein', 'Luce Lefeuvre', 'Chloé Clavel']",http://arxiv.org/pdf/2109.09366v1.pdf,2021-09-20,"['cs.cl', 'cs.lg']","  Several recent studies on dyadic human-human interactions have been done onconversations without specific business objectives. However, many companiesmight benefit from studies dedicated to more precise environments such as aftersales services or customer satisfaction surveys. In this work, we placeourselves in the scope of a live chat customer service in which we want todetect emotions and their evolution in the conversation flow. This contextleads to multiple challenges that range from exploiting restricted, small andmostly unlabeled datasets to finding and adapting methods for such context.Wetackle these challenges by using Few-Shot Learning while making the hypothesisit can serve conversational emotion classification for different languages andsparse labels. We contribute by proposing a variation of Prototypical Networksfor sequence labeling in conversation that we name ProtoSeq. We test thismethod on two datasets with different languages: daily conversations in Englishand customer service chat conversations in French. When applied to emotionclassification in conversations, our method proved to be competitive even whencompared to other ones."
Few-Shot Sound Source Distance Estimation Using Relation Networks,"['Amirreza Sobhdel', 'Roozbeh Razavi-Far', 'Saeed Shahrivari']",http://arxiv.org/pdf/2109.10561v2.pdf,2021-09-22,"['cs.sd', 'eess.as']","  In this paper, we study the performance of few-shot learning, specificallymeta learning empowered few-shot relation networks, over classic supervisedlearning in the problem of sound source distance estimation(SSDE). In previousresearch on deep supervised SSDE, obtaining low accuracies due to the mismatchbetween the training data(sound from known environments) and the testdata(sound from unknown environments) has almost always been the case. Byperforming comparative experiments on a sufficient amount of data, we show thatthe few-shot relation network outperform a classic CNN which is a superviseddeep learning approach, and hence it is possible to calibrate amicrophone-equipped system, with a few labeled examples of audio recorded in aparticular unknown environment to adjust and generalize our classifier to thepossible input data and gain higher accuracies."
Learning by Examples Based on Multi-level Optimization,"['Shentong Mo', 'Pengtao Xie']",http://arxiv.org/pdf/2109.10824v1.pdf,2021-09-22,['cs.lg'],"  Learning by examples, which learns to solve a new problem by looking into howsimilar problems are solved, is an effective learning method in human learning.When a student learns a new topic, he/she finds out exemplar topics that aresimilar to this new topic and studies the exemplar topics to deepen theunderstanding of the new topic. We aim to investigate whether this powerfullearning skill can be borrowed from humans to improve machine learning as well.In this work, we propose a novel learning approach called Learning By Examples(LBE). Our approach automatically retrieves a set of training examples that aresimilar to query examples and predicts labels for query examples by using classlabels of the retrieved examples. We propose a three-level optimizationframework to formulate LBE which involves three stages of learning: learning aSiamese network to retrieve similar examples; learning a matching network tomake predictions on query examples by leveraging class labels of retrievedsimilar examples; learning the ``ground-truth'' similarities between trainingexamples by minimizing the validation loss. We develop an efficient algorithmto solve the LBE problem and conduct extensive experiments on variousbenchmarks where the results demonstrate the effectiveness of our method onboth supervised and few-shot learning."
Multimodality in Meta-Learning: A Comprehensive Survey,"['Yao Ma', 'Shilin Zhao', 'Weixiao Wang', 'Yaoman Li', 'Irwin King']",http://arxiv.org/pdf/2109.13576v2.pdf,2021-09-28,['cs.lg'],"  Meta-learning has gained wide popularity as a training framework that is moredata-efficient than traditional machine learning methods. However, itsgeneralization ability in complex task distributions, such as multimodal tasks,has not been thoroughly studied. Recently, some studies on multimodality-basedmeta-learning have emerged. This survey provides a comprehensive overview ofthe multimodality-based meta-learning landscape in terms of the methodologiesand applications. We first formalize the definition of meta-learning inmultimodality, along with the research challenges in this growing field, suchas how to enrich the input in few-shot learning (FSL) or zero-shot learning(ZSL) in multimodal scenarios and how to generalize the models to new tasks. Wethen propose a new taxonomy to discuss typical meta-learning algorithms inmultimodal tasks systematically. We investigate the contributions of relatedpapers and summarize them by our taxonomy. Finally, we propose potentialresearch directions for this promising field."
MetaHistoSeg: A Python Framework for Meta Learning in Histopathology  Image Segmentation,"['Zheng Yuan', 'Andre Esteva', 'Ran Xu']",http://arxiv.org/pdf/2109.14754v1.pdf,2021-09-29,"['eess.iv', 'cs.cv']","  Few-shot learning is a standard practice in most deep learning basedhistopathology image segmentation, given the relatively low number of digitizedslides that are generally available. While many models have been developed fordomain specific histopathology image segmentation, cross-domain generalizationremains a key challenge for properly validating models. Here, tooling anddatasets to benchmark model performance across histopathological domains arelacking. To address this limitation, we introduce MetaHistoSeg - a Pythonframework that implements unique scenarios in both meta learning and instancebased transfer learning. Designed for easy extension to customized datasets andtask sampling schemes, the framework empowers researchers with the ability ofrapid model design and experimentation. We also curate a histopathology metadataset - a benchmark dataset for training and validating models onout-of-distribution performance across a range of cancer types. In experimentswe showcase the usage of MetaHistoSeg with the meta dataset and find that bothmeta-learning and instance based transfer learning deliver comparable resultson average, but in some cases tasks can greatly benefit from one over theother."
UserIdentifier: Implicit User Representations for Simple and Effective  Personalized Sentiment Analysis,"['Fatemehsadat Mireshghallah', 'Vaishnavi Shrivastava', 'Milad Shokouhi', 'Taylor Berg-Kirkpatrick', 'Robert Sim', 'Dimitrios Dimitriadis']",http://arxiv.org/pdf/2110.00135v2.pdf,2021-10-01,"['cs.lg', 'cs.ai', 'cs.cl']","  Global models are trained to be as generalizable as possible, with userinvariance considered desirable since the models are shared across multitudesof users. As such, these models are often unable to produce personalizedresponses for individual users, based on their data. Contrary to widely-usedpersonalization techniques based on few-shot learning, we proposeUserIdentifier, a novel scheme for training a single shared model for allusers. Our approach produces personalized responses by adding fixed,non-trainable user identifiers to the input data. We empirically demonstratethat this proposed method outperforms the prefix-tuning based state-of-the-artapproach by up to 13%, on a suite of sentiment analysis datasets. We also showthat, unlike prior work, this method needs neither any additional modelparameters nor any extra rounds of few-shot fine-tuning."
Self-Attentive Constituency Parsing for UCCA-based Semantic Parsing,"['Necva Bölücü', 'Burcu Can']",http://arxiv.org/pdf/2110.00621v1.pdf,2021-10-01,"['cs.cl', '68t50', 'i.2.7']","  Semantic parsing provides a way to extract the semantic structure of a textthat could be understood by machines. It is utilized in various NLPapplications that require text comprehension such as summarization and questionanswering. Graph-based representation is one of the semantic representationapproaches to express the semantic structure of a text. Such representationsgenerate expressive and adequate graph-based target structures. In this paper,we focus primarily on UCCA graph-based semantic representation. The paper notonly presents the existing approaches proposed for UCCA representation, butalso proposes a novel self-attentive neural parsing model for the UCCArepresentation. We present the results for both single-lingual andcross-lingual tasks using zero-shot and few-shot learning for low-resourcelanguages."
Multi-Objective Few-shot Learning for Fair Classification,"['Ishani Mondal', 'Procheta Sen', 'Debasis Ganguly']",http://arxiv.org/pdf/2110.01951v1.pdf,2021-10-05,"['cs.lg', 'cs.cl']","  In this paper, we propose a general framework for mitigating the disparitiesof the predicted classes with respect to secondary attributes within the data(e.g., race, gender etc.). Our proposed method involves learning amulti-objective function that in addition to learning the primary objective ofpredicting the primary class labels from the data, also employs aclustering-based heuristic to minimize the disparities of the class labeldistribution with respect to the cluster memberships, with the assumption thateach cluster should ideally map to a distinct combination of attribute values.Experiments demonstrate effective mitigation of cognitive biases on a benchmarkdataset without the use of annotations of secondary attribute values (thezero-shot case) or with the use of a small number of attribute valueannotations (the few-shot case)."
Weak Novel Categories without Tears: A Survey on Weak-Shot Learning,['Li Niu'],http://arxiv.org/pdf/2110.02651v3.pdf,2021-10-06,['cs.cv'],"  Deep learning is a data-hungry approach, which requires massive trainingdata. However, it is time-consuming and labor-intensive to collect abundantfully-annotated training data for all categories. Assuming the existence ofbase categories with adequate fully-annotated training samples, differentparadigms requiring fewer training samples or weaker annotations for novelcategories have attracted growing research interest. Among them, zero-shot(resp., few-shot) learning explores using zero (resp., a few) training samplesfor novel categories, which lowers the quantity requirement for novelcategories. Instead, weak-shot learning lowers the quality requirement fornovel categories. Specifically, sufficient training samples are collected fornovel categories but they only have weak annotations. In different tasks, weakannotations are presented in different forms (e.g., noisy labels for imageclassification, image labels for object detection, bounding boxes forsegmentation), similar to the definitions in weakly supervised learning.Therefore, weak-shot learning can also be treated as weakly supervised learningwith auxiliary fully supervised categories. In this paper, we discuss theexisting weak-shot learning methodologies in different tasks and summarize thecodes at https://github.com/bcmi/Awesome-Weak-Shot-Learning."
Can Explanations Be Useful for Calibrating Black Box Models?,"['Xi Ye', 'Greg Durrett']",http://arxiv.org/pdf/2110.07586v2.pdf,2021-10-14,['cs.cl'],"  NLP practitioners often want to take existing trained models and apply themto data from new domains. While fine-tuning or few-shot learning can be used toadapt a base model, there is no single recipe for making these techniques work;moreover, one may not have access to the original model weights if it isdeployed as a black box. We study how to improve a black box model'sperformance on a new domain by leveraging explanations of the model's behavior.Our approach first extracts a set of features combining human intuition aboutthe task with model attributions generated by black box interpretationtechniques, then uses a simple calibrator, in the form of a classifier, topredict whether the base model was correct or not. We experiment with ourmethod on two tasks, extractive question answering and natural languageinference, covering adaptation from several pairs of domains with limitedtarget-domain data. The experimental results across all the domain pairs showthat explanations are useful for calibrating these models, boosting accuracywhen predictions do not have to be returned on every example. We further showthat the calibration model transfers to some extent between tasks."
Hyperseed: Unsupervised Learning with Vector Symbolic Architectures,"['Evgeny Osipov', 'Sachin Kahawala', 'Dilantha Haputhanthri', 'Thimal Kempitiya', 'Daswin De Silva', 'Damminda Alahakoon', 'Denis Kleyko']",http://arxiv.org/pdf/2110.08343v2.pdf,2021-10-15,['cs.ai'],"  Motivated by recent innovations in biologically-inspired neuromorphichardware, this article presents a novel unsupervised machine learning algorithmnamed Hyperseed that draws on the principles of Vector Symbolic Architectures(VSA) for fast learning of a topology preserving feature map of unlabelleddata. It relies on two major operations of VSA, binding and bundling. Thealgorithmic part of Hyperseed is expressed within Fourier Holographic ReducedRepresentations model, which is specifically suited for implementation onspiking neuromorphic hardware. The two primary contributions of the Hyperseedalgorithm are, few-shot learning and a learning rule based on single vectoroperation. These properties are empirically evaluated on synthetic datasets aswell as on illustrative benchmark use-cases, IRIS classification, and alanguage identification task using n-gram statistics. The results of theseexperiments confirm the capabilities of Hyperseed and its applications inneuromorphic hardware."
HRKD: Hierarchical Relational Knowledge Distillation for Cross-domain  Language Model Compression,"['Chenhe Dong', 'Yaliang Li', 'Ying Shen', 'Minghui Qiu']",http://arxiv.org/pdf/2110.08551v1.pdf,2021-10-16,"['cs.cl', 'cs.ai', 'cs.lg']","  On many natural language processing tasks, large pre-trained language models(PLMs) have shown overwhelming performances compared with traditional neuralnetwork methods. Nevertheless, their huge model size and low inference speedhave hindered the deployment on resource-limited devices in practice. In thispaper, we target to compress PLMs with knowledge distillation, and propose ahierarchical relational knowledge distillation (HRKD) method to capture bothhierarchical and domain relational information. Specifically, to enhance themodel capability and transferability, we leverage the idea of meta-learning andset up domain-relational graphs to capture the relational information acrossdifferent domains. And to dynamically select the most representative prototypesfor each domain, we propose a hierarchical compare-aggregate mechanism tocapture hierarchical relationships. Extensive experiments on publicmulti-domain datasets demonstrate the superior performance of our HRKD methodas well as its strong few-shot learning ability. For reproducibility, werelease the code at https://github.com/cheneydon/hrkd."
Contextual Gradient Scaling for Few-Shot Learning,"['Sanghyuk Lee', 'Seunghyun Lee', 'Byung Cheol Song']",http://arxiv.org/pdf/2110.10353v1.pdf,2021-10-20,['cs.cv'],"  Model-agnostic meta-learning (MAML) is a well-known optimization-basedmeta-learning algorithm that works well in various computer vision tasks, e.g.,few-shot classification. MAML is to learn an initialization so that a model canadapt to a new task in a few steps. However, since the gradient norm of aclassifier (head) is much bigger than those of backbone layers, the modelfocuses on learning the decision boundary of the classifier with similarrepresentations. Furthermore, gradient norms of high-level layers are smallthan those of the other layers. So, the backbone of MAML usually learnstask-generic features, which results in deteriorated adaptation performance inthe inner-loop. To resolve or mitigate this problem, we propose contextualgradient scaling (CxGrad), which scales gradient norms of the backbone tofacilitate learning task-specific knowledge in the inner-loop. Since thescaling factors are generated from task-conditioned parameters, gradient normsof the backbone can be scaled in a task-wise fashion. Experimental results showthat CxGrad effectively encourages the backbone to learn task-specificknowledge in the inner-loop and improves the performance of MAML up to asignificant margin in both same- and cross-domain few-shot classification."
MaskSplit: Self-supervised Meta-learning for Few-shot Semantic  Segmentation,"['Mustafa Sercan Amac', 'Ahmet Sencan', 'Orhun Bugra Baran', 'Nazli Ikizler-Cinbis', 'Ramazan Gokberk Cinbis']",http://arxiv.org/pdf/2110.12207v2.pdf,2021-10-23,['cs.cv'],"  Just like other few-shot learning problems, few-shot segmentation aims tominimize the need for manual annotation, which is particularly costly insegmentation tasks. Even though the few-shot setting reduces this cost fornovel test classes, there is still a need to annotate the training data. Toalleviate this need, we propose a self-supervised training approach forlearning few-shot segmentation models. We first use unsupervised saliencyestimation to obtain pseudo-masks on images. We then train a simple prototypebased model over different splits of pseudo masks and augmentations of images.Our extensive experiments show that the proposed approach achieves promisingresults, highlighting the potential of self-supervised training. To the best ofour knowledge this is the first work that addresses unsupervised few-shotsegmentation problem on natural images."
Meta-Learning for Multi-Label Few-Shot Classification,"['Christian Simon', 'Piotr Koniusz', 'Mehrtash Harandi']",http://arxiv.org/pdf/2110.13494v1.pdf,2021-10-26,['cs.cv'],"  Even with the luxury of having abundant data, multi-label classification iswidely known to be a challenging task to address. This work targets the problemof multi-label meta-learning, where a model learns to predict multiple labelswithin a query (e.g., an image) by just observing a few supporting examples. Indoing so, we first propose a benchmark for Few-Shot Learning (FSL) withmultiple labels per sample. Next, we discuss and extend several solutionsspecifically designed to address the conventional and single-label FSL, to workin the multi-label regime. Lastly, we introduce a neural module to estimate thelabel count of a given sample by exploiting the relational inference. We willshow empirically the benefit of the label count module, the label propagationalgorithm, and the extensions of conventional FSL methods on three challengingdatasets, namely MS-COCO, iMaterialist, and Open MIC. Overall, our thoroughexperiments suggest that the proposed label-propagation algorithm inconjunction with the neural label count module (NLC) shall be considered as themethod of choice."
Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP,"['Trapit Bansal', 'Karthick Gunasekaran', 'Tong Wang', 'Tsendsuren Munkhdalai', 'Andrew McCallum']",http://arxiv.org/pdf/2111.01322v1.pdf,2021-11-02,"['cs.cl', 'cs.lg']","  Meta-learning considers the problem of learning an efficient learning processthat can leverage its past experience to accurately solve new tasks. However,the efficacy of meta-learning crucially depends on the distribution of tasksavailable for training, and this is often assumed to be known a priori orconstructed from limited supervised datasets. In this work, we aim to providetask distributions for meta-learning by considering self-supervised tasksautomatically proposed from unlabeled text, to enable large-scale meta-learningin NLP. We design multiple distributions of self-supervised tasks byconsidering important aspects of task diversity, difficulty, type, domain, andcurriculum, and investigate how they affect meta-learning performance. Ouranalysis shows that all these factors meaningfully alter the task distribution,some inducing significant improvements in downstream few-shot accuracy of themeta-learned models. Empirically, results on 20 downstream tasks showsignificant improvements in few-shot learning -- adding up to +4.2% absoluteaccuracy (on average) to the previous unsupervised meta-learning method, andperform comparably to supervised methods on the FewRel 2.0 benchmark."
A MIMO Radar-Based Metric Learning Approach for Activity Recognition,"['Fady Aziz', 'Omar Metwally', 'Pascal Weller', 'Urs Schneider', 'Marco F. Huber']",http://arxiv.org/pdf/2111.01939v1.pdf,2021-11-02,"['eess.sp', 'cs.lg']","  Human activity recognition is seen of great importance in the medical andsurveillance fields. Radar has shown great feasibility for this field based onthe captured micro-Doppler ({\mu}-D) signatures. In this paper, a MIMO radar isused to formulate a novel micro-motion spectrogram for the angular velocity({\mu}-{\omega}) in non-tangential scenarios. Combining both the {\mu}-D andthe {\mu}-{\omega} signatures have shown better performance. Classificationaccuracy of 88.9% was achieved based on a metric learning approach. Theexperimental setup was designed to capture micro-motion signatures on differentaspect angles and line of sight (LOS). The utilized training dataset was ofsmaller size compared to the state-of-the-art techniques, where eightactivities were captured. A few-shot learning approach is used to adapt thepre-trained model for fall detection. The final model has shown aclassification accuracy of 86.42% for ten activities."
LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs,"['Christoph Schuhmann', 'Richard Vencu', 'Romain Beaumont', 'Robert Kaczmarczyk', 'Clayton Mullis', 'Aarush Katta', 'Theo Coombes', 'Jenia Jitsev', 'Aran Komatsuzaki']",http://arxiv.org/pdf/2111.02114v1.pdf,2021-11-03,"['cs.cv', 'cs.cl', 'cs.lg']","  Multi-modal language-vision models trained on hundreds of millions ofimage-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkablecapability to perform zero- or few-shot learning and transfer even in absenceof per-sample labels on target image data. Despite this trend, to date therehas been no publicly available datasets of sufficient scale for training suchmodels from scratch. To address this issue, in a community effort we build andrelease for public LAION-400M, a dataset with CLIP-filtered 400 millionimage-text pairs, their CLIP embeddings and kNN indices that allow efficientsimilarity search."
CoLLIE: Continual Learning of Language Grounding from Language-Image  Embeddings,"['Gabriel Skantze', 'Bram Willemsen']",http://arxiv.org/pdf/2111.07993v3.pdf,2021-11-15,['cs.cl'],"  This paper presents CoLLIE: a simple, yet effective model for continuallearning of how language is grounded in vision. Given a pre-trained multimodalembedding model, where language and images are projected in the same semanticspace (in this case CLIP by OpenAI), CoLLIE learns a transformation functionthat adjusts the language embeddings when needed to accommodate new languageuse. This is done by predicting the difference vector that needs to be applied,as well as a scaling factor for this vector, so that the adjustment is onlyapplied when needed. Unlike traditional few-shot learning, the model does notjust learn new classes and labels, but can also generalize to similar languageuse and leverage semantic compositionality. We verify the model's performanceon two different tasks of identifying the targets of referring expressions,where it has to learn new language use. The results show that the model canefficiently learn and generalize from only a few examples, with littleinterference with the model's original zero-shot performance."
ShufaNet: Classification method for calligraphers who have reached the  professional level,"['Ge Yunfei', 'Diao Changyu', 'Li Min', 'Yu Ruohan', 'Qiu Linshan', 'Xu Duanqing']",http://arxiv.org/pdf/2111.11350v1.pdf,2021-11-22,['cs.cv'],"  The authenticity of calligraphy is significant but difficult task in therealm of art, where the key problem is the few-shot classification ofcalligraphy. We propose a novel method, ShufaNet (""Shufa"" is the pinyin ofChinese calligraphy), to classify Chinese calligraphers' styles based on metriclearning in the case of few-shot, whose classification accuracy exceeds thelevel of students majoring in calligraphy. We present a new networkarchitecture, including the unique expression of the style of handwriting fontscalled ShufaLoss and the calligraphy category information as prior knowledge.Meanwhile, we modify the spatial attention module and create ShufaAttention forhandwriting fonts based on the traditional Chinese nine Palace thought. For thetraining of the model, we build a calligraphers' data set. Our method achieved65% accuracy rate in our data set for few-shot learning, surpassing resNet andother mainstream CNNs. Meanwhile, we conducted battle for calligraphy majorstudents, and finally surpassed them. This is the first attempt of deeplearning in the field of calligrapher classification, and we expect to provideideas for subsequent research."
Few-shot Named Entity Recognition with Cloze Questions,"['Valerio La Gatta', 'Vincenzo Moscato', 'Marco Postiglione', 'Giancarlo Sperlì']",http://arxiv.org/pdf/2111.12421v1.pdf,2021-11-24,"['cs.cl', 'cs.ai', 'cs.ir']","  Despite the huge and continuous advances in computational linguistics, thelack of annotated data for Named Entity Recognition (NER) is still achallenging issue, especially in low-resource languages and when domainknowledge is required for high-quality annotations. Recent findings in NLP showthe effectiveness of cloze-style questions in enabling language models toleverage the knowledge they acquired during the pre-training phase. In ourwork, we propose a simple and intuitive adaptation of Pattern-ExploitingTraining (PET), a recent approach which combines the cloze-questions mechanismand fine-tuning for few-shot learning: the key idea is to rephrase the NER taskwith patterns. Our approach achieves considerably better performance thanstandard fine-tuning and comparable or improved results with respect to otherfew-shot baselines without relying on manually annotated data or distantsupervision on three benchmark datasets: NCBI-disease, BC2GM and a privateItalian biomedical corpus."
Coarse-To-Fine Incremental Few-Shot Learning,"['Xiang Xiang', 'Yuwen Tan', 'Qian Wan', 'Jing Ma']",http://arxiv.org/pdf/2111.14806v1.pdf,2021-11-24,"['cs.cv', 'cs.lg']","  Different from fine-tuning models pre-trained on a large-scale dataset ofpreset classes, class-incremental learning (CIL) aims to recognize novelclasses over time without forgetting pre-trained classes. However, a givenmodel will be challenged by test images with finer-grained classes, e.g., abasenji is at most recognized as a dog. Such images form a new training set(i.e., support set) so that the incremental model is hoped to recognize abasenji (i.e., query) as a basenji next time. This paper formulates such ahybrid natural problem of coarse-to-fine few-shot (C2FS) recognition as a CILproblem named C2FSCIL, and proposes a simple, effective, andtheoretically-sound strategy Knowe: to learn, normalize, and freeze aclassifier's weights from fine labels, once learning an embedding spacecontrastively from coarse labels. Besides, as CIL aims at astability-plasticity balance, new overall performance metrics are proposed. Inthat sense, on CIFAR-100, BREEDS, and tieredImageNet, Knowe outperforms allrecent relevant CIL/FSCIL methods that are tailored to the new problem settingfor the first time."
CoNeRF: Controllable Neural Radiance Fields,"['Kacper Kania', 'Kwang Moo Yi', 'Marek Kowalski', 'Tomasz Trzciński', 'Andrea Tagliasacchi']",http://arxiv.org/pdf/2112.01983v2.pdf,2021-12-03,"['cs.cv', 'cs.gr']","  We extend neural 3D representations to allow for intuitive and interpretableuser control beyond novel view rendering (i.e. camera control). We allow theuser to annotate which part of the scene one wishes to control with just asmall number of mask annotations in the training images. Our key idea is totreat the attributes as latent variables that are regressed by the neuralnetwork given the scene encoding. This leads to a few-shot learning framework,where attributes are discovered automatically by the framework, whenannotations are not provided. We apply our method to various scenes withdifferent types of controllable attributes (e.g. expression control on humanfaces, or state control in movement of inanimate objects). Overall, wedemonstrate, to the best of our knowledge, for the first time novel view andnovel attribute re-rendering of scenes from a single video."
Learning from the Tangram to Solve Mini Visual Tasks,"['Yizhou Zhao', 'Liang Qiu', 'Pan Lu', 'Feng Shi', 'Tian Han', 'Song-Chun Zhu']",http://arxiv.org/pdf/2112.06113v1.pdf,2021-12-12,['cs.cv'],"  Current pre-training methods in computer vision focus on natural images inthe daily-life context. However, abstract diagrams such as icons and symbolsare common and important in the real world. This work is inspired by Tangram, agame that requires replicating an abstract pattern from seven dissected shapes.By recording human experience in solving tangram puzzles, we present theTangram dataset and show that a pre-trained neural model on the Tangram helpssolve some mini visual tasks based on low-resolution vision. Extensiveexperiments demonstrate that our proposed method generates intelligentsolutions for aesthetic tasks such as folding clothes and evaluating roomlayouts. The pre-trained feature extractor can facilitate the convergence offew-shot learning tasks on human handwriting and improve the accuracy inidentifying icons by their contours. The Tangram dataset is available athttps://github.com/yizhouzhao/Tangram."
Bioacoustic Event Detection with prototypical networks and data  augmentation,"['Mark Anderson', 'Naomi Harte']",http://arxiv.org/pdf/2112.09006v1.pdf,2021-12-16,['eess.as'],"  This report presents deep learning and data augmentation techniques used by asystem entered into the Few-Shot Bioacoustic Event Detection for the DCASE2021Challenge. The remit was to develop a few-shot learning system for animal(mammal and bird) vocalisations. Participants were tasked with developing amethod that can extract information from five exemplar vocalisations, or shots,of mammals or birds and detect and classify sounds in field recordings. In thesystem described in this report, prototypical networks are used to learn ametric space, from which classification is performed by computing the distanceof a query point to class prototypes, classifying based on shortest distance.We describe the architecture of this network, feature extraction methods, anddata augmentation performed on the given dataset and compare our work to thechallenge's baseline networks."
Pose Adaptive Dual Mixup for Few-Shot Single-View 3D Reconstruction,"['Ta-Ying Cheng', 'Hsuan-Ru Yang', 'Niki Trigoni', 'Hwann-Tzong Chen', 'Tyng-Luh Liu']",http://arxiv.org/pdf/2112.12484v1.pdf,2021-12-23,['cs.cv'],"  We present a pose adaptive few-shot learning procedure and a two-stage datainterpolation regularization, termed Pose Adaptive Dual Mixup (PADMix), forsingle-image 3D reconstruction. While augmentations via interpolatingfeature-label pairs are effective in classification tasks, they fall short inshape predictions potentially due to inconsistencies between interpolatedproducts of two images and volumes when rendering viewpoints are unknown.PADMix targets this issue with two sets of mixup procedures performedsequentially. We first perform an input mixup which, combined with a poseadaptive learning procedure, is helpful in learning 2D feature extraction andpose adaptive latent encoding. The stagewise training allows us to build uponthe pose invariant representations to perform a follow-up latent mixup underone-to-one correspondences between features and ground-truth volumes. PADMixsignificantly outperforms previous literature on few-shot settings over theShapeNet dataset and sets new benchmarks on the more challenging real-worldPix3D dataset."
3D Skeleton-based Few-shot Action Recognition with JEANIE is not so  Naïve,"['Lei Wang', 'Jun Liu', 'Piotr Koniusz']",http://arxiv.org/pdf/2112.12668v1.pdf,2021-12-23,"['cs.cv', 'cs.hc', 'cs.lg']","  In this paper, we propose a Few-shot Learning pipeline for 3D skeleton-basedaction recognition by Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE).To factor out misalignment between query and support sequences of 3D bodyjoints, we propose an advanced variant of Dynamic Time Warping which jointlymodels each smooth path between the query and support frames to achievesimultaneously the best alignment in the temporal and simulated cameraviewpoint spaces for end-to-end learning under the limited few-shot trainingdata. Sequences are encoded with a temporal block encoder based on SimpleSpectral Graph Convolution, a lightweight linear Graph Neural Network backbone(we also include a setting with a transformer). Finally, we propose asimilarity-based loss which encourages the alignment of sequences of the sameclass while preventing the alignment of unrelated sequences. We demonstratestate-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3DMultiview Activity II."
Associative Adversarial Learning Based on Selective Attack,"['Runqi Wang', 'Xiaoyue Duan', 'Baochang Zhang', 'Song Xue', 'Wentao Zhu', 'David Doermann', 'Guodong Guo']",http://arxiv.org/pdf/2112.13989v2.pdf,2021-12-28,['cs.cv'],"  A human's attention can intuitively adapt to corrupted areas of an image byrecalling a similar uncorrupted image they have previously seen. Thisobservation motivates us to improve the attention of adversarial images byconsidering their clean counterparts. To accomplish this, we introduceAssociative Adversarial Learning (AAL) into adversarial learning to guide aselective attack. We formulate the intrinsic relationship between attention andattack (perturbation) as a coupling optimization problem to improve theirinteraction. This leads to an attention backtracking algorithm that caneffectively enhance the attention's adversarial robustness. Our method isgeneric and can be used to address a variety of tasks by simply choosingdifferent kernels for the associative attention that select other regions for aspecific attack. Experimental results show that the selective attack improvesthe model's performance. We show that our method improves the recognitionaccuracy of adversarial training on ImageNet by 8.32% compared with thebaseline. It also increases object detection mAP on PascalVOC by 2.02% andrecognition accuracy of few-shot learning on miniImageNet by 1.63%."
On the Role of Neural Collapse in Transfer Learning,"['Tomer Galanti', 'András György', 'Marcus Hutter']",http://arxiv.org/pdf/2112.15121v2.pdf,2021-12-30,['cs.lg'],"  We study the ability of foundation models to learn representations forclassification that are transferable to new, unseen classes. Recent results inthe literature show that representations learned by a single classifier overmany classes are competitive on few-shot learning problems with representationslearned by special-purpose algorithms designed for such problems. In this paperwe provide an explanation for this behavior based on the recently observedphenomenon that the features learned by overparameterized classificationnetworks show an interesting clustering property, called neural collapse. Wedemonstrate both theoretically and empirically that neural collapse generalizesto new samples from the training classes, and -- more importantly -- to newclasses as well, allowing foundation models to provide feature maps that workwell in transfer learning and, specifically, in the few-shot setting."
Instance-aware Prompt Learning for Language Understanding and Generation,"['Feihu Jin', 'Jinliang Lu', 'Jiajun Zhang', 'Chengqing Zong']",http://arxiv.org/pdf/2201.07126v1.pdf,2022-01-18,['cs.cl'],"  Recently, prompt learning has become a new paradigm to utilize pre-trainedlanguage models (PLMs) and achieves promising results in downstream tasks witha negligible increase of parameters. The current usage of discrete andcontinuous prompts assumes that the prompt is fixed for a specific task and allsamples in the task share the same prompt. However, a task may contain quitediverse samples in which some are easy and others are difficult, and diverseprompts are desirable. In this paper, we propose an instance-aware promptlearning method that learns a different prompt for each instance. Specifically,we suppose that each learnable prompt token has a different contribution todifferent instances, and we learn the contribution by calculating the relevancescore between an instance and each prompt token. The contribution weightedprompt would be instance aware. We apply our method to both unidirectional andbidirectional PLMs on both language understanding and generation tasks.Extensive experiments demonstrate that our method obtains considerableimprovements compared to strong baselines. Especially, our method achieves thestate-of-the-art on the SuperGLUE few-shot learning benchmark."
Learning from One and Only One Shot,"['Haizi Yu', 'Igor Mineyev', 'Lav R. Varshney', 'James A. Evans']",http://arxiv.org/pdf/2201.08815v1.pdf,2022-01-14,"['cs.cv', 'cs.ai']","  Humans can generalize from only a few examples and from little pre-trainingon similar tasks. Yet, machine learning (ML) typically requires large data tolearn or pre-learn to transfer. Inspired by nativism, we directly model basichuman-innate priors in abstract visual tasks e.g., character/doodlerecognition. This yields a white-box model that learns general-appearancesimilarity -- how any two images look in general -- by mimicking how humansnaturally ""distort"" an object at first sight. Using simply the nearest-neighborclassifier on this similarity space, we achieve human-level characterrecognition using only 1--10 examples per class and nothing else (nopre-training). This differs from few-shot learning (FSL) using significantpre-training. On standard benchmarks MNIST/EMNIST and the Omniglot challenge,we outperform both neural-network-based and classical ML in the ""tiny-data""regime, including FSL pre-trained on large data. Our model enables unsupervisedlearning too: by learning the non-Euclidean, general-appearance similarityspace in a k-means style, we can generate human-intuitive archetypes as cluster``centroids''."
EASY: Ensemble Augmented-Shot Y-shaped Learning: State-Of-The-Art  Few-Shot Classification with Simple Ingredients,"['Yassir Bendou', 'Yuqing Hu', 'Raphael Lafargue', 'Giulia Lioi', 'Bastien Pasdeloup', 'Stéphane Pateux', 'Vincent Gripon']",http://arxiv.org/pdf/2201.09699v2.pdf,2022-01-24,"['cs.lg', 'cs.ai', 'cs.ne', 'eess.iv']","  Few-shot learning aims at leveraging knowledge learned by one or more deeplearning models, in order to obtain good classification performance on newproblems, where only a few labeled samples per class are available. Recentyears have seen a fair number of works in the field, introducing methods withnumerous ingredients. A frequent problem, though, is the use of suboptimallytrained models to extract knowledge, leading to interrogations on whetherproposed approaches bring gains compared to using better initial models withoutthe introduced ingredients. In this work, we propose a simple methodology, thatreaches or even beats state of the art performance on multiple standardizedbenchmarks of the field, while adding almost no hyperparameters or parametersto those used for training the initial deep learning models on the genericdataset. This methodology offers a new baseline on which to propose (and fairlycompare) new techniques or adapt existing ones."
From Examples to Rules: Neural Guided Rule Synthesis for Information  Extraction,"['Robert Vacareanu', 'Marco A. Valenzuela-Escarcega', 'George C. G. Barbosa', 'Rebecca Sharp', 'Mihai Surdeanu']",http://arxiv.org/pdf/2202.00475v1.pdf,2022-01-16,"['cs.cl', 'cs.ir', 'cs.lg']","  While deep learning approaches to information extraction have had manysuccesses, they can be difficult to augment or maintain as needs shift.Rule-based methods, on the other hand, can be more easily modified. However,crafting rules requires expertise in linguistics and the domain of interest,making it infeasible for most users. Here we attempt to combine the advantagesof these two directions while mitigating their drawbacks. We adapt recentadvances from the adjacent field of program synthesis to informationextraction, synthesizing rules from provided examples. We use atransformer-based architecture to guide an enumerative search, and show thatthis reduces the number of steps that need to be explored before a rule isfound. Further, we show that without training the synthesis algorithm on thespecific domain, our synthesized rules achieve state-of-the-art performance onthe 1-shot scenario of a task that focuses on few-shot learning for relationclassification, and competitive performance in the 5-shot scenario."
Advances in MetaDL: AAAI 2021 challenge and workshop,"['Adrian El Baz', 'Isabelle Guyon', 'Zhengying Liu', 'Jan van Rijn', 'Sebastien Treguer', 'Joaquin Vanschoren']",http://arxiv.org/pdf/2202.01890v1.pdf,2022-02-01,['cs.cv'],"  To stimulate advances in metalearning using deep learning techniques(MetaDL), we organized in 2021 a challenge and an associated workshop. Thispaper presents the design of the challenge and its results, and summarizespresentations made at the workshop. The challenge focused on few-shot learningclassification tasks of small images. Participants' code submissions were runin a uniform manner, under tight computational constraints. This put pressureon solution designs to use existing architecture backbones and/or pre-trainednetworks. Winning methods featured various classifiers trained on top of thesecond last layer of popular CNN backbones, fined-tuned on the meta-trainingdata (not necessarily in an episodic manner), then trained on the labeledsupport and tested on the unlabeled query sets of the meta-test data."
Exemplar-Based Contrastive Self-Supervised Learning with Few-Shot Class  Incremental Learning,['Daniel T. Chang'],http://arxiv.org/pdf/2202.02601v1.pdf,2022-02-05,['cs.lg'],"  Humans are capable of learning new concepts from only a few (labeled)exemplars, incrementally and continually. This happens within the context thatwe can differentiate among the exemplars, and between the exemplars and largeamounts of other data (unlabeled and labeled). This suggests, in humanlearning, supervised learning of concepts based on exemplars takes place withinthe larger context of contrastive self-supervised learning (CSSL) based onunlabeled and labeled data. We discuss extending CSSL (1) to be based mainly onexemplars and only secondly on data augmentation, and (2) to apply to bothunlabeled data (a large amount is available in general) and labeled data (a fewexemplars can be obtained with valuable supervised knowledge). A major benefitof the extensions is that exemplar-based CSSL, with supervised finetuning,supports few-shot class incremental learning (CIL). Specifically, we discussexemplar-based CSSL including: nearest-neighbor CSSL, neighborhood CSSL withsupervised pretraining, and exemplar CSSL with supervised finetuning. Wefurther discuss using exemplar-based CSSL to facilitate few-shot learning and,in particular, few-shot CIL."
MAML and ANIL Provably Learn Representations,"['Liam Collins', 'Aryan Mokhtari', 'Sewoong Oh', 'Sanjay Shakkottai']",http://arxiv.org/pdf/2202.03483v2.pdf,2022-02-07,['cs.lg'],"  Recent empirical evidence has driven conventional wisdom to believe thatgradient-based meta-learning (GBML) methods perform well at few-shot learningbecause they learn an expressive data representation that is shared acrosstasks. However, the mechanics of GBML have remained largely mysterious from atheoretical perspective. In this paper, we prove that two well-known GBMLmethods, MAML and ANIL, as well as their first-order approximations, arecapable of learning common representation among a set of given tasks.Specifically, in the well-known multi-task linear representation learningsetting, they are able to recover the ground-truth representation at anexponentially fast rate. Moreover, our analysis illuminates that the drivingforce causing MAML and ANIL to recover the underlying representation is thatthey adapt the final layer of their model, which harnesses the underlying taskdiversity to improve the representation in all directions of interest. To thebest of our knowledge, these are the first results to show that MAML and/orANIL learn expressive representations and to rigorously explain why they do so."
Generating Training Data with Language Models: Towards Zero-Shot  Language Understanding,"['Yu Meng', 'Jiaxin Huang', 'Yu Zhang', 'Jiawei Han']",http://arxiv.org/pdf/2202.04538v2.pdf,2022-02-09,"['cs.cl', 'cs.lg']","  Pretrained language models (PLMs) have demonstrated remarkable performance invarious natural language processing tasks: Unidirectional PLMs (e.g., GPT) arewell known for their superior text generation capabilities; bidirectional PLMs(e.g., BERT) have been the prominent choice for natural language understanding(NLU) tasks. While both types of models have achieved promising few-shotlearning performance, their potential for zero-shot learning has beenunderexplored. In this paper, we present a simple approach that uses both typesof PLMs for fully zero-shot learning of NLU tasks without requiring anytask-specific data: A unidirectional PLM generates class-conditioned textsguided by prompts, which are used as the training data for fine-tuning abidirectional PLM. With quality training data selected based on the generationprobability and regularization techniques (label smoothing and temporalensembling) applied to the fine-tuning stage for better generalization andstability, our approach demonstrates strong performance across sevenclassification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and92.8 on SST-2), significantly outperforming zero-shot prompting methods andachieving even comparable results to strong few-shot approaches using 32training samples per class."
Bias-Eliminated Semantic Refinement for Any-Shot Learning,"['Liangjun Feng', 'Chunhui Zhao', 'Xi Li']",http://arxiv.org/pdf/2202.04827v1.pdf,2022-02-10,"['cs.cv', 'cs.ai']","  When training samples are scarce, the semantic embedding technique, ie,describing class labels with attributes, provides a condition to generatevisual features for unseen objects by transferring the knowledge from seenobjects. However, semantic descriptions are usually obtained in an externalparadigm, such as manual annotation, resulting in weak consistency betweendescriptions and visual features. In this paper, we refine the coarse-grainedsemantic description for any-shot learning tasks, ie, zero-shot learning (ZSL),generalized zero-shot learning (GZSL), and few-shot learning (FSL). A newmodel, namely, the semantic refinement Wasserstein generative adversarialnetwork (SRWGAN) model, is designed with the proposed multihead representationand hierarchical alignment techniques. Unlike conventional methods, semanticrefinement is performed with the aim of identifying a bias-eliminated conditionfor disjoint-class feature generation and is applicable in both inductive andtransductive settings. We extensively evaluate model performance on sixbenchmark datasets and observe state-of-the-art results for any-shot learning;eg, we obtain 70.2% harmonic accuracy for the Caltech UCSD Birds (CUB) datasetand 82.2% harmonic accuracy for the Oxford Flowers (FLO) dataset in thestandard GZSL setting. Various visualizations are also provided to show thebias-eliminated generation of SRWGAN. Our code is available."
Cross Domain Few-Shot Learning via Meta Adversarial Training,"['Jirui Qi', 'Richong Zhang', 'Chune Li', 'Yongyi Mao']",http://arxiv.org/pdf/2202.05713v3.pdf,2022-02-11,"['cs.lg', 'cs.ai']","  Few-shot relation classification (RC) is one of the critical problems inmachine learning. Current research merely focuses on the set-ups that bothtraining and testing are from the same domain. However, in practice, thisassumption is not always guaranteed. In this study, we present a novel modelthat takes into consideration the afore-mentioned cross-domain situation. Notlike previous models, we only use the source domain data to train theprototypical networks and test the model on target domain data. A meta-basedadversarial training framework (MBATF) is proposed to fine-tune the trainednetworks for adapting to data from the target domain. Empirical studies confirmthe effectiveness of the proposed model."
A Modern Self-Referential Weight Matrix That Learns to Modify Itself,"['Kazuki Irie', 'Imanol Schlag', 'Róbert Csordás', 'Jürgen Schmidhuber']",http://arxiv.org/pdf/2202.05780v2.pdf,2022-02-11,['cs.lg'],"  The weight matrix (WM) of a neural network (NN) is its program. The programsof many traditional NNs are learned through gradient descent in some errorfunction, then remain fixed. The WM of a self-referential NN, however, can keeprapidly modifying all of itself during runtime. In principle, such NNs canmeta-learn to learn, and meta-meta-learn to meta-learn to learn, and so on, inthe sense of recursive self-improvement. While NN architectures potentiallycapable of implementing such behaviour have been proposed since the '90s, therehave been few if any practical studies. Here we revisit such NNs, building uponrecent successes of fast weight programmers and closely related linearTransformers. We propose a scalable self-referential WM (SRWM) that learns touse outer products and the delta update rule to modify itself. We evaluate ourSRWM in supervised few-shot learning and in multi-task reinforcement learningwith procedurally generated game environments. Our experiments demonstrate bothpractical applicability and competitive performance of the proposed SRWM. Ourcode is public."
Task-Adaptive Feature Transformer with Semantic Enrichment for Few-Shot  Segmentation,"['Jun Seo', 'Young-Hyun Park', 'Sung Whan Yoon', 'Jaekyun Moon']",http://arxiv.org/pdf/2202.06498v1.pdf,2022-02-14,['cs.cv'],"  Few-shot learning allows machines to classify novel classes using only a fewlabeled samples. Recently, few-shot segmentation aiming at semanticsegmentation on low sample data has also seen great interest. In this paper, wepropose a learnable module that can be placed on top of existing segmentationnetworks for performing few-shot segmentation. This module, called thetask-adaptive feature transformer (TAFT), linearly transforms task-specifichigh-level features to a set of task agnostic features well-suited toconducting few-shot segmentation. The task-conditioned feature transformationallows an effective utilization of the semantic information in novel classes togenerate tight segmentation masks. We also propose a semantic enrichment (SE)module that utilizes a pixel-wise attention module for high-level feature andan auxiliary loss from an auxiliary segmentation network conducting thesemantic segmentation for all training classes. Experiments on PASCAL-$5^i$ andCOCO-$20^i$ datasets confirm that the added modules successfully extend thecapability of existing segmentators to yield highly competitive few-shotsegmentation performances."
Variational Autoencoder with Disentanglement Priors for Low-Resource  Task-Specific Natural Language Generation,"['Zhuang Li', 'Lizhen Qu', 'Qiongkai Xu', 'Tongtong Wu', 'Tianyang Zhan', 'Gholamreza Haffari']",http://arxiv.org/pdf/2202.13363v3.pdf,2022-02-27,['cs.cl'],"  In this paper, we propose a variational autoencoder with disentanglementpriors, VAE-DPRIOR, for task-specific natural language generation with none ora handful of task-specific labeled examples. In order to tackle compositionalgeneralization across tasks, our model performs disentangled representationlearning by introducing a conditional prior for the latent content space andanother conditional prior for the latent label space. Both types of priorssatisfy a novel property called $\epsilon$-disentangled. We show bothempirically and theoretically that the novel priors can disentanglerepresentations even without specific regularizations as in the prior work. Thecontent prior enables directly sampling diverse content representations fromthe content space learned from the seen tasks, and fuse them with therepresentations of novel tasks for generating semantically diverse texts in thelow-resource settings. Our extensive experiments demonstrate the superiorperformance of our model over competitive baselines in terms of i) dataaugmentation in continuous zero/few-shot learning, and ii) text style transferin the few-shot setting."
ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer  for Event-Centric Generation and Classification,"['Yucheng Zhou', 'Tao Shen', 'Xiubo Geng', 'Guodong Long', 'Daxin Jiang']",http://arxiv.org/pdf/2203.02225v2.pdf,2022-03-04,['cs.cl'],"  Generating new events given context with correlated ones plays a crucial rolein many event-centric reasoning tasks. Existing works either limit their scopeto specific scenarios or overlook event-level correlations. In this paper, wepropose to pre-train a general Correlation-aware context-to-Event Transformer(ClarET) for event-centric reasoning. To achieve this, we propose three novelevent-centric objectives, i.e., whole event recovering, contrastiveevent-correlation encoding and prompt-based event locating, which highlightevent-level correlations with effective training. The proposed ClarET isapplicable to a wide range of event-centric reasoning scenarios, consideringits versatility of (i) event-correlation types (e.g., causal, temporal,contrast), (ii) application formulations (i.e., generation and classification),and (iii) reasoning types (e.g., abductive, counterfactual and endingreasoning). Empirical fine-tuning results, as well as zero- and few-shotlearning, on 9 benchmarks (5 generation and 4 classification tasks covering 4reasoning types with diverse event correlations), verify its effectiveness andgeneralization ability."
Pre-trained Token-replaced Detection Model as Few-shot Learner,"['Zicheng Li', 'Shoushan Li', 'Guodong Zhou']",http://arxiv.org/pdf/2203.03235v2.pdf,2022-03-07,"['cs.cl', 'cs.ai']","  Pre-trained masked language models have demonstrated remarkable ability asfew-shot learners. In this paper, as an alternative, we propose a novelapproach to few-shot learning with pre-trained token-replaced detection modelslike ELECTRA. In this approach, we reformulate a classification or a regressiontask as a token-replaced detection problem. Specifically, we first define atemplate and label description words for each task and put them into the inputto form a natural language prompt. Then, we employ the pre-trainedtoken-replaced detection model to predict which label description word is themost original (i.e., least replaced) among all label description words in theprompt. A systematic evaluation on 16 datasets demonstrates that our approachoutperforms few-shot learners with pre-trained masked language models in bothone-sentence and two-sentence learning tasks."
InstructionNER: A Multi-Task Instruction-Based Generative Framework for  Few-shot NER,"['Liwen Wang', 'Rumei Li', 'Yang Yan', 'Yuanmeng Yan', 'Sirui Wang', 'Wei Wu', 'Weiran Xu']",http://arxiv.org/pdf/2203.03903v1.pdf,2022-03-08,['cs.cl'],"  Recently, prompt-based methods have achieved significant performance infew-shot learning scenarios by bridging the gap between language modelpre-training and fine-tuning for downstream tasks. However, existing prompttemplates are mostly designed for sentence-level tasks and are inappropriatefor sequence labeling objectives. To address the above issue, we propose amulti-task instruction-based generative framework, named InstructionNER, forlow-resource named entity recognition. Specifically, we reformulate the NERtask as a generation problem, which enriches source sentences withtask-specific instructions and answer options, then inferences the entities andtypes in natural language. We further propose two auxiliary tasks, includingentity extraction and entity typing, which enable the model to capture moreboundary information of entities and deepen the understanding of entity typesemantics, respectively. Experimental results show that our method consistentlyoutperforms other baselines on five datasets in few-shot settings."
Worst Case Matters for Few-Shot Recognition,"['Minghao Fu', 'Yun-Hao Cao', 'Jianxin Wu']",http://arxiv.org/pdf/2203.06574v2.pdf,2022-03-13,"['cs.cv', 'cs.ai']","  Few-shot recognition learns a recognition model with very few (e.g., 1 or 5)images per category, and current few-shot learning methods focus on improvingthe average accuracy over many episodes. We argue that in real-worldapplications we may often only try one episode instead of many, and hencemaximizing the worst-case accuracy is more important than maximizing theaverage accuracy. We empirically show that a high average accuracy notnecessarily means a high worst-case accuracy. Since this objective is notaccessible, we propose to reduce the standard deviation and increase theaverage accuracy simultaneously. In turn, we devise two strategies from thebias-variance tradeoff perspective to implicitly reach this goal: a simple yeteffective stability regularization (SR) loss together with model ensemble toreduce variance during fine-tuning, and an adaptability calibration mechanismto reduce the bias. Extensive experiments on benchmark datasets demonstrate theeffectiveness of the proposed strategies, which outperforms currentstate-of-the-art methods with a significant margin in terms of not onlyaverage, but also worst-case accuracy. Our code is available athttps://github.com/heekhero/ACSR."
Deep Transfer Learning with Graph Neural Network for Sensor-Based Human  Activity Recognition,"['Yan Yan', 'Tianzheng Liao', 'Jinjin Zhao', 'Jiahong Wang', 'Liang Ma', 'Wei Lv', 'Jing Xiong', 'Lei Wang']",http://arxiv.org/pdf/2203.07910v1.pdf,2022-03-14,"['cs.cv', 'cs.hc', 'cs.lg']","  The sensor-based human activity recognition (HAR) in mobile applicationscenarios is often confronted with sensor modalities variation and annotateddata deficiency. Given this observation, we devised a graph-inspired deeplearning approach toward the sensor-based HAR tasks, which was further used tobuild a deep transfer learning model toward giving a tentative solution forthese two challenging problems. Specifically, we present a multi-layer residualstructure involved graph convolutional neural network (ResGCNN) toward thesensor-based HAR tasks, namely the HAR-ResGCNN approach. Experimental resultson the PAMAP2 and mHealth data sets demonstrate that our ResGCNN is effectiveat capturing the characteristics of actions with comparable results compared toother sensor-based HAR models (with an average accuracy of 98.18% and 99.07%,respectively). More importantly, the deep transfer learning experiments usingthe ResGCNN model show excellent transferability and few-shot learningperformance. The graph-based framework shows good meta-learning ability and issupposed to be a promising solution in sensor-based HAR tasks."
Label Semantics for Few Shot Named Entity Recognition,"['Jie Ma', 'Miguel Ballesteros', 'Srikanth Doss', 'Rishita Anubhai', 'Sunil Mallya', 'Yaser Al-Onaizan', 'Dan Roth']",http://arxiv.org/pdf/2203.08985v1.pdf,2022-03-16,['cs.cl'],"  We study the problem of few shot learning for named entity recognition.Specifically, we leverage the semantic information in the names of the labelsas a way of giving the model additional signal and enriched priors. We proposea neural architecture that consists of two BERT encoders, one to encode thedocument and its tokens and another one to encode each of the labels in naturallanguage format. Our model learns to match the representations of namedentities computed by the first encoder with label representations computed bythe second encoder. The label semantics signal is shown to support improvedstate-of-the-art results in multiple few shot NER benchmarks and on-parperformance in standard benchmarks. Our model is especially effective in lowresource settings."
Prototypical Verbalizer for Prompt-based Few-shot Tuning,"['Ganqu Cui', 'Shengding Hu', 'Ning Ding', 'Longtao Huang', 'Zhiyuan Liu']",http://arxiv.org/pdf/2203.09770v1.pdf,2022-03-18,"['cs.cl', 'cs.lg']","  Prompt-based tuning for pre-trained language models (PLMs) has shown itseffectiveness in few-shot learning. Typically, prompt-based tuning wraps theinput text into a cloze question. To make predictions, the model maps theoutput words to labels via a verbalizer, which is either manually designed orautomatically built. However, manual verbalizers heavily depend ondomain-specific prior knowledge and human efforts, while finding appropriatelabel words automatically still remains challenging.In this work, we proposethe prototypical verbalizer (ProtoVerb) which is built directly from trainingdata. Specifically, ProtoVerb learns prototype vectors as verbalizers bycontrastive learning. In this way, the prototypes summarize training instancesand are able to enclose rich class-level semantics. We conduct experiments onboth topic classification and entity typing tasks, and the results demonstratethat ProtoVerb significantly outperforms current automatic verbalizers,especially when training data is extremely scarce. More surprisingly, ProtoVerbconsistently boosts prompt-based tuning even on untuned PLMs, indicating anelegant non-tuning way to utilize PLMs. Our codes are avaliable athttps://github.com/thunlp/OpenPrompt."
HyperShot: Few-Shot Learning by Kernel HyperNetworks,"['Marcin Sendera', 'Marcin Przewięźlikowski', 'Konrad Karanowski', 'Maciej Zięba', 'Jacek Tabor', 'Przemysław Spurek']",http://arxiv.org/pdf/2203.11378v1.pdf,2022-03-21,"['cs.lg', 'cs.ai', 'cs.cv']","  Few-shot models aim at making predictions using a minimal number of labeledexamples from a given task. The main challenge in this area is the one-shotsetting where only one element represents each class. We propose HyperShot -the fusion of kernels and hypernetwork paradigm. Compared to referenceapproaches that apply a gradient-based adjustment of the parameters, our modelaims to switch the classification module parameters depending on the task'sembedding. In practice, we utilize a hypernetwork, which takes the aggregatedinformation from support data and returns the classifier's parametershandcrafted for the considered problem. Moreover, we introduce the kernel-basedrepresentation of the support examples delivered to hypernetwork to create theparameters of the classification module. Consequently, we rely on relationsbetween embeddings of the support examples instead of direct feature valuesprovided by the backbone models. Thanks to this approach, our model can adaptto highly different tasks."
Multidimensional Belief Quantification for Label-Efficient Meta-Learning,"['Deep Pandey', 'Qi Yu']",http://arxiv.org/pdf/2203.12768v1.pdf,2022-03-23,['cs.cv'],"  Optimization-based meta-learning offers a promising direction for few-shotlearning that is essential for many real-world computer vision applications.However, learning from few samples introduces uncertainty, and quantifyingmodel confidence for few-shot predictions is essential for many criticaldomains. Furthermore, few-shot tasks used in meta training are usually sampledrandomly from a task distribution for an iterative model update, leading tohigh labeling costs and computational overhead in meta-training. We propose anovel uncertainty-aware task selection model for label efficient meta-learning.The proposed model formulates a multidimensional belief measure, which canquantify the known uncertainty and lower bound the unknown uncertainty of anygiven task. Our theoretical result establishes an important relationshipbetween the conflicting belief and the incorrect belief. The theoretical resultallows us to estimate the total uncertainty of a task, which provides aprincipled criterion for task selection. A novel multi-query task formulationis further developed to improve both the computational and labeling efficiencyof meta-learning. Experiments conducted over multiple real-world few-shot imageclassification tasks demonstrate the effectiveness of the proposed model."
Few-Shot Learning with Siamese Networks and Label Tuning,"['Thomas Müller', 'Guillermo Pérez-Torró', 'Marc Franco-Salvador']",http://arxiv.org/pdf/2203.14655v2.pdf,2022-03-28,"['cs.cl', 'cs.lg']","  We study the problem of building text classifiers with little or no trainingdata, commonly known as zero and few-shot text classification. In recent years,an approach based on neural textual entailment models has been found to givestrong results on a diverse range of tasks. In this work, we show that withproper pre-training, Siamese Networks that embed texts and labels offer acompetitive alternative. These models allow for a large reduction in inferencecost: constant in the number of labels rather than linear. Furthermore, weintroduce label tuning, a simple and computationally efficient approach thatallows to adapt the models in a few-shot setup by only changing the labelembeddings. While giving lower performance than model fine-tuning, thisapproach has the architectural advantage that a single encoder can be shared bymany different tasks."
$k$NN-NER: Named Entity Recognition with Nearest Neighbor Search,"['Shuhe Wang', 'Xiaoya Li', 'Yuxian Meng', 'Tianwei Zhang', 'Rongbin Ouyang', 'Jiwei Li', 'Guoyin Wang']",http://arxiv.org/pdf/2203.17103v1.pdf,2022-03-31,['cs.cl'],"  Inspired by recent advances in retrieval augmented methods inNLP~\citep{khandelwal2019generalization,khandelwal2020nearest,meng2021gnn}, inthis paper, we introduce a $k$ nearest neighbor NER ($k$NN-NER) framework,which augments the distribution of entity labels by assigning $k$ nearestneighbors retrieved from the training set. This strategy makes the model morecapable of handling long-tail cases, along with better few-shot learningabilities. $k$NN-NER requires no additional operation during the trainingphase, and by interpolating $k$ nearest neighbors search into the vanilla NERmodel, $k$NN-NER consistently outperforms its vanilla counterparts: we achievea new state-of-the-art F1-score of 72.03 (+1.25) on the Chinese Weibo datasetand improved results on a variety of widely used NER benchmarks. Additionally,we show that $k$NN-NER can achieve comparable results to the vanilla NER modelwith 40\% less amount of training data. Code available at\url{https://github.com/ShannonAI/KNN-NER}."
On the Efficiency of Integrating Self-supervised Learning and  Meta-learning for User-defined Few-shot Keyword Spotting,"['Wei-Tsung Kao', 'Yuan-Kuei Wu', 'Chia-Ping Chen', 'Zhi-Sheng Chen', 'Yu-Pao Tsai', 'Hung-Yi Lee']",http://arxiv.org/pdf/2204.00352v3.pdf,2022-04-01,"['cs.lg', 'eess.as']","  User-defined keyword spotting is a task to detect new spoken terms defined byusers. This can be viewed as a few-shot learning problem since it isunreasonable for users to define their desired keywords by providing manyexamples. To solve this problem, previous works try to incorporateself-supervised learning models or apply meta-learning algorithms. But it isunclear whether self-supervised learning and meta-learning are complementaryand which combination of the two types of approaches is most effective forfew-shot keyword discovery. In this work, we systematically study thesequestions by utilizing various self-supervised learning models and combiningthem with a wide variety of meta-learning algorithms. Our result shows thatHuBERT combined with Matching network achieves the best result and is robust tothe changes of few-shot examples."
Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging,"['Yutai Hou', 'Cheng Chen', 'Xianzhen Luo', 'Bohan Li', 'Wanxiang Che']",http://arxiv.org/pdf/2204.00885v1.pdf,2022-04-02,"['cs.cl', 'cs.ai']","  Prompting methods recently achieve impressive success in few-shot learning.These methods modify input samples with prompt sentence pieces, and decodelabel tokens to map samples to corresponding labels. However, such a paradigmis very inefficient for the task of slot tagging. Since slot tagging samplesare multiple consecutive words in a sentence, the prompting methods have toenumerate all n-grams token spans to find all the possible slots, which greatlyslows down the prediction. To tackle this, we introduce an inverse paradigm forprompting. Different from the classic prompts mapping tokens to labels, wereversely predict slot values given slot types. Such inverse prompting onlyrequires a one-turn prediction for each slot type and greatly speeds up theprediction. Besides, we propose a novel Iterative Prediction Strategy, fromwhich the model learns to refine predictions by considering the relationsbetween different slot types. We find, somewhat surprisingly, the proposedmethod not only predicts faster but also significantly improves the effect(improve over 6.1 F1-scores on 10-shot setting) and achieves newstate-of-the-art performance."
AutoProtoNet: Interpretability for Prototypical Networks,"['Pedro Sandoval-Segura', 'Wallace Lawson']",http://arxiv.org/pdf/2204.00929v1.pdf,2022-04-02,['cs.lg'],"  In meta-learning approaches, it is difficult for a practitioner to make senseof what kind of representations the model employs. Without this ability, it canbe difficult to both understand what the model knows as well as to makemeaningful corrections. To address these challenges, we introduce AutoProtoNet,which builds interpretability into Prototypical Networks by training anembedding space suitable for reconstructing inputs, while remaining convenientfor few-shot learning. We demonstrate how points in this embedding space can bevisualized and used to understand class representations. We also devise aprototype refinement method, which allows a human to debug inadequateclassification parameters. We use this debugging technique on a customclassification task and find that it leads to accuracy improvements on avalidation set consisting of in-the-wild images. We advocate forinterpretability in meta-learning approaches and show that there areinteractive ways for a human to enhance meta-learning algorithms."
MetaAudio: A Few-Shot Audio Classification Benchmark,"['Calum Heggan', 'Sam Budgett', 'Timothy Hospedales', 'Mehrdad Yaghoobi']",http://arxiv.org/pdf/2204.02121v2.pdf,2022-04-05,"['cs.sd', 'cs.lg', 'eess.as']","  Currently available benchmarks for few-shot learning (machine learning withfew training examples) are limited in the domains they cover, primarilyfocusing on image classification. This work aims to alleviate this reliance onimage-based benchmarks by offering the first comprehensive, public and fullyreproducible audio based alternative, covering a variety of sound domains andexperimental settings. We compare the few-shot classification performance of avariety of techniques on seven audio datasets (spanning environmental sounds tohuman-speech). Extending this, we carry out in-depth analyses of joint training(where all datasets are used during training) and cross-dataset adaptationprotocols, establishing the possibility of a generalised audio few-shotclassification algorithm. Our experimentation shows gradient-basedmeta-learning methods such as MAML and Meta-Curvature consistently outperformboth metric and baseline methods. We also demonstrate that the joint trainingroutine helps overall generalisation for the environmental sound databasesincluded, as well as being a somewhat-effective method of tackling thecross-dataset/domain setting."
Too Big to Fail? Active Few-Shot Learning Guided Logic Synthesis,"['Animesh Basak Chowdhury', 'Benjamin Tan', 'Ryan Carey', 'Tushit Jain', 'Ramesh Karri', 'Siddharth Garg']",http://arxiv.org/pdf/2204.02368v1.pdf,2022-04-05,"['cs.lg', 'cs.ai', 'cs.ar']","  Generating sub-optimal synthesis transformation sequences (""synthesisrecipe"") is an important problem in logic synthesis. Manually crafted synthesisrecipes have poor quality. State-of-the art machine learning (ML) works togenerate synthesis recipes do not scale to large netlists as the models need tobe trained from scratch, for which training data is collected using timeconsuming synthesis runs. We propose a new approach, Bulls-Eye, that fine-tunesa pre-trained model on past synthesis data to accurately predict the quality ofa synthesis recipe for an unseen netlist. This approach on achieves 2x-10xrun-time improvement and better quality-of-result (QoR) than state-of-the-artmachine learning approaches."
Leveraging pre-trained language models for conversational information  seeking from text,"['Patrizio Bellan', 'Mauro Dragoni', 'Chiara Ghidini']",http://arxiv.org/pdf/2204.03542v1.pdf,2022-03-31,"['cs.cl', 'cs.ai']","  Recent advances in Natural Language Processing, and in particular on theconstruction of very large pre-trained language representation models, isopening up new perspectives on the construction of conversational informationseeking (CIS) systems. In this paper we investigate the usage of in-contextlearning and pre-trained language representation models to address the problemof information extraction from process description documents, in an incrementalquestion and answering oriented fashion. In particular we investigate the usageof the native GPT-3 (Generative Pre-trained Transformer 3) model, together withtwo in-context learning customizations that inject conceptual definitions and alimited number of samples in a few shot-learning fashion. The results highlightthe potential of the approach and the usefulness of the in-context learningcustomizations, which can substantially contribute to address the ""trainingdata challenge"" of deep learning based NLP techniques the BPM field. It alsohighlight the challenge posed by control flow relations for which furthertraining needs to be devised."
Embedding Learning in Hybrid Quantum-Classical Neural Networks,"['Minzhao Liu', 'Junyu Liu', 'Rui Liu', 'Henry Makhanov', 'Danylo Lykov', 'Anuj Apte', 'Yuri Alexeev']",http://arxiv.org/pdf/2204.04550v2.pdf,2022-04-09,['quant-ph'],"  Quantum embedding learning is an important step in the application of quantummachine learning to classical data. In this paper we propose a quantum few-shotembedding learning paradigm, which learns embeddings useful for trainingdownstream quantum machine learning tasks. Crucially, we identify the circuitbypass problem in hybrid neural networks, where learned classical parameters donot utilize the Hilbert space efficiently. We observe that the few-shot learnedembeddings generalize to unseen classes and suffer less from the circuit bypassproblem compared with other approaches."
MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text  Classification,"['Jianhai Zhang', 'Mieradilijiang Maimaiti', 'Xing Gao', 'Yuanhang Zheng', 'Ji Zhang']",http://arxiv.org/pdf/2204.04952v3.pdf,2022-04-11,['cs.cl'],"  Text classification struggles to generalize to unseen classes with very fewlabeled text instances per class. In such a few-shot learning (FSL) setting,metric-based meta-learning approaches have shown promising results. Previousstudies mainly aim to derive a prototype representation for each class.However, they neglect that it is challenging-yet-unnecessary to construct acompact representation which expresses the entire meaning for each class. Theyalso ignore the importance to capture the inter-dependency between query andthe support set for few-shot text classification. To deal with these issues, wepropose a meta-learning based method MGIMN which performs instance-wisecomparison followed by aggregation to generate class-wise matching vectorsinstead of prototype learning. The key of instance-wise comparison is theinteractive matching within the class-specific context and episode-specificcontext. Extensive experiments demonstrate that the proposed methodsignificantly outperforms the existing state-of-the-art approaches, under boththe standard FSL and generalized FSL settings."
A Simple Approach to Adversarial Robustness in Few-shot Image  Classification,"['Akshayvarun Subramanya', 'Hamed Pirsiavash']",http://arxiv.org/pdf/2204.05432v1.pdf,2022-04-11,"['cs.cv', 'cs.ai', 'cs.lg']","  Few-shot image classification, where the goal is to generalize to tasks withlimited labeled data, has seen great progress over the years. However, theclassifiers are vulnerable to adversarial examples, posing a question regardingtheir generalization capabilities. Recent works have tried to combinemeta-learning approaches with adversarial training to improve the robustness offew-shot classifiers. We show that a simple transfer-learning based approachcan be used to train adversarially robust few-shot classifiers. We also presenta method for novel classification task based on calibrating the centroid of thefew-shot category towards the base classes. We show that standard adversarialtraining on base categories along with calibrated centroid-based classifier inthe novel categories, outperforms or is on-par with state-of-the-art advancedmethods on standard benchmarks for few-shot learning. Our method is simple,easy to scale, and with little effort can lead to robust few-shot classifiers.Code is available here: \url{https://github.com/UCDvision/Simple_few_shot.git}"
Learning Compositional Representations for Effective Low-Shot  Generalization,"['Samarth Mishra', 'Pengkai Zhu', 'Venkatesh Saligrama']",http://arxiv.org/pdf/2204.08090v1.pdf,2022-04-17,"['cs.cv', 'cs.ai', 'cs.lg']","  We propose Recognition as Part Composition (RPC), an image encoding approachinspired by human cognition. It is based on the cognitive theory that humansrecognize complex objects by components, and that they build a small compactvocabulary of concepts to represent each instance with. RPC encodes images byfirst decomposing them into salient parts, and then encoding each part as amixture of a small number of prototypes, each representing a certain concept.We find that this type of learning inspired by human cognition can overcomehurdles faced by deep convolutional networks in low-shot generalization tasks,like zero-shot learning, few-shot learning and unsupervised domain adaptation.Furthermore, we find a classifier using an RPC image encoder is fairly robustto adversarial attacks, that deep neural networks are known to be prone to.Given that our image encoding principle is based on human cognition, one wouldexpect the encodings to be interpretable by humans, which we find to be thecase via crowd-sourcing experiments. Finally, we propose an application ofthese interpretable encodings in the form of generating synthetic attributeannotations for evaluating zero-shot learning methods on new datasets."
Zero and Few-shot Learning for Author Profiling,"['Mara Chinea-Rios', 'Thomas Müller', 'Gretel Liz De la Peña Sarracén', 'Francisco Rangel', 'Marc Franco-Salvador']",http://arxiv.org/pdf/2204.10543v2.pdf,2022-04-22,['cs.cl'],"  Author profiling classifies author characteristics by analyzing how languageis shared among people. In this work, we study that task from a low-resourceviewpoint: using little or no training data. We explore different zero andfew-shot models based on entailment and evaluate our systems on severalprofiling tasks in Spanish and English. In addition, we study the effect ofboth the entailment hypothesis and the size of the few-shot training sample. Wefind that entailment-based models out-perform supervised text classifiers basedon roberta-XLM and that we can reach 80% of the accuracy of previous approachesusing less than 50\% of the training data on average."
Super-Prompting: Utilizing Model-Independent Contextual Data to Reduce  Data Annotation Required in Visual Commonsense Tasks,"['Navid Rezaei', 'Marek Z. Reformat']",http://arxiv.org/pdf/2204.11922v1.pdf,2022-04-25,"['cs.cl', 'cs.ai']","  Pre-trained language models have shown excellent results in few-shot learningscenarios using in-context learning. Although it is impressive, the size oflanguage models can be prohibitive to make them usable in on-deviceapplications, such as sensors or smartphones. With smaller language models,task-specific data annotation is needed to fine-tune the language model for aspecific purpose. However, data annotation can have a substantial financial andtime burden for small research groups, startups, and even companies. In thispaper, we analyze different prompt-based fine-tuning techniques to improveresults on both language and multimodal causal transformer models. To evaluateour results, we use a dataset focusing on visual commonsense reasoning in time.Our results show that by simple model-agnostic prompt-based fine-tuning,comparable results can be reached by only using 35%-40% of the fine-tuningtraining dataset. The proposed approaches result in significant time andfinancial savings. As the proposed methods make minimal architecturalassumptions, other researchers can use the results in their transformer modelswith minimal adaptations. We plan to release the source code freely to make iteasier for the community to use and contribute to our work."
Function-words Enhanced Attention Networks for Few-Shot Inverse Relation  Classification,"['Chunliu Dou', 'Shaojuan Wu', 'Xiaowang Zhang', 'Zhiyong Feng', 'Kewen Wang']",http://arxiv.org/pdf/2204.12111v1.pdf,2022-04-26,['cs.cl'],"  The relation classification is to identify semantic relations between twoentities in a given text. While existing models perform well for classifyinginverse relations with large datasets, their performance is significantlyreduced for few-shot learning. In this paper, we propose a function wordsadaptively enhanced attention framework (FAEA) for few-shot inverse relationclassification, in which a hybrid attention model is designed to attendclass-related function words based on meta-learning. As the involvement offunction words brings in significant intra-class redundancy, an adaptivemessage passing mechanism is introduced to capture and transfer inter-classdifferences.We mathematically analyze the negative impact of function wordsfrom dot-product measurement, which explains why message passing mechanismeffectively reduces the impact. Our experimental results show that FAEAoutperforms strong baselines, especially the inverse relation accuracy isimproved by 14.33% under 1-shot setting in FewRel1.0."
Building a Role Specified Open-Domain Dialogue System Leveraging  Large-Scale Language Models,"['Sanghwan Bae', 'Donghyun Kwak', 'Sungdong Kim', 'Donghoon Ham', 'Soyoung Kang', 'Sang-Woo Lee', 'Woomyoung Park']",http://arxiv.org/pdf/2205.00176v1.pdf,2022-04-30,['cs.cl'],"  Recent open-domain dialogue models have brought numerous breakthroughs.However, building a chat system is not scalable since it often requires aconsiderable volume of human-human dialogue data, especially when enforcingfeatures such as persona, style, or safety. In this work, we study thechallenge of imposing roles on open-domain dialogue systems, with the goal ofmaking the systems maintain consistent roles while conversing naturally withhumans. To accomplish this, the system must satisfy a role specification thatincludes certain conditions on the stated features as well as a system policyon whether or not certain types of utterances are allowed. For this, we proposean efficient data collection framework leveraging in-context few-shot learningof large-scale language models for building role-satisfying dialogue datasetfrom scratch. We then compare various architectures for open-domain dialoguesystems in terms of meeting role specifications while maintainingconversational abilities. Automatic and human evaluations show that our modelsreturn few out-of-bounds utterances, keeping competitive performance on generalmetrics. We release a Korean dialogue dataset we built for further research."
EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language  Processing,"['Chengyu Wang', 'Minghui Qiu', 'Chen Shi', 'Taolin Zhang', 'Tingting Liu', 'Lei Li', 'Jianing Wang', 'Ming Wang', 'Jun Huang', 'Wei Lin']",http://arxiv.org/pdf/2205.00258v2.pdf,2022-04-30,['cs.cl'],"  The success of Pre-Trained Models (PTMs) has reshaped the development ofNatural Language Processing (NLP). Yet, it is not easy to obtainhigh-performing models and deploy them online for industrial practitioners. Tobridge this gap, EasyNLP is designed to make it easy to build NLP applications,which supports a comprehensive suite of NLP algorithms. It further featuresknowledge-enhanced pre-training, knowledge distillation and few-shot learningfunctionalities for large-scale PTMs, and provides a unified framework of modeltraining, inference and deployment for real-world applications. Currently,EasyNLP has powered over ten business units within Alibaba Group and isseamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.The source code of our EasyNLP toolkit is released at GitHub(https://github.com/alibaba/EasyNLP)."
POLITICS: Pretraining with Same-story Article Comparison for Ideology  Prediction and Stance Detection,"['Yujian Liu', 'Xinliang Frederick Zhang', 'David Wegsman', 'Nick Beauchamp', 'Lu Wang']",http://arxiv.org/pdf/2205.00619v1.pdf,2022-05-02,['cs.cl'],"  Ideology is at the core of political science research. Yet, there still doesnot exist general-purpose tools to characterize and predict ideology acrossdifferent genres of text. To this end, we study Pretrained Language Modelsusing novel ideology-driven pretraining objectives that rely on the comparisonof articles on the same story written by media of different ideologies. Wefurther collect a large-scale dataset, consisting of more than 3.6M politicalnews articles, for pretraining. Our model POLITICS outperforms strong baselinesand the previous state-of-the-art models on ideology prediction and stancedetection tasks. Further analyses show that POLITICS is especially good atunderstanding long or formally written texts, and is also robust in few-shotlearning scenarios."
On the generalization capabilities of FSL methods through domain  adaptation: a case study in endoscopic kidney stone image classification,"['Mauricio Mendez-Ruiz', 'Francisco Lopez-Tiro', 'Jonathan El-Beze', 'Vincent Estrade', 'Gilberto Ochoa-Ruiz1', 'Jacques Hubert', 'Andres Mendez-Vazquez', 'Christian Daul']",http://arxiv.org/pdf/2205.00895v1.pdf,2022-05-02,['cs.cv'],"  Deep learning has shown great promise in diverse areas of computer vision,such as image classification, object detection and semantic segmentation, amongmany others. However, as it has been repeatedly demonstrated, deep learningmethods trained on a dataset do not generalize well to datasets from otherdomains or even to similar datasets, due to data distribution shifts. In thiswork, we propose the use of a meta-learning based few-shot learning approach toalleviate these problems. In order to demonstrate its efficacy, we use twodatasets of kidney stones samples acquired with different endoscopes anddifferent acquisition conditions. The results show how such methods are indeedcapable of handling domain-shifts by attaining an accuracy of 74.38% and 88.52%in the 5-way 5-shot and 5-way 20-shot settings respectively. Instead, in thesame dataset, traditional Deep Learning (DL) methods attain only an accuracy of45%."
Local Stochastic Bilevel Optimization with Momentum-Based Variance  Reduction,"['Junyi Li', 'Feihu Huang', 'Heng Huang']",http://arxiv.org/pdf/2205.01608v1.pdf,2022-05-03,"['cs.lg', 'math.oc']","  Bilevel Optimization has witnessed notable progress recently with newemerging efficient algorithms and has been applied to many machine learningtasks such as data cleaning, few-shot learning, and neural architecture search.However, little attention has been paid to solve the bilevel problems underdistributed setting. Federated learning (FL) is an emerging paradigm whichsolves machine learning tasks over distributed-located data. FL problems arechallenging to solve due to the heterogeneity and communication bottleneck.However, it is unclear how these challenges will affect the convergence ofBilevel Optimization algorithms. In this paper, we study Federated BilevelOptimization problems. Specifically, we first propose the FedBiO, adeterministic gradient-based algorithm and we show it requires$O(\epsilon^{-2})$ number of iterations to reach an $\epsilon$-stationarypoint. Then we propose FedBiOAcc to accelerate FedBiO with the momentum-basedvariance-reduction technique under the stochastic scenario. We show FedBiOAcchas complexity of $O(\epsilon^{-1.5})$. Finally, we validate our proposedalgorithms via the important Fair Federated Learning task. More specifically,we define a bilevel-based group fair FL objective. Our algorithms show superiorperformances compared to other baselines in numerical experiments."
Generating Representative Samples for Few-Shot Classification,"['Jingyi Xu', 'Hieu Le']",http://arxiv.org/pdf/2205.02918v1.pdf,2022-05-05,['cs.cv'],"  Few-shot learning (FSL) aims to learn new categories with a few visualsamples per class. Few-shot class representations are often biased due to datascarcity. To mitigate this issue, we propose to generate visual samples basedon semantic embeddings using a conditional variational autoencoder (CVAE)model. We train this CVAE model on base classes and use it to generate featuresfor novel classes. More importantly, we guide this VAE to strictly generaterepresentative samples by removing non-representative samples from the basetraining set when training the CVAE model. We show that this training schemeenhances the representativeness of the generated samples and therefore,improves the few-shot classification results. Experimental results show thatour method improves three FSL baseline methods by substantial margins,achieving state-of-the-art few-shot classification performance on miniImageNetand tieredImageNet datasets for both 1-shot and 5-shot settings. Code isavailable at: https://github.com/cvlab-stonybrook/fsl-rsvae."
KECP: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive  Question Answering,"['Jianing Wang', 'Chengyu Wang', 'Minghui Qiu', 'Qiuhui Shi', 'Hongbin Wang', 'Jun Huang', 'Ming Gao']",http://arxiv.org/pdf/2205.03071v1.pdf,2022-05-06,"['cs.cl', 'cs.ai']","  Extractive Question Answering (EQA) is one of the most important tasks inMachine Reading Comprehension (MRC), which can be solved by fine-tuning thespan selecting heads of Pre-trained Language Models (PLMs). However, mostexisting approaches for MRC may perform poorly in the few-shot learningscenario. To solve this issue, we propose a novel framework named KnowledgeEnhanced Contrastive Prompt-tuning (KECP). Instead of adding pointer heads toPLMs, we introduce a seminal paradigm for EQA that transform the task into anon-autoregressive Masked Language Modeling (MLM) generation problem.Simultaneously, rich semantics from the external knowledge base (KB) and thepassage context are support for enhancing the representations of the query. Inaddition, to boost the performance of PLMs, we jointly train the model by theMLM and contrastive learning objectives. Experiments on multiple benchmarksdemonstrate that our method consistently outperforms state-of-the-artapproaches in few-shot settings by a large margin."
ProQA: Structural Prompt-based Pre-training for Unified Question  Answering,"['Wanjun Zhong', 'Yifan Gao', 'Ning Ding', 'Yujia Qin', 'Zhiyuan Liu', 'Ming Zhou', 'Jiahai Wang', 'Jian Yin', 'Nan Duan']",http://arxiv.org/pdf/2205.04040v2.pdf,2022-05-09,['cs.cl'],"  Question Answering (QA) is a longstanding challenge in natural languageprocessing. Existing QA works mostly focus on specific question types,knowledge domains, or reasoning skills. The specialty in QA research hinderssystems from modeling commonalities between tasks and generalization for widerapplications. To address this issue, we present ProQA, a unified QA paradigmthat solves various tasks through a single model. ProQA takes a unifiedstructural prompt as the bridge and improves the QA-centric ability bystructural prompt-based pre-training. Through a structurally designedprompt-based input schema, ProQA concurrently models the knowledgegeneralization for all QA tasks while keeping the knowledge customization forevery specific QA task. Furthermore, ProQA is pre-trained with structuralprompt-formatted large-scale synthesized corpus, which empowers the model withthe commonly-required QA ability. Experimental results on 11 QA benchmarksdemonstrate that ProQA consistently boosts performance on both full datafine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore,ProQA exhibits strong ability in both continual learning and transfer learningby taking the advantages of the structural prompt."
ALLSH: Active Learning Guided by Local Sensitivity and Hardness,"['Shujian Zhang', 'Chengyue Gong', 'Xingchao Liu', 'Pengcheng He', 'Weizhu Chen', 'Mingyuan Zhou']",http://arxiv.org/pdf/2205.04980v2.pdf,2022-05-10,"['cs.cl', 'cs.ai', 'cs.lg']","  Active learning, which effectively collects informative unlabeled data forannotation, reduces the demand for labeled data. In this work, we propose toretrieve unlabeled samples with a local sensitivity and hardness-awareacquisition function. The proposed method generates data copies through localperturbations and selects data points whose predictive likelihoods diverge themost from their copies. We further empower our acquisition function byinjecting the select-worst case perturbation. Our method achieves consistentgains over the commonly used active learning strategies in variousclassification tasks. Furthermore, we observe consistent improvements over thebaselines on the study of prompt selection in prompt-based few-shot learning.These experiments demonstrate that our acquisition guided by local sensitivityand hardness can be effective and beneficial for many NLP tasks."
On the Economics of Multilingual Few-shot Learning: Modeling the  Cost-Performance Trade-offs of Machine Translated and Manual Data,"['Kabir Ahuja', 'Monojit Choudhury', 'Sandipan Dandapat']",http://arxiv.org/pdf/2205.06350v2.pdf,2022-05-12,['cs.cl'],"  Borrowing ideas from {\em Production functions} in micro-economics, in thispaper we introduce a framework to systematically evaluate the performance andcost trade-offs between machine-translated and manually-created labelled datafor task-specific fine-tuning of massively multilingual language models. Weillustrate the effectiveness of our framework through a case-study on theTyDIQA-GoldP dataset. One of the interesting conclusions of the study is thatif the cost of machine translation is greater than zero, the optimalperformance at least cost is always achieved with at least some or onlymanually-created data. To our knowledge, this is the first attempt towardsextending the concept of production functions to study data collectionstrategies for training multilingual models, and can serve as a valuable toolfor other similar cost vs data trade-offs in NLP."
Prototypical Calibration for Few-shot Learning of Language Models,"['Zhixiong Han', 'Yaru Hao', 'Li Dong', 'Yutao Sun', 'Furu Wei']",http://arxiv.org/pdf/2205.10183v2.pdf,2022-05-20,['cs.cl'],"  In-context learning of GPT-like models has been recognized as fragile acrossdifferent hand-crafted templates, and demonstration permutations. In this work,we propose prototypical calibration to adaptively learn a more robust decisionboundary for zero- and few-shot classification, instead of greedy decoding.Concretely, our method first adopts Gaussian mixture distribution to estimatethe prototypical clusters for all categories. Then we assign each cluster tothe corresponding label by solving a weighted bipartite matching problem. Givenan example, its prediction is calibrated by the likelihood of prototypicalclusters. Experimental results show that prototypical calibration yields asubstantial improvement on a diverse set of tasks. Extensive analysis acrossdifferent scales also indicates that our method calibrates the decisionboundary as expected, greatly improving the robustness of GPT to templates,permutations, and class imbalance."
Learning Meta Representations of One-shot Relations for Temporal  Knowledge Graph Link Prediction,"['Zifeng Ding', 'Bailan He', 'Yunpu Ma', 'Zhen Han', 'Volker Tresp']",http://arxiv.org/pdf/2205.10621v2.pdf,2022-05-21,"['cs.lg', 'cs.ai']","  Few-shot relational learning for static knowledge graphs (KGs) has drawngreater interest in recent years, while few-shot learning for temporalknowledge graphs (TKGs) has hardly been studied. Compared to KGs, TKGs containrich temporal information, thus requiring temporal reasoning techniques formodeling. This poses a greater challenge in learning few-shot relations in thetemporal context. In this paper, we follow the previous work that focuses onfew-shot relational learning on static KGs and extend two fundamental TKGreasoning tasks, i.e., interpolated and extrapolated link prediction, to theone-shot setting. We propose four new large-scale benchmark datasets anddevelop a TKG reasoning model for learning one-shot relations in TKGs.Experimental results show that our model can achieve superior performance onall datasets in both TKG link prediction tasks."
BBTv2: Towards a Gradient-Free Future with Large Language Models,"['Tianxiang Sun', 'Zhengfu He', 'Hong Qian', 'Yunhua Zhou', 'Xuanjing Huang', 'Xipeng Qiu']",http://arxiv.org/pdf/2205.11200v2.pdf,2022-05-23,"['cs.cl', 'cs.ai']","  Most downstream adaptation methods tune all or part of the parameters ofpre-trained models (PTMs) through gradient descent, where the tuning costincreases linearly with the growth of the model size. By contrast,gradient-free methods only require the forward computation of the PTM to tunethe prompt, retaining the benefits of efficient tuning and deployment. Though,past work on gradient-free tuning often introduces gradient descent to seek agood initialization of prompt and lacks versatility across tasks and PTMs. Inthis paper, we present BBTv2, an improved version of Black-Box Tuning, to drivePTMs for few-shot learning. We prepend continuous prompts to every layer of thePTM and propose a divide-and-conquer gradient-free algorithm to optimize theprompts at different layers alternately. Extensive experiments across varioustasks and PTMs show that BBTv2 can achieve comparable performance to full modeltuning and state-of-the-art parameter-efficient methods (e.g., Adapter, LoRA,BitFit, etc.) under few-shot settings while maintaining much fewer tunableparameters."
Naive Few-Shot Learning: Uncovering the fluid intelligence of machines,"['Tomer Barak', 'Yonatan Loewenstein']",http://arxiv.org/pdf/2205.12013v3.pdf,2022-05-24,"['cs.ai', 'cs.cv', 'cs.lg', 'cs.ne']","  In this paper, we aimed to help bridge the gap between human fluidintelligence - the ability to solve novel tasks without prior training - andthe performance of deep neural networks, which typically require extensiveprior training. An essential cognitive component for solving intelligencetests, which in humans are used to measure fluid intelligence, is the abilityto identify regularities in sequences. This motivated us to construct abenchmark task, which we term \textit{sequence consistency evaluation} (SCE),whose solution requires the ability to identify regularities in sequences.Given the proven capabilities of deep networks, their ability to solve suchtasks after extensive training is expected. Surprisingly, however, we show thatnaive (randomly initialized) deep learning models that are trained on a\textit{single} SCE with a \textit{single} optimization step can still solvenon-trivial versions of the task relatively well. We extend our findings tosolve, without any prior training, real-world anomaly detection tasks in thevisual and auditory modalities. These results demonstrate the fluid-intelligentcomputational capabilities of deep networks. We discuss the implications of ourwork for constructing fluid-intelligent machines."
ToKen: Task Decomposition and Knowledge Infusion for Few-Shot Hate  Speech Detection,"['Badr AlKhamissi', 'Faisal Ladhak', 'Srini Iyer', 'Ves Stoyanov', 'Zornitsa Kozareva', 'Xian Li', 'Pascale Fung', 'Lambert Mathias', 'Asli Celikyilmaz', 'Mona Diab']",http://arxiv.org/pdf/2205.12495v2.pdf,2022-05-25,['cs.cl'],"  Hate speech detection is complex; it relies on commonsense reasoning,knowledge of stereotypes, and an understanding of social nuance that differsfrom one culture to the next. It is also difficult to collect a large-scalehate speech annotated dataset. In this work, we frame this problem as afew-shot learning task, and show significant gains with decomposing the taskinto its ""constituent"" parts. In addition, we see that infusing knowledge fromreasoning datasets (e.g. Atomic2020) improves the performance even further.Moreover, we observe that the trained models generalize to out-of-distributiondatasets, showing the superiority of task decomposition and knowledge infusioncompared to previously used methods. Concretely, our method outperforms thebaseline by 17.83% absolute gain in the 16-shot case."
Easter2.0: Improving convolutional models for handwritten text  recognition,"['Kartik Chaudhary', 'Raghav Bali']",http://arxiv.org/pdf/2205.14879v1.pdf,2022-05-30,"['cs.cv', 'cs.ai']","  Convolutional Neural Networks (CNN) have shown promising results for the taskof Handwritten Text Recognition (HTR) but they still fall behind RecurrentNeural Networks (RNNs)/Transformer based models in terms of performance. Inthis paper, we propose a CNN based architecture that bridges this gap. Ourwork, Easter2.0, is composed of multiple layers of 1D Convolution, BatchNormalization, ReLU, Dropout, Dense Residual connection, Squeeze-and-Excitationmodule and make use of Connectionist Temporal Classification (CTC) loss. Inaddition to the Easter2.0 architecture, we propose a simple and effective dataaugmentation technique 'Tiling and Corruption (TACO)' relevant for the task ofHTR/OCR. Our work achieves state-of-the-art results on IAM handwriting databasewhen trained using only publicly available training data. In our experiments,we also present the impact of TACO augmentations and Squeeze-and-Excitation(SE) on text recognition accuracy. We further show that Easter2.0 is suitablefor few-shot learning tasks and outperforms current best methods includingTransformers when trained on limited amount of annotated data. Code and modelis available at: https://github.com/kartikgill/Easter2"
Task-Prior Conditional Variational Auto-Encoder for Few-Shot Image  Classification,['Zaiyun Yang'],http://arxiv.org/pdf/2205.15014v1.pdf,2022-05-30,"['cs.cv', 'cs.lg', 'eess.iv']","  Transductive methods always outperform inductive methods in few-shot imageclassification scenarios. However, the existing few-shot methods contain alatent condition: the number of samples in each class is the same, which may beunrealistic. To cope with those cases where the query shots of each class arenonuniform (i.e. nonuniform few-shot learning), we propose a Task-PriorConditional Variational Auto-Encoder model named TP-VAE, conditioned on supportshots and constrained by a task-level prior regularization. Our method obtainshigh performance in the more challenging nonuniform few-shot scenarios.Moreover, our method outperforms the state-of-the-art in a wide range ofstandard few-shot image classification scenarios. Among them, the accuracy of1-shot increased by about 3\%."
Zero-Shot and Few-Shot Learning for Lung Cancer Multi-Label  Classification using Vision Transformer,"['Fu-Ming Guo', 'Yingfang Fan']",http://arxiv.org/pdf/2205.15290v2.pdf,2022-05-30,"['cs.cv', 'cs.ai', 'cs.lg']","  Lung cancer is the leading cause of cancer-related death worldwide. Lungadenocarcinoma (LUAD) and lung squamous cell carcinoma (LUSC) are the mostcommon histologic subtypes of non-small-cell lung cancer (NSCLC). Histology isan essential tool for lung cancer diagnosis. Pathologists make classificationsaccording to the dominant subtypes. Although morphology remains the standardfor diagnosis, significant tool needs to be developed to elucidate thediagnosis. In our study, we utilize the pre-trained Vision Transformer (ViT)model to classify multiple label lung cancer on histologic slices (from datasetLC25000), in both Zero-Shot and Few-Shot settings. Then we compare theperformance of Zero-Shot and Few-Shot ViT on accuracy, precision, recall,sensitivity and specificity. Our study show that the pre-trained ViT model hasa good performance in Zero-Shot setting, a competitive accuracy ($99.87\%$) inFew-Shot setting ({epoch = 1}) and an optimal result ($100.00\%$ on bothvalidation set and test set) in Few-Shot seeting ({epoch = 5})."
Few-Shot Diffusion Models,"['Giorgio Giannone', 'Didrik Nielsen', 'Ole Winther']",http://arxiv.org/pdf/2205.15463v1.pdf,2022-05-30,"['cs.cv', 'cs.lg', 'stat.ml']","  Denoising diffusion probabilistic models (DDPM) are powerful hierarchicallatent variable models with remarkable sample generation quality and trainingstability. These properties can be attributed to parameter sharing in thegenerative hierarchy, as well as a parameter-free diffusion-based inferenceprocedure. In this paper, we present Few-Shot Diffusion Models (FSDM), aframework for few-shot generation leveraging conditional DDPMs. FSDMs aretrained to adapt the generative process conditioned on a small set of imagesfrom a given class by aggregating image patch information using a set-basedVision Transformer (ViT). At test time, the model is able to generate samplesfrom previously unseen classes conditioned on as few as 5 samples from thatclass. We empirically show that FSDM can perform few-shot generation andtransfer to new datasets. We benchmark variants of our method on complex visiondatasets for few-shot learning and compare to unconditional and conditionalDDPM baselines. Additionally, we show how conditioning the model on patch-basedinput set information improves training convergence."
Metric Based Few-Shot Graph Classification,"['Donato Crisostomi', 'Simone Antonelli', 'Valentino Maiorca', 'Luca Moschella', 'Riccardo Marin', 'Emanuele Rodolà']",http://arxiv.org/pdf/2206.03695v2.pdf,2022-06-08,"['cs.lg', 'cs.ai']","  Many modern deep-learning techniques do not work without enormous datasets.At the same time, several fields demand methods working in scarcity of data.This problem is even more complex when the samples have varying structures, asin the case of graphs. Graph representation learning techniques have recentlyproven successful in a variety of domains. Nevertheless, the employedarchitectures perform miserably when faced with data scarcity. On the otherhand, few-shot learning allows employing modern deep learning models in scarcedata regimes without waiving their effectiveness. In this work, we tackle theproblem of few-shot graph classification, showing that equipping a simpledistance metric learning baseline with a state-of-the-art graph embedder allowsto obtain competitive results on the task.While the simplicity of thearchitecture is enough to outperform more complex ones, it also allowsstraightforward additions. To this end, we show that additional improvementsmay be obtained by encouraging a task-conditioned embedding space. Finally, wepropose a MixUp-based online data augmentation technique acting in the latentspace and show its effectiveness on the task."
Neural Prompt Search,"['Yuanhan Zhang', 'Kaiyang Zhou', 'Ziwei Liu']",http://arxiv.org/pdf/2206.04673v2.pdf,2022-06-09,"['cs.cv', 'cs.ai', 'cs.lg']","  The size of vision models has grown exponentially over the last few years,especially after the emergence of Vision Transformer. This has motivated thedevelopment of parameter-efficient tuning methods, such as learning adapterlayers or visual prompt tokens, which allow a tiny portion of model parametersto be trained whereas the vast majority obtained from pre-training are frozen.However, designing a proper tuning method is non-trivial: one might need to tryout a lengthy list of design choices, not to mention that each downstreamdataset often requires custom designs. In this paper, we view the existingparameter-efficient tuning methods as ""prompt modules"" and propose NeuralprOmpt seArcH (NOAH), a novel approach that learns, for large vision models,the optimal design of prompt modules through a neural architecture searchalgorithm, specifically for each downstream dataset. By conducting extensiveexperiments on over 20 vision datasets, we demonstrate that NOAH (i) issuperior to individual prompt modules, (ii) has a good few-shot learningability, and (iii) is domain-generalizable. The code and models are availableat https://github.com/Davidzhangyuanhan/NOAH."
Lifelong Wandering: A realistic few-shot online continual learning  setting,"['Mayank Lunayach', 'James Smith', 'Zsolt Kira']",http://arxiv.org/pdf/2206.07932v1.pdf,2022-06-16,"['cs.cv', 'cs.lg']","  Online few-shot learning describes a setting where models are trained andevaluated on a stream of data while learning emerging classes. While prior workin this setting has achieved very promising performance on instanceclassification when learning from data-streams composed of a single indoorenvironment, we propose to extend this setting to consider objectclassification on a series of several indoor environments, which is likely tooccur in applications such as robotics. Importantly, our setting, which werefer to as online few-shot continual learning, injects the well-studied issueof catastrophic forgetting into the few-shot online learning paradigm. In thiswork, we benchmark several existing methods and adapted baselines within oursetting, and show there exists a trade-off between catastrophic forgetting andonline performance. Our findings motivate the need for future work in thissetting, which can achieve better online performance without catastrophicforgetting."
Channel Importance Matters in Few-Shot Image Classification,"['Xu Luo', 'Jing Xu', 'Zenglin Xu']",http://arxiv.org/pdf/2206.08126v2.pdf,2022-06-16,['cs.cv'],"  Few-Shot Learning (FSL) requires vision models to quickly adapt to brand-newclassification tasks with a shift in task distribution. Understanding thedifficulties posed by this task distribution shift is central to FSL. In thispaper, we show that a simple channel-wise feature transformation may be the keyto unraveling this secret from a channel perspective. When facing novelfew-shot tasks in the test-time datasets, this transformation can greatlyimprove the generalization ability of learned image representations, whilebeing agnostic to the choice of training algorithms and datasets. Through anin-depth analysis of this transformation, we find that the difficulty ofrepresentation transfer in FSL stems from the severe channel bias problem ofimage representations: channels may have different importance in differenttasks, while convolutional neural networks are likely to be insensitive, orrespond incorrectly to such a shift. This points out a core problem of thegeneralization ability of modern vision systems and needs further attention inthe future. Our code is available athttps://github.com/Frankluox/Channel_Importance_FSL."
EEML: Ensemble Embedded Meta-learning,"['Geng Li', 'Boyuan Ren', 'Hongzhi Wang']",http://arxiv.org/pdf/2206.09195v1.pdf,2022-06-18,"['cs.lg', 'cs.ai']","  To accelerate learning process with few samples, meta-learning resorts toprior knowledge from previous tasks. However, the inconsistent taskdistribution and heterogeneity is hard to be handled through a global sharingmodel initialization. In this paper, based on gradient-based meta-learning, wepropose an ensemble embedded meta-learning algorithm (EEML) that explicitlyutilizes multi-model-ensemble to organize prior knowledge into diverse specificexperts. We rely on a task embedding cluster mechanism to deliver diverse tasksto matching experts in training process and instruct how experts collaborate intest phase. As a result, the multi experts can focus on their own area ofexpertise and cooperate in upcoming task to solve the task heterogeneity. Theexperimental results show that the proposed method outperforms recentstate-of-the-arts easily in few-shot learning problem, which validates theimportance of differentiation and cooperation."
$C^*$-algebra Net: A New Approach Generalizing Neural Network Parameters  to $C^*$-algebra,"['Yuka Hashimoto', 'Zhao Wang', 'Tomoko Matsui']",http://arxiv.org/pdf/2206.09513v2.pdf,2022-06-20,"['stat.ml', 'cs.lg']","  We propose a new framework that generalizes the parameters of neural networkmodels to $C^*$-algebra-valued ones. $C^*$-algebra is a generalization of thespace of complex numbers. A typical example is the space of continuousfunctions on a compact space. This generalization enables us to combinemultiple models continuously and use tools for functions such as regression andintegration. Consequently, we can learn features of data efficiently and adaptthe models to problems continuously. We apply our framework to practicalproblems such as density estimation and few-shot learning and show that ourframework enables us to learn features of data even with a limited number ofsamples. Our new framework highlights the potential possibility of applying thetheory of $C^*$-algebra to general neural network models."
Few-Max: Few-Shot Domain Adaptation for Unsupervised Contrastive  Representation Learning,"['Ali Lotfi Rezaabad', 'Sidharth Kumar', 'Sriram Vishwanath', 'Jonathan I. Tamir']",http://arxiv.org/pdf/2206.10137v2.pdf,2022-06-21,"['cs.cv', 'cs.ai', 'cs.lg']","  Contrastive self-supervised learning methods learn to map data points such asimages into non-parametric representation space without requiring labels. Whilehighly successful, current methods require a large amount of data in thetraining phase. In situations where the target training set is limited in size,generalization is known to be poor. Pretraining on a large source data set andfine-tuning on the target samples is prone to overfitting in the few-shotregime, where only a small number of target samples are available. Motivated bythis, we propose a domain adaption method for self-supervised contrastivelearning, termed Few-Max, to address the issue of adaptation to a targetdistribution under few-shot learning. To quantify the representation quality,we evaluate Few-Max on a range of source and target datasets, includingImageNet, VisDA, and fastMRI, on which Few-Max consistently outperforms otherapproaches."
Low Resource Pipeline for Spoken Language Understanding via Weak  Supervision,"['Ayush Kumar', 'Rishabh Kumar Tripathi', 'Jithendra Vepa']",http://arxiv.org/pdf/2206.10559v1.pdf,2022-06-21,['cs.cl'],"  In Weak Supervised Learning (WSL), a model is trained over noisy labelsobtained from semantic rules and task-specific pre-trained models. Rules offerlimited generalization over tasks and require significant manual efforts whilepre-trained models are available only for limited tasks. In this work, wepropose to utilize prompt-based methods as weak sources to obtain the noisylabels on unannotated data. We show that task-agnostic prompts aregeneralizable and can be used to obtain noisy labels for different SpokenLanguage Understanding (SLU) tasks such as sentiment classification, disfluencydetection and emotion classification. These prompts could additionally beupdated to add task-specific contexts, thus providing flexibility to designtask-specific prompts. We demonstrate that prompt-based methods generatereliable labels for the above SLU tasks and thus can be used as a universalweak source to train a weak-supervised model (WSM) in absence of labeled data.Our proposed WSL pipeline trained over prompt-based weak source outperformsother competitive low-resource benchmarks on zero and few-shot learning by morethan 4% on Macro-F1 on all of the three benchmark SLU datasets. The proposedmethod also outperforms a conventional rule based WSL pipeline by more than 5%on Macro-F1."
Understanding Benign Overfitting in Gradient-Based Meta Learning,"['Lisha Chen', 'Songtao Lu', 'Tianyi Chen']",http://arxiv.org/pdf/2206.13482v2.pdf,2022-06-27,"['cs.lg', 'math.oc', 'stat.ml']","  Meta learning has demonstrated tremendous success in few-shot learning withlimited supervised data. In those settings, the meta model is usuallyoverparameterized. While the conventional statistical learning theory suggeststhat overparameterized models tend to overfit, empirical evidence reveals thatoverparameterized meta learning methods still work well -- a phenomenon oftencalled ""benign overfitting."" To understand this phenomenon, we focus on themeta learning settings with a challenging bilevel structure that we term thegradient-based meta learning, and analyze its generalization performance underan overparameterized meta linear regression model. While our analysis uses therelatively tractable linear models, our theory contributes to understanding thedelicate interplay among data heterogeneity, model adaptation and benignoverfitting in gradient-based meta learning tasks. We corroborate ourtheoretical claims through numerical simulations."
Prompting Decision Transformer for Few-Shot Policy Generalization,"['Mengdi Xu', 'Yikang Shen', 'Shun Zhang', 'Yuchen Lu', 'Ding Zhao', 'Joshua B. Tenenbaum', 'Chuang Gan']",http://arxiv.org/pdf/2206.13499v1.pdf,2022-06-27,"['cs.lg', 'cs.ai', 'cs.cv', 'cs.ro']","  Humans can leverage prior experience and learn novel tasks from a handful ofdemonstrations. In contrast to offline meta-reinforcement learning, which aimsto achieve quick adaptation through better algorithm design, we investigate theeffect of architecture inductive bias on the few-shot learning capability. Wepropose a Prompt-based Decision Transformer (Prompt-DT), which leverages thesequential modeling ability of the Transformer architecture and the promptframework to achieve few-shot adaptation in offline RL. We design thetrajectory prompt, which contains segments of the few-shot demonstrations, andencodes task-specific information to guide policy generation. Our experimentsin five MuJoCo control benchmarks show that Prompt-DT is a strong few-shotlearner without any extra finetuning on unseen target tasks. Prompt-DToutperforms its variants and strong meta offline RL baselines by a large marginwith a trajectory prompt containing only a few timesteps. Prompt-DT is alsorobust to prompt length changes and can generalize to out-of-distribution (OOD)environments."
Domain Agnostic Few-shot Learning for Speaker Verification,"['Seunghan Yang', 'Debasmit Das', 'Janghoon Cho', 'Hyoungwoo Park', 'Sungrack Yun']",http://arxiv.org/pdf/2206.13700v1.pdf,2022-06-28,"['cs.sd', 'cs.lg', 'eess.as']","  Deep learning models for verification systems often fail to generalize to newusers and new environments, even though they learn highly discriminativefeatures. To address this problem, we propose a few-shot domain generalizationframework that learns to tackle distribution shift for new users and newdomains. Our framework consists of domain-specific and domain-aggregationnetworks, which are the experts on specific and combined domains, respectively.By using these networks, we generate episodes that mimic the presence of bothnovel users and novel domains in the training phase to eventually producebetter generalization. To save memory, we reduce the number of domain-specificnetworks by clustering similar domains together. Upon extensive evaluation onartificially generated noise domains, we can explicitly show generalizationability of our framework. In addition, we apply our proposed methods to theexisting competitive architecture on the standard benchmark, which showsfurther performance improvements."
Few-Shot Cross-Lingual TTS Using Transferable Phoneme Embedding,"['Wei-Ping Huang', 'Po-Chun Chen', 'Sung-Feng Huang', 'Hung-yi Lee']",http://arxiv.org/pdf/2206.15427v2.pdf,2022-06-27,"['eess.as', 'cs.ai', 'cs.lg']","  This paper studies a transferable phoneme embedding framework that aims todeal with the cross-lingual text-to-speech (TTS) problem under the few-shotsetting. Transfer learning is a common approach when it comes to few-shotlearning since training from scratch on few-shot training data is bound tooverfit. Still, we find that the naive transfer learning approach fails toadapt to unseen languages under extremely few-shot settings, where less than 8minutes of data is provided. We deal with the problem by proposing a frameworkthat consists of a phoneme-based TTS model and a codebook module to projectphonemes from different languages into a learned latent space. Furthermore, byutilizing phoneme-level averaged self-supervised learned features, weeffectively improve the quality of synthesized speeches. Experiments show thatusing 4 utterances, which is about 30 seconds of data, is enough to synthesizeintelligible speech when adapting to an unseen language using our framework."
FewSOL: A Dataset for Few-Shot Object Learning in Robotic Environments,"['Jishnu Jaykumar P', 'Yu-Wei Chao', 'Yu Xiang']",http://arxiv.org/pdf/2207.03333v3.pdf,2022-07-06,"['cs.cv', 'cs.ai', 'cs.lg', 'cs.ro']","  We introduce the Few-Shot Object Learning (FewSOL) dataset for objectrecognition with a few images per object. We captured 336 real-world objectswith 9 RGB-D images per object from different views. Object segmentation masks,object poses and object attributes are provided. In addition, synthetic imagesgenerated using 330 3D object models are used to augment the dataset. Weinvestigated (i) few-shot object classification and (ii) joint objectsegmentation and few-shot classification with the state-of-the-art methods forfew-shot learning and meta-learning using our dataset. The evaluation resultsshow that there is still a large margin to be improved for few-shot objectclassification in robotic environments. Our dataset can be used to study a setof few-shot object recognition problems such as classification, detection andsegmentation, shape reconstruction, pose estimation, keypoint correspondencesand attribute recognition. The dataset and code are available athttps://irvlutd.github.io/FewSOL."
Generating Pseudo-labels Adaptively for Few-shot Model-Agnostic  Meta-Learning,"['Guodong Liu', 'Tongling Wang', 'Shuoxi Zhang', 'Kun He']",http://arxiv.org/pdf/2207.04217v1.pdf,2022-07-09,"['cs.lg', 'cs.ai']","  Model-Agnostic Meta-Learning (MAML) is a famous few-shot learning method thathas inspired many follow-up efforts, such as ANIL and BOIL. However, as aninductive method, MAML is unable to fully utilize the information of query set,limiting its potential of gaining higher generality. To address this issue, wepropose a simple yet effective method that generates psuedo-labels adaptivelyand could boost the performance of the MAML family. The proposed methods,dubbed Generative Pseudo-label based MAML (GP-MAML), GP-ANIL and GP-BOIL,leverage statistics of the query set to improve the performance on new tasks.Specifically, we adaptively add pseudo labels and pick samples from the queryset, then re-train the model using the picked query samples together with thesupport set. The GP series can also use information from the pseudo query setto re-train the network during the meta-testing. While some transductivemethods, such as Transductive Propagation Network (TPN), struggle to achievethis goal."
Few-shot training LLMs for project-specific code-summarization,"['Toufique Ahmed', 'Premkumar Devanbu']",http://arxiv.org/pdf/2207.04237v2.pdf,2022-07-09,"['cs.se', 'cs.lg']","  Very large language models (LLMs), such as GPT-3 and Codex have achievedstate-of-the-art performance on several natural-language tasks, and show greatpromise also for code. A particularly exciting aspect of LLMs is their knackfor few-shot and zero-shot learning: they can learn to perform a task with veryfew examples. Few-shotting has particular synergies in software engineering,where there are a lot of phenomena (identifier names, APIs, terminology, codingpatterns) that are known to be highly project-specific. However,project-specific data can be quite limited, especially early in the history ofa project; thus the few-shot learning capacity of LLMs might be very relevant.In this paper, we investigate the use few-shot training with the very large GPT(Generative Pre-trained Transformer) Codex model, and find evidence suggestingthat one can significantly surpass state-of-the-art models forcode-summarization, leveraging project-specific training."
"Proceedings of the ICML 2022 Expressive Vocalizations Workshop and  Competition: Recognizing, Generating, and Personalizing Vocal Bursts","['Alice Baird', 'Panagiotis Tzirakis', 'Gauthier Gidel', 'Marco Jiralerspong', 'Eilif B. Muller', 'Kory Mathewson', 'Björn Schuller', 'Erik Cambria', 'Dacher Keltner', 'Alan Cowen']",http://arxiv.org/pdf/2207.06958v2.pdf,2022-07-14,"['cs.sd', 'cs.lg', 'eess.as']","  This is the Proceedings of the ICML Expressive Vocalization (ExVo)Competition. The ExVo competition focuses on understanding and generating vocalbursts: laughs, gasps, cries, and other non-verbal vocalizations that arecentral to emotional expression and communication. ExVo 2022, included threecompetition tracks using a large-scale dataset of 59,201 vocalizations from1,702 speakers. The first, ExVo-MultiTask, requires participants to train amulti-task model to recognize expressed emotions and demographic traits fromvocal bursts. The second, ExVo-Generate, requires participants to train agenerative model that produces vocal bursts conveying ten different emotions.The third, ExVo-FewShot, requires participants to leverage few-shot learningincorporating speaker identity to train a model for the recognition of 10emotions conveyed by vocal bursts."
Tree Structure-Aware Few-Shot Image Classification via Hierarchical  Aggregation,"['Min Zhang', 'Siteng Huang', 'Wenbin Li', 'Donglin Wang']",http://arxiv.org/pdf/2207.06989v1.pdf,2022-07-14,['cs.cv'],"  In this paper, we mainly focus on the problem of how to learn additionalfeature representations for few-shot image classification through pretext tasks(e.g., rotation or color permutation and so on). This additional knowledgegenerated by pretext tasks can further improve the performance of few-shotlearning (FSL) as it differs from human-annotated supervision (i.e., classlabels of FSL tasks). To solve this problem, we present a plug-in HierarchicalTree Structure-aware (HTS) method, which not only learns the relationship ofFSL and pretext tasks, but more importantly, can adaptively select andaggregate feature representations generated by pretext tasks to maximize theperformance of FSL tasks. A hierarchical tree constructing component and agated selection aggregating component is introduced to construct the treestructure and find richer transferable knowledge that can rapidly adapt tonovel classes with a few labeled images. Extensive experiments show that ourHTS can significantly enhance multiple few-shot methods to achieve newstate-of-the-art performance on four benchmark datasets. The code is availableat: https://github.com/remiMZ/HTS-ECCV22."
Convolutional Bypasses Are Better Vision Transformer Adapters,"['Shibo Jie', 'Zhi-Hong Deng']",http://arxiv.org/pdf/2207.07039v3.pdf,2022-07-14,['cs.cv'],"  The pretrain-then-finetune paradigm has been widely adopted in computervision. But as the size of Vision Transformer (ViT) grows exponentially, thefull finetuning becomes prohibitive in view of the heavier storage overhead.Motivated by parameter-efficient transfer learning (PETL) on languagetransformers, recent studies attempt to insert lightweight adaptation modules(e.g., adapter layers or prompt tokens) to pretrained ViT and only finetunethese modules while the pretrained weights are frozen. However, these moduleswere originally proposed to finetune language models and did not take intoaccount the prior knowledge specifically for visual tasks. In this paper, wepropose to construct Convolutional Bypasses (Convpass) in ViT as adaptationmodules, introducing only a small amount (less than 0.5% of model parameters)of trainable parameters to adapt the large ViT. Different from other PETLmethods, Convpass benefits from the hard-coded inductive bias of convolutionallayers and thus is more suitable for visual tasks, especially in the low-dataregime. Experimental results on VTAB-1K benchmark and few-shot learningdatasets show that Convpass outperforms current language-oriented adaptationmodules, demonstrating the necessity to tailor vision-oriented adaptationmodules for adapting vision models."
Learn-to-Decompose: Cascaded Decomposition Network for Cross-Domain  Few-Shot Facial Expression Recognition,"['Xinyi Zou', 'Yan Yan', 'Jing-Hao Xue', 'Si Chen', 'Hanzi Wang']",http://arxiv.org/pdf/2207.07973v1.pdf,2022-07-16,['cs.cv'],"  Most existing compound facial expression recognition (FER) methods rely onlarge-scale labeled compound expression data for training. However, collectingsuch data is labor-intensive and time-consuming. In this paper, we address thecompound FER task in the cross-domain few-shot learning (FSL) setting, whichrequires only a few samples of compound expressions in the target domain.Specifically, we propose a novel cascaded decomposition network (CDNet), whichcascades several learn-to-decompose modules with shared parameters based on asequential decomposition mechanism, to obtain a transferable feature space. Toalleviate the overfitting problem caused by limited base classes in our task, apartial regularization strategy is designed to effectively exploit the best ofboth episodic training and batch training. By training across similar tasks onmultiple basic expression datasets, CDNet learns the ability oflearn-to-decompose that can be easily adapted to identify unseen compoundexpressions. Extensive experiments on both in-the-lab and in-the-wild compoundexpression datasets demonstrate the superiority of our proposed CDNet againstseveral state-of-the-art FSL methods. Code is available at:https://github.com/zouxinyi0625/CDNet."
STT: Soft Template Tuning for Few-Shot Adaptation,"['Ping Yu', 'Wei Wang', 'Chunyuan Li', 'Ruiyi Zhang', 'Zhanpeng Jin', 'Changyou Chen']",http://arxiv.org/pdf/2207.08408v1.pdf,2022-07-18,"['cs.cl', 'cs.ai']","  Prompt tuning has been an extremely effective tool to adapt a pre-trainedmodel to downstream tasks. However, standard prompt-based methods mainlyconsider the case of sufficient data of downstream tasks. It is still unclearwhether the advantage can be transferred to the few-shot regime, where onlylimited data are available for each downstream task. Although some works havedemonstrated the potential of prompt-tuning under the few-shot setting, themain stream methods via searching discrete prompts or tuning soft prompts withlimited data are still very challenging. Through extensive empirical studies,we find that there is still a gap between prompt tuning and fully fine-tuningfor few-shot learning. To bridge the gap, we propose a new prompt-tuningframework, called Soft Template Tuning (STT). STT combines manual and autoprompts, and treats downstream classification tasks as a masked languagemodeling task. Comprehensive evaluation on different settings suggests STT canclose the gap between fine-tuning and prompt-based methods without introducingadditional parameters. Significantly, it can even outperform the time- andresource-consuming fine-tuning method on sentiment classification tasks."
On the cross-lingual transferability of multilingual prototypical models  across NLU tasks,"['Oralie Cattan', 'Christophe Servan', 'Sophie Rosset']",http://arxiv.org/pdf/2207.09157v1.pdf,2022-07-19,"['cs.cl', '68t50', 'i.2.7']","  Supervised deep learning-based approaches have been applied to task-orienteddialog and have proven to be effective for limited domain and languageapplications when a sufficient number of training examples are available. Inpractice, these approaches suffer from the drawbacks of domain-driven designand under-resourced languages. Domain and language models are supposed to growand change as the problem space evolves. On one hand, research on transferlearning has demonstrated the cross-lingual ability of multilingualTransformers-based models to learn semantically rich representations. On theother, in addition to the above approaches, meta-learning have enabled thedevelopment of task and language learning algorithms capable of fargeneralization. Through this context, this article proposes to investigate thecross-lingual transferability of using synergistically few-shot learning withprototypical neural networks and multilingual Transformers-based models.Experiments in natural language understanding tasks on MultiATIS++ corpus showsthat our approach substantially improves the observed transfer learningperformances between the low and the high resource languages. More generallyour approach confirms that the meaningful latent space learned in a givenlanguage can be can be generalized to unseen and under-resourced ones usingmeta-learning."
Self-Supervision Can Be a Good Few-Shot Learner,"['Yuning Lu', 'Liangjian Wen', 'Jianzhuang Liu', 'Yajing Liu', 'Xinmei Tian']",http://arxiv.org/pdf/2207.09176v1.pdf,2022-07-19,['cs.cv'],"  Existing few-shot learning (FSL) methods rely on training with a largelabeled dataset, which prevents them from leveraging abundant unlabeled data.From an information-theoretic perspective, we propose an effective unsupervisedFSL method, learning representations with self-supervision. Following theInfoMax principle, our method learns comprehensive representations by capturingthe intrinsic structure of the data. Specifically, we maximize the mutualinformation (MI) of instances and their representations with a low-bias MIestimator to perform self-supervised pre-training. Rather than supervisedpre-training focusing on the discriminable features of the seen classes, ourself-supervised model has less bias toward the seen classes, resulting inbetter generalization for unseen classes. We explain that supervisedpre-training and self-supervised pre-training are actually maximizing differentMI objectives. Extensive experiments are further conducted to analyze their FSLperformance with various training settings. Surprisingly, the results show thatself-supervised pre-training can outperform supervised pre-training under theappropriate conditions. Compared with state-of-the-art FSL methods, ourapproach achieves comparable performance on widely used FSL benchmarks withoutany labels of the base classes."
Similarity of Pre-trained and Fine-tuned Representations,"['Thomas Goerttler', 'Klaus Obermayer']",http://arxiv.org/pdf/2207.09225v1.pdf,2022-07-19,"['cs.lg', 'cs.ai', 'stat.ml']","  In transfer learning, only the last part of the networks - the so-called head- is often fine-tuned. Representation similarity analysis shows that the mostsignificant change still occurs in the head even if all weights are updatable.However, recent results from few-shot learning have shown that representationchange in the early layers, which are mostly convolutional, is beneficial,especially in the case of cross-domain adaption. In our paper, we find outwhether that also holds true for transfer learning. In addition, we analyze thechange of representation in transfer learning, both during pre-training andfine-tuning, and find out that pre-trained structure is unlearned if notusable."
Language Model Cascades,"['David Dohan', 'Winnie Xu', 'Aitor Lewkowycz', 'Jacob Austin', 'David Bieber', 'Raphael Gontijo Lopes', 'Yuhuai Wu', 'Henryk Michalewski', 'Rif A. Saurous', 'Jascha Sohl-dickstein', 'Kevin Murphy', 'Charles Sutton']",http://arxiv.org/pdf/2207.10342v2.pdf,2022-07-21,"['cs.cl', 'cs.ai']","  Prompted models have demonstrated impressive few-shot learning abilities.Repeated interactions at test-time with a single model, or the composition ofmultiple models together, further expands capabilities. These compositions areprobabilistic models, and may be expressed in the language of graphical modelswith random variables whose values are complex data types such as strings.Cases with control flow and dynamic structure require techniques fromprobabilistic programming, which allow implementing disparate model structuresand inference strategies in a unified language. We formalize several existingtechniques from this perspective, including scratchpads / chain of thought,verifiers, STaR, selection-inference, and tool use. We refer to the resultingprograms as language model cascades."
Few-shot Adaptation Works with UnpredicTable Data,"['Jun Shern Chan', 'Michael Pieler', 'Jonathan Jao', 'Jérémy Scheurer', 'Ethan Perez']",http://arxiv.org/pdf/2208.01009v2.pdf,2022-08-01,"['cs.cl', 'cs.ai', 'cs.lg']","  Prior work on language models (LMs) shows that training on a large number ofdiverse tasks improves few-shot learning (FSL) performance on new tasks. Wetake this to the extreme, automatically extracting 413,299 tasks from internettables - orders of magnitude more than the next-largest public datasets.Finetuning on the resulting dataset leads to improved FSL performance onNatural Language Processing (NLP) tasks, but not proportionally to datasetscale. In fact, we find that narrow subsets of our dataset sometimes outperformmore diverse datasets. For example, finetuning on software documentation fromsupport.google.com raises FSL performance by a mean of +7.5% on 52 downstreamtasks, which beats training on 40 human-curated NLP datasets (+6.7%).Finetuning on various narrow datasets leads to similar broad improvementsacross test tasks, suggesting that the gains are not from domain adaptation butadapting to FSL in general. We do not observe clear patterns between thedatasets that lead to FSL gains, leaving open questions about why certain datahelps with FSL."
Robotic Interestingness via Human-Informed Few-Shot Object Detection,"['Seungchan Kim', 'Chen Wang', 'Bowen Li', 'Sebastian Scherer']",http://arxiv.org/pdf/2208.01084v1.pdf,2022-08-01,['cs.ro'],"  Interestingness recognition is crucial for decision making in autonomousexploration for mobile robots. Previous methods proposed an unsupervised onlinelearning approach that can adapt to environments and detect interesting scenesquickly, but lack the ability to adapt to human-informed interesting objects.To solve this problem, we introduce a human-interactive framework,AirInteraction, that can detect human-informed objects via few-shot onlinelearning. To reduce the communication bandwidth, we first apply an onlineunsupervised learning algorithm on the unmanned vehicle for interestingnessrecognition and then only send the potential interesting scenes to abase-station for human inspection. The human operator is able to draw andprovide bounding box annotations for particular interesting objects, which aresent back to the robot to detect similar objects via few-shot learning. Onlyusing few human-labeled examples, the robot can learn novel interesting objectcategories during the mission and detect interesting scenes that contain theobjects. We evaluate our method on various interesting scene recognitiondatasets. To the best of our knowledge, it is the first human-informed few-shotobject detection framework for autonomous exploration."
Cross-lingual Approaches for the Detection of Adverse Drug Reactions in  German from a Patient's Perspective,"['Lisa Raithel', 'Philippe Thomas', 'Roland Roller', 'Oliver Sapina', 'Sebastian Möller', 'Pierre Zweigenbaum']",http://arxiv.org/pdf/2208.02031v1.pdf,2022-08-03,"['cs.cl', 'cs.lg']","  In this work, we present the first corpus for German Adverse Drug Reaction(ADR) detection in patient-generated content. The data consists of 4,169 binaryannotated documents from a German patient forum, where users talk about healthissues and get advice from medical doctors. As is common in social media datain this domain, the class labels of the corpus are very imbalanced. This and ahigh topic imbalance make it a very challenging dataset, since often, the samesymptom can have several causes and is not always related to a medicationintake. We aim to encourage further multi-lingual efforts in the domain of ADRdetection and provide preliminary experiments for binary classification usingdifferent methods of zero- and few-shot learning based on a multi-lingualmodel. When fine-tuning XLM-RoBERTa first on English patient forum data andthen on the new German data, we achieve an F1-score of 37.52 for the positiveclass. We make the dataset and models publicly available for the community."
Free-HeadGAN: Neural Talking Head Synthesis with Explicit Gaze Control,"['Michail Christos Doukas', 'Evangelos Ververas', 'Viktoriia Sharmanska', 'Stefanos Zafeiriou']",http://arxiv.org/pdf/2208.02210v1.pdf,2022-08-03,['cs.cv'],"  We present Free-HeadGAN, a person-generic neural talking head synthesissystem. We show that modeling faces with sparse 3D facial landmarks aresufficient for achieving state-of-the-art generative performance, withoutrelying on strong statistical priors of the face, such as 3D Morphable Models.Apart from 3D pose and facial expressions, our method is capable of fullytransferring the eye gaze, from a driving actor to a source identity. Ourcomplete pipeline consists of three components: a canonical 3D key-pointestimator that regresses 3D pose and expression-related deformations, a gazeestimation network and a generator that is built upon the architecture ofHeadGAN. We further experiment with an extension of our generator toaccommodate few-shot learning using an attention mechanism, in case more thanone source images are available. Compared to the latest models for reenactmentand motion transfer, our system achieves higher photo-realism combined withsuperior identity preservation, while offering explicit gaze control."
Solving the Baby Intuitions Benchmark with a Hierarchically Bayesian  Theory of Mind,"['Tan Zhi-Xuan', 'Nishad Gothoskar', 'Falk Pollok', 'Dan Gutfreund', 'Joshua B. Tenenbaum', 'Vikash K. Mansinghka']",http://arxiv.org/pdf/2208.02914v1.pdf,2022-08-04,['cs.ai'],"  To facilitate the development of new models to bridge the gap between machineand human social intelligence, the recently proposed Baby Intuitions Benchmark(arXiv:2102.11938) provides a suite of tasks designed to evaluate commonsensereasoning about agents' goals and actions that even young infants exhibit. Herewe present a principled Bayesian solution to this benchmark, based on ahierarchically Bayesian Theory of Mind (HBToM). By including hierarchicalpriors on agent goals and dispositions, inference over our HBToM model enablesfew-shot learning of the efficiency and preferences of an agent, which can thenbe used in commonsense plausibility judgements about subsequent agent behavior.This approach achieves near-perfect accuracy on most benchmark tasks,outperforming deep learning and imitation learning baselines while producinginterpretable human-like inferences, demonstrating the advantages of structuredBayesian models of human social cognition."
Convolutional Ensembling based Few-Shot Defect Detection Technique,"['Soumyajit Karmakar', 'Abeer Banerjee', 'Prashant Sadashiv Gidde', 'Sumeet Saurav', 'Sanjay Singh']",http://arxiv.org/pdf/2208.03288v3.pdf,2022-08-05,"['cs.cv', 'cs.ai']","  Over the past few years, there has been a significant improvement in thedomain of few-shot learning. This learning paradigm has shown promising resultsfor the challenging problem of anomaly detection, where the general task is todeal with heavy class imbalance. Our paper presents a new approach to few-shotclassification, where we employ the knowledge-base of multiple pre-trainedconvolutional models that act as the backbone for our proposed few-shotframework. Our framework uses a novel ensembling technique for boosting theaccuracy while drastically decreasing the total parameter count, thus pavingthe way for real-time implementation. We perform an extensive hyperparametersearch using a power-line defect detection dataset and obtain an accuracy of92.30% for the 5-way 5-shot task. Without further tuning, we evaluate our modelon competing standards with the existing state-of-the-art methods andoutperform them."
Atlas: Few-shot Learning with Retrieval Augmented Language Models,"['Gautier Izacard', 'Patrick Lewis', 'Maria Lomeli', 'Lucas Hosseini', 'Fabio Petroni', 'Timo Schick', 'Jane Dwivedi-Yu', 'Armand Joulin', 'Sebastian Riedel', 'Edouard Grave']",http://arxiv.org/pdf/2208.03299v3.pdf,2022-08-05,['cs.cl'],"  Large language models have shown impressive few-shot results on a wide rangeof tasks. However, when knowledge is key for such results, as is the case fortasks such as question answering and fact checking, massive parameter counts tostore knowledge seem to be needed. Retrieval augmented models are known toexcel at knowledge intensive tasks without the need for as many parameters, butit is unclear whether they work in few-shot settings. In this work we presentAtlas, a carefully designed and pre-trained retrieval augmented language modelable to learn knowledge intensive tasks with very few training examples. Weperform evaluations on a wide range of tasks, including MMLU, KILT andNaturalQuestions, and study the impact of the content of the document index,showing that it can easily be updated. Notably, Atlas reaches over 42% accuracyon Natural Questions using only 64 examples, outperforming a 540B parametersmodel by 3% despite having 50x fewer parameters."
Limits of an AI program for solving college math problems,['Ernest Davis'],http://arxiv.org/pdf/2208.06906v1.pdf,2022-08-14,['cs.ai'],"  Drori et al. (2022) report that ""A neural network solves, explains, andgenerates university math problems by program synthesis and few-shot learningat human level ... [It] automatically answers 81\% of university-levelmathematics problems."" The system they describe is indeed impressive; however,the above description is very much overstated. The work of solving the problemsis done, not by a neural network, but by the symbolic algebra package Sympy.Problems of various formats are excluded from consideration. The so-called""explanations"" are just rewordings of lines of code. Answers are marked ascorrect that are not in the form specified in the problem. Most seriously, itseems that in many cases the system uses the correct answer given in the testcorpus to guide its path to solving the problem."
Transductive Decoupled Variational Inference for Few-Shot Classification,"['Anuj Singh', 'Hadi Jamali-Rad']",http://arxiv.org/pdf/2208.10559v1.pdf,2022-08-22,['cs.cv'],"  The versatility to learn from a handful of samples is the hallmark of humanintelligence. Few-shot learning is an endeavour to transcend this capabilitydown to machines. Inspired by the promise and power of probabilistic deeplearning, we propose a novel variational inference network for few-shotclassification (coined as TRIDENT) to decouple the representation of an imageinto semantic and label latent variables, and simultaneously infer them in anintertwined fashion. To induce task-awareness, as part of the inferencemechanics of TRIDENT, we exploit information across both query and supportimages of a few-shot task using a novel built-in attention-based transductivefeature extraction module (we call AttFEX). Our extensive experimental resultscorroborate the efficacy of TRIDENT and demonstrate that, using the simplest ofbackbones, it sets a new state-of-the-art in the most commonly adopted datasetsminiImageNet and tieredImageNet (offering up to 4% and 5% improvements,respectively), as well as for the recent challenging cross-domain miniImagenet--> CUB scenario offering a significant margin (up to 20% improvement) beyondthe best existing cross-domain baselines. Code and experimentation can be foundin our GitHub repository: https://github.com/anujinho/trident"
Unsupervised Question Answering via Answer Diversifying,"['Yuxiang Nie', 'Heyan Huang', 'Zewen Chi', 'Xian-Ling Mao']",http://arxiv.org/pdf/2208.10813v1.pdf,2022-08-23,['cs.cl'],"  Unsupervised question answering is an attractive task due to its independenceon labeled data. Previous works usually make use of heuristic rules as well aspre-trained models to construct data and train QA models. However, most ofthese works regard named entity (NE) as the only answer type, which ignores thehigh diversity of answers in the real world. To tackle this problem, we proposea novel unsupervised method by diversifying answers, named DiverseQA.Specifically, the proposed method is composed of three modules: dataconstruction, data augmentation and denoising filter. Firstly, the dataconstruction module extends the extracted named entity into a longer sentenceconstituent as the new answer span to construct a QA dataset with diverseanswers. Secondly, the data augmentation module adopts an answer-type dependentdata augmentation process via adversarial training in the embedding level.Thirdly, the denoising filter module is designed to alleviate the noise in theconstructed data. Extensive experiments show that the proposed methodoutperforms previous unsupervised models on five benchmark datasets, includingSQuADv1.1, NewsQA, TriviaQA, BioASQ, and DuoRC. Besides, the proposed methodshows strong performance in the few-shot learning setting."
Transfer Learning Application of Self-supervised Learning in ARPES,"['Sandy Adhitia Ekahana', 'Genta Indra Winata', 'Y. Soh', 'Gabriel Aeppli', 'Radovic Milan', 'Ming Shi']",http://arxiv.org/pdf/2208.10893v1.pdf,2022-08-23,"['physics.ins-det', 'cs.lg', 'physics.data-an']","  Recent development in angle-resolved photoemission spectroscopy (ARPES)technique involves spatially resolving samples while maintaining thehigh-resolution feature of momentum space. This development easily expands thedata size and its complexity for data analysis, where one of it is to labelsimilar dispersion cuts and map them spatially. In this work, we demonstratethat the recent development in representational learning (self-supervisedlearning) model combined with k-means clustering can help automate that part ofdata analysis and save precious time, albeit with low performance. Finally, weintroduce a few-shot learning (k-nearest neighbour or kNN) in representationalspace where we selectively choose one (k=1) image reference for each knownlabel and subsequently label the rest of the data with respect to the nearestreference image. This last approach demonstrates the strength of theself-supervised learning to automate the image analysis in ARPES in particularand can be generalized into any science data analysis that heavily involvesimage data."
AniWho : A Quick and Accurate Way to Classify Anime Character Faces in  Images,"['Martinus Grady Naftali', 'Jason Sebastian Sulistyawan', 'Kelvin Julian']",http://arxiv.org/pdf/2208.11012v3.pdf,2022-08-23,"['cs.cv', 'cs.lg']","  In order to classify Japanese animation-style character faces, this paperattempts to delve further into the many models currently available, includingInceptionV3, InceptionResNetV2, MobileNetV2, and EfficientNet, employingtransfer learning. This paper demonstrates that EfficientNet-B7, which achievesa top-1 accuracy of 85.08%, has the highest accuracy rate. MobileNetV2, whichachieves a less accurate result with a top-1 accuracy of 81.92%, benefits froma significantly faster inference time and fewer required parameters. However,from the experiment, MobileNet-V2 is prone to overfitting; EfficienNet-B0 fixedthe overfitting issue but with a cost of a little slower in inference time thanMobileNet-V2 but a little more accurate result, top-1 accuracy of 83.46%. Thispaper also uses a few-shot learning architecture called Prototypical Networks,which offers an adequate substitute for conventional transfer learningtechniques."
Automatic Identification of Coal and Rock/Gangue Based on DenseNet and  Gaussian Process,['Yufan Li'],http://arxiv.org/pdf/2208.14871v1.pdf,2022-08-31,['cs.cv'],"  To improve the purity of coal and prevent damage to the coal mining machine,it is necessary to identify coal and rock in underground coal mines. At thesame time, the mined coal needs to be purified to remove rock and gangue. Thesetwo procedures are manually operated by workers in most coal mines. Therealization of automatic identification and purification is not only conduciveto the automation of coal mines, but also ensures the safety of workers. Wediscuss the possibility of using image-based methods to distinguish them. Inorder to find a solution that can be used in both scenarios, a model thatforwards image feature extracted by DenseNet to Gaussian process is proposed,which is trained on images taken on surface and achieves high accuracy onimages taken underground. This indicates our method is powerful in few-shotlearning such as identification of coal and rock/gangue and might be beneficialfor realizing automation in coal mines."
IMG2IMU: Applying Knowledge from Large-Scale Images to IMU Applications  via Contrastive Learning,"['Hyungjun Yoon', 'Hyeongheon Cha', 'Canh Hoang Nguyen', 'Taesik Gong', 'Sung-Ju Lee']",http://arxiv.org/pdf/2209.00945v1.pdf,2022-09-02,"['cs.lg', '68t20']","  Recent advances in machine learning showed that pre-training representationsacquired via self-supervised learning could achieve high accuracy on tasks withsmall training data. Unlike in vision and natural language processing domains,such pre-training for IMU-based applications is challenging, as there are onlya few publicly available datasets with sufficient size and diversity to learngeneralizable representations. To overcome this problem, we propose IMG2IMU, anovel approach that adapts pre-train representation from large-scale images todiverse few-shot IMU sensing tasks. We convert the sensor data into visuallyinterpretable spectrograms for the model to utilize the knowledge gained fromvision. Further, we apply contrastive learning on an augmentation set wedesigned to learn representations that are tailored to interpreting sensordata. Our extensive evaluations on five different IMU sensing tasks show thatIMG2IMU consistently outperforms the baselines, illustrating that visionknowledge can be incorporated into a few-shot learning environment for IMUsensing tasks."
A Study on Representation Transfer for Few-Shot Learning,"['Chun-Nam Yu', 'Yi Xie']",http://arxiv.org/pdf/2209.02073v1.pdf,2022-09-05,"['cs.cv', 'cs.lg']","  Few-shot classification aims to learn to classify new object categories wellusing only a few labeled examples. Transferring feature representations fromother models is a popular approach for solving few-shot classificationproblems. In this work we perform a systematic study of various featurerepresentations for few-shot classification, including representations learnedfrom MAML, supervised classification, and several common self-supervised tasks.We find that learning from more complex tasks tend to give betterrepresentations for few-shot classification, and thus we propose the use ofrepresentations learned from multiple tasks for few-shot classification.Coupled with new tricks on feature selection and voting to handle the issue ofsmall sample size, our direct transfer learning method offers performancecomparable to state-of-art on several benchmark datasets."
Few-Shot Classification with Contrastive Learning,"['Zhanyuan Yang', 'Jinghua Wang', 'Yingying Zhu']",http://arxiv.org/pdf/2209.08224v1.pdf,2022-09-17,['cs.cv'],"  A two-stage training paradigm consisting of sequential pre-training andmeta-training stages has been widely used in current few-shot learning (FSL)research. Many of these methods use self-supervised learning and contrastivelearning to achieve new state-of-the-art results. However, the potential ofcontrastive learning in both stages of FSL training paradigm is still not fullyexploited. In this paper, we propose a novel contrastive learning-basedframework that seamlessly integrates contrastive learning into both stages toimprove the performance of few-shot classification. In the pre-training stage,we propose a self-supervised contrastive loss in the forms of feature vectorvs. feature map and feature map vs. feature map, which uses global and localinformation to learn good initial representations. In the meta-training stage,we propose a cross-view episodic training mechanism to perform the nearestcentroid classification on two different views of the same episode and adopt adistance-scaled contrastive loss based on them. These two strategies force themodel to overcome the bias between views and promote the transferability ofrepresentations. Extensive experiments on three benchmark datasets demonstratethat our method achieves competitive results."
A Few Shot Multi-Representation Approach for N-gram Spotting in  Historical Manuscripts,"['Giuseppe De Gregorio', 'Sanket Biswas', 'Mohamed Ali Souibgui', 'Asma Bensalah', 'Josep Lladós', 'Alicia Fornés', 'Angelo Marcelli']",http://arxiv.org/pdf/2209.10441v1.pdf,2022-09-21,['cs.cv'],"  Despite recent advances in automatic text recognition, the performanceremains moderate when it comes to historical manuscripts. This is mainlybecause of the scarcity of available labelled data to train the data-hungryHandwritten Text Recognition (HTR) models. The Keyword Spotting System (KWS)provides a valid alternative to HTR due to the reduction in error rate, but itis usually limited to a closed reference vocabulary. In this paper, we proposea few-shot learning paradigm for spotting sequences of a few characters(N-gram) that requires a small amount of labelled training data. We exhibitthat recognition of important n-grams could reduce the system's dependency onvocabulary. In this case, an out-of-vocabulary (OOV) word in an inputhandwritten line image could be a sequence of n-grams that belong to thelexicon. An extensive experimental evaluation of our proposedmulti-representation approach was carried out on a subset of Bentham'shistorical manuscript collections to obtain some really promising results inthis direction."
Efficient Few-Shot Learning Without Prompts,"['Lewis Tunstall', 'Nils Reimers', 'Unso Eun Seo Jo', 'Luke Bates', 'Daniel Korat', 'Moshe Wasserblat', 'Oren Pereg']",http://arxiv.org/pdf/2209.11055v1.pdf,2022-09-22,['cs.cl'],"  Recent few-shot methods, such as parameter-efficient fine-tuning (PEFT) andpattern exploiting training (PET), have achieved impressive results inlabel-scarce settings. However, they are difficult to employ since they aresubject to high variability from manually crafted prompts, and typicallyrequire billion-parameter language models to achieve high accuracy. To addressthese shortcomings, we propose SetFit (Sentence Transformer Fine-tuning), anefficient and prompt-free framework for few-shot fine-tuning of SentenceTransformers (ST). SetFit works by first fine-tuning a pretrained ST on a smallnumber of text pairs, in a contrastive Siamese manner. The resulting model isthen used to generate rich text embeddings, which are used to train aclassification head. This simple framework requires no prompts or verbalizers,and achieves high accuracy with orders of magnitude less parameters thanexisting techniques. Our experiments show that SetFit obtains comparableresults with PEFT and PET techniques, while being an order of magnitude fasterto train. We also show that SetFit can be applied in multilingual settings bysimply switching the ST body. Our code is available athttps://github.com/huggingface/setfit and our datasets athttps://huggingface.co/setfit ."
Learning Chess With Language Models and Transformers,"['Michael DeLeo', 'Erhan Guven']",http://arxiv.org/pdf/2209.11902v1.pdf,2022-09-24,"['cs.ai', 'cs.cl', 'cs.gt', 'cs.lg']","  Representing a board game and its positions by text-based notation enablesthe possibility of NLP applications. Language models, can help gain insightinto a variety of interesting problems such as unsupervised learning rules of agame, detecting player behavior patterns, player attribution, and ultimatelylearning the game to beat state of the art. In this study, we applied BERTmodels, first to the simple Nim game to analyze its performance in the presenceof noise in a setup of a few-shot learning architecture. We analyzed the modelperformance via three virtual players, namely Nim Guru, Random player, andQ-learner. In the second part, we applied the game learning language model tothe chess game, and a large set of grandmaster games with exhaustiveencyclopedia openings. Finally, we have shown that model practically learns therules of the chess game and can survive games against Stockfish at a category-Arating level."
Active Transfer Prototypical Network: An Efficient Labeling Algorithm  for Time-Series Data,"['Yuqicheng Zhu', 'Mohamed-Ali Tnani', 'Timo Jahnz', 'Klaus Diepold']",http://arxiv.org/pdf/2209.14199v1.pdf,2022-09-28,['cs.lg'],"  The paucity of labeled data is a typical challenge in the automotiveindustry. Annotating time-series measurements requires solid domain knowledgeand in-depth exploratory data analysis, which implies a high labeling effort.Conventional Active Learning (AL) addresses this issue by actively querying themost informative instances based on the estimated classification probabilityand retraining the model iteratively. However, the learning efficiency stronglyrelies on the initial model, resulting in the trade-off between the size of theinitial dataset and the query number. This paper proposes a novel Few-ShotLearning (FSL)-based AL framework, which addresses the trade-off problem byincorporating a Prototypical Network (ProtoNet) in the AL iterations. Theresults show an improvement, on the one hand, in the robustness to the initialmodel and, on the other hand, in the learning efficiency of the ProtoNetthrough the active selection of the support set in each iteration. Thisframework was validated on UCI HAR/HAPT dataset and a real-world brakingmaneuver dataset. The learning performance significantly surpasses traditionalAL algorithms on both datasets, achieving 90% classification accuracy with 10%and 5% labeling effort, respectively."
AI-Driven Road Maintenance Inspection v2: Reducing Data Dependency &  Quantifying Road Damage,"['Haris Iqbal', 'Hemang Chawla', 'Arnav Varma', 'Terence Brouns', 'Ahmed Badar', 'Elahe Arani', 'Bahram Zonooz']",http://arxiv.org/pdf/2210.03570v1.pdf,2022-10-07,['cs.cv'],"  Road infrastructure maintenance inspection is typically a labor-intensive andcritical task to ensure the safety of all road users. Existing state-of-the-arttechniques in Artificial Intelligence (AI) for object detection andsegmentation help automate a huge chunk of this task given adequate annotateddata. However, annotating videos from scratch is cost-prohibitive. Forinstance, it can take an annotator several days to annotate a 5-minute videorecorded at 30 FPS. Hence, we propose an automated labelling pipeline byleveraging techniques like few-shot learning and out-of-distribution detectionto generate labels for road damage detection. In addition, our pipelineincludes a risk factor assessment for each damage by instance quantification toprioritize locations for repairs which can lead to optimal deployment of roadmaintenance machinery. We show that the AI models trained with these techniquescan not only generalize better to unseen real-world data with reducedrequirement for human annotation but also provide an estimate of maintenanceurgency, thereby leading to safer roads."
Multi-Modal Fusion by Meta-Initialization,"['Matthew T. Jackson', 'Shreshth A. Malik', 'Michael T. Matthews', 'Yousuf Mohamed-Ahmed']",http://arxiv.org/pdf/2210.04843v1.pdf,2022-10-10,"['cs.lg', 'cs.cv']","  When experience is scarce, models may have insufficient information to adaptto a new task. In this case, auxiliary information - such as a textualdescription of the task - can enable improved task inference and adaptation. Inthis work, we propose an extension to the Model-Agnostic Meta-Learningalgorithm (MAML), which allows the model to adapt using auxiliary informationas well as task experience. Our method, Fusion by Meta-Initialization (FuMI),conditions the model initialization on auxiliary information using ahypernetwork, rather than learning a single, task-agnostic initialization.Furthermore, motivated by the shortcomings of existing multi-modal few-shotlearning benchmarks, we constructed iNat-Anim - a large-scale imageclassification dataset with succinct and visually pertinent textual classdescriptions. On iNat-Anim, FuMI significantly outperforms uni-modal baselinessuch as MAML in the few-shot regime. The code for this project and a datasetexploration tool for iNat-Anim are publicly available athttps://github.com/s-a-malik/multi-few ."
CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation,"['Tanay Dixit', 'Bhargavi Paranjape', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer']",http://arxiv.org/pdf/2210.04873v2.pdf,2022-10-10,['cs.cl'],"  Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbedinputs during training -- helps reduce model reliance on spurious correlationsand improves generalization to out-of-distribution (OOD) data. Prior work ongenerating counterfactuals only considered restricted classes of perturbations,limiting their effectiveness. We present COunterfactual Generation viaRetrieval and Editing (CORE), a retrieval-augmented generation framework forcreating diverse counterfactual perturbations for CDA. For each trainingexample, CORE first performs a dense retrieval over a task-related unlabeledtext corpus using a learned bi-encoder and extracts relevant counterfactualexcerpts. CORE then incorporates these into prompts to a large language modelwith few-shot learning capabilities, for counterfactual editing. Conditioninglanguage model edits on naturally occurring data results in diverseperturbations. Experiments on natural language inference and sentiment analysisbenchmarks show that CORE counterfactuals are more effective at improvinggeneralization to OOD data compared to other DA approaches. We also show thatthe CORE retrieval framework can be used to encourage diversity in manuallyauthored perturbations"
Continual Training of Language Models for Few-Shot Learning,"['Zixuan Ke', 'Haowei Lin', 'Yijia Shao', 'Hu Xu', 'Lei Shu', 'Bing Liu']",http://arxiv.org/pdf/2210.05549v1.pdf,2022-10-11,"['cs.cl', 'cs.ai', 'cs.lg', 'cs.ne']","  Recent work on applying large language models (LMs) achieves impressiveperformance in many NLP applications. Adapting or posttraining an LM using anunlabeled domain corpus can produce even better performance for end-tasks inthe domain. This paper proposes the problem of continually extending an LM byincrementally post-train the LM with a sequence of unlabeled domain corpora toexpand its knowledge without forgetting its previous skills. The goal is toimprove the few-shot end-task learning in these domains. The resulting systemis called CPT (Continual PostTraining), which to our knowledge, is the firstcontinual post-training system. Experimental results verify its effectiveness."
FontTransformer: Few-shot High-resolution Chinese Glyph Image Synthesis  via Stacked Transformers,"['Yitian Liu', 'Zhouhui Lian']",http://arxiv.org/pdf/2210.06301v2.pdf,2022-10-12,['cs.cv'],"  Automatic generation of high-quality Chinese fonts from a few online trainingsamples is a challenging task, especially when the amount of samples is verysmall. Existing few-shot font generation methods can only synthesizelow-resolution glyph images that often possess incorrect topological structuresor/and incomplete strokes. To address the problem, this paper proposesFontTransformer, a novel few-shot learning model, for high-resolution Chineseglyph image synthesis by using stacked Transformers. The key idea is to applythe parallel Transformer to avoid the accumulation of prediction errors andutilize the serial Transformer to enhance the quality of synthesized strokes.Meanwhile, we also design a novel encoding scheme to feed more glyphinformation and prior knowledge to our model, which further enables thegeneration of high-resolution and visually-pleasing glyph images. Bothqualitative and quantitative experimental results demonstrate the superiorityof our method compared to other existing approaches in the few-shot Chinesefont synthesis task."
Knowledge-grounded Dialog State Tracking,"['Dian Yu', 'Mingqiu Wang', 'Yuan Cao', 'Izhak Shafran', 'Laurent El Shafey', 'Hagen Soltau']",http://arxiv.org/pdf/2210.06656v1.pdf,2022-10-13,['cs.cl'],"  Knowledge (including structured knowledge such as schema and ontology, andunstructured knowledge such as web corpus) is a critical part of dialogunderstanding, especially for unseen tasks and domains. Traditionally, suchdomain-specific knowledge is encoded implicitly into model parameters for theexecution of downstream tasks, which makes training inefficient. In addition,such models are not easily transferable to new tasks with different schemas. Inthis work, we propose to perform dialog state tracking grounded on knowledgeencoded externally. We query relevant knowledge of various forms based on thedialog context where such information can ground the prediction of dialogstates. We demonstrate superior performance of our proposed method over strongbaselines, especially in the few-shot learning setting."
Unified Vision and Language Prompt Learning,"['Yuhang Zang', 'Wei Li', 'Kaiyang Zhou', 'Chen Huang', 'Chen Change Loy']",http://arxiv.org/pdf/2210.07225v1.pdf,2022-10-13,"['cs.cv', 'cs.ai']","  Prompt tuning, a parameter- and data-efficient transfer learning paradigmthat tunes only a small number of parameters in a model's input space, hasbecome a trend in the vision community since the emergence of largevision-language models like CLIP. We present a systematic study on tworepresentative prompt tuning methods, namely text prompt tuning and visualprompt tuning. A major finding is that none of the unimodal prompt tuningmethods performs consistently well: text prompt tuning fails on data with highintra-class visual variances while visual prompt tuning cannot handle lowinter-class variances. To combine the best from both worlds, we propose asimple approach called Unified Prompt Tuning (UPT), which essentially learns atiny neural network to jointly optimize prompts across different modalities.Extensive experiments on over 11 vision datasets show that UPT achieves abetter trade-off than the unimodal counterparts on few-shot learningbenchmarks, as well as on domain generalization benchmarks. Code and modelswill be released to facilitate future research."
Conditional Neural Processes for Molecules,"['Miguel Garcia-Ortegon', 'Andreas Bender', 'Sergio Bacallado']",http://arxiv.org/pdf/2210.09211v3.pdf,2022-10-17,"['stat.ml', 'cs.lg']","  Neural processes (NPs) are models for transfer learning with propertiesreminiscent of Gaussian Processes (GPs). They are adept at modelling dataconsisting of few observations of many related functions on the same inputspace and are trained by minimizing a variational objective, which iscomputationally much less expensive than the Bayesian updating required by GPs.So far, most studies of NPs have focused on low-dimensional datasets which arenot representative of realistic transfer learning tasks. Drug discovery is oneapplication area that is characterized by datasets consisting of many chemicalproperties or functions which are sparsely observed, yet depend on sharedfeatures or representations of the molecular inputs. This paper applies theconditional neural process (CNP) to DOCKSTRING, a dataset of docking scores forbenchmarking ML models. CNPs show competitive performance in few-shot learningtasks relative to supervised learning baselines common in chemoinformatics, aswell as an alternative model for transfer learning based on pre-training andrefining neural network regressors. We present a Bayesian optimizationexperiment which showcases the probabilistic nature of CNPs and discussshortcomings of the model in uncertainty quantification."
"Vision-Language Pre-training: Basics, Recent Advances, and Future Trends","['Zhe Gan', 'Linjie Li', 'Chunyuan Li', 'Lijuan Wang', 'Zicheng Liu', 'Jianfeng Gao']",http://arxiv.org/pdf/2210.09263v1.pdf,2022-10-17,"['cs.cv', 'cs.cl']","  This paper surveys vision-language pre-training (VLP) methods for multimodalintelligence that have been developed in the last few years. We group theseapproaches into three categories: ($i$) VLP for image-text tasks, such as imagecaptioning, image-text retrieval, visual question answering, and visualgrounding; ($ii$) VLP for core computer vision tasks, such as (open-set) imageclassification, object detection, and segmentation; and ($iii$) VLP forvideo-text tasks, such as video captioning, video-text retrieval, and videoquestion answering. For each category, we present a comprehensive review ofstate-of-the-art methods, and discuss the progress that has been made andchallenges still being faced, using specific systems and models as casestudies. In addition, for each category, we discuss advanced topics beingactively explored in the research community, such as big foundation models,unified modeling, in-context few-shot learning, knowledge, robustness, andcomputer vision in the wild, to name a few."
"Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus","['Yudong Xu', 'Elias B. Khalil', 'Scott Sanner']",http://arxiv.org/pdf/2210.09880v2.pdf,2022-10-18,['cs.ai'],"  The Abstraction and Reasoning Corpus (ARC) aims at benchmarking theperformance of general artificial intelligence algorithms. The ARC's focus onbroad generalization and few-shot learning has made it difficult to solve usingpure machine learning. A more promising approach has been to perform programsynthesis within an appropriately designed Domain Specific Language (DSL).However, these too have seen limited success. We propose Abstract Reasoningwith Graph Abstractions (ARGA), a new object-centric framework that firstrepresents images using graphs and then performs a search for a correct programin a DSL that is based on the abstracted graph space. The complexity of thiscombinatorial search is tamed through the use of constraint acquisition, statehashing, and Tabu search. An extensive set of experiments demonstrates thepromise of ARGA in tackling some of the complicated object-centric tasks of theARC rather efficiently, producing programs that are correct and easy tounderstand."
LAVA: Label-efficient Visual Learning and Adaptation,"['Islam Nassar', 'Munawar Hayat', 'Ehsan Abbasnejad', 'Hamid Rezatofighi', 'Mehrtash Harandi', 'Gholamreza Haffari']",http://arxiv.org/pdf/2210.10317v1.pdf,2022-10-19,['cs.cv'],"  We present LAVA, a simple yet effective method for multi-domain visualtransfer learning with limited data. LAVA builds on a few recent innovations toenable adapting to partially labelled datasets with class and domain shifts.First, LAVA learns self-supervised visual representations on the source datasetand ground them using class label semantics to overcome transfer collapseproblems associated with supervised pretraining. Secondly, LAVA maximises thegains from unlabelled target data via a novel method which uses multi-cropaugmentations to obtain highly robust pseudo-labels. By combining theseingredients, LAVA achieves a new state-of-the-art on ImageNet semi-supervisedprotocol, as well as on 7 out of 10 datasets in multi-domain few-shot learningon the Meta-dataset. Code and models are made available."
Self-Supervised Representation Learning for CAD,"['Benjamin T. Jones', 'Michael Hu', 'Vladimir G. Kim', 'Adriana Schulz']",http://arxiv.org/pdf/2210.10807v1.pdf,2022-10-19,"['cs.cv', 'cs.gr']","  The design of man-made objects is dominated by computer aided design (CAD)tools. Assisting design with data-driven machine learning methods is hamperedby lack of labeled data in CAD's native format; the parametric boundaryrepresentation (B-Rep). Several data sets of mechanical parts in B-Rep formathave recently been released for machine learning research. However, large scaledatabases are largely unlabeled, and labeled datasets are small. Additionally,task specific label sets are rare, and costly to annotate. This work proposesto leverage unlabeled CAD geometry on supervised learning tasks. We learn anovel, hybrid implicit/explicit surface representation for B-Rep geometry, andshow that this pre-training significantly improves few-shot learningperformance and also achieves state-of-the-art performance on several existingB-Rep benchmarks."
Better Few-Shot Relation Extraction with Label Prompt Dropout,"['Peiyuan Zhang', 'Wei Lu']",http://arxiv.org/pdf/2210.13733v1.pdf,2022-10-25,['cs.cl'],"  Few-shot relation extraction aims to learn to identify the relation betweentwo entities based on very limited training examples. Recent efforts found thattextual labels (i.e., relation names and relation descriptions) could beextremely useful for learning class representations, which will benefit thefew-shot learning task. However, what is the best way to leverage such labelinformation in the learning process is an important research question. Existingworks largely assume such textual labels are always present during bothlearning and prediction. In this work, we argue that such approaches may notalways lead to optimal results. Instead, we present a novel approach calledlabel prompt dropout, which randomly removes label descriptions in the learningprocess. Our experiments show that our approach is able to lead to improvedclass representations, yielding significantly better results on the few-shotrelation extraction task."
Dictionary-Assisted Supervised Contrastive Learning,"['Patrick Y. Wu', 'Richard Bonneau', 'Joshua A. Tucker', 'Jonathan Nagler']",http://arxiv.org/pdf/2210.15172v1.pdf,2022-10-27,"['cs.cl', 'cs.lg']","  Text analysis in the social sciences often involves using specializeddictionaries to reason with abstract concepts, such as perceptions about theeconomy or abuse on social media. These dictionaries allow researchers toimpart domain knowledge and note subtle usages of words relating to aconcept(s) of interest. We introduce the dictionary-assisted supervisedcontrastive learning (DASCL) objective, allowing researchers to leveragespecialized dictionaries when fine-tuning pretrained language models. The textis first keyword simplified: a common, fixed token replaces any word in thecorpus that appears in the dictionary(ies) relevant to the concept of interest.During fine-tuning, a supervised contrastive objective draws closer theembeddings of the original and keyword-simplified texts of the same class whilepushing further apart the embeddings of different classes. Thekeyword-simplified texts of the same class are more textually similar thantheir original text counterparts, which additionally draws the embeddings ofthe same class closer together. Combining DASCL and cross-entropy improvesclassification performance metrics in few-shot learning settings and socialscience applications compared to using cross-entropy alone and alternativecontrastive and data augmentation methods."
A few-shot learning approach with domain adaptation for personalized  real-life stress detection in close relationships,"['Kexin Feng', 'Jacqueline B. Duong', 'Kayla E. Carta', 'Sierra Walters', 'Gayla Margolin', 'Adela C. Timmons', 'Theodora Chaspari']",http://arxiv.org/pdf/2210.15247v1.pdf,2022-10-27,"['cs.lg', 'cs.mm']","  We design a metric learning approach that aims to address computationalchallenges that yield from modeling human outcomes from ambulatory real-lifedata. The proposed metric learning is based on a Siamese neural network (SNN)that learns the relative difference between pairs of samples from a target userand non-target users, thus being able to address the scarcity of labelled datafrom the target. The SNN further minimizes the Wasserstein distance of thelearned embeddings between target and non-target users, thus mitigating thedistribution mismatch between the two. Finally, given the fact that the baserate of focal behaviors is different per user, the proposed method approximatesthe focal base rate based on labelled samples that lay closest to the target,based on which further minimizes the Wasserstein distance. Our method isexemplified for the purpose of hourly stress classification using real-lifemultimodal data from 72 dating couples. Results in few-shot and one-shotlearning experiments indicate that proposed formulation benefits stressclassification and can help mitigate the aforementioned challenges."
STPrompt: Semantic-guided and Task-driven prompts for Effective Few-shot  Classification,"['Jinta Weng', 'Yue Hu', 'Jing Qiu', 'Heyan Huan']",http://arxiv.org/pdf/2210.16489v1.pdf,2022-10-29,"['cs.cl', 'cs.ai']","  The effectiveness of prompt learning has been demonstrated in differentpre-trained language models. By formulating suitable template and choosingrepresentative label mapping, prompt learning can be used as an efficientknowledge probe. However, finding suitable prompt in existing methods requiresmultiple experimental attempts or appropriate vector initialization onformulating suitable template and choosing representative label mapping, whichit is more common in few-shot learning tasks. Motivating by PLM workingprocess, we try to construct the prompt from task semantic perspective and thuspropose the STPrompt -Semantic-guided and Task-driven Prompt model.Specifically, two novel prompts generated from the semantic dependency tree(Dep-prompt) and task-specific metadata description (Meta-prompt), are firstlyconstructed in a prompt augmented pool, and the proposed model wouldautomatically select a suitable semantic prompt to motivating the promptlearning process. Our results show that the proposed model achieves thestate-of-the-art performance in five different datasets of few-shot textclassification tasks, which prove that more semantic and significant promptscould assume as a better knowledge proving tool."
Temporal-Viewpoint Transportation Plan for Skeletal Few-shot Action  Recognition,"['Lei Wang', 'Piotr Koniusz']",http://arxiv.org/pdf/2210.16820v1.pdf,2022-10-30,"['cs.cv', 'cs.ai', 'cs.lg']","  We propose a Few-shot Learning pipeline for 3D skeleton-based actionrecognition by Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE). Tofactor out misalignment between query and support sequences of 3D body joints,we propose an advanced variant of Dynamic Time Warping which jointly modelseach smooth path between the query and support frames to achieve simultaneouslythe best alignment in the temporal and simulated camera viewpoint spaces forend-to-end learning under the limited few-shot training data. Sequences areencoded with a temporal block encoder based on Simple Spectral GraphConvolution, a lightweight linear Graph Neural Network backbone. We alsoinclude a setting with a transformer. Finally, we propose a similarity-basedloss which encourages the alignment of sequences of the same class whilepreventing the alignment of unrelated sequences. We show state-of-the-artresults on NTU-60, NTU-120, Kinetics-skeleton and UWA3D Multiview Activity II."
Few-shot Bioacoustic Event Detection with Machine Learning Methods,"['Leah Chowenhill', 'Gaurav Satyanath', 'Shubhranshu Singh', 'Madhav Mahendra Wagh']",http://arxiv.org/pdf/2211.00569v1.pdf,2022-08-09,"['eess.as', 'cs.sd', 'eess.sp']","  Few-shot learning is a type of classification through which predictions aremade based on a limited number of samples for each class. This type ofclassification is sometimes referred to as a meta-learning problem, in whichthe model learns how to learn to identify rare cases. We seek to extractinformation from five exemplar vocalisations of mammals or birds and detect andclassify these sounds in field recordings [2]. This task was provided in theDetection and Classification of Acoustic Scenes and Events (DCASE) Challenge of2021. Rather than utilize deep learning, as is most commonly done, weformulated a novel solution using only machine learning methods. Various modelswere tested, and it was found that logistic regression outperformed both linearregression and template matching. However, all of these methods over-predictedthe number of events in the field recordings."
Retrieval-Augmented Generative Question Answering for Event Argument  Extraction,"['Xinya Du', 'Heng Ji']",http://arxiv.org/pdf/2211.07067v1.pdf,2022-11-14,['cs.cl'],"  Event argument extraction has long been studied as a sequential predictionproblem with extractive-based methods, tackling each argument in isolation.Although recent work proposes generation-based methods to capturecross-argument dependency, they require generating and post-processing acomplicated target sequence (template). Motivated by these observations andrecent pretrained language models' capabilities of learning fromdemonstrations. We propose a retrieval-augmented generative QA model (R-GQA)for event argument extraction. It retrieves the most similar QA pair andaugments it as prompt to the current example's context, then decodes thearguments as answers. Our approach outperforms substantially prior methodsacross various settings (i.e. fully supervised, domain transfer, and fewshotlearning). Finally, we propose a clustering-based sampling strategy (JointEnc)and conduct a thorough analysis of how different strategies influence thefew-shot learning performance. The implementations are available at https://github.com/xinyadu/RGQA"
AdaptKeyBERT: An Attention-Based approach towards Few-Shot & Zero-Shot  Domain Adaptation of KeyBERT,"['Aman Priyanshu', 'Supriti Vijay']",http://arxiv.org/pdf/2211.07499v2.pdf,2022-11-14,"['cs.cl', 'cs.lg']","  Keyword extraction has been an important topic for modern natural languageprocessing. With its applications ranging from ontology generation, factverification in summarized text, and recommendation systems. While it has hadsignificant data-intensive applications, it is often hampered when the data setis small. Downstream training for keyword extractors is a lengthy process andrequires a significant amount of data. Recently, Few-shot Learning (FSL) andZero-Shot Learning (ZSL) have been proposed to tackle this problem. Therefore,we propose AdaptKeyBERT, a pipeline for training keyword extractors with LLMbases by incorporating the concept of regularized attention into a pre-trainingphase for downstream domain adaptation. As we believe our work has implicationsto be utilized in the pipeline of FSL/ZSL and keyword extraction, weopen-source our code as well as provide the fine-tuning library of the samename AdaptKeyBERT at https://github.com/AmanPriyanshu/AdaptKeyBERT."
ProtSi: Prototypical Siamese Network with Data Augmentation for Few-Shot  Subjective Answer Evaluation,"['Yining Lu', 'Jingxi Qiu', 'Gaurav Gupta']",http://arxiv.org/pdf/2211.09855v1.pdf,2022-11-17,['cs.cl'],"  Subjective answer evaluation is a time-consuming and tedious task, and thequality of the evaluation is heavily influenced by a variety of subjectivepersonal characteristics. Instead, machine evaluation can effectively assisteducators in saving time while also ensuring that evaluations are fair andrealistic. However, most existing methods using regular machine learning andnatural language processing techniques are generally hampered by a lack ofannotated answers and poor model interpretability, making them unsuitable forreal-world use. To solve these challenges, we propose ProtSi Network, a uniquesemi-supervised architecture that for the first time uses few-shot learning tosubjective answer evaluation. To evaluate students' answers by similarityprototypes, ProtSi Network simulates the natural process of evaluator scoringanswers by combining Siamese Network which consists of BERT and encoder layerswith Prototypical Network. We employed an unsupervised diverse paraphrasingmodel ProtAugment, in order to prevent overfitting for effective few-shot textclassification. By integrating contrastive learning, the discriminative textissue can be mitigated. Experiments on the Kaggle Short Scoring Datasetdemonstrate that the ProtSi Network outperforms the most recent baseline modelsin terms of accuracy and quadratic weighted kappa."
Self-Transriber: Few-shot Lyrics Transcription with Self-training,"['Xiaoxue Gao', 'Xianghu Yue', 'Haizhou Li']",http://arxiv.org/pdf/2211.10152v2.pdf,2022-11-18,"['eess.as', 'cs.sd']","  The current lyrics transcription approaches heavily rely on supervisedlearning with labeled data, but such data are scarce and manual labeling ofsinging is expensive. How to benefit from unlabeled data and alleviate limiteddata problem have not been explored for lyrics transcription. We propose thefirst semi-supervised lyrics transcription paradigm, Self-Transcriber, byleveraging on unlabeled data using self-training with noisy studentaugmentation. We attempt to demonstrate the possibility of lyrics transcriptionwith a few amount of labeled data. Self-Transcriber generates pseudo labels ofthe unlabeled singing using teacher model, and augments pseudo-labels to thelabeled data for student model update with both self-training and supervisedtraining losses. This work closes the gap between supervised andsemi-supervised learning as well as opens doors for few-shot learning of lyricstranscription. Our experiments show that our approach using only 12.7 hours oflabeled data achieves competitive performance compared with the supervisedapproaches trained on 149.1 hours of labeled data for lyrics transcription."
TEMPERA: Test-Time Prompting via Reinforcement Learning,"['Tianjun Zhang', 'Xuezhi Wang', 'Denny Zhou', 'Dale Schuurmans', 'Joseph E. Gonzalez']",http://arxiv.org/pdf/2211.11890v1.pdf,2022-11-21,"['cs.cl', 'cs.ai']","  Careful prompt design is critical to the use of large language models inzero-shot or few-shot learning. As a consequence, there is a growing interestin automated methods to design optimal prompts. In this work, we proposeTest-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast toprior prompt generation methods, TEMPERA can efficiently leverage priorknowledge, is adaptive to different queries and provides an interpretableprompt for every query. To achieve this, we design a novel action space thatallows flexible editing of the initial prompts covering a wide set ofcommonly-used components like instructions, few-shot exemplars, andverbalizers. The proposed method achieves significant gains compared withrecent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across avariety of tasks including sentiment analysis, topic classification, naturallanguage inference, and reading comprehension. Our method achieves 5.33x onaverage improvement in sample efficiency when compared to the traditionalfine-tuning methods."
Towards Practical Few-shot Federated NLP,"['Dongqi Cai', 'Yaozong Wu', 'Haitao Yuan', 'Shangguang Wang', 'Felix Xiaozhu Lin', 'Mengwei Xu']",http://arxiv.org/pdf/2212.00192v2.pdf,2022-12-01,"['cs.cl', 'cs.lg']","  Transformer-based pre-trained models have emerged as the predominant solutionfor natural language processing (NLP). Fine-tuning such pre-trained models fordownstream tasks often requires a considerable amount of labeled private data.In practice, private data is often distributed across heterogeneous mobiledevices and may be prohibited from being uploaded. Moreover, well-curatedlabeled data is often scarce, presenting an additional challenge. To addressthese challenges, we first introduce a data generator for federated few-shotlearning tasks, which encompasses the quantity and skewness of scarce labeleddata in a realistic setting. Subsequently, we propose AUG-FedPrompt, aprompt-based federated learning system that exploits abundant unlabeled datafor data augmentation. Our experiments indicate that AUG-FedPrompt can performon par with full-set fine-tuning with a limited amount of labeled data.However, such competitive performance comes at a significant system cost."
Few-Shot Nested Named Entity Recognition,"['Hong Ming', 'Jiaoyun Yang', 'Lili Jiang', 'Yan Pan', 'Ning An']",http://arxiv.org/pdf/2212.00953v1.pdf,2022-12-02,"['cs.cl', 'cs.ai']","  While Named Entity Recognition (NER) is a widely studied task, makinginferences of entities with only a few labeled data has been challenging,especially for entities with nested structures. Unlike flat entities, entitiesand their nested entities are more likely to have similar semantic featurerepresentations, drastically increasing difficulties in classifying differententity categories in the few-shot setting. Although prior work has brieflydiscussed nested structures in the context of few-shot learning, to our bestknowledge, this paper is the first one specifically dedicated to studying thefew-shot nested NER task. Leveraging contextual dependency to distinguishnested entities, we propose a Biaffine-based Contrastive Learning (BCL)framework. We first design a Biaffine span representation module for learningthe contextual span dependency representation for each entity span rather thanonly learning its semantic representation. We then merge these tworepresentations by the residual connection to distinguish nested entities.Finally, we build a contrastive learning framework to adjust the representationdistribution for larger margin boundaries and more generalized domain transferlearning ability. We conducted experimental studies on three English, German,and Russian nested NER datasets. The results show that the BCL outperformedthree baseline models on the 1-shot and 5-shot tasks in terms of F1 score."
Improving Few-Shot Performance of Language Models via Nearest Neighbor  Calibration,"['Feng Nie', 'Meixi Chen', 'Zhirui Zhang', 'Xu Cheng']",http://arxiv.org/pdf/2212.02216v1.pdf,2022-12-05,['cs.cl'],"  Pre-trained language models (PLMs) have exhibited remarkable few-shotlearning capabilities when provided a few examples in a natural language promptas demonstrations of test instances, i.e., in-context learning. However, theperformance of in-context learning is susceptible to the choice of promptformat, training examples and the ordering of the training examples. In thispaper, we propose a novel nearest-neighbor calibration framework for in-contextlearning to ease this issue. It is inspired by a phenomenon that the in-contextlearning paradigm produces incorrect labels when inferring training instances,which provides a useful supervised signal to calibrate predictions. Thus, ourmethod directly augments the predictions with a $k$-nearest-neighbor ($k$NN)classifier over a datastore of cached few-shot instance representationsobtained by PLMs and their corresponding labels. Then adaptive neighborselection and feature regularization modules are introduced to make full use ofa few support instances to reduce the $k$NN retrieval noise. Experiments onvarious few-shot text classification tasks demonstrate that our methodsignificantly improves in-context learning, while even achieving comparableperformance with state-of-the-art tuning-based approaches in some sentimentanalysis tasks."
JamPatoisNLI: A Jamaican Patois Natural Language Inference Dataset,"['Ruth-Ann Armstrong', 'John Hewitt', 'Christopher Manning']",http://arxiv.org/pdf/2212.03419v1.pdf,2022-12-07,"['cs.cl', 'cs.lg', 'i.2.7']","  JamPatoisNLI provides the first dataset for natural language inference in acreole language, Jamaican Patois. Many of the most-spoken low-resourcelanguages are creoles. These languages commonly have a lexicon derived from amajor world language and a distinctive grammar reflecting the languages of theoriginal speakers and the process of language birth by creolization. This givesthem a distinctive place in exploring the effectiveness of transfer from largemonolingual or multilingual pretrained models. While our work, along withprevious work, shows that transfer from these models to low-resource languagesthat are unrelated to languages in their training set is not very effective, wewould expect stronger results from transfer to creoles. Indeed, our experimentsshow considerably better results from few-shot learning of JamPatoisNLI thanfor such unrelated languages, and help us begin to understand how the uniquerelationship between creoles and their high-resource base languages affectcross-lingual transfer. JamPatoisNLI, which consists of naturally-occurringpremises and expert-written hypotheses, is a step towards steering researchinto a traditionally underserved language and a useful benchmark forunderstanding cross-lingual NLP."
Learn to Explore: on Bootstrapping Interactive Data Exploration with  Meta-learning,"['Yukun Cao', 'Xike Xie', 'Kexin Huang']",http://arxiv.org/pdf/2212.03423v4.pdf,2022-12-07,"['cs.db', 'cs.ai']","  Interactive data exploration (IDE) is an effective way of comprehending bigdata, whose volume and complexity are beyond human abilities. The main goal ofIDE is to discover user interest regions from a database through multi-roundsof user labelling. Existing IDEs adopt active-learning framework, where usersiteratively discriminate or label the interestingness of selected tuples. Theprocess of data exploration can be viewed as the process of training aclassifier, which determines whether a database tuple is interesting to a user.An efficient exploration thus takes very few iterations of user labelling toreach the data region of interest. In this work, we consider the dataexploration as the process of few-shot learning, where the classifier islearned with only a few training examples, or exploration iterations. To thisend, we propose a learning-to-explore framework, based on meta-learning, whichlearns how to learn a classifier with automatically generated meta-tasks, sothat the exploration process can be much shortened. Extensive experiments onreal datasets show that our proposal outperforms existing explore-by-examplesolutions in terms of accuracy and efficiency."
Few-shot Medical Image Segmentation with Cycle-resemblance Attention,"['Hao Ding', 'Changchang Sun', 'Hao Tang', 'Dawen Cai', 'Yan Yan']",http://arxiv.org/pdf/2212.03967v1.pdf,2022-12-07,['cs.cv'],"  Recently, due to the increasing requirements of medical imaging applicationsand the professional requirements of annotating medical images, few-shotlearning has gained increasing attention in the medical image semanticsegmentation field. To perform segmentation with limited number of labeledmedical images, most existing studies use Proto-typical Networks (PN) and haveobtained compelling success. However, these approaches overlook the query imagefeatures extracted from the proposed representation network, failing topreserving the spatial connection between query and support images. In thispaper, we propose a novel self-supervised few-shot medical image segmentationnetwork and introduce a novel Cycle-Resemblance Attention (CRA) module to fullyleverage the pixel-wise relation between query and support medical images.Notably, we first line up multiple attention blocks to refine more abundantrelation information. Then, we present CRAPNet by integrating the CRA modulewith a classic prototype network, where pixel-wise relations between query andsupport features are well recaptured for segmentation. Extensive experiments ontwo different medical image datasets, e.g., abdomen MRI and abdomen CT,demonstrate the superiority of our model over existing state-of-the-artmethods."
Demystifying Prompts in Language Models via Perplexity Estimation,"['Hila Gonen', 'Srini Iyer', 'Terra Blevins', 'Noah A. Smith', 'Luke Zettlemoyer']",http://arxiv.org/pdf/2212.04037v1.pdf,2022-12-08,['cs.cl'],"  Language models can be prompted to perform a wide variety of zero- andfew-shot learning problems. However, performance varies significantly with thechoice of prompt, and we do not yet understand why this happens or how to pickthe best prompts. In this work, we analyze the factors that contribute to thisvariance and establish a new empirical hypothesis: the performance of a promptis coupled with the extent to which the model is familiar with the language itcontains. Over a wide range of tasks, we show that the lower the perplexity ofthe prompt is, the better the prompt is able to perform the task. As a result,we devise a method for creating prompts: (1) automatically extend a small seedset of manually written prompts by paraphrasing using GPT3 and backtranslationand (2) choose the lowest perplexity prompts to get significant gains inperformance."
Technical Report -- Competition Solution for Prompt Tuning using  Pretrained Language Model,"['Jiang-Long Song', 'Wu-He Zou', 'Feng Li', 'Xiao-Lei Qin', 'Wei-Dong Zhang']",http://arxiv.org/pdf/2212.06369v3.pdf,2022-12-13,['cs.cl'],"  Prompt tuning recently becomes a hot-spot in the applications of largepretrained language models on specific downstream tasks. Regarding the LanguageModel as a Service (LMaaS), black-box tuning using derivative-free optimization(DFO) provides a novel approach to expand the practical scenarios of pretrainedmodels and enrich the researches of few-shot learning. In this report, wepresent our solution in this competition that is based on the LMaaS scenario.Our solution consists of several modifications to BBTv2, including multiplelabel words, selection of P0, rolling update strategy, multi-task loss from MLPclassifier, and finally using the ensemble method to further improvegeneralization ability. We also shared some strategies that we tried but didn'tuse in the final submission for further discussion. In the end we raised aquestion about the SNLI dataset and the impact on the results, as well as ourconcerns about the competition."
A Statistical Model for Predicting Generalization in Few-Shot  Classification,"['Yassir Bendou', 'Vincent Gripon', 'Bastien Pasdeloup', 'Lukas Mauch', 'Stefan Uhlich', 'Fabien Cardinaux', 'Ghouthi Boukli Hacene', 'Javier Alonso Garcia']",http://arxiv.org/pdf/2212.06461v2.pdf,2022-12-13,"['cs.lg', 'cs.ai', 'cs.cv', 'stat.ml']","  The estimation of the generalization error of classifiers often relies on avalidation set. Such a set is hardly available in few-shot learning scenarios,a highly disregarded shortcoming in the field. In these scenarios, it is commonto rely on features extracted from pre-trained neural networks combined withdistance-based classifiers such as nearest class mean. In this work, weintroduce a Gaussian model of the feature distribution. By estimating theparameters of this model, we are able to predict the generalization error onnew classification tasks with few samples. We observe that accurate distanceestimates between class-conditional densities are the key to accurate estimatesof the generalization performance. Therefore, we propose an unbiased estimatorfor these distances and integrate it in our numerical analysis. We empiricallyshow that our approach outperforms alternatives such as the leave-one-outcross-validation strategy."
Localized Latent Updates for Fine-Tuning Vision-Language Models,"['Moritz Ibing', 'Isaak Lim', 'Leif Kobbelt']",http://arxiv.org/pdf/2212.06556v1.pdf,2022-12-13,"['cs.cv', 'cs.cl', 'cs.lg']","  Although massive pre-trained vision-language models like CLIP show impressivegeneralization capabilities for many tasks, still it often remains necessary tofine-tune them for improved performance on specific datasets. When doing so, itis desirable that updating the model is fast and that the model does not loseits capabilities on data outside of the dataset, as is often the case withclassical fine-tuning approaches. In this work we suggest a lightweightadapter, that only updates the models predictions close to seen datapoints. Wedemonstrate the effectiveness and speed of this relatively simple approach inthe context of few-shot learning, where our results both on classes seen andunseen during training are comparable with or improve on the state of the art."
ALERT: Adapting Language Models to Reasoning Tasks,"['Ping Yu', 'Tianlu Wang', 'Olga Golovneva', 'Badr AlKhamissi', 'Siddharth Verma', 'Zhijing Jin', 'Gargi Ghosh', 'Mona Diab', 'Asli Celikyilmaz']",http://arxiv.org/pdf/2212.08286v2.pdf,2022-12-16,['cs.cl'],"  Current large language models can perform reasonably well on complex tasksthat require step-by-step reasoning with few-shot learning. Are these modelsapplying reasoning skills they have learnt during pre-training and reasonoutside of their training context, or are they simply memorizing their trainingcorpus at finer granularity and have learnt to better understand their context?To tease apart these possibilities, we introduce ALERT, a benchmark and suiteof analyses for assessing language models' reasoning ability comparingpre-trained and finetuned models on complex tasks that require reasoning skillsto solve. ALERT provides a test bed to asses any language model on fine-grainedreasoning skills, which spans over 20 datasets and covers 10 differentreasoning skills. We leverage ALERT to further investigate the role offinetuning. With extensive empirical analysis we find that language modelslearn more reasoning skills such as textual entailment, abductive reasoning,and analogical reasoning during finetuning stage compared to pretraining state.We also find that when language models are finetuned they tend to overfit tothe prompt template, which hurts the robustness of models causinggeneralization problems."
Check-worthy Claim Detection across Topics for Automated Fact-checking,"['Amani S. Abumansour', 'Arkaitz Zubiaga']",http://arxiv.org/pdf/2212.08514v1.pdf,2022-12-16,['cs.cl'],"  An important component of an automated fact-checking system is the claimcheck-worthiness detection system, which ranks sentences by prioritising thembased on their need to be checked. Despite a body of research tackling thetask, previous research has overlooked the challenging nature of identifyingcheck-worthy claims across different topics. In this paper, we assess andquantify the challenge of detecting check-worthy claims for new, unseen topics.After highlighting the problem, we propose the AraCWA model to mitigate theperformance deterioration when detecting check-worthy claims across topics. TheAraCWA model enables boosting the performance for new topics by incorporatingtwo components for few-shot learning and data augmentation. Using a publiclyavailable dataset of Arabic tweets consisting of 14 different topics, wedemonstrate that our proposed data augmentation strategy achieves substantialimprovements across topics overall, where the extent of the improvement variesacross topics. Further, we analyse the semantic similarities between topics,suggesting that the similarity metric could be used as a proxy to determine thedifficulty level of an unseen topic prior to undertaking the task of labellingthe underlying sentences."
Learning from Taxonomy: Multi-label Few-Shot Classification for Everyday  Sound Recognition,"['Jinhua Liang', 'Huy Phan', 'Emmanouil Benetos']",http://arxiv.org/pdf/2212.08952v1.pdf,2022-12-17,"['cs.sd', 'eess.as']","  Everyday sound recognition aims to infer types of sound events in audiostreams. While many works succeeded in training models with high performance ina fully-supervised manner, they are still restricted to the demand of largequantities of labelled data and the range of predefined classes. To overcomethese drawbacks, this work firstly curates a new database named FSD-FS formulti-label few-shot audio classification. It then explores how to incorporateaudio taxonomy in few-shot learning. Specifically, this work proposeslabel-dependent prototypical networks (LaD-protonet) to exploit parent-childrenrelationships between labels. Plus, it applies taxonomy-aware label smoothingtechniques to boost model performance. Experiments demonstrate thatLaD-protonet outperforms original prototypical networks as well as otherstate-of-the-art methods. Moreover, its performance can be further boosted whencombined with taxonomy-aware label smoothing."
Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations,"['Xinxi Lyu', 'Sewon Min', 'Iz Beltagy', 'Luke Zettlemoyer', 'Hannaneh Hajishirzi']",http://arxiv.org/pdf/2212.09865v2.pdf,2022-12-19,"['cs.cl', 'cs.ai']","  Although large language models can be prompted for both zero- and few-shotlearning, performance drops significantly when no demonstrations are available.In this paper, we introduce Z-ICL, a new zero-shot method that closes the gapby constructing pseudo-demonstrations for a given test input using a raw textcorpus. Concretely, pseudo-demonstrations are constructed by (1) finding thenearest neighbors to the test input from the corpus and pairing them withrandom task labels, and (2) applying a set of techniques to reduce the amountof direct copying the model does from the resulting demonstrations. Evaluationon nine classification datasets shows that Z-ICL outperforms previous zero-shotmethods by a significant margin, and is on par with in-context learning withlabeled training data in the few-shot setting. Overall, Z-ICL provides asignificantly higher estimate of the zero-shot performance levels of a model,and supports future efforts to develop better pseudo-demonstrations thatfurther improve zero-shot results."
OpineSum: Entailment-based self-training for abstractive opinion  summarization,"['Annie Louis', 'Joshua Maynez']",http://arxiv.org/pdf/2212.10791v1.pdf,2022-12-21,"['cs.cl', 'cs.ai']","  A typical product or place often has hundreds of reviews, and summarizationof these texts is an important and challenging problem. Recent progress onabstractive summarization in domains such as news has been driven by supervisedsystems trained on hundreds of thousands of news articles paired withhuman-written summaries. However for opinion texts, such large scale datasetsare rarely available. Unsupervised methods, self-training, and few-shotlearning approaches bridge that gap. In this work, we present a novelself-training approach, OpineSum, for abstractive opinion summarization. Thesummaries in this approach are built using a novel application of textualentailment and capture the consensus of opinions across the various reviews foran item. This method can be used to obtain silver-standard summaries on a largescale and train both unsupervised and few-shot abstractive summarizationsystems. OpineSum achieves state-of-the-art performance in both settings."
P3DC-Shot: Prior-Driven Discrete Data Calibration for Nearest-Neighbor  Few-Shot Classification,"['Shuangmei Wang', 'Rui Ma', 'Tieru Wu', 'Yang Cao']",http://arxiv.org/pdf/2301.00740v1.pdf,2023-01-02,['cs.cv'],"  Nearest-Neighbor (NN) classification has been proven as a simple andeffective approach for few-shot learning. The query data can be classifiedefficiently by finding the nearest support class based on features extracted bypretrained deep models. However, NN-based methods are sensitive to the datadistribution and may produce false prediction if the samples in the support sethappen to lie around the distribution boundary of different classes. To solvethis issue, we present P3DC-Shot, an improved nearest-neighbor based few-shotclassification method empowered by prior-driven data calibration. Inspired bythe distribution calibration technique which utilizes the distribution orstatistics of the base classes to calibrate the data for few-shot tasks, wepropose a novel discrete data calibration operation which is more suitable forNN-based few-shot classification. Specifically, we treat the prototypesrepresenting each base class as priors and calibrate each support data based onits similarity to different base prototypes. Then, we perform NN classificationusing these discretely calibrated support data. Results from extensiveexperiments on various datasets show our efficient non-learning based methodcan outperform or at least comparable to SOTA methods which need additionallearning steps."
A Survey On Few-shot Knowledge Graph Completion with Structural and  Commonsense Knowledge,"['Haodi Ma', 'Daisy Zhe Wang']",http://arxiv.org/pdf/2301.01172v1.pdf,2023-01-03,"['cs.cl', 'cs.ai', 'cs.lg']","  Knowledge graphs (KG) have served as the key component of various naturallanguage processing applications. Commonsense knowledge graphs (CKG) are aspecial type of KG, where entities and relations are composed of free-formtext. However, previous works in KG completion and CKG completion suffer fromlong-tail relations and newly-added relations which do not have many knowtriples for training. In light of this, few-shot KG completion (FKGC), whichrequires the strengths of graph representation learning and few-shot learning,has been proposed to challenge the problem of limited annotated data. In thispaper, we comprehensively survey previous attempts on such tasks in the form ofa series of methods and applications. Specifically, we first introduce FKGCchallenges, commonly used KGs, and CKGs. Then we systematically categorize andsummarize existing works in terms of the type of KGs and the methods. Finally,we present applications of FKGC models on prediction tasks in different areasand share our thoughts on future research directions of FKGC."
Task Weighting in Meta-learning with Trajectory Optimisation,"['Cuong Nguyen', 'Thanh-Toan Do', 'Gustavo Carneiro']",http://arxiv.org/pdf/2301.01400v1.pdf,2023-01-04,"['cs.lg', 'cs.ai']","  Developing meta-learning algorithms that are un-biased toward a subset oftraining tasks often requires hand-designed criteria to weight tasks,potentially resulting in sub-optimal solutions. In this paper, we introduce anew principled and fully-automated task-weighting algorithm for meta-learningmethods. By considering the weights of tasks within the same mini-batch as anaction, and the meta-parameter of interest as the system state, we cast thetask-weighting meta-learning problem to a trajectory optimisation and employthe iterative linear quadratic regulator to determine the optimal action orweights of tasks. We theoretically show that the proposed algorithm convergesto an $\epsilon_{0}$-stationary point, and empirically demonstrate that theproposed approach out-performs common hand-engineering weighting methods in twofew-shot learning benchmarks."
Continual Few-Shot Learning Using HyperTransformers,"['Max Vladymyrov', 'Andrey Zhmoginov', 'Mark Sandler']",http://arxiv.org/pdf/2301.04584v2.pdf,2023-01-11,"['cs.lg', 'cs.cv']","  We focus on the problem of learning without forgetting from multiple tasksarriving sequentially, where each task is defined using a few-shot episode ofnovel or already seen classes. We approach this problem using the recentlypublished HyperTransformer (HT), a Transformer-based hypernetwork thatgenerates specialized task-specific CNN weights directly from the support set.In order to learn from a continual sequence of tasks, we propose to recursivelyre-use the generated weights as input to the HT for the next task. This way,the generated CNN weights themselves act as a representation of previouslylearned tasks, and the HT is trained to update these weights so that the newtask can be learned without forgetting past tasks. This approach is differentfrom most continual learning algorithms that typically rely on using replaybuffers, weight regularization or task-dependent architectural changes. Wedemonstrate that our proposed Continual HyperTransformer method equipped with aprototypical loss is capable of learning and retaining knowledge about pasttasks for a variety of scenarios, including learning from mini-batches, andtask-incremental and class-incremental learning scenarios."
ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition,"['Mathias Zinnen', 'Prathmesh Madhu', 'Ronak Kosti', 'Peter Bell', 'Andreas Maier', 'Vincent Christlein']",http://arxiv.org/pdf/2301.09878v1.pdf,2023-01-24,['cs.cv'],"  The Odeuropa Challenge on Olfactory Object Recognition aims to foster thedevelopment of object detection in the visual arts and to promote an olfactoryperspective on digital heritage. Object detection in historical artworks isparticularly challenging due to varying styles and artistic periods. Moreover,the task is complicated due to the particularity and historical variance ofpredefined target objects, which exhibit a large intra-class variance, and thelong tail distribution of the dataset labels, with some objects having onlyvery few training examples. These challenges should encourage participants tocreate innovative approaches using domain adaptation or few-shot learning. Weprovide a dataset of 2647 artworks annotated with 20 120 tightly fit boundingboxes that are split into a training and validation set (public). A test setcontaining 1140 artworks and 15 480 annotations is kept private for thechallenge evaluation."
FewShotTextGCN: K-hop neighborhood regularization for few-shot learning  on graphs,"['Niels van der Heijden', 'Ekaterina Shutova', 'Helen Yannakoudakis']",http://arxiv.org/pdf/2301.10481v2.pdf,2023-01-25,"['cs.cl', 'cs.lg']","  We present FewShotTextGCN, a novel method designed to effectively utilize theproperties of word-document graphs for improved learning in low-resourcesettings. We introduce K-hop Neighbourhood Regularization, a regularizer forheterogeneous graphs, and show that it stabilizes and improves learning whenonly a few training samples are available. We furthermore propose asimplification in the graph-construction method, which results in a graph thatis $\sim$7 times less dense and yields better performance in little-resourcesettings while performing on par with the state of the art in high-resourcesettings. Finally, we introduce a new variant of Adaptive Pseudo-Labelingtailored for word-document graphs. When using as little as 20 samples fortraining, we outperform a strong TextGCN baseline with 17% in absolute accuracyon average over eight languages. We demonstrate that our method can be appliedto document classification without any language model pretraining on a widerange of typologically diverse languages while performing on par with largepretrained language models."
Hebbian and Gradient-based Plasticity Enables Robust Memory and Rapid  Learning in RNNs,"['Yu Duan', 'Zhongfan Jia', 'Qian Li', 'Yi Zhong', 'Kaisheng Ma']",http://arxiv.org/pdf/2302.03235v1.pdf,2023-02-07,"['cs.ne', 'cs.ai']","  Rapidly learning from ongoing experiences and remembering past events with aflexible memory system are two core capacities of biological intelligence.While the underlying neural mechanisms are not fully understood, variousevidence supports that synaptic plasticity plays a critical role in memoryformation and fast learning. Inspired by these results, we equip RecurrentNeural Networks (RNNs) with plasticity rules to enable them to adapt theirparameters according to ongoing experiences. In addition to the traditionallocal Hebbian plasticity, we propose a global, gradient-based plasticity rule,which allows the model to evolve towards its self-determined target. Our modelsshow promising results on sequential and associative memory tasks, illustratingtheir ability to robustly form and retain memories. In the meantime, thesemodels can cope with many challenging few-shot learning problems. Comparingdifferent plasticity rules under the same framework shows that Hebbianplasticity is well-suited for several memory and associative learning tasks;however, it is outperformed by gradient-based plasticity on few-shot regressiontasks which require the model to infer the underlying mapping. Code isavailable at https://github.com/yuvenduan/PlasticRNNs."
AutoWS: Automated Weak Supervision Framework for Text Classification,"['Abhinav Bohra', 'Huy Nguyen', 'Devashish Khatwani']",http://arxiv.org/pdf/2302.03297v1.pdf,2023-02-07,['cs.cl'],"  Creating large, good quality labeled data has become one of the majorbottlenecks for developing machine learning applications. Multiple techniqueshave been developed to either decrease the dependence of labeled data(zero/few-shot learning, weak supervision) or to improve the efficiency oflabeling process (active learning). Among those, Weak Supervision has beenshown to reduce labeling costs by employing hand crafted labeling functionsdesigned by domain experts. We propose AutoWS -- a novel framework forincreasing the efficiency of weak supervision process while decreasing thedependency on domain experts. Our method requires a small set of labeledexamples per label class and automatically creates a set of labeling functionsto assign noisy labels to numerous unlabeled data. Noisy labels can then beaggregated into probabilistic labels used by a downstream discriminativeclassifier. Our framework is fully automatic and requires no hyper-parameterspecification by users. We compare our approach with different state-of-the-artwork on weak supervision and noisy training. Experimental results show that ourmethod outperforms competitive baselines."
Meta-Learning Siamese Network for Few-Shot Text Classification,"['Chengcheng Han', 'Yuhe Wang', 'Yingnan Fu', 'Xiang Li', 'Minghui Qiu', 'Ming Gao', 'Aoying Zhou']",http://arxiv.org/pdf/2302.03507v2.pdf,2023-02-05,"['cs.cl', 'cs.ai']","  Few-shot learning has been used to tackle the problem of label scarcity intext classification, of which meta-learning based methods have shown to beeffective, such as the prototypical networks (PROTO). Despite the success ofPROTO, there still exist three main problems: (1) ignore the randomness of thesampled support sets when computing prototype vectors; (2) disregard theimportance of labeled samples; (3) construct meta-tasks in a purely randommanner. In this paper, we propose a Meta-Learning Siamese Network, namely,Meta-SN, to address these issues. Specifically, instead of computing prototypevectors from the sampled support sets, Meta-SN utilizes external knowledge(e.g. class names and descriptive texts) for class labels, which is encoded asthe low-dimensional embeddings of prototype vectors. In addition, Meta-SNpresents a novel sampling strategy for constructing meta-tasks, which giveshigher sampling probabilities to hard-to-classify samples. Extensiveexperiments are conducted on six benchmark datasets to show the clearsuperiority of Meta-SN over other state-of-the-art models. For reproducibility,all the datasets and codes are provided at https://github.com/hccngu/Meta-SN."
Distillation of encoder-decoder transformers for sequence labelling,"['Marco Farina', 'Duccio Pappadopulo', 'Anant Gupta', 'Leslie Huang', 'Ozan İrsoy', 'Thamar Solorio']",http://arxiv.org/pdf/2302.05454v1.pdf,2023-02-10,"['cs.cl', 'cs.ir']","  Driven by encouraging results on a wide range of tasks, the field of NLP isexperiencing an accelerated race to develop bigger language models. This racefor bigger models has also underscored the need to continue the pursuit ofpractical distillation approaches that can leverage the knowledge acquired bythese big models in a compute-efficient manner. Having this goal in mind, webuild on recent work to propose a hallucination-free framework for sequencetagging that is especially suited for distillation. We show empirical resultsof new state-of-the-art performance across multiple sequence labelling datasetsand validate the usefulness of this framework for distilling a large model in afew-shot learning scenario."
Learning to Initialize: Can Meta Learning Improve Cross-task  Generalization in Prompt Tuning?,"['Chengwei Qin', 'Qian Li', 'Ruochen Zhao', 'Shafiq Joty']",http://arxiv.org/pdf/2302.08143v3.pdf,2023-02-16,"['cs.cl', 'cs.ai']","  Prompt tuning (PT) which only tunes the embeddings of an additional sequenceof tokens per task, keeping the pre-trained language model (PLM) frozen, hasshown remarkable performance in few-shot learning. Despite this, PT has beenshown to rely heavily on good initialization of the prompt embeddings. In thiswork, we study meta prompt tuning (MPT) to systematically explore howmeta-learning can help improve (if it can) cross-task generalization in PTthrough learning to initialize the prompt embeddings from other relevant tasks.We empirically analyze a representative set of meta learning algorithms in awide range of adaptation settings with different source/target taskconfigurations on a large set of few-shot tasks. With extensive experiments andanalysis, we demonstrate the effectiveness of MPT. We find the improvement tobe significant particularly on classification tasks. For other kinds of taskssuch as question answering, we observe that while MPT can outperform PT in mostcases, it does not always outperform multi-task learning. We further provide anin-depth analysis from the perspective of task similarity."
Few-shot 3D LiDAR Semantic Segmentation for Autonomous Driving,"['Jilin Mei', 'Junbao Zhou', 'Yu Hu']",http://arxiv.org/pdf/2302.08785v2.pdf,2023-02-17,"['cs.ro', 'cs.cv']","  In autonomous driving, the novel objects and lack of annotations challengethe traditional 3D LiDAR semantic segmentation based on deep learning. Few-shotlearning is a feasible way to solve these issues. However, currently few-shotsemantic segmentation methods focus on camera data, and most of them onlypredict the novel classes without considering the base classes. This settingcannot be directly applied to autonomous driving due to safety concerns. Thus,we propose a few-shot 3D LiDAR semantic segmentation method that predicts bothnovel classes and base classes simultaneously. Our method tries to solve thebackground ambiguity problem in generalized few-shot semantic segmentation. Wefirst review the original cross-entropy and knowledge distillation losses, thenpropose a new loss function that incorporates the background information toachieve 3D LiDAR few-shot semantic segmentation. Extensive experiments onSemanticKITTI demonstrate the effectiveness of our method."
Scalable Prompt Generation for Semi-supervised Learning with Language  Models,"['Yuhang Zhou', 'Suraj Maharjan', 'Beiye Liu']",http://arxiv.org/pdf/2302.09236v1.pdf,2023-02-18,"['cs.cl', 'cs.ai']","  Prompt-based learning methods in semi-supervised learning (SSL) settings havebeen shown to be effective on multiple natural language understanding (NLU)datasets and tasks in the literature. However, manually designing multipleprompts and verbalizers requires domain knowledge and human effort, making itdifficult and expensive to scale across different datasets. In this paper, wepropose two methods to automatically design multiple prompts and integrateautomatic verbalizer in SSL settings without sacrificing performance. The firstmethod uses various demonstration examples with learnable continuous prompttokens to create diverse prompt models. The second method uses a varying numberof soft prompt tokens to encourage language models to learn different prompts.For the verbalizer, we use the prototypical verbalizer to replace the manualone. In summary, we obtained the best average accuracy of 73.2% (a relativeimprovement of 2.52% over even the previous state-of-the-art SSL method withmanual prompts and verbalizers) in different few-shot learning settings."
Neural Attention Memory,"['Hyoungwook Nam', 'Seung Byum Seo']",http://arxiv.org/pdf/2302.09422v2.pdf,2023-02-18,['cs.lg'],"  We propose a novel perspective of the attention mechanism by reinventing itas a memory architecture for neural networks, namely Neural Attention Memory(NAM). NAM is a memory structure that is both readable and writable viadifferentiable linear algebra operations. We explore three use cases of NAM:memory-augmented neural network (MANN), few-shot learning, and efficientlong-range attention. First, we design two NAM-based MANNs of Long Short-termMemory (LSAM) and NAM Turing Machine (NAM-TM) that show better computationalpowers in algorithmic zero-shot generalization tasks compared to otherbaselines such as differentiable neural computer (DNC). Next, we apply NAM tothe N-way K-shot learning task and show that it is more effective at reducingfalse positives compared to the baseline cosine classifier. Finally, weimplement an efficient Transformer with NAM and evaluate it with long-rangearena tasks to show that NAM can be an efficient and effective alternative forscaled dot-product attention."
DGP-Net: Dense Graph Prototype Network for Few-Shot SAR Target  Recognition,"['Xiangyu Zhou', 'Qianru Wei', 'Yuhui Zhang']",http://arxiv.org/pdf/2302.09584v1.pdf,2023-02-19,['cs.cv'],"  The inevitable feature deviation of synthetic aperture radar (SAR) image dueto the special imaging principle (depression angle variation) leads to poorrecognition accuracy, especially in few-shot learning (FSL). To deal with thisproblem, we propose a dense graph prototype network (DGP-Net) to eliminate thefeature deviation by learning potential features, and classify by learningfeature distribution. The role of the prototype in this model is to solve theproblem of large distance between congeneric samples taken due to thecontingency of single sampling in FSL, and enhance the robustness of the model.Experimental results on the MSTAR dataset show that the DGP-Net has goodclassification results for SAR images with different depression angles and therecognition accuracy of it is higher than typical FSL methods."
Meta-World Conditional Neural Processes,"['Suzan Ece Ada', 'Emre Ugur']",http://arxiv.org/pdf/2302.10320v1.pdf,2023-02-20,"['cs.lg', 'cs.ai', 'cs.ne', 'cs.ro']","  We propose Meta-World Conditional Neural Processes (MW-CNP), a conditionalworld model generator that leverages sample efficiency and scalability ofConditional Neural Processes to enable an agent to sample from its own""hallucination"". We intend to reduce the agent's interaction with the targetenvironment at test time as much as possible. To reduce the number of samplesrequired at test time, we first obtain a latent representation of thetransition dynamics from a single rollout from the test environment with hiddenparameters. Then, we obtain rollouts for few-shot learning by interacting withthe ""hallucination"" generated by the meta-world model. Using the world modelrepresentation from MW-CNP, the meta-RL agent can adapt to an unseen targetenvironment with significantly fewer samples collected from the targetenvironment compared to the baselines. We emphasize that the agent does nothave access to the task parameters throughout training and testing, and MW-CNPis trained on offline interaction data logged during meta-training."
KG-ECO: Knowledge Graph Enhanced Entity Correction for Query Rewriting,"['Jinglun Cai', 'Mingda Li', 'Ziyan Jiang', 'Eunah Cho', 'Zheng Chen', 'Yang Liu', 'Xing Fan', 'Chenlei Guo']",http://arxiv.org/pdf/2302.10454v2.pdf,2023-02-21,"['cs.cl', 'cs.lg']","  Query Rewriting (QR) plays a critical role in large-scale dialogue systemsfor reducing frictions. When there is an entity error, it imposes extrachallenges for a dialogue system to produce satisfactory responses. In thiswork, we propose KG-ECO: Knowledge Graph enhanced Entity COrrection for queryrewriting, an entity correction system with corrupt entity span detection andentity retrieval/re-ranking functionalities. To boost the model performance, weincorporate Knowledge Graph (KG) to provide entity structural information(neighboring entities encoded by graph neural networks) and textual information(KG entity descriptions encoded by RoBERTa). Experimental results show that ourapproach yields a clear performance gain over two baselines: utterance level QRand entity correction without utilizing KG information. The proposed system isparticularly effective for few-shot learning cases where target entities arerarely seen in training or there is a KG relation between the target entity andother contextual entities in the query."
Language Models are Few-shot Learners for Prognostic Prediction,"['Zekai Chen', 'Mariann Micsinai Balan', 'Kevin Brown']",http://arxiv.org/pdf/2302.12692v4.pdf,2023-02-24,"['cs.cl', 'cs.ai', 'cs.lg', 'q-bio.qm']","  Clinical prediction is an essential task in the healthcare industry. However,the recent success of transformers, on which large language models are built,has not been extended to this domain. In this research, we explore the use oftransformers and language models in prognostic prediction for immunotherapyusing real-world patients' clinical data and molecular profiles. This paperinvestigates the potential of transformers to improve clinical predictioncompared to conventional machine learning approaches and addresses thechallenge of few-shot learning in predicting rare disease areas. The studybenchmarks the efficacy of baselines and language models on prognosticprediction across multiple cancer types and investigates the impact ofdifferent pretrained language models under few-shot regimes. The resultsdemonstrate significant improvements in accuracy and highlight the potential ofNLP in clinical research to improve early detection and intervention fordifferent diseases."
Pre-Finetuning for Few-Shot Emotional Speech Recognition,"['Maximillian Chen', 'Zhou Yu']",http://arxiv.org/pdf/2302.12921v2.pdf,2023-02-24,"['cs.cl', 'cs.lg', 'cs.sd', 'eess.as']","  Speech models have long been known to overfit individual speakers for manyclassification tasks. This leads to poor generalization in settings where thespeakers are out-of-domain or out-of-distribution, as is common in productionenvironments. We view speaker adaptation as a few-shot learning problem andpropose investigating transfer learning approaches inspired by recent successwith pre-trained models in natural language tasks. We propose pre-finetuningspeech models on difficult tasks to distill knowledge into few-shot downstreamclassification objectives. We pre-finetune Wav2Vec2.0 on every permutation offour multiclass emotional speech recognition corpora and evaluate ourpre-finetuned models through 33,600 few-shot fine-tuning trials on theEmotional Speech Dataset."
CLR-GAM: Contrastive Point Cloud Learning with Guided Augmentation and  Feature Mapping,"['Srikanth Malla', 'Yi-Ting Chen']",http://arxiv.org/pdf/2302.14306v1.pdf,2023-02-28,"['cs.cv', 'cs.ai', 'cs.lg']","  Point cloud data plays an essential role in robotics and self-drivingapplications. Yet, annotating point cloud data is time-consuming and nontrivialwhile they enable learning discriminative 3D representations that empowerdownstream tasks, such as classification and segmentation. Recently,contrastive learning-based frameworks have shown promising results for learning3D representations in a self-supervised manner. However, existing contrastivelearning methods cannot precisely encode and associate structural features andsearch the higher dimensional augmentation space efficiently. In this paper, wepresent CLR-GAM, a novel contrastive learning-based framework with GuidedAugmentation (GA) for efficient dynamic exploration strategy and Guided FeatureMapping (GFM) for similar structural feature association between augmentedpoint clouds. We empirically demonstrate that the proposed approach achievesstate-of-the-art performance on both simulated and real-world 3D point clouddatasets for three different downstream tasks, i.e., 3D point cloudclassification, few-shot learning, and object part segmentation."
Self-Supervised Few-Shot Learning for Ischemic Stroke Lesion  Segmentation,"['Luca Tomasetti', 'Stine Hansen', 'Mahdieh Khanmohammadi', 'Kjersti Engan', 'Liv Jorunn Høllesli', 'Kathinka Dæhli Kurz', 'Michael Kampffmeyer']",http://arxiv.org/pdf/2303.01332v2.pdf,2023-03-02,"['eess.iv', 'cs.cv', 'cs.lg']","  Precise ischemic lesion segmentation plays an essential role in improvingdiagnosis and treatment planning for ischemic stroke, one of the prevalentdiseases with the highest mortality rate. While numerous deep neural networkapproaches have recently been proposed to tackle this problem, these methodsrequire large amounts of annotated regions during training, which can beimpractical in the medical domain where annotated data is scarce. As a remedy,we present a prototypical few-shot segmentation approach for ischemic lesionsegmentation using only one annotated sample during training. The proposedapproach leverages a novel self-supervised training mechanism that is tailoredto the task of ischemic stroke lesion segmentation by exploiting color-codedparametric maps generated from Computed Tomography Perfusion scans. Weillustrate the benefits of our proposed training mechanism, leading toconsiderable improvements in performance in the few-shot setting. Given asingle annotated patient, an average Dice score of 0.58 is achieved for thesegmentation of ischemic lesions."
Mixture of Soft Prompts for Controllable Data Generation,"['Derek Chen', 'Celine Lee', 'Yunan Lu', 'Domenic Rosati', 'Zhou Yu']",http://arxiv.org/pdf/2303.01580v2.pdf,2023-03-02,['cs.cl'],"  Large language models (LLMs) effectively generate fluent text when the targetoutput follows natural language patterns. However, structured prediction tasksconfine the output format to a limited ontology, causing even very large modelsto struggle since they were never trained with such restrictions in mind. Thedifficulty of using LLMs for direct prediction is exacerbated in few-shotlearning scenarios, which commonly arise due to domain shift and resourcelimitations. We flip the problem on its head by leveraging the LLM as a toolfor data augmentation rather than direct prediction. Our proposed Mixture ofSoft Prompts (MSP) serves as a parameter-efficient procedure for generatingdata in a controlled manner. Denoising mechanisms are further applied toimprove the quality of synthesized data. Automatic metrics show our method iscapable of producing diverse and natural text, while preserving labelsemantics. Moreover, MSP achieves state-of-the-art results on three benchmarkswhen compared against strong baselines. Our method offers an alternatedata-centric approach for applying LLMs to complex prediction tasks."
A Few-Shot Attention Recurrent Residual U-Net for Crack Segmentation,"['Iason Katsamenis', 'Eftychios Protopapadakis', 'Nikolaos Bakalos', 'Anastasios Doulamis', 'Nikolaos Doulamis', 'Athanasios Voulodimos']",http://arxiv.org/pdf/2303.01582v1.pdf,2023-03-02,"['cs.cv', 'cs.lg', 'eess.iv', '68t07 (primary) 68t45 (secondary)', 'i.2.10; i.4.6']","  Recent studies indicate that deep learning plays a crucial role in theautomated visual inspection of road infrastructures. However, current learningschemes are static, implying no dynamic adaptation to users' feedback. Toaddress this drawback, we present a few-shot learning paradigm for theautomated segmentation of road cracks, which is based on a U-Net architecturewith recurrent residual and attention modules (R2AU-Net). The retrainingstrategy dynamically fine-tunes the weights of the U-Net as a few new rectifiedsamples are being fed into the classifier. Extensive experiments show that theproposed few-shot R2AU-Net framework outperforms other state-of-the-artnetworks in terms of Dice and IoU metrics, on a new dataset, named CrackMap,which is made publicly available at https://github.com/ikatsamenis/CrackMap."
Prismer: A Vision-Language Model with An Ensemble of Experts,"['Shikun Liu', 'Linxi Fan', 'Edward Johns', 'Zhiding Yu', 'Chaowei Xiao', 'Anima Anandkumar']",http://arxiv.org/pdf/2303.02506v2.pdf,2023-03-04,"['cs.lg', 'cs.ai', 'cs.cv']","  Recent vision-language models have shown impressive multi-modal generationcapabilities. However, typically they require training huge models on massivedatasets. As a more scalable alternative, we introduce Prismer, a data- andparameter-efficient vision-language model that leverages an ensemble of domainexperts. Prismer only requires training of a small number of components, withthe majority of network weights inherited from readily-available, pre-traineddomain experts, and kept frozen during training. By leveraging experts from awide range of domains, we show that Prismer can efficiently pool this expertknowledge and adapt it to various vision-language reasoning tasks. In ourexperiments, we show that Prismer achieves fine-tuned and few-shot learningperformance which is competitive with current state-of-the-art models, whilstrequiring up to two orders of magnitude less training data. Code is availableat https://github.com/NVlabs/prismer."
Enhancing Activity Prediction Models in Drug Discovery with the Ability  to Understand Human Language,"['Philipp Seidl', 'Andreu Vall', 'Sepp Hochreiter', 'Günter Klambauer']",http://arxiv.org/pdf/2303.03363v2.pdf,2023-03-06,"['q-bio.bm', 'cs.cl', 'cs.lg', 'stat.ml']","  Activity and property prediction models are the central workhorses in drugdiscovery and materials sciences, but currently they have to be trained orfine-tuned for new tasks. Without training or fine-tuning, scientific languagemodels could be used for such low-data tasks through their announced zero- andfew-shot capabilities. However, their predictive quality at activity predictionis lacking. In this work, we envision a novel type of activity prediction modelthat is able to adapt to new prediction tasks at inference time, viaunderstanding textual information describing the task. To this end, we proposea new architecture with separate modules for chemical and natural languageinputs, and a contrastive pre-training objective on data from large biochemicaldatabases. In extensive experiments, we show that our method CLAMP yieldsimproved predictive performance on few-shot learning benchmarks and zero-shotproblems in drug discovery. We attribute the advances of our method to themodularized architecture and to our pre-training objective."
MenuCraft: Interactive Menu System Design with Large Language Models,"['Amir Hossein Kargaran', 'Nafiseh Nikeghbal', 'Abbas Heydarnoori', 'Hinrich Schütze']",http://arxiv.org/pdf/2303.04496v2.pdf,2023-03-08,"['cs.cl', 'cs.ai', 'cs.hc']","  Menu system design is a challenging task involving many design options andvarious human factors. For example, one crucial factor that designers need toconsider is the semantic and systematic relation of menu commands. However,capturing these relations can be challenging due to limited availableresources. With the advancement of neural language models, large languagemodels can utilize their vast pre-existing knowledge in designing and refiningmenu systems. In this paper, we propose MenuCraft, an AI-assisted designer formenu design that enables collaboration between the designer and a dialoguesystem to design menus. MenuCraft offers an interactive language-based menudesign tool that simplifies the menu design process and enables easycustomization of design options. MenuCraft supports a variety of interactionsthrough dialog that allows performing zero/few-shot learning."
Consistency Analysis of ChatGPT,"['Myeongjun Erik Jang', 'Thomas Lukasiewicz']",http://arxiv.org/pdf/2303.06273v3.pdf,2023-03-11,"['cs.cl', 'cs.ai']","  ChatGPT has gained a huge popularity since its introduction. Its positiveaspects have been reported through many media platforms, and some analyses evenshowed that ChatGPT achieved a decent grade in professional exams, adding extrasupport to the claim that AI can now assist and even replace humans inindustrial fields. Others, however, doubt its reliability and trustworthiness.This paper investigates the trustworthiness of ChatGPT and GPT-4 regardinglogically consistent behaviour, focusing specifically on semantic consistencyand the properties of negation, symmetric, and transitive consistency. Ourfindings suggest that while both models appear to show an enhanced languageunderstanding and reasoning ability, they still frequently fall short ofgenerating logically consistent predictions. We also ascertain via experimentsthat prompt designing, few-shot learning and employing larger large languagemodels (LLMs) are unlikely to be the ultimate solution to resolve theinconsistency issue of LLMs."
HazardNet: Road Debris Detection by Augmentation of Synthetic Models,"['Tae Eun Choe', 'Jane Wu', 'Xiaolin Lin', 'Karen Kwon', 'Minwoo Park']",http://arxiv.org/pdf/2303.07547v1.pdf,2023-03-14,"['cs.cv', 'acm-class: i.1.4']","  We present an algorithm to detect unseen road debris using a small set ofsynthetic models. Early detection of road debris is critical for safeautonomous or assisted driving, yet the development of a robust road debrisdetection model has not been widely discussed. There are two main challenges tobuilding a road debris detector: first, data collection of road debris ischallenging since hazardous objects on the road are rare to encounter in realdriving scenarios; second, the variability of road debris is broad, rangingfrom a very small brick to a large fallen tree. To overcome these challenges,we propose a novel approach to few-shot learning of road debris that usessemantic augmentation and domain randomization to augment real road images withsynthetic models. We constrain the problem domain to uncommon objects on theroad and allow the deep neural network, HazardNet, to learn the semanticmeaning of road debris to eventually detect unseen road debris. Our resultsdemonstrate that HazardNet is able to accurately detect real road debris whenonly trained on synthetic objects in augmented images."
A Survey of Deep Visual Cross-Domain Few-Shot Learning,"['Wenjian Wang', 'Lijuan Duan', 'Yuxi Wang', 'Junsong Fan', 'Zhi Gong', 'Zhaoxiang Zhang']",http://arxiv.org/pdf/2303.09253v1.pdf,2023-03-16,['cs.cv'],"  Few-Shot transfer learning has become a major focus of research as it allowsrecognition of new classes with limited labeled data. While it is assumed thattrain and test data have the same data distribution, this is often not the casein real-world applications. This leads to decreased model transfer effects whenthe new class distribution differs significantly from the learned classes.Research into Cross-Domain Few-Shot (CDFS) has emerged to address this issue,forming a more challenging and realistic setting. In this survey, we provide adetailed taxonomy of CDFS from the problem setting and corresponding solutionsview. We summarise the existing CDFS network architectures and discuss thesolution ideas for each direction the taxonomy indicates. Furthermore, weintroduce various CDFS downstream applications and outline classification,detection, and segmentation benchmarks and corresponding standards forevaluation. We also discuss the challenges of CDFS research and explorepotential directions for future investigation. Through this review, we aim toprovide comprehensive guidance on CDFS research, enabling researchers to gaininsight into the state-of-the-art while allowing them to build upon existingsolutions to develop their own CDFS models."
Generalizable Denoising of Microscopy Images using Generative  Adversarial Networks and Contrastive Learning,"['Felix Fuentes-Hurtado', 'Jean-Baptiste Sibarita', 'Virgile Viasnoff']",http://arxiv.org/pdf/2303.15214v2.pdf,2023-03-27,"['eess.iv', 'cs.cv', 'cs.lg']","  Microscopy images often suffer from high levels of noise, which can hinderfurther analysis and interpretation. Content-aware image restoration (CARE)methods have been proposed to address this issue, but they often require largeamounts of training data and suffer from over-fitting. To overcome thesechallenges, we propose a novel framework for few-shot microscopy imagedenoising. Our approach combines a generative adversarial network (GAN) trainedvia contrastive learning (CL) with two structure preserving loss terms(Structural Similarity Index and Total Variation loss) to further improve thequality of the denoised images using little data. We demonstrate theeffectiveness of our method on three well-known microscopy imaging datasets,and show that we can drastically reduce the amount of training data whileretaining the quality of the denoising, thus alleviating the burden ofacquiring paired data and enabling few-shot learning. The proposed frameworkcan be easily extended to other image restoration tasks and has the potentialto significantly advance the field of microscopy image analysis."
Learning Expressive Prompting With Residuals for Vision Transformers,"['Rajshekhar Das', 'Yonatan Dukler', 'Avinash Ravichandran', 'Ashwin Swaminathan']",http://arxiv.org/pdf/2303.15591v1.pdf,2023-03-27,['cs.cv'],"  Prompt learning is an efficient approach to adapt transformers by insertinglearnable set of parameters into the input and intermediate representations ofa pre-trained model. In this work, we present Expressive Prompts with Residuals(EXPRES) which modifies the prompt learning paradigm specifically for effectiveadaptation of vision transformers (ViT). Out method constructs downstreamrepresentations via learnable ``output'' tokens, that are akin to the learnedclass tokens of the ViT. Further for better steering of the downstreamrepresentation processed by the frozen transformer, we introduce residuallearnable tokens that are added to the output of various computations. We applyEXPRES for image classification, few shot learning, and semantic segmentation,and show our method is capable of achieving state of the art prompt tuning on3/3 categories of the VTAB benchmark. In addition to strong performance, weobserve that our approach is an order of magnitude more prompt efficient thanexisting visual prompting baselines. We analytically show the computationalbenefits of our approach over weight space adaptation techniques likefinetuning. Lastly we systematically corroborate the architectural design ofour method via a series of ablation experiments."
Point2Vec for Self-Supervised Representation Learning on Point Clouds,"['Karim Abou Zeid', 'Jonas Schult', 'Alexander Hermans', 'Bastian Leibe']",http://arxiv.org/pdf/2303.16570v2.pdf,2023-03-29,['cs.cv'],"  Recently, the self-supervised learning framework data2vec has shown inspiringperformance for various modalities using a masked student-teacher approach.However, it remains open whether such a framework generalizes to the uniquechallenges of 3D point clouds. To answer this question, we extend data2vec tothe point cloud domain and report encouraging results on several downstreamtasks. In an in-depth analysis, we discover that the leakage of positionalinformation reveals the overall object shape to the student even under heavymasking and thus hampers data2vec to learn strong representations for pointclouds. We address this 3D-specific shortcoming by proposing point2vec, whichunleashes the full potential of data2vec-like pre-training on point clouds. Ourexperiments show that point2vec outperforms other self-supervised methods onshape classification and few-shot learning on ModelNet40 and ScanObjectNN,while achieving competitive results on part segmentation on ShapeNetParts.These results suggest that the learned representations are strong andtransferable, highlighting point2vec as a promising direction forself-supervised learning of point cloud representations."
Cross-Cultural Transfer Learning for Chinese Offensive Language  Detection,"['Li Zhou', 'Laura Cabello', 'Yong Cao', 'Daniel Hershcovich']",http://arxiv.org/pdf/2303.17927v1.pdf,2023-03-31,['cs.cl'],"  Detecting offensive language is a challenging task. Generalizing acrossdifferent cultures and languages becomes even more challenging: besideslexical, syntactic and semantic differences, pragmatic aspects such as culturalnorms and sensitivities, which are particularly relevant in this context, varygreatly. In this paper, we target Chinese offensive language detection and aimto investigate the impact of transfer learning using offensive languagedetection data from different cultural backgrounds, specifically Korean andEnglish. We find that culture-specific biases in what is considered offensivenegatively impact the transferability of language models (LMs) and that LMstrained on diverse cultural data are sensitive to different features in Chineseoffensive language detection. In a few-shot learning scenario, however, ourstudy shows promising prospects for non-English offensive language detectionwith limited resources. Our findings highlight the importance of cross-culturaltransfer learning in improving offensive language detection and promotinginclusive digital spaces."
Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior  Refinement,"['Xiangyang Zhu', 'Renrui Zhang', 'Bowei He', 'Aojun Zhou', 'Dong Wang', 'Bin Zhao', 'Peng Gao']",http://arxiv.org/pdf/2304.01195v1.pdf,2023-04-03,"['cs.cv', 'cs.ai', 'cs.mm']","  The popularity of Contrastive Language-Image Pre-training (CLIP) haspropelled its application to diverse downstream vision tasks. To improve itscapacity on downstream tasks, few-shot learning has become a widely-adoptedtechnique. However, existing methods either exhibit limited performance orsuffer from excessive learnable parameters. In this paper, we propose APE, anAdaptive Prior rEfinement method for CLIP's pre-trained knowledge, whichachieves superior accuracy with high computational efficiency. Via a priorrefinement module, we analyze the inter-class disparity in the downstream dataand decouple the domain-specific knowledge from the CLIP-extracted cache model.On top of that, we introduce two model variants, a training-free APE and atraining-required APE-T. We explore the trilateral affinities between the testimage, prior cache model, and textual representations, and only enable alightweight category-residual module to be trained. For the average accuracyover 11 benchmarks, both APE and APE-T attain state-of-the-art and respectivelyoutperform the second-best by +1.59% and +1.99% under 16 shots with x30 lesslearnable parameters."
Sociocultural knowledge is needed for selection of shots in hate speech  detection tasks,"['Antonis Maronikolakis', 'Abdullatif Köksal', 'Hinrich Schütze']",http://arxiv.org/pdf/2304.01890v4.pdf,2023-04-04,"['cs.cl', 'cs.ai', 'cs.lg']","  We introduce HATELEXICON, a lexicon of slurs and targets of hate speech forthe countries of Brazil, Germany, India and Kenya, to aid training andinterpretability of models. We demonstrate how our lexicon can be used tointerpret model predictions, showing that models developed to classify extremespeech rely heavily on target words when making predictions. Further, wepropose a method to aid shot selection for training in low-resource settingsvia HATELEXICON. In few-shot learning, the selection of shots is of paramountimportance to model performance. In our work, we simulate a few-shot settingfor German and Hindi, using HASOC data for training and the MultilingualHateCheck (MHC) as a benchmark. We show that selecting shots based on ourlexicon leads to models performing better on MHC than models trained on shotssampled randomly. Thus, when given only a few training examples, using ourlexicon to select shots containing more sociocultural information leads tobetter few-shot performance."
Learning to Learn with Indispensable Connections,"['Sambhavi Tiwari', 'Manas Gogoi', 'Shekhar Verma', 'Krishna Pratap Singh']",http://arxiv.org/pdf/2304.02862v1.pdf,2023-04-06,['cs.lg'],"  Meta-learning aims to solve unseen tasks with few labelled instances.Nevertheless, despite its effectiveness for quick learning in existingoptimization-based methods, it has several flaws. Inconsequential connectionsare frequently seen during meta-training, which results in anover-parameterized neural network. Because of this, meta-testing observesunnecessary computations and extra memory overhead. To overcome such flaws. Wepropose a novel meta-learning method called Meta-LTH that includesindispensible (necessary) connections. We applied the lottery ticket hypothesistechnique known as magnitude pruning to generate these crucial connections thatcan effectively solve few-shot learning problem. We aim to perform two things:(a) to find a sub-network capable of more adaptive meta-learning and (b) tolearn new low-level features of unseen tasks and recombine those features withthe already learned features during the meta-test phase. Experimental resultsshow that our proposed Met-LTH method outperformed existing first-order MAMLalgorithm for three different classification datasets. Our method improves theclassification accuracy by approximately 2% (20-way 1-shot task setting) foromniglot dataset."
Revisiting Automated Prompting: Are We Actually Doing Better?,"['Yulin Zhou', 'Yiren Zhao', 'Ilia Shumailov', 'Robert Mullins', 'Yarin Gal']",http://arxiv.org/pdf/2304.03609v2.pdf,2023-04-07,"['cs.cl', 'cs.lg']","  Current literature demonstrates that Large Language Models (LLMs) are greatfew-shot learners, and prompting significantly increases their performance on arange of downstream tasks in a few-shot learning setting. An attempt toautomate human-led prompting followed, with some progress achieved. Inparticular, subsequent work demonstrates automation can outperform fine-tuningin certain K-shot learning scenarios.  In this paper, we revisit techniques for automated prompting on six differentdownstream tasks and a larger range of K-shot learning settings. We find thatautomated prompting does not consistently outperform simple manual prompts. Ourwork suggests that, in addition to fine-tuning, manual prompts should be usedas a baseline in this line of research."
"WikiGoldSK: Annotated Dataset, Baselines and Few-Shot Learning  Experiments for Slovak Named Entity Recognition","['Dávid Šuba', 'Marek Šuppa', 'Jozef Kubík', 'Endre Hamerlik', 'Martin Takáč']",http://arxiv.org/pdf/2304.04026v1.pdf,2023-04-08,"['cs.cl', 'cs.ai']","  Named Entity Recognition (NER) is a fundamental NLP tasks with a wide rangeof practical applications. The performance of state-of-the-art NER methodsdepends on high quality manually anotated datasets which still do not exist forsome languages. In this work we aim to remedy this situation in Slovak byintroducing WikiGoldSK, the first sizable human labelled Slovak NER dataset. Webenchmark it by evaluating state-of-the-art multilingual Pretrained LanguageModels and comparing it to the existing silver-standard Slovak NER dataset. Wealso conduct few-shot experiments and show that training on a sliver-standarddataset yields better results. To enable future work that can be based onSlovak NER, we release the dataset, code, as well as the trained modelspublicly under permissible licensing terms athttps://github.com/NaiveNeuron/WikiGoldSK."
MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning,"['Bohan Li', 'Longxu Dou', 'Yutai Hou', 'Yunlong Feng', 'Honglin Mu', 'Qingfu Zhu', 'Qinghua Sun', 'Wanxiang Che']",http://arxiv.org/pdf/2304.09402v2.pdf,2023-04-19,"['cs.cl', 'cs.lg']","  Prompt-based learning has shown considerable promise in reformulating variousdownstream tasks as cloze problems by combining original input with apredetermined template. This approach demonstrates its effectiveness,especially in few-shot learning scenarios, where the model is trained on ascarce amount of data. Despite its successes, the limited templates and text infew-shot prompt-based learning scenarios leave significant room for performanceimprovement. Moreover, existing methods sometimes resort to model ensembles,which, while effective, could potentially hamper model efficiency due toincreased computational demands. To address these issues, we introduce MixPro,an augmentation method designed to augment both the vanilla input text and thetemplates. We implement this through the token-level, the sentence-level, andthe template-level Mixup strategies. The experimental results on five few-shotdatasets show that MixPro outperforms other augmentation baselines, improvingmodel performance by an average of 5.08% compared to before augmentation."
Information Extraction from Documents: Question Answering vs Token  Classification in real-world setups,"['Laurent Lam', 'Pirashanth Ratnamogan', 'Joël Tang', 'William Vanhuffel', 'Fabien Caspani']",http://arxiv.org/pdf/2304.10994v1.pdf,2023-04-21,['cs.cl'],"  Research in Document Intelligence and especially in Document Key InformationExtraction (DocKIE) has been mainly solved as Token Classification problem.Recent breakthroughs in both natural language processing (NLP) and computervision helped building document-focused pre-training methods, leveraging amultimodal understanding of the document text, layout and image modalities.However, these breakthroughs also led to the emergence of a new DocKIE subtaskof extractive document Question Answering (DocQA), as part of the MachineReading Comprehension (MRC) research field. In this work, we compare theQuestion Answering approach with the classical token classification approachfor document key information extraction. We designed experiments to benchmarkfive different experimental setups : raw performances, robustness to noisyenvironment, capacity to extract long entities, fine-tuning speed on Few-ShotLearning and finally Zero-Shot Learning. Our research showed that when dealingwith clean and relatively short entities, it is still best to use tokenclassification-based approach, while the QA approach could be a goodalternative for noisy environment or long entities use-cases."
Discern and Answer: Mitigating the Impact of Misinformation in  Retrieval-Augmented Models with Discriminators,"['Giwon Hong', 'Jeonghwan Kim', 'Junmo Kang', 'Sung-Hyon Myaeng', 'Joyce Jiyoung Whang']",http://arxiv.org/pdf/2305.01579v1.pdf,2023-05-02,"['cs.cl', 'cs.ai']","  Most existing retrieval-augmented language models (LMs) for questionanswering assume all retrieved information is factually correct. In this work,we study a more realistic scenario in which retrieved documents may containmisinformation, causing conflicts among them. We observe that the existingmodels are highly brittle to such information in both fine-tuning andin-context few-shot learning settings. We propose approaches to makeretrieval-augmented LMs robust to misinformation by explicitly fine-tuning adiscriminator or prompting to elicit discrimination capability in GPT-3. Ourempirical results on open-domain question answering show that these approachessignificantly improve LMs' robustness to knowledge conflicts. We also provideour findings on interleaving the fine-tuned model's decision with thein-context learning process, paving a new path to leverage the best of bothworlds."
Causal Interventions-based Few-Shot Named Entity Recognition,"['Zhen Yang', 'Yongbin Liu', 'Chunping Ouyang']",http://arxiv.org/pdf/2305.01914v1.pdf,2023-05-03,['cs.cl'],"  Few-shot named entity recognition (NER) systems aims at recognizing newclasses of entities based on a few labeled samples. A significant challenge inthe few-shot regime is prone to overfitting than the tasks with abundantsamples. The heavy overfitting in few-shot learning is mainly led by spuriouscorrelation caused by the few samples selection bias. To alleviate the problemof the spurious correlation in the few-shot NER, in this paper, we propose acausal intervention-based few-shot NER method. Based on the prototypicalnetwork, the method intervenes in the context and prototype via backdooradjustment during training. In particular, intervening in the context of theone-shot scenario is very difficult, so we intervene in the prototype viaincremental learning, which can also avoid catastrophic forgetting. Ourexperiments on different benchmarks show that our approach achieves newstate-of-the-art results (achieving up to 29% absolute improvement and 12% onaverage for all tasks)."
Learning to Detect Novel and Fine-Grained Acoustic Sequences Using  Pretrained Audio Representations,"['Vasudha Kowtha', 'Miquel Espi Marques', 'Jonathan Huang', 'Yichi Zhang', 'Carlos Avendano']",http://arxiv.org/pdf/2305.02382v1.pdf,2023-05-03,"['cs.sd', 'cs.lg', 'eess.as']","  This work investigates pretrained audio representations for few shot SoundEvent Detection. We specifically address the task of few shot detection ofnovel acoustic sequences, or sound events with semantically meaningful temporalstructure, without assuming access to non-target audio. We develop proceduresfor pretraining suitable representations, and methods which transfer them toour few shot learning scenario. Our experiments evaluate the general purposeutility of our pretrained representations on AudioSet, and the utility ofproposed few shot methods via tasks constructed from real-world acousticsequences. Our pretrained embeddings are suitable to the proposed task, andenable multiple aspects of our few shot framework."
Plug-and-Play Multilingual Few-shot Spoken Words Recognition,"['Aaqib Saeed', 'Vasileios Tsouvalas']",http://arxiv.org/pdf/2305.03058v1.pdf,2023-05-03,"['eess.as', 'cs.lg', 'cs.sd']","  As technology advances and digital devices become prevalent, seamlesshuman-machine communication is increasingly gaining significance. The growingadoption of mobile, wearable, and other Internet of Things (IoT) devices haschanged how we interact with these smart devices, making accurate spoken wordsrecognition a crucial component for effective interaction. However, buildingrobust spoken words detection system that can handle novel keywords remainschallenging, especially for low-resource languages with limited training data.Here, we propose PLiX, a multilingual and plug-and-play keyword spotting systemthat leverages few-shot learning to harness massive real-world data and enablethe recognition of unseen spoken words at test-time. Our few-shot deep modelsare learned with millions of one-second audio clips across 20 languages,achieving state-of-the-art performance while being highly efficient. Extensiveevaluations show that PLiX can generalize to novel spoken words given as few asjust one support example and performs well on unseen languages out of the box.We release models and inference code to serve as a foundation for futureresearch and voice-enabled user interface development for emerging devices."
Data Curation for Image Captioning with Text-to-Image Generative Models,"['Wenyan Li', 'Jonas F. Lotz', 'Chen Qiu', 'Desmond Elliott']",http://arxiv.org/pdf/2305.03610v1.pdf,2023-05-05,"['cs.cv', 'cs.ai', 'cs.cl']","  Recent advances in image captioning are mainly driven by large-scalevision-language pretraining, relying heavily on computational resources andincreasingly large multimodal datasets. Instead of scaling up pretraining data,we ask whether it is possible to improve performance by improving the qualityof the samples in existing datasets. We pursue this question through twoapproaches to data curation: one that assumes that some examples should beavoided due to mismatches between the image and caption, and one that assumesthat the mismatch can be addressed by replacing the image, for which we use thestate-of-the-art Stable Diffusion model. These approaches are evaluated usingthe BLIP model on MS COCO and Flickr30K in both finetuning and few-shotlearning settings. Our simple yet effective approaches consistently outperformbaselines, indicating that better image captioning models can be trained bycurating existing resources. Finally, we conduct a human study to understandthe errors made by the Stable Diffusion model and highlight directions forfuture work in text-to-image generation."
Meta Omnium: A Benchmark for General-Purpose Learning-to-Learn,"['Ondrej Bohdal', 'Yinbing Tian', 'Yongshuo Zong', 'Ruchika Chavhan', 'Da Li', 'Henry Gouk', 'Li Guo', 'Timothy Hospedales']",http://arxiv.org/pdf/2305.07625v1.pdf,2023-05-12,"['cs.cv', 'cs.lg', 'stat.ml']","  Meta-learning and other approaches to few-shot learning are widely studiedfor image recognition, and are increasingly applied to other vision tasks suchas pose estimation and dense prediction. This naturally raises the question ofwhether there is any few-shot meta-learning algorithm capable of generalizingacross these diverse task types? To support the community in answering thisquestion, we introduce Meta Omnium, a dataset-of-datasets spanning multiplevision tasks including recognition, keypoint localization, semanticsegmentation and regression. We experiment with popular few-shot meta-learningbaselines and analyze their ability to generalize across tasks and to transferknowledge between them. Meta Omnium enables meta-learning researchers toevaluate model generalization to a much wider array of tasks than previouslypossible, and provides a single framework for evaluating meta-learners across awide suite of vision applications in a consistent manner."
Make Prompt-based Black-Box Tuning Colorful: Boosting Model  Generalization from Three Orthogonal Perspectives,"['Qiushi Sun', 'Chengcheng Han', 'Nuo Chen', 'Renyu Zhu', 'Jingyang Gong', 'Xiang Li', 'Ming Gao']",http://arxiv.org/pdf/2305.08088v1.pdf,2023-05-14,"['cs.cl', 'cs.ai']","  Large language models (LLMs) have shown increasing power on various naturallanguage processing (NLP) tasks. However, tuning these models for downstreamtasks usually needs exorbitant costs or is unavailable due to commercialconsiderations. Recently, black-box tuning has been proposed to address thisproblem by optimizing task-specific prompts without accessing the gradients andhidden representations. However, most existing works have yet fully exploitedthe potential of gradient-free optimization under the scenario of few-shotlearning. In this paper, we describe BBT-RGB, a suite of straightforward andcomplementary techniques for enhancing the efficiency and performance ofblack-box optimization. Specifically, our method includes three plug-and-playcomponents: (1) Two-stage derivative-free optimization strategy thatfacilitates fast convergence and mitigates overfitting; (2) Automaticverbalizer construction with its novel usage under few-shot settings; (3)Better prompt initialization policy based on instruction search andauto-selected demonstration. Extensive experiments across various tasks onnatural language understanding and inference demonstrate the effectiveness ofour method. Our codes are publicly available athttps://github.com/QiushiSun/BBT-RGB."
CPL-NoViD: Context-Aware Prompt-based Learning for Norm Violation  Detection in Online Communities,"['Zihao He', 'Jonathan May', 'Kristina Lerman']",http://arxiv.org/pdf/2305.09846v2.pdf,2023-05-16,"['cs.cl', 'cs.si']","  Detecting norm violations in online communities is critical to maintaininghealthy and safe spaces for online discussions. Existing machine learningapproaches often struggle to adapt to the diverse rules and interpretationsacross different communities due to the inherent challenges of fine-tuningmodels for such context-specific tasks. In this paper, we introduceContext-aware Prompt-based Learning for Norm Violation Detection (CPL-NoViD), anovel method that employs prompt-based learning to detect norm violationsacross various types of rules. CPL-NoViD outperforms the baseline byincorporating context through natural language prompts and demonstratesimproved performance across different rule types. Significantly, it not onlyexcels in cross-rule-type and cross-community norm violation detection but alsoexhibits adaptability in few-shot learning scenarios. Most notably, itestablishes a new state-of-the-art in norm violation detection, surpassingexisting benchmarks. Our work highlights the potential of prompt-based learningfor context-sensitive norm violation detection and paves the way for futureresearch on more adaptable, context-aware models to better support onlinecommunity moderators."
HMSN: Hyperbolic Self-Supervised Learning by Clustering with Ideal  Prototypes,"['Aiden Durrant', 'Georgios Leontidis']",http://arxiv.org/pdf/2305.10926v1.pdf,2023-05-18,['cs.cv'],"  Hyperbolic manifolds for visual representation learning allow for effectivelearning of semantic class hierarchies by naturally embedding tree-likestructures with low distortion within a low-dimensional representation space.The highly separable semantic class hierarchies produced by hyperbolic learninghave shown to be powerful in low-shot tasks, however, their application inself-supervised learning is yet to be explored fully. In this work, we explorethe use of hyperbolic representation space for self-supervised representationlearning for prototype-based clustering approaches. First, we extend the MaskedSiamese Networks to operate on the Poincar\'e ball model of hyperbolic space,secondly, we place prototypes on the ideal boundary of the Poincar\'e ball.Unlike previous methods we project to the hyperbolic space at the output of theencoder network and utilise a hyperbolic projection head to ensure that therepresentations used for downstream tasks remain hyperbolic. Empirically wedemonstrate the ability of these methods to perform comparatively to Euclideanmethods in lower dimensions for linear evaluation tasks, whilst showingimprovements in extreme few-shot learning tasks."
A Weak Supervision Approach for Few-Shot Aspect Based Sentiment,"['Robert Vacareanu', 'Siddharth Varia', 'Kishaloy Halder', 'Shuai Wang', 'Giovanni Paolini', 'Neha Anna John', 'Miguel Ballesteros', 'Smaranda Muresan']",http://arxiv.org/pdf/2305.11979v1.pdf,2023-05-19,['cs.cl'],"  We explore how weak supervision on abundant unlabeled data can be leveragedto improve few-shot performance in aspect-based sentiment analysis (ABSA)tasks. We propose a pipeline approach to construct a noisy ABSA dataset, and weuse it to adapt a pre-trained sequence-to-sequence model to the ABSA tasks. Wetest the resulting model on three widely used ABSA datasets, before and afterfine-tuning. Our proposed method preserves the full fine-tuning performancewhile showing significant improvements (15.84% absolute F1) in the few-shotlearning scenario for the harder tasks. In zero-shot (i.e., withoutfine-tuning), our method outperforms the previous state of the art on theaspect extraction sentiment classification (AESC) task and is, additionally,capable of performing the harder aspect sentiment triplet extraction (ASTE)task."
Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data  Synthesis,"['Mingda Chen', 'Xilun Chen', 'Wen-tau Yih']",http://arxiv.org/pdf/2305.13691v1.pdf,2023-05-23,['cs.cl'],"  Few-shot learning for open domain multi-hop question answering typicallyrelies on large language models (LLMs). While powerful, LLMs are inefficient atthe inference time. We propose a data synthesis framework for multi-hopquestion answering that allows for improving smaller language models with lessthan 10 human-annotated question answer pairs. The framework is built upon thedata generation functions parameterized by LLMs and prompts, which requiresminimal hand-crafted features. Empirically, we synthesize millions of multi-hopquestions and claims. After finetuning language models on the synthetic data,we evaluate the models on popular benchmarks on multi-hop question answeringand fact verification. Our experimental results show that finetuning on thesynthetic data improves model performance significantly, allowing our finetunedmodels to be competitive with prior models while being almost one-third thesize in terms of parameter counts."
Images in Language Space: Exploring the Suitability of Large Language  Models for Vision & Language Tasks,"['Sherzod Hakimov', 'David Schlangen']",http://arxiv.org/pdf/2305.13782v1.pdf,2023-05-23,['cs.cl'],"  Large language models have demonstrated robust performance on variouslanguage tasks using zero-shot or few-shot learning paradigms. While beingactively researched, multimodal models that can additionally handle images asinput have yet to catch up in size and generality with language-only models. Inthis work, we ask whether language-only models can be utilised for tasks thatrequire visual input -- but also, as we argue, often require a strong reasoningcomponent. Similar to some recent related work, we make visual informationaccessible to the language model using separate verbalisation models.Specifically, we investigate the performance of open-source, open-accesslanguage models against GPT-3 on five vision-language tasks when giventextually-encoded visual information. Our results suggest that language modelsare effective for solving vision-language tasks even with limited samples. Thisapproach also enhances the interpretability of a model's output by providing ameans of tracing the output back through the verbalised image content."
Improving Factuality and Reasoning in Language Models through Multiagent  Debate,"['Yilun Du', 'Shuang Li', 'Antonio Torralba', 'Joshua B. Tenenbaum', 'Igor Mordatch']",http://arxiv.org/pdf/2305.14325v1.pdf,2023-05-23,"['cs.cl', 'cs.ai', 'cs.cv', 'cs.lg']","  Large language models (LLMs) have demonstrated remarkable capabilities inlanguage generation, understanding, and few-shot learning in recent years. Anextensive body of work has explored how their performance may be furtherimproved through the tools of prompting, ranging from verification,self-consistency, or intermediate scratchpads. In this paper, we present acomplementary approach to improve language responses where multiple languagemodel instances propose and debate their individual responses and reasoningprocesses over multiple rounds to arrive at a common final answer. Our findingsindicate that this approach significantly enhances mathematical and strategicreasoning across a number of tasks. We also demonstrate that our approachimproves the factual validity of generated content, reducing fallacious answersand hallucinations that contemporary models are prone to. Our approach may bedirectly applied to existing black-box models and uses identical procedure andprompts for all tasks we investigate. Overall, our findings suggest that such""society of minds"" approach has the potential to significantly advance thecapabilities of LLMs and pave the way for further breakthroughs in languagegeneration and understanding."
Are Large Language Models Robust Coreference Resolvers?,"['Nghia T. Le', 'Alan Ritter']",http://arxiv.org/pdf/2305.14489v2.pdf,2023-05-23,['cs.cl'],"  Recent work on extending coreference resolution across domains and languagesrelies on annotated data in both the target domain and language. At the sametime, pre-trained large language models (LMs) have been reported to exhibitstrong zero- and few-shot learning abilities across a wide range of NLP tasks.However, prior work mostly studied this ability using artificial sentence-leveldatasets such as the Winograd Schema Challenge. In this paper, we assess thefeasibility of prompt-based coreference resolution by evaluatinginstruction-tuned language models on difficult, linguistically-complexcoreference benchmarks (e.g., CoNLL-2012). We show that prompting forcoreference can outperform current unsupervised coreference systems, althoughthis approach appears to be reliant on high-quality mention detectors. Furtherinvestigations reveal that instruction-tuned LMs generalize surprisingly wellacross domains, languages, and time periods; yet continued fine-tuning ofneural models should still be preferred if small amounts of annotated examplesare available."
Training on Thin Air: Improve Image Classification with Generated Data,"['Yongchao Zhou', 'Hshmat Sahak', 'Jimmy Ba']",http://arxiv.org/pdf/2305.15316v1.pdf,2023-05-24,"['cs.cv', 'cs.lg']","  Acquiring high-quality data for training discriminative models is a crucialyet challenging aspect of building effective predictive systems. In this paper,we present Diffusion Inversion, a simple yet effective method that leveragesthe pre-trained generative model, Stable Diffusion, to generate diverse,high-quality training data for image classification. Our approach captures theoriginal data distribution and ensures data coverage by inverting images to thelatent space of Stable Diffusion, and generates diverse novel training imagesby conditioning the generative model on noisy versions of these vectors. Weidentify three key components that allow our generated images to successfullysupplant the original dataset, leading to a 2-3x enhancement in samplecomplexity and a 6.5x decrease in sampling time. Moreover, our approachconsistently outperforms generic prompt-based steering methods and KNNretrieval baseline across a wide range of datasets. Additionally, wedemonstrate the compatibility of our approach with widely-used dataaugmentation techniques, as well as the reliability of the generated data insupporting various neural architectures and enhancing few-shot learning."
ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR  Back-Translation,"['Kuan-Hao Huang', 'Varun Iyer', 'I-Hung Hsu', 'Anoop Kumar', 'Kai-Wei Chang', 'Aram Galstyan']",http://arxiv.org/pdf/2305.16585v1.pdf,2023-05-26,['cs.cl'],"  Paraphrase generation is a long-standing task in natural language processing(NLP). Supervised paraphrase generation models, which rely on human-annotatedparaphrase pairs, are cost-inefficient and hard to scale up. On the other hand,automatically annotated paraphrase pairs (e.g., by machine back-translation),usually suffer from the lack of syntactic diversity -- the generated paraphrasesentences are very similar to the source sentences in terms of syntax. In thiswork, we present ParaAMR, a large-scale syntactically diverse paraphrasedataset created by abstract meaning representation back-translation. Ourquantitative analysis, qualitative examples, and human evaluation demonstratethat the paraphrases of ParaAMR are syntactically more diverse compared toexisting large-scale paraphrase datasets while preserving good semanticsimilarity. In addition, we show that ParaAMR can be used to improve on threeNLP tasks: learning sentence embeddings, syntactically controlled paraphrasegeneration, and data augmentation for few-shot learning. Our results thusshowcase the potential of ParaAMR for improving various NLP applications."
Parallel Corpus for Indigenous Language Translation: Spanish-Mazatec and  Spanish-Mixtec,"['Atnafu Lambebo Tonja', 'Christian Maldonado-Sifuentes', 'David Alejandro Mendoza Castillo', 'Olga Kolesnikova', 'Noé Castro-Sánchez', 'Grigori Sidorov', 'Alexander Gelbukh']",http://arxiv.org/pdf/2305.17404v1.pdf,2023-05-27,['cs.cl'],"  In this paper, we present a parallel Spanish-Mazatec and Spanish-Mixteccorpus for machine translation (MT) tasks, where Mazatec and Mixtec are twoindigenous Mexican languages. We evaluated the usability of the collectedcorpus using three different approaches: transformer, transfer learning, andfine-tuning pre-trained multilingual MT models. Fine-tuning the FacebookM2M100-48 model outperformed the other approaches, with BLEU scores of 12.09and 22.25 for Mazatec-Spanish and Spanish-Mazatec translations, respectively,and 16.75 and 22.15 for Mixtec-Spanish and Spanish-Mixtec translations,respectively. The findings show that the dataset size (9,799 sentences inMazatec and 13,235 sentences in Mixtec) affects translation performance andthat indigenous languages work better when used as target languages. Thefindings emphasize the importance of creating parallel corpora for indigenouslanguages and fine-tuning models for low-resource translation tasks. Futureresearch will investigate zero-shot and few-shot learning approaches to furtherimprove translation performance in low-resource settings. The dataset andscripts are available at\url{https://github.com/atnafuatx/Machine-Translation-Resources}"
Adapting Language-Audio Models as Few-Shot Audio Learners,"['Jinhua Liang', 'Xubo Liu', 'Haohe Liu', 'Huy Phan', 'Emmanouil Benetos', 'Mark D. Plumbley', 'Wenwu Wang']",http://arxiv.org/pdf/2305.17719v1.pdf,2023-05-28,"['eess.as', 'cs.sd']","  We presented the Treff adapter, a training-efficient adapter for CLAP, toboost zero-shot classification performance by making use of a small set oflabelled data. Specifically, we designed CALM to retrieve the probabilitydistribution of text-audio clips over classes using a set of audio-label pairsand combined it with CLAP's zero-shot classification results. Furthermore, wedesigned a training-free version of the Treff adapter by using CALM as a cosinesimilarity measure. Experiments showed that the proposed Treff adapter iscomparable and even better than fully-supervised methods and adaptation methodsin low-shot and data-abundant scenarios. While the Treff adapter shows thatcombining large-scale pretraining and rapid learning of domain-specificknowledge is non-trivial for obtaining generic representations for few-shotlearning, it is still limited to audio classification tasks. In the future, wewill explore how to use audio-language models in diverse audio domains."
Transfer Learning for Power Outage Detection Task with Limited Training  Data,['Olukunle Owolabi'],http://arxiv.org/pdf/2305.17817v1.pdf,2023-05-28,"['cs.cl', 'stat.ap']","  Early detection of power outages is crucial for maintaining a reliable powerdistribution system. This research investigates the use of transfer learningand language models in detecting outages with limited labeled data. Byleveraging pretraining and transfer learning, models can generalize to unseenclasses.  Using a curated balanced dataset of social media tweets related to poweroutages, we conducted experiments using zero-shot and few-shot learning. Ourhypothesis is that Language Models pretrained with limited data could achievehigh performance in outage detection tasks over baseline models. Results showthat while classical models outperform zero-shot Language Models, few-shotfine-tuning significantly improves their performance. For example, with 10%fine-tuning, BERT achieves 81.3% accuracy (+15.3%), and GPT achieves 74.5%accuracy (+8.5%). This has practical implications for analyzing and localizingoutages in scenarios with limited data availability.  Our evaluation provides insights into the potential of few-shot fine-tuningwith Language Models for power outage detection, highlighting their strengthsand limitations. This research contributes to the knowledge base of leveragingadvanced natural language processing techniques for managing criticalinfrastructure."
Deeply Coupled Cross-Modal Prompt Learning,"['Xuejing Liu', 'Wei Tang', 'Jinghui Lu', 'Rui Zhao', 'Zhaojun Guo', 'Fei Tan']",http://arxiv.org/pdf/2305.17903v3.pdf,2023-05-29,['cs.cv'],"  Recent advancements in multimodal foundation models (e.g., CLIP) haveexcelled in zero-shot generalization. Prompt tuning involved in the knowledgetransfer from foundation models to downstream tasks has gained significantattention recently. Existing prompt-tuning methods in cross-modal learning,however, either solely focus on language branch, or learn vision-languageinteraction in a shallow mechanism. In this context, we propose a Deeplycoupled Cross-modal Prompt learning (DCP) method based on CLIP. DCP flexiblyaccommodates the interplay between vision and language with a Cross-ModalPrompt Attention (CMPA) mechanism, which enables the mutual exchange ofrespective representation through a well-connected multi-head attention moduleprogressively and strongly. We then conduct comprehensive few-shot learningexperiments on 11 image classification datasets and analyze the robustness todomain shift as well. Thorough experimental analysis evidently demonstrates thesuperb few-shot generalization and compelling domain adaption capacity of awell-executed DCP. The code can be found at https://github.com/GingL/CMPA."
Improving Textless Spoken Language Understanding with Discrete Units as  Intermediate Target,"['Guan-Wei Wu', 'Guan-Ting Lin', 'Shang-Wen Li', 'Hung-yi Lee']",http://arxiv.org/pdf/2305.18096v2.pdf,2023-05-29,"['cs.cl', 'eess.as']","  Spoken Language Understanding (SLU) is a task that aims to extract semanticinformation from spoken utterances. Previous research has made progress inend-to-end SLU by using paired speech-text data, such as pre-trained AutomaticSpeech Recognition (ASR) models or paired text as intermediate targets.However, acquiring paired transcripts is expensive and impractical forunwritten languages. On the other hand, Textless SLU extracts semanticinformation from speech without utilizing paired transcripts. However, theabsence of intermediate targets and training guidance for textless SLU oftenresults in suboptimal performance. In this work, inspired by thecontent-disentangled discrete units from self-supervised speech models, weproposed to use discrete units as intermediate guidance to improve textless SLUperformance. Our method surpasses the baseline method on five SLU benchmarkcorpora. Additionally, we find that unit guidance facilitates few-shot learningand enhances the model's ability to handle noise."
Few-shot Classification with Shrinkage Exemplars,"['Tao Zhang', 'Wu Huang']",http://arxiv.org/pdf/2305.18970v1.pdf,2023-05-30,['cs.cv'],"  Prototype is widely used to represent internal structure of category forfew-shot learning, which was proposed as a simple inductive bias to address theissue of overfitting. However, since prototype representation is normallyaveraged from individual samples, it cannot flexibly adjust the retentionability of sample differences that may leads to underfitting in some cases ofsample distribution. To address this problem, in this work, we proposeShrinkage Exemplar Networks (SENet) for few-shot classification. SENet balancesthe prototype representations (high-bias, low-variance) and examplerepresentations (low-bias, high-variance) using a shrinkage estimator, wherethe categories are represented by the embedings of samples that shrink to theirmean via spectral filtering. Furthermore, a shrinkage exemplar loss is proposedto replace the widely used cross entropy loss for capturing the information ofindividual shrinkage samples. Several experiments were conducted onminiImageNet, tiered-ImageNet and CIFAR-FS datasets. We demonstrate that ourproposed model is superior to the example model and the prototype model forsome tasks."
"What does the Failure to Reason with ""Respectively"" in Zero/Few-Shot  Settings Tell Us about Language Models?","['Ruixiang Cui', 'Seolhwa Lee', 'Daniel Hershcovich', 'Anders Søgaard']",http://arxiv.org/pdf/2305.19597v1.pdf,2023-05-31,"['cs.cl', 'cs.ai']","  Humans can effortlessly understand the coordinate structure of sentences suchas ""Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle,respectively"". In the context of natural language inference (NLI), we examinehow language models (LMs) reason with respective readings (Gawron and Kehler,2004) from two perspectives: syntactic-semantic and commonsense-worldknowledge. We propose a controlled synthetic dataset WikiResNLI and a naturallyoccurring dataset NatResNLI to encompass various explicit and implicitrealizations of ""respectively"". We show that fine-tuned NLI models strugglewith understanding such readings without explicit supervision. While few-shotlearning is easy in the presence of explicit cues, longer training is requiredwhen the reading is evoked implicitly, leaving models to rely on common senseinferences. Furthermore, our fine-grained analysis indicates models fail togeneralize across different constructions. To conclude, we demonstrate that LMsstill lag behind humans in generalizing to the long tail of linguisticconstructions."
Measuring the Robustness of Natural Language Processing Models to Domain  Shifts,"['Nitay Calderon', 'Naveh Porat', 'Eyal Ben-David', 'Zorik Gekhman', 'Nadav Oved', 'Roi Reichart']",http://arxiv.org/pdf/2306.00168v2.pdf,2023-05-31,['cs.cl'],"  Existing research on Domain Robustness (DR) suffers from disparate setups,lack of evaluation task variety, and reliance on challenge sets. In this paper,we pose a fundamental question: What is the state of affairs of the DRchallenge in the era of Large Language Models (LLMs)? To this end, we constructa DR benchmark comprising diverse NLP tasks, including sentence and token-levelclassification, QA, and generation, each task consists of several domains. Weexplore the DR challenge of fine-tuned and few-shot learning models in naturaldomain shift settings and devise two diagnostic metrics of Out-of-Distribution(OOD) performance degradation: The commonly used Source Drop (SD) and theoverlooked Target Drop (TD). Our findings reveal important insights: First,despite their capabilities, zero-to-few shot LLMs and fine-tuning approachesstill fail to meet satisfactory performance in the OOD context; Second, TDapproximates better than SD the average OOD degradation; Third, in asignificant proportion of domain shifts, either SD or TD is positive, but notboth, and therefore disregarding one can lead to incorrect DR conclusions."
Few-Shot Open-Set Learning for On-Device Customization of KeyWord  Spotting Systems,"['Manuele Rusci', 'Tinne Tuytelaars']",http://arxiv.org/pdf/2306.02161v1.pdf,2023-06-03,['cs.lg'],"  A personalized KeyWord Spotting (KWS) pipeline typically requires thetraining of a Deep Learning model on a large set of user-defined speechutterances, preventing fast customization directly applied on-device. To fillthis gap, this paper investigates few-shot learning methods for open-set KWSclassification by combining a deep feature encoder with a prototype-basedclassifier. With user-defined keywords from 10 classes of the Google SpeechCommand dataset, our study reports an accuracy of up to 76% in a 10-shotscenario while the false acceptance rate of unknown data is kept to 5%. In theanalyzed settings, the usage of the triplet loss to train an encoder withnormalized output features performs better than the prototypical networksjointly trained with a generator of dummy unknown-class prototypes. This designis also more effective than encoders trained on a classification problem andfeatures fewer parameters than other iso-accuracy approaches."
Human-like Few-Shot Learning via Bayesian Reasoning over Natural  Language,['Kevin Ellis'],http://arxiv.org/pdf/2306.02797v3.pdf,2023-06-05,"['cs.cl', 'cs.ai', 'cs.lg']","  A core tension in models of concept learning is that the model must carefullybalance the tractability of inference against the expressivity of thehypothesis class. Humans, however, can efficiently learn a broad range ofconcepts. We introduce a model of inductive learning that seeks to behuman-like in that sense. It implements a Bayesian reasoning process where alanguage model first proposes candidate hypotheses expressed in naturallanguage, which are then re-weighed by a prior and a likelihood. By estimatingthe prior from human data, we can predict human judgments on learning problemsinvolving numbers and sets, spanning concepts that are generative,discriminative, propositional, and higher-order."
Few Shot Rationale Generation using Self-Training with Dual Teachers,"['Aditya Srikanth Veerubhotla', 'Lahari Poddar', 'Jun Yin', 'György Szarvas', 'Sharanya Eswaran']",http://arxiv.org/pdf/2306.03315v1.pdf,2023-06-05,"['cs.cl', 'cs.ai']","  Self-rationalizing models that also generate a free-text explanation fortheir predicted labels are an important tool to build trustworthy AIapplications. Since generating explanations for annotated labels is a laboriousand costly pro cess, recent models rely on large pretrained language models(PLMs) as their backbone and few-shot learning. In this work we explore aself-training approach leveraging both labeled and unlabeled data to furtherimprove few-shot models, under the assumption that neither human writtenrationales nor annotated task labels are available at scale. We introduce anovel dual-teacher learning framework, which learns two specialized teachermodels for task prediction and rationalization using self-training and distillstheir knowledge into a multi-tasking student model that can jointly generatethe task label and rationale. Furthermore, we formulate a new loss function,Masked Label Regularization (MLR) which promotes explanations to be stronglyconditioned on predicted labels. Evaluation on three public datasetsdemonstrate that the proposed methods are effective in modeling task labels andgenerating faithful rationales."
GSHOT: Few-shot Generative Modeling of Labeled Graphs,"['Sahil Manchanda', 'Shubham Gupta', 'Sayan Ranu', 'Srikanta Bedathur']",http://arxiv.org/pdf/2306.03480v2.pdf,2023-06-06,"['cs.lg', 'cs.ai']","  Deep graph generative modeling has gained enormous attraction in recent yearsdue to its impressive ability to directly learn the underlying hidden graphdistribution. Despite their initial success, these techniques, like much of theexisting deep generative methods, require a large number of training samples tolearn a good model. Unfortunately, large number of training samples may notalways be available in scenarios such as drug discovery for rare diseases. Atthe same time, recent advances in few-shot learning have opened door toapplications where available training data is limited. In this work, weintroduce the hitherto unexplored paradigm of few-shot graph generativemodeling. Towards this, we develop GSHOT, a meta-learning based framework forfew-shot labeled graph generative modeling. GSHOT learns to transfermeta-knowledge from similar auxiliary graph datasets. Utilizing these priorexperiences, GSHOT quickly adapts to an unseen graph dataset through self-pacedfine-tuning. Through extensive experiments on datasets from diverse domainshaving limited training samples, we establish that GSHOT generates graphs ofsuperior fidelity compared to existing baselines."
A New Dataset and Empirical Study for Sentence Simplification in Chinese,"['Shiping Yang', 'Renliang Sun', 'Xiaojun Wan']",http://arxiv.org/pdf/2306.04188v1.pdf,2023-06-07,['cs.cl'],"  Sentence Simplification is a valuable technique that can benefit languagelearners and children a lot. However, current research focuses more on Englishsentence simplification. The development of Chinese sentence simplification isrelatively slow due to the lack of data. To alleviate this limitation, thispaper introduces CSS, a new dataset for assessing sentence simplification inChinese. We collect manual simplifications from human annotators and performdata analysis to show the difference between English and Chinese sentencesimplifications. Furthermore, we test several unsupervised and zero/few-shotlearning methods on CSS and analyze the automatic evaluation and humanevaluation results. In the end, we explore whether Large Language Models canserve as high-quality Chinese sentence simplification systems by evaluatingthem on CSS."
Improving neural network representations using human similarity  judgments,"['Lukas Muttenthaler', 'Lorenz Linhardt', 'Jonas Dippel', 'Robert A. Vandermeulen', 'Katherine Hermann', 'Andrew K. Lampinen', 'Simon Kornblith']",http://arxiv.org/pdf/2306.04507v2.pdf,2023-06-07,"['cs.cv', 'cs.lg']","  Deep neural networks have reached human-level performance on many computervision tasks. However, the objectives used to train these networks enforce onlythat similar images are embedded at similar locations in the representationspace, and do not directly constrain the global structure of the resultingspace. Here, we explore the impact of supervising this global structure bylinearly aligning it with human similarity judgments. We find that a naiveapproach leads to large changes in local representational structure that harmdownstream performance. Thus, we propose a novel method that aligns the globalstructure of representations while preserving their local structure. Thisglobal-local transform considerably improves accuracy across a variety offew-shot learning and anomaly detection tasks. Our results indicate that humanvisual representations are globally organized in a way that facilitateslearning from few examples, and incorporating this global structure into neuralnetwork representations improves performance on downstream tasks."
Can AI Moderate Online Communities?,"['Henrik Axelsen', 'Johannes Rude Jensen', 'Sebastian Axelsen', 'Valdemar Licht', 'Omri Ross']",http://arxiv.org/pdf/2306.05122v1.pdf,2023-06-08,['cs.cy'],"  The task of cultivating healthy communication in online communities becomesincreasingly urgent, as gaming and social media experiences becomeprogressively more immersive and life-like. We approach the challenge ofmoderating online communities by training student models using a large languagemodel (LLM). We use zero-shot learning models to distill and expand datasetsfollowed by a few-shot learning and a fine-tuning approach, leveragingopen-access generative pre-trained transformer models (GPT) from OpenAI. Ourpreliminary findings suggest, that when properly trained, LLMs can excel inidentifying actor intentions, moderating toxic comments, and rewarding positivecontributions. The student models perform above-expectation in non-contextualassignments such as identifying classically toxic behavior and performsufficiently on contextual assignments such as identifying positivecontributions to online discourse. Further, using open-access models likeOpenAI's GPT we experience a step-change in the development process for whathas historically been a complex modeling task. We contribute to the informationsystem (IS) discourse with a rapid development framework on the application ofgenerative AI in content online moderation and management of culture indecentralized, pseudonymous communities by providing a sample model suite ofindustrial-ready generative AI models based on open-access LLMs."
EMO: Episodic Memory Optimization for Few-Shot Meta-Learning,"['Yingjun Du', 'Jiayi Shen', 'Xiantong Zhen', 'Cees G. M. Snoek']",http://arxiv.org/pdf/2306.05189v3.pdf,2023-06-08,['cs.lg'],"  Few-shot meta-learning presents a challenge for gradient descent optimizationdue to the limited number of training samples per task. To address this issue,we propose an episodic memory optimization for meta-learning, we call EMO,which is inspired by the human ability to recall past learning experiences fromthe brain's memory. EMO retains the gradient history of past experienced tasksin external memory, enabling few-shot learning in a memory-augmented way. Bylearning to retain and recall the learning process of past training tasks, EMOnudges parameter updates in the right direction, even when the gradientsprovided by a limited number of examples are uninformative. We provetheoretically that our algorithm converges for smooth, strongly convexobjectives. EMO is generic, flexible, and model-agnostic, making it a simpleplug-and-play optimizer that can be seamlessly embedded into existingoptimization-based few-shot meta-learning approaches. Empirical results showthat EMO scales well with most few-shot classification benchmarks and improvesthe performance of optimization-based meta-learning methods, resulting inaccelerated convergence."
The ADAIO System at the BEA-2023 Shared Task on Generating AI Teacher  Responses in Educational Dialogues,"['Adaeze Adigwe', 'Zheng Yuan']",http://arxiv.org/pdf/2306.05360v1.pdf,2023-06-08,"['cs.cl', 'cs.ai', 'cs.cy']","  This paper presents the ADAIO team's system entry in the Building EducationalApplications (BEA) 2023 Shared Task on Generating AI Teacher Responses inEducational Dialogues. The task aims to assess the performance ofstate-of-the-art generative models as AI teachers in producing suitableresponses within a student-teacher dialogue. Our system comprises evaluatingvarious baseline models using OpenAI GPT-3 and designing diverse prompts toprompt the OpenAI models for teacher response generation. After the challenge,our system achieved second place by employing a few-shot prompt-based approachwith the OpenAI text-davinci-003 model. The results highlight the few-shotlearning capabilities of large-language models, particularly OpenAI's GPT-3, inthe role of AI teachers."
Prompt-based Extraction of Social Determinants of Health Using Few-shot  Learning,"['Giridhar Kaushik Ramachandran', 'Yujuan Fu', 'Bin Han', 'Kevin Lybarger', 'Nicholas J Dobbins', 'Özlem Uzuner', 'Meliha Yetisgen']",http://arxiv.org/pdf/2306.07170v1.pdf,2023-06-12,['cs.cl'],"  Social determinants of health (SDOH) documented in the electronic healthrecord through unstructured text are increasingly being studied to understandhow SDOH impacts patient health outcomes. In this work, we utilize the SocialHistory Annotation Corpus (SHAC), a multi-institutional corpus of de-identifiedsocial history sections annotated for SDOH, including substance use,employment, and living status information. We explore the automatic extractionof SDOH information with SHAC in both standoff and inline annotation formatsusing GPT-4 in a one-shot prompting setting. We compare GPT-4 extractionperformance with a high-performing supervised approach and perform thorougherror analyses. Our prompt-based GPT-4 method achieved an overall 0.652 F1 onthe SHAC test set, similar to the 7th best-performing system among all teams inthe n2c2 challenge with SHAC."
Rethink the Effectiveness of Text Data Augmentation: An Empirical  Analysis,"['Zhengxiang Shi', 'Aldo Lipani']",http://arxiv.org/pdf/2306.07664v1.pdf,2023-06-13,"['cs.cl', 'cs.ai', 'cs.lg']","  In recent years, language models (LMs) have made remarkable progress inadvancing the field of natural language processing (NLP). However, the impactof data augmentation (DA) techniques on the fine-tuning (FT) performance ofthese LMs has been a topic of ongoing debate. In this study, we evaluate theeffectiveness of three different FT methods in conjugation withback-translation across an array of 7 diverse NLP tasks, includingclassification and regression types, covering single-sentence and sentence-pairtasks. Contrary to prior assumptions that DA does not contribute to theenhancement of LMs' FT performance, our findings reveal that continuedpre-training on augmented data can effectively improve the FT performance ofthe downstream tasks. In the most favourable case, continued pre-trainingimproves the performance of FT by more than 10% in the few-shot learningsetting. Our finding highlights the potential of DA as a powerful tool forbolstering LMs' performance."
Inductive Linear Probing for Few-shot Node Classification,"['Hirthik Mathavan', 'Zhen Tan', 'Nivedh Mudiam', 'Huan Liu']",http://arxiv.org/pdf/2306.08192v1.pdf,2023-06-14,"['cs.lg', 'cs.si']","  Meta-learning has emerged as a powerful training strategy for few-shot nodeclassification, demonstrating its effectiveness in the transductive setting.However, the existing literature predominantly focuses on transductive few-shotnode classification, neglecting the widely studied inductive setting in thebroader few-shot learning community. This oversight limits our comprehensiveunderstanding of the performance of meta-learning based methods on graph data.In this work, we conduct an empirical study to highlight the limitations ofcurrent frameworks in the inductive few-shot node classification setting.Additionally, we propose a simple yet competitive baseline approachspecifically tailored for inductive few-shot node classification tasks. We hopeour work can provide a new path forward to better understand how themeta-learning paradigm works in the graph domain."
Neural Fine-Tuning Search for Few-Shot Learning,"['Panagiotis Eustratiadis', 'Łukasz Dudziak', 'Da Li', 'Timothy Hospedales']",http://arxiv.org/pdf/2306.09295v1.pdf,2023-06-15,"['cs.cv', 'cs.lg']","  In few-shot recognition, a classifier that has been trained on one set ofclasses is required to rapidly adapt and generalize to a disjoint, novel set ofclasses. To that end, recent studies have shown the efficacy of fine-tuningwith carefully crafted adaptation architectures. However this raises thequestion of: How can one design the optimal adaptation strategy? In this paper,we study this question through the lens of neural architecture search (NAS).Given a pre-trained neural network, our algorithm discovers the optimalarrangement of adapters, which layers to keep frozen and which to fine-tune. Wedemonstrate the generality of our NAS method by applying it to both residualnetworks and vision transformers and report state-of-the-art performance onMeta-Dataset and Meta-Album."
Channel-Spatial-Based Few-Shot Bird Sound Event Detection,"['Lingwen Liu', 'Yuxuan Feng', 'Haitao Fu', 'Yajie Yang', 'Xin Pan', 'Chenlei Jin']",http://arxiv.org/pdf/2306.10499v2.pdf,2023-06-18,"['eess.as', 'cs.sd']","  In this paper, we propose a model for bird sound event detection that focuseson a small number of training samples within the everyday long-taildistribution. As a result, we investigate bird sound detection using thefew-shot learning paradigm. By integrating channel and spatial attentionmechanisms, improved feature representations can be learned from few-shottraining datasets. We develop a Metric Channel-Spatial Network model byincorporating a Channel Spatial Squeeze-Excitation block into the prototypenetwork, combining it with these attention mechanisms. We evaluate the MetricChannel Spatial Network model on the DCASE 2022 Take5 dataset benchmark,achieving an F-measure of 66.84% and a PSDS of 58.98%. Our experimentdemonstrates that the combination of channel and spatial attention mechanismseffectively enhances the performance of bird sound classification anddetection."
Multilingual Few-Shot Learning via Language Model Retrieval,"['Genta Indra Winata', 'Liang-Kang Huang', 'Soumya Vadlamannati', 'Yash Chandarana']",http://arxiv.org/pdf/2306.10964v1.pdf,2023-06-19,['cs.cl'],"  Transformer-based language models have achieved remarkable success infew-shot in-context learning and drawn a lot of research interest. However,these models' performance greatly depends on the choice of the example promptsand also has high variability depending on how samples are chosen. In thispaper, we conduct a comprehensive study of retrieving semantically similarfew-shot samples and using them as the context, as it helps the model decidethe correct label without any gradient update in the multilingual andcross-lingual settings. We evaluate the proposed method on five naturallanguage understanding datasets related to intent detection, questionclassification, sentiment analysis, and topic classification. The proposedmethod consistently outperforms random sampling in monolingual andcross-lingual tasks in non-English languages."
Language models are weak learners,"['Hariharan Manikandan', 'Yiding Jiang', 'J Zico Kolter']",http://arxiv.org/pdf/2306.14101v1.pdf,2023-06-25,"['cs.lg', 'cs.ai']","  A central notion in practical and theoretical machine learning is that of a$\textit{weak learner}$, classifiers that achieve better-than-randomperformance (on any given distribution over data), even by a small margin. Suchweak learners form the practical basis for canonical machine learning methodssuch as boosting. In this work, we illustrate that prompt-based large languagemodels can operate effectively as said weak learners. Specifically, weillustrate the use of a large language model (LLM) as a weak learner in aboosting algorithm applied to tabular data. We show that by providing (properlysampled according to the distribution of interest) text descriptions of tabulardata samples, LLMs can produce a summary of the samples that serves as atemplate for classification and achieves the aim of acting as a weak learner onthis task. We incorporate these models into a boosting approach, which in somesettings can leverage the knowledge within the LLM to outperform traditionaltree-based boosting. The model outperforms both few-shot learning andoccasionally even more involved fine-tuning procedures, particularly for tasksinvolving small numbers of data points. The results illustrate the potentialfor prompt-based LLMs to function not just as few-shot learners themselves, butas components of larger machine learning pipelines."
RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated  Adversarial Perturbations,"['Yilun Zhao', 'Chen Zhao', 'Linyong Nan', 'Zhenting Qi', 'Wenlin Zhang', 'Xiangru Tang', 'Boyu Mi', 'Dragomir Radev']",http://arxiv.org/pdf/2306.14321v1.pdf,2023-06-25,"['cs.cl', 'cs.ai']","  Despite significant progress having been made in question answering ontabular data (Table QA), it's unclear whether, and to what extent existingTable QA models are robust to task-specific perturbations, e.g., replacing keyquestion entities or shuffling table columns. To systematically study therobustness of Table QA models, we propose a benchmark called RobuT, whichbuilds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) andincludes human-annotated adversarial perturbations in terms of table header,table content, and question. Our results indicate that both state-of-the-artTable QA models and large language models (e.g., GPT-3) with few-shot learningfalter in these adversarial sets. We propose to address this problem by usinglarge language models to generate adversarial examples to enhance training,which significantly improves the robustness of Table QA models. Our data andcode is publicly available at https://github.com/yilunzhao/RobuT."
Benchmarking Large Language Model Capabilities for Conditional  Generation,"['Joshua Maynez', 'Priyanka Agrawal', 'Sebastian Gehrmann']",http://arxiv.org/pdf/2306.16793v1.pdf,2023-06-29,['cs.cl'],"  Pre-trained large language models (PLMs) underlie most new developments innatural language processing. They have shifted the field fromapplication-specific model pipelines to a single model that is adapted to awide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongsidetechniques like few-shot learning, have additionally shifted the outputmodality to generation instead of classification or regression. Despite theirubiquitous use, the generation quality of language models is rarely evaluatedwhen these models are introduced. Additionally, it is unclear how existinggeneration tasks--while they can be used to compare systems at a highlevel--relate to the real world use cases for which people have been adoptingthem. In this work, we discuss how to adapt existing application-specificgeneration benchmarks to PLMs and provide an in-depth, empirical study of thelimitations and capabilities of PLMs in natural language generation tasks alongdimensions such as scale, architecture, input and output language. Our resultsshow that PLMs differ in their applicability to different data regimes andtheir generalization to multiple languages and inform which PLMs to use for agiven generation task setup. We share best practices to be taken intoconsideration when benchmarking generation capabilities during the developmentof upcoming PLMs."
On Conditional and Compositional Language Model Differentiable Prompting,"['Jonathan Pilault', 'Can Liu', 'Mohit Bansal', 'Markus Dreyer']",http://arxiv.org/pdf/2307.01446v1.pdf,2023-07-04,"['cs.cl', 'cs.lg']","  Prompts have been shown to be an effective method to adapt a frozenPretrained Language Model (PLM) to perform well on downstream tasks. Promptscan be represented by a human-engineered word sequence or by a learnedcontinuous embedding. In this work, we investigate conditional andcompositional differentiable prompting. We propose a new model, PromptProduction System (PRopS), which learns to transform task instructions or inputmetadata, into continuous prompts that elicit task-specific outputs from thePLM. Our model uses a modular network structure based on our neural formulationof Production Systems, which allows the model to learn discrete rules -- neuralfunctions that learn to specialize in transforming particular prompt inputpatterns, making it suitable for compositional transfer learning and few-shotlearning. We present extensive empirical and theoretical analysis and show thatPRopS consistently surpasses other PLM adaptation techniques, and oftenimproves upon fully fine-tuned models, on compositional generalization tasks,controllable summarization and multilingual translation, while needing fewertrainable parameters."
Diverse Retrieval-Augmented In-Context Learning for Dialogue State  Tracking,"['Brendan King', 'Jeffrey Flanigan']",http://arxiv.org/pdf/2307.01453v1.pdf,2023-07-04,['cs.cl'],"  There has been significant interest in zero and few-shot learning fordialogue state tracking (DST) due to the high cost of collecting and annotatingtask-oriented dialogues. Recent work has demonstrated that in-context learningrequires very little data and zero parameter updates, and even outperformstrained methods in the few-shot setting (Hu et al. 2022). We propose RefPyDST,which advances the state of the art with three advancements to in-contextlearning for DST. First, we formulate DST as a Python programming task,explicitly modeling language coreference as variable reference in Python.Second, since in-context learning depends highly on the context examples, wepropose a method to retrieve a diverse set of relevant examples to improveperformance. Finally, we introduce a novel re-weighting method during decodingthat takes into account probabilities of competing surface forms, and producesa more accurate dialogue state prediction. We evaluate our approach usingMultiWOZ and achieve state-of-the-art multi-domain joint-goal accuracy in zeroand few-shot settings."
Optimal and Efficient Binary Questioning for Human-in-the-Loop  Annotation,"['Franco Marchesoni-Acland', 'Jean-Michel Morel', 'Josselin Kherroubi', 'Gabriele Facciolo']",http://arxiv.org/pdf/2307.01578v1.pdf,2023-07-04,"['cs.lg', 'cs.ai', 'cs.cv', 'cs.hc', 'cs.it', 'math.it']","  Even though data annotation is extremely important for interpretability,research and development of artificial intelligence solutions, most researchefforts such as active learning or few-shot learning focus on the sampleefficiency problem. This paper studies the neglected complementary problem ofgetting annotated data given a predictor. For the simple binary classificationsetting, we present the spectrum ranging from optimal general solutions topractical efficient methods. The problem is framed as the full annotation of abinary classification dataset with the minimal number of yes/no questions whena predictor is available. For the case of general binary questions the solutionis found in coding theory, where the optimal questioning strategy is given bythe Huffman encoding of the possible labelings. However, this approach iscomputationally intractable even for small dataset sizes. We propose analternative practical solution based on several heuristics and lookaheadminimization of proxy cost functions. The proposed solution is analysed,compared with optimal solutions and evaluated on several synthetic andreal-world datasets. On these datasets, the method allows a significantimprovement ($23-86\%$) in annotation efficiency."
Feature Activation Map: Visual Explanation of Deep Learning Models for  Image Classification,"['Yi Liao', 'Yongsheng Gao', 'Weichuan Zhang']",http://arxiv.org/pdf/2307.05017v1.pdf,2023-07-11,"['cs.cv', 'cs.ai', 'cs.lg', 'cs.pf']","  Decisions made by convolutional neural networks(CNN) can be understood andexplained by visualizing discriminative regions on images. To this end, ClassActivation Map (CAM) based methods were proposed as powerful interpretationtools, making the prediction of deep learning models more explainable,transparent, and trustworthy. However, all the CAM-based methods (e.g., CAM,Grad-CAM, and Relevance-CAM) can only be used for interpreting CNN models withfully-connected (FC) layers as a classifier. It is worth noting that many deeplearning models classify images without FC layers, e.g., few-shot learningimage classification, contrastive learning image classification, and imageretrieval tasks. In this work, a post-hoc interpretation tool named featureactivation map (FAM) is proposed, which can interpret deep learning modelswithout FC layers as a classifier. In the proposed FAM algorithm, thechannel-wise contribution weights are derived from the similarity scoresbetween two image embeddings. The activation maps are linearly combined withthe corresponding normalized contribution weights, forming the explanation mapfor visualization. The quantitative and qualitative experiments conducted onten deep learning models for few-shot image classification, contrastivelearning image classification and image retrieval tasks demonstrate theeffectiveness of the proposed FAM algorithm."
Generating Efficient Training Data via LLM-based Attribute Manipulation,"['Letian Peng', 'Yuwei Zhang', 'Jingbo Shang']",http://arxiv.org/pdf/2307.07099v1.pdf,2023-07-14,['cs.cl'],"  In this paper, we propose a novel method, Chain-of-Thoughts AttributeManipulation (CoTAM), to guide few-shot learning by carefully crafted data fromLarge Language Models (LLMs). The main idea is to create data with changes onlyin the attribute targeted by the task. Inspired by facial attributemanipulation, our approach generates label-switched data by leveraging LLMs tomanipulate task-specific attributes and reconstruct new sentences in acontrolled manner. Instead of conventional latent representation controlling,we implement chain-of-thoughts decomposition and reconstruction to adapt theprocedure to LLMs. Extensive results on text classification and other tasksverify the advantage of CoTAM over other LLM-based text generation methods withthe same number of training examples. Analysis visualizes the attributemanipulation effectiveness of CoTAM and presents the potential of LLM-guidedlearning with even less supervision."
Learning to Sample Tasks for Meta Learning,"['Jingyao Wang', 'Zeen Song', 'Xingzhe Su', 'Lingyu Si', 'Hongwei Dong', 'Wenwen Qiang', 'Changwen Zheng']",http://arxiv.org/pdf/2307.08924v2.pdf,2023-07-18,"['cs.lg', 'cs.cv']","  Through experiments on various meta-learning methods, task samplers, andfew-shot learning tasks, this paper arrives at three conclusions. Firstly,there are no universal task sampling strategies to guarantee the performance ofmeta-learning models. Secondly, task diversity can cause the models to eitherunderfit or overfit during training. Lastly, the generalization performance ofthe models are influenced by task divergence, task entropy, and taskdifficulty. In response to these findings, we propose a novel task samplercalled Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takestask divergence, task entropy, and task difficulty to sample tasks. To optimizeASr, we rethink and propose a simple and general meta-learning algorithm.Finally, a large number of empirical experiments demonstrate the effectivenessof the proposed ASr."
Overthinking the Truth: Understanding how Language Models Process False  Demonstrations,"['Danny Halawi', 'Jean-Stanislas Denain', 'Jacob Steinhardt']",http://arxiv.org/pdf/2307.09476v1.pdf,2023-07-18,"['cs.lg', 'cs.ai', 'cs.cl']","  Modern language models can imitate complex patterns through few-shotlearning, enabling them to complete challenging tasks without fine-tuning.However, imitation can also lead models to reproduce inaccuracies or harmfulcontent if present in the context. We study harmful imitation through the lensof a model's internal representations, and identify two related phenomena:overthinking and false induction heads. The first phenomenon, overthinking,appears when we decode predictions from intermediate layers, given correct vs.incorrect few-shot demonstrations. At early layers, both demonstrations inducesimilar model behavior, but the behavior diverges sharply at some ""criticallayer"", after which the accuracy given incorrect demonstrations progressivelydecreases. The second phenomenon, false induction heads, are a possiblemechanistic cause of overthinking: these are heads in late layers that attendto and copy false information from previous demonstrations, and whose ablationreduces overthinking. Beyond scientific understanding, our results suggest thatstudying intermediate model computations could be a promising avenue forunderstanding and guarding against harmful model behaviors."
Does Correction Remain A Problem For Large Language Models?,"['Xiaowu Zhang', 'Xiaotian Zhang', 'Cheng Yang', 'Hang Yan', 'Xipeng Qiu']",http://arxiv.org/pdf/2308.01776v2.pdf,2023-08-03,['cs.cl'],"  As large language models, such as GPT, continue to advance the capabilitiesof natural language processing (NLP), the question arises: does the problem ofcorrection still persist? This paper investigates the role of correction in thecontext of large language models by conducting two experiments. The firstexperiment focuses on correction as a standalone task, employing few-shotlearning techniques with GPT-like models for error correction. The secondexperiment explores the notion of correction as a preparatory task for otherNLP tasks, examining whether large language models can tolerate and performadequately on texts containing certain levels of noise or errors. By addressingthese experiments, we aim to shed light on the significance of correction inthe era of large language models and its implications for various NLPapplications."
Thespian: Multi-Character Text Role-Playing Game Agents,"['Christopher Cui', 'Xiangyu Peng', 'Mark Riedl']",http://arxiv.org/pdf/2308.01872v1.pdf,2023-08-03,"['cs.ai', 'cs.cl']","  Text-adventure games and text role-playing games are grand challenges forreinforcement learning game playing agents. Text role-playing games areopen-ended environments where an agent must faithfully play a particularcharacter. We consider the distinction between characters and actors, where anactor agent has the ability to play multiple characters. We present a frameworkwe call a thespian agent that can learn to emulate multiple characters alongwith a soft prompt that can be used to direct it as to which character to playat any time. We further describe an attention mechanism that allows the agentto learn new characters that are based on previously learned characters in afew-shot fashion. We show that our agent outperforms the state of the art agentframework in multi-character learning and few-shot learning."
Meta-learning in healthcare: A survey,"['Alireza Rafiei', 'Ronald Moore', 'Sina Jahromi', 'Farshid Hajati', 'Rishikesan Kamaleswaran']",http://arxiv.org/pdf/2308.02877v1.pdf,2023-08-05,"['cs.lg', 'cs.ai']","  As a subset of machine learning, meta-learning, or learning to learn, aims atimproving the model's capabilities by employing prior knowledge and experience.A meta-learning paradigm can appropriately tackle the conventional challengesof traditional learning approaches, such as insufficient number of samples,domain shifts, and generalization. These unique characteristics positionmeta-learning as a suitable choice for developing influential solutions invarious healthcare contexts, where the available data is often insufficient,and the data collection methodologies are different. This survey discussesmeta-learning broad applications in the healthcare domain to provide insightinto how and where it can address critical healthcare challenges. We firstdescribe the theoretical foundations and pivotal methods of meta-learning. Wethen divide the employed meta-learning approaches in the healthcare domain intotwo main categories of multi/single-task learning and many/few-shot learningand survey the studies. Finally, we highlight the current challenges inmeta-learning research, discuss the potential solutions and provide futureperspectives on meta-learning in healthcare."
AutoConv: Automatically Generating Information-seeking Conversations  with Large Language Models,"['Siheng Li', 'Cheng Yang', 'Yichun Yin', 'Xinyu Zhu', 'Zesen Cheng', 'Lifeng Shang', 'Xin Jiang', 'Qun Liu', 'Yujiu Yang']",http://arxiv.org/pdf/2308.06507v1.pdf,2023-08-12,['cs.cl'],"  Information-seeking conversation, which aims to help users gather informationthrough conversation, has achieved great progress in recent years. However, theresearch is still stymied by the scarcity of training data. To alleviate thisproblem, we propose AutoConv for synthetic conversation generation, which takesadvantage of the few-shot learning ability and generation capacity of largelanguage models (LLM). Specifically, we formulate the conversation generationproblem as a language modeling task, then finetune an LLM with a few humanconversations to capture the characteristics of the information-seeking processand use it for generating synthetic conversations with high quality.Experimental results on two frequently-used datasets verify that AutoConv hassubstantial improvements over strong baselines and alleviates the dependence onhuman annotation. In addition, we also provide several analysis studies topromote future research."
Few-shot Class-incremental Learning: A Survey,"['Jinghua Zhang', 'Li Liu', 'Olli Silven', 'Matti Pietikäinen', 'Dewen Hu']",http://arxiv.org/pdf/2308.06764v1.pdf,2023-08-13,"['cs.lg', 'cs.ai']","  Few-shot Class-Incremental Learning (FSCIL) presents a unique challenge inmachine learning, as it necessitates the continuous learning of new classesfrom sparse labeled training samples without forgetting previous knowledge.While this field has seen recent progress, it remains an active area ofexploration. This paper aims to provide a comprehensive and systematic reviewof FSCIL. In our in-depth examination, we delve into various facets of FSCIL,encompassing the problem definition, the discussion of primary challenges ofunreliable empirical risk minimization and the stability-plasticity dilemma,general schemes, and relevant problems of incremental learning and few-shotlearning. Besides, we offer an overview of benchmark datasets and evaluationmetrics. Furthermore, we introduce the classification methods in FSCIL fromdata-based, structure-based, and optimization-based approaches and the objectdetection methods in FSCIL from anchor-free and anchor-based approaches. Beyondthese, we illuminate several promising research directions within FSCIL thatmerit further investigation."
Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation,"['William Shen', 'Ge Yang', 'Alan Yu', 'Jansen Wong', 'Leslie Pack Kaelbling', 'Phillip Isola']",http://arxiv.org/pdf/2308.07931v1.pdf,2023-07-27,"['cs.cv', 'cs.ai', 'cs.cl', 'cs.lg', 'cs.ro']","  Self-supervised and language-supervised image models contain rich knowledgeof the world that is important for generalization. Many robotic tasks, however,require a detailed understanding of 3D geometry, which is often lacking in 2Dimage features. This work bridges this 2D-to-3D gap for robotic manipulation byleveraging distilled feature fields to combine accurate 3D geometry with richsemantics from 2D foundation models. We present a few-shot learning method for6-DOF grasping and placing that harnesses these strong spatial and semanticpriors to achieve in-the-wild generalization to unseen objects. Using featuresdistilled from a vision-language model, CLIP, we present a way to designatenovel objects for manipulation via free-text natural language, and demonstrateits ability to generalize to unseen expressions and novel categories ofobjects."
Knowledge-Enhanced Multi-Label Few-Shot Product Attribute-Value  Extraction,"['Jiaying Gong', 'Wei-Te Chen', 'Hoda Eldardiry']",http://arxiv.org/pdf/2308.08413v1.pdf,2023-08-16,"['cs.ir', 'cs.cl']","  Existing attribute-value extraction (AVE) models require large quantities oflabeled data for training. However, new products with new attribute-value pairsenter the market every day in real-world e-Commerce. Thus, we formulate AVE inmulti-label few-shot learning (FSL), aiming to extract unseen attribute valuepairs based on a small number of training examples. We propose aKnowledge-Enhanced Attentive Framework (KEAF) based on prototypical networks,leveraging the generated label description and category information to learnmore discriminative prototypes. Besides, KEAF integrates with hybrid attentionto reduce noise and capture more informative semantics for each class bycalculating the label-relevant and query-related weights. To achievemulti-label inference, KEAF further learns a dynamic threshold by integratingthe semantic information from both the support set and the query set. Extensiveexperiments with ablation studies conducted on two datasets demonstrate thatKEAF outperforms other SOTA models for information extraction in FSL. The codecan be found at: https://github.com/gjiaying/KEAF"
Refashioning Emotion Recognition Modelling: The Advent of Generalised  Large Models,"['Zixing Zhang', 'Liyizhe Peng', 'Tao Pang', 'Jing Han', 'Huan Zhao', 'Bjorn W. Schuller']",http://arxiv.org/pdf/2308.11578v1.pdf,2023-08-21,"['cs.cl', 'cs.ai', 'cs.lg']","  After the inception of emotion recognition or affective computing, it hasincreasingly become an active research topic due to its broad applications.Over the past couple of decades, emotion recognition models have graduallymigrated from statistically shallow models to neural network-based deep models,which can significantly boost the performance of emotion recognition models andconsistently achieve the best results on different benchmarks. Therefore, inrecent years, deep models have always been considered the first option foremotion recognition. However, the debut of large language models (LLMs), suchas ChatGPT, has remarkably astonished the world due to their emergedcapabilities of zero/few-shot learning, in-context learning, chain-of-thought,and others that are never shown in previous deep models. In the present paper,we comprehensively investigate how the LLMs perform in emotion recognition interms of diverse aspects, including in-context learning, few-short learning,accuracy, generalisation, and explanation. Moreover, we offer some insights andpose other potential challenges, hoping to ignite broader discussions aboutenhancing emotion recognition in the new era of advanced and generalised largemodels."
Compressor-Based Classification for Atrial Fibrillation Detection,"['Nikita Markov', 'Konstantin Ushenin', 'Yakov Bozhko', 'Olga Solovyova']",http://arxiv.org/pdf/2308.13328v2.pdf,2023-08-25,"['eess.sp', 'cs.lg', '92c55, 68t10, 68t99,', 'j.3; g.3']","  Atrial fibrillation (AF) is one of the most common arrhythmias withchallenging public health implications. Therefore, automatic detection of AFepisodes on ECG is one of the essential tasks in biomedical engineering. Inthis paper, we applied the recently introduced method of compressor-based textclassification with gzip algorithm for AF detection (binary classificationbetween heart rhythms). We investigated the normalized compression distanceapplied to RR-interval and $\Delta$RR-interval sequences ($\Delta$RR-intervalis the difference between subsequent RR-intervals). Here, the configuration ofthe k-nearest neighbour classifier, an optimal window length, and the choice ofdata types for compression were analyzed. We achieved good classificationresults while learning on the full MIT-BIH Atrial Fibrillation database, closeto the best specialized AF detection algorithms (avg. sensitivity = 97.1\%,avg. specificity = 91.7\%, best sensitivity of 99.8\%, best specificity of97.6\% with fivefold cross-validation). In addition, we evaluated theclassification performance under the few-shot learning setting. Our resultssuggest that gzip compression-based classification, originally proposed fortexts, is suitable for biomedical data and quantized continuous stochasticsequences in general."
Semi-Supervised SAR ATR Framework with Transductive Auxiliary  Segmentation,"['Chenwei Wang', 'Xiaoyu Liu', 'Yulin Huang', 'Siyi Luo', 'Jifang Pei', 'Jianyu Yang', 'Deqing Mao']",http://arxiv.org/pdf/2308.16633v1.pdf,2023-08-31,['cs.cv'],"  Convolutional neural networks (CNNs) have achieved high performance insynthetic aperture radar (SAR) automatic target recognition (ATR). However, theperformance of CNNs depends heavily on a large amount of training data. Theinsufficiency of labeled training SAR images limits the recognition performanceand even invalidates some ATR methods. Furthermore, under few labeled trainingdata, many existing CNNs are even ineffective. To address these challenges, wepropose a Semi-supervised SAR ATR Framework with transductive AuxiliarySegmentation (SFAS). The proposed framework focuses on exploiting thetransductive generalization on available unlabeled samples with an auxiliaryloss serving as a regularizer. Through auxiliary segmentation of unlabeled SARsamples and information residue loss (IRL) in training, the framework canemploy the proposed training loop process and gradually exploit the informationcompilation of recognition and segmentation to construct a helpful inductivebias and achieve high performance. Experiments conducted on the MSTAR datasethave shown the effectiveness of our proposed SFAS for few-shot learning. Therecognition performance of 94.18\% can be achieved under 20 training samples ineach class with simultaneous accurate segmentation results. Facing variances ofEOCs, the recognition ratios are higher than 88.00\% when 10 training sampleseach class."
Adaptive Parametric Prototype Learning for Cross-Domain Few-Shot  Classification,"['Marzi Heidari', 'Abdullah Alchihabi', 'Qing En', 'Yuhong Guo']",http://arxiv.org/pdf/2309.01342v1.pdf,2023-09-04,"['cs.cv', 'cs.lg']","  Cross-domain few-shot classification induces a much more challenging problemthan its in-domain counterpart due to the existence of domain shifts betweenthe training and test tasks. In this paper, we develop a novel AdaptiveParametric Prototype Learning (APPL) method under the meta-learning conventionfor cross-domain few-shot classification. Different from existing prototypicalfew-shot methods that use the averages of support instances to calculate theclass prototypes, we propose to learn class prototypes from the concatenatedfeatures of the support set in a parametric fashion and meta-learn the model byenforcing prototype-based regularization on the query set. In addition, wefine-tune the model in the target domain in a transductive manner using aweighted-moving-average self-training approach on the query instances. Weconduct experiments on multiple cross-domain few-shot benchmark datasets. Theempirical results demonstrate that APPL yields superior performance than manystate-of-the-art cross-domain few-shot learning methods."
Few-Shot Medical Image Segmentation via a Region-enhanced Prototypical  Transformer,"['Yazhou Zhu', 'Shidong Wang', 'Tong Xin', 'Haofeng Zhang']",http://arxiv.org/pdf/2309.04825v1.pdf,2023-09-09,['cs.cv'],"  Automated segmentation of large volumes of medical images is often plagued bythe limited availability of fully annotated data and the diversity of organsurface properties resulting from the use of different acquisition protocolsfor different patients. In this paper, we introduce a more promising few-shotlearning-based method named Region-enhanced Prototypical Transformer (RPT) tomitigate the effects of large intra-class diversity/bias. First, a subdivisionstrategy is introduced to produce a collection of regional prototypes from theforeground of the support prototype. Second, a self-selection mechanism isproposed to incorporate into the Bias-alleviated Transformer (BaT) block tosuppress or remove interferences present in the query prototype and regionalsupport prototypes. By stacking BaT blocks, the proposed RPT can iterativelyoptimize the generated regional prototypes and finally produce rectified andmore accurate global prototypes for Few-Shot Medical Image Segmentation (FSMS).Extensive experiments are conducted on three publicly available medical imagedatasets, and the obtained results show consistent improvements compared tostate-of-the-art FSMS methods. The source code is available at:https://github.com/YazhouZhu19/RPT."
Gpachov at CheckThat! 2023: A Diverse Multi-Approach Ensemble for  Subjectivity Detection in News Articles,"['Georgi Pachov', 'Dimitar Dimitrov', 'Ivan Koychev', 'Preslav Nakov']",http://arxiv.org/pdf/2309.06844v1.pdf,2023-09-13,"['cs.cl', 'cs.ai', 'cs.mm']","  The wide-spread use of social networks has given rise to subjective,misleading, and even false information on the Internet. Thus, subjectivitydetection can play an important role in ensuring the objectiveness and thequality of a piece of information. This paper presents the solution built bythe Gpachov team for the CLEF-2023 CheckThat! lab Task~2 on subjectivitydetection. Three different research directions are explored. The first one isbased on fine-tuning a sentence embeddings encoder model and dimensionalityreduction. The second one explores a sample-efficient few-shot learning model.The third one evaluates fine-tuning a multilingual transformer on an altereddataset, using data from multiple languages. Finally, the three approaches arecombined in a simple majority voting ensemble, resulting in 0.77 macro F1 onthe test set and achieving 2nd place on the English subtask."
Regularized Contrastive Pre-training for Few-shot Bioacoustic Sound  Detection,"['Ilyass Moummad', 'Romain Serizel', 'Nicolas Farrugia']",http://arxiv.org/pdf/2309.08971v1.pdf,2023-09-16,"['cs.sd', 'cs.lg', 'eess.as']","  Bioacoustic sound event detection allows for better understanding of animalbehavior and for better monitoring biodiversity using audio. Deep learningsystems can help achieve this goal, however it is difficult to acquiresufficient annotated data to train these systems from scratch. To address thislimitation, the Detection and Classification of Acoustic Scenes and Events(DCASE) community has recasted the problem within the framework of few-shotlearning and organize an annual challenge for learning to detect animal soundsfrom only five annotated examples. In this work, we regularize supervisedcontrastive pre-training to learn features that can transfer well on new targettasks with animal sounds unseen during training, achieving a high F-score of61.52%(0.48) when no feature adaptation is applied, and an F-score of68.19%(0.75) when we further adapt the learned features for each new targettask. This work aims to lower the entry bar to few-shot bioacoustic sound eventdetection by proposing a simple and yet effective framework for this task, byalso providing open-source code."
Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of  the Same Coin,"['Gabriel Moreira', 'Manuel Marques', 'João Paulo Costeira', 'Alexander Hauptmann']",http://arxiv.org/pdf/2309.10013v1.pdf,2023-09-18,"['cs.cv', 'cs.lg']","  Recent research in representation learning has shown that hierarchical datalends itself to low-dimensional and highly informative representations inhyperbolic space. However, even if hyperbolic embeddings have gatheredattention in image recognition, their optimization is prone to numericalhurdles. Further, it remains unclear which applications stand to benefit themost from the implicit bias imposed by hyperbolicity, when compared totraditional Euclidean features. In this paper, we focus on prototypicalhyperbolic neural networks. In particular, the tendency of hyperbolicembeddings to converge to the boundary of the Poincar\'e ball in highdimensions and the effect this has on few-shot classification. We show that thebest few-shot results are attained for hyperbolic embeddings at a commonhyperbolic radius. In contrast to prior benchmark results, we demonstrate thatbetter performance can be achieved by a fixed-radius encoder equipped with theEuclidean metric, regardless of the embedding dimension."
"An Empathy-Based Sandbox Approach to Bridge Attitudes, Goals, Knowledge,  and Behaviors in the Privacy Paradox","['Chaoran Chen', 'Weijun Li', 'Wenxin Song', 'Yanfang Ye', 'Yaxing Yao', 'Toby Jia-jun Li']",http://arxiv.org/pdf/2309.14510v1.pdf,2023-09-25,['cs.hc'],"  The ""privacy paradox"" describes the discrepancy between users' privacyattitudes and their actual behaviors. Mitigating this discrepancy requiressolutions that account for both system opaqueness and users' hesitations intesting different privacy settings due to fears of unintended data exposure. Weintroduce an empathy-based approach that allows users to experience how privacybehaviors may alter system outcomes in a risk-free sandbox environment from theperspective of artificially generated personas. To generate realistic personas,we introduce a novel pipeline that augments the outputs of large languagemodels using few-shot learning, contextualization, and chain of thoughts. Ourempirical studies demonstrated the adequate quality of generated personas andhighlighted the changes in privacy-related applications (e.g., onlineadvertising) caused by different personas. Furthermore, users demonstratedcognitive and emotional empathy towards the personas when interacting with oursandbox. We offered design implications for downstream applications inimproving user privacy literacy and promoting behavior changes."
Boosting In-Context Learning with Factual Knowledge,"['Jianing Wang', 'Chengyu Wang', 'Chuanqi Tan', 'Jun Huang', 'Ming Gao']",http://arxiv.org/pdf/2309.14771v1.pdf,2023-09-26,"['cs.cl', 'cs.ai']","  In-Context Learning (ICL) over Large language models (LLMs) aims at solvingpreviously unseen tasks by conditioning on a few training examples, eliminatingthe need for parameter updates and achieving competitive performance. In thispaper, we demonstrate that factual knowledge is imperative for the performanceof ICL in three core facets, i.e., the inherent knowledge learned in LLMs, thefactual knowledge derived from the selected in-context examples, and theknowledge biases in LLMs for output generation. To unleash the power of LLMs infew-shot learning scenarios, we introduce a novel Knowledgeable In-ContextTuning (KICT) framework to further improve the performance of ICL: 1) injectingfactual knowledge to LLMs during continual self-supervised pre-training, 2)judiciously selecting the examples with high knowledge relevance, and 3)calibrating the prediction results based on prior knowledge. We evaluate theproposed approaches on auto-regressive LLMs (e.g., GPT-style models) overmultiple text classification and question answering tasks. Experimental resultsdemonstrate that KICT substantially outperforms strong baselines, and improvesby more than 13% and 7% of accuracy on text classification and questionanswering tasks, respectively."
Recurrent Hypernetworks are Surprisingly Strong in Meta-RL,"['Jacob Beck', 'Risto Vuorio', 'Zheng Xiong', 'Shimon Whiteson']",http://arxiv.org/pdf/2309.14970v3.pdf,2023-09-26,"['cs.lg', 'cs.ai', 'cs.ro']","  Deep reinforcement learning (RL) is notoriously impractical to deploy due tosample inefficiency. Meta-RL directly addresses this sample inefficiency bylearning to perform few-shot learning when a distribution of related tasks isavailable for meta-training. While many specialized meta-RL methods have beenproposed, recent work suggests that end-to-end learning in conjunction with anoff-the-shelf sequential model, such as a recurrent network, is a surprisinglystrong baseline. However, such claims have been controversial due to limitedsupporting evidence, particularly in the face of prior work establishingprecisely the opposite. In this paper, we conduct an empirical investigation.While we likewise find that a recurrent network can achieve strong performance,we demonstrate that the use of hypernetworks is crucial to maximizing theirpotential. Surprisingly, when combined with hypernetworks, the recurrentbaselines that are far simpler than existing specialized methods actuallyachieve the strongest performance of all methods evaluated."
Robust Internal Representations for Domain Generalization,['Mohammad Rostami'],http://arxiv.org/pdf/2309.15522v1.pdf,2023-09-27,"['cs.lg', 'cs.ai']","  This paper which is part of the New Faculty Highlights Invited SpeakerProgram of AAAI'23, serves as a comprehensive survey of my research in transferlearning by utilizing embedding spaces. The work reviewed in this paperspecifically revolves around the inherent challenges associated with continuallearning and limited availability of labeled data. By providing an overview ofmy past and ongoing contributions, this paper aims to present a holisticunderstanding of my research, paving the way for future explorations andadvancements in the field. My research delves into the various settings oftransfer learning, including, few-shot learning, zero-shot learning, continuallearning, domain adaptation, and distributed learning. I hope this surveyprovides a forward-looking perspective for researchers who would like to focuson similar research directions."
Self-Supervised Open-Ended Classification with Small Visual Language  Models,"['Mohammad Mahdi Derakhshani', 'Ivona Najdenkoska', 'Cees G. M. Snoek', 'Marcel Worring', 'Yuki M. Asano']",http://arxiv.org/pdf/2310.00500v2.pdf,2023-09-30,['cs.cv'],"  We present Self-Context Adaptation (SeCAt), a self-supervised approach thatunlocks few-shot abilities for open-ended classification with small visuallanguage models. Our approach imitates image captions in a self-supervised waybased on clustering a large pool of images followed by assigningsemantically-unrelated names to clusters. By doing so, we construct a trainingsignal consisting of interleaved sequences of image and pseudocaption pairs anda query image, which we denote as the 'self-context' sequence. Based on thissignal the model is trained to produce the right pseudo-caption. We demonstratethe performance and flexibility of SeCAt on several multimodal few-shotdatasets, spanning various granularities. By using models with approximately 1Bparameters we outperform the few-shot abilities of much larger models, such asFrozen and FROMAGe. SeCAt opens new possibilities for research and applicationsin open-ended few-shot learning that otherwise requires access to large orproprietary models."
Hierarchical Adaptation with Hypernetworks for Few-shot Molecular  Property Prediction,"['Shiguang Wu', 'Yaqing Wang', 'Quanming Yao']",http://arxiv.org/pdf/2310.00614v1.pdf,2023-10-01,"['cs.lg', 'cs.ai']","  Molecular property prediction (MPP) is important in biomedical applications,which naturally suffers from a lack of labels, thus forming a few-shot learningproblem. State-of-the-art approaches are usually based on gradient-based metalearning strategy, which ignore difference in model parameter and molecule'slearning difficulty. To address above problems, we propose a novel hierarchicaladaptation mechanism for few-shot MPP (HiMPP). The model follows aencoder-predictor framework. First, to make molecular representationproperty-adaptive, we selectively adapt encoder's parameter by designing ahypernetwork to modulate node embeddings during message propagation. Next, wemake molecule-level adaptation by design another hypernetwork, which assignslarger propagating steps for harder molecules in predictor. In this way,molecular representation is transformed by HiMPP hierarchically fromproperty-level to molecular level. Extensive results show that HiMPP obtainsthe state-of-the-art performance in few-shot MPP problems, and our proposedhierarchical adaptation mechanism is rational and effective."
Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation,"['Matthias Lindemann', 'Alexander Koller', 'Ivan Titov']",http://arxiv.org/pdf/2310.00796v1.pdf,2023-10-01,['cs.cl'],"  Strong inductive biases enable learning from little data and helpgeneralization outside of the training distribution. Popular neuralarchitectures such as Transformers lack strong structural inductive biases forseq2seq NLP tasks on their own. Consequently, they struggle with systematicgeneralization beyond the training distribution, e.g. with extrapolating tolonger inputs, even when pre-trained on large amounts of text. We show how astructural inductive bias can be injected into a seq2seq model by pre-trainingit to simulate structural transformations on synthetic data. Specifically, weinject an inductive bias towards Finite State Transducers (FSTs) into aTransformer by pre-training it to simulate FSTs given their descriptions. Ourexperiments show that our method imparts the desired inductive bias, resultingin improved systematic generalization and better few-shot learning for FST-liketasks."
TRAM: Benchmarking Temporal Reasoning for Large Language Models,"['Yuqing Wang', 'Yun Zhao']",http://arxiv.org/pdf/2310.00835v2.pdf,2023-10-02,['cs.cl'],"  Reasoning about time is essential for understanding the nuances of eventsdescribed in natural language. Previous research on this topic has been limitedin scope, characterized by a lack of standardized benchmarks that would allowfor consistent evaluations across different studies. In this paper, weintroduce TRAM, a temporal reasoning benchmark composed of ten datasets,encompassing various temporal aspects of events such as order, arithmetic,frequency, and duration, designed to facilitate a comprehensive evaluation ofthe temporal reasoning capabilities of large language models (LLMs). We conductan extensive evaluation using popular LLMs, such as GPT-4 and Llama2, in bothzero-shot and few-shot learning scenarios. Additionally, we employ BERT-basedmodels to establish the baseline evaluations. Our findings indicate that thesemodels still trail human performance in temporal reasoning tasks. It is ouraspiration that TRAM will spur further progress in enhancing the temporalreasoning abilities of LLMs."
SHOT: Suppressing the Hessian along the Optimization Trajectory for  Gradient-Based Meta-Learning,"['JunHoo Lee', 'Jayeon Yoo', 'Nojun Kwak']",http://arxiv.org/pdf/2310.02751v1.pdf,2023-10-04,"['cs.lg', 'cs.cv']","  In this paper, we hypothesize that gradient-based meta-learning (GBML)implicitly suppresses the Hessian along the optimization trajectory in theinner loop. Based on this hypothesis, we introduce an algorithm called SHOT(Suppressing the Hessian along the Optimization Trajectory) that minimizes thedistance between the parameters of the target and reference models to suppressthe Hessian in the inner loop. Despite dealing with high-order terms, SHOT doesnot increase the computational complexity of the baseline model much. It isagnostic to both the algorithm and architecture used in GBML, making it highlyversatile and applicable to any GBML baseline. To validate the effectiveness ofSHOT, we conduct empirical tests on standard few-shot learning tasks andqualitatively analyze its dynamics. We confirm our hypothesis empirically anddemonstrate that SHOT outperforms the corresponding baseline. Code is availableat: https://github.com/JunHoo-Lee/SHOT"
Procedural Text Mining with Large Language Models,"['Anisa Rula', ""Jennifer D'Souza""]",http://arxiv.org/pdf/2310.03376v1.pdf,2023-10-05,"['cs.cl', 'cs.ai', 'cs.it', 'math.it']","  Recent advancements in the field of Natural Language Processing, particularlythe development of large-scale language models that are pretrained on vastamounts of knowledge, are creating novel opportunities within the realm ofKnowledge Engineering. In this paper, we investigate the usage of largelanguage models (LLMs) in both zero-shot and in-context learning settings totackle the problem of extracting procedures from unstructured PDF text in anincremental question-answering fashion. In particular, we leverage the currentstate-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,accompanied by two variations of in-context learning that involve an ontologywith definitions of procedures and steps and a limited number of samples offew-shot learning. The findings highlight both the promise of this approach andthe value of the in-context learning customisations. These modifications havethe potential to significantly address the challenge of obtaining sufficienttraining data, a hurdle often encountered in deep learning-based NaturalLanguage Processing techniques for procedure extraction."
PrototypeFormer: Learning to Explore Prototype Relationships for  Few-shot Image Classification,"['Feihong He', 'Gang Li', 'Lingyu Si', 'Leilei Yan', 'Fanzhang Li', 'Fuchun Sun']",http://arxiv.org/pdf/2310.03517v1.pdf,2023-10-05,['cs.cv'],"  Few-shot image classification has received considerable attention foraddressing the challenge of poor classification performance with limitedsamples in novel classes. However, numerous studies have employed sophisticatedlearning strategies and diversified feature extraction methods to address thisissue. In this paper, we propose our method called PrototypeFormer, which aimsto significantly advance traditional few-shot image classification approachesby exploring prototype relationships. Specifically, we utilize a transformerarchitecture to build a prototype extraction module, aiming to extract classrepresentations that are more discriminative for few-shot classification.Additionally, during the model training process, we propose a contrastivelearning-based optimization approach to optimize prototype features in few-shotlearning scenarios. Despite its simplicity, the method performs remarkablywell, with no bells and whistles. We have experimented with our approach onseveral popular few-shot image classification benchmark datasets, which showsthat our method outperforms all current state-of-the-art methods. Inparticular, our method achieves 97.07% and 90.88% on 5-way 5-shot and 5-way1-shot tasks of miniImageNet, which surpasses the state-of-the-art results withaccuracy of 7.27% and 8.72%, respectively. The code will be released later."
A Holistic Evaluation of Piano Sound Quality,"['Monan Zhou', 'Shangda Wu', 'Shaohua Ji', 'Zijin Li', 'Wei Li']",http://arxiv.org/pdf/2310.04722v1.pdf,2023-10-07,"['cs.sd', 'cs.ai', 'eess.as']","  This paper aims to develop a holistic evaluation method for piano soundquality to assist in purchasing decisions. Unlike previous studies that focusedon the effect of piano performance techniques on sound quality, this studyevaluates the inherent sound quality of different pianos. To derive qualityevaluation systems, the study uses subjective questionnaires based on a pianosound quality dataset. The method selects the optimal piano classificationmodels by comparing the fine-tuning results of different pre-training models ofConvolutional Neural Networks (CNN). To improve the interpretability of themodels, the study applies Equivalent Rectangular Bandwidth (ERB) analysis. Theresults reveal that musically trained individuals are better able todistinguish between the sound quality differences of different pianos. The bestfine-tuned CNN pre-trained backbone achieves a high accuracy of 98.3\% as thepiano classifier. However, the dataset is limited, and the audio is sliced toincrease its quantity, resulting in a lack of diversity and balance, so we usefocal loss to reduce the impact of data imbalance. To optimize the method, thedataset will be expanded, or few-shot learning techniques will be employed infuture research."
Task Aware Modulation using Representation Learning: An Approach for Few  Shot Learning in Heterogeneous Systems,"['Arvind Renganathan', 'Rahul Ghosh', 'Ankush Khandelwal', 'Vipin Kumar']",http://arxiv.org/pdf/2310.04727v1.pdf,2023-10-07,"['cs.lg', 'cs.ai']","  We present a Task-aware modulation using Representation Learning (TAM-RL)framework that enhances personalized predictions in few-shot settings forheterogeneous systems when individual task characteristics are not known.TAM-RL extracts embeddings representing the actual inherent characteristics ofthese entities and uses these characteristics to personalize the predictionsfor each entity/task. Using real-world hydrological and flux tower benchmarkdata sets, we show that TAM-RL can significantly outperform existing baselineapproaches such as MAML and multi-modal MAML (MMAML) while being much fasterand simpler to train due to less complexity. Specifically, TAM-RL eliminatesthe need for sensitive hyper-parameters like inner loop steps and inner looplearning rate, which are crucial for model convergence in MAML, MMAML. Wefurther present an empirical evaluation via synthetic data to explore theimpact of heterogeneity amongst the entities on the relative performance ofMAML, MMAML, and TAM-RL. We show that TAM-RL significantly improves predictiveperformance for cases where it is possible to learn distinct representationsfor different tasks."
Argumentative Stance Prediction: An Exploratory Study on Multimodality  and Few-Shot Learning,"['Arushi Sharma', 'Abhibha Gupta', 'Maneesh Bilalpur']",http://arxiv.org/pdf/2310.07093v1.pdf,2023-10-11,['cs.cl'],"  To advance argumentative stance prediction as a multimodal problem, the FirstShared Task in Multimodal Argument Mining hosted stance prediction in crucialsocial topics of gun control and abortion. Our exploratory study attempts toevaluate the necessity of images for stance prediction in tweets and compareout-of-the-box text-based large-language models (LLM) in few-shot settingsagainst fine-tuned unimodal and multimodal models. Our work suggests anensemble of fine-tuned text-based language models (0.817 F1-score) outperformsboth the multimodal (0.677 F1-score) and text-based few-shot prediction using arecent state-of-the-art LLM (0.550 F1-score). In addition to the differences inperformance, our findings suggest that the multimodal models tend to performbetter when image content is summarized as natural language over their nativepixel structure and, using in-context examples improves few-shot performance ofLLMs."
LLM-augmented Preference Learning from Natural Language,"['Inwon Kang', 'Sikai Ruan', 'Tyler Ho', 'Jui-Chien Lin', 'Farhad Mohsin', 'Oshani Seneviratne', 'Lirong Xia']",http://arxiv.org/pdf/2310.08523v1.pdf,2023-10-12,['cs.cl'],"  Finding preferences expressed in natural language is an important butchallenging task. State-of-the-art(SotA) methods leverage transformer-basedmodels such as BERT, RoBERTa, etc. and graph neural architectures such as graphattention networks. Since Large Language Models (LLMs) are equipped to dealwith larger context lengths and have much larger model sizes than thetransformer-based model, we investigate their ability to classify comparativetext directly. This work aims to serve as a first step towards using LLMs forthe CPC task. We design and conduct a set of experiments that format theclassification task into an input prompt for the LLM and a methodology to get afixed-format response that can be automatically evaluated. Comparingperformances with existing methods, we see that pre-trained LLMs are able tooutperform the previous SotA models with no fine-tuning involved. Our resultsshow that the LLMs can consistently outperform the SotA when the target text islarge -- i.e. composed of multiple sentences --, and are still comparable tothe SotA performance in shorter text. We also find that few-shot learningyields better performance than zero-shot learning."
In-Context Learning for Few-Shot Molecular Property Prediction,"['Christopher Fifty', 'Jure Leskovec', 'Sebastian Thrun']",http://arxiv.org/pdf/2310.08863v1.pdf,2023-10-13,['cs.lg'],"  In-context learning has become an important approach for few-shot learning inLarge Language Models because of its ability to rapidly adapt to new taskswithout fine-tuning model parameters. However, it is restricted to applicationsin natural language and inapplicable to other domains. In this paper, we adaptthe concepts underpinning in-context learning to develop a new algorithm forfew-shot molecular property prediction. Our approach learns to predictmolecular properties from a context of (molecule, property measurement) pairsand rapidly adapts to new properties without fine-tuning. On the FS-Mol andBACE molecular property prediction benchmarks, we find this method surpassesthe performance of recent meta-learning algorithms at small support sizes andis competitive with the best methods at large support sizes."
Plug-and-Play Feature Generation for Few-Shot Medical Image  Classification,"['Qianyu Guo', 'Huifang Du', 'Xing Jia', 'Shuyong Gao', 'Yan Teng', 'Haofen Wang', 'Wenqiang Zhang']",http://arxiv.org/pdf/2310.09471v1.pdf,2023-10-14,['cs.cv'],"  Few-shot learning (FSL) presents immense potential in enhancing modelgeneralization and practicality for medical image classification with limitedtraining data; however, it still faces the challenge of severe overfitting inclassifier training due to distribution bias caused by the scarce trainingsamples. To address the issue, we propose MedMFG, a flexible and lightweightplug-and-play method designed to generate sufficient class-distinctive featuresfrom limited samples. Specifically, MedMFG first re-represents the limitedprototypes to assign higher weights for more important information features.Then, the prototypes are variationally generated into abundant effectivefeatures. Finally, the generated features and prototypes are together to traina more generalized classifier. Experiments demonstrate that MedMFG outperformsthe previous state-of-the-art methods on cross-domain benchmarks involving thetransition from natural images to medical images, as well as medical imageswith different lesions. Notably, our method achieves over 10% performanceimprovement compared to several baselines. Fusion experiments further validatethe adaptability of MedMFG, as it seamlessly integrates into various backbonesand baselines, consistently yielding improvements of over 2.9% across allresults."
In-Context Few-Shot Relation Extraction via Pre-Trained Language Models,"['Yilmazcan Ozyurt', 'Stefan Feuerriegel', 'Ce Zhang']",http://arxiv.org/pdf/2310.11085v1.pdf,2023-10-17,"['cs.cl', 'cs.ai', 'cs.lg']","  Relation extraction aims at inferring structured human knowledge from textualdocuments. State-of-the-art methods based on language models commonly have twolimitations: (1) they require named entities to be either given as input orinfer them, which introduces additional noise, and (2) they require humanannotations of documents. As a remedy, we present a novel framework forin-context few-shot relation extraction via pre-trained language models. To thebest of our knowledge, we are the first to reformulate the relation extractiontask as a tailored in-context few-shot learning paradigm. Thereby, we achievecrucial benefits in that we eliminate the need for both named entityrecognition and human annotation of documents. Unlike existing methods based onfine-tuning, our framework is flexible in that it can be easily updated for anew set of relations without re-training. We evaluate our framework usingDocRED, the largest publicly available dataset for document-level relationextraction, and demonstrate that our framework achieves state-of-the-artperformance. Finally, our framework allows us to identify missing annotations,and we thus show that our framework actually performs much better than theoriginal labels from the development set of DocRED."
Group Preference Optimization: Few-Shot Alignment of Large Language  Models,"['Siyan Zhao', 'John Dang', 'Aditya Grover']",http://arxiv.org/pdf/2310.11523v1.pdf,2023-10-17,"['cs.lg', 'cs.ai', 'cs.cl']","  Many applications of large language models (LLMs), ranging from chatbots tocreative writing, require nuanced subjective judgments that can differsignificantly across different groups. Existing alignment algorithms can beexpensive to align for each group, requiring prohibitive amounts ofgroup-specific preference data and computation for real-world use cases. Weintroduce Group Preference Optimization (GPO), an alignment framework thatsteers language models to preferences of individual groups in a few-shotmanner. In GPO, we augment the base LLM with an independent transformer moduletrained to predict the preferences of a group for the LLM generations. Forfew-shot learning, we parameterize this module as an in-context autoregressivetransformer and train it via meta-learning on several groups. We empiricallyvalidate the efficacy of GPO through rigorous evaluations using LLMs withvaried sizes on three human opinion adaptation tasks. These tasks involveadapting to the preferences of US demographic groups, global countries, andindividual users. Our results demonstrate that GPO not only aligns models moreaccurately but also requires fewer group-specific preferences, and lesstraining and inference computing resources, outperforming existing strategiessuch as in-context steering and fine-tuning methods."
CLARA: Multilingual Contrastive Learning for Audio Representation  Acquisition,"['Kari A Noriy', 'Xiaosong Yang', 'Marcin Budka', 'Jian Jun Zhang']",http://arxiv.org/pdf/2310.11830v2.pdf,2023-10-18,"['cs.sd', 'cs.lg', 'cs.mm', 'eess.as']","  Multilingual speech processing requires understanding emotions, a task madedifficult by limited labelled data. CLARA, minimizes reliance on labelled data,enhancing generalization across languages. It excels at fostering sharedrepresentations, aiding cross-lingual transfer of speech and emotions, evenwith little data. Our approach adeptly captures emotional nuances in speech,overcoming subjective assessment issues. Using a large multilingual audiocorpus and self-supervised learning, CLARA develops speech representationsenriched with emotions, advancing emotion-aware multilingual speech processing.  Our method expands the data range using data augmentation, textual embeddingfor visual understanding, and transfers knowledge from high- to low-resourcelanguages. CLARA demonstrates excellent performance in emotion recognition,language comprehension, and audio benchmarks, excelling in zero-shot andfew-shot learning. It adapts to low-resource languages, marking progress inmultilingual speech representation learning."
A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for  Fairer Instruction-Tuned Machine Translation,"['Giuseppe Attanasio', 'Flor Miriam Plaza-del-Arco', 'Debora Nozza', 'Anne Lauscher']",http://arxiv.org/pdf/2310.12127v2.pdf,2023-10-18,"['cs.cl', 'cs.lg']","  Recent instruction fine-tuned models can solve multiple NLP tasks whenprompted to do so, with machine translation (MT) being a prominent use case.However, current research often focuses on standard performance benchmarks,leaving compelling fairness and ethical considerations behind. In MT, thismight lead to misgendered translations, resulting, among other harms, in theperpetuation of stereotypes and prejudices. In this work, we address this gapby investigating whether and to what extent such models exhibit gender bias inmachine translation and how we can mitigate it. Concretely, we computeestablished gender bias metrics on the WinoMT corpus from English to German andSpanish. We discover that IFT models default to male-inflected translations,even disregarding female occupational stereotypes. Next, using interpretabilitymethods, we unveil that models systematically overlook the pronoun indicatingthe gender of a target occupation in misgendered translations. Finally, basedon this finding, we propose an easy-to-implement and effective bias mitigationsolution based on few-shot learning that leads to significantly fairertranslations."
Few-Shot In-Context Imitation Learning via Implicit Graph Alignment,"['Vitalis Vosylius', 'Edward Johns']",http://arxiv.org/pdf/2310.12238v1.pdf,2023-10-18,"['cs.ro', 'cs.ai', 'cs.lg']","  Consider the following problem: given a few demonstrations of a task across afew different objects, how can a robot learn to perform that same task on new,previously unseen objects? This is challenging because the large variety ofobjects within a class makes it difficult to infer the task-relevantrelationship between the new objects and the objects in the demonstrations. Weaddress this by formulating imitation learning as a conditional alignmentproblem between graph representations of objects. Consequently, we show thatthis conditioning allows for in-context learning, where a robot can perform atask on a set of new objects immediately after the demonstrations, without anyprior knowledge about the object class or any further training. In ourexperiments, we explore and validate our design choices, and we show that ourmethod is highly effective for few-shot learning of several real-world,everyday tasks, whilst outperforming baselines. Videos are available on ourproject webpage at https://www.robot-learning.uk/implicit-graph-alignment."
New Environment Adaptation with Few Shots for OFDM Receiver and mmWave  Beamforming,"['Ouya Wang', 'Shenglong Zhou', 'Geoffrey Ye Li']",http://arxiv.org/pdf/2310.12343v1.pdf,2023-10-18,['cs.dc'],"  Few-shot learning (FSL) enables adaptation to new tasks with only limitedtraining data. In wireless communications, channel environments can varydrastically; therefore, FSL techniques can quickly adjust transceiveraccordingly. In this paper, we develop two FSL frameworks that fit in wirelesstransceiver design. Both frameworks are base on optimization programs that canbe solved by well-known algorithms like the inexact alternating directionmethod of multipliers (iADMM) and the inexact alternating direction method(iADM). As examples, we demonstrate how the proposed two FSL frameworks areused for the OFDM receiver and beamforming (BF) for the millimeter wave(mmWave) system. The numerical experiments confirm their desirable performancein both applications compared to other popular approaches, such as transferlearning (TL) and model-agnostic meta-learning."
An Exploration of In-Context Learning for Speech Language Model,"['Ming-Hao Hsu', 'Kai-Wei Chang', 'Shang-Wen Li', 'Hung-yi Lee']",http://arxiv.org/pdf/2310.12477v1.pdf,2023-10-19,"['eess.as', 'cs.ai', 'cs.cl']","  Ever since the development of GPT-3 in the natural language processing (NLP)field, in-context learning (ICL) has played an important role in utilizinglarge language models (LLMs). By presenting the LM utterance-labeldemonstrations at the input, the LM can accomplish few-shot learning withoutrelying on gradient descent or requiring explicit modification of itsparameters. This enables the LM to learn and adapt in a black-box manner.Despite the success of ICL in NLP, little work is exploring the possibility ofICL in speech processing. This study proposes the first exploration of ICL witha speech LM without text supervision. We first show that the current speech LMdoes not have the ICL capability. With the proposed warmup training, the speechLM can, therefore, perform ICL on unseen tasks. In this work, we verify thefeasibility of ICL for speech LM on speech classification tasks."
Unsupervised Representation Learning to Aid Semi-Supervised Meta  Learning,"['Atik Faysal', 'Mohammad Rostami', 'Huaxia Wang', 'Avimanyu Sahoo', 'Ryan Antle']",http://arxiv.org/pdf/2310.13085v1.pdf,2023-10-19,"['cs.lg', 'cs.ai']","  Few-shot learning or meta-learning leverages the data scarcity problem inmachine learning. Traditionally, training data requires a multitude of samplesand labeling for supervised learning. To address this issue, we propose aone-shot unsupervised meta-learning to learn the latent representation of thetraining samples. We use augmented samples as the query set during the trainingphase of the unsupervised meta-learning. A temperature-scaled cross-entropyloss is used in the inner loop of meta-learning to prevent overfitting duringunsupervised learning. The learned parameters from this step are applied to thetargeted supervised meta-learning in a transfer-learning fashion forinitialization and fast adaptation with improved accuracy. The proposed methodis model agnostic and can aid any meta-learning model to improve accuracy. Weuse model agnostic meta-learning (MAML) and relation network (RN) on Omniglotand mini-Imagenet datasets to demonstrate the performance of the proposedmethod. Furthermore, a meta-learning model with the proposed initialization canachieve satisfactory accuracy with significantly fewer training samples."
Large Language Models are biased to overestimate profoundness,"['Eugenio Herrera-Berg', 'Tomás Vergara Browne', 'Pablo León-Villagrá', 'Marc-Lluís Vives', 'Cristian Buc Calderon']",http://arxiv.org/pdf/2310.14422v1.pdf,2023-10-22,['cs.cl'],"  Recent advancements in natural language processing by large language models(LLMs), such as GPT-4, have been suggested to approach Artificial GeneralIntelligence. And yet, it is still under dispute whether LLMs possess similarreasoning abilities to humans. This study evaluates GPT-4 and various otherLLMs in judging the profoundness of mundane, motivational, and pseudo-profoundstatements. We found a significant statement-to-statement correlation betweenthe LLMs and humans, irrespective of the type of statements and the promptingtechnique used. However, LLMs systematically overestimate the profoundness ofnonsensical statements, with the exception of Tk-instruct, which uniquelyunderestimates the profoundness of statements. Only few-shot learning prompts,as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans.Furthermore, this work provides insights into the potential biases induced byReinforcement Learning from Human Feedback (RLHF), inducing an increase in thebias to overestimate the profoundness of statements."
Improving Few-shot Generalization of Safety Classifiers via Data  Augmented Parameter-Efficient Fine-Tuning,"['Ananth Balashankar', 'Xiao Ma', 'Aradhana Sinha', 'Ahmad Beirami', 'Yao Qin', 'Jilin Chen', 'Alex Beutel']",http://arxiv.org/pdf/2310.16959v1.pdf,2023-10-25,['cs.lg'],"  As large language models (LLMs) are widely adopted, new safety issues andpolicies emerge, to which existing safety classifiers do not generalize well.If we have only observed a few examples of violations of a new safety rule, howcan we build a classifier to detect violations? In this paper, we study thenovel setting of domain-generalized few-shot learning for LLM-based text safetyclassifiers. Unlike prior few-shot work, these new safety issues can be hard touncover and we do not get to choose the few examples. We demonstrate thatexisting few-shot techniques do not perform well in this setting, and rather wepropose to do parameter-efficient fine-tuning (PEFT) combined with augmentingtraining data based on similar examples in prior existing rules. We empiricallyshow that our approach of similarity-based data-augmentation + prompt-tuning(DAPT) consistently outperforms baselines that either do not rely on dataaugmentation or on PEFT by 7-17% F1 score in the Social Chemistry moraljudgement and 9-13% AUC in the Toxicity detection tasks, even when the new ruleis loosely correlated with existing ones."
Retrofitting Light-weight Language Models for Emotions using Supervised  Contrastive Learning,"['Sapan Shah', 'Sreedhar Reddy', 'Pushpak Bhattacharyya']",http://arxiv.org/pdf/2310.18930v1.pdf,2023-10-29,['cs.cl'],"  We present a novel retrofitting method to induce emotion aspects intopre-trained language models (PLMs) such as BERT and RoBERTa. Our method updatespre-trained network weights using contrastive learning so that the textfragments exhibiting similar emotions are encoded nearby in the representationspace, and the fragments with different emotion content are pushed apart. Whiledoing so, it also ensures that the linguistic knowledge already present in PLMsis not inadvertently perturbed. The language models retrofitted by our method,i.e., BERTEmo and RoBERTaEmo, produce emotion-aware text representations, asevaluated through different clustering and retrieval metrics. For thedownstream tasks on sentiment analysis and sarcasm detection, they performbetter than their pre-trained counterparts (about 1% improvement in F1-score)and other existing approaches. Additionally, a more significant boost inperformance is observed for the retrofitted models over pre-trained ones infew-shot learning setting."
Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for  Propaganda and Disinformation Detection,"['Yunze Xiao', 'Firoj Alam']",http://arxiv.org/pdf/2311.03184v1.pdf,2023-11-06,"['cs.cl', 'cs.ai', 'cs.si', '68t50', 'f.2.2; i.2.7']","  The spread of disinformation and propagandistic content poses a threat tosocietal harmony, undermining informed decision-making and trust in reliablesources. Online platforms often serve as breeding grounds for such content, andmalicious actors exploit the vulnerabilities of audiences to shape publicopinion. Although there have been research efforts aimed at the automaticidentification of disinformation and propaganda in social media content, thereremain challenges in terms of performance. The ArAIEval shared task aims tofurther research on these particular issues within the context of the Arabiclanguage. In this paper, we discuss our participation in these shared tasks. Wecompeted in subtasks 1A and 2A, where our submitted system secured positions9th and 10th, respectively. Our experiments consist of fine-tuning transformermodels and using zero- and few-shot learning with GPT-4."
Multilingual Mathematical Autoformalization,"['Albert Q. Jiang', 'Wenda Li', 'Mateja Jamnik']",http://arxiv.org/pdf/2311.03755v2.pdf,2023-11-07,"['cs.cl', 'cs.lg']","  Autoformalization is the task of translating natural language materials intomachine-verifiable formalisations. Progress in autoformalization research ishindered by the lack of a sizeable dataset consisting of informal-formal pairsexpressing the same essence. Existing methods tend to circumvent this challengeby manually curating small corpora or using few-shot learning with largelanguage models. But these methods suffer from data scarcity and formallanguage acquisition difficulty. In this work, we create $\texttt{MMA}$, alarge, flexible, multilingual, and multi-domain dataset of informal-formalpairs, by using a language model to translate in the reverse direction, thatis, from formal mathematical statements into corresponding informal ones.Experiments show that language models fine-tuned on $\texttt{MMA}$ produce$16-18\%$ of statements acceptable with minimal corrections on the$\texttt{miniF2F}$ and $\texttt{ProofNet}$ benchmarks, up from $0\%$ with thebase model. We demonstrate that fine-tuning on multilingual formal data resultsin more capable autoformalization models even when deployed on monolingualtasks."
Enhancing Instance-Level Image Classification with Set-Level Labels,"['Renyu Zhang', 'Aly A. Khan', 'Yuxin Chen', 'Robert L. Grossman']",http://arxiv.org/pdf/2311.05659v2.pdf,2023-11-09,"['cs.lg', 'cs.ai']","  Instance-level image classification tasks have traditionally relied onsingle-instance labels to train models, e.g., few-shot learning and transferlearning. However, set-level coarse-grained labels that capture relationshipsamong instances can provide richer information in real-world scenarios. In thispaper, we present a novel approach to enhance instance-level imageclassification by leveraging set-level labels. We provide a theoreticalanalysis of the proposed method, including recognition conditions for fastexcess risk rate, shedding light on the theoretical foundations of ourapproach. We conducted experiments on two distinct categories of datasets:natural image datasets and histopathology image datasets. Our experimentalresults demonstrate the effectiveness of our approach, showcasing improvedclassification performance compared to traditional single-instance label-basedmethods. Notably, our algorithm achieves 13% improvement in classificationaccuracy compared to the strongest baseline on the histopathology imageclassification benchmarks. Importantly, our experimental findings align withthe theoretical analysis, reinforcing the robustness and reliability of ourproposed method. This work bridges the gap between instance-level and set-levelimage classification, offering a promising avenue for advancing thecapabilities of image classification models with set-level coarse-grainedlabels."
Chatbots Are Not Reliable Text Annotators,"['Ross Deans Kristensen-McLachlan', 'Miceal Canavan', 'Márton Kardos', 'Mia Jacobsen', 'Lene Aarøe']",http://arxiv.org/pdf/2311.05769v1.pdf,2023-11-09,"['cs.cl', 'cs.ai']","  Recent research highlights the significant potential of ChatGPT for textannotation in social science research. However, ChatGPT is a closed-sourceproduct which has major drawbacks with regards to transparency,reproducibility, cost, and data protection. Recent advances in open-source (OS)large language models (LLMs) offer alternatives which remedy these challenges.This means that it is important to evaluate the performance of OS LLMs relativeto ChatGPT and standard approaches to supervised machine learningclassification. We conduct a systematic comparative evaluation of theperformance of a range of OS LLM models alongside ChatGPT, using both zero- andfew-shot learning as well as generic and custom prompts, with results comparedto more traditional supervised classification models. Using a new dataset ofTweets from US news media, and focusing on simple binary text annotation tasksfor standard social science concepts, we find significant variation in theperformance of ChatGPT and OS models across the tasks, and that supervisedclassifiers consistently outperform both. Given the unreliable performance ofChatGPT and the significant challenges it poses to Open Science we adviseagainst using ChatGPT for substantive text annotation tasks in social scienceresearch."
In-context Learning and Gradient Descent Revisited,"['Gilad Deutch', 'Nadav Magar', 'Tomer Bar Natan', 'Guy Dar']",http://arxiv.org/pdf/2311.07772v3.pdf,2023-11-13,"['cs.cl', 'cs.lg']","  In-context learning (ICL) has shown impressive results in few-shot learningtasks, yet its underlying mechanism is still not fully understood. Recent workssuggest that ICL can be thought of as a gradient descent (GD) basedoptimization process. While promising, these results mainly focus on simplifiedsettings of ICL and provide only a preliminary evaluation of the similaritiesbetween the two methods. In this work, we revisit the comparison between ICLand GD-based finetuning and study what properties of ICL an equivalent processmust follow. We highlight a major difference in the flow of information betweenICL and standard finetuning. Namely, ICL can only rely on information fromlower layers at every point, while finetuning depends on loss gradients fromdeeper layers. We refer to this discrepancy as Layer Causality and show that alayer causal variant of the finetuning process aligns with ICL on par withvanilla finetuning and is even better in most cases across relevant metrics. Tothe best of our knowledge, this is the first work to discuss this discrepancyexplicitly and suggest a solution that tackles this problem with minimalchanges."
Large Language Models in Finance: A Survey,"['Yinheng Li', 'Shaofei Wang', 'Han Ding', 'Hang Chen']",http://arxiv.org/pdf/2311.10723v1.pdf,2023-09-28,"['q-fin.gn', 'cs.ai', 'cs.cl']","  Recent advances in large language models (LLMs) have opened new possibilitiesfor artificial intelligence applications in finance. In this paper, we providea practical survey focused on two key aspects of utilizing LLMs for financialtasks: existing solutions and guidance for adoption.  First, we review current approaches employing LLMs in finance, includingleveraging pretrained models via zero-shot or few-shot learning, fine-tuning ondomain-specific data, and training custom LLMs from scratch. We summarize keymodels and evaluate their performance improvements on financial naturallanguage processing tasks.  Second, we propose a decision framework to guide financial professionals inselecting the appropriate LLM solution based on their use case constraintsaround data, compute, and performance needs. The framework provides a pathwayfrom lightweight experimentation to heavy investment in customized LLMs.  Lastly, we discuss limitations and challenges around leveraging LLMs infinancial applications. Overall, this survey aims to synthesize thestate-of-the-art and provide a roadmap for responsibly applying LLMs to advancefinancial AI."
Generalization of Fitness Exercise Recognition from Doppler Measurements  by Domain-adaption and Few-Shot Learning,"['Biying Fu', 'Naser Damer', 'Florian Kirchbuchner', 'Arjan Kuijper']",http://arxiv.org/pdf/2311.11910v1.pdf,2023-11-20,"['cs.ai', 'cs.cv']","  In previous works, a mobile application was developed using an unmodifiedcommercial off-the-shelf smartphone to recognize whole-body exercises. Theworking principle was based on the ultrasound Doppler sensing with the devicebuilt-in hardware. Applying such a lab-environment trained model on realisticapplication variations causes a significant drop in performance, and thusdecimate its applicability. The reason of the reduced performance can bemanifold. It could be induced by the user, environment, and device variationsin realistic scenarios. Such scenarios are often more complex and diverse,which can be challenging to anticipate in the initial training data. To studyand overcome this issue, this paper presents a database with controlled anduncontrolled subsets of fitness exercises. We propose two concepts to utilizesmall adaption data to successfully improve model generalization in anuncontrolled environment, increasing the recognition accuracy by two to sixfolds compared to the baseline for different users."
Language-guided Few-shot Semantic Segmentation,"['Jing Wang', 'Yuang Liu', 'Qiang Zhou', 'Fan Wang']",http://arxiv.org/pdf/2311.13865v1.pdf,2023-11-23,['cs.cv'],"  Few-shot learning is a promising way for reducing the label cost in newcategories adaptation with the guidance of a small, well labeled support set.But for few-shot semantic segmentation, the pixel-level annotations of supportimages are still expensive. In this paper, we propose an innovative solution totackle the challenge of few-shot semantic segmentation using only languageinformation, i.e.image-level text labels. Our approach involves avision-language-driven mask distillation scheme, which contains avision-language pretraining (VLP) model and a mask refiner, to generate highquality pseudo-semantic masks from text prompts. We additionally introduce adistributed prototype supervision method and complementary correlation matchingmodule to guide the model in digging precise semantic relations among supportand query images. The experiments on two benchmark datasets demonstrate thatour method establishes a new baseline for language-guided few-shot semanticsegmentation and achieves competitive results to recent vision-guided methods."
Learning to Skip for Language Modeling,"['Dewen Zeng', 'Nan Du', 'Tao Wang', 'Yuanzhong Xu', 'Tao Lei', 'Zhifeng Chen', 'Claire Cui']",http://arxiv.org/pdf/2311.15436v1.pdf,2023-11-26,['cs.cl'],"  Overparameterized large-scale language models have impressive generalizationperformance of in-context few-shot learning. However, most language modelsallocate the same amount of parameters or computation to each token,disregarding the complexity or importance of the input data. We argue that inlanguage model pretraining, a variable amount of computation should be assignedto different tokens, and this can be efficiently achieved via a simple routingmechanism. Different from conventional early stopping techniques where tokenscan early exit at only early layers, we propose a more general method thatdynamically skips the execution of a layer (or module) for any input token witha binary router. In our extensive evaluation across 24 NLP tasks, wedemonstrate that the proposed method can significantly improve the 1-shotperformance compared to other competitive baselines only at mild extra cost forinference."
CaesarNeRF: Calibrated Semantic Representation for Few-shot  Generalizable Neural Rendering,"['Haidong Zhu', 'Tianyu Ding', 'Tianyi Chen', 'Ilya Zharkov', 'Ram Nevatia', 'Luming Liang']",http://arxiv.org/pdf/2311.15510v1.pdf,2023-11-27,['cs.cv'],"  Generalizability and few-shot learning are key challenges in Neural RadianceFields (NeRF), often due to the lack of a holistic understanding in pixel-levelrendering. We introduce CaesarNeRF, an end-to-end approach that leveragesscene-level CAlibratEd SemAntic Representation along with pixel-levelrepresentations to advance few-shot, generalizable neural rendering,facilitating a holistic understanding without compromising high-qualitydetails. CaesarNeRF explicitly models pose differences of reference views tocombine scene-level semantic representations, providing a calibrated holisticunderstanding. This calibration process aligns various viewpoints with preciselocation and is further enhanced by sequential refinement to capture varyingdetails. Extensive experiments on public datasets, including LLFF, Shiny,mip-NeRF 360, and MVImgNet, show that CaesarNeRF delivers state-of-the-artperformance across varying numbers of reference views, proving effective evenwith a single reference image. The project page of this work can be found athttps://haidongz-usc.github.io/project/caesarnerf."
Deficiency of Large Language Models in Finance: An Empirical Examination  of Hallucination,"['Haoqiang Kang', 'Xiao-Yang Liu']",http://arxiv.org/pdf/2311.15548v1.pdf,2023-11-27,"['cs.cl', 'cs.ai', 'cs.lg', 'q-fin.st']","  The hallucination issue is recognized as a fundamental deficiency of largelanguage models (LLMs), especially when applied to fields such as finance,education, and law. Despite the growing concerns, there has been a lack ofempirical investigation. In this paper, we provide an empirical examination ofLLMs' hallucination behaviors in financial tasks. First, we empiricallyinvestigate LLM model's ability of explaining financial concepts andterminologies. Second, we assess LLM models' capacity of querying historicalstock prices. Third, to alleviate the hallucination issue, we evaluate theefficacy of four practical methods, including few-shot learning, Decoding byContrasting Layers (DoLa), the Retrieval Augmentation Generation (RAG) methodand the prompt-based tool learning method for a function to generate a querycommand. Finally, our major finding is that off-the-shelf LLMs experienceserious hallucination behaviors in financial tasks. Therefore, there is anurgent need to call for research efforts in mitigating LLMs' hallucination."
Improving Denoising Diffusion Probabilistic Models via Exploiting Shared  Representations,"['Delaram Pirhayatifard', 'Mohammad Taha Toghani', 'Guha Balakrishnan', 'César A. Uribe']",http://arxiv.org/pdf/2311.16353v1.pdf,2023-11-27,"['cs.lg', 'cs.ai', 'cs.cv', 'eess.iv', 'eess.sp']","  In this work, we address the challenge of multi-task image generation withlimited data for denoising diffusion probabilistic models (DDPM), a class ofgenerative models that produce high-quality images by reversing a noisydiffusion process. We propose a novel method, SR-DDPM, that leveragesrepresentation-based techniques from few-shot learning to effectively learnfrom fewer samples across different tasks. Our method consists of a core metaarchitecture with shared parameters, i.e., task-specific layers with exclusiveparameters. By exploiting the similarity between diverse data distributions,our method can scale to multiple tasks without compromising the image quality.We evaluate our method on standard image datasets and show that it outperformsboth unconditional and conditional DDPM in terms of FID and SSIM metrics."
ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate  Statements?,"['Romain Lacombe', 'Kerrie Wu', 'Eddie Dilworth']",http://arxiv.org/pdf/2311.17107v1.pdf,2023-11-28,"['cs.lg', 'cs.ai', 'cs.cl', 'cs.cy', 'cs.ir']","  Evaluating the accuracy of outputs generated by Large Language Models (LLMs)is especially important in the climate science and policy domain. We introducethe Expert Confidence in Climate Statements (ClimateX) dataset, a novel,curated, expert-labeled dataset consisting of 8094 climate statements collectedfrom the latest Intergovernmental Panel on Climate Change (IPCC) reports,labeled with their associated confidence levels. Using this dataset, we showthat recent LLMs can classify human expert confidence in climate-relatedstatements, especially in a few-shot learning setting, but with limited (up to47%) accuracy. Overall, models exhibit consistent and significantover-confidence on low and medium confidence statements. We highlightimplications of our results for climate communication, LLMs evaluationstrategies, and the use of LLMs in information retrieval systems."
On the Effects of Randomness on Stability of Learning with Limited  Labelled Data: A Systematic Literature Review,"['Branislav Pecher', 'Ivan Srba', 'Maria Bielikova']",http://arxiv.org/pdf/2312.01082v1.pdf,2023-12-02,"['cs.lg', 'cs.ai', 'cs.cl']","  Learning with limited labelled data, such as few-shot learning, meta-learningor transfer learning, aims to effectively train a model using only small amountof labelled samples. However, these approaches were observed to be excessivelysensitive to the effects of uncontrolled randomness caused by non-determinismin the training process. The randomness negatively affects the stability of themodels, leading to large variance in results across training runs. When suchinstability is disregarded, it can unintentionally, but unfortunately alsointentionally, create an imaginary perception of research progress. Recently,this area started to attract a research attention and the number of relevantstudies is continuously growing. In this survey, we provide a comprehensiveoverview of 134 papers addressing the effects of randomness on the stability oflearning with limited labelled data. We distinguish between four main tasksaddressed in the papers (investigate/evaluate; determine; mitigate;benchmark/compare/report randomness effects), providing findings for each one.Furthermore, we identify and discuss seven challenges and open problemstogether with possible directions to facilitate further research. The ultimategoal of this survey is to emphasise the importance of this growing researcharea, which so far has not received appropriate level of attention."
VidereX: A Navigational Application inspired by ants,"['Nam Ho Koh', 'Doran Amos', 'Paul Graham', 'Andrew Philippides']",http://arxiv.org/pdf/2312.03000v1.pdf,2023-12-03,['cs.hc'],"  Navigation is a crucial element in any person's life, whether for work,education, social living or any other miscellaneous reason; naturally, theimportance of it is universally recognised and valued. One of the criticalcomponents of navigation is vision, which facilitates movement from one placeto another. Navigating unfamiliar settings, especially for the blind orvisually impaired, can pose significant challenges, impacting theirindependence and quality of life. Current assistive travel solutions haveshortcomings, including GPS limitations and a demand for an efficient,user-friendly, and portable model. Addressing these concerns, this paperpresents VidereX: a smartphone-based solution using an ant-inspired navigationalgorithm. Emulating ants' ability to learn a route between nest and feedinggrounds after a single traversal, VidereX enables users to rapidly acquirenavigational data using a one/few-shot learning strategy. A key component ofVidereX is its emphasis on active user engagement. Like ants with a scanningbehaviour to actively investigate their environment, users wield the camera,actively exploring the visual landscape. Far from the passive reception ofdata, this process constitutes a dynamic exploration, echoing nature'snavigational mechanisms."
Background Clustering Pre-training for Few-shot Segmentation,"['Zhimiao Yu', 'Tiancheng Lin', 'Yi Xu']",http://arxiv.org/pdf/2312.03322v1.pdf,2023-12-06,['cs.cv'],"  Recent few-shot segmentation (FSS) methods introduce an extra pre-trainingstage before meta-training to obtain a stronger backbone, which has become astandard step in few-shot learning. Despite the effectiveness, currentpre-training scheme suffers from the merged background problem: only baseclasses are labelled as foregrounds, making it hard to distinguish betweennovel classes and actual background. In this paper, we propose a newpre-training scheme for FSS via decoupling the novel classes from background,called Background Clustering Pre-Training (BCPT). Specifically, we adopt onlineclustering to the pixel embeddings of merged background to explore theunderlying semantic structures, bridging the gap between pre-training andadaptation to novel classes. Given the clustering results, we further proposethe background mining loss and leverage base classes to guide the clusteringprocess, improving the quality and stability of clustering results. Experimentson PASCAL-5i and COCO-20i show that BCPT yields advanced performance. Code willbe available."
Parameter-Efficient Transfer Learning of Audio Spectrogram Transformers,"['Umberto Cappellazzo', 'Daniele Falavigna', 'Alessio Brutti', 'Mirco Ravanelli']",http://arxiv.org/pdf/2312.03694v2.pdf,2023-12-06,['eess.as'],"  The common modus operandi of fine-tuning large pre-trained Transformer modelsentails the adaptation of all their parameters (i.e., full fine-tuning). Whileachieving striking results on multiple tasks, this approach becomes unfeasibleas the model size and the number of downstream tasks increase. In naturallanguage processing and computer vision, parameter-efficient approaches likeprompt-tuning and adapters have emerged as solid alternatives by fine-tuningonly a small number of extra parameters, without sacrificing performanceaccuracy. Specifically, adapters, due to their flexibility, have recentlygarnered significant attention, leading to several variants. For audioclassification tasks, the Audio Spectrogram Transformer model shows impressiveresults. However, surprisingly, how to efficiently adapt it to severaldownstream tasks has not been tackled before. In this paper, we bridge this gapand present a detailed investigation of common parameter-efficient methods,revealing that adapters consistently outperform the other methods across fourbenchmarks. This trend is also confirmed in few-shot learning settings and whenthe total number of trainable parameters increases, demonstrating adapterssuperior scalability. We finally study the best adapter configuration, as wellas the role of residual connections in the learning process."
Metacognition-Enhanced Few-Shot Prompting With Positive Reinforcement,"['Yu Ji', 'Wen Wu', 'Yi Hu', 'Hong Zheng', 'Liang He']",http://arxiv.org/pdf/2312.08642v1.pdf,2023-12-14,"['cs.cl', 'cs.ai']","  Few-shot prompting elicits the remarkable abilities of large language modelsby equipping them with a few demonstration examples in the input. However, thetraditional method of providing large language models with all demonstrationinput-output pairs at once may not effectively guide large language models tolearn the specific input-output mapping relationship. In this paper, inspiredby the regulatory and supportive role of metacognition in students' learning,we propose a novel metacognition-enhanced few-shot prompting, which guideslarge language models to reflect on their thought processes to comprehensivelylearn the given demonstration examples. Furthermore, considering that positivereinforcement can improve students' learning motivation, we introduce positivereinforcement into our metacognition-enhanced few-shot prompting to promote thefew-shot learning of large language models by providing response-based positivefeedback. The experimental results on two real-world datasets show that ourmetacognition-enhanced few-shot prompting with positive reinforcement surpassestraditional few-shot prompting in classification accuracy and macro F1."
Extending Context Window of Large Language Models via Semantic  Compression,"['Weizhi Fei', 'Xueyan Niu', 'Pingyi Zhou', 'Lu Hou', 'Bo Bai', 'Lei Deng', 'Wei Han']",http://arxiv.org/pdf/2312.09571v1.pdf,2023-12-15,"['cs.cl', 'cs.it', 'math.it']","  Transformer-based Large Language Models (LLMs) often impose limitations onthe length of the text input to ensure the generation of fluent and relevantresponses. This constraint restricts their applicability in scenarios involvinglong texts. We propose a novel semantic compression method that enablesgeneralization to texts that are 6-8 times longer, without incurringsignificant computational costs or requiring fine-tuning. Our proposedframework draws inspiration from source coding in information theory andemploys a pre-trained model to reduce the semantic redundancy of long inputsbefore passing them to the LLMs for downstream tasks. Experimental resultsdemonstrate that our method effectively extends the context window of LLMsacross a range of tasks including question answering, summarization, few-shotlearning, and information retrieval. Furthermore, the proposed semanticcompression method exhibits consistent fluency in text generation whilereducing the associated computational overhead."
Grammatical information in BERT sentence embeddings as two-dimensional  arrays,"['Vivi Nastase', 'Paola Merlo']",http://arxiv.org/pdf/2312.09890v1.pdf,2023-12-15,"['cs.cl', 'i.2.7']","  Sentence embeddings induced with various transformer architectures encodemuch semantic and syntactic information in a distributed manner in aone-dimensional array. We investigate whether specific grammatical informationcan be accessed in these distributed representations. Using data from a taskdeveloped to test rule-like generalizations, our experiments on detectingsubject-verb agreement yield several promising results. First, we show thatwhile the usual sentence representations encoded as one-dimensional arrays donot easily support extraction of rule-like regularities, a two-dimensionalreshaping of these vectors allows various learning architectures to access suchinformation. Next, we show that various architectures can detect patterns inthese two-dimensional reshaped sentence embeddings and successfully learn amodel based on smaller amounts of simpler training data, which performs well onmore complex test data. This indicates that current sentence embeddings containinformation that is regularly distributed, and which can be captured when theembeddings are reshaped into higher dimensional arrays. Our results cast lighton representations produced by language models and help move towards developingfew-shot learning approaches."
Few-Shot Object Recognition from Machine-Labeled Web Images,"['Zhongwen Xu', 'Linchao Zhu', 'Yi Yang']",http://arxiv.org/pdf/1612.06152v1.pdf,2016-12-19,['cs.cv'],"  With the tremendous advances of Convolutional Neural Networks (ConvNets) onobject recognition, we can now obtain reliable enough machine-labeledannotations easily by predictions from off-the-shelf ConvNets. In this work, wepresent an abstraction memory based framework for few-shot learning, buildingupon machine-labeled image annotations. Our method takes some large-scalemachine-annotated datasets (e.g., OpenImages) as an external memory bank. Inthe external memory bank, the information is stored in the memory slots withthe form of key-value, where image feature is regarded as key and labelembedding serves as value. When queried by the few-shot examples, our modelselects visually similar data from the external memory bank, and writes theuseful information obtained from related external data into another memorybank, i.e., abstraction memory. Long Short-Term Memory (LSTM) controllers andattention mechanisms are utilized to guarantee the data written to theabstraction memory is correlated to the query example. The abstraction memoryconcentrates information from the external memory bank, so that it makes thefew-shot recognition effective. In the experiments, we firstly confirm that ourmodel can learn to conduct few-shot object recognition on clean human-labeleddata from ImageNet dataset. Then, we demonstrate that with our model,machine-labeled image annotations are very effective and abundant resources toperform object recognition on novel categories. Experimental results show thatour proposed model with machine-labeled annotations achieves great performance,only with a gap of 1% between of the one with human-labeled annotations."
A Simple Exponential Family Framework for Zero-Shot Learning,"['Vinay Kumar Verma', 'Piyush Rai']",http://arxiv.org/pdf/1707.08040v3.pdf,2017-07-25,"['cs.lg', 'cs.cv', 'stat.ml']","  We present a simple generative framework for learning to predict previouslyunseen classes, based on estimating class-attribute-gated class-conditionaldistributions. We model each class-conditional distribution as an exponentialfamily distribution and the parameters of the distribution of each seen/unseenclass are defined as functions of the respective observed class attributes.These functions can be learned using only the seen class data and can be usedto predict the parameters of the class-conditional distribution of each unseenclass. Unlike most existing methods for zero-shot learning that representclasses as fixed embeddings in some vector space, our generative modelnaturally represents each class as a probability distribution. It is simple toimplement and also allows leveraging additional unlabeled data from unseenclasses to improve the estimates of their class-conditional distributions usingtransductive/semi-supervised learning. Moreover, it extends seamlessly tofew-shot learning by easily updating these distributions when provided with asmall number of additional labelled examples from unseen classes. Through acomprehensive set of experiments on several benchmark data sets, we demonstratethe efficacy of our framework."
Variational Memory Addressing in Generative Models,"['Jörg Bornschein', 'Andriy Mnih', 'Daniel Zoran', 'Danilo J. Rezende']",http://arxiv.org/pdf/1709.07116v1.pdf,2017-09-21,['cs.lg'],"  Aiming to augment generative models with external memory, we interpret theoutput of a memory module with stochastic addressing as a conditional mixturedistribution, where a read operation corresponds to sampling a discrete memoryaddress and retrieving the corresponding content from memory. This perspectiveallows us to apply variational inference to memory addressing, which enableseffective training of the memory module by using the target information toguide memory lookups. Stochastic addressing is particularly well-suited forgenerative models as it naturally encourages multimodality which is a prominentaspect of most high-dimensional datasets. Treating the chosen address as alatent variable also allows us to quantify the amount of information gainedwith a memory lookup and measure the contribution of the memory module to thegenerative process. To illustrate the advantages of this approach weincorporate it into a variational autoencoder and apply the resulting model tothe task of generative few-shot learning. The intuition behind thisarchitecture is that the memory module can pick a relevant template from memoryand the continuous part of the model can concentrate on modeling remainingvariations. We demonstrate empirically that our model is able to identify andaccess the relevant memory contents even with hundreds of unseen Omniglotcharacters in memory"
Data Augmentation Generative Adversarial Networks,"['Antreas Antoniou', 'Amos Storkey', 'Harrison Edwards']",http://arxiv.org/pdf/1711.04340v3.pdf,2017-11-12,"['stat.ml', 'cs.cv', 'cs.lg', 'cs.ne']","  Effective training of neural networks requires much data. In the low-dataregime, parameters are underdetermined, and learnt networks generalise poorly.Data Augmentation alleviates this by using existing data more effectively.However standard data augmentation produces only limited plausible alternativedata. Given there is potential to generate a much broader set of augmentations,we design and train a generative model to do data augmentation. The model,based on image conditional Generative Adversarial Networks, takes data from asource domain and learns to take any data item and generalise it to generateother within-class data items. As this generative process does not depend onthe classes themselves, it can be applied to novel unseen classes of data. Weshow that a Data Augmentation Generative Adversarial Network (DAGAN) augmentsstandard vanilla classifiers well. We also show a DAGAN can enhance few-shotlearning systems such as Matching Networks. We demonstrate these approaches onOmniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. Inour experiments we can see over 13% increase in accuracy in the low-data regimeexperiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face(4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5%(from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%)."
Zero-Shot Learning via Class-Conditioned Deep Generative Models,"['Wenlin Wang', 'Yunchen Pu', 'Vinay Kumar Verma', 'Kai Fan', 'Yizhe Zhang', 'Changyou Chen', 'Piyush Rai', 'Lawrence Carin']",http://arxiv.org/pdf/1711.05820v2.pdf,2017-11-15,"['cs.lg', 'cs.cv']","  We present a deep generative model for learning to predict classes not seenat training time. Unlike most existing methods for this problem, that representeach class as a point (via a semantic embedding), we represent each seen/unseenclass using a class-specific latent-space distribution, conditioned on classattributes. We use these latent-space distributions as a prior for a supervisedvariational autoencoder (VAE), which also facilitates learning highlydiscriminative feature representations for the inputs. The entire framework islearned end-to-end using only the seen-class training data. The model inferscorresponding attributes of a test image by maximizing the VAE lower bound; theinferred attributes may be linked to labels not seen when training. We furtherextend our model to a (1) semi-supervised/transductive setting by leveragingunlabeled unseen-class data via an unsupervised learning module, and (2)few-shot learning where we also have a small number of labeled inputs from theunseen classes. We compare our model with several state-of-the-art methodsthrough a comprehensive set of experiments on a variety of benchmark data sets."
Museum Exhibit Identification Challenge for Domain Adaptation and Beyond,"['Piotr Koniusz', 'Yusuf Tas', 'Hongguang Zhang', 'Mehrtash Harandi', 'Fatih Porikli', 'Rui Zhang']",http://arxiv.org/pdf/1802.01093v1.pdf,2018-02-04,['cs.cv'],"  In this paper, we approach an open problem of artwork identification andpropose a new dataset dubbed Open Museum Identification Challenge (Open MIC).It contains photos of exhibits captured in 10 distinct exhibition spaces ofseveral museums which showcase paintings, timepieces, sculptures, glassware,relics, science exhibits, natural history pieces, ceramics, pottery, tools andindigenous crafts. The goal of Open MIC is to stimulate research in domainadaptation, egocentric recognition and few-shot learning by providing a testbedcomplementary to the famous Office dataset which reaches 90% accuracy. To formour dataset, we captured a number of images per art piece with a mobile phoneand wearable cameras to form the source and target data splits, respectively.To achieve robust baselines, we build on a recent approach that alignsper-class scatter matrices of the source and target CNN streams [15]. Moreover,we exploit the positive definite nature of such representations by usingend-to-end Bregman divergences and the Riemannian metric. We present baselinessuch as training/evaluation per exhibition and training/evaluation on thecombined set covering 866 exhibit identities. As each exhibition poses distinctchallenges e.g., quality of lighting, motion blur, occlusions, clutter,viewpoint and scale variations, rotations, glares, transparency, non-planarity,clipping, we break down results w.r.t. these factors."
Few-Shot Text Classification with Pre-Trained Word Embeddings and a  Human in the Loop,"['Katherine Bailey', 'Sunny Chopra']",http://arxiv.org/pdf/1804.02063v1.pdf,2018-04-05,['cs.cl'],"  Most of the literature around text classification treats it as a supervisedlearning problem: given a corpus of labeled documents, train a classifier suchthat it can accurately predict the classes of unseen documents. In industry,however, it is not uncommon for a business to have entire corpora of documentswhere few or none have been classified, or where existing classifications havebecome meaningless. With web content, for example, poor taxonomy management canresult in labels being applied indiscriminately, making filtering by theselabels unhelpful. Our work aims to make it possible to classify an entirecorpus of unlabeled documents using a human-in-the-loop approach, where thecontent owner manually classifies just one or two documents per category andthe rest can be automatically classified. This ""few-shot"" learning approachrequires rich representations of the documents such that those that have beenmanually labeled can be treated as prototypes, and automatic classification ofthe rest is a simple case of measuring the distance to prototypes. Thisapproach uses pre-trained word embeddings, where documents are representedusing a simple weighted average of constituent word embeddings. We have testedthe accuracy of the approach on existing labeled datasets and provide theresults here. We have also made code available for reproducing the results wegot on the 20 Newsgroups dataset."
Attention-based Few-Shot Person Re-identification Using Meta Learning,"['Alireza Rahimpour', 'Hairong Qi']",http://arxiv.org/pdf/1806.09613v3.pdf,2018-06-24,['cs.cv'],"  In this paper, we investigate the challenging task of personre-identification from a new perspective and propose an end-to-endattention-based architecture for few-shot re-identification throughmeta-learning. The motivation for this task lies in the fact that humans, canusually identify another person after just seeing that given person a few times(or even once) by attending to their memory. On the other hand, the uniquenature of the person re-identification problem, i.e., only few examples existper identity and new identities always appearing during testing, calls for afew shot learning architecture with the capacity of handling new identities.Hence, we frame the problem within a meta-learning setting, where a neuralnetwork based meta-learner is trained to optimize a learner i.e., anattention-based matching function. Another challenge of the personre-identification problem is the small inter-class difference between differentidentities and large intra-class difference of the same identity. In order toincrease the discriminative power of the model, we propose a newattention-based feature encoding scheme that takes into account the criticalintra-view and cross-view relationship of images. We refer to the proposedAttention-based Re-identification Metalearning model as ARM. Extensiveevaluations demonstrate the advantages of the ARM as compared to thestate-of-the-art on the challenging PRID2011, CUHK01, CUHK03 and Market1501datasets."
Open Set Chinese Character Recognition using Multi-typed Attributes,"['Sheng He', 'Lambert Schomaker']",http://arxiv.org/pdf/1808.08993v1.pdf,2018-08-27,['cs.cv'],"  Recognition of Off-line Chinese characters is still a challenging problem,especially in historical documents, not only in the number of classes extremelylarge in comparison to contemporary image retrieval methods, but also newunseen classes can be expected under open learning conditions (even for CNN).Chinese character recognition with zero or a few training samples is adifficult problem and has not been studied yet. In this paper, we propose a newChinese character recognition method by multi-type attributes, which are basedon pronunciation, structure and radicals of Chinese characters, applied tocharacter recognition in historical books. This intermediate attribute code hasa strong advantage over the common `one-hot' class representation because itallows for understanding complex and unseen patterns symbolically usingattributes. First, each character is represented by four groups of attributetypes to cover a wide range of character possibilities: Pinyin label, layoutstructure, number of strokes, three different input methods such as Cangjie,Zhengma and Wubi, as well as a four-corner encoding method. A convolutionalneural network (CNN) is trained to learn these attributes. Subsequently,characters can be easily recognized by these attributes using a distance metricand a complete lexicon that is encoded in attribute space. We evaluate theproposed method on two open data sets: printed Chinese character recognitionfor zero-shot learning, historical characters for few-shot learning and aclosed set: handwritten Chinese characters. Experimental results show a goodgeneral classification of seen classes but also a very promising generalizationability to unseen characters."
Task-Embedded Control Networks for Few-Shot Imitation Learning,"['Stephen James', 'Michael Bloesch', 'Andrew J. Davison']",http://arxiv.org/pdf/1810.03237v1.pdf,2018-10-08,"['cs.ro', 'cs.ai', 'cs.cv', 'cs.lg']","  Much like humans, robots should have the ability to leverage knowledge frompreviously learned tasks in order to learn new tasks quickly in new andunfamiliar environments. Despite this, most robot learning approaches havefocused on learning a single task, from scratch, with a limited notion ofgeneralisation, and no way of leveraging the knowledge to learn other tasksmore efficiently. One possible solution is meta-learning, but many of therelated approaches are limited in their ability to scale to a large number oftasks and to learn further tasks without forgetting previously learned ones.With this in mind, we introduce Task-Embedded Control Networks, which employideas from metric learning in order to create a task embedding that can be usedby a robot to learn new tasks from one or more demonstrations. In the area ofvisually-guided manipulation, we present simulation results in which we surpassthe performance of a state-of-the-art method when using only visual informationfrom each demonstration. Additionally, we demonstrate that our approach canalso be used in conjunction with domain randomisation to train our few-shotlearning ability in simulation and then deploy in the real world without anyadditional training. Once deployed, the robot can learn new tasks from a singlereal-world demonstration."
Class-Agnostic Counting,"['Erika Lu', 'Weidi Xie', 'Andrew Zisserman']",http://arxiv.org/pdf/1811.00472v1.pdf,2018-11-01,"['cs.cv', 'cs.lg']","  Nearly all existing counting methods are designed for a specific objectclass. Our work, however, aims to create a counting model able to count anyclass of object. To achieve this goal, we formulate counting as a matchingproblem, enabling us to exploit the image self-similarity property thatnaturally exists in object counting problems. We make the following threecontributions: first, a Generic Matching Network (GMN) architecture that canpotentially count any object in a class-agnostic manner; second, byreformulating the counting problem as one of matching objects, we can takeadvantage of the abundance of video data labeled for tracking, which containsnatural repetitions suitable for training a counting model. Such data enablesus to train the GMN. Third, to customize the GMN to different userrequirements, an adapter module is used to specialize the model with minimaleffort, i.e. using a few labeled examples, and adapting only a small fractionof the trained parameters. This is a form of few-shot learning, which ispractical for domains where labels are limited due to requiring expertknowledge (e.g. microbiology). We demonstrate the flexibility of our method ona diverse set of existing counting benchmarks: specifically cells, cars, andhuman crowds. The model achieves competitive performance on cell and crowdcounting datasets, and surpasses the state-of-the-art on the car dataset usingonly three training images. When training on the entire dataset, the proposedmethod outperforms all previous methods by a large margin."
IDD: A Dataset for Exploring Problems of Autonomous Navigation in  Unconstrained Environments,"['Girish Varma', 'Anbumani Subramanian', 'Anoop Namboodiri', 'Manmohan Chandraker', 'C V Jawahar']",http://arxiv.org/pdf/1811.10200v1.pdf,2018-11-26,"['cs.cv', 'cs.ro']","  While several datasets for autonomous navigation have become available inrecent years, they tend to focus on structured driving environments. Thisusually corresponds to well-delineated infrastructure such as lanes, a smallnumber of well-defined categories for traffic participants, low variation inobject or background appearance and strict adherence to traffic rules. Wepropose IDD, a novel dataset for road scene understanding in unstructuredenvironments where the above assumptions are largely not satisfied. It consistsof 10,004 images, finely annotated with 34 classes collected from 182 drivesequences on Indian roads. The label set is expanded in comparison to popularbenchmarks such as Cityscapes, to account for new classes. It also reflectslabel distributions of road scenes significantly different from existingdatasets, with most classes displaying greater within-class diversity.Consistent with real driving behaviours, it also identifies new classes such asdrivable areas besides the road. We propose a new four-level label hierarchy,which allows varying degrees of complexity and opens up possibilities for newtraining methods. Our empirical study provides an in-depth analysis of thelabel characteristics. State-of-the-art methods for semantic segmentationachieve much lower accuracies on our dataset, demonstrating its distinctioncompared to Cityscapes. Finally, we propose that our dataset is an idealopportunity for new problems such as domain adaptation, few-shot learning andbehaviour prediction in road scenes."
Few-shot Object Detection via Feature Reweighting,"['Bingyi Kang', 'Zhuang Liu', 'Xin Wang', 'Fisher Yu', 'Jiashi Feng', 'Trevor Darrell']",http://arxiv.org/pdf/1812.01866v2.pdf,2018-12-05,['cs.cv'],"  Conventional training of a deep CNN based object detector demands a largenumber of bounding box annotations, which may be unavailable for rarecategories. In this work we develop a few-shot object detector that can learnto detect novel objects from only a few annotated examples. Our proposed modelleverages fully labeled base classes and quickly adapts to novel classes, usinga meta feature learner and a reweighting module within a one-stage detectionarchitecture. The feature learner extracts meta features that are generalizableto detect novel object classes, using training data from base classes withsufficient samples. The reweighting module transforms a few support examplesfrom the novel classes to a global vector that indicates the importance orrelevance of meta features for detecting the corresponding objects. These twomodules, together with a detection prediction module, are trained end-to-endbased on an episodic few-shot learning scheme and a carefully designed lossfunction. Through extensive experiments we demonstrate that our modeloutperforms well-established baselines by a large margin for few-shot objectdetection, on multiple datasets and settings. We also present analysis onvarious aspects of our proposed model, aiming to provide some inspiration forfuture few-shot detection works."
GOGGLES: Automatic Image Labeling with Affinity Coding,"['Nilaksh Das', 'Sanya Chaba', 'Renzhi Wu', 'Sakshi Gandhi', 'Duen Horng Chau', 'Xu Chu']",http://arxiv.org/pdf/1903.04552v3.pdf,2019-03-11,"['cs.cv', 'cs.db']","  Generating large labeled training data is becoming the biggest bottleneck inbuilding and deploying supervised machine learning models. Recently, the dataprogramming paradigm has been proposed to reduce the human cost in labelingtraining data. However, data programming relies on designing labeling functionswhich still requires significant domain expertise. Also, it is prohibitivelydifficult to write labeling functions for image datasets as it is hard toexpress domain knowledge using raw features for images (pixels).  We propose affinity coding, a new domain-agnostic paradigm for automatedtraining data labeling. The core premise of affinity coding is that theaffinity scores of instance pairs belonging to the same class on average shouldbe higher than those of pairs belonging to different classes, according to someaffinity functions. We build the GOGGLES system that implements affinity codingfor labeling image datasets by designing a novel set of reusable affinityfunctions for images, and propose a novel hierarchical generative model forclass inference using a small development set.  We compare GOGGLES with existing data programming systems on 5 image labelingtasks from diverse domains. GOGGLES achieves labeling accuracies ranging from aminimum of 71% to a maximum of 98% without requiring any extensive humanannotation. In terms of end-to-end performance, GOGGLES outperforms thestate-of-the-art data programming system Snuba by 21% and a state-of-the-artfew-shot learning technique by 5%, and is only 7% away from the fullysupervised upper bound."
Weakly-supervised Compositional FeatureAggregation for Few-shot  Recognition,"['Ping Hu', 'Ximeng Sun', 'Kate Saenko', 'Stan Sclaroff']",http://arxiv.org/pdf/1906.04833v1.pdf,2019-06-11,"['cs.cv', 'cs.lg']","  Learning from a few examples is a challenging task for machine learning.While recent progress has been made for this problem, most of the existingmethods ignore the compositionality in visual concept representation (e.g.objects are built from parts or composed of semantic attributes), which is keyto the human ability to easily learn from a small number of examples. Toenhance the few-shot learning models with compositionality, in this paper wepresent the simple yet powerful Compositional Feature Aggregation (CFA) moduleas a weakly-supervised regularization for deep networks. Given the deep featuremaps extracted from the input, our CFA module first disentangles the featurespace into disjoint semantic subspaces that model different attributes, andthen bilinearly aggregates the local features within each of these subspaces.CFA explicitly regularizes the representation with both semantic and spatialcompositionality to produce discriminative representations for few-shotrecognition tasks. Moreover, our method does not need any supervision forattributes and object parts during training, thus can be conveniently pluggedinto existing models for end-to-end optimization while keeping the model sizeand computation cost nearly the same. Extensive experiments on few-shot imageclassification and action recognition tasks demonstrate that our methodprovides substantial improvements over recent state-of-the-art methods."
Learning Predicates as Functions to Enable Few-shot Scene Graph  Prediction,"['Apoorva Dornadula', 'Austin Narcomey', 'Ranjay Krishna', 'Michael Bernstein', 'Li Fei-Fei']",http://arxiv.org/pdf/1906.04876v4.pdf,2019-06-12,['cs.cv'],"  Scene graph prediction --- classifying the set of objects and predicates in avisual scene --- requires substantial training data. However, most predicatesonly occur a handful of times making them difficult to learn. We introduce thefirst scene graph prediction model that supports few-shot learning ofpredicates. Existing scene graph generation models represent objects usingpretrained object detectors or word embeddings that capture semantic objectinformation at the cost of encoding information about which relationships theyafford. So, these object representations are unable to generalize to newfew-shot relationships. We introduce a framework that induces objectrepresentations that are structured according to their visual relationships.Unlike past methods, our framework embeds objects that afford similarrelationships closer together. This property allows our model to perform wellin the few-shot setting. For example, applying the 'riding' predicatetransformation to 'person' modifies the representation towards objects like'skateboard' and 'horse' that enable riding. We generate object representationsby learning predicates trained as message passing functions within a new graphconvolution framework. The object representations are used to build few-shotpredicate classifiers for rare predicates with as few as 1 labeled example. Weachieve a 5-shot performance of 22.70 recall@50, a 3.7 increase when comparedto strong transfer learning baselines."
Few-Shot Sequence Labeling with Label Dependency Transfer and Pair-wise  Embedding,"['Yutai Hou', 'Zhihan Zhou', 'Yijia Liu', 'Ning Wang', 'Wanxiang Che', 'Han Liu', 'Ting Liu']",http://arxiv.org/pdf/1906.08711v3.pdf,2019-06-20,"['cs.cl', 'cs.lg']","  While few-shot classification has been widely explored with similarity basedmethods, few-shot sequence labeling poses a unique challenge as it also callsfor modeling the label dependencies. To consider both the item similarity andlabel dependency, we propose to leverage the conditional random fields (CRFs)in few-shot sequence labeling. It calculates emission score with similaritybased methods and obtains transition score with a specially designed transfermechanism. When applying CRF in the few-shot scenarios, the discrepancy oflabel sets among different domains makes it hard to use the label dependencylearned in prior domains. To tackle this, we introduce the dependency transfermechanism that transfers abstract label transition patterns. In addition, thesimilarity methods rely on the high quality sample representation, which ischallenging for sequence labeling, because sense of a word is different whenmeasuring its similarity to words in different sentences. To remedy this, wetake advantage of recent contextual embedding technique, and further propose apair-wise embedder. It provides additional certainty for word sense byembedding query and support sentence pairwisely. Experimental results on slottagging and named entity recognition show that our model significantlyoutperforms the strongest few-shot learning baseline by 11.76 (21.2%) and 12.18(97.7%) F1 scores respectively in the one-shot setting."
Meta-Learning with Implicit Gradients,"['Aravind Rajeswaran', 'Chelsea Finn', 'Sham Kakade', 'Sergey Levine']",http://arxiv.org/pdf/1909.04630v1.pdf,2019-09-10,"['cs.lg', 'cs.ai', 'math.oc', 'stat.ml']","  A core capability of intelligent systems is the ability to quickly learn newtasks by drawing on prior experience. Gradient (or optimization) basedmeta-learning has recently emerged as an effective approach for few-shotlearning. In this formulation, meta-parameters are learned in the outer loop,while task-specific models are learned in the inner-loop, by using only a smallamount of data from the current task. A key challenge in scaling theseapproaches is the need to differentiate through the inner loop learningprocess, which can impose considerable computational and memory burdens. Bydrawing upon implicit differentiation, we develop the implicit MAML algorithm,which depends only on the solution to the inner level optimization and not thepath taken by the inner loop optimizer. This effectively decouples themeta-gradient computation from the choice of inner loop optimizer. As a result,our approach is agnostic to the choice of inner loop optimizer and cangracefully handle many gradient steps without vanishing gradients or memoryconstraints. Theoretically, we prove that implicit MAML can compute accuratemeta-gradients with a memory footprint that is, up to small constant factors,no more than that which is required to compute a single inner loop gradient andat no overall increase in the total computational cost. Experimentally, we showthat these benefits of implicit MAML translate into empirical gains on few-shotimage recognition benchmarks."
Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness  of MAML,"['Aniruddh Raghu', 'Maithra Raghu', 'Samy Bengio', 'Oriol Vinyals']",http://arxiv.org/pdf/1909.09157v2.pdf,2019-09-19,"['cs.lg', 'stat.ml']","  An important research direction in machine learning has centered arounddeveloping meta-learning algorithms to tackle few-shot learning. An especiallysuccessful algorithm has been Model Agnostic Meta-Learning (MAML), a methodthat consists of two optimization loops, with the outer loop finding ameta-initialization, from which the inner loop can efficiently learn new tasks.Despite MAML's popularity, a fundamental open question remains -- is theeffectiveness of MAML due to the meta-initialization being primed for rapidlearning (large, efficient changes in the representations) or due to featurereuse, with the meta initialization already containing high quality features?We investigate this question, via ablation studies and analysis of the latentrepresentations, finding that feature reuse is the dominant factor. This leadsto the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where weremove the inner loop for all but the (task-specific) head of a MAML-trainednetwork. ANIL matches MAML's performance on benchmark few-shot imageclassification and RL and offers computational improvements over MAML. Wefurther study the precise contributions of the head and body of the network,showing that performance on the test tasks is entirely determined by thequality of the learned features, and we can remove even the head of the network(the NIL algorithm). We conclude with a discussion of the rapid learning vsfeature reuse question for meta-learning algorithms more broadly."
A Theoretical Analysis of the Number of Shots in Few-Shot Learning,"['Tianshi Cao', 'Marc Law', 'Sanja Fidler']",http://arxiv.org/pdf/1909.11722v2.pdf,2019-09-25,"['cs.lg', 'stat.ml']","  Few-shot classification is the task of predicting the category of an examplefrom a set of few labeled examples. The number of labeled examples per categoryis called the number of shots (or shot number). Recent works tackle this taskthrough meta-learning, where a meta-learner extracts information from observedtasks during meta-training to quickly adapt to new tasks during meta-testing.In this formulation, the number of shots exploited during meta-training has animpact on the recognition performance at meta-test time. Generally, the shotnumber used in meta-training should match the one used in meta-testing toobtain the best performance. We introduce a theoretical analysis of the impactof the shot number on Prototypical Networks, a state-of-the-art few-shotclassification method. From our analysis, we propose a simple method that isrobust to the choice of shot number used during meta-training, which is acrucial hyperparameter. The performance of our model trained for an arbitrarymeta-training shot number shows great performance for different values ofmeta-testing shot numbers. We experimentally demonstrate our approach ondifferent few-shot classification benchmarks."
RLBench: The Robot Learning Benchmark & Learning Environment,"['Stephen James', 'Zicong Ma', 'David Rovick Arrojo', 'Andrew J. Davison']",http://arxiv.org/pdf/1909.12271v1.pdf,2019-09-26,"['cs.ro', 'cs.ai', 'cs.cv', 'cs.lg']","  We present a challenging new benchmark and learning-environment for robotlearning: RLBench. The benchmark features 100 completely unique, hand-designedtasks ranging in difficulty, from simple target reaching and door opening, tolonger multi-stage tasks, such as opening an oven and placing a tray in it. Weprovide an array of both proprioceptive observations and visual observations,which include rgb, depth, and segmentation masks from an over-the-shoulderstereo camera and an eye-in-hand monocular camera. Uniquely, each task comeswith an infinite supply of demos through the use of motion planners operatingon a series of waypoints given during task creation time; enabling an excitingflurry of demonstration-based learning. RLBench has been designed withscalability in mind; new tasks, along with their motion-planned demos, can beeasily created and then verified by a series of tools, allowing users to submittheir own tasks to the RLBench task repository. This large-scale benchmark aimsto accelerate progress in a number of vision-guided manipulation researchareas, including: reinforcement learning, imitation learning, multi-tasklearning, geometric computer vision, and in particular, few-shot learning. Withthe benchmark's breadth of tasks and demonstrations, we propose the firstlarge-scale few-shot challenge in robotics. We hope that the scale anddiversity of RLBench offers unparalleled research opportunities in the robotlearning community and beyond."
BEAN: Interpretable Representation Learning with Biologically-Enhanced  Artificial Neuronal Assembly Regularization,"['Yuyang Gao', 'Giorgio A. Ascoli', 'Liang Zhao']",http://arxiv.org/pdf/1909.13698v2.pdf,2019-09-27,"['cs.ne', 'cs.lg', 'stat.ml']","  Deep neural networks (DNNs) are known for extracting useful information fromlarge amounts of data. However, the representations learned in DNNs aretypically hard to interpret, especially in dense layers. One crucial issue ofthe classical DNN model such as multilayer perceptron (MLP) is that neurons inthe same layer of DNNs are conditionally independent of each other, which makesco-training and emergence of higher modularity difficult. In contrast to DNNs,biological neurons in mammalian brains display substantial dependency patterns.Specifically, biological neural networks encode representations by so-calledneuronal assemblies: groups of neurons interconnected by strong synapticinteractions and sharing joint semantic content. The resulting populationcoding is essential for human cognitive and mnemonic processes. Here, wepropose a novel Biologically Enhanced Artificial Neuronal assembly (BEAN)regularization to model neuronal correlations and dependencies, inspired bycell assembly theory from neuroscience. Experimental results show that BEANenables the formation of interpretable neuronal functional clusters andconsequently promotes a sparse, memory/computation-efficient network withoutloss of model performance. Moreover, our few-shot learning experimentsdemonstrate that BEAN could also enhance the generalizability of the model whentraining samples are extremely limited."
Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge  Transfer Networks,"['Igor Shalyminov', 'Sungjin Lee', 'Arash Eshghi', 'Oliver Lemon']",http://arxiv.org/pdf/1910.01302v1.pdf,2019-10-03,"['cs.cl', 'i.2.7']","  Goal-oriented dialogue systems are now being widely adopted in industry whereit is of key importance to maintain a rapid prototyping cycle for new productsand domains. Data-driven dialogue system development has to be adapted to meetthis requirement --- therefore, reducing the amount of data and annotationsnecessary for training such systems is a central research problem.  In this paper, we present the Dialogue Knowledge Transfer Network (DiKTNet),a state-of-the-art approach to goal-oriented dialogue generation which onlyuses a few example dialogues (i.e. few-shot learning), none of which has to beannotated. We achieve this by performing a 2-stage training. Firstly, weperform unsupervised dialogue representation pre-training on a large source ofgoal-oriented dialogues in multiple domains, the MetaLWOz corpus. Secondly, atthe transfer stage, we train DiKTNet using this representation together with 2other textual knowledge sources with different levels of generality: ELMoencoder and the main dataset's source domains.  Our main dataset is the Stanford Multi-Domain dialogue corpus. We evaluateour model on it in terms of BLEU and Entity F1 scores, and show that ourapproach significantly and consistently improves upon a series of baselinemodels as well as over the previous state-of-the-art dialogue generation model,ZSDG. The improvement upon the latter --- up to 10% in Entity F1 and theaverage of 3% in BLEU score --- is achieved using only the equivalent of 10% ofZSDG's in-domain training data."
Artistic Glyph Image Synthesis via One-Stage Few-Shot Learning,"['Yue Gao', 'Yuan Guo', 'Zhouhui Lian', 'Yingmin Tang', 'Jianguo Xiao']",http://arxiv.org/pdf/1910.04987v2.pdf,2019-10-11,"['cs.cv', 'cs.gr']","  Automatic generation of artistic glyph images is a challenging task thatattracts many research interests. Previous methods either are specificallydesigned for shape synthesis or focus on texture transfer. In this paper, wepropose a novel model, AGIS-Net, to transfer both shape and texture styles inone-stage with only a few stylized samples. To achieve this goal, we firstdisentangle the representations for content and style by using two encoders,ensuring the multi-content and multi-style generation. Then we utilize twocollaboratively working decoders to generate the glyph shape image and itstexture image simultaneously. In addition, we introduce a local texturerefinement loss to further improve the quality of the synthesized textures. Inthis manner, our one-stage model is much more efficient and effective thanother multi-stage stacked methods. We also propose a large-scale dataset withChinese glyph images in various shape and texture styles, rendered from 35professional-designed artistic fonts with 7,326 characters and 2,460 syntheticartistic fonts with 639 characters, to validate the effectiveness andextendability of our method. Extensive experiments on both English and Chineseartistic glyph image datasets demonstrate the superiority of our model ingenerating high-quality stylized glyph images against other state-of-the-artmethods."
Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels,"['Massimiliano Patacchiola', 'Jack Turner', 'Elliot J. Crowley', ""Michael O'Boyle"", 'Amos Storkey']",http://arxiv.org/pdf/1910.05199v4.pdf,2019-10-11,"['cs.lg', 'stat.ml']","  Recently, different machine learning methods have been introduced to tacklethe challenging few-shot learning scenario that is, learning from a smalllabeled dataset related to a specific task. Common approaches have taken theform of meta-learning: learning to learn on the new problem given the old.Following the recognition that meta-learning is implementing learning in amulti-level model, we present a Bayesian treatment for the meta-learning innerloop through the use of deep kernels. As a result we can learn a kernel thattransfers to new tasks; we call this Deep Kernel Transfer (DKT). This approachhas many advantages: is straightforward to implement as a single optimizer,provides uncertainty quantification, and does not require estimation oftask-specific parameters. We empirically demonstrate that DKT outperformsseveral state-of-the-art algorithms in few-shot classification, and is thestate of the art for cross-domain adaptation and regression. We conclude thatcomplex meta-learning routines can be replaced by a simpler Bayesian modelwithout loss of accuracy."
"Face Behavior a la carte: Expressions, Affect and Action Units in a  Single Network","['Dimitrios Kollias', 'Viktoriia Sharmanska', 'Stefanos Zafeiriou']",http://arxiv.org/pdf/1910.11111v3.pdf,2019-10-15,"['cs.cv', 'cs.hc', 'cs.lg', 'stat.ml']","  Automatic facial behavior analysis has a long history of studies in theintersection of computer vision, physiology and psychology. However it is onlyrecently, with the collection of large-scale datasets and powerful machinelearning methods such as deep neural networks, that automatic facial behavioranalysis started to thrive. Three of its iconic tasks are automatic recognitionof basic expressions (e.g. happy, sad, surprised), estimation of continuousemotions (e.g., valence and arousal), and detection of facial action units(activations of e.g. upper/inner eyebrows, nose wrinkles). Up until now thesetasks have been mostly studied independently collecting a dataset for the task.We present the first and the largest study of all facial behaviour taskslearned jointly in a single multi-task, multi-domain and multi-label network,which we call FaceBehaviorNet. For this we utilize all publicly availabledatasets in the community (around 5M images) that study facial behaviour tasksin-the-wild. We demonstrate that training jointly an end-to-end network for alltasks has consistently better performance than training each of the single-tasknetworks. Furthermore, we propose two simple strategies for coupling the tasksduring training, co-annotation and distribution matching, and show theadvantages of this approach. Finally we show that FaceBehaviorNet has learnedfeatures that encapsulate all aspects of facial behaviour, and can besuccessfully applied to perform tasks (compound emotion recognition) beyond theones that it has been trained in a zero- and few-shot learning setting."
Meta-Learning with Dynamic-Memory-Based Prototypical Network for  Few-Shot Event Detection,"['Shumin Deng', 'Ningyu Zhang', 'Jiaojian Kang', 'Yichi Zhang', 'Wei Zhang', 'Huajun Chen']",http://arxiv.org/pdf/1910.11621v2.pdf,2019-10-25,"['cs.cl', 'cs.ai', 'cs.ir', 'cs.lg']","  Event detection (ED), a sub-task of event extraction, involves identifyingtriggers and categorizing event mentions. Existing methods primarily rely uponsupervised learning and require large-scale labeled event datasets which areunfortunately not readily available in many real-life applications. In thispaper, we consider and reformulate the ED task with limited labeled data as aFew-Shot Learning problem. We propose a Dynamic-Memory-Based PrototypicalNetwork (DMB-PN), which exploits Dynamic Memory Network (DMN) to not only learnbetter prototypes for event types, but also produce more robust sentenceencodings for event mentions. Differing from vanilla prototypical networkssimply computing event prototypes by averaging, which only consume eventmentions once, our model is more robust and is capable of distilling contextualinformation from event mentions for multiple times due to the multi-hopmechanism of DMNs. The experiments show that DMB-PN not only deals with samplescarcity better than a series of baseline models but also performs morerobustly when the variety of event types is relatively large and the instancequantity is extremely small."
Shoestring: Graph-Based Semi-Supervised Learning with Severely Limited  Labeled Data,"['Wanyu Lin', 'Zhaolin Gao', 'Baochun Li']",http://arxiv.org/pdf/1910.12976v2.pdf,2019-10-28,"['cs.cv', 'cs.lg']","  Graph-based semi-supervised learning has been shown to be one of the mosteffective approaches for classification tasks from a wide range of domains,such as image classification and text classification, as they can exploit theconnectivity patterns between labeled and unlabeled samples to improve learningperformance. In this work, we advance this effective learning paradigm towardsa scenario where labeled data are severely limited. More specifically, weaddress the problem of graph-based semi-supervised learning in the presence ofseverely limited labeled samples, and propose a new framework, called {\emShoestring}, that improves the learning performance through semantic transferfrom these very few labeled samples to large numbers of unlabeled samples.  In particular, our framework learns a metric space in which classificationcan be performed by computing the similarity to centroid embedding of eachclass. {\em Shoestring} is trained in an end-to-end fashion to learn toleverage the semantic knowledge of limited labeled samples as well as theirconnectivity patterns with large numbers of unlabeled samples simultaneously.By combining {\em Shoestring} with graph convolutional networks, labelpropagation and their recent label-efficient variations (IGCN and GLP), we areable to achieve state-of-the-art node classification performance in thepresence of very few labeled samples. In addition, we demonstrate theeffectiveness of our framework on image classification tasks in the few-shotlearning regime, with significant gains on miniImageNet ($2.57\%\sim3.59\%$)and tieredImageNet ($1.05\%\sim2.70\%$)."
Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation,"['Risto Vuorio', 'Shao-Hua Sun', 'Hexiang Hu', 'Joseph J. Lim']",http://arxiv.org/pdf/1910.13616v1.pdf,2019-10-30,"['cs.lg', 'cs.ai', 'stat.ml']","  Model-agnostic meta-learners aim to acquire meta-learned parameters fromsimilar tasks to adapt to novel tasks from the same distribution with fewgradient updates. With the flexibility in the choice of models, thoseframeworks demonstrate appealing performance on a variety of domains such asfew-shot image classification and reinforcement learning. However, oneimportant limitation of such frameworks is that they seek a commoninitialization shared across the entire task distribution, substantiallylimiting the diversity of the task distributions that they are able to learnfrom. In this paper, we augment MAML with the capability to identify the modeof tasks sampled from a multimodal task distribution and adapt quickly throughgradient updates. Specifically, we propose a multimodal MAML (MMAML) framework,which is able to modulate its meta-learned prior parameters according to theidentified mode, allowing more efficient fast adaptation. We evaluate theproposed model on a diverse set of few-shot learning tasks, includingregression, image classification, and reinforcement learning. The results notonly demonstrate the effectiveness of our model in modulating the meta-learnedprior in response to the characteristics of tasks but also show that trainingon a multimodal distribution can produce an improvement over unimodal training."
Metric Learning with Background Noise Class for Few-shot Detection of  Rare Sound Events,"['Kazuki Shimada', 'Yuichiro Koyama', 'Akira Inoue']",http://arxiv.org/pdf/1910.13724v2.pdf,2019-10-30,"['eess.as', 'cs.lg', 'cs.sd']","  Few-shot learning systems for sound event recognition have gained interestssince they require only a few examples to adapt to new target classes withoutfine-tuning. However, such systems have only been applied to chunks of soundsfor classification or verification. In this paper, we aim to achieve few-shotdetection of rare sound events, from query sequence that contain not only thetarget events but also the other events and background noise. Therefore, it isrequired to prevent false positive reactions to both the other events andbackground noise. We propose metric learning with background noise class forthe few-shot detection. The contribution is to present the explicit inclusionof background noise as an independent class, a suitable loss function thatemphasizes this additional class, and a corresponding sampling strategy thatassists training. It provides a feature space where the event classes and thebackground noise class are sufficiently separated. Evaluations on few-shotdetection tasks, using DCASE 2017 task2 and ESC-50, show that our proposedmethod outperforms metric learning without considering the background noiseclass. The few-shot detection performance is also comparable to that of theDCASE 2017 task2 baseline system, which requires huge amount of annotated audiodata."
A Neural Topic-Attention Model for Medical Term Abbreviation  Disambiguation,"['Irene Li', 'Michihiro Yasunaga', 'Muhammed Yavuz Nuzumlalı', 'Cesar Caraballo', 'Shiwani Mahajan', 'Harlan Krumholz', 'Dragomir Radev']",http://arxiv.org/pdf/1910.14076v1.pdf,2019-10-30,['cs.cl'],"  Automated analysis of clinical notes is attracting increasing attention.However, there has not been much work on medical term abbreviationdisambiguation. Such abbreviations are abundant, and highly ambiguous, inclinical documents. One of the main obstacles is the lack of large scale,balance labeled data sets. To address the issue, we propose a few-shot learningapproach to take advantage of limited labeled data. Specifically, a neuraltopic-attention model is applied to learn improved contextualized sentencerepresentations for medical term abbreviation disambiguation. Another vitalissue is that the existing scarce annotations are noisy and missing. Were-examine and correct an existing dataset for training and collect a test setto evaluate the models fairly especially for rare senses. We train our model onthe training set which contains 30 abbreviation terms as categories (onaverage, 479 samples and 3.24 classes in each term) selected from a publicabbreviation disambiguation dataset, and then test on a manually-createdbalanced dataset (each class in each term has 15 samples). We show thatenhancing the sentence representation with topic information improves theperformance on small-scale unbalanced training datasets by a large margin,compared to a number of baseline models."
CLOSURE: Assessing Systematic Generalization of CLEVR Models,"['Dzmitry Bahdanau', 'Harm de Vries', ""Timothy J. O'Donnell"", 'Shikhar Murty', 'Philippe Beaudoin', 'Yoshua Bengio', 'Aaron Courville']",http://arxiv.org/pdf/1912.05783v2.pdf,2019-12-12,"['cs.ai', 'cs.lg']","  The CLEVR dataset of natural-looking questions about 3D-rendered scenes hasrecently received much attention from the research community. A number ofmodels have been proposed for this task, many of which achieved very highaccuracies of around 97-99%. In this work, we study how systematic thegeneralization of such models is, that is to which extent they are capable ofhandling novel combinations of known linguistic constructs. To this end, wetest models' understanding of referring expressions based on matching objectproperties (such as e.g. ""another cube that is the same size as the browncube"") in novel contexts. Our experiments on the thereby constructed CLOSUREbenchmark show that state-of-the-art models often do not exhibit systematicityafter being trained on CLEVR. Surprisingly, we find that an explicitlycompositional Neural Module Network model also generalizes badly on CLOSURE,even when it has access to the ground-truth programs at test time. We improvethe NMN's systematic generalization by developing a novel Vector-NMN modulearchitecture with vector-valued inputs and outputs. Lastly, we investigate howmuch few-shot transfer learning can help models that are pretrained on CLEVR toadapt to CLOSURE. Our few-shot learning experiments contrast the adaptationbehavior of the models with intermediate discrete programs with that of theend-to-end continuous models."
Towards Contextual Learning in Few-shot Object Classification,"['Mathieu Pagé Fortin', 'Brahim Chaib-draa']",http://arxiv.org/pdf/1912.06679v3.pdf,2019-12-13,['cs.cv'],"  Few-shot Learning (FSL) aims to classify new concepts from a small number ofexamples. While there have been an increasing amount of work on few-shot objectclassification in the last few years, most current approaches are limited toimages with only one centered object. On the opposite, humans are able toleverage prior knowledge to quickly learn new concepts, such as semanticrelations with contextual elements. Inspired by the concept of contextuallearning in educational sciences, we propose to make a step towards adoptingthis principle in FSL by studying the contribution that context can have inobject classification in a low-data regime. To this end, we first propose anapproach to perform FSL on images of complex scenes. We develop twoplug-and-play modules that can be incorporated into existing FSL methods toenable them to leverage contextual learning. More specifically, these modulesare trained to weight the most important context elements while learning aparticular concept, and then use this knowledge to ground visual classrepresentations in context semantics. Extensive experiments on Visual Genomeand Open Images show the superiority of contextual learning over learningindividual objects in isolation."
Dependable Neural Networks for Safety Critical Tasks,"[""Molly O'Brien"", 'William Goble', 'Greg Hager', 'Julia Bukowski']",http://arxiv.org/pdf/1912.09902v1.pdf,2019-12-20,"['cs.lg', 'stat.ml']","  Neural Networks are being integrated into safety critical systems, e.g.,perception systems for autonomous vehicles, which require trained networks toperform safely in novel scenarios. It is challenging to verify neural networksbecause their decisions are not explainable, they cannot be exhaustivelytested, and finite test samples cannot capture the variation across alloperating conditions. Existing work seeks to train models robust to newscenarios via domain adaptation, style transfer, or few-shot learning. Butthese techniques fail to predict how a trained model will perform when theoperating conditions differ from the testing conditions. We propose a metric,Machine Learning (ML) Dependability, that measures the network's probability ofsuccess in specified operating conditions which need not be the testingconditions. In addition, we propose the metrics Task Undependability andHarmful Undependability to distinguish network failures by their consequences.We evaluate the performance of a Neural Network agent trained usingReinforcement Learning in a simulated robot manipulation task. Our resultsdemonstrate that we can accurately predict the ML Dependability, TaskUndependability, and Harmful Undependability for operating conditions that aresignificantly different from the testing conditions. Finally, we design aSafety Function, using harmful failures identified during testing, that reducesharmful failures, in one example, by a factor of 700 while maintaining a highprobability of success."
Variational Metric Scaling for Metric-Based Meta-Learning,"['Jiaxin Chen', 'Li-Ming Zhan', 'Xiao-Ming Wu', 'Fu-lai Chung']",http://arxiv.org/pdf/1912.11809v2.pdf,2019-12-26,"['cs.lg', 'stat.ml']","  Metric-based meta-learning has attracted a lot of attention due to itseffectiveness and efficiency in few-shot learning. Recent studies show thatmetric scaling plays a crucial role in the performance of metric-basedmeta-learning algorithms. However, there still lacks a principled method forlearning the metric scaling parameter automatically. In this paper, we recastmetric-based meta-learning from a Bayesian perspective and develop avariational metric scaling framework for learning a proper metric scalingparameter. Firstly, we propose a stochastic variational method to learn asingle global scaling parameter. To better fit the embedding space to a givendata distribution, we extend our method to learn a dimensional scaling vectorto transform the embedding space. Furthermore, to learn task-specificembeddings, we generate task-dependent dimensional scaling vectors withamortized variational inference. Our method is end-to-end without anypre-training and can be used as a simple plug-and-play module for existingmetric-based meta-algorithms. Experiments on mini-ImageNet show that ourmethods can be used to consistently improve the performance of existingmetric-based meta-algorithms including prototypical networks and TADAM. Thesource code can be downloaded fromhttps://github.com/jiaxinchen666/variational-scaling."
Few-Shot Microscopy Image Cell Segmentation,"['Youssef Dawoud', 'Julia Hornauer', 'Gustavo Carneiro', 'Vasileios Belagiannis']",http://arxiv.org/pdf/2007.01671v1.pdf,2020-06-29,"['cs.cv', 'cs.lg', 'stat.ml']","  Automatic cell segmentation in microscopy images works well with the supportof deep neural networks trained with full supervision. Collecting andannotating images, though, is not a sustainable solution for every newmicroscopy database and cell type. Instead, we assume that we can access aplethora of annotated image data sets from different domains (sources) and alimited number of annotated image data sets from the domain of interest(target), where each domain denotes not only different image appearance butalso a different type of cell segmentation problem. We pose this problem asmeta-learning where the goal is to learn a generic and adaptable few-shotlearning model from the available source domain data sets and cell segmentationtasks. The model can be afterwards fine-tuned on the few annotated images ofthe target domain that contains different image appearance and different celltype. In our meta-learning training, we propose the combination of threeobjective functions to segment the cells, move the segmentation results awayfrom the classification boundary using cross-domain tasks, and learn aninvariant representation between tasks of the source domains. Our experimentson five public databases show promising results from 1- to 10-shotmeta-learning using standard segmentation neural network architectures."
Few-Shot One-Class Classification via Meta-Learning,"['Ahmed Frikha', 'Denis Krompaß', 'Hans-Georg Köpken', 'Volker Tresp']",http://arxiv.org/pdf/2007.04146v2.pdf,2020-07-08,"['cs.lg', 'stat.ml']","  Although few-shot learning and one-class classification (OCC), i.e., learninga binary classifier with data from only one class, have been separately wellstudied, their intersection remains rather unexplored. Our work addresses thefew-shot OCC problem and presents a method to modify the episodic data samplingstrategy of the model-agnostic meta-learning (MAML) algorithm to learn a modelinitialization particularly suited for learning few-shot OCC tasks. This isdone by explicitly optimizing for an initialization which only requires fewgradient steps with one-class minibatches to yield a performance increase onclass-balanced test data. We provide a theoretical analysis that explains whyour approach works in the few-shot OCC scenario, while other meta-learningalgorithms fail, including the unmodified MAML. Our experiments on eightdatasets from the image and time-series domains show that our method leads tobetter results than classical OCC and few-shot classification approaches, anddemonstrate the ability to learn unseen tasks from only few normal classsamples. Moreover, we successfully train anomaly detectors for a real-worldapplication on sensor readings recorded during industrial manufacturing ofworkpieces with a CNC milling machine, by using few normal examples. Finally,we empirically demonstrate that the proposed data sampling technique increasesthe performance of more recent meta-learning algorithms in few-shot OCC andyields state-of-the-art results in this problem setting."
"Evaluation for Weakly Supervised Object Localization: Protocol, Metrics,  and Datasets","['Junsuk Choe', 'Seong Joon Oh', 'Sanghyuk Chun', 'Seungho Lee', 'Zeynep Akata', 'Hyunjung Shim']",http://arxiv.org/pdf/2007.04178v2.pdf,2020-07-08,['cs.cv'],"  Weakly-supervised object localization (WSOL) has gained popularity over thelast years for its promise to train localization models with only image-levellabels. Since the seminal WSOL work of class activation mapping (CAM), thefield has focused on how to expand the attention regions to cover objects morebroadly and localize them better. However, these strategies rely on fulllocalization supervision for validating hyperparameters and model selection,which is in principle prohibited under the WSOL setup. In this paper, we arguethat WSOL task is ill-posed with only image-level labels, and propose a newevaluation protocol where full supervision is limited to only a small held-outset not overlapping with the test set. We observe that, under our protocol, thefive most recent WSOL methods have not made a major improvement over the CAMbaseline. Moreover, we report that existing WSOL methods have not reached thefew-shot learning baseline, where the full-supervision at validation time isused for model training instead. Based on our findings, we discuss some futuredirections for WSOL."
Explanation-Guided Training for Cross-Domain Few-Shot Classification,"['Jiamei Sun', 'Sebastian Lapuschkin', 'Wojciech Samek', 'Yunqing Zhao', 'Ngai-Man Cheung', 'Alexander Binder']",http://arxiv.org/pdf/2007.08790v2.pdf,2020-07-17,"['cs.cv', 'cs.lg']","  Cross-domain few-shot classification task (CD-FSC) combines few-shotclassification with the requirement to generalize across domains represented bydatasets. This setup faces challenges originating from the limited labeled datain each class and, additionally, from the domain shift between training andtest sets. In this paper, we introduce a novel training approach for existingFSC models. It leverages on the explanation scores, obtained from existingexplanation methods when applied to the predictions of FSC models, computed forintermediate feature maps of the models. Firstly, we tailor the layer-wiserelevance propagation (LRP) method to explain the predictions of FSC models.Secondly, we develop a model-agnostic explanation-guided training strategy thatdynamically finds and emphasizes the features which are important for thepredictions. Our contribution does not target a novel explanation method butlies in a novel application of explanations for the training phase. We showthat explanation-guided training effectively improves the model generalization.We observe improved accuracy for three different FSC models: RelationNet, crossattention network, and a graph neural network-based formulation, on fivefew-shot learning datasets: miniImagenet, CUB, Cars, Places, and Plantae. Thesource code is available at https://github.com/SunJiamei/few-shot-lrp-guided"
Annotation-Efficient Untrimmed Video Action Recognition,"['Yixiong Zou', 'Shanghang Zhang', 'Guangyao Chen', 'Yonghong Tian', 'Kurt Keutzer', 'José M. F. Moura']",http://arxiv.org/pdf/2011.14478v3.pdf,2020-11-30,['cs.cv'],"  Deep learning has achieved great success in recognizing video actions, butthe collection and annotation of training data are still quite laborious, whichmainly lies in two aspects: (1) the amount of required annotated data is large;(2) temporally annotating the location of each action is time-consuming. Workssuch as few-shot learning or untrimmed video recognition have been proposed tohandle either one aspect or the other. However, very few existing works canhandle both issues simultaneously. In this paper, we target a new problem,Annotation-Efficient Video Recognition, to reduce the requirement ofannotations for both large amount of samples and the action location. Suchproblem is challenging due to two aspects: (1) the untrimmed videos only haveweak supervision; (2) video segments not relevant to current actions ofinterests (background, BG) could contain actions of interests (foreground, FG)in novel classes, which is a widely existing phenomenon but has rarely beenstudied in few-shot untrimmed video recognition. To achieve this goal, byanalyzing the property of BG, we categorize BG into informative BG (IBG) andnon-informative BG (NBG), and we propose (1) an open-set detection based methodto find the NBG and FG, (2) a contrastive learning method to learn IBG anddistinguish NBG in a self-supervised way, and (3) a self-weighting mechanismfor the better distinguishing of IBG and FG. Extensive experiments onActivityNet v1.2 and ActivityNet v1.3 verify the rationale and effectiveness ofthe proposed methods."
Feature Learning in Infinite-Width Neural Networks,"['Greg Yang', 'Edward J. Hu']",http://arxiv.org/pdf/2011.14522v3.pdf,2020-11-30,"['cs.lg', 'cond-mat.dis-nn', 'cs.ne']","  As its width tends to infinity, a deep neural network's behavior undergradient descent can become simplified and predictable (e.g. given by theNeural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTKparametrization). However, we show that the standard and NTK parametrizationsof a neural network do not admit infinite-width limits that can learn features,which is crucial for pretraining and transfer learning such as with BERT. Wepropose simple modifications to the standard parametrization to allow forfeature learning in the limit. Using the *Tensor Programs* technique, we deriveexplicit formulas for such limits. On Word2Vec and few-shot learning onOmniglot via MAML, two canonical tasks that rely crucially on feature learning,we compute these limits exactly. We find that they outperform both NTKbaselines and finite-width networks, with the latter approaching theinfinite-width feature learning performance as width increases.  More generally, we classify a natural space of neural networkparametrizations that generalizes standard, NTK, and Mean Fieldparametrizations. We show 1) any parametrization in this space either admitsfeature learning or has an infinite-width training dynamics given by kernelgradient descent, but not both; 2) any such infinite-width limit can becomputed using the Tensor Programs technique. Code for our experiments can befound at github.com/edwardjhu/TP4."
Revisiting Unsupervised Meta-Learning via the Characteristics of  Few-Shot Tasks,"['Han-Jia Ye', 'Lu Han', 'De-Chuan Zhan']",http://arxiv.org/pdf/2011.14663v3.pdf,2020-11-30,['cs.cv'],"  Meta-learning has become a practical approach towards few-shot imageclassification, where ""a strategy to learn a classifier"" is meta-learned onlabeled base classes and can be applied to tasks with novel classes. We removethe requirement of base class labels and learn generalizable embeddings viaUnsupervised Meta-Learning (UML). Specifically, episodes of tasks areconstructed with data augmentations from unlabeled base classes duringmeta-training, and we apply embedding-based classifiers to novel tasks withlabeled few-shot examples during meta-test. We observe two elements playimportant roles in UML, i.e., the way to sample tasks and measure similaritiesbetween instances. Thus we obtain a strong baseline with two simplemodifications -- a sufficient sampling strategy constructing multiple tasks perepisode efficiently together with a semi-normalized similarity. We then takeadvantage of the characteristics of tasks from two directions to get furtherimprovements. First, synthesized confusing instances are incorporated to helpextract more discriminative embeddings. Second, we utilize an additionaltask-specific embedding transformation as an auxiliary component duringmeta-training to promote the generalization ability of the pre-adaptedembeddings. Experiments on few-shot learning benchmarks verify that ourapproaches outperform previous UML methods and achieve comparable or evenbetter performance than its supervised variants."
What Makes Good In-Context Examples for GPT-$3$?,"['Jiachang Liu', 'Dinghan Shen', 'Yizhe Zhang', 'Bill Dolan', 'Lawrence Carin', 'Weizhu Chen']",http://arxiv.org/pdf/2101.06804v1.pdf,2021-01-17,['cs.cl'],"  GPT-$3$ has attracted lots of attention due to its superior performanceacross a wide range of NLP tasks, especially with its powerful and versatilein-context few-shot learning ability. Despite its success, we found that theempirical results of GPT-$3$ depend heavily on the choice of in-contextexamples. In this work, we investigate whether there are more effectivestrategies for judiciously selecting in-context examples (relative to randomsampling) that better leverage GPT-$3$'s few-shot capabilities. Inspired by therecent success of leveraging a retrieval module to augment large-scale neuralnetwork models, we propose to retrieve examples that are semantically-similarto a test sample to formulate its corresponding prompt. Intuitively, thein-context examples selected with such a strategy may serve as more informativeinputs to unleash GPT-$3$'s extensive knowledge. We evaluate the proposedapproach on several natural language understanding and generation benchmarks,where the retrieval-based prompt selection approach consistently outperformsthe random baseline. Moreover, it is observed that the sentence encodersfine-tuned on task-related datasets yield even more helpful retrieval results.Notably, significant gains are observed on tasks such as table-to-textgeneration (41.9% on the ToTTo dataset) and open-domain question answering(45.5% on the NQ dataset). We hope our investigation could help understand thebehaviors of GPT-$3$ and large-scale pre-trained LMs in general and enhancetheir few-shot capabilities."
Piecewise classifier mappings: Learning fine-grained learners for novel  categories with few examples,"['Xiu-Shen Wei', 'Peng Wang', 'Lingqiao Liu', 'Chunhua Shen', 'Jianxin Wu']",http://arxiv.org/pdf/1805.04288v2.pdf,2018-05-11,['cs.cv'],"  Humans are capable of learning a new fine-grained concept with very littlesupervision, \emph{e.g.}, few exemplary images for a species of bird, yet ourbest deep learning systems need hundreds or thousands of labeled examples. Inthis paper, we try to reduce this gap by studying the fine-grained imagerecognition problem in a challenging few-shot learning setting, termed few-shotfine-grained recognition (FSFG). The task of FSFG requires the learning systemsto build classifiers for novel fine-grained categories from few examples (onlyone or less than five). To solve this problem, we propose an end-to-endtrainable deep network which is inspired by the state-of-the-art fine-grainedrecognition model and is tailored for the FSFG task.  Specifically, our network consists of a bilinear feature learning module anda classifier mapping module: while the former encodes the discriminativeinformation of an exemplar image into a feature vector, the latter maps theintermediate feature into the decision boundary of the novel category. The keynovelty of our model is a ""piecewise mappings"" function in the classifiermapping module, which generates the decision boundary via learning a set ofmore attainable sub-classifiers in a more parameter-economic way. We learn theexemplar-to-classifier mapping based on an auxiliary dataset in a meta-learningfashion, which is expected to be able to generalize to novel categories. Byconducting comprehensive experiments on three fine-grained datasets, wedemonstrate that the proposed method achieves superior performance over thecompeting baselines."
A Closer Look at Few-shot Classification,"['Wei-Yu Chen', 'Yen-Cheng Liu', 'Zsolt Kira', 'Yu-Chiang Frank Wang', 'Jia-Bin Huang']",http://arxiv.org/pdf/1904.04232v2.pdf,2019-04-08,['cs.cv'],"  Few-shot classification aims to learn a classifier to recognize unseenclasses during training with limited labeled examples. While significantprogress has been made, the growing complexity of network designs,meta-learning algorithms, and differences in implementation details make a faircomparison difficult. In this paper, we present 1) a consistent comparativeanalysis of several representative few-shot classification algorithms, withresults showing that deeper backbones significantly reduce the performancedifferences among methods on datasets with limited domain differences, 2) amodified baseline method that surprisingly achieves competitive performancewhen compared with the state-of-the-art on both the \miniI and the CUBdatasets, and 3) a new experimental setting for evaluating the cross-domaingeneralization ability for few-shot classification algorithms. Our resultsreveal that reducing intra-class variation is an important factor when thefeature backbone is shallow, but not as critical when using deeper backbones.In a realistic cross-domain evaluation setting, we show that a baseline methodwith a standard fine-tuning practice compares favorably against otherstate-of-the-art few-shot learning algorithms."
Large-Scale Long-Tailed Recognition in an Open World,"['Ziwei Liu', 'Zhongqi Miao', 'Xiaohang Zhan', 'Jiayun Wang', 'Boqing Gong', 'Stella X. Yu']",http://arxiv.org/pdf/1904.05160v2.pdf,2019-04-10,"['cs.cv', 'cs.lg']","  Real world data often have a long-tailed and open-ended distribution. Apractical recognition system must classify among majority and minority classes,generalize from a few known instances, and acknowledge novelty upon a neverseen instance. We define Open Long-Tailed Recognition (OLTR) as learning fromsuch naturally distributed data and optimizing the classification accuracy overa balanced test set which include head, tail, and open classes. OLTR musthandle imbalanced classification, few-shot learning, and open-set recognitionin one integrated algorithm, whereas existing classification approaches focusonly on one aspect and deliver poorly over the entire class spectrum. The keychallenges are how to share visual knowledge between head and tail classes andhow to reduce confusion between tail and open classes. We develop an integratedOLTR algorithm that maps an image to a feature space such that visual conceptscan easily relate to each other based on a learned metric that respects theclosed-world classification while acknowledging the novelty of the open world.Our so-called dynamic meta-embedding combines a direct image feature and anassociated memory feature, with the feature norm indicating the familiarity toknown classes. On three large-scale OLTR datasets we curate from object-centricImageNet, scene-centric Places, and face-centric MS1M data, our methodconsistently outperforms the state-of-the-art. Our code, datasets, and modelsenable future OLTR research and are publicly available athttps://liuziwei7.github.io/projects/LongTail.html."
Synthetic Examples Improve Generalization for Rare Classes,"['Sara Beery', 'Yang Liu', 'Dan Morris', 'Jim Piavis', 'Ashish Kapoor', 'Markus Meister', 'Neel Joshi', 'Pietro Perona']",http://arxiv.org/pdf/1904.05916v2.pdf,2019-04-11,['cs.cv'],"  The ability to detect and classify rare occurrences in images has importantapplications - for example, counting rare and endangered species when studyingbiodiversity, or detecting infrequent traffic scenarios that pose a danger toself-driving cars. Few-shot learning is an open problem: current computervision systems struggle to categorize objects they have seen only rarely duringtraining, and collecting a sufficient number of training examples of rareevents is often challenging and expensive, and sometimes outright impossible.We explore in depth an approach to this problem: complementing the fewavailable training images with ad-hoc simulated data.  Our testbed is animal species classification, which has a real-worldlong-tailed distribution. We analyze the effect of different axes of variationin simulation, such as pose, lighting, model, and simulation method, and weprescribe best practices for efficiently incorporating simulated data forreal-world performance gain. Our experiments reveal that synthetic data canconsiderably reduce error rates for classes that are rare, that as the amountof simulated data is increased, accuracy on the target class improves, and thathigh variation of simulated data provides maximum performance gain."
TAFE-Net: Task-Aware Feature Embeddings for Low Shot Learning,"['Xin Wang', 'Fisher Yu', 'Ruth Wang', 'Trevor Darrell', 'Joseph E. Gonzalez']",http://arxiv.org/pdf/1904.05967v1.pdf,2019-04-11,"['cs.cv', 'cs.ai']","  Learning good feature embeddings for images often requires substantialtraining data. As a consequence, in settings where training data is limited(e.g., few-shot and zero-shot learning), we are typically forced to use ageneric feature embedding across various tasks. Ideally, we want to constructfeature embeddings that are tuned for the given task. In this work, we proposeTask-Aware Feature Embedding Networks (TAFE-Nets) to learn how to adapt theimage representation to a new task in a meta learning fashion. Our network iscomposed of a meta learner and a prediction network. Based on a task input, themeta learner generates parameters for the feature layers in the predictionnetwork so that the feature embedding can be accurately adjusted for that task.We show that TAFE-Net is highly effective in generalizing to new tasks orconcepts and evaluate the TAFE-Net on a range of benchmarks in zero-shot andfew-shot learning. Our model matches or exceeds the state-of-the-art on alltasks. In particular, our approach improves the prediction accuracy of unseenattribute-object pairs by 4 to 15 points on the challenging visualattribute-object composition task."
Learning Meta Model for Zero- and Few-shot Face Anti-spoofing,"['Yunxiao Qin', 'Chenxu Zhao', 'Xiangyu Zhu', 'Zezheng Wang', 'Zitong Yu', 'Tianyu Fu', 'Feng Zhou', 'Jingping Shi', 'Zhen Lei']",http://arxiv.org/pdf/1904.12490v3.pdf,2019-04-29,['cs.cv'],"  Face anti-spoofing is crucial to the security of face recognition systems.Most previous methods formulate face anti-spoofing as a supervised learningproblem to detect various predefined presentation attacks, which need largescale training data to cover as many attacks as possible. However, the trainedmodel is easy to overfit several common attacks and is still vulnerable tounseen attacks. To overcome this challenge, the detector should: 1) learndiscriminative features that can generalize to unseen spoofing types frompredefined presentation attacks; 2) quickly adapt to new spoofing types bylearning from both the predefined attacks and a few examples of the newspoofing types. Therefore, we define face anti-spoofing as a zero- and few-shotlearning problem. In this paper, we propose a novel Adaptive Inner-update MetaFace Anti-Spoofing (AIM-FAS) method to tackle this problem throughmeta-learning. Specifically, AIM-FAS trains a meta-learner focusing on the taskof detecting unseen spoofing types by learning from predefined living andspoofing faces and a few examples of new attacks. To assess the proposedapproach, we propose several benchmarks for zero- and few-shot FAS. Experimentsshow its superior performances on the presented benchmarks to existing methodsin existing zero-shot FAS protocols."
On-Device Machine Learning: An Algorithms and Learning Theory  Perspective,"['Sauptik Dhar', 'Junyao Guo', 'Jiayi Liu', 'Samarth Tripathi', 'Unmesh Kurup', 'Mohak Shah']",http://arxiv.org/pdf/1911.00623v2.pdf,2019-11-02,"['cs.lg', 'cs.dc', 'stat.ml']","  The predominant paradigm for using machine learning models on a device is totrain a model in the cloud and perform inference using the trained model on thedevice. However, with increasing number of smart devices and improved hardware,there is interest in performing model training on the device. Given this surgein interest, a comprehensive survey of the field from a device-agnosticperspective sets the stage for both understanding the state-of-the-art and foridentifying open challenges and future avenues of research. However, on-devicelearning is an expansive field with connections to a large number of relatedtopics in AI and machine learning (including online learning, model adaptation,one/few-shot learning, etc.). Hence, covering such a large number of topics ina single survey is impractical. This survey finds a middle ground byreformulating the problem of on-device learning as resource constrainedlearning where the resources are compute and memory. This reformulation allowstools, techniques, and algorithms from a wide variety of research areas to becompared equitably. In addition to summarizing the state-of-the-art, the surveyalso identifies a number of challenges and next steps for both the algorithmicand theoretical aspects of on-device learning."
AMP0: Species-Specific Prediction of Anti-microbial Peptides using Zero  and Few Shot Learning,"['Sadaf Gull', 'Fayyaz Minhas']",http://arxiv.org/pdf/1911.06106v1.pdf,2019-10-28,"['q-bio.bm', 'cs.lg', 'stat.ml']","  The evolution of drug-resistant microbial species is one of the majorchallenges to global health. The development of new antimicrobial treatmentssuch as antimicrobial peptides needs to be accelerated to combat this threat.However, the discovery of novel antimicrobial peptides is hampered bylow-throughput biochemical assays. Computational techniques can be used forrapid screening of promising antimicrobial peptide candidates prior to testingin the wet lab. The vast majority of existing antimicrobial peptide predictorsare non-targeted in nature, i.e., they can predict whether a given peptidesequence is antimicrobial, but they are unable to predict whether the sequencecan target a particular microbial species. In this work, we have developed atargeted antimicrobial peptide activity predictor that can predict whether apeptide is effective against a given microbial species or not. This has beenmade possible through zero-shot and few-shot machine learning. The proposedpredictor called AMP0 takes in the peptide amino acid sequence and anyN/C-termini modifications together with the genomic sequence of a targetmicrobial species to generate targeted predictions. It is important to notethat the proposed method can generate predictions for species that are not partof its training set. The accuracy of predictions for novel test species can befurther improved by providing a few example peptides for that species. Ourcomputational cross-validation results show that the pro-posed scheme isparticularly effective for targeted antimicrobial prediction in comparison toexisting approaches and can be used for screening potential antimicrobialpeptides in a targeted manner especially for cases in which the number oftraining examples is small. The webserver of the method is available athttp://ampzero.pythonanywhere.com."
Program synthesis performance constrained by non-linear spatial  relations in Synthetic Visual Reasoning Test,"['Lu Yihe', 'Scott C. Lowe', 'Penelope A. Lewis', 'Mark C. W. van Rossum']",http://arxiv.org/pdf/1911.07721v2.pdf,2019-11-18,"['cs.lg', 'stat.ml']","  Despite remarkable advances in automated visual recognition by machines, somevisual tasks remain challenging for machines. Fleuret et al. (2011) introducedthe Synthetic Visual Reasoning Test (SVRT) to highlight this point, whichrequired classification of images consisting of randomly generated shapes basedon hidden abstract rules using only a few examples. Ellis et al. (2015)demonstrated that a program synthesis approach could solve some of the SVRTproblems with unsupervised, few-shot learning, whereas they remainedchallenging for several convolutional neural networks trained with thousands ofexamples. Here we re-considered the human and machine experiments, because theyfollowed different protocols and yielded different statistics. We thus proposeda quantitative reintepretation of the data between the protocols, so that wecould make fair comparison between human and machine performance. We improvedthe program synthesis classifier by correcting the image parsings, and comparedthe results to the performance of other machine agents and human subjects. Wegrouped the SVRT problems into different types by the two aspects of the corecharacteristics for classification: shape specification and location relation.We found that the program synthesis classifier could not solve problemsinvolving shape distances, because it relied on symbolic computation whichscales poorly with input dimension and adding distances into such computationwould increase the dimension combinatorially with the number of shapes in animage. Therefore, although the program synthesis classifier is capable ofabstract reasoning, its performance is highly constrained by the accessibleinformation in image parsings."
Differentiable Meta-learning Model for Few-shot Semantic Segmentation,"['Pinzhuo Tian', 'Zhangkai Wu', 'Lei Qi', 'Lei Wang', 'Yinghuan Shi', 'Yang Gao']",http://arxiv.org/pdf/1911.10371v1.pdf,2019-11-23,['cs.cv'],"  To address the annotation scarcity issue in some cases of semanticsegmentation, there have been a few attempts to develop the segmentation modelin the few-shot learning paradigm. However, most existing methods only focus onthe traditional 1-way segmentation setting (i.e., one image only contains asingle object). This is far away from practical semantic segmentation taskswhere the K-way setting (K>1) is usually required by performing the accuratemulti-object segmentation. To deal with this issue, we formulate the few-shotsemantic segmentation task as a learning-based pixel classification problem andpropose a novel framework called MetaSegNet based on meta-learning. InMetaSegNet, an architecture of embedding module consisting of the global andlocal feature branches is developed to extract the appropriate meta-knowledgefor the few-shot segmentation. Moreover, we incorporate a linear model intoMetaSegNet as a base learner to directly predict the label of each pixel forthe multi-object segmentation. Furthermore, our MetaSegNet can be trained bythe episodic training mechanism in an end-to-end manner from scratch.Experiments on two popular semantic segmentation datasets, i.e., PASCAL VOC andCOCO, reveal the effectiveness of the proposed MetaSegNet in the K-way few-shotsemantic segmentation task."
Invenio: Discovering Hidden Relationships Between Tasks/Domains Using  Structured Meta Learning,"['Sameeksha Katoch', 'Kowshik Thopalli', 'Jayaraman J. Thiagarajan', 'Pavan Turaga', 'Andreas Spanias']",http://arxiv.org/pdf/1911.10600v2.pdf,2019-11-24,['cs.cv'],"  Exploiting known semantic relationships between fine-grained tasks iscritical to the success of recent model agnostic approaches. These approachesoften rely on meta-optimization to make a model robust to systematic task ordomain shifts. However, in practice, the performance of these methods cansuffer, when there are no coherent semantic relationships between the tasks (ordomains). We present Invenio, a structured meta-learning algorithm to infersemantic similarities between a given set of tasks and to provide insights intothe complexity of transferring knowledge between different tasks. In contrastto existing techniques such as Task2Vec and Taskonomy, which measuresimilarities between pre-trained models, our approach employs a novelself-supervised learning strategy to discover these relationships in thetraining loop and at the same time utilizes them to update task-specific modelsin the meta-update step. Using challenging task and domain databases, underfew-shot learning settings, we show that Invenio can discover intricatedependencies between tasks or domains, and can provide significant gains overexisting approaches in terms of generalization performance. The learnedsemantic structure between tasks/domains from Invenio is interpretable and canbe used to construct meaningful priors for tasks or domains."
FDFtNet: Facing Off Fake Images using Fake Detection Fine-tuning Network,"['Hyeonseong Jeon', 'Youngoh Bang', 'Simon S. Woo']",http://arxiv.org/pdf/2001.01265v2.pdf,2020-01-05,['cs.cv'],"  Creating fake images and videos such as ""Deepfake"" has become much easierthese days due to the advancement in Generative Adversarial Networks (GANs).Moreover, recent research such as the few-shot learning can create highlyrealistic personalized fake images with only a few images. Therefore, thethreat of Deepfake to be used for a variety of malicious intents such aspropagating fake images and videos becomes prevalent. And detecting thesemachine-generated fake images has been quite challenging than ever. In thiswork, we propose a light-weight robust fine-tuning neural network-basedclassifier architecture called Fake Detection Fine-tuning Network (FDFtNet),which is capable of detecting many of the new fake face image generationmodels, and can be easily combined with existing image classification networksand finetuned on a few datasets. In contrast to many existing methods, ourapproach aims to reuse popular pre-trained models with only a few images forfine-tuning to effectively detect fake images. The core of our approach is tointroduce an image-based self-attention module called Fine-Tune Transformerthat uses only the attention module and the down-sampling layer. This module isadded to the pre-trained model and fine-tuned on a few data to search for newsets of feature space to detect fake images. We experiment with our FDFtNet onthe GANsbased dataset (Progressive Growing GAN) and Deepfake-based dataset(Deepfake and Face2Face) with a small input image resolution of 64x64 thatcomplicates detection. Our FDFtNet achieves an overall accuracy of 90.29% indetecting fake images generated from the GANs-based dataset, outperforming thestate-of-the-art."
Few-shot Action Recognition with Permutation-invariant Attention,"['Hongguang Zhang', 'Li Zhang', 'Xiaojuan Qi', 'Hongdong Li', 'Philip H. S. Torr', 'Piotr Koniusz']",http://arxiv.org/pdf/2001.03905v3.pdf,2020-01-12,['cs.cv'],"  Many few-shot learning models focus on recognising images. In contrast, wetackle a challenging task of few-shot action recognition from videos. We buildon a C3D encoder for spatio-temporal video blocks to capture short-range actionpatterns. Such encoded blocks are aggregated by permutation-invariant poolingto make our approach robust to varying action lengths and long-range temporaldependencies whose patterns are unlikely to repeat even in clips of the sameclass. Subsequently, the pooled representations are combined into simplerelation descriptors which encode so-called query and support clips. Finally,relation descriptors are fed to the comparator with the goal of similaritylearning between query and support clips. Importantly, to re-weight blockcontributions during pooling, we exploit spatial and temporal attention modulesand self-supervision. In naturalistic clips (of the same class) there exists atemporal distribution shift--the locations of discriminative temporal actionhotspots vary. Thus, we permute blocks of a clip and align the resultingattention regions with similarly permuted attention regions of non-permutedclip to train the attention mechanism invariant to block (and thus long-termhotspot) permutations. Our method outperforms the state of the art on theHMDB51, UCF101, miniMIT datasets."
Evaluating Weakly Supervised Object Localization Methods Right,"['Junsuk Choe', 'Seong Joon Oh', 'Seungho Lee', 'Sanghyuk Chun', 'Zeynep Akata', 'Hyunjung Shim']",http://arxiv.org/pdf/2001.07437v2.pdf,2020-01-21,"['cs.cv', 'cs.lg']","  Weakly-supervised object localization (WSOL) has gained popularity over thelast years for its promise to train localization models with only image-levellabels. Since the seminal WSOL work of class activation mapping (CAM), thefield has focused on how to expand the attention regions to cover objects morebroadly and localize them better. However, these strategies rely on fulllocalization supervision to validate hyperparameters and for model selection,which is in principle prohibited under the WSOL setup. In this paper, we arguethat WSOL task is ill-posed with only image-level labels, and propose a newevaluation protocol where full supervision is limited to only a small held-outset not overlapping with the test set. We observe that, under our protocol, thefive most recent WSOL methods have not made a major improvement over the CAMbaseline. Moreover, we report that existing WSOL methods have not reached thefew-shot learning baseline, where the full-supervision at validation time isused for model training instead. Based on our findings, we discuss some futuredirections for WSOL."
Weakly Supervised Few-shot Object Segmentation using Co-Attention with  Visual and Semantic Embeddings,"['Mennatullah Siam', 'Naren Doraiswamy', 'Boris N. Oreshkin', 'Hengshuai Yao', 'Martin Jagersand']",http://arxiv.org/pdf/2001.09540v3.pdf,2020-01-26,['cs.cv'],"  Significant progress has been made recently in developing few-shot objectsegmentation methods. Learning is shown to be successful in few-shotsegmentation settings, using pixel-level, scribbles and bounding boxsupervision. This paper takes another approach, i.e., only requiringimage-level label for few-shot object segmentation. We propose a novelmulti-modal interaction module for few-shot object segmentation that utilizes aco-attention mechanism using both visual and word embedding. Our model usingimage-level labels achieves 4.8% improvement over previously proposedimage-level few-shot object segmentation. It also outperforms state-of-the-artmethods that use weak bounding box supervision on PASCAL-5i. Our results showthat few-shot segmentation benefits from utilizing word embeddings, and that weare able to perform few-shot segmentation using stacked joint visual semanticprocessing with weak image-level labels. We further propose a novel setup,Temporal Object Segmentation for Few-shot Learning (TOSFL) for videos. TOSFLcan be used on a variety of public video data such as Youtube-VOS, asdemonstrated in both instance-level and category-level TOSFL experiments."
Revisiting Meta-Learning as Supervised Learning,"['Wei-Lun Chao', 'Han-Jia Ye', 'De-Chuan Zhan', 'Mark Campbell', 'Kilian Q. Weinberger']",http://arxiv.org/pdf/2002.00573v1.pdf,2020-02-03,"['cs.lg', 'cs.cv', 'stat.ml']","  Recent years have witnessed an abundance of new publications and approacheson meta-learning. This community-wide enthusiasm has sparked great insights buthas also created a plethora of seemingly different frameworks, which can behard to compare and evaluate. In this paper, we aim to provide a principled,unifying framework by revisiting and strengthening the connection betweenmeta-learning and traditional supervised learning. By treating pairs oftask-specific data sets and target models as (feature, label) samples, we canreduce many meta-learning algorithms to instances of supervised learning. Thisview not only unifies meta-learning into an intuitive and practical frameworkbut also allows us to transfer insights from supervised learning directly toimprove meta-learning. For example, we obtain a better understanding ofgeneralization properties, and we can readily transfer well-understoodtechniques, such as model ensemble, pre-training, joint training, dataaugmentation, and even nearest neighbor based methods. We provide an intuitiveanalogy of these methods in the context of meta-learning and show that theygive rise to significant improvements in model performance on few-shotlearning."
Few-Shot Learning on Graphs via Super-Classes based on Graph Spectral  Measures,"['Jatin Chauhan', 'Deepak Nathani', 'Manohar Kaul']",http://arxiv.org/pdf/2002.12815v1.pdf,2020-02-27,"['cs.lg', 'stat.ml']","  We propose to study the problem of few shot graph classification in graphneural networks (GNNs) to recognize unseen classes, given limited labeled graphexamples. Despite several interesting GNN variants being proposed recently fornode and graph classification tasks, when faced with scarce labeled examples inthe few shot setting, these GNNs exhibit significant loss in classificationperformance. Here, we present an approach where a probability measure isassigned to each graph based on the spectrum of the graphs normalizedLaplacian. This enables us to accordingly cluster the graph base labelsassociated with each graph into super classes, where the Lp Wassersteindistance serves as our underlying distance metric. Subsequently, a super graphconstructed based on the super classes is then fed to our proposed GNNframework which exploits the latent inter class relationships made explicit bythe super graph to achieve better class label separation among the graphs. Weconduct exhaustive empirical evaluations of our proposed method and show thatit outperforms both the adaptation of state of the art graph classificationmethods to few shot scenario and our naive baseline GNNs. Additionally, we alsoextend and study the behavior of our method to semi supervised and activelearning scenarios."
On the Texture Bias for Few-Shot CNN Segmentation,"['Reza Azad', 'Abdur R Fayjie', 'Claude Kauffman', 'Ismail Ben Ayed', 'Marco Pedersoli', 'Jose Dolz']",http://arxiv.org/pdf/2003.04052v3.pdf,2020-03-09,['cs.cv'],"  Despite the initial belief that Convolutional Neural Networks (CNNs) aredriven by shapes to perform visual recognition tasks, recent evidence suggeststhat texture bias in CNNs provides higher performing models when learning onlarge labeled training datasets. This contrasts with the perceptual bias in thehuman visual cortex, which has a stronger preference towards shape components.Perceptual differences may explain why CNNs achieve human-level performancewhen large labeled datasets are available, but their performance significantlydegrades in lowlabeled data scenarios, such as few-shot semantic segmentation.To remove the texture bias in the context of few-shot learning, we propose anovel architecture that integrates a set of Difference of Gaussians (DoG) toattenuate high-frequency local components in the feature space. This produces aset of modified feature maps, whose high-frequency components are diminished atdifferent standard deviation values of the Gaussian distribution in the spatialdomain. As this results in multiple feature maps for a single image, we employa bi-directional convolutional long-short-term-memory to efficiently merge themulti scale-space representations. We perform extensive experiments on threewell-known few-shot segmentation benchmarks -- Pascal i5, COCO-20i and FSS-1000-- and demonstrate that our method outperforms state-of-the-art approaches intwo datasets under the same conditions. The code is available at:https://github.com/rezazad68/fewshot-segmentation"
Incremental Few-Shot Object Detection,"['Juan-Manuel Perez-Rua', 'Xiatian Zhu', 'Timothy Hospedales', 'Tao Xiang']",http://arxiv.org/pdf/2003.04668v2.pdf,2020-03-10,['cs.cv'],"  Most existing object detection methods rely on the availability of abundantlabelled training samples per class and offline model training in a batch mode.These requirements substantially limit their scalability to open-endedaccommodation of novel classes with limited labelled training data. We presenta study aiming to go beyond these limitations by considering the IncrementalFew-Shot Detection (iFSD) problem setting, where new classes must be registeredincrementally (without revisiting base classes) and with few examples. To thisend we propose OpeN-ended Centre nEt (ONCE), a detector designed forincrementally learning to detect novel class objects with few examples. This isachieved by an elegant adaptation of the CentreNet detector to the few-shotlearning scenario, and meta-learning a class-specific code generator model forregistering novel classes. ONCE fully respects the incremental learningparadigm, with novel class registration requiring only a single forward pass offew-shot training samples, and no access to base classes -- thus making itsuitable for deployment on embedded devices. Extensive experiments conducted onboth the standard object detection and fashion landmark detection tasks showthe feasibility of iFSD for the first time, opening an interesting and veryimportant line of research."
DeepEMD: Differentiable Earth Mover's Distance for Few-Shot Learning,"['Chi Zhang', 'Yujun Cai', 'Guosheng Lin', 'Chunhua Shen']",http://arxiv.org/pdf/2003.06777v5.pdf,2020-03-15,"['cs.cv', 'cs.lg', 'eess.iv']","  In this work, we develop methods for few-shot image classification from a newperspective of optimal matching between image regions. We employ the EarthMover's Distance (EMD) as a metric to compute a structural distance betweendense image representations to determine image relevance. The EMD generates theoptimal matching flows between structural elements that have the minimummatching cost, which is used to calculate the image distance forclassification. To generate the important weights of elements in the EMDformulation, we design a cross-reference mechanism, which can effectivelyalleviate the adverse impact caused by the cluttered background and largeintra-class appearance variations. To implement k-shot classification, wepropose to learn a structured fully connected layer that can directly classifydense image representations with the EMD. Based on the implicit functiontheorem, the EMD can be inserted as a layer into the network for end-to-endtraining. Our extensive experiments validate the effectiveness of our algorithmwhich outperforms state-of-the-art methods by a significant margin on fivewidely used few-shot classification benchmarks, namely, miniImageNet,tieredImageNet, Fewshot-CIFAR100 (FC100), Caltech-UCSD Birds-200-2011 (CUB),and CIFAR-FewShot (CIFAR-FS). We also demonstrate the effectiveness of ourmethod on the image retrieval task in our experiments."
Context-Transformer: Tackling Object Confusion for Few-Shot Detection,"['Ze Yang', 'Yali Wang', 'Xianyu Chen', 'Jianzhuang Liu', 'Yu Qiao']",http://arxiv.org/pdf/2003.07304v1.pdf,2020-03-16,"['cs.cv', 'cs.ai', 'cs.lg']","  Few-shot object detection is a challenging but realistic scenario, where onlya few annotated training images are available for training detectors. A popularapproach to handle this problem is transfer learning, i.e., fine-tuning adetector pretrained on a source-domain benchmark. However, such transferreddetector often fails to recognize new objects in the target domain, due to lowdata diversity of training samples. To tackle this problem, we propose a novelContext-Transformer within a concise deep transfer framework. Specifically,Context-Transformer can effectively leverage source-domain object knowledge asguidance, and automatically exploit contexts from only a few training images inthe target domain. Subsequently, it can adaptively integrate these relationalclues to enhance the discriminative power of detector, in order to reduceobject confusion in few-shot scenarios. Moreover, Context-Transformer isflexibly embedded in the popular SSD-style detectors, which makes it aplug-and-play module for end-to-end few-shot learning. Finally, we evaluateContext-Transformer on the challenging settings of few-shot detection andincremental few-shot detection. The experimental results show that, ourframework outperforms the recent state-of-the-art approaches."
Task-Adaptive Clustering for Semi-Supervised Few-Shot Classification,"['Jun Seo', 'Sung Whan Yoon', 'Jaekyun Moon']",http://arxiv.org/pdf/2003.08221v1.pdf,2020-03-18,"['cs.lg', 'cs.cv', 'stat.ml']","  Few-shot learning aims to handle previously unseen tasks using only a smallamount of new training data. In preparing (or meta-training) a few-shotlearner, however, massive labeled data are necessary. In the real world,unfortunately, labeled data are expensive and/or scarce. In this work, wepropose a few-shot learner that can work well under the semi-supervised settingwhere a large portion of training data is unlabeled. Our method employsexplicit task-conditioning in which unlabeled sample clustering for the currenttask takes place in a new projection space different from the embedding featurespace. The conditioned clustering space is linearly constructed so as toquickly close the gap between the class centroids for the current task and theindependent per-class reference vectors meta-trained across tasks. In a moregeneral setting, our method introduces a concept of controlling the degree oftask-conditioning for meta-learning: the amount of task-conditioning varieswith the number of repetitive updates for the clustering space. Extensivesimulation results based on the miniImageNet and tieredImageNet datasets showstate-of-the-art semi-supervised few-shot classification performance of theproposed method. Simulation results also indicate that the proposedtask-adaptive clustering shows graceful degradation with a growing number ofdistractor samples, i.e., unlabeled sample images coming from outside thecandidate classes."
An Artificial Intelligence-Based System to Assess Nutrient Intake for  Hospitalised Patients,"['Ya Lu', 'Thomai Stathopoulou', 'Maria F. Vasiloglou', 'Stergios Christodoulidis', 'Zeno Stanga', 'Stavroula Mougiakakou']",http://arxiv.org/pdf/2003.08273v1.pdf,2020-03-18,"['cs.cv', 'cs.lg', 'eess.iv']","  Regular monitoring of nutrient intake in hospitalised patients plays acritical role in reducing the risk of disease-related malnutrition. Althoughseveral methods to estimate nutrient intake have been developed, there is stilla clear demand for a more reliable and fully automated technique, as this couldimprove data accuracy and reduce both the burden on participants and healthcosts. In this paper, we propose a novel system based on artificialintelligence (AI) to accurately estimate nutrient intake, by simply processingRGB Depth (RGB-D) image pairs captured before and after meal consumption. Thesystem includes a novel multi-task contextual network for food segmentation, afew-shot learning-based classifier built by limited training samples for foodrecognition, and an algorithm for 3D surface construction. This allowssequential food segmentation, recognition, and estimation of the consumed foodvolume, permitting fully automatic estimation of the nutrient intake for eachmeal. For the development and evaluation of the system, a dedicated newdatabase containing images and nutrient recipes of 322 meals is assembled,coupled to data annotation using innovative strategies. Experimental resultsdemonstrate that the estimated nutrient intake is highly correlated (> 0.91) tothe ground truth and shows very small mean relative errors (< 20%),outperforming existing techniques proposed for nutrient intake assessment."
Learning to Segment the Tail,"['Xinting Hu', 'Yi Jiang', 'Kaihua Tang', 'Jingyuan Chen', 'Chunyan Miao', 'Hanwang Zhang']",http://arxiv.org/pdf/2004.00900v2.pdf,2020-04-02,['cs.cv'],"  Real-world visual recognition requires handling the extreme sample imbalancein large-scale long-tailed data. We propose a ""divide&conquer"" strategy for thechallenging LVIS task: divide the whole data into balanced parts and then applyincremental learning to conquer each one. This derives a novel learningparadigm: class-incremental few-shot learning, which is especially effectivefor the challenge evolving over time: 1) the class imbalance among theold-class knowledge review and 2) the few-shot data in new-class learning. Wecall our approach Learning to Segment the Tail (LST). In particular, we designan instance-level balanced replay scheme, which is a memory-efficientapproximation to balance the instance-level samples from the old-class images.We also propose to use a meta-module for new-class learning, where the moduleparameters are shared across incremental phases, gaining the learning-to-learnknowledge incrementally, from the data-rich head to the data-poor tail. Weempirically show that: at the expense of a little sacrifice of head-classforgetting, we can gain a significant 8.3% AP improvement for the tail classeswith less than 10 instances, achieving an overall 2.0% AP boost for the whole1,230 classes."
A Comprehensive Overview and Survey of Recent Advances in Meta-Learning,['Huimin Peng'],http://arxiv.org/pdf/2004.11149v7.pdf,2020-04-17,"['cs.lg', 'stat.ml']","  This article reviews meta-learning also known as learning-to-learn whichseeks rapid and accurate model adaptation to unseen tasks with applications inhighly automated AI, few-shot learning, natural language processing androbotics. Unlike deep learning, meta-learning can be applied to few-shothigh-dimensional datasets and considers further improving model generalizationto unseen tasks. Deep learning is focused upon in-sample prediction andmeta-learning concerns model adaptation for out-of-sample prediction.Meta-learning can continually perform self-improvement to achieve highlyautonomous AI. Meta-learning may serve as an additional generalization blockcomplementary for original deep learning model. Meta-learning seeks adaptationof machine learning models to unseen tasks which are vastly different fromtrained tasks. Meta-learning with coevolution between agent and environmentprovides solutions for complex tasks unsolvable by training from scratch.Meta-learning methodology covers a wide range of great minds and thoughts. Webriefly introduce meta-learning methodologies in the following categories:black-box meta-learning, metric-based meta-learning, layered meta-learning andBayesian meta-learning framework. Recent applications concentrate upon theintegration of meta-learning with other machine learning framework to providefeasible integrated problem solutions. We briefly present recent meta-learningadvances and discuss potential future research directions."
MICK: A Meta-Learning Framework for Few-shot Relation Classification  with Small Training Data,"['Xiaoqing Geng', 'Xiwen Chen', 'Kenny Q. Zhu', 'Libin Shen', 'Yinggong Zhao']",http://arxiv.org/pdf/2004.14164v2.pdf,2020-04-26,"['cs.cl', 'cs.lg', 'stat.ml']","  Few-shot relation classification seeks to classify incoming query instancesafter meeting only few support instances. This ability is gained by trainingwith large amount of in-domain annotated data. In this paper, we tackle an evenharder problem by further limiting the amount of data available at trainingtime. We propose a few-shot learning framework for relation classification,which is particularly powerful when the training data is very small. In thisframework, models not only strive to classify query instances, but also seekunderlying knowledge about the support instances to obtain better instancerepresentations. The framework also includes a method for aggregatingcross-domain knowledge into models by open-source task enrichment.Additionally, we construct a brand new dataset: the TinyRel-CM dataset, afew-shot relation classification dataset in health domain with purposely smalltraining data and challenging relation classes. Experimental resultsdemonstrate that our framework brings performance gains for most underlyingclassification models, outperforms the state-of-the-art results given smalltraining data, and achieves competitive results with sufficiently largetraining data."
Physarum Powered Differentiable Linear Programming Layers and  Applications,"['Zihang Meng', 'Sathya N. Ravi', 'Vikas Singh']",http://arxiv.org/pdf/2004.14539v2.pdf,2020-04-30,"['cs.lg', 'cs.cv', 'stat.ml']","  Consider a learning algorithm, which involves an internal call to anoptimization routine such as a generalized eigenvalue problem, a coneprogramming problem or even sorting. Integrating such a method as a layer(s)within a trainable deep neural network (DNN) in an efficient and numericallystable way is not straightforward -- for instance, only recently, strategieshave emerged for eigendecomposition and differentiable sorting. We propose anefficient and differentiable solver for general linear programming problemswhich can be used in a plug and play manner within DNNs as a layer. Ourdevelopment is inspired by a fascinating but not widely used link betweendynamics of slime mold (physarum) and optimization schemes such as steepestdescent. We describe our development and show the use of our solver in a videosegmentation task and meta-learning for few-shot learning. We review theexisting results and provide a technical analysis describing its applicabilityfor our use cases. Our solver performs comparably with a customized projectedgradient descent method on the first task and outperforms the differentiableCVXPY-SCS solver on the second task. Experiments show that our solver convergesquickly without the need for a feasible initial point. Our proposal is easy toimplement and can easily serve as layers whenever a learning procedure needs afast approximate solution to a LP, within a larger network."
Few-Shot Learning for Opinion Summarization,"['Arthur Bražinskas', 'Mirella Lapata', 'Ivan Titov']",http://arxiv.org/pdf/2004.14884v3.pdf,2020-04-30,"['cs.lg', 'cs.cl', 'cs.ne', 'stat.ml']","  Opinion summarization is the automatic creation of text reflecting subjectiveinformation expressed in multiple documents, such as user reviews of a product.The task is practically important and has attracted a lot of attention.However, due to the high cost of summary production, datasets large enough fortraining supervised models are lacking. Instead, the task has beentraditionally approached with extractive methods that learn to select textfragments in an unsupervised or weakly-supervised way. Recently, it has beenshown that abstractive summaries, potentially more fluent and better atreflecting conflicting information, can also be produced in an unsupervisedfashion. However, these models, not being exposed to actual summaries, fail tocapture their essential properties. In this work, we show that even a handfulof summaries is sufficient to bootstrap generation of the summary text with allexpected properties, such as writing style, informativeness, fluency, andsentiment preservation. We start by training a conditional Transformer languagemodel to generate a new product review given other available reviews of theproduct. The model is also conditioned on review properties that are directlyrelated to summaries; the properties are derived from reviews with no manualeffort. In the second stage, we fine-tune a plug-in module that learns topredict property values on a handful of summaries. This lets us switch thegenerator to the summarization mode. We show on Amazon and Yelp datasets thatour approach substantially outperforms previous extractive and abstractivemethods in automatic and human evaluation."
Gradual Relation Network: Decoding Intuitive Upper Extremity Movement  Imaginations Based on Few-Shot EEG Learning,"['Kyung-Hwan Shim', 'Ji-Hoon Jeong', 'Seong-Whan Lee']",http://arxiv.org/pdf/2005.02602v1.pdf,2020-05-06,"['cs.ne', 'eess.sp', 'q-bio.nc']","  Brain-computer interface (BCI) is a communication tool that connects usersand external devices. In a real-time BCI environment, a calibration procedureis particularly necessary for each user and each session. This procedureconsumes a significant amount of time that hinders the application of a BCIsystem in a real-world scenario. To avoid this problem, we adopt the metricbased few-shot learning approach for decoding intuitive upper-extremitymovement imagination (MI) using a gradual relation network (GRN) that cangradually consider the combination of temporal and spectral groups. We acquiredthe MI data of the upper-arm, forearm, and hand associated with intuitiveupper-extremity movement from 25 subjects. The grand average multiclassclassification results under offline analysis were 42.57%, 55.60%, and 80.85%in 1-, 5-, and 25-shot settings, respectively. In addition, we coulddemonstrate the feasibility of intuitive MI decoding using the few-shotapproach in real-time robotic arm control scenarios. Five participants couldachieve a success rate of 78% in the drinking task. Hence, we demonstrated thefeasibility of the online robotic arm control with shortened calibration timeby focusing on human body parts but also the accommodation of various untrainedintuitive MI decoding based on the proposed GRN."
Incremental Few-Shot Object Detection for Robotics,"['Yiting Li', 'Haiyue Zhu', 'Sichao Tian', 'Fan Feng', 'Jun Ma', 'Chek Sing Teo', 'Cheng Xiang', 'Prahlad Vadakkepat', 'Tong Heng Lee']",http://arxiv.org/pdf/2005.02641v2.pdf,2020-05-06,['cs.cv'],"  Incremental few-shot learning is highly expected for practical roboticsapplications. On one hand, robot is desired to learn new tasks quickly andflexibly using only few annotated training samples; on the other hand, such newadditional tasks should be learned in a continuous and incremental mannerwithout forgetting the previous learned knowledge dramatically. In this work,we propose a novel Class-Incremental Few-Shot Object Detection (CI-FSOD)framework that enables deep object detection network to perform effectivecontinual learning from just few-shot samples without re-accessing the previoustraining data. We achieve this by equipping the widely-used Faster-RCNNdetector with three elegant components. Firstly, to best preserve performanceon the pre-trained base classes, we propose a novel Dual-Embedding-Space (DES)architecture which decouples the representation learning of base and novelcategories into different spaces. Secondly, to mitigate the catastrophicforgetting on the accumulated novel classes, we propose a Sequential ModelFusion (SMF) method, which is able to achieve long-term memory withoutadditional storage cost. Thirdly, to promote inter-task class separation infeature space, we propose a novel regularization technique that extends theclassification boundary further away from the previous classes to avoidmisclassification. Overall, our framework is simple yet effective andoutperforms the previous SOTA with a significant margin of 2.4 points in APperformance."
Supervision and Source Domain Impact on Representation Learning: A  Histopathology Case Study,"['Milad Sikaroudi', 'Amir Safarpoor', 'Benyamin Ghojogh', 'Sobhan Shafiei', 'Mark Crowley', 'H. R. Tizhoosh']",http://arxiv.org/pdf/2005.08629v1.pdf,2020-05-10,"['cs.cv', 'cs.lg']","  As many algorithms depend on a suitable representation of data, learningunique features is considered a crucial task. Although supervised techniquesusing deep neural networks have boosted the performance of representationlearning, the need for a large set of labeled data limits the application ofsuch methods. As an example, high-quality delineations of regions of interestin the field of pathology is a tedious and time-consuming task due to the largeimage dimensions. In this work, we explored the performance of a deep neuralnetwork and triplet loss in the area of representation learning. Weinvestigated the notion of similarity and dissimilarity in pathologywhole-slide images and compared different setups from unsupervised andsemi-supervised to supervised learning in our experiments. Additionally,different approaches were tested, applying few-shot learning on two publiclyavailable pathology image datasets. We achieved high accuracy andgeneralization when the learned representations were applied to two differentpathology datasets."
SSM-Net for Plants Disease Identification in Low Data Regime,['Shruti Jadon'],http://arxiv.org/pdf/2005.13140v4.pdf,2020-05-27,"['cs.cv', 'cs.lg', 'eess.iv']","  Plant disease detection is an essential factor in increasing agriculturalproduction. Due to the difficulty of disease detection, farmers spray variouspesticides on their crops to protect them, causing great harm to crop growthand food standards. Deep learning can offer critical aid in detecting suchdiseases. However, it is highly inconvenient to collect a large volume of dataon all forms of the diseases afflicting a specific plant species. In thispaper, we propose a new metrics-based few-shot learning SSM net architecture,which consists of stacked siamese and matching network components to addressthe problem of disease detection in low data regimes. We demonstrated ourexperiments on two datasets: mini-leaves diseases and sugarcane diseasesdataset. We have showcased that the SSM-Net approach can achieve betterdecision boundaries with an accuracy of 92.7% on the mini-leaves dataset and94.3% on the sugarcane dataset. The accuracy increased by ~10% and ~5%respectively, compared to the widely used VGG16 transfer learning approach.Furthermore, we attained F1 score of 0.90 using SSM Net on the sugarcanedataset and 0.91 on the mini-leaves dataset. Our code implementation isavailable on Github: https://github.com/shruti-jadon/PlantsDiseaseDetection."
Brain-inspired global-local learning incorporated with neuromorphic  computing,"['Yujie Wu', 'Rong Zhao', 'Jun Zhu', 'Feng Chen', 'Mingkun Xu', 'Guoqi Li', 'Sen Song', 'Lei Deng', 'Guanrui Wang', 'Hao Zheng', 'Jing Pei', 'Youhui Zhang', 'Mingguo Zhao', 'Luping Shi']",http://arxiv.org/pdf/2006.03226v3.pdf,2020-06-05,"['cs.ne', 'cs.ai', 'q-bio.nc']","  Two main routes of learning methods exist at present including error-drivenglobal learning and neuroscience-oriented local learning. Integrating them intoone network may provide complementary learning capabilities for versatilelearning scenarios. At the same time, neuromorphic computing holds greatpromise, but still needs plenty of useful algorithms and algorithm-hardwareco-designs for exploiting the advantages. Here, we report a neuromorphic hybridlearning model by introducing a brain-inspired meta-learning paradigm and adifferentiable spiking model incorporating neuronal dynamics and synapticplasticity. It can meta-learn local plasticity and receive top-down supervisioninformation for multiscale synergic learning. We demonstrate the advantages ofthis model in multiple different tasks, including few-shot learning, continuallearning, and fault-tolerance learning in neuromorphic vision sensors. Itachieves significantly higher performance than single-learning methods, andshows promise in empowering neuromorphic applications revolution. We furtherimplemented the hybrid model in the Tianjic neuromorphic platform by exploitingalgorithm-hardware co-designs and proved that the model can fully utilizeneuromorphic many-core architecture to develop hybrid computation paradigm."
Few-shot Object Detection on Remote Sensing Images,"['Jingyu Deng', 'Xiang Li', 'Yi Fang']",http://arxiv.org/pdf/2006.07826v2.pdf,2020-06-14,['cs.cv'],"  In this paper, we deal with the problem of object detection on remote sensingimages. Previous methods have developed numerous deep CNN-based methods forobject detection on remote sensing images and the report remarkableachievements in detection performance and efficiency. However, currentCNN-based methods mostly require a large number of annotated samples to traindeep neural networks and tend to have limited generalization abilities forunseen object categories. In this paper, we introduce a few-shot learning-basedmethod for object detection on remote sensing images where only a few annotatedsamples are provided for the unseen object categories. More specifically, ourmodel contains three main components: a meta feature extractor that learns toextract feature representations from input images, a reweighting module thatlearn to adaptively assign different weights for each feature representationfrom the support images, and a bounding box prediction module that carries outobject detection on the reweighted feature maps. We build our few-shot objectdetection model upon YOLOv3 architecture and develop a multi-scale objectdetection framework. Experiments on two benchmark datasets demonstrate thatwith only a few annotated samples our model can still achieve a satisfyingdetection performance on remote sensing images and the performance of our modelis significantly better than the well-established baseline models."
Graph Meta Learning via Local Subgraphs,"['Kexin Huang', 'Marinka Zitnik']",http://arxiv.org/pdf/2006.07889v4.pdf,2020-06-14,"['cs.lg', 'stat.ml']","  Prevailing methods for graphs require abundant label and edge information forlearning. When data for a new task are scarce, meta-learning can learn fromprior experiences and form much-needed inductive biases for fast adaption tonew tasks. Here, we introduce G-Meta, a novel meta-learning algorithm forgraphs. G-Meta uses local subgraphs to transfer subgraph-specific informationand learn transferable knowledge faster via meta gradients. G-Meta learns howto quickly adapt to a new task using only a handful of nodes or edges in thenew task and does so by learning from data points in other graphs or related,albeit disjoint label sets. G-Meta is theoretically justified as we show thatthe evidence for a prediction can be found in the local subgraph surroundingthe target node or edge. Experiments on seven datasets and nine baselinemethods show that G-Meta outperforms existing methods by up to 16.3%. Unlikeprevious methods, G-Meta successfully learns in challenging, few-shot learningsettings that require generalization to completely new graphs andnever-before-seen labels. Finally, G-Meta scales to large graphs, which wedemonstrate on a new Tree-of-Life dataset comprising of 1,840 graphs, atwo-orders of magnitude increase in the number of graphs used in prior work."
Generalized Zero and Few-Shot Transfer for Facial Forgery Detection,"['Shivangi Aneja', 'Matthias Nießner']",http://arxiv.org/pdf/2006.11863v1.pdf,2020-06-21,"['cs.cv', 'cs.lg', 'eess.iv']","  We propose Deep Distribution Transfer(DDT), a new transfer learning approachto address the problem of zero and few-shot transfer in the context of facialforgery detection. We examine how well a model (pre-)trained with one forgerycreation method generalizes towards a previously unseen manipulation techniqueor different dataset. To facilitate this transfer, we introduce a new mixturemodel-based loss formulation that learns a multi-modal distribution, with modescorresponding to class categories of the underlying data of the source forgerymethod. Our core idea is to first pre-train an encoder neural network, whichmaps each mode of this distribution to the respective class labels, i.e., realor fake images in the source domain by minimizing wasserstein distance betweenthem. In order to transfer this model to a new domain, we associate a fewtarget samples with one of the previously trained modes. In addition, wepropose a spatial mixup augmentation strategy that further helps generalizationacross domains. We find this learning strategy to be surprisingly effective atdomain transfer compared to a traditional classification or evenstate-of-the-art domain adaptation/few-shot learning methods. For instance,compared to the best baseline, our method improves the classification accuracyby 4.88% for zero-shot and by 8.38% for the few-shot case transferred from theFaceForensics++ to Dessa dataset."
Revisiting Mid-Level Patterns for Cross-Domain Few-Shot Recognition,"['Yixiong Zou', 'Shanghang Zhang', 'JianPeng Yu', 'Yonghong Tian', 'José M. F. Moura']",http://arxiv.org/pdf/2008.03128v4.pdf,2020-08-07,['cs.cv'],"  Existing few-shot learning (FSL) methods usually assume base classes andnovel classes are from the same domain (in-domain setting). However, inpractice, it may be infeasible to collect sufficient training samples for somespecial domains to construct base classes. To solve this problem, cross-domainFSL (CDFSL) is proposed very recently to transfer knowledge from general-domainbase classes to special-domain novel classes. Existing CDFSL works mostly focuson transferring between near domains, while rarely consider transferringbetween distant domains, which is in practical need as any novel classes couldappear in real-world applications, and is even more challenging. In this paper,we study a challenging subset of CDFSL where the novel classes are in distantdomains from base classes, by revisiting the mid-level features, which are moretransferable yet under-explored in main stream FSL work. To boost thediscriminability of mid-level features, we propose a residual-prediction taskto encourage mid-level features to learn discriminative information of eachsample. Notably, such mechanism also benefits the in-domain FSL and CDFSL innear domains. Therefore, we provide two types of features for both cross- andin-domain FSL respectively, under the same training framework. Experimentsunder both settings on six public datasets, including two challenging medicaldatasets, validate the our rationale and demonstrate state-of-the-artperformance. Code will be released."
Learning to Reason in Round-based Games: Multi-task Sequence Generation  for Purchasing Decision Making in First-person Shooters,"['Yilei Zeng', 'Deren Lei', 'Beichen Li', 'Gangrong Jiang', 'Emilio Ferrara', 'Michael Zyda']",http://arxiv.org/pdf/2008.05131v1.pdf,2020-08-12,"['cs.ai', 'cs.lg']","  Sequential reasoning is a complex human ability, with extensive previousresearch focusing on gaming AI in a single continuous game, round-baseddecision makings extending to a sequence of games remain less explored.Counter-Strike: Global Offensive (CS:GO), as a round-based game with abundantexpert demonstrations, provides an excellent environment for multi-playerround-based sequential reasoning. In this work, we propose a Sequence Reasonerwith Round Attribute Encoder and Multi-Task Decoder to interpret the strategiesbehind the round-based purchasing decisions. We adopt few-shot learning tosample multiple rounds in a match, and modified model agnostic meta-learningalgorithm Reptile for the meta-learning loop. We formulate each round as amulti-task sequence generation problem. Our state representations combineaction encoder, team encoder, player features, round attribute encoder, andeconomy encoders to help our agent learn to reason under this specificmulti-player round-based scenario. A complete ablation study and comparisonwith the greedy approach certify the effectiveness of our model. Our researchwill open doors for interpretable AI for understanding episodic and long-termpurchasing strategies beyond the gaming community."
Domain Generalizer: A Few-shot Meta Learning Framework for Domain  Generalization in Medical Imaging,"['Pulkit Khandelwal', 'Paul Yushkevich']",http://arxiv.org/pdf/2008.07724v1.pdf,2020-08-18,"['cs.cv', 'cs.lg', 'eess.iv']","  Deep learning models perform best when tested on target (test) data domainswhose distribution is similar to the set of source (train) domains. However,model generalization can be hindered when there is significant difference inthe underlying statistics between the target and source domains. In this work,we adapt a domain generalization method based on a model-agnostic meta-learningframework to biomedical imaging. The method learns a domain-agnostic featurerepresentation to improve generalization of models to the unseen testdistribution. The method can be used for any imaging task, as it does notdepend on the underlying model architecture. We validate the approach through acomputed tomography (CT) vertebrae segmentation task across healthy andpathological cases on three datasets. Next, we employ few-shot learning, i.e.training the generalized model using very few examples from the unseen domain,to quickly adapt the model to new unseen data distribution. Our results suggestthat the method could help generalize models across different medical centers,image acquisition protocols, anatomies, different regions in a given scan,healthy and diseased populations across varied imaging modalities."
Few-Shot Learning with Intra-Class Knowledge Transfer,"['Vivek Roy', 'Yan Xu', 'Yu-Xiong Wang', 'Kris Kitani', 'Ruslan Salakhutdinov', 'Martial Hebert']",http://arxiv.org/pdf/2008.09892v1.pdf,2020-08-22,['cs.cv'],"  We consider the few-shot classification task with an unbalanced dataset, inwhich some classes have sufficient training samples while other classes onlyhave limited training samples. Recent works have proposed to solve this task byaugmenting the training data of the few-shot classes using generative modelswith the few-shot training samples as the seeds. However, due to the limitednumber of the few-shot seeds, the generated samples usually have smalldiversity, making it difficult to train a discriminative classifier for thefew-shot classes. To enrich the diversity of the generated samples, we proposeto leverage the intra-class knowledge from the neighbor many-shot classes withthe intuition that neighbor classes share similar statistical information. Suchintra-class information is obtained with a two-step mechanism. First, aregressor trained only on the many-shot classes is used to evaluate thefew-shot class means from only a few samples. Second, superclasses areclustered, and the statistical mean and feature variance of each superclass areused as transferable knowledge inherited by the children few-shot classes. Suchknowledge is then used by a generator to augment the sparse training data tohelp the downstream classification tasks. Extensive experiments show that ourmethod achieves state-of-the-art across different datasets and $n$-shotsettings."
Learning to Profile: User Meta-Profile Network for Few-Shot Learning,"['Hao Gong', 'Qifang Zhao', 'Tianyu Li', 'Derek Cho', 'DuyKhuong Nguyen']",http://arxiv.org/pdf/2008.12258v2.pdf,2020-08-21,['cs.lg'],"  Meta-learning approaches have shown great success in vision and languagedomains. However, few studies discuss the practice of meta-learning forlarge-scale industrial applications. Although e-commerce companies have spentmany efforts on learning representations to provide a better user experience,we argue that such efforts cannot be stopped at this step. In addition tolearning a strong profile, the challenging question about how to effectivelytransfer the learned representation is raised simultaneously. This paperintroduces the contributions that we made to address these challenges fromthree aspects. 1) Meta-learning model: In the context of representationlearning with e-commerce user behavior data, we propose a meta-learningframework called the Meta-Profile Network, which extends the ideas of matchingnetwork and relation network for knowledge transfer and fast adaptation; 2)Encoding strategy: To keep high fidelity of large-scale long-term sequentialbehavior data, we propose a time-heatmap encoding strategy that allows themodel to encode data effectively; 3) Deep network architecture: A multi-modalmodel combined with multi-task learning architecture is utilized to address thecross-domain knowledge learning and insufficient label problems. Moreover, weargue that an industrial model should not only have good performance in termsof accuracy, but also have better robustness and uncertainty performance underextreme conditions. We evaluate the performance of our model with extensivecontrol experiments in various extreme scenarios, i.e. out-of-distributiondetection, data insufficiency and class imbalance scenarios. The Meta-ProfileNetwork shows significant improvement in the model performance when compared tobaseline models."
Region Comparison Network for Interpretable Few-shot Image  Classification,"['Zhiyu Xue', 'Lixin Duan', 'Wen Li', 'Lin Chen', 'Jiebo Luo']",http://arxiv.org/pdf/2009.03558v1.pdf,2020-09-08,"['cs.cv', 'cs.ai']","  While deep learning has been successfully applied to many real-world computervision tasks, training robust classifiers usually requires a large amount ofwell-labeled data. However, the annotation is often expensive andtime-consuming. Few-shot image classification has thus been proposed toeffectively use only a limited number of labeled examples to train models fornew classes. Recent works based on transferable metric learning methods haveachieved promising classification performance through learning the similaritybetween the features of samples from the query and support sets. However, rareof them explicitly considers the model interpretability, which can actually berevealed during the training phase.  For that, in this work, we propose a metric learning based method namedRegion Comparison Network (RCN), which is able to reveal how few-shot learningworks as in a neural network as well as to find out specific regions that arerelated to each other in images coming from the query and support sets.Moreover, we also present a visualization strategy named Region ActivationMapping (RAM) to intuitively explain what our method has learned by visualizingintermediate variables in our network. We also present a new way to generalizethe interpretability from the level of tasks to categories, which can also beviewed as a method to find the prototypical parts for supporting the finaldecision of our RCN. Extensive experiments on four benchmark datasets clearlyshow the effectiveness of our method over existing baselines."
Few-Shot Unsupervised Continual Learning through Meta-Examples,"['Alessia Bertugli', 'Stefano Vincenzi', 'Simone Calderara', 'Andrea Passerini']",http://arxiv.org/pdf/2009.08107v3.pdf,2020-09-17,"['cs.lg', 'cs.cv', 'stat.ml']","  In real-world applications, data do not reflect the ones commonly used forneural networks training, since they are usually few, unlabeled and can beavailable as a stream. Hence many existing deep learning solutions suffer froma limited range of applications, in particular in the case of online streamingdata that evolve over time. To narrow this gap, in this work we introduce anovel and complex setting involving unsupervised meta-continual learning withunbalanced tasks. These tasks are built through a clustering procedure appliedto a fitted embedding space. We exploit a meta-learning scheme thatsimultaneously alleviates catastrophic forgetting and favors the generalizationto new tasks. Moreover, to encourage feature reuse during themeta-optimization, we exploit a single inner loop taking advantage of anaggregated representation achieved through the use of a self-attentionmechanism. Experimental results on few-shot learning benchmarks showcompetitive performance even compared to the supervised case. Additionally, weempirically observe that in an unsupervised scenario, the small tasks and thevariability in the clusters pooling play a crucial role in the generalizationcapability of the network. Further, on complex datasets, the exploitation ofmore clusters than the true number of classes leads to higher results, evencompared to the ones obtained with full supervision, suggesting that apredefined partitioning into classes can miss relevant structural information."
PennSyn2Real: Training Object Recognition Models without Human Labeling,"['Ty Nguyen', 'Ian D. Miller', 'Avi Cohen', 'Dinesh Thakur', 'Shashank Prasad', 'Camillo J. Taylor', 'Pratik Chaudrahi', 'Vijay Kumar']",http://arxiv.org/pdf/2009.10292v2.pdf,2020-09-22,['cs.cv'],"  Scalable training data generation is a critical problem in deep learning. Wepropose PennSyn2Real - a photo-realistic synthetic dataset consisting of morethan 100,000 4K images of more than 20 types of micro aerial vehicles (MAVs).The dataset can be used to generate arbitrary numbers of training images forhigh-level computer vision tasks such as MAV detection and classification. Ourdata generation framework bootstraps chroma-keying, a mature cinematographytechnique with a motion tracking system, providing artifact-free and curatedannotated images where object orientations and lighting are controlled. Thisframework is easy to set up and can be applied to a broad range of objects,reducing the gap between synthetic and real-world data. We show that syntheticdata generated using this framework can be directly used to train CNN modelsfor common object recognition tasks such as detection and segmentation. Wedemonstrate competitive performance in comparison with training using only realimages. Furthermore, bootstrapping the generated synthetic data in few-shotlearning can significantly improve the overall performance, reducing the numberof required training data samples to achieve the desired accuracy."
Sense and Learn: Self-Supervision for Omnipresent Sensors,"['Aaqib Saeed', 'Victor Ungureanu', 'Beat Gfeller']",http://arxiv.org/pdf/2009.13233v2.pdf,2020-09-28,"['cs.lg', 'stat.ml']","  Learning general-purpose representations from multisensor data produced bythe omnipresent sensing systems (or IoT in general) has numerous applicationsin diverse use cases. Existing purely supervised end-to-end deep learningtechniques depend on the availability of a massive amount of well-curated data,acquiring which is notoriously difficult but required to achieve a sufficientlevel of generalization on a task of interest. In this work, we leverage theself-supervised learning paradigm towards realizing the vision of continuallearning from unlabeled inputs. We present a generalized framework named Senseand Learn for representation or feature learning from raw sensory data. Itconsists of several auxiliary tasks that can learn high-level and broadlyuseful features entirely from unannotated data without any human involvement inthe tedious labeling process. We demonstrate the efficacy of our approach onseveral publicly available datasets from different domains and in varioussettings, including linear separability, semi-supervised or few shot learning,and transfer learning. Our methodology achieves results that are competitivewith the supervised approaches and close the gap through fine-tuning a networkwhile learning the downstream tasks in most cases. In particular, we show thatthe self-supervised network can be utilized as initialization to significantlyboost the performance in a low-data regime with as few as 5 labeled instancesper class, which is of high practical importance to real-world problems.Likewise, the learned representations with self-supervision are found to behighly transferable between related datasets, even when few labeled instancesare available from the target domains. The self-learning nature of ourmethodology opens up exciting possibilities for on-device continual learning."
Learning to Generate Image Source-Agnostic Universal Adversarial  Perturbations,"['Pu Zhao', 'Parikshit Ram', 'Songtao Lu', 'Yuguang Yao', 'Djallel Bouneffouf', 'Xue Lin', 'Sijia Liu']",http://arxiv.org/pdf/2009.13714v4.pdf,2020-09-29,"['cs.lg', 'cs.ai', 'cs.cv', 'stat.ml']","  Adversarial perturbations are critical for certifying the robustness of deeplearning models. A universal adversarial perturbation (UAP) can simultaneouslyattack multiple images, and thus offers a more unified threat model, obviatingan image-wise attack algorithm. However, the existing UAP generator isunderdeveloped when images are drawn from different image sources (e.g., withdifferent image resolutions). Towards an authentic universality across imagesources, we take a novel view of UAP generation as a customized instance offew-shot learning, which leverages bilevel optimization andlearning-to-optimize (L2O) techniques for UAP generation with improved attacksuccess rate (ASR). We begin by considering the popular model agnosticmeta-learning (MAML) framework to meta-learn a UAP generator. However, we seethat the MAML framework does not directly offer the universal attack acrossimage sources, requiring us to integrate it with another meta-learningframework of L2O. The resulting scheme for meta-learning a UAP generator (i)has better performance (50% higher ASR) than baselines such as ProjectedGradient Descent, (ii) has better performance (37% faster) than the vanilla L2Oand MAML frameworks (when applicable), and (iii) is able to simultaneouslyhandle UAP generation for different victim models and image data sources."
Few-shot Image Recognition with Manifolds,"['Debasmit Das', 'J. H. Moon', 'C. S. George Lee']",http://arxiv.org/pdf/2010.12084v1.pdf,2020-10-22,['cs.cv'],"  In this paper, we extend the traditional few-shot learning (FSL) problem tothe situation when the source-domain data is not accessible but only high-levelinformation in the form of class prototypes is available. This limitedinformation setup for the FSL problem deserves much attention due to itsimplication of privacy-preserving inaccessibility to the source-domain data butit has rarely been addressed before. Because of limited training data, wepropose a non-parametric approach to this FSL problem by assuming that all theclass prototypes are structurally arranged on a manifold. Accordingly, weestimate the novel-class prototype locations by projecting the few-shot samplesonto the average of the subspaces on which the surrounding classes lie. Duringclassification, we again exploit the structural arrangement of the categoriesby inducing a Markov chain on the graph constructed with the class prototypes.This manifold distance obtained using the Markov chain is expected to producebetter results compared to a traditional nearest-neighbor-based Euclideandistance. To evaluate our proposed framework, we have tested it on two imagedatasets - the large-scale ImageNet and the small-scale but fine-grainedCUB-200. We have also studied parameter sensitivity to better understand ourframework."
Discriminative Nearest Neighbor Few-Shot Intent Detection by  Transferring Natural Language Inference,"['Jian-Guo Zhang', 'Kazuma Hashimoto', 'Wenhao Liu', 'Chien-Sheng Wu', 'Yao Wan', 'Philip S. Yu', 'Richard Socher', 'Caiming Xiong']",http://arxiv.org/pdf/2010.13009v1.pdf,2020-10-25,"['cs.cl', 'cs.ai']","  Intent detection is one of the core components of goal-oriented dialogsystems, and detecting out-of-scope (OOS) intents is also a practicallyimportant skill. Few-shot learning is attracting much attention to mitigatedata scarcity, but OOS detection becomes even more challenging. In this paper,we present a simple yet effective approach, discriminative nearest neighborclassification with deep self-attention. Unlike softmax classifiers, weleverage BERT-style pairwise encoding to train a binary classifier thatestimates the best matched training example for a user input. We propose toboost the discriminative ability by transferring a natural language inference(NLI) model. Our extensive experiments on a large-scale multi-domain intentdetection task show that our method achieves more stable and accurate in-domainand OOS detection accuracy than RoBERTa-based classifiers and embedding-basednearest neighbor approaches. More notably, the NLI transfer enables our 10-shotmodel to perform competitively with 50-shot or even full-shot classifiers,while we can keep the inference time constant by leveraging a faster embeddingretrieval model."
Fine-grained Angular Contrastive Learning with Coarse Labels,"['Guy Bukchin', 'Eli Schwartz', 'Kate Saenko', 'Ori Shahar', 'Rogerio Feris', 'Raja Giryes', 'Leonid Karlinsky']",http://arxiv.org/pdf/2012.03515v3.pdf,2020-12-07,['cs.cv'],"  Few-shot learning methods offer pre-training techniques optimized for easierlater adaptation of the model to new classes (unseen during training) using oneor a few examples. This adaptivity to unseen classes is especially importantfor many practical applications where the pre-trained label space cannot remainfixed for effective use and the model needs to be ""specialized"" to support newcategories on the fly. One particularly interesting scenario, essentiallyoverlooked by the few-shot literature, is Coarse-to-Fine Few-Shot (C2FS), wherethe training classes (e.g. animals) are of much `coarser granularity' than thetarget (test) classes (e.g. breeds). A very practical example of C2FS is whenthe target classes are sub-classes of the training classes. Intuitively, it isespecially challenging as (both regular and few-shot) supervised pre-trainingtends to learn to ignore intra-class variability which is essential forseparating sub-classes. In this paper, we introduce a novel 'Angularnormalization' module that allows to effectively combine supervised andself-supervised contrastive pre-training to approach the proposed C2FS task,demonstrating significant gains in a broad study over multiple baselines anddatasets. We hope that this work will help to pave the way for future researchon this new, challenging, and very practical topic of C2FS classification."
Direct multimodal few-shot learning of speech and images,"['Leanne Nortje', 'Herman Kamper']",http://arxiv.org/pdf/2012.05680v2.pdf,2020-12-10,"['cs.cl', 'cs.sd', 'eess.as']","  We propose direct multimodal few-shot models that learn a shared embeddingspace of spoken words and images from only a few paired examples. Imagine anagent is shown an image along with a spoken word describing the object in thepicture, e.g. pen, book and eraser. After observing a few paired examples ofeach class, the model is asked to identify the ""book"" in a set of unseenpictures. Previous work used a two-step indirect approach relying on learnedunimodal representations: speech-speech and image-image comparisons areperformed across the support set of given speech-image pairs. We propose twodirect models which instead learn a single multimodal space where inputs fromdifferent modalities are directly comparable: a multimodal triplet network(MTriplet) and a multimodal correspondence autoencoder (MCAE). To train thesedirect models, we mine speech-image pairs: the support set is used to pair upunlabelled in-domain speech and images. In a speech-to-image digit matchingtask, direct models outperform indirect models, with the MTriplet achieving thebest multimodal five-shot accuracy. We show that the improvements are due tothe combination of unsupervised and transfer learning in the direct models, andthe absence of two-step compounding errors."
"Power Normalizations in Fine-grained Image, Few-shot Image and Graph  Classification","['Piotr Koniusz', 'Hongguang Zhang']",http://arxiv.org/pdf/2012.13975v2.pdf,2020-12-27,['cs.cv'],"  Power Normalizations (PN) are useful non-linear operators which tacklefeature imbalances in classification problems. We study PNs in the deeplearning setup via a novel PN layer pooling feature maps. Our layer combinesthe feature vectors and their respective spatial locations in the feature mapsproduced by the last convolutional layer of CNN into a positive definite matrixwith second-order statistics to which PN operators are applied, formingso-called Second-order Pooling (SOP). As the main goal of this paper is tostudy Power Normalizations, we investigate the role and meaning of MaxExp andGamma, two popular PN functions. To this end, we provide probabilisticinterpretations of such element-wise operators and discover surrogates withwell-behaved derivatives for end-to-end training. Furthermore, we look at thespectral applicability of MaxExp and Gamma by studying Spectral PowerNormalizations (SPN). We show that SPN on the autocorrelation/covariance matrixand the Heat Diffusion Process (HDP) on a graph Laplacian matrix are closelyrelated, thus sharing their properties. Such a finding leads us to theculmination of our work, a fast spectral MaxExp which is a variant of HDP forcovariances/autocorrelation matrices. We evaluate our ideas on fine-grainedrecognition, scene recognition, and material classification, as well as infew-shot learning and graph classification."
Few-shot Learning for CT Scan based COVID-19 Diagnosis,"['Yifan Jiang', 'Han Chen', 'David K. Han', 'Hanseok Ko']",http://arxiv.org/pdf/2102.00596v1.pdf,2021-02-01,"['eess.iv', 'cs.cv', 'cs.lg']","  Coronavirus disease 2019 (COVID-19) is a Public Health Emergency ofInternational Concern infecting more than 40 million people across 188countries and territories. Chest computed tomography (CT) imaging techniquebenefits from its high diagnostic accuracy and robustness, it has become anindispensable way for COVID-19 mass testing. Recently, deep learning approacheshave become an effective tool for automatic screening of medical images, and itis also being considered for COVID-19 diagnosis. However, the high infectionrisk involved with COVID-19 leads to relative sparseness of collected labeleddata limiting the performance of such methodologies. Moreover, accuratelylabeling CT images require expertise of radiologists making the processexpensive and time-consuming. In order to tackle the above issues, we propose asupervised domain adaption based COVID-19 CT diagnostic method which canperform effectively when only a small samples of labeled CT scans areavailable. To compensate for the sparseness of labeled data, the proposedmethod utilizes a large amount of synthetic COVID-19 CT images and adjusts thenetworks from the source domain (synthetic data) to the target domain (realdata) with a cross-domain training mechanism. Experimental results show thatthe proposed method achieves state-of-the-art performance on few-shot COVID-19CT imaging based diagnostic tasks."
Multi-Level Fine-Tuning: Closing Generalization Gaps in Approximation of  Solution Maps under a Limited Budget for Training Data,"['Zhihan Li', 'Yuwei Fan', 'Lexing Ying']",http://arxiv.org/pdf/2102.07169v1.pdf,2021-02-14,"['math.na', 'cs.na']","  In scientific machine learning, regression networks have been recentlyapplied to approximate solution maps (e.g., potential-ground state map ofSchr\""odinger equation). In this paper, we aim to reduce the generalizationerror without spending more time in generating training samples. However, toreduce the generalization error, the regression network needs to be fit on alarge number of training samples (e.g., a collection of potential-ground statepairs). The training samples can be produced by running numerical solvers,which takes much time in many applications. In this paper, we aim to reduce thegeneralization error without spending more time in generating training samples.Inspired by few-shot learning techniques, we develop the Multi-LevelFine-Tuning algorithm by introducing levels of training: we first train theregression network on samples generated at the coarsest grid and thensuccessively fine-tune the network on samples generated at finer grids. Withinthe same amount of time, numerical solvers generate more samples on coarsegrids than on fine grids. We demonstrate a significant reduction ofgeneralization error in numerical experiments on challenging problems withoscillations, discontinuities, or rough coefficients. Further analysis can beconducted in the Neural Tangent Kernel regime and we provide practicalestimators to the generalization error. The number of training samples atdifferent levels can be optimized for the smallest estimated generalizationerror under the constraint of budget for training data. The optimizeddistribution of budget over levels provides practical guidance with theoreticalinsight as in the celebrated Multi-Level Monte Carlo algorithm."
Large-Scale Meta-Learning with Continual Trajectory Shifting,"['Jaewoong Shin', 'Hae Beom Lee', 'Boqing Gong', 'Sung Ju Hwang']",http://arxiv.org/pdf/2102.07215v3.pdf,2021-02-14,['cs.lg'],"  Meta-learning of shared initialization parameters has shown to be highlyeffective in solving few-shot learning tasks. However, extending the frameworkto many-shot scenarios, which may further enhance its practicality, has beenrelatively overlooked due to the technical difficulties of meta-learning overlong chains of inner-gradient steps. In this paper, we first show that allowingthe meta-learners to take a larger number of inner gradient steps bettercaptures the structure of heterogeneous and large-scale task distributions,thus results in obtaining better initialization points. Further, in order toincrease the frequency of meta-updates even with the excessively longinner-optimization trajectories, we propose to estimate the required shift ofthe task-specific parameters with respect to the change of the initializationparameters. By doing so, we can arbitrarily increase the frequency ofmeta-updates and thus greatly improve the meta-level convergence as well as thequality of the learned initializations. We validate our method on aheterogeneous set of large-scale tasks and show that the algorithm largelyoutperforms the previous first-order meta-learning methods in terms of bothgeneralization performance and convergence, as well as multi-task learning andfine-tuning baselines."
Grid Cell Path Integration For Movement-Based Visual Object Recognition,"['Niels Leadholm', 'Marcus Lewis', 'Subutai Ahmad']",http://arxiv.org/pdf/2102.09076v1.pdf,2021-02-17,['cs.ai'],"  Grid cells enable the brain to model the physical space of the world andnavigate effectively via path integration, updating self-position usinginformation from self-movement. Recent proposals suggest that the brain mightuse similar mechanisms to understand the structure of objects in diversesensory modalities, including vision. In machine vision, object recognitiongiven a sequence of sensory samples of an image, such as saccades, is achallenging problem when the sequence does not follow a consistent, fixedpattern - yet this is something humans do naturally and effortlessly. Weexplore how grid cell-based path integration in a cortical network can supportreliable recognition of objects given an arbitrary sequence of inputs. Ournetwork (GridCellNet) uses grid cell computations to integrate visualinformation and make predictions based on movements. We use local Hebbianplasticity rules to learn rapidly from a handful of examples (few-shotlearning), and consider the task of recognizing MNIST digits given only asequence of image feature patches. We compare GridCellNet to k-NearestNeighbour (k-NN) classifiers as well as recurrent neural networks (RNNs), bothof which lack explicit mechanisms for handling arbitrary sequences of inputsamples. We show that GridCellNet can reliably perform classification,generalizing to both unseen examples and completely novel sequencetrajectories. We further show that inference is often successful after samplinga fraction of the input space, enabling the predictive GridCellNet toreconstruct the rest of the image given just a few movements. We propose thatdynamically moving agents with active sensors can use grid cell representationsnot only for navigation, but also for efficient recognition and featureprediction of seen objects."
Few-shot Network Anomaly Detection via Cross-network Meta-learning,"['Kaize Ding', 'Qinghai Zhou', 'Hanghang Tong', 'Huan Liu']",http://arxiv.org/pdf/2102.11165v1.pdf,2021-02-22,['cs.lg'],"  Network anomaly detection aims to find network elements (e.g., nodes, edges,subgraphs) with significantly different behaviors from the vast majority. Ithas a profound impact in a variety of applications ranging from finance,healthcare to social network analysis. Due to the unbearable labeling cost,existing methods are predominately developed in an unsupervised manner.Nonetheless, the anomalies they identify may turn out to be data noises oruninteresting data instances due to the lack of prior knowledge on theanomalies of interest. Hence, it is critical to investigate and developfew-shot learning for network anomaly detection. In real-world scenarios, fewlabeled anomalies are also easy to be accessed on similar networks from thesame domain as of the target network, while most of the existing works omit toleverage them and merely focus on a single network. Taking advantage of thispotential, in this work, we tackle the problem of few-shot network anomalydetection by (1) proposing a new family of graph neural networks -- GraphDeviation Networks (GDN) that can leverage a small number of labeled anomaliesfor enforcing statistically significant deviations between abnormal and normalnodes on a network; and (2) equipping the proposed GDN with a new cross-networkmeta-learning algorithm to realize few-shot network anomaly detection bytransferring meta-knowledge from multiple auxiliary networks. Extensiveevaluations demonstrate the efficacy of the proposed approach on few-shot oreven one-shot network anomaly detection."
Dual-Awareness Attention for Few-Shot Object Detection,"['Tung-I Chen', 'Yueh-Cheng Liu', 'Hung-Ting Su', 'Yu-Cheng Chang', 'Yu-Hsiang Lin', 'Jia-Fong Yeh', 'Wen-Chin Chen', 'Winston H. Hsu']",http://arxiv.org/pdf/2102.12152v3.pdf,2021-02-24,['cs.cv'],"  While recent progress has significantly boosted few-shot classification (FSC)performance, few-shot object detection (FSOD) remains challenging for modernlearning systems. Existing FSOD systems follow FSC approaches, ignoringcritical issues such as spatial variability and uncertain representations, andconsequently result in low performance. Observing this, we propose a novel\textbf{Dual-Awareness Attention (DAnA)} mechanism that enables networks toadaptively interpret the given support images. DAnA transforms support imagesinto \textbf{query-position-aware} (QPA) features, guiding detection networksprecisely by assigning customized support information to each local region ofthe query. In addition, the proposed DAnA component is flexible and adaptableto multiple existing object detection frameworks. By adopting DAnA,conventional object detection networks, Faster R-CNN and RetinaNet, which arenot designed explicitly for few-shot learning, reach state-of-the-artperformance in FSOD tasks. In comparison with previous methods, our modelsignificantly increases the performance by 47\% (+6.9 AP), showing remarkableability under various evaluation settings."
OmniNet: Omnidirectional Representations from Transformers,"['Yi Tay', 'Mostafa Dehghani', 'Vamsi Aribandi', 'Jai Gupta', 'Philip Pham', 'Zhen Qin', 'Dara Bahri', 'Da-Cheng Juan', 'Donald Metzler']",http://arxiv.org/pdf/2103.01075v1.pdf,2021-03-01,"['cs.cv', 'cs.ai', 'cs.cl', 'cs.lg']","  This paper proposes Omnidirectional Representations from Transformers(OmniNet). In OmniNet, instead of maintaining a strictly horizontal receptivefield, each token is allowed to attend to all tokens in the entire network.This process can also be interpreted as a form of extreme or intensiveattention mechanism that has the receptive field of the entire width and depthof the network. To this end, the omnidirectional attention is learned via ameta-learner, which is essentially another self-attention based model. In orderto mitigate the computationally expensive costs of full receptive fieldattention, we leverage efficient self-attention models such as kernel-based(Choromanski et al.), low-rank attention (Wang et al.) and/or Big Bird (Zaheeret al.) as the meta-learner. Extensive experiments are conducted onautoregressive language modeling (LM1B, C4), Machine Translation, Long RangeArena (LRA), and Image Recognition. The experiments show that OmniNet achievesconsiderable improvements across these tasks, including achievingstate-of-the-art performance on LM1B, WMT'14 En-De/En-Fr, and Long Range Arena.Moreover, using omnidirectional representation in Vision Transformers leads tosignificant improvements on image recognition tasks on both few-shot learningand fine-tuning setups."
Few-shot Open-set Recognition by Transformation Consistency,"['Minki Jeong', 'Seokeon Choi', 'Changick Kim']",http://arxiv.org/pdf/2103.01537v2.pdf,2021-03-02,['cs.cv'],"  In this paper, we attack a few-shot open-set recognition (FSOSR) problem,which is a combination of few-shot learning (FSL) and open-set recognition(OSR). It aims to quickly adapt a model to a given small set of labeled sampleswhile rejecting unseen class samples. Since OSR requires rich data and FSLconsiders closed-set classification, existing OSR and FSL methods show poorperformances in solving FSOSR problems. The previous FSOSR method follows thepseudo-unseen class sample-based methods, which collect pseudo-unseen samplesfrom the other dataset or synthesize samples to model unseen classrepresentations. However, this approach is heavily dependent on the compositionof the pseudo samples. In this paper, we propose a novel unknown class sampledetector, named SnaTCHer, that does not require pseudo-unseen samples. Based onthe transformation consistency, our method measures the difference between thetransformed prototypes and a modified prototype set. The modified set iscomposed by replacing a query feature and its predicted class prototype.SnaTCHer rejects samples with large differences to the transformed prototypes.Our method alters the unseen class distribution estimation problem to arelative feature transformation problem, independent of pseudo-unseen classsamples. We investigate our SnaTCHer with various prototype transformationmethods and observe that our method consistently improves unseen class sampledetection performance without closed-set classification reduction."
Multi-level Metric Learning for Few-shot Image Recognition,"['Haoxing Chen', 'Huaxiong Li', 'Yaohui Li', 'Chunlin Chen']",http://arxiv.org/pdf/2103.11383v4.pdf,2021-03-21,['cs.cv'],"  Few-shot learning is devoted to training a model on few samples. Most ofthese approaches learn a model based on a pixel-level or global-level featurerepresentation. However, using global features may lose local information, andusing pixel-level features may lose the contextual semantics of the image.Moreover, such works can only measure the relations between them on a singlelevel, which is not comprehensive and effective. And if query images cansimultaneously be well classified via three distinct level similarity metrics,the query images within a class can be more tightly distributed in a smallerfeature space, generating more discriminative feature maps. Motivated by this,we propose a novel Part-level Embedding Adaptation with Graph (PEAG) method togenerate task-specific features. Moreover, a Multi-level Metric Learning (MML)method is proposed, which not only calculates the pixel-level similarity butalso considers the similarity of part-level features and global-level features.Extensive experiments on popular few-shot image recognition datasets prove theeffectiveness of our method compared with the state-of-the-art methods. Ourcode is available at \url{https://github.com/chenhaoxing/M2L}."
Prototypical Representation Learning for Relation Extraction,"['Ning Ding', 'Xiaobin Wang', 'Yao Fu', 'Guangwei Xu', 'Rui Wang', 'Pengjun Xie', 'Ying Shen', 'Fei Huang', 'Hai-Tao Zheng', 'Rui Zhang']",http://arxiv.org/pdf/2103.11647v1.pdf,2021-03-22,"['cs.cl', 'cs.ai']","  Recognizing relations between entities is a pivotal task of relationallearning. Learning relation representations from distantly-labeled datasets isdifficult because of the abundant label noise and complicated expressions inhuman language. This paper aims to learn predictive, interpretable, and robustrelation representations from distantly-labeled data that are effective indifferent settings, including supervised, distantly supervised, and few-shotlearning. Instead of solely relying on the supervision from noisy labels, wepropose to learn prototypes for each relation from contextual information tobest explore the intrinsic semantics of relations. Prototypes arerepresentations in the feature space abstracting the essential semantics ofrelations between entities in sentences. We learn prototypes based onobjectives with clear geometric interpretation, where the prototypes are unitvectors uniformly dispersed in a unit ball, and statement embeddings arecentered at the end of their corresponding prototype vectors on the surface ofthe ball. This approach allows us to learn meaningful, interpretable prototypesfor the final classification. Results on several relation learning tasks showthat our model significantly outperforms the previous state-of-the-art models.We further demonstrate the robustness of the encoder and the interpretabilityof prototypes with extensive experiments."
Spirit Distillation: Precise Real-time Semantic Segmentation of Road  Scenes with Insufficient Data,"['Zhiyuan Wu', 'Yu Jiang', 'Chupeng Cui', 'Zongmin Yang', 'Xinhui Xue', 'Hong Qi']",http://arxiv.org/pdf/2103.13733v2.pdf,2021-03-25,"['cs.cv', 'cs.ai', 'cs.lg']","  Semantic segmentation of road scenes is one of the key technologies forrealizing autonomous driving scene perception, and the effectiveness of deepConvolutional Neural Networks(CNNs) for this task has been demonstrated.State-of-art CNNs for semantic segmentation suffer from excessive computationsas well as large-scale training data requirement. Inspired by the ideas ofFine-tuning-based Transfer Learning (FTT) and feature-based knowledgedistillation, we propose a new knowledge distillation method for cross-domainknowledge transference and efficient data-insufficient network training, namedSpirit Distillation(SD), which allow the student network to mimic the teachernetwork to extract general features, so that a compact and accurate studentnetwork can be trained for real-time semantic segmentation of road scenes.Then, in order to further alleviate the trouble of insufficient data andimprove the robustness of the student, an Enhanced Spirit Distillation (ESD)method is proposed, which commits to exploit a more comprehensive generalfeatures extraction capability by considering images from both the target andthe proximity domains as input. To our knowledge, this paper is a pioneeringwork on the application of knowledge distillation to few-shot learning.Persuasive experiments conducted on Cityscapes semantic segmentation with theprior knowledge transferred from COCO2017 and KITTI demonstrate that ourmethods can train a better student network (mIOU and high-precision accuracyboost by 1.4% and 8.2% respectively, with 78.2% segmentation variance) withonly 41.8% FLOPs (see Fig. 1)."
Orthogonal Projection Loss,"['Kanchana Ranasinghe', 'Muzammal Naseer', 'Munawar Hayat', 'Salman Khan', 'Fahad Shahbaz Khan']",http://arxiv.org/pdf/2103.14021v1.pdf,2021-03-25,['cs.cv'],"  Deep neural networks have achieved remarkable performance on a range ofclassification tasks, with softmax cross-entropy (CE) loss emerging as thede-facto objective function. The CE loss encourages features of a class to havea higher projection score on the true class-vector compared to the negativeclasses. However, this is a relative constraint and does not explicitly forcedifferent class features to be well-separated. Motivated by the observationthat ground-truth class representations in CE loss are orthogonal (one-hotencoded vectors), we develop a novel loss function termed `OrthogonalProjection Loss' (OPL) which imposes orthogonality in the feature space. OPLaugments the properties of CE loss and directly enforces inter-class separationalongside intra-class clustering in the feature space through orthogonalityconstraints on the mini-batch level. As compared to other alternatives of CE,OPL offers unique advantages e.g., no additional learnable parameters, does notrequire careful negative mining and is not sensitive to the batch size. Giventhe plug-and-play nature of OPL, we evaluate it on a diverse range of tasksincluding image recognition (CIFAR-100), large-scale classification (ImageNet),domain generalization (PACS) and few-shot learning (miniImageNet, CIFAR-FS,tiered-ImageNet and Meta-dataset) and demonstrate its effectiveness across theboard. Furthermore, OPL offers better robustness against practical nuisancessuch as adversarial attacks and label noise. Code is available at:https://github.com/kahnchana/opl."
IUP: An Intelligent Utility Prediction Scheme for Solid-State  Fermentation in 5G IoT,"['Min Wang', 'Shanchen Pang', 'Tong Ding', 'Sibo Qiao', 'Xue Zhai', 'Shuo Wang', 'Neal N. Xiong', 'Zhengwen Huang']",http://arxiv.org/pdf/2103.15073v1.pdf,2021-03-28,"['cs.lg', 'cs.ai', 'cs.sy', 'eess.sy']","  At present, SOILD-STATE Fermentation (SSF) is mainly controlled by artificialexperience, and the product quality and yield are not stable. Accuratelypredicting the quality and yield of SSF is of great significance for improvinghuman food security and supply. In this paper, we propose an IntelligentUtility Prediction (IUP) scheme for SSF in 5G Industrial Internet of Things(IoT), including parameter collection and utility prediction of SSF process.This IUP scheme is based on the environmental perception and intelligentlearning algorithms of the 5G Industrial IoT. We build a workflow model basedon rewritable petri net to verify the correctness of the system model functionand process. In addition, we design a utility prediction model for SSF based onthe Generative Adversarial Networks (GAN) and Fully Connected Neural Network(FCNN). We design a GAN with constraint of mean square error (MSE-GAN) to solvethe problem of few-shot learning of SSF, and then combine with the FCNN torealize the utility prediction (usually use the alcohol) of SSF. Based on theproduction of liquor in laboratory, the experiments show that the proposedmethod is more accurate than the other prediction methods in the utilityprediction of SSF, and provide the basis for the numerical analysis of theproportion of preconfigured raw materials and the appropriate setting of cellartemperature."
Learning Domain Adaptation with Model Calibration for Surgical Report  Generation in Robotic Surgery,"['Mengya Xu', 'Mobarakol Islam', 'Chwee Ming Lim', 'Hongliang Ren']",http://arxiv.org/pdf/2103.17120v1.pdf,2021-03-31,['cs.ro'],"  Generating a surgical report in robot-assisted surgery, in the form ofnatural language expression of surgical scene understanding, can play asignificant role in document entry tasks, surgical training, and post-operativeanalysis. Despite the state-of-the-art accuracy of the deep learning algorithm,the deployment performance often drops when applied to the Target Domain (TD)data. For this purpose, we develop a multi-layer transformer-based model withthe gradient reversal adversarial learning to generate a caption for themulti-domain surgical images that can describe the semantic relationshipbetween instruments and surgical Region of Interest (ROI). In the gradientreversal adversarial learning scheme, the gradient multiplies with a negativeconstant and updates adversarially in backward propagation, discriminatingbetween the source and target domains and emerging domain-invariant features.We also investigate model calibration with label smoothing technique and theeffect of a well-calibrated model for the penultimate layer's featurerepresentation and Domain Adaptation (DA). We annotate two robotic surgerydatasets of MICCAI robotic scene segmentation and Transoral Robotic Surgery(TORS) with the captions of procedures and empirically show that our proposedmethod improves the performance in both source and target domain surgicalreports generation in the manners of unsupervised, zero-shot, one-shot, andfew-shot learning."
Efficient Personalized Speech Enhancement through Self-Supervised  Learning,"['Aswin Sivaraman', 'Minje Kim']",http://arxiv.org/pdf/2104.02017v2.pdf,2021-04-05,"['eess.as', 'cs.lg', 'cs.sd']","  This work presents self-supervised learning methods for developing monauralspeaker-specific (i.e., personalized) speech enhancement models. Whilegeneralist models must broadly address many speakers, specialist models canadapt their enhancement function towards a particular speaker's voice,expecting to solve a narrower problem. Hence, specialists are capable ofachieving more optimal performance in addition to reducing computationalcomplexity. However, naive personalization methods can require clean speechfrom the target user, which is inconvenient to acquire, e.g., due to subparrecording conditions. To this end, we pose personalization as either azero-shot task, in which no additional clean speech of the target speaker isused for training, or a few-shot learning task, in which the goal is tominimize the duration of the clean speech used for transfer learning. With thispaper, we propose self-supervised learning methods as a solution to both zero-and few-shot personalization tasks. The proposed methods are designed to learnthe personalized speech features from unlabeled data (i.e., in-the-wild noisyrecordings from the target user) without knowing the corresponding cleansources. Our experiments investigate three different self-supervised learningmechanisms. The results show that self-supervised models achieve zero-shot andfew-shot personalization using fewer model parameters and less clean data fromthe target user, achieving the data efficiency and model compression goals."
Towards Enabling Meta-Learning from Target Models,"['Su Lu', 'Han-Jia Ye', 'Le Gan', 'De-Chuan Zhan']",http://arxiv.org/pdf/2104.03736v5.pdf,2021-04-08,"['cs.lg', 'cs.ai', 'cs.cv']","  Meta-learning can extract an inductive bias from previous learning experienceand assist the training of new tasks. It is often realized through optimizing ameta-model with the evaluation loss of task-specific solvers. Most existingalgorithms sample non-overlapping $\mathit{support}$ sets and $\mathit{query}$sets to train and evaluate the solvers respectively due to simplicity($\mathcal{S}$/$\mathcal{Q}$ protocol). Different from$\mathcal{S}$/$\mathcal{Q}$ protocol, we can also evaluate a task-specificsolver by comparing it to a target model $\mathcal{T}$, which is the optimalmodel for this task or a model that behaves well enough on this task($\mathcal{S}$/$\mathcal{T}$ protocol). Although being short of research,$\mathcal{S}$/$\mathcal{T}$ protocol has unique advantages such as offeringmore informative supervision, but it is computationally expensive. This paperlooks into this special evaluation method and takes a step towards putting itinto practice. We find that with a small ratio of tasks armed with targetmodels, classic meta-learning algorithms can be improved a lot withoutconsuming many resources. We empirically verify the effectiveness of$\mathcal{S}$/$\mathcal{T}$ protocol in a typical application of meta-learning,$\mathit{i.e.}$, few-shot learning. In detail, after constructing target modelsby fine-tuning the pre-trained network on those hard tasks, we match thetask-specific solvers and target models via knowledge distillation."
Few-Shot Action Recognition with Compromised Metric via Optimal  Transport,"['Su Lu', 'Han-Jia Ye', 'De-Chuan Zhan']",http://arxiv.org/pdf/2104.03737v1.pdf,2021-04-08,"['cs.cv', 'cs.ai', 'cs.lg']","  Although vital to computer vision systems, few-shot action recognition isstill not mature despite the wide research of few-shot image classification.Popular few-shot learning algorithms extract a transferable embedding from seenclasses and reuse it on unseen classes by constructing a metric-basedclassifier. One main obstacle to applying these algorithms in actionrecognition is the complex structure of videos. Some existing solutions sampleframes from a video and aggregate their embeddings to form a video-levelrepresentation, neglecting important temporal relations. Others perform anexplicit sequence matching between two videos and define their distance asmatching cost, imposing too strong restrictions on sequence ordering. In thispaper, we propose Compromised Metric via Optimal Transport (CMOT) to combinethe advantages of these two solutions. CMOT simultaneously considers semanticand temporal information in videos under Optimal Transport framework, and isdiscriminative for both content-sensitive and ordering-sensitive tasks. Indetail, given two videos, we sample segments from them and cast the calculationof their distance as an optimal transport problem between two segmentsequences. To preserve the inherent temporal ordering information, weadditionally amend the ground cost matrix by penalizing it with the positionaldistance between a pair of segments. Empirical results on benchmark datasetsdemonstrate the superiority of CMOT."
ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition,"['Daniela Massiceti', 'Luisa Zintgraf', 'John Bronskill', 'Lida Theodorou', 'Matthew Tobias Harris', 'Edward Cutrell', 'Cecily Morrison', 'Katja Hofmann', 'Simone Stumpf']",http://arxiv.org/pdf/2104.03841v5.pdf,2021-04-08,['cs.cv'],"  Object recognition has made great advances in the last decade, butpredominately still relies on many high-quality training examples per objectcategory. In contrast, learning new objects from only a few examples couldenable many impactful applications from robotics to user personalization. Mostfew-shot learning research, however, has been driven by benchmark datasets thatlack the high variation that these applications will face when deployed in thereal-world. To close this gap, we present the ORBIT dataset and benchmark,grounded in the real-world application of teachable object recognizers forpeople who are blind/low-vision. The dataset contains 3,822 videos of 486objects recorded by people who are blind/low-vision on their mobile phones. Thebenchmark reflects a realistic, highly challenging recognition problem,providing a rich playground to drive research in robustness to few-shot,high-variation conditions. We set the benchmark's first state-of-the-art andshow there is massive scope for further innovation, holding the potential toimpact a broad range of real-world vision applications including tools for theblind/low-vision community. We release the dataset athttps://doi.org/10.25383/city.14294597 and benchmark code athttps://github.com/microsoft/ORBIT-Dataset."
Contextual HyperNetworks for Novel Feature Adaptation,"['Angus Lamb', 'Evgeny Saveliev', 'Yingzhen Li', 'Sebastian Tschiatschek', 'Camilla Longden', 'Simon Woodhead', 'José Miguel Hernández-Lobato', 'Richard E. Turner', 'Pashmina Cameron', 'Cheng Zhang']",http://arxiv.org/pdf/2104.05860v1.pdf,2021-04-12,['cs.lg'],"  While deep learning has obtained state-of-the-art results in manyapplications, the adaptation of neural network architectures to incorporate newoutput features remains a challenge, as neural networks are commonly trained toproduce a fixed output dimension. This issue is particularly severe in onlinelearning settings, where new output features, such as items in a recommendersystem, are added continually with few or no associated observations. As such,methods for adapting neural networks to novel features which are both time anddata-efficient are desired. To address this, we propose the ContextualHyperNetwork (CHN), an auxiliary model which generates parameters for extendingthe base model to a new feature, by utilizing both existing data as well as anyobservations and/or metadata associated with the new feature. At predictiontime, the CHN requires only a single forward pass through a neural network,yielding a significant speed-up when compared to re-training and fine-tuningapproaches.  To assess the performance of CHNs, we use a CHN to augment a partialvariational autoencoder (P-VAE), a deep generative model which can impute thevalues of missing features in sparsely-observed data. We show that this systemobtains improved few-shot learning performance for novel features over existingimputation and meta-learning baselines across recommender systems, e-learning,and healthcare tasks."
A Deep Learning Framework for Lifelong Machine Learning,"['Charles X. Ling', 'Tanner Bohn']",http://arxiv.org/pdf/2105.00157v1.pdf,2021-05-01,['cs.ai'],"  Humans can learn a variety of concepts and skills incrementally over thecourse of their lives while exhibiting many desirable properties, such ascontinual learning without forgetting, forward transfer and backward transferof knowledge, and learning a new concept or task with only a few examples.Several lines of machine learning research, such as lifelong machine learning,few-shot learning, and transfer learning attempt to capture these properties.However, most previous approaches can only demonstrate subsets of theseproperties, often by different complex mechanisms. In this work, we propose asimple yet powerful unified deep learning framework that supports almost all ofthese properties and approaches through one central mechanism. Experiments ontoy examples support our claims. We also draw connections between manypeculiarities of human learning (such as memory loss and ""rain man"") and ourframework.  As academics, we often lack resources required to build and train, deepneural networks with billions of parameters on hundreds of TPUs. Thus, whileour framework is still conceptual, and our experiment results are surely notSOTA, we hope that this unified lifelong learning framework inspires new worktowards large-scale experiments and understanding human learning in general.  This paper is summarized in two short YouTube videos:https://youtu.be/gCuUyGETbTU (part 1) and https://youtu.be/XsaGI01b-1o (part2)."
PEMNET: A Transfer Learning-based Modeling Approach of High-Temperature  Polymer Electrolyte Membrane Electrochemical Systems,"['Luis A. Briceno-Mena', 'Christopher G. Arges', 'Jose A. Romagnoli']",http://arxiv.org/pdf/2105.03057v2.pdf,2021-05-07,"['cs.lg', 'physics.chem-ph', 'i.6.5; j.6']","  Widespread adoption of high-temperature polymer electrolyte membrane fuelcells (HT-PEMFCs) and HT-PEM electrochemical hydrogen pumps (HT-PEM ECHPs)requires models and computational tools that provide accurate scale-up andoptimization. Knowledge-based modeling has limitations as it is time consumingand requires information about the system that is not always available (e.g.,material properties and interfacial behavior between different materials).Data-driven modeling on the other hand, is easier to implement, but oftennecessitates large datasets that could be difficult to obtain. In thiscontribution, knowledge-based modeling and data-driven modeling are uniquelycombined by implementing a Few-Shot Learning (FSL) approach. A knowledge-basedmodel originally developed for a HT-PEMFC was used to generate simulated data(887,735 points) and used to pretrain a neural network source model.Furthermore, the source model developed for HT-PEMFCs was successfully appliedto HT-PEM ECHPs - a different electrochemical system that utilizes similarmaterials to the fuel cell. Experimental datasets from both HT-PEMFCs andHT-PEM ECHPs with different materials and operating conditions (~50 pointseach) were used to train 8 target models via FSL. Models for the unseen datareached high accuracies in all cases (rRMSE between 1.04 and 3.73% for HT-PEMCsand between 6.38 and 8.46% for HT-PEM ECHPs)."
Semi-supervised Contrastive Learning with Similarity Co-calibration,"['Yuhang Zhang', 'Xiaopeng Zhang', 'Robert. C. Qiu', 'Jie Li', 'Haohang Xu', 'Qi Tian']",http://arxiv.org/pdf/2105.07387v1.pdf,2021-05-16,['cs.cv'],"  Semi-supervised learning acts as an effective way to leverage massiveunlabeled data. In this paper, we propose a novel training strategy, termed asSemi-supervised Contrastive Learning (SsCL), which combines the well-knowncontrastive loss in self-supervised learning with the cross entropy loss insemi-supervised learning, and jointly optimizes the two objectives in anend-to-end way. The highlight is that different from self-training basedsemi-supervised learning that conducts prediction and retraining over the samemodel weights, SsCL interchanges the predictions over the unlabeled databetween the two branches, and thus formulates a co-calibration procedure, whichwe find is beneficial for better prediction and avoid being trapped in localminimum. Towards this goal, the contrastive loss branch models pairwisesimilarities among samples, using the nearest neighborhood generated from thecross entropy branch, and in turn calibrates the prediction distribution of thecross entropy branch with the contrastive similarity. We show that SsCLproduces more discriminative representation and is beneficial to few shotlearning. Notably, on ImageNet with ResNet50 as the backbone, SsCL achieves60.2% and 72.1% top-1 accuracy with 1% and 10% labeled samples, respectively,which significantly outperforms the baseline, and is better than previoussemi-supervised and self-supervised methods."
Semi-Supervised Few-Shot Classification with Deep Invertible Hybrid  Models,"['Yusuke Ohtsubo', 'Tetsu Matsukawa', 'Einoshin Suzuki']",http://arxiv.org/pdf/2105.10644v1.pdf,2021-05-22,"['cs.cv', 'cs.lg']","  In this paper, we propose a deep invertible hybrid model which integratesdiscriminative and generative learning at a latent space level forsemi-supervised few-shot classification. Various tasks for classifying newspecies from image data can be modeled as a semi-supervised few-shotclassification, which assumes a labeled and unlabeled training examples and asmall support set of the target classes. Predicting target classes with a fewsupport examples per class makes the learning task difficult for existingsemi-supervised classification methods, including selftraining, whichiteratively estimates class labels of unlabeled training examples to learn aclassifier for the training classes. To exploit unlabeled training exampleseffectively, we adopt as the objective function the composite likelihood, whichintegrates discriminative and generative learning and suits better with deepneural networks than the parameter coupling prior, the other popular integratedlearning approach. In our proposed model, the discriminative and generativemodels are respectively Prototypical Networks, which have shown excellentperformance in various kinds of few-shot learning, and Normalizing Flow a deepinvertible model which returns the exact marginal likelihood unlike the otherthree major methods, i.e., VAE, GAN, and autoregressive model. Our mainoriginality lies in our integration of these components at a latent spacelevel, which is effective in preventing overfitting. Experiments usingmini-ImageNet and VGG-Face datasets show that our method outperformsselftraining based Prototypical Networks."
Few-Shot Action Localization without Knowing Boundaries,"['Ting-Ting Xie', 'Christos Tzelepis', 'Fan Fu', 'Ioannis Patras']",http://arxiv.org/pdf/2106.04150v2.pdf,2021-06-08,['cs.cv'],"  Learning to localize actions in long, cluttered, and untrimmed videos is ahard task, that in the literature has typically been addressed assuming theavailability of large amounts of annotated training samples for each class --either in a fully-supervised setting, where action boundaries are known, or ina weakly-supervised setting, where only class labels are known for each video.In this paper, we go a step further and show that it is possible to learn tolocalize actions in untrimmed videos when a) only one/few trimmed examples ofthe target action are available at test time, and b) when a large collection ofvideos with only class label annotation (some trimmed and some weakly annotateduntrimmed ones) are available for training; with no overlap between the classesused during training and testing. To do so, we propose a network that learns toestimate Temporal Similarity Matrices (TSMs) that model a fine-grainedsimilarity pattern between pairs of videos (trimmed or untrimmed), and usesthem to generate Temporal Class Activation Maps (TCAMs) for seen or unseenclasses. The TCAMs serve as temporal attention mechanisms to extractvideo-level representations of untrimmed videos, and to temporally localizeactions at test time. To the best of our knowledge, we are the first to proposea weakly-supervised, one/few-shot action localization network that can betrained in an end-to-end fashion. Experimental results on THUMOS14 andActivityNet1.2 datasets, show that our method achieves performance comparableor better to state-of-the-art fully-supervised, few-shot learning methods."
Learning to Affiliate: Mutual Centralized Learning for Few-shot  Classification,"['Yang Liu', 'Weifeng Zhang', 'Chao Xiang', 'Tu Zheng', 'Deng Cai', 'Xiaofei He']",http://arxiv.org/pdf/2106.05517v4.pdf,2021-06-10,['cs.cv'],"  Few-shot learning (FSL) aims to learn a classifier that can be easily adaptedto accommodate new tasks not seen during training, given only a few examples.To handle the limited-data problem in few-shot regimes, recent methods tend tocollectively use a set of local features to densely represent an image insteadof using a mixed global feature. They generally explore a unidirectionalquery-to-support paradigm in FSL, e.g., find the nearest/optimal supportfeature for each query feature and aggregate these local matches for a jointclassification. In this paper, we propose a new method Mutual CentralizedLearning (MCL) to fully affiliate the two disjoint sets of dense features in abidirectional paradigm. We associate each local feature with a particle thatcan bidirectionally random walk in a discrete feature space by theaffiliations. To estimate the class probability, we propose the features'accessibility that measures the expected number of visits to the supportfeatures of that class in a Markov process. We relate our method to learning acentrality on an affiliation network and demonstrate its capability to beplugged in existing methods by highlighting centralized local features.Experiments show that our method achieves the state-of-the-art on bothminiImageNet and tieredImageNet."
Learning Compositional Shape Priors for Few-Shot 3D Reconstruction,"['Mateusz Michalkiewicz', 'Stavros Tsogkas', 'Sarah Parisot', 'Mahsa Baktashmotlagh', 'Anders Eriksson', 'Eugene Belilovsky']",http://arxiv.org/pdf/2106.06440v2.pdf,2021-06-11,"['cs.cv', 'cs.lg']","  The impressive performance of deep convolutional neural networks insingle-view 3D reconstruction suggests that these models perform non-trivialreasoning about the 3D structure of the output space. Recent work haschallenged this belief, showing that, on standard benchmarks, complexencoder-decoder architectures perform similarly to nearest-neighbor baselinesor simple linear decoder models that exploit large amounts of per-categorydata. However, building large collections of 3D shapes for supervised trainingis a laborious process; a more realistic and less constraining task isinferring 3D shapes for categories with few available training examples,calling for a model that can successfully generalize to novel object classes.In this work we experimentally demonstrate that naive baselines fail in thisfew-shot learning setting, in which the network must learn informative shapepriors for inference of new categories. We propose three ways to learn aclass-specific global shape prior, directly from data. Using these techniques,we are able to capture multi-scale information about the 3D shape, and accountfor intra-class variability by virtue of an implicit compositional structure.Experiments on the popular ShapeNet dataset show that our method outperforms azero-shot baseline by over 40%, and the current state-of-the-art by over 10%,in terms of relative performance, in the few-shot setting."
Few-shot learning of new sound classes for target sound extraction,"['Marc Delcroix', 'Jorge Bennasar Vázquez', 'Tsubasa Ochiai', 'Keisuke Kinoshita', 'Shoko Araki']",http://arxiv.org/pdf/2106.07144v1.pdf,2021-06-14,"['eess.as', 'cs.sd']","  Target sound extraction consists of extracting the sound of a target acousticevent (AE) class from a mixture of AE sounds. It can be realized using a neuralnetwork that extracts the target sound conditioned on a 1-hot vector thatrepresents the desired AE class. With this approach, embedding vectorsassociated with the AE classes are directly optimized for the extraction ofsound classes seen during training. However, it is not easy to extend thisframework to new AE classes, i.e. unseen during training. Recently, speech,music, or AE sound extraction based on enrollment audio of the desired soundoffers the potential of extracting any target sound in a mixture given only ashort audio signal of a similar sound. In this work, we propose combining1-hot- and enrollment-based target sound extraction, allowing optimalperformance for seen AE classes and simple extension to new classes. Inexperiments with synthesized sound mixtures generated with the FreesoundDataset (FSD) datasets, we demonstrate the benefit of the combined frameworkfor both seen and new AE classes. Besides, we also propose adapting theembedding vectors obtained from a few enrollment audio samples (few-shot) tofurther improve performance on new classes."
Mutual-Information Based Few-Shot Classification,"['Malik Boudiaf', 'Ziko Imtiaz Masud', 'Jérôme Rony', 'Jose Dolz', 'Ismail Ben Ayed', 'Pablo Piantanida']",http://arxiv.org/pdf/2106.12252v1.pdf,2021-06-23,['cs.cv'],"  We introduce Transductive Infomation Maximization (TIM) for few-shotlearning. Our method maximizes the mutual information between the queryfeatures and their label predictions for a given few-shot task, in conjunctionwith a supervision loss based on the support set. We motivate our transductiveloss by deriving a formal relation between the classification accuracy andmutual-information maximization. Furthermore, we propose a newalternating-direction solver, which substantially speeds up transductiveinference over gradient-based optimization, while yielding competitiveaccuracy. We also provide a convergence analysis of our solver based onZangwill's theory and bound-optimization arguments. TIM inference is modular:it can be used on top of any base-training feature extractor. Followingstandard transductive few-shot settings, our comprehensive experimentsdemonstrate that TIM outperforms state-of-the-art methods significantly acrossvarious datasets and networks, while used on top of a fixed feature extractortrained with simple cross-entropy on the base classes, without resorting tocomplex meta-learning schemes. It consistently brings between 2 % and 5 %improvement in accuracy over the best performing method, not only on all thewell-established few-shot benchmarks but also on more challenging scenarios,with random tasks, domain shift and larger numbers of classes, as in therecently introduced META-DATASET. Our code is publicly available athttps://github.com/mboudiaf/TIM. We also publicly release a standalone PyTorchimplementation of META-DATASET, along with additional benchmarking results, athttps://github.com/mboudiaf/pytorch-meta-dataset."
Long-term Cross Adversarial Training: A Robust Meta-learning Method for  Few-shot Classification Tasks,"['Fan Liu', 'Shuyu Zhao', 'Xuelong Dai', 'Bin Xiao']",http://arxiv.org/pdf/2106.12900v3.pdf,2021-06-22,"['cs.lg', 'cs.cr', 'cs.cv']","  Meta-learning model can quickly adapt to new tasks using few-shot labeleddata. However, despite achieving good generalization on few-shot classificationtasks, it is still challenging to improve the adversarial robustness of themeta-learning model in few-shot learning. Although adversarial training (AT)methods such as Adversarial Query (AQ) can improve the adversarially robustperformance of meta-learning models, AT is still computationally expensivetraining. On the other hand, meta-learning models trained with AT will dropsignificant accuracy on the original clean images. This paper proposed ameta-learning method on the adversarially robust neural network calledLong-term Cross Adversarial Training (LCAT). LCAT will update meta-learningmodel parameters cross along the natural and adversarial sample distributiondirection with long-term to improve both adversarial and clean few-shotclassification accuracy. Due to cross-adversarial training, LCAT only needshalf of the adversarial training epoch than AQ, resulting in a low adversarialtraining computation. Experiment results show that LCAT achieves superiorperformance both on the clean and adversarial few-shot classification accuracythan SOTA adversarial training methods for meta-learning models."
Circumpapillary OCT-Focused Hybrid Learning for Glaucoma Grading Using  Tailored Prototypical Neural Networks,"['Gabriel García', 'Rocío del Amor', 'Adrián Colomer', 'Rafael Verdú-Monedero', 'Juan Morales-Sánchez', 'Valery Naranjo']",http://arxiv.org/pdf/2106.13551v1.pdf,2021-06-25,"['eess.iv', 'cs.cv', 'cs.lg']","  Glaucoma is one of the leading causes of blindness worldwide and OpticalCoherence Tomography (OCT) is the quintessential imaging technique for itsdetection. Unlike most of the state-of-the-art studies focused on glaucomadetection, in this paper, we propose, for the first time, a novel framework forglaucoma grading using raw circumpapillary B-scans. In particular, we set out anew OCT-based hybrid network which combines hand-driven and deep learningalgorithms. An OCT-specific descriptor is proposed to extract hand-craftedfeatures related to the retinal nerve fibre layer (RNFL). In parallel, aninnovative CNN is developed using skip-connections to include tailored residualand attention modules to refine the automatic features of the latent space. Theproposed architecture is used as a backbone to conduct a novel few-shotlearning based on static and dynamic prototypical networks. The k-shot paradigmis redefined giving rise to a supervised end-to-end system which providessubstantial improvements discriminating between healthy, early and advancedglaucoma samples. The training and evaluation processes of the dynamicprototypical network are addressed from two fused databases acquired viaHeidelberg Spectralis system. Validation and testing results reach acategorical accuracy of 0.9459 and 0.8788 for glaucoma grading, respectively.Besides, the high performance reported by the proposed model for glaucomadetection deserves a special mention. The findings from the class activationmaps are directly in line with the clinicians' opinion since the heatmapspointed out the RNFL as the most relevant structure for glaucoma diagnosis."
Dizygotic Conditional Variational AutoEncoder for Multi-Modal and  Partial Modality Absent Few-Shot Learning,"['Yi Zhang', 'Sheng Huang', 'Xi Peng', 'Dan Yang']",http://arxiv.org/pdf/2106.14467v1.pdf,2021-06-28,"['cs.cv', 'cs.ai']","  Data augmentation is a powerful technique for improving the performance ofthe few-shot classification task. It generates more samples as supplements, andthen this task can be transformed into a common supervised learning issue forsolution. However, most mainstream data augmentation based approaches onlyconsider the single modality information, which leads to the low diversity andquality of generated features. In this paper, we present a novel multi-modaldata augmentation approach named Dizygotic Conditional Variational AutoEncoder(DCVAE) for addressing the aforementioned issue. DCVAE conducts featuresynthesis via pairing two Conditional Variational AutoEncoders (CVAEs) with thesame seed but different modality conditions in a dizygotic symbiosis manner.Subsequently, the generated features of two CVAEs are adaptively combined toyield the final feature, which can be converted back into its paired conditionswhile ensuring these conditions are consistent with the original conditions notonly in representation but also in function. DCVAE essentially provides a newidea of data augmentation in various multi-modal scenarios by exploiting thecomplement of different modality prior information. Extensive experimentalresults demonstrate our work achieves state-of-the-art performances onminiImageNet, CIFAR-FS and CUB datasets, and is able to work well in thepartial modality absence case."
Class-Incremental Domain Adaptation with Smoothing and Calibration for  Surgical Report Generation,"['Mengya Xu', 'Mobarakol Islam', 'Chwee Ming Lim', 'Hongliang Ren']",http://arxiv.org/pdf/2107.11091v1.pdf,2021-07-23,"['cs.cv', 'eess.iv']","  Generating surgical reports aimed at surgical scene understanding inrobot-assisted surgery can contribute to documenting entry tasks andpost-operative analysis. Despite the impressive outcome, the deep learningmodel degrades the performance when applied to different domains encounteringdomain shifts. In addition, there are new instruments and variations insurgical tissues appeared in robotic surgery. In this work, we proposeclass-incremental domain adaptation (CIDA) with a multi-layer transformer-basedmodel to tackle the new classes and domain shift in the target domain togenerate surgical reports during robotic surgery. To adapt incremental classesand extract domain invariant features, a class-incremental (CI) learning methodwith supervised contrastive (SupCon) loss is incorporated with a featureextractor. To generate caption from the extracted feature, curriculum byone-dimensional gaussian smoothing (CBS) is integrated with a multi-layertransformer-based caption prediction model. CBS smoothes the features embeddingusing anti-aliasing and helps the model to learn domain invariant features. Wealso adopt label smoothing (LS) to calibrate prediction probability and obtainbetter feature representation with both feature extractor and captioning model.The proposed techniques are empirically evaluated by using the datasets of twosurgical domains, such as nephrectomy operations and transoral robotic surgery.We observe that domain invariant feature learning and the well-calibratednetwork improves the surgical report generation performance in both source andtarget domain under domain shift and unseen classes in the manners of one-shotand few-shot learning. The code is publicly available athttps://github.com/XuMengyaAmy/CIDACaptioning."
Modelling Latent Translations for Cross-Lingual Transfer,"['Edoardo Maria Ponti', 'Julia Kreutzer', 'Ivan Vulić', 'Siva Reddy']",http://arxiv.org/pdf/2107.11353v1.pdf,2021-07-23,['cs.cl'],"  While achieving state-of-the-art results in multiple tasks and languages,translation-based cross-lingual transfer is often overlooked in favour ofmassively multilingual pre-trained encoders. Arguably, this is due to its mainlimitations: 1) translation errors percolating to the classification phase and2) the insufficient expressiveness of the maximum-likelihood translation. Toremedy this, we propose a new technique that integrates both steps of thetraditional pipeline (translation and classification) into a single model, bytreating the intermediate translations as a latent random variable. As aresult, 1) the neural machine translation system can be fine-tuned with avariant of Minimum Risk Training where the reward is the accuracy of thedownstream task classifier. Moreover, 2) multiple samples can be drawn toapproximate the expected loss across all possible translations duringinference. We evaluate our novel latent translation-based model on a series ofmultilingual NLU tasks, including commonsense reasoning, paraphraseidentification, and natural language inference. We report gains for bothzero-shot and few-shot learning setups, up to 2.7 accuracy points on average,which are even more prominent for low-resource languages (e.g., HaitianCreole). Finally, we carry out in-depth analyses comparing different underlyingNMT models and assessing the impact of alternative translations on thedownstream performance."
ProtoTransformer: A Meta-Learning Approach to Providing Student Feedback,"['Mike Wu', 'Noah Goodman', 'Chris Piech', 'Chelsea Finn']",http://arxiv.org/pdf/2107.14035v2.pdf,2021-07-23,"['cs.cy', 'cs.lg']","  High-quality computer science education is limited by the difficulty ofproviding instructor feedback to students at scale. While this feedback couldin principle be automated, supervised approaches to predicting the correctfeedback are bottlenecked by the intractability of annotating large quantitiesof student code. In this paper, we instead frame the problem of providingfeedback as few-shot classification, where a meta-learner adapts to givefeedback to student code on a new programming question from just a few examplesannotated by instructors. Because data for meta-training is limited, we proposea number of amendments to the typical few-shot learning framework, includingtask augmentation to create synthetic tasks, and additional side information tobuild stronger priors about each task. These additions are combined with atransformer architecture to embed discrete sequences (e.g. code) to aprototypical representation of a feedback class label. On a suite of few-shotnatural language processing tasks, we match or outperform state-of-the-artperformance. Then, on a collection of student solutions to exam questions froman introductory university course, we show that our approach reaches an averageprecision of 88% on unseen questions, surpassing the 82% precision of teachingassistants. Our approach was successfully deployed to deliver feedback to16,000 student exam-solutions in a programming course offered by a tier 1university. This is, to the best of our knowledge, the first successfuldeployment of a machine learning based feedback to open-ended student code."
Elaborative Rehearsal for Zero-shot Action Recognition,"['Shizhe Chen', 'Dong Huang']",http://arxiv.org/pdf/2108.02833v2.pdf,2021-08-05,['cs.cv'],"  The growing number of action classes has posed a new challenge for videounderstanding, making Zero-Shot Action Recognition (ZSAR) a thriving direction.The ZSAR task aims to recognize target (unseen) actions without trainingexamples by leveraging semantic representations to bridge seen and unseenactions. However, due to the complexity and diversity of actions, it remainschallenging to semantically represent action classes and transfer knowledgefrom seen data. In this work, we propose an ER-enhanced ZSAR model inspired byan effective human memory technique Elaborative Rehearsal (ER), which involveselaborating a new concept and relating it to known concepts. Specifically, weexpand each action class as an Elaborative Description (ED) sentence, which ismore discriminative than a class name and less costly than manual-definedattributes. Besides directly aligning class semantics with videos, weincorporate objects from the video as Elaborative Concepts (EC) to improvevideo semantics and generalization from seen actions to unseen actions. OurER-enhanced ZSAR model achieves state-of-the-art results on three existingbenchmarks. Moreover, we propose a new ZSAR evaluation protocol on the Kineticsdataset to overcome limitations of current benchmarks and demonstrate the firstcase where ZSAR performance is comparable to few-shot learning baselines onthis more realistic setting. We will release our codes and collected EDs athttps://github.com/DeLightCMU/ElaborativeRehearsal."
MCML: A Novel Memory-based Contrastive Meta-Learning Method for Few Shot  Slot Tagging,"['Hongru Wang', 'Zezhong Wang', 'Wai Chung Kwan', 'Kam-Fai Wong']",http://arxiv.org/pdf/2108.11635v3.pdf,2021-08-26,['cs.ai'],"  Meta-learning is widely used for few-shot slot tagging in task of few-shotlearning. The performance of existing methods is, however, seriously affectedby \textit{sample forgetting issue}, where the model forgets the historicallylearned meta-training tasks while solely relying on support sets when adaptingto new tasks. To overcome this predicament, we propose the\textbf{M}emory-based \textbf{C}ontrastive \textbf{M}eta-\textbf{L}earning(aka, MCML) method, including \textit{learn-from-the-memory} and\textit{adaption-from-the-memory} modules, which bridge the distribution gapbetween training episodes and between training and testing respectively.Specifically, the former uses an explicit memory bank to keep track of thelabel representations of previously trained episodes, with a contrastiveconstraint between the label representations in the current episode with thehistorical ones stored in the memory. In addition, the\emph{adaption-from-memory} mechanism is introduced to learn more accurate androbust representations based on the shift between the same labels embedded inthe testing episodes and memory. Experimental results show that the MCMLoutperforms several state-of-the-art methods on both SNIPS and NER datasets anddemonstrates strong scalability with consistent improvement when the number ofshots gets greater."
Binocular Mutual Learning for Improving Few-shot Classification,"['Ziqi Zhou', 'Xi Qiu', 'Jiangtao Xie', 'Jianan Wu', 'Chi Zhang']",http://arxiv.org/pdf/2108.12104v1.pdf,2021-08-27,['cs.cv'],"  Most of the few-shot learning methods learn to transfer knowledge fromdatasets with abundant labeled data (i.e., the base set). From the perspectiveof class space on base set, existing methods either focus on utilizing allclasses under a global view by normal pretraining, or pay more attention toadopt an episodic manner to train meta-tasks within few classes in a localview. However, the interaction of the two views is rarely explored. As the twoviews capture complementary information, we naturally think of thecompatibility of them for achieving further performance gains. Inspired by themutual learning paradigm and binocular parallax, we propose a unifiedframework, namely Binocular Mutual Learning (BML), which achieves thecompatibility of the global view and the local view through both intra-view andcross-view modeling. Concretely, the global view learns in the whole classspace to capture rich inter-class relationships. Meanwhile, the local viewlearns in the local class space within each episode, focusing on matchingpositive pairs correctly. In addition, cross-view mutual interaction furtherpromotes the collaborative learning and the implicit exploration of usefulknowledge from each other. During meta-test, binocular embeddings areaggregated together to support decision-making, which greatly improve theaccuracy of classification. Extensive experiments conducted on multiplebenchmarks including cross-domain validation confirm the effectiveness of ourmethod."
Anomaly Detection of Defect using Energy of Point Pattern Features  within Random Finite Set Framework,"['Ammar Mansoor Kamoona', 'Amirali Khodadadian Gostar', 'Alireza Bab-Hadiashar', 'Reza Hoseinnezhad']",http://arxiv.org/pdf/2108.12159v1.pdf,2021-08-27,"['cs.cv', 'cs.lg']","  In this paper, we propose an efficient approach for industrial defectdetection that is modeled based on anomaly detection using point pattern data.Most recent works use \textit{global features} for feature extraction tosummarize image content. However, global features are not robust againstlighting and viewpoint changes and do not describe the image's geometricalinformation to be fully utilized in the manufacturing industry. To the best ofour knowledge, we are the first to propose using transfer learning oflocal/point pattern features to overcome these limitations and capturegeometrical information of the image regions. We model these local/pointpattern features as a random finite set (RFS). In addition we propose RFSenergy, in contrast to RFS likelihood as anomaly score. The similaritydistribution of point pattern features of the normal sample has been modeled asa multivariate Gaussian. Parameters learning of the proposed RFS energy doesnot require any heavy computation. We evaluate the proposed approach on theMVTec AD dataset, a multi-object defect detection dataset. Experimental resultsshow the outstanding performance of our proposed approach compared to thestate-of-the-art methods, and the proposed RFS energy outperforms thestate-of-the-art in the few shot learning settings."
Robust Retrieval Augmented Generation for Zero-shot Slot Filling,"['Michael Glass', 'Gaetano Rossiello', 'Md Faisal Mahbub Chowdhury', 'Alfio Gliozzo']",http://arxiv.org/pdf/2108.13934v2.pdf,2021-08-31,"['cs.cl', 'cs.ai', 'cs.ir']","  Automatically inducing high quality knowledge graphs from a given collectionof documents still remains a challenging problem in AI. One way to make headwayfor this problem is through advancements in a related task known as slotfilling. In this task, given an entity query in form of [Entity, Slot, ?], asystem is asked to fill the slot by generating or extracting the missing valueexploiting evidence extracted from relevant passage(s) in the given documentcollection. The recent works in the field try to solve this task in anend-to-end fashion using retrieval-based language models. In this paper, wepresent a novel approach to zero-shot slot filling that extends dense passageretrieval with hard negatives and robust training procedures for retrievalaugmented generation models. Our model reports large improvements on both T-RExand zsRE slot filling datasets, improving both passage retrieval and slot valuegeneration, and ranking at the top-1 position in the KILT leaderboard.Moreover, we demonstrate the robustness of our system showing its domainadaptation capability on a new variant of the TACRED dataset for slot filling,through a combination of zero/few-shot learning. We release the source code andpre-trained models."
Global Convolutional Neural Processes,"['Xuesong Wang', 'Lina Yao', 'Xianzhi Wang', 'Hye-young Paik', 'Sen Wang']",http://arxiv.org/pdf/2109.00691v1.pdf,2021-09-02,['cs.lg'],"  The ability to deal with uncertainty in machine learning models has becomeequally, if not more, crucial to their predictive ability itself. For instance,during the pandemic, governmental policies and personal decisions areconstantly made around uncertainties. Targeting this, Neural Process Families(NPFs) have recently shone a light on prediction with uncertainties by bridgingGaussian processes and neural networks. Latent neural process, a member of NPF,is believed to be capable of modelling the uncertainty on certain points (localuncertainty) as well as the general function priors (global uncertainties).Nonetheless, some critical questions remain unresolved, such as a formaldefinition of global uncertainties, the causality behind global uncertainties,and the manipulation of global uncertainties for generative models. Regardingthis, we build a member GloBal Convolutional Neural Process(GBCoNP) thatachieves the SOTA log-likelihood in latent NPFs. It designs a globaluncertainty representation p(z), which is an aggregation on a discretized inputspace. The causal effect between the degree of global uncertainty and theintra-task diversity is discussed. The learnt prior is analyzed on a variety ofscenarios, including 1D, 2D, and a newly proposed spatial-temporal COVIDdataset. Our manipulation of the global uncertainty not only achievesgenerating the desired samples to tackle few-shot learning, but also enablesthe probability evaluation on the functional priors."
CAM-loss: Towards Learning Spatially Discriminative Feature  Representations,"['Chaofei Wang', 'Jiayu Xiao', 'Yizeng Han', 'Qisen Yang', 'Shiji Song', 'Gao Huang']",http://arxiv.org/pdf/2109.01359v2.pdf,2021-09-03,['cs.cv'],"  The backbone of traditional CNN classifier is generally considered as afeature extractor, followed by a linear layer which performs theclassification. We propose a novel loss function, termed as CAM-loss, toconstrain the embedded feature maps with the class activation maps (CAMs) whichindicate the spatially discriminative regions of an image for particularcategories. CAM-loss drives the backbone to express the features of targetcategory and suppress the features of non-target categories or background, soas to obtain more discriminative feature representations. It can be simplyapplied in any CNN architecture with neglectable additional parameters andcalculations. Experimental results show that CAM-loss is applicable to avariety of network structures and can be combined with mainstreamregularization methods to improve the performance of image classification. Thestrong generalization ability of CAM-loss is validated in the transfer learningand few shot learning tasks. Based on CAM-loss, we also propose a novelCAAM-CAM matching knowledge distillation method. This method directly uses theCAM generated by the teacher network to supervise the CAAM generated by thestudent network, which effectively improves the accuracy and convergence rateof the student network."
FewshotQA: A simple framework for few-shot learning of question  answering tasks using pre-trained text-to-text models,"['Rakesh Chada', 'Pradeep Natarajan']",http://arxiv.org/pdf/2109.01951v3.pdf,2021-09-04,"['cs.cl', 'cs.ai', 'cs.lg']","  The task of learning from only a few examples (called a few-shot setting) isof key importance and relevance to a real-world setting. For question answering(QA), the current state-of-the-art pre-trained models typically needfine-tuning on tens of thousands of examples to obtain good results. Theirperformance degrades significantly in a few-shot setting (< 100 examples). Toaddress this, we propose a simple fine-tuning framework that leveragespre-trained text-to-text models and is directly aligned with their pre-trainingframework. Specifically, we construct the input as a concatenation of thequestion, a mask token representing the answer span and a context. Given thisinput, the model is fine-tuned using the same objective as that of itspre-training objective. Through experimental studies on various few-shotconfigurations, we show that this formulation leads to significant gains onmultiple QA benchmarks (an absolute gain of 34.2 F1 points on average whenthere are only 16 training examples). The gains extend further when used withlarger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)and translate well to a multilingual setting . On the multilingual TydiQAbenchmark, our model outperforms the XLM-Roberta-large by an absolute margin ofupto 40 F1 points and an average of 33 F1 points in a few-shot setting (<= 64training examples). We conduct detailed ablation studies to analyze factorscontributing to these gains."
Prior Omission of Dissimilar Source Domain(s) for Cost-Effective  Few-Shot Learning,"['Zezhong Wang', 'Hongru Wang', 'Kwan Wai Chung', 'Jia Zhu', 'Gabriel Pui Cheong Fung', 'Kam-Fai Wong']",http://arxiv.org/pdf/2109.05234v1.pdf,2021-09-11,"['cs.cl', 'cs.ai']","  Few-shot slot tagging is an emerging research topic in the field of NaturalLanguage Understanding (NLU). With sufficient annotated data from sourcedomains, the key challenge is how to train and adapt the model to anothertarget domain which only has few labels. Conventional few-shot approaches useall the data from the source domains without considering inter-domain relationsand implicitly assume each sample in the domain contributes equally. However,our experiments show that the data distribution bias among different domainswill significantly affect the adaption performance. Moreover, transferringknowledge from dissimilar domains will even introduce some extra noises so thataffect the performance of models. To tackle this problem, we propose aneffective similarity-based method to select data from the source domains. Inaddition, we propose a Shared-Private Network (SP-Net) for the few-shot slottagging task. The words from the same class would have some shared features. Weextract those shared features from the limited annotated data on the targetdomain and merge them together as the label embedding to help us predict otherunlabelled data on the target domain. The experiment shows that our methodoutperforms the state-of-the-art approaches with fewer source data. The resultalso proves that some training data from dissimilar sources are redundant andeven negative for the adaption."
Online Unsupervised Learning of Visual Representations and Categories,"['Mengye Ren', 'Tyler R. Scott', 'Michael L. Iuzzolino', 'Michael C. Mozer', 'Richard Zemel']",http://arxiv.org/pdf/2109.05675v4.pdf,2021-09-13,"['cs.cv', 'cs.lg', 'stat.ml']","  Real world learning scenarios involve a nonstationary distribution of classeswith sequential dependencies among the samples, in contrast to the standardmachine learning formulation of drawing samples independently from a fixed,typically uniform distribution. Furthermore, real world interactions demandlearning on-the-fly from few or no class labels. In this work, we propose anunsupervised model that simultaneously performs online visual representationlearning and few-shot learning of new categories without relying on any classlabels. Our model is a prototype-based memory network with a control componentthat determines when to form a new class prototype. We formulate it as anonline mixture model, where components are created with only a single newexample, and assignments do not have to be balanced, which permits anapproximation to natural imbalanced distributions from uncurated raw data.Learning includes a contrastive loss that encourages different views of thesame image to be assigned to the same prototype. The result is a mechanism thatforms categorical representations of objects in nonstationary environments.Experiments show that our method can learn from an online stream of visualinput data and its learned representations are significantly better at categoryrecognition compared to state-of-the-art self-supervised learning methods."
Towards Generalized and Incremental Few-Shot Object Detection,"['Yiting Li', 'Haiyue Zhu', 'Jun Ma', 'Chek Sing Teo', 'Cheng Xiang', 'Prahlad Vadakkepat', 'Tong Heng Lee']",http://arxiv.org/pdf/2109.11336v1.pdf,2021-09-23,['cs.cv'],"  Real-world object detection is highly desired to be equipped with thelearning expandability that can enlarge its detection classes incrementally.Moreover, such learning from only few annotated training samples further addsthe flexibility for the object detector, which is highly expected in manyapplications such as autonomous driving, robotics, etc. However, suchsequential learning scenario with few-shot training samples generally causescatastrophic forgetting and dramatic overfitting. In this paper, to address theabove incremental few-shot learning issues, a novel Incremental Few-Shot ObjectDetection (iFSOD) method is proposed to enable the effective continual learningfrom few-shot samples. Specifically, a Double-Branch Framework (DBF) isproposed to decouple the feature representation of base and novel (few-shot)class, which facilitates both the old-knowledge retention and new-classadaption simultaneously. Furthermore, a progressive model updating rule iscarried out to preserve the long-term memory on old classes effectively whenadapt to sequential new classes. Moreover, an inter-task class separation lossis proposed to extend the decision region of new-coming classes for betterfeature discrimination. We conduct experiments on both Pascal VOC and MS-COCO,which demonstrate that our method can effectively solve the problem ofincremental few-shot detection and significantly improve the detection accuracyon both base and novel classes."
Paint4Poem: A Dataset for Artistic Visualization of Classical Chinese  Poems,"['Dan Li', 'Shuai Wang', 'Jie Zou', 'Chang Tian', 'Elisha Nieuwburg', 'Fengyuan Sun', 'Evangelos Kanoulas']",http://arxiv.org/pdf/2109.11682v2.pdf,2021-09-23,"['cs.cv', 'cs.ai']","  In this work we propose a new task: artistic visualization of classicalChinese poems, where the goal is to generatepaintings of a certain artisticstyle for classical Chinese poems. For this purpose, we construct a new datasetcalled Paint4Poem. Thefirst part of Paint4Poem consists of 301 high-qualitypoem-painting pairs collected manually from an influential modern ChineseartistFeng Zikai. As its small scale poses challenges for effectively trainingpoem-to-painting generation models, we introduce the secondpart of Paint4Poem,which consists of 3,648 caption-painting pairs collected manually from FengZikai's paintings and 89,204 poem-painting pairs collected automatically fromthe web. We expect the former to help learning the artist painting style as itcontainshis most paintings, and the latter to help learning the semanticrelevance between poems and paintings. Further, we analyze Paint4Poem regardingpoem diversity, painting style, and the semantic relevance between poems andpaintings. We create abenchmark for Paint4Poem: we train two representativetext-to-image generation models: AttnGAN and MirrorGAN, and evaluatetheirperformance regarding painting pictorial quality, painting stylisticrelevance, and semantic relevance between poems and paintings.The resultsindicate that the models are able to generate paintings that have goodpictorial quality and mimic Feng Zikai's style, but thereflection of poemsemantics is limited. The dataset also poses many interesting researchdirections on this task, including transferlearning, few-shot learning,text-to-image generation for low-resource data etc. The dataset is publiclyavailable.(https://github.com/paint4poem/paint4poem)"
Template-free Prompt Tuning for Few-shot NER,"['Ruotian Ma', 'Xin Zhou', 'Tao Gui', 'Yiding Tan', 'Linyang Li', 'Qi Zhang', 'Xuanjing Huang']",http://arxiv.org/pdf/2109.13532v3.pdf,2021-09-28,"['cs.cl', 'cs.ai']","  Prompt-based methods have been successfully applied in sentence-levelfew-shot learning tasks, mostly owing to the sophisticated design of templatesand label words. However, when applied to token-level labeling tasks such asNER, it would be time-consuming to enumerate the template queries over allpotential entity spans. In this work, we propose a more elegant method toreformulate NER tasks as LM problems without any templates. Specifically, wediscard the template construction process while maintaining the word predictionparadigm of pre-training models to predict a class-related pivot word (or labelword) at the entity position. Meanwhile, we also explore principled ways toautomatically search for appropriate label words that the pre-trained modelscan easily adapt to. While avoiding complicated template-based process, theproposed LM objective also reduces the gap between different objectives used inpre-training and fine-tuning, thus it can better benefit the few-shotperformance. Experimental results demonstrate the effectiveness of the proposedmethod over bert-tagger and template-based method under few-shot setting.Moreover, the decoding speed of the proposed method is up to 1930.12 timesfaster than the template-based method."
RAFT: A Real-World Few-Shot Text Classification Benchmark,"['Neel Alex', 'Eli Lifland', 'Lewis Tunstall', 'Abhishek Thakur', 'Pegah Maham', 'C. Jess Riedel', 'Emmie Hine', 'Carolyn Ashurst', 'Paul Sedille', 'Alexis Carlier', 'Michael Noetel', 'Andreas Stuhlmüller']",http://arxiv.org/pdf/2109.14076v3.pdf,2021-09-28,"['cs.cl', 'cs.ai', 'cs.lg']","  Large pre-trained language models have shown promise for few-shot learning,completing text-based tasks given only a few task-specific examples. Willmodels soon solve classification tasks that have so far been reserved for humanresearch assistants? Existing benchmarks are not designed to measure progressin applied settings, and so don't directly answer this question. The RAFTbenchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurringtasks and uses an evaluation setup that mirrors deployment. Baselineevaluations on RAFT reveal areas current techniques struggle with: reasoningover long texts and tasks with many classes. Human baselines show that someclassification tasks are difficult for non-expert humans, reflecting thatreal-world value sometimes depends on domain expertise. Yet even non-experthuman baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasetsand leaderboard will track which model improvements translate into real-worldbenefits at https://raft.elicit.org ."
Sparse MoEs meet Efficient Ensembles,"['James Urquhart Allingham', 'Florian Wenzel', 'Zelda E Mariet', 'Basil Mustafa', 'Joan Puigcerver', 'Neil Houlsby', 'Ghassen Jerfel', 'Vincent Fortuin', 'Balaji Lakshminarayanan', 'Jasper Snoek', 'Dustin Tran', 'Carlos Riquelme Ruiz', 'Rodolphe Jenatton']",http://arxiv.org/pdf/2110.03360v2.pdf,2021-10-07,"['cs.lg', 'cs.cv', 'stat.ml']","  Machine learning models based on the aggregated outputs of submodels, eitherat the activation or prediction levels, often exhibit strong performancecompared to individual models. We study the interplay of two popular classes ofsuch models: ensembles of neural networks and sparse mixture of experts (sparseMoEs). First, we show that the two approaches have complementary features whosecombination is beneficial. This includes a comprehensive evaluation of sparseMoEs in uncertainty related benchmarks. Then, we present Efficient Ensemble ofExperts (E$^3$), a scalable and simple ensemble of sparse MoEs that takes thebest of both classes of models, while using up to 45% fewer FLOPs than a deepensemble. Extensive experiments demonstrate the accuracy, log-likelihood,few-shot learning, robustness, and uncertainty improvements of E$^3$ overseveral challenging vision Transformer-based baselines. E$^3$ not onlypreserves its efficiency while scaling to models with up to 2.7B parameters,but also provides better predictive performance and uncertainty estimates forlarger models."
Unsupervised Representation Learning Meets Pseudo-Label Supervised  Self-Distillation: A New Approach to Rare Disease Classification,"['Jinghan Sun', 'Dong Wei', 'Kai Ma', 'Liansheng Wang', 'Yefeng Zheng']",http://arxiv.org/pdf/2110.04558v1.pdf,2021-10-09,['cs.cv'],"  Rare diseases are characterized by low prevalence and are often chronicallydebilitating or life-threatening. Imaging-based classification of rare diseasesis challenging due to the severe shortage in training examples. Few-shotlearning (FSL) methods tackle this challenge by extracting generalizable priorknowledge from a large base dataset of common diseases and normal controls, andtransferring the knowledge to rare diseases. Yet, most existing methods requirethe base dataset to be labeled and do not make full use of the preciousexamples of the rare diseases. To this end, we propose in this work a novelhybrid approach to rare disease classification, featuring two key noveltiestargeted at the above drawbacks. First, we adopt the unsupervisedrepresentation learning (URL) based on self-supervising contrastive loss,whereby to eliminate the overhead in labeling the base dataset. Second, weintegrate the URL with pseudo-label supervised classification for effectiveself-distillation of the knowledge about the rare diseases, composing a hybridapproach taking advantages of both unsupervised and (pseudo-) supervisedlearning on the base dataset. Experimental results on classification of rareskin lesions show that our hybrid approach substantially outperforms existingFSL methods (including those using fully supervised base dataset) for raredisease classification via effective integration of the URL and pseudo-labeldriven self-distillation, thus establishing a new state of the art."
A Closer Look at Prototype Classifier for Few-shot Image Classification,"['Mingcheng Hou', 'Issei Sato']",http://arxiv.org/pdf/2110.05076v5.pdf,2021-10-11,"['cs.cv', 'cs.lg']","  The prototypical network is a prototype classifier based on meta-learning andis widely used for few-shot learning because it classifies unseen examples byconstructing class-specific prototypes without adjusting hyper-parametersduring meta-testing. Interestingly, recent research has attracted a lot ofattention, showing that training a new linear classifier, which does not use ameta-learning algorithm, performs comparably with the prototypical network.However, the training of a new linear classifier requires the retraining of theclassifier every time a new class appears. In this paper, we analyze how aprototype classifier works equally well without training a new linearclassifier or meta-learning. We experimentally find that directly using thefeature vectors, which is extracted by using standard pre-trained models toconstruct a prototype classifier in meta-testing, does not perform as well asthe prototypical network and training new linear classifiers on the featurevectors of pre-trained models. Thus, we derive a novel generalization bound fora prototypical classifier and show that the transformation of a feature vectorcan improve the performance of prototype classifiers. We experimentallyinvestigate several normalization methods for minimizing the derived bound andfind that the same performance can be obtained by using the L2 normalizationand minimizing the ratio of the within-class variance to the between-classvariance without training a new classifier or meta-learning."
Scaling Laws for the Few-Shot Adaptation of Pre-trained Image  Classifiers,"['Gabriele Prato', 'Simon Guiroy', 'Ethan Caballero', 'Irina Rish', 'Sarath Chandar']",http://arxiv.org/pdf/2110.06990v2.pdf,2021-10-13,"['cs.lg', 'cs.ai', 'cs.cv']","  Empirical science of neural scaling laws is a rapidly growing area ofsignificant importance to the future of machine learning, particularly in thelight of recent breakthroughs achieved by large-scale pre-trained models suchas GPT-3, CLIP and DALL-e. Accurately predicting the neural network performancewith increasing resources such as data, compute and model size provides a morecomprehensive evaluation of different approaches across multiple scales, asopposed to traditional point-wise comparisons of fixed-size models onfixed-size benchmarks, and, most importantly, allows for focus on thebest-scaling, and thus most promising in the future, approaches. In this work,we consider a challenging problem of few-shot learning in image classification,especially when the target data distribution in the few-shot phase is differentfrom the source, training, data distribution, in a sense that it includes newimage classes not encountered during training. Our current main goal is toinvestigate how the amount of pre-training data affects the few-shotgeneralization performance of standard image classifiers. Our key observationsare that (1) such performance improvements are well-approximated by power laws(linear log-log plots) as the training set size increases, (2) this applies toboth cases of target data coming from either the same or from a differentdomain (i.e., new classes) as the training data, and (3) few-shot performanceon new classes converges at a faster rate than the standard classificationperformance on previously seen classes. Our findings shed new light on therelationship between scale and generalization."
LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based  on Prompt Tuning of T5,"['Chengwei Qin', 'Shafiq Joty']",http://arxiv.org/pdf/2110.07298v3.pdf,2021-10-14,['cs.cl'],"  Existing approaches to lifelong language learning rely on plenty of labeleddata for learning a new task, which is hard to obtain in most real scenarios.Considering that humans can continually learn new tasks from a handful ofexamples, we expect the models also to be able to generalize well on newfew-shot tasks without forgetting the previous ones. In this work, we definethis more challenging yet practical problem as Lifelong Few-shot LanguageLearning (LFLL) and propose a unified framework for it based on prompt tuningof T5. Our framework called LFPT5 takes full advantage of PT's strong few-shotlearning ability, and simultaneously trains the model as a task solver and adata generator. Before learning a new domain of the same task type, LFPT5generates pseudo (labeled) samples of previously learned domains, and latergets trained on those samples to alleviate forgetting of previous knowledge asit learns the new domain. In addition, a KL divergence loss is minimized toachieve label consistency between the previous and the current model. Whileadapting to a new task type, LFPT5 includes and tunes additional promptembeddings for the new task. With extensive experiments, we demonstrate thatLFPT5 can be applied to various different types of tasks and significantlyoutperform previous methods in different LFLL settings."
Inconsistent Few-Shot Relation Classification via Cross-Attentional  Prototype Networks with Contrastive Learning,"['Hongru Wang', 'Zhijing Jin', 'Jiarun Cao', 'Gabriel Pui Cheong Fung', 'Kam-Fai Wong']",http://arxiv.org/pdf/2110.08254v1.pdf,2021-10-13,"['cs.lg', 'cs.cl']","  Standard few-shot relation classification (RC) is designed to learn a robustclassifier with only few labeled data for each class. However, previous worksrarely investigate the effects of a different number of classes (i.e., $N$-way)and number of labeled data per class (i.e., $K$-shot) during training vs.testing. In this work, we define a new task, \textit{inconsistent few-shot RC},where the model needs to handle the inconsistency of $N$ and $K$ betweentraining and testing. To address this new task, we propose PrototypeNetwork-based cross-attention contrastive learning (ProtoCACL) to capture therich mutual interactions between the support set and query set. Experimentalresults demonstrate that our ProtoCACL can outperform the state-of-the-artbaseline model under both inconsistent $K$ and inconsistent $N$ settings, owingto its more robust and discriminate representations. Moreover, we identify thatin the inconsistent few-shot learning setting, models can achieve betterperformance with \textit{less data} than the standard few-shot setting withcarefully-selected $N$ and $K$. In the end of the paper, we provide furtheranalyses and suggestions to systematically guide the selection of $N$ and $K$under different scenarios."
One Representative-Shot Learning Using a Population-Driven Template with  Application to Brain Connectivity Classification and Evolution Prediction,"['Umut Guvercin', 'Mohammed Amine Gharsallaoui', 'Islem Rekik']",http://arxiv.org/pdf/2110.11238v1.pdf,2021-10-06,"['cs.ne', 'cs.cv', 'cs.lg']","  Few-shot learning presents a challenging paradigm for training discriminativemodels on a few training samples representing the target classes todiscriminate. However, classification methods based on deep learning areill-suited for such learning as they need large amounts of training data --letalone one-shot learning. Recently, graph neural networks (GNNs) have beenintroduced to the field of network neuroscience, where the brain connectivityis encoded in a graph. However, with scarce neuroimaging datasets particularlyfor rare diseases and low-resource clinical facilities, such data-devouringarchitectures might fail in learning the target task. In this paper, we take avery different approach in training GNNs, where we aim to learn with one sampleand achieve the best performance --a formidable challenge to tackle.Specifically, we present the first one-shot paradigm where a GNN is trained ona single population-driven template --namely a connectional brain template(CBT). A CBT is a compact representation of a population of brain graphscapturing the unique connectivity patterns shared across individuals. It isanalogous to brain image atlases for neuroimaging datasets. Using aone-representative CBT as a training sample, we alleviate the training load ofGNN models while boosting their performance across a variety of classificationand regression tasks. We demonstrate that our method significantly outperformedbenchmark one-shot learning methods with downstream classification andtime-dependent brain graph data forecasting tasks while competing with thetrain-on-all conventional training strategy. Our source code can be found athttps://github.com/basiralab/one-representative-shot-learning."
Neural View Synthesis and Matching for Semi-Supervised Few-Shot Learning  of 3D Pose,"['Angtian Wang', 'Shenxiao Mei', 'Alan Yuille', 'Adam Kortylewski']",http://arxiv.org/pdf/2110.14213v1.pdf,2021-10-27,['cs.cv'],"  We study the problem of learning to estimate the 3D object pose from a fewlabelled examples and a collection of unlabelled data. Our main contribution isa learning framework, neural view synthesis and matching, that can transfer the3D pose annotation from the labelled to unlabelled images reliably, despiteunseen 3D views and nuisance variations such as the object shape, texture,illumination or scene context. In our approach, objects are represented as 3Dcuboid meshes composed of feature vectors at each mesh vertex. The model isinitialized from a few labelled images and is subsequently used to synthesizefeature representations of unseen 3D views. The synthesized views are matchedwith the feature representations of unlabelled images to generate pseudo-labelsof the 3D pose. The pseudo-labelled data is, in turn, used to train the featureextractor such that the features at each mesh vertex are more invariant acrossvarying 3D views of the object. Our model is trained in an EM-type manneralternating between increasing the 3D pose invariance of the feature extractorand annotating unlabelled data through neural view synthesis and matching. Wedemonstrate the effectiveness of the proposed semi-supervised learningframework for 3D pose estimation on the PASCAL3D+ and KITTI datasets. We findthat our approach outperforms all baselines by a wide margin, particularly inan extreme few-shot setting where only 7 annotated images are given.Remarkably, we observe that our model also achieves an exceptional robustnessin out-of-distribution scenarios that involve partial occlusion."
MetaICL: Learning to Learn In Context,"['Sewon Min', 'Mike Lewis', 'Luke Zettlemoyer', 'Hannaneh Hajishirzi']",http://arxiv.org/pdf/2110.15943v2.pdf,2021-10-29,"['cs.cl', 'cs.ai']","  We introduce MetaICL (Meta-training for In-Context Learning), a newmeta-training framework for few-shot learning where a pretrained language modelis tuned to do in-context learning on a large set of training tasks. Thismeta-training enables the model to more effectively learn a new task in contextat test time, by simply conditioning on a few training examples with noparameter updates or task-specific templates. We experiment on a large, diversecollection of tasks consisting of 142 NLP datasets including classification,question answering, natural language inference, paraphrase detection and more,across seven different meta-training/target splits. MetaICL outperforms a rangeof baselines including in-context learning without meta-training and multi-tasklearning followed by zero-shot transfer. We find that the gains areparticularly significant for target tasks that have domain shifts from themeta-training tasks, and that using a diverse set of the meta-training tasks iskey to improvements. We also show that MetaICL approaches (and sometimes beats)the performance of models fully finetuned on the target task, and outperformsmuch bigger models with nearly 8x parameters. Finally, we show that MetaICL iscomplementary to human-written instructions, and the best performance can beachieved by combining both approaches."
MFNet: Multi-class Few-shot Segmentation Network with Pixel-wise Metric  Learning,"['Miao Zhang', 'Miaojing Shi', 'Li Li']",http://arxiv.org/pdf/2111.00232v4.pdf,2021-10-30,['cs.cv'],"  In visual recognition tasks, few-shot learning requires the ability to learnobject categories with few support examples. Its re-popularity in light of thedeep learning development is mainly in image classification. This work focuseson few-shot semantic segmentation, which is still a largely unexplored field. Afew recent advances are often restricted to single-class few-shot segmentation.In this paper, we first present a novel multi-way (class) encoding and decodingarchitecture which effectively fuses multi-scale query information andmulti-class support information into one query-support embedding. Multi-classsegmentation is directly decoded upon this embedding. For better featurefusion, a multi-level attention mechanism is proposed within the architecture,which includes the attention for support feature modulation and attention formulti-scale combination. Last, to enhance the embedding space learning, anadditional pixel-wise metric learning module is introduced with triplet lossformulated on the pixel-level embedding of the input image. Extensiveexperiments on standard benchmarks PASCAL-5i and COCO-20i show clear benefitsof our method over the state of the art in few-shot segmentation"
Crowdsourcing with Meta-Workers: A New Way to Save the Budget,"['Guangyang Han', 'Guoxian Yu', 'Lizhen Cui', 'Carlotta Domeniconi', 'Xiangliang Zhang']",http://arxiv.org/pdf/2111.04068v1.pdf,2021-11-07,"['cs.lg', 'cs.ai', 'cs.hc']","  Due to the unreliability of Internet workers, it's difficult to complete acrowdsourcing project satisfactorily, especially when the tasks are multipleand the budget is limited. Recently, meta learning has brought new vitality tofew-shot learning, making it possible to obtain a classifier with a fairperformance using only a few training samples. Here we introduce the concept of\emph{meta-worker}, a machine annotator trained by meta learning for types oftasks (i.e., image classification) that are well-fit for AI. Unlike regularcrowd workers, meta-workers can be reliable, stable, and more importantly,tireless and free. We first cluster unlabeled data and ask crowd workers torepeatedly annotate the instances nearby the cluster centers; we then leveragethe annotated data and meta-training datasets to build a cluster ofmeta-workers using different meta learning algorithms. Subsequently,meta-workers are asked to annotate the remaining crowdsourced tasks. TheJensen-Shannon divergence is used to measure the disagreement among theannotations provided by the meta-workers, which determines whether or not crowdworkers should be invited for further annotation of the same task. Finally, wemodel meta-workers' preferences and compute the consensus annotation byweighted majority voting. Our empirical study confirms that, by combiningmachine and human intelligence, we can accomplish a crowdsourcing project witha lower budget than state-of-the-art task assignment methods, while achieving asuperior or comparable quality."
SEGA: Semantic Guided Attention on Visual Prototype for Few-Shot  Learning,"['Fengyuan Yang', 'Ruiping Wang', 'Xilin Chen']",http://arxiv.org/pdf/2111.04316v1.pdf,2021-11-08,['cs.cv'],"  Teaching machines to recognize a new category based on few training samplesespecially only one remains challenging owing to the incomprehensiveunderstanding of the novel category caused by the lack of data. However, humancan learn new classes quickly even given few samples since human can tell whatdiscriminative features should be focused on about each category based on boththe visual and semantic prior knowledge. To better utilize those priorknowledge, we propose the SEmantic Guided Attention (SEGA) mechanism where thesemantic knowledge is used to guide the visual perception in a top-down mannerabout what visual features should be paid attention to when distinguishing acategory from the others. As a result, the embedding of the novel class evenwith few samples can be more discriminative. Concretely, a feature extractor istrained to embed few images of each novel class into a visual prototype withthe help of transferring visual prior knowledge from base classes. Then welearn a network that maps semantic knowledge to category-specific attentionvectors which will be used to perform feature selection to enhance the visualprototypes. Extensive experiments on miniImageNet, tieredImageNet, CIFAR-FS,and CUB indicate that our semantic guided attention realizes anticipatedfunction and outperforms state-of-the-art results."
Scaling ASR Improves Zero and Few Shot Learning,"['Alex Xiao', 'Weiyi Zheng', 'Gil Keren', 'Duc Le', 'Frank Zhang', 'Christian Fuegen', 'Ozlem Kalinli', 'Yatharth Saraf', 'Abdelrahman Mohamed']",http://arxiv.org/pdf/2111.05948v3.pdf,2021-11-10,"['cs.cl', 'cs.sd', 'eess.as']","  With 4.5 million hours of English speech from 10 different sources across 120countries and models of up to 10 billion parameters, we explore the frontiersof scale for automatic speech recognition. We propose data selection techniquesto efficiently scale training data to find the most valuable samples in massivedatasets. To efficiently scale model sizes, we leverage various optimizationssuch as sparse transducer loss and model sharding. By training 1-10B parameteruniversal English ASR models, we push the limits of speech recognitionperformance across many domains. Furthermore, our models learn powerful speechrepresentations with zero and few-shot capabilities on novel domains and stylesof speech, exceeding previous results across multiple in-house and publicbenchmarks. For speakers with disorders due to brain damage, our best zero-shotand few-shot models achieve 22% and 60% relative improvement on the AphasiaBanktest set, respectively, while realizing the best performance on public socialmedia videos. Furthermore, the same universal model reaches equivalentperformance with 500x less in-domain data on the SPGISpeech financial-domaindataset."
Feature Generation for Long-tail Classification,"['Rahul Vigneswaran', 'Marc T. Law', 'Vineeth N. Balasubramanian', 'Makarand Tapaswi']",http://arxiv.org/pdf/2111.05956v1.pdf,2021-11-10,"['cs.cv', 'cs.lg']","  The visual world naturally exhibits an imbalance in the number of object orscene instances resulting in a \emph{long-tailed distribution}. This imbalanceposes significant challenges for classification models based on deep learning.Oversampling instances of the tail classes attempts to solve this imbalance.However, the limited visual diversity results in a network with poorrepresentation ability. A simple counter to this is decoupling therepresentation and classifier networks and using oversampling only to train theclassifier. In this paper, instead of repeatedly re-sampling the same image(and thereby features), we explore a direction that attempts to generatemeaningful features by estimating the tail category's distribution. Inspired byideas from recent work on few-shot learning, we create calibrated distributionsto sample additional features that are subsequently used to train theclassifier. Through several experiments on the CIFAR-100-LT (long-tail) datasetwith varying imbalance factors and on mini-ImageNet-LT (long-tail), we show theefficacy of our approach and establish a new state-of-the-art. We also presenta qualitative analysis of generated features using t-SNE visualizations andanalyze the nearest neighbors used to calibrate the tail class distributions.Our code is available at https://github.com/rahulvigneswaran/TailCalibX."
Deep metric learning improves lab of origin prediction of genetically  engineered plasmids,"['Igor M. Soares', 'Fernando H. F. Camargo', 'Adriano Marques', 'Oliver M. Crook']",http://arxiv.org/pdf/2111.12606v1.pdf,2021-11-24,"['cs.lg', 'cs.ai', 'cs.ne', 'i.5.4; i.2.1']","  Genome engineering is undergoing unprecedented development and is nowbecoming widely available. To ensure responsible biotechnology innovation andto reduce misuse of engineered DNA sequences, it is vital to develop tools toidentify the lab-of-origin of engineered plasmids. Genetic engineeringattribution (GEA), the ability to make sequence-lab associations, would supportforensic experts in this process. Here, we propose a method, based on metriclearning, that ranks the most likely labs-of-origin whilst simultaneouslygenerating embeddings for plasmid sequences and labs. These embeddings can beused to perform various downstream tasks, such as clustering DNA sequences andlabs, as well as using them as features in machine learning models. Ourapproach employs a circular shift augmentation approach and is able tocorrectly rank the lab-of-origin $90\%$ of the time within its top 10predictions - outperforming all current state-of-the-art approaches. We alsodemonstrate that we can perform few-shot-learning and obtain $76\%$ top-10accuracy using only $10\%$ of the sequences. This means, we outperform theprevious CNN approach using only one-tenth of the data. We also demonstratethat we are able to extract key signatures in plasmid sequences for particularlabs, allowing for an interpretable examination of the model's outputs."
"PartImageNet: A Large, High-Quality Dataset of Parts","['Ju He', 'Shuo Yang', 'Shaokang Yang', 'Adam Kortylewski', 'Xiaoding Yuan', 'Jie-Neng Chen', 'Shuai Liu', 'Cheng Yang', 'Qihang Yu', 'Alan Yuille']",http://arxiv.org/pdf/2112.00933v3.pdf,2021-12-02,['cs.cv'],"  It is natural to represent objects in terms of their parts. This has thepotential to improve the performance of algorithms for object recognition andsegmentation but can also help for downstream tasks like activity recognition.Research on part-based models, however, is hindered by the lack of datasetswith per-pixel part annotations. This is partly due to the difficulty and highcost of annotating object parts so it has rarely been done except for humans(where there exists a big literature on part-based models). To help addressthis problem, we propose PartImageNet, a large, high-quality dataset with partsegmentation annotations. It consists of $158$ classes from ImageNet withapproximately $24,000$ images. PartImageNet is unique because it offerspart-level annotations on a general set of classes including non-rigid,articulated objects, while having an order of magnitude larger size compared toexisting part datasets (excluding datasets of humans). It can be utilized formany vision tasks including Object Segmentation, Semantic Part Segmentation,Few-shot Learning and Part Discovery. We conduct comprehensive experimentswhich study these tasks and set up a set of baselines. The dataset and scriptsare released at https://github.com/TACJu/PartImageNet."
A Survey: Deep Learning for Hyperspectral Image Classification with Few  Labeled Samples,"['Sen Jia', 'Shuguo Jiang', 'Zhijie Lin', 'Nanying Li', 'Meng Xu', 'Shiqi Yu']",http://arxiv.org/pdf/2112.01800v1.pdf,2021-12-03,"['cs.cv', 'cs.ai', 'eess.iv']","  With the rapid development of deep learning technology and improvement incomputing capability, deep learning has been widely used in the field ofhyperspectral image (HSI) classification. In general, deep learning modelsoften contain many trainable parameters and require a massive number of labeledsamples to achieve optimal performance. However, in regard to HSIclassification, a large number of labeled samples is generally difficult toacquire due to the difficulty and time-consuming nature of manual labeling.Therefore, many research works focus on building a deep learning model for HSIclassification with few labeled samples. In this article, we concentrate onthis topic and provide a systematic review of the relevant literature.Specifically, the contributions of this paper are twofold. First, the researchprogress of related methods is categorized according to the learning paradigm,including transfer learning, active learning and few-shot learning. Second, anumber of experiments with various state-of-the-art approaches has been carriedout, and the results are summarized to reveal the potential researchdirections. More importantly, it is notable that although there is a vast gapbetween deep learning models (that usually need sufficient labeled samples) andthe HSI scenario with few labeled samples, the issues of small-sample sets canbe well characterized by fusion of deep learning methods and relatedtechniques, such as transfer learning and a lightweight model. Forreproducibility, the source codes of the methods assessed in the paper can befound at https://github.com/ShuGuoJ/HSI-Classification.git."
PointCLIP: Point Cloud Understanding by CLIP,"['Renrui Zhang', 'Ziyu Guo', 'Wei Zhang', 'Kunchang Li', 'Xupeng Miao', 'Bin Cui', 'Yu Qiao', 'Peng Gao', 'Hongsheng Li']",http://arxiv.org/pdf/2112.02413v1.pdf,2021-12-04,"['cs.cv', 'cs.ai', 'cs.ro']","  Recently, zero-shot and few-shot learning via Contrastive Vision-LanguagePre-training (CLIP) have shown inspirational performance on 2D visualrecognition, which learns to match images with their corresponding texts inopen-vocabulary settings. However, it remains under explored that whether CLIP,pre-trained by large-scale image-text pairs in 2D, can be generalized to 3Drecognition. In this paper, we identify such a setting is feasible by proposingPointCLIP, which conducts alignment between CLIP-encoded point cloud and 3Dcategory texts. Specifically, we encode a point cloud by projecting it intomulti-view depth maps without rendering, and aggregate the view-wise zero-shotprediction to achieve knowledge transfer from 2D to 3D. On top of that, wedesign an inter-view adapter to better extract the global feature andadaptively fuse the few-shot knowledge learned from 3D into CLIP pre-trained in2D. By just fine-tuning the lightweight adapter in the few-shot settings, theperformance of PointCLIP could be largely improved. In addition, we observe thecomplementary property between PointCLIP and classical 3D-supervised networks.By simple ensembling, PointCLIP boosts baseline's performance and evensurpasses state-of-the-art models. Therefore, PointCLIP is a promisingalternative for effective 3D point cloud understanding via CLIP under lowresource cost and data regime. We conduct thorough experiments onwidely-adopted ModelNet10, ModelNet40 and the challenging ScanObjectNN todemonstrate the effectiveness of PointCLIP. The code is released athttps://github.com/ZrrSkywalker/PointCLIP."
A Survey of Deep Learning for Low-Shot Object Detection,"['Qihan Huang', 'Haofei Zhang', 'Mengqi Xue', 'Jie Song', 'Mingli Song']",http://arxiv.org/pdf/2112.02814v4.pdf,2021-12-06,"['cs.cv', 'cs.ai']","  Object detection has achieved a huge breakthrough with deep neural networksand massive annotated data. However, current detection methods cannot bedirectly transferred to the scenario where the annotated data is scarce due tothe severe overfitting problem. Although few-shot learning and zero-shotlearning have been extensively explored in the field of image classification,it is indispensable to design new methods for object detection in thedata-scarce scenario since object detection has an additional challenginglocalization task. Low-Shot Object Detection (LSOD) is an emerging researchtopic of detecting objects from a few or even no annotated samples, consistingof One-Shot Object Detection (OSOD), Few-Shot Object Detection (FSOD) andZero-Shot Object Detection (ZSD). This survey provides a comprehensive reviewof LSOD methods. First, we propose a thorough taxonomy of LSOD methods andanalyze them systematically, comprising some extensional topics of LSOD(semi-supervised LSOD, weakly-supervised LSOD, and incremental LSOD). Then, weindicate the pros and cons of current LSOD methods with a comparison of theirperformance. Finally, we discuss the challenges and promising directions ofLSOD to provide guidance for future works."
DemoGrasp: Few-Shot Learning for Robotic Grasping with Human  Demonstration,"['Pengyuan Wang', 'Fabian Manhardt', 'Luca Minciullo', 'Lorenzo Garattoni', 'Sven Meie', 'Nassir Navab', 'Benjamin Busam']",http://arxiv.org/pdf/2112.02849v1.pdf,2021-12-06,"['cs.ro', 'cs.cv']","  The ability to successfully grasp objects is crucial in robotics, as itenables several interactive downstream applications. To this end, mostapproaches either compute the full 6D pose for the object of interest or learnto predict a set of grasping points. While the former approaches do not scalewell to multiple object instances or classes yet, the latter require largeannotated datasets and are hampered by their poor generalization capabilitiesto new geometries. To overcome these shortcomings, we propose to teach a robothow to grasp an object with a simple and short human demonstration. Hence, ourapproach neither requires many annotated images nor is it restricted to aspecific geometry. We first present a small sequence of RGB-D images displayinga human-object interaction. This sequence is then leveraged to build associatedhand and object meshes that represent the depicted interaction. Subsequently,we complete missing parts of the reconstructed object shape and estimate therelative transformation between the reconstruction and the visible object inthe scene. Finally, we transfer the a-priori knowledge from the relative posebetween object and human hand with the estimate of the current object pose inthe scene into necessary grasping instructions for the robot. Exhaustiveevaluations with Toyota's Human Support Robot (HSR) in real and syntheticenvironments demonstrate the applicability of our proposed methodology and itsadvantage in comparison to previous approaches."
Label Hallucination for Few-Shot Classification,"['Yiren Jian', 'Lorenzo Torresani']",http://arxiv.org/pdf/2112.03340v1.pdf,2021-12-06,"['cs.cv', 'cs.lg']","  Few-shot classification requires adapting knowledge learned from a largeannotated base dataset to recognize novel unseen classes, each represented byfew labeled examples. In such a scenario, pretraining a network with highcapacity on the large dataset and then finetuning it on the few examples causessevere overfitting. At the same time, training a simple linear classifier ontop of ""frozen"" features learned from the large labeled dataset fails to adaptthe model to the properties of the novel classes, effectively inducingunderfitting. In this paper we propose an alternative approach to both of thesetwo popular strategies. First, our method pseudo-labels the entire largedataset using the linear classifier trained on the novel classes. Thiseffectively ""hallucinates"" the novel classes in the large dataset, despite thenovel categories not being present in the base database (novel and base classesare disjoint). Then, it finetunes the entire model with a distillation loss onthe pseudo-labeled base examples, in addition to the standard cross-entropyloss on the novel dataset. This step effectively trains the network torecognize contextual and appearance cues that are useful for the novel-categoryrecognition but using the entire large-scale base dataset and thus overcomingthe inherent data-scarcity problem of few-shot learning. Despite the simplicityof the approach, we show that that our method outperforms the state-of-the-arton four well-established few-shot classification benchmarks."
Anomaly Crossing: New Horizons for Video Anomaly Detection as  Cross-domain Few-shot Learning,"['Guangyu Sun', 'Zhang Liu', 'Lianggong Wen', 'Jing Shi', 'Chenliang Xu']",http://arxiv.org/pdf/2112.06320v3.pdf,2021-12-12,['cs.cv'],"  Video anomaly detection aims to identify abnormal events that occurred invideos. Since anomalous events are relatively rare, it is not feasible tocollect a balanced dataset and train a binary classifier to solve the task.Thus, most previous approaches learn only from normal videos using unsupervisedor semi-supervised methods. Obviously, they are limited in capturing andutilizing discriminative abnormal characteristics, which leads to compromisedanomaly detection performance. In this paper, to address this issue, we proposea new learning paradigm by making full use of both normal and abnormal videosfor video anomaly detection. In particular, we formulate a new learning task:cross-domain few-shot anomaly detection, which can transfer knowledge learnedfrom numerous videos in the source domain to help solve few-shot abnormalitydetection in the target domain. Concretely, we leverage self-supervisedtraining on the target normal videos to reduce the domain gap and devise a metacontext perception module to explore the video context of the event in thefew-shot setting. Our experiments show that our method significantlyoutperforms baseline methods on DoTA and UCF-Crime datasets, and the new taskcontributes to a more practical training paradigm for anomaly detection."
Hierarchical Variational Memory for Few-shot Learning Across Domains,"['Yingjun Du', 'Xiantong Zhen', 'Ling Shao', 'Cees G. M. Snoek']",http://arxiv.org/pdf/2112.08181v2.pdf,2021-12-15,['cs.lg'],"  Neural memory enables fast adaptation to new tasks with just a few trainingsamples. Existing memory models store features only from the single last layer,which does not generalize well in presence of a domain shift between trainingand test distributions. Rather than relying on a flat memory, we propose ahierarchical alternative that stores features at different semantic levels. Weintroduce a hierarchical prototype model, where each level of the prototypefetches corresponding information from the hierarchical memory. The model isendowed with the ability to flexibly rely on features at different semanticlevels if the domain shift circumstances so demand. We meta-learn the model bya newly derived hierarchical variational inference framework, wherehierarchical memory and prototypes are jointly optimized. To explore andexploit the importance of different semantic levels, we further propose tolearn the weights associated with the prototype at each level in a data-drivenway, which enables the model to adaptively choose the most generalizablefeatures. We conduct thorough ablation studies to demonstrate the effectivenessof each component in our model. The new state-of-the-art performance oncross-domain and competitive performance on traditional few-shot classificationfurther substantiates the benefit of hierarchical variational memory."
DA-FDFtNet: Dual Attention Fake Detection Fine-tuning Network to Detect  Various AI-Generated Fake Images,"['Young Oh Bang', 'Simon S. Woo']",http://arxiv.org/pdf/2112.12001v1.pdf,2021-12-22,['cs.cv'],"  Due to the advancement of Generative Adversarial Networks (GAN),Autoencoders, and other AI technologies, it has been much easier to create fakeimages such as ""Deepfakes"". More recent research has introduced few-shotlearning, which uses a small amount of training data to produce fake images andvideos more effectively. Therefore, the ease of generating manipulated imagesand the difficulty of distinguishing those images can cause a serious threat toour society, such as propagating fake information. However, detecting realisticfake images generated by the latest AI technology is challenging due to thereasons mentioned above. In this work, we propose Dual Attention Fake DetectionFine-tuning Network (DA-FDFtNet) to detect the manipulated fake face imagesfrom the real face data. Our DA-FDFtNet integrates the pre-trained model withFine-Tune Transformer, MBblockV3, and a channel attention module to improve theperformance and robustness across different types of fake images. Inparticular, Fine-Tune Transformer consists of multiple numbers of animage-based self-attention module and a down-sampling layer. The channelattention module is also connected with the pre-trained model to capture thefake images feature space. We experiment with our DA-FDFtNet with theFaceForensics++ dataset and various GAN-generated datasets, and we show thatour approach outperforms the previous baseline models."
Few-Shot Classification in Unseen Domains by Episodic Meta-Learning  Across Visual Domains,"['Yuan-Chia Cheng', 'Ci-Siang Lin', 'Fu-En Yang', 'Yu-Chiang Frank Wang']",http://arxiv.org/pdf/2112.13539v1.pdf,2021-12-27,['cs.cv'],"  Few-shot classification aims to carry out classification given only fewlabeled examples for the categories of interest. Though several approaches havebeen proposed, most existing few-shot learning (FSL) models assume that baseand novel classes are drawn from the same data domain. When it comes torecognizing novel-class data in an unseen domain, this becomes an even morechallenging task of domain generalized few-shot classification. In this paper,we present a unique learning framework for domain-generalized few-shotclassification, where base classes are from homogeneous multiple sourcedomains, while novel classes to be recognized are from target domains which arenot seen during training. By advancing meta-learning strategies, our learningframework exploits data across multiple source domains to capturedomain-invariant features, with FSL ability introduced by metric-learning basedmechanisms across support and query data. We conduct extensive experiments toverify the effectiveness of our proposed learning framework and show learningfrom small yet homogeneous source data is able to perform preferably againstlearning from large-scale one. Moreover, we provide insights into choices ofbackbone models for domain-generalized few-shot classification."
Recursive Least-Squares Estimator-Aided Online Learning for Visual  Tracking,"['Jin Gao', 'Yan Lu', 'Xiaojuan Qi', 'Yutong Kou', 'Bing Li', 'Liang Li', 'Shan Yu', 'Weiming Hu']",http://arxiv.org/pdf/2112.14016v2.pdf,2021-12-28,['cs.cv'],"  Tracking visual objects from a single initial exemplar in the testing phasehas been broadly cast as a one-/few-shot problem, i.e., one-shot learning forinitial adaptation and few-shot learning for online adaptation. The recentfew-shot online adaptation methods incorporate the prior knowledge from largeamounts of annotated training data via complex meta-learning optimization inthe offline phase. This helps the online deep trackers to achieve fastadaptation and reduce overfitting risk in tracking. In this paper, we propose asimple yet effective recursive least-squares estimator-aided online learningapproach for few-shot online adaptation without requiring offline training. Itallows an in-built memory retention mechanism for the model to remember theknowledge about the object seen before, and thus the seen data can be safelyremoved from training. This also bears certain similarities to the emergingcontinual learning field in preventing catastrophic forgetting. This mechanismenables us to unveil the power of modern online deep trackers without incurringtoo much extra computational cost. We evaluate our approach based on twonetworks in the online learning families for tracking, i.e., multi-layerperceptrons in RT-MDNet and convolutional neural networks in DiMP. Theconsistent improvements on several challenging tracking benchmarks demonstrateits effectiveness and efficiency."
ReWiS: Reliable Wi-Fi Sensing Through Few-Shot Multi-Antenna  Multi-Receiver CSI Learning,"['Niloofar Bahadori', 'Jonathan Ashdown', 'Francesco Restuccia']",http://arxiv.org/pdf/2201.00869v2.pdf,2022-01-03,"['cs.ni', 'eess.sp']","  Thanks to the ubiquitousness of Wi-Fi access points and devices, Wi-Fisensing enables transformative applications in remote health care, security,and surveillance. Existing work has explored the usage of machine learning onchannel state information (CSI) computed from Wi-Fi packets to classify eventsof interest. However, most of these algorithms require a significant amount ofdata collection, as well as extensive computational power for additional CSIfeature extraction. Moreover, the majority of these models suffer from pooraccuracy when tested in a new/untrained environment. In this paper, we proposeReWiS, a novel framework for robust and environment-independent Wi-Fi sensing.The key innovation of ReWiS is to leverage few-shot learning (FSL) as theinference engine, which (i) reduces the need for extensive data collection andapplication-specific feature extraction; (ii) can rapidly generalize to newtasks by leveraging only a few new samples. We prototype ReWiS usingoff-the-shelf Wi-Fi equipment and showcase its performance by considering acompelling use case of human activity recognition. Thus, we perform anextensive data collection campaign in three different propagation environmentswith two human subjects. We evaluate the impact of each diversity component onthe performance and compare ReWiS with a traditional convolutional neuralnetwork (CNN) approach. Experimental results show that ReWiS improves theperformance by about 40% with respect to existing single-antenna low-resolutionapproaches. Moreover, when compared to a CNN-based approach, ReWiS shows a 35%more accuracy and less than 10% drop in accuracy when tested in differentenvironments, while the CNN drops by more than 45%."
Resolving Camera Position for a Practical Application of Gaze Estimation  on Edge Devices,"['Linh Van Ma', 'Tin Trung Tran', 'Moongu Jeon']",http://arxiv.org/pdf/2201.02946v2.pdf,2022-01-09,"['cs.cv', 'cs.ai']","  Most Gaze estimation research only works on a setup condition that a cameraperfectly captures eyes gaze. They have not literarily specified how to set upa camera correctly for a given position of a person. In this paper, we carryout a study on gaze estimation with a logical camera setup position. We furtherbring our research in a practical application by using inexpensive edge deviceswith a realistic scenario. That is, we first set up a shopping environmentwhere we want to grasp customers gazing behaviors. This setup needs an optimalcamera position in order to maintain estimation accuracy from existing gazeestimation research. We then apply the state-of-the-art of few-shot learninggaze estimation to reduce training sampling in the inference phase. In theexperiment, we perform our implemented research on NVIDIA Jetson TX2 andachieve a reasonable speed, 12 FPS which is faster compared with our referencework, without much degradation of gaze estimation accuracy. The source code isreleased at https://github.com/linh-gist/GazeEstimationTX2."
Towards Sample-efficient Overparameterized Meta-learning,"['Yue Sun', 'Adhyyan Narang', 'Halil Ibrahim Gulluk', 'Samet Oymak', 'Maryam Fazel']",http://arxiv.org/pdf/2201.06142v1.pdf,2022-01-16,"['cs.lg', 'stat.ml']","  An overarching goal in machine learning is to build a generalizable modelwith few samples. To this end, overparameterization has been the subject ofimmense interest to explain the generalization ability of deep nets even whenthe size of the dataset is smaller than that of the model. While the priorliterature focuses on the classical supervised setting, this paper aims todemystify overparameterization for meta-learning. Here we have a sequence oflinear-regression tasks and we ask: (1) Given earlier tasks, what is theoptimal linear representation of features for a new downstream task? and (2)How many samples do we need to build this representation? This work shows thatsurprisingly, overparameterization arises as a natural answer to thesefundamental meta-learning questions. Specifically, for (1), we first show thatlearning the optimal representation coincides with the problem of designing atask-aware regularization to promote inductive bias. We leverage this inductivebias to explain how the downstream task actually benefits fromoverparameterization, in contrast to prior works on few-shot learning. For (2),we develop a theory to explain how feature covariance can implicitly helpreduce the sample complexity well below the degrees of freedom and lead tosmall estimation error. We then integrate these findings to obtain an overallperformance guarantee for our meta-learning algorithm. Numerical experiments onreal and synthetic data verify our insights on overparameterized meta-learning."
Deep-SWIM: A few-shot learning approach to classify Solar WInd Magnetic  field structures,"['Hala Lamdouar', 'Sairam Sundaresan', 'Anna Jungbluth', 'Sudeshna Boro Saikia', 'Amanda Joy Camarata', 'Nathan Miles', 'Marcella Scoczynski', 'Mavis Stone', 'Anthony Sarah', 'Andrés Muñoz-Jaramillo', 'Ayris Narock', 'Adam Szabo']",http://arxiv.org/pdf/2203.01184v1.pdf,2022-03-02,"['astro-ph.sr', 'physics.space-ph']","  The solar wind consists of charged particles ejected from the Sun intointerplanetary space and towards Earth. Understanding the magnetic field of thesolar wind is crucial for predicting future space weather and planetaryatmospheric loss. Compared to large-scale magnetic events, smaller-scalestructures like magnetic discontinuities are hard to detect but entailimportant information on the evolution of the solar wind. A lack of labeleddata makes an automated detection of these discontinuities challenging. Wepropose Deep-SWIM, an approach leveraging advances in contrastive learning,pseudo-labeling and online hard example mining to robustly identifydiscontinuities in solar wind magnetic field data. Through a systematicablation study, we show that we can accurately classify discontinuities despitelearning from only limited labeled data. Additionally, we show that ourapproach generalizes well and produces results that agree with experthand-labeling."
"Vision-Language Intelligence: Tasks, Representation Learning, and Large  Models","['Feng Li', 'Hao Zhang', 'Yi-Fan Zhang', 'Shilong Liu', 'Jian Guo', 'Lionel M. Ni', 'PengChuan Zhang', 'Lei Zhang']",http://arxiv.org/pdf/2203.01922v1.pdf,2022-03-03,"['cs.cv', 'cs.ai', 'cs.cl']","  This paper presents a comprehensive survey of vision-language (VL)intelligence from the perspective of time. This survey is inspired by theremarkable progress in both computer vision and natural language processing,and recent trends shifting from single modality processing to multiple modalitycomprehension. We summarize the development in this field into three timeperiods, namely task-specific methods, vision-language pre-training (VLP)methods, and larger models empowered by large-scale weakly-labeled data. Wefirst take some common VL tasks as examples to introduce the development oftask-specific methods. Then we focus on VLP methods and comprehensively reviewkey components of the model structures and training methods. After that, weshow how recent work utilizes large-scale raw image-text data to learnlanguage-aligned visual representations that generalize better on zero or fewshot learning tasks. Finally, we discuss some potential future trends towardsmodality cooperation, unified representation, and knowledge incorporation. Webelieve that this review will be of help for researchers and practitioners ofAI and ML, especially those interested in computer vision and natural languageprocessing."
Anomaly Detection-Inspired Few-Shot Medical Image Segmentation Through  Self-Supervision With Supervoxels,"['Stine Hansen', 'Srishti Gautam', 'Robert Jenssen', 'Michael Kampffmeyer']",http://arxiv.org/pdf/2203.02048v1.pdf,2022-03-03,"['eess.iv', 'cs.cv']","  Recent work has shown that label-efficient few-shot learning throughself-supervision can achieve promising medical image segmentation results.However, few-shot segmentation models typically rely on prototyperepresentations of the semantic classes, resulting in a loss of localinformation that can degrade performance. This is particularly problematic forthe typically large and highly heterogeneous background class in medical imagesegmentation problems. Previous works have attempted to address this issue bylearning additional prototypes for each class, but since the prototypes arebased on a limited number of slices, we argue that this ad-hoc solution isinsufficient to capture the background properties. Motivated by this, and theobservation that the foreground class (e.g., one organ) is relativelyhomogeneous, we propose a novel anomaly detection-inspired approach to few-shotmedical image segmentation in which we refrain from modeling the backgroundexplicitly. Instead, we rely solely on a single foreground prototype to computeanomaly scores for all query pixels. The segmentation is then performed bythresholding these anomaly scores using a learned threshold. Assisted by anovel self-supervision task that exploits the 3D structure of medical imagesthrough supervoxels, our proposed anomaly detection-inspired few-shot medicalimage segmentation model outperforms previous state-of-the-art approaches ontwo representative MRI datasets for the tasks of abdominal organ segmentationand cardiac segmentation."
Rethinking Task Sampling for Few-shot Vision-Language Transfer Learning,"['Zhenhailong Wang', 'Hang Yu', 'Manling Li', 'Han Zhao', 'Heng Ji']",http://arxiv.org/pdf/2203.04904v3.pdf,2022-03-09,"['cs.mm', 'cs.cl', 'cs.cv']","  Despite achieving state-of-the-art zero-shot performance, existingvision-language models still fall short of few-shot transfer ability ondomain-specific problems. Classical fine-tuning often fails to prevent highlyexpressive models from exploiting spurious correlations. Althoughmodel-agnostic meta-learning (MAML) presents as a natural alternative forfew-shot transfer learning, the expensive computation due to implicitsecond-order optimization limits its use on large-scale vision-language modelssuch as CLIP. While much literature has been devoted to exploring alternativeoptimization strategies, we identify another essential aspect towards effectivefew-shot transfer learning, task sampling, which is previously only be viewedas part of data pre-processing in MAML. To show the impact of task sampling, wepropose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), whichdifferentiates classical fine-tuning only on uniformly sampling multiple tasks.Despite its simplicity, we show that MAMF consistently outperforms classicalfine-tuning on five few-shot vision-language classification tasks. We furthershow that the effectiveness of the bi-level optimization in MAML is highlysensitive to the zero-shot performance of a task in the context of few-shotvision-language classification. The goal of this paper is to provide newinsights on what makes few-shot learning work, and encourage more research intoinvestigating better task sampling strategies."
Partitioning Image Representation in Contrastive Learning,"['Hyunsub Lee', 'Heeyoul Choi']",http://arxiv.org/pdf/2203.10454v3.pdf,2022-03-20,"['cs.cv', 'cs.ai']","  In contrastive learning in the image domain, the anchor and positive samplesare forced to have as close representations as possible. However, forcing thetwo samples to have the same representation could be misleading because thedata augmentation techniques make the two samples different. In this paper, weintroduce a new representation, partitioned representation, which can learnboth common and unique features of the anchor and positive samples incontrastive learning. The partitioned representation consists of two parts: thecontent part and the style part. The content part represents common features ofthe class, and the style part represents the own features of each sample, whichcan lead to the representation of the data augmentation method. We can achievethe partitioned representation simply by decomposing a loss function ofcontrastive learning into two terms on the two separate representations,respectively. To evaluate our representation with two parts, we take twoframework models: Variational AutoEncoder (VAE) and BootstrapYour OwnLatent(BYOL) to show the separability of content and style, and to confirm thegeneralization ability in classification, respectively. Based on theexperiments, we show that our approach can separate two types of information inthe VAE framework and outperforms the conventional BYOL in linear separabilityand a few-shot learning task as downstream tasks."
Enabling hand gesture customization on wrist-worn devices,"['Xuhai Xu', 'Jun Gong', 'Carolina Brum', 'Lilian Liang', 'Bongsoo Suh', 'Kumar Gupta', 'Yash Agarwal', 'Laurence Lindsey', 'Runchang Kang', 'Behrooz Shahsavari', 'Tu Nguyen', 'Heriberto Nieto', 'Scott E. Hudson', 'Charlie Maalouf', 'Seyed Mousavi', 'Gierad Laput']",http://arxiv.org/pdf/2203.15239v2.pdf,2022-03-29,"['cs.hc', 'cs.ai', '68u35', 'h.5.2; i.2.m']","  We present a framework for gesture customization requiring minimal examplesfrom users, all without degrading the performance of existing gesture sets. Toachieve this, we first deployed a large-scale study (N=500+) to collect dataand train an accelerometer-gyroscope recognition model with a cross-useraccuracy of 95.7% and a false-positive rate of 0.6 per hour when tested oneveryday non-gesture data. Next, we design a few-shot learning framework whichderives a lightweight model from our pre-trained model, enabling knowledgetransfer without performance degradation. We validate our approach through auser study (N=20) examining on-device customization from 12 new gestures,resulting in an average accuracy of 55.3%, 83.1%, and 87.2% on using one,three, or five shots when adding a new gesture, while maintaining the samerecognition accuracy and false-positive rate from the pre-existing gesture set.We further evaluate the usability of our real-time implementation with a userexperience study (N=20). Our results highlight the effectiveness, learnability,and usability of our customization framework. Our approach paves the way for afuture where users are no longer bound to pre-existing gestures, freeing themto creatively introduce new gestures tailored to their preferences andabilities."
BankNote-Net: Open dataset for assistive universal currency recognition,"['Felipe Oviedo', 'Srinivas Vinnakota', 'Eugene Seleznev', 'Hemant Malhotra', 'Saqib Shaikh', 'Juan Lavista Ferres']",http://arxiv.org/pdf/2204.03738v1.pdf,2022-04-07,"['cs.cv', 'cs.hc', 'cs.lg']","  Millions of people around the world have low or no vision. Assistive softwareapplications have been developed for a variety of day-to-day tasks, includingoptical character recognition, scene identification, person recognition, andcurrency recognition. This last task, the recognition of banknotes fromdifferent denominations, has been addressed by the use of computer visionmodels for image recognition. However, the datasets and models available forthis task are limited, both in terms of dataset size and in variety ofcurrencies covered. In this work, we collect a total of 24,826 images ofbanknotes in variety of assistive settings, spanning 17 currencies and 112denominations. Using supervised contrastive learning, we develop a machinelearning model for universal currency recognition. This model learns compliantembeddings of banknote images in a variety of contexts, which can be sharedpublicly (as a compressed vector representation), and can be used to train andtest specialized downstream models for any currency, including those notcovered by our dataset or for which only a few real images per denomination areavailable (few-shot learning). We deploy a variation of this model for publicuse in the last version of the Seeing AI app developed by Microsoft. We shareour encoder model and the embeddings as an open dataset in our BankNote-Netrepository."
Evaluating few shot and Contrastive learning Methods for Code Clone  Detection,"['Mohamad Khajezade', 'Fatemeh Hendijani Fard', 'Mohamed S. Shehata']",http://arxiv.org/pdf/2204.07501v3.pdf,2022-04-15,['cs.se'],"  Context: Code Clone Detection (CCD) is a software engineering task that isused for plagiarism detection, code search, and code comprehension. Recently,deep learning-based models have achieved an F1 score (a metric used to assessclassifiers) of $\sim$95\% on the CodeXGLUE benchmark. These models requiremany training data, mainly fine-tuned on Java or C++ datasets. However, noprevious study evaluates the generalizability of these models where a limitedamount of annotated data is available.  Objective: The main objective of this research is to assess the ability ofthe CCD models as well as few shot learning algorithms for unseen programmingproblems and new languages (i.e., the model is not trained on theseproblems/languages).  Method: We assess the generalizability of the state of the art models for CCDin few shot settings (i.e., only a few samples are available for fine-tuning)by setting three scenarios: i) unseen problems, ii) unseen languages, iii)combination of new languages and new problems. We choose three datasets ofBigCloneBench, POJ-104, and CodeNet and Java, C++, and Ruby languages. Then, weemploy Model Agnostic Meta-learning (MAML), where the model learns ameta-learner capable of extracting transferable knowledge from the train set;so that the model can be fine-tuned using a few samples. Finally, we combinecontrastive learning with MAML to further study whether it can improve theresults of MAML."
mGPT: Few-Shot Learners Go Multilingual,"['Oleh Shliazhko', 'Alena Fenogenova', 'Maria Tikhonova', 'Vladislav Mikhailov', 'Anastasia Kozlova', 'Tatiana Shavrina']",http://arxiv.org/pdf/2204.07580v2.pdf,2022-04-15,"['cs.cl', 'cs.ai', '68-06, 68-04, 68t50, 68t01', 'i.2; i.2.7']","  Recent studies report that autoregressive language models can successfullysolve many NLP tasks via zero- and few-shot learning paradigms, which opens upnew possibilities for using the pre-trained language models. This paperintroduces two autoregressive GPT-like models with 1.3 billion and 13 billionparameters trained on 60 languages from 25 language families using Wikipediaand Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture usingGPT-2 sources and the sparse attention mechanism; Deepspeed and Megatronframeworks allow us to parallelize the training and inference stepseffectively. The resulting models show performance on par with the recentlyreleased XGLM models by Facebook, covering more languages and enhancing NLPpossibilities for low resource languages of CIS countries and Russian smallnations. We detail the motivation for the choices of the architecture design,thoroughly describe the data preparation pipeline, and train five smallversions of the model to choose the most optimal multilingual tokenizationstrategy. We measure the model perplexity in all covered languages and evaluateit on the wide spectre of multilingual tasks, including classification,generative, sequence labeling and knowledge probing. The models were evaluatedwith the zero-shot and few-shot methods. Furthermore, we compared theclassification tasks with the state-of-the-art multilingual model XGLM. sourcecode and the mGPT XL model are publicly released."
In-BoXBART: Get Instructions into Biomedical Multi-Task Learning,"['Mihir Parmar', 'Swaroop Mishra', 'Mirali Purohit', 'Man Luo', 'M. Hassan Murad', 'Chitta Baral']",http://arxiv.org/pdf/2204.07600v1.pdf,2022-04-15,['cs.cl'],"  Single-task models have proven pivotal in solving specific tasks; however,they have limitations in real-world applications where multi-tasking isnecessary and domain shifts are exhibited. Recently, instructional prompts haveshown significant improvement towards multi-task generalization; however, theeffect of instructional prompts and Multi-Task Learning (MTL) has not beensystematically studied in the biomedical domain. Motivated by this, this paperexplores the impact of instructional prompts for biomedical MTL. We introducethe BoX, a collection of 32 instruction tasks for Biomedical NLP across (X)various categories. Using this meta-dataset, we propose a unified model termedIn-BoXBART, that can jointly learn all tasks of the BoX without anytask-specific modules. To the best of our knowledge, this is the first attemptto propose a unified model in the biomedical domain and use instructions toachieve generalization across several biomedical tasks. Experimental resultsindicate that the proposed model: 1) outperforms the single-task baseline by~3% and multi-task (without instruction) baseline by ~18% on an average, and 2)shows ~23% improvement compared to the single-task baseline in few-shotlearning (i.e., 32 instances per task) on an average. Our analysis indicatesthat there is significant room for improvement across tasks in the BoX,implying the scope for future research direction."
Few-Shot Transfer Learning to improve Chest X-Ray pathology detection  using limited triplets,"['Ananth Reddy Bhimireddy', 'John Lee Burns', 'Saptarshi Purkayastha', 'Judy Wawira Gichoya']",http://arxiv.org/pdf/2204.07824v1.pdf,2022-04-16,"['eess.iv', 'cs.cv']","  Deep learning approaches applied to medical imaging have reached near-humanor better-than-human performance on many diagnostic tasks. For instance, theCheXpert competition on detecting pathologies in chest x-rays has shownexcellent multi-class classification performance. However, training andvalidating deep learning models require extensive collections of images andstill produce false inferences, as identified by a human-in-the-loop. In thispaper, we introduce a practical approach to improve the predictions of apre-trained model through Few-Shot Learning (FSL). After training andvalidating a model, a small number of false inference images are collected toretrain the model using \textbf{\textit{Image Triplets}} - a false positive orfalse negative, a true positive, and a true negative. The retrained FSL modelproduces considerable gains in performance with only a few epochs and fewimages. In addition, FSL opens rapid retraining opportunities forhuman-in-the-loop systems, where a radiologist can relabel false inferences,and the model can be quickly retrained. We compare our retrained modelperformance with existing FSL approaches in medical imaging that train andevaluate models at once."
Less than Few: Self-Shot Video Instance Segmentation,"['Pengwan Yang', 'Yuki M. Asano', 'Pascal Mettes', 'Cees G. M. Snoek']",http://arxiv.org/pdf/2204.08874v1.pdf,2022-04-19,['cs.cv'],"  The goal of this paper is to bypass the need for labelled examples infew-shot video understanding at run time. While proven effective, in manypractical video settings even labelling a few examples appears unrealistic.This is especially true as the level of details in spatio-temporal videounderstanding and with it, the complexity of annotations continues to increase.Rather than performing few-shot learning with a human oracle to provide a fewdensely labelled support videos, we propose to automatically learn to findappropriate support videos given a query. We call this self-shot learning andwe outline a simple self-supervised learning method to generate an embeddingspace well-suited for unsupervised retrieval of relevant samples. To showcasethis novel setting, we tackle, for the first time, video instance segmentationin a self-shot (and few-shot) setting, where the goal is to segment instancesat the pixel-level across the spatial and temporal domains. We provide strongbaseline performances that utilize a novel transformer-based model and showthat self-shot learning can even surpass few-shot and can be positivelycombined for further performance gains. Experiments on new benchmarks show thatour approach achieves strong performance, is competitive to oracle support insome settings, scales to large unlabelled video collections, and can becombined in a semi-supervised setting."
Few-Shot Object Detection with Proposal Balance Refinement,"['Sueyeon Kim', 'Woo-Jeoung Nam', 'Seong-Whan Lee']",http://arxiv.org/pdf/2204.10527v1.pdf,2022-04-22,['cs.cv'],"  Few-shot object detection has gained significant attention in recent years asit has the potential to greatly reduce the reliance on large amounts ofmanually annotated bounding boxes. While most existing few-shot objectdetection literature primarily focuses on bounding box classification byobtaining as discriminative feature embeddings as possible, we emphasize thenecessity of handling the lack of intersection-over-union (IoU) variationsinduced by a biased distribution of novel samples. In this paper, we analyzethe IoU imbalance that is caused by the relatively high number of low-qualityregion proposals, and reveal that it plays a critical role in improvingfew-shot learning capabilities. The well-known two stage fine-tuning techniquecauses insufficient quality and quantity of the novel positive samples, whichhinders the effective object detection of unseen novel classes. To alleviatethis issue, we present a few-shot object detection model with proposal balancerefinement, a simple yet effective approach in learning object proposals usingan auxiliary sequential bounding box refinement process. This process enablesthe detector to be optimized on the various IoU scores through additional novelclass samples. To fully exploit our sequential stage architecture, we revisethe fine-tuning strategy and expose the Region Proposal Network to the novelclasses in order to provide increased learning opportunities for theregion-of-interest (RoI) classifiers and regressors. Our extensive assessmentson PASCAL VOC and COCO demonstrate that our framework substantially outperformsother existing few-shot object detection approaches."
Look Closer to Supervise Better: One-Shot Font Generation via  Component-Based Discriminator,"['Yuxin Kong', 'Canjie Luo', 'Weihong Ma', 'Qiyuan Zhu', 'Shenggao Zhu', 'Nicholas Yuan', 'Lianwen Jin']",http://arxiv.org/pdf/2205.00146v2.pdf,2022-04-30,['cs.cv'],"  Automatic font generation remains a challenging research issue due to thelarge amounts of characters with complicated structures. Typically, only a fewsamples can serve as the style/content reference (termed few-shot learning),which further increases the difficulty to preserve local style patterns ordetailed glyph structures. We investigate the drawbacks of previous studies andfind that a coarse-grained discriminator is insufficient for supervising a fontgenerator. To this end, we propose a novel Component-Aware Module (CAM), whichsupervises the generator to decouple content and style at a more fine-grainedlevel, i.e., the component level. Different from previous studies struggling toincrease the complexity of generators, we aim to perform more effectivesupervision for a relatively simple generator to achieve its full potential,which is a brand new perspective for font generation. The whole frameworkachieves remarkable results by coupling component-level supervision withadversarial learning, hence we call it Component-Guided GAN, shortly CG-GAN.Extensive experiments show that our approach outperforms state-of-the-artone-shot font generation methods. Furthermore, it can be applied to handwrittenword synthesis and scene text image editing, suggesting the generalization ofour approach."
OPT: Open Pre-trained Transformer Language Models,"['Susan Zhang', 'Stephen Roller', 'Naman Goyal', 'Mikel Artetxe', 'Moya Chen', 'Shuohui Chen', 'Christopher Dewan', 'Mona Diab', 'Xian Li', 'Xi Victoria Lin', 'Todor Mihaylov', 'Myle Ott', 'Sam Shleifer', 'Kurt Shuster', 'Daniel Simig', 'Punit Singh Koura', 'Anjali Sridhar', 'Tianlu Wang', 'Luke Zettlemoyer']",http://arxiv.org/pdf/2205.01068v4.pdf,2022-05-02,"['cs.cl', 'cs.lg']","  Large language models, which are often trained for hundreds of thousands ofcompute days, have shown remarkable capabilities for zero- and few-shotlearning. Given their computational cost, these models are difficult toreplicate without significant capital. For the few that are available throughAPIs, no access is granted to the full model weights, making them difficult tostudy. We present Open Pre-trained Transformers (OPT), a suite of decoder-onlypre-trained transformers ranging from 125M to 175B parameters, which we aim tofully and responsibly share with interested researchers. We show that OPT-175Bis comparable to GPT-3, while requiring only 1/7th the carbon footprint todevelop. We are also releasing our logbook detailing the infrastructurechallenges we faced, along with code for experimenting with all of the releasedmodels."
"The ICML 2022 Expressive Vocalizations Workshop and Competition:  Recognizing, Generating, and Personalizing Vocal Bursts","['Alice Baird', 'Panagiotis Tzirakis', 'Gauthier Gidel', 'Marco Jiralerspong', 'Eilif B. Muller', 'Kory Mathewson', 'Björn Schuller', 'Erik Cambria', 'Dacher Keltner', 'Alan Cowen']",http://arxiv.org/pdf/2205.01780v3.pdf,2022-05-03,"['eess.as', 'cs.lg', 'cs.sd']","  The ICML Expressive Vocalization (ExVo) Competition is focused onunderstanding and generating vocal bursts: laughs, gasps, cries, and othernon-verbal vocalizations that are central to emotional expression andcommunication. ExVo 2022, includes three competition tracks using a large-scaledataset of 59,201 vocalizations from 1,702 speakers. The first, ExVo-MultiTask,requires participants to train a multi-task model to recognize expressedemotions and demographic traits from vocal bursts. The second, ExVo-Generate,requires participants to train a generative model that produces vocal burstsconveying ten different emotions. The third, ExVo-FewShot, requiresparticipants to leverage few-shot learning incorporating speaker identity totrain a model for the recognition of 10 emotions conveyed by vocal bursts. Thispaper describes the three tracks and provides performance measures for baselinemodels using state-of-the-art machine learning strategies. The baseline foreach track is as follows, for ExVo-MultiTask, a combined score, computing theharmonic mean of Concordance Correlation Coefficient (CCC), Unweighted AverageRecall (UAR), and inverted Mean Absolute Error (MAE) ($S_{MTL}$) is at best,0.335 $S_{MTL}$; for ExVo-Generate, we report Fr\'echet inception distance(FID) scores ranging from 4.81 to 8.27 (depending on the emotion) between thetraining set and generated samples. We then combine the inverted FID withperceptual ratings of the generated samples ($S_{Gen}$) and obtain 0.174$S_{Gen}$; and for ExVo-FewShot, a mean CCC of 0.444 is obtained."
Generalized Knowledge Distillation via Relationship Matching,"['Han-Jia Ye', 'Su Lu', 'De-Chuan Zhan']",http://arxiv.org/pdf/2205.01915v1.pdf,2022-05-04,"['cs.cv', 'cs.lg']","  The knowledge of a well-trained deep neural network (a.k.a. the ""teacher"") isvaluable for learning similar tasks. Knowledge distillation extracts knowledgefrom the teacher and integrates it with the target model (a.k.a. the""student""), which expands the student's knowledge and improves its learningefficacy. Instead of enforcing the teacher to work on the same task as thestudent, we borrow the knowledge from a teacher trained from a general labelspace -- in this ""Generalized Knowledge Distillation (GKD)"", the classes of theteacher and the student may be the same, completely different, or partiallyoverlapped. We claim that the comparison ability between instances acts as anessential factor threading knowledge across tasks, and propose the RElationshipFacIlitated Local cLassifiEr Distillation (REFILLED) approach, which decouplesthe GKD flow of the embedding and the top-layer classifier. In particular,different from reconciling the instance-label confidence between models,REFILLED requires the teacher to reweight the hard tuples pushed forward by thestudent and then matches the similarity comparison levels between instances. Anembedding-induced classifier based on the teacher model supervises thestudent's classification confidence and adaptively emphasizes the most relatedsupervision from the teacher. REFILLED demonstrates strong discriminativeability when the classes of the teacher vary from the same to a fullynon-overlapped set w.r.t. the student. It also achieves state-of-the-artperformance on standard knowledge distillation, one-step incremental learning,and few-shot learning tasks."
Lifelong Ensemble Learning based on Multiple Representations for  Few-Shot Object Recognition,"['Hamidreza Kasaei', 'Songsong Xiong']",http://arxiv.org/pdf/2205.01982v4.pdf,2022-05-04,"['cs.ro', 'cs.lg']","  Service robots are integrating more and more into our daily lives to help uswith various tasks. In such environments, robots frequently face new objectswhile working in the environment and need to learn them in an open-endedfashion. Furthermore, such robots must be able to recognize a wide range ofobject categories. In this paper, we present a lifelong ensemble learningapproach based on multiple representations to address the few-shot objectrecognition problem. In particular, we form ensemble methods based on deeprepresentations and handcrafted 3D shape descriptors. To facilitate lifelonglearning, each approach is equipped with a memory unit for storing andretrieving object information instantly. The proposed model is suitable foropen-ended learning scenarios where the number of 3D object categories is notfixed and can grow over time. We have performed extensive sets of experimentsto assess the performance of the proposed approach in offline, and open-endedscenarios. For the evaluation purpose, in addition to real object datasets, wegenerate a large synthetic household objects dataset consisting of 27000 viewsof 90 objects. Experimental results demonstrate the effectiveness of theproposed method on online few-shot 3D object recognition tasks, as well as itssuperior performance over the state-of-the-art open-ended learning approaches.Furthermore, our results show that while ensemble learning is modestlybeneficial in offline settings, it is significantly beneficial in lifelongfew-shot learning situations. Additionally, we demonstrated the effectivenessof our approach in both simulated and real-robot settings, where the robotrapidly learned new categories from limited examples."
Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt  Tuning,"['Xiang Chen', 'Lei Li', 'Ningyu Zhang', 'Chuanqi Tan', 'Fei Huang', 'Luo Si', 'Huajun Chen']",http://arxiv.org/pdf/2205.02355v2.pdf,2022-05-04,"['cs.cl', 'cs.ai', 'cs.ir', 'cs.lg']","  Pre-trained language models have contributed significantly to relationextraction by demonstrating remarkable few-shot learning abilities. However,prompt tuning methods for relation extraction may still fail to generalize tothose rare or hard patterns. Note that the previous parametric learningparadigm can be viewed as memorization regarding training data as a book andinference as the close-book test. Those long-tailed or hard patterns can hardlybe memorized in parameters given few-shot instances. To this end, we regard REas an open-book examination and propose a new semiparametric paradigm ofretrieval-enhanced prompt tuning for relation extraction. We construct anopen-book datastore for retrieval regarding prompt-based instancerepresentations and corresponding relation labels as memorized key-value pairs.During inference, the model can infer relations by linearly interpolating thebase output of PLM with the non-parametric nearest neighbor distribution overthe datastore. In this way, our model not only infers relation throughknowledge stored in the weights during training but also assistsdecision-making by unwinding and querying examples in the open-book datastore.Extensive experiments on benchmark datasets show that our method can achievestate-of-the-art in both standard supervised and few-shot settings. Code areavailable in https://github.com/zjunlp/PromptKG/tree/main/research/RetrievalRE."
Towards Unified Prompt Tuning for Few-shot Text Classification,"['Jianing Wang', 'Chengyu Wang', 'Fuli Luo', 'Chuanqi Tan', 'Minghui Qiu', 'Fei Yang', 'Qiuhui Shi', 'Songfang Huang', 'Ming Gao']",http://arxiv.org/pdf/2205.05313v1.pdf,2022-05-11,"['cs.cl', 'cs.ai']","  Prompt-based fine-tuning has boosted the performance of Pre-trained LanguageModels (PLMs) on few-shot text classification by employing task-specificprompts. Yet, PLMs are unfamiliar with prompt-style expressions duringpre-training, which limits the few-shot learning performance on downstreamtasks. It would be desirable if the models can acquire some prompting knowledgebefore adaptation to specific NLP tasks. We present the Unified Prompt Tuning(UPT) framework, leading to better few-shot text classification for BERT-stylemodels by explicitly capturing prompting semantics from non-target NLPdatasets. In UPT, a novel paradigm Prompt-Options-Verbalizer is proposed forjoint prompt learning across different NLP tasks, forcing PLMs to capturetask-invariant prompting knowledge. We further design a self-supervised tasknamed Knowledge-enhanced Selective Masked Language Modeling to improve thePLM's generalization abilities for accurate adaptation to previously unseentasks. After multi-task learning across multiple tasks, the PLM can be betterprompt-tuned towards any dissimilar target tasks in low-resourced settings.Experiments over a variety of NLP tasks show that UPT consistently outperformsstate-of-the-arts for prompt-based fine-tuning."
Towards Answering Open-ended Ethical Quandary Questions,"['Yejin Bang', 'Nayeon Lee', 'Tiezheng Yu', 'Leila Khalatbari', 'Yan Xu', 'Samuel Cahyawijaya', 'Dan Su', 'Bryan Wilie', 'Romain Barraud', 'Elham J. Barezi', 'Andrea Madotto', 'Hayden Kee', 'Pascale Fung']",http://arxiv.org/pdf/2205.05989v3.pdf,2022-05-12,"['cs.cl', 'cs.ai', 'cs.lg']","  Considerable advancements have been made in various NLP tasks based on theimpressive power of large language models (LLMs) and many NLP applications aredeployed in our daily lives. In this work, we challenge the capability of LLMswith the new task of Ethical Quandary Generative Question Answering. Ethicalquandary questions are more challenging to address because multiple conflictinganswers may exist to a single quandary. We explore the current capability ofLLMs in providing an answer with a deliberative exchange of differentperspectives to an ethical quandary, in the approach of Socratic philosophy,instead of providing a closed answer like an oracle. We propose a model thatsearches for different ethical principles applicable to the ethical quandaryand generates an answer conditioned on the chosen principles throughprompt-based few-shot learning. We also discuss the remaining challenges andethical issues involved in this task and suggest the direction towarddeveloping responsible NLP systems by incorporating human values explicitly."
EyeDAS: Securing Perception of Autonomous Cars Against the  Stereoblindness Syndrome,"['Efrat Levy', 'Ben Nassi', 'Raz Swissa', 'Yuval Elovici']",http://arxiv.org/pdf/2205.06765v1.pdf,2022-05-13,"['cs.lg', 'cs.cr']","  The ability to detect whether an object is a 2D or 3D object is extremelyimportant in autonomous driving, since a detection error can havelife-threatening consequences, endangering the safety of the driver,passengers, pedestrians, and others on the road. Methods proposed todistinguish between 2 and 3D objects (e.g., liveness detection methods) are notsuitable for autonomous driving, because they are object dependent or do notconsider the constraints associated with autonomous driving (e.g., the need forreal-time decision-making while the vehicle is moving). In this paper, wepresent EyeDAS, a novel few-shot learning-based method aimed at securing anobject detector (OD) against the threat posed by the stereoblindness syndrome(i.e., the inability to distinguish between 2D and 3D objects). We evaluateEyeDAS's real-time performance using 2,000 objects extracted from seven YouTubevideo recordings of street views taken by a dash cam from the driver's seatperspective. When applying EyeDAS to seven state-of-the-art ODs as acountermeasure, EyeDAS was able to reduce the 2D misclassification rate from71.42-100% to 2.4% with a 3D misclassification rate of 0% (TPR of 1.0). We alsoshow that EyeDAS outperforms the baseline method and achieves an AUC of over0.999 and a TPR of 1.0 with an FPR of 0.024."
"How to Fine-tune Models with Few Samples: Update, Data Augmentation, and  Test-time Augmentation","['Yujin Kim', 'Jaehoon Oh', 'Sungnyun Kim', 'Se-Young Yun']",http://arxiv.org/pdf/2205.07874v3.pdf,2022-05-13,"['cs.lg', 'cs.cv']","  Most of the recent few-shot learning (FSL) algorithms are based on transferlearning, where a model is pre-trained using a large amount of source data, andthe pre-trained model is fine-tuned using a small amount of target data. Intransfer learning-based FSL, sophisticated pre-training methods have beenwidely studied for universal representation. Therefore, it has become moreimportant to utilize the universal representation for downstream tasks, butthere are few studies on fine-tuning in FSL. In this paper, we focus on how totransfer pre-trained models to few-shot downstream tasks from the threeperspectives: update, data augmentation, and test-time augmentation. First, wecompare the two popular update methods, full fine-tuning (i.e., updating theentire network, FT) and linear probing (i.e., updating only a linearclassifier, LP). We find that LP is better than FT with extremely few samples,whereas FT outperforms LP as training samples increase. Next, we show that dataaugmentation cannot guarantee few-shot performance improvement and investigatethe effectiveness of data augmentation based on the intensity of augmentation.Finally, we adopt augmentation to both a support set for update (i.e., dataaugmentation) as well as a query set for prediction (i.e., test-timeaugmentation), considering support-query distribution shifts, and improvefew-shot performance. The code is available athttps://github.com/kimyuji/updating_FSL."
Region-Aware Metric Learning for Open World Semantic Segmentation via  Meta-Channel Aggregation,"['Hexin Dong', 'Zifan Chen', 'Mingze Yuan', 'Yutong Xie', 'Jie Zhao', 'Fei Yu', 'Bin Dong', 'Li Zhang']",http://arxiv.org/pdf/2205.08083v1.pdf,2022-05-17,['cs.cv'],"  As one of the most challenging and practical segmentation tasks, open-worldsemantic segmentation requires the model to segment the anomaly regions in theimages and incrementally learn to segment out-of-distribution (OOD) objects,especially under a few-shot condition. The current state-of-the-art (SOTA)method, Deep Metric Learning Network (DMLNet), relies on pixel-level metriclearning, with which the identification of similar regions having differentsemantics is difficult. Therefore, we propose a method called region-awaremetric learning (RAML), which first separates the regions of the images andgenerates region-aware features for further metric learning. RAML improves theintegrity of the segmented anomaly regions. Moreover, we propose a novelmeta-channel aggregation (MCA) module to further separate anomaly regions,forming high-quality sub-region candidates and thereby improving the modelperformance for OOD objects. To evaluate the proposed RAML, we have conductedextensive experiments and ablation studies on Lost And Found and Road Anomalydatasets for anomaly segmentation and the CityScapes dataset for incrementalfew-shot learning. The results show that the proposed RAML achieves SOTAperformance in both stages of open world segmentation. Our code and appendixare available at https://github.com/czifan/RAML."
PromptDA: Label-guided Data Augmentation for Prompt-based Few-shot  Learners,"['Canyu Chen', 'Kai Shu']",http://arxiv.org/pdf/2205.09229v3.pdf,2022-05-18,"['cs.cl', 'cs.ai']","  Recent advances in large pre-trained language models (PLMs) lead toimpressive gains in natural language understanding (NLU) tasks withtask-specific fine-tuning. However, directly fine-tuning PLMs heavily relies onsufficient labeled training instances, which are usually hard to obtain.Prompt-based tuning on PLMs has shown to be powerful for various downstreamfew-shot tasks. Existing works studying prompt-based tuning for few-shot NLUtasks mainly focus on deriving proper label words with a verbalizer orgenerating prompt templates to elicit semantics from PLMs. In addition,conventional data augmentation strategies such as synonym substitution, thoughwidely adopted in low-resource scenarios, only bring marginal improvements forprompt-based few-shot learning. Thus, an important research question arises:how to design effective data augmentation methods for prompt-based few-shottuning? To this end, considering the label semantics are essential inprompt-based tuning, we propose a novel label-guided data augmentationframework PromptDA, which exploits the enriched label semantic information fordata augmentation. Extensive experiment results on few-shot text classificationtasks demonstrate the superior performance of the proposed framework byeffectively leveraging label semantics and data augmentation for naturallanguage understanding. Our code is available athttps://github.com/canyuchen/PromptDA."
Self-mentoring: a new deep learning pipeline to train a self-supervised  U-net for few-shot learning of bio-artificial capsule segmentation,"['Arnaud Deleruyelle', 'Cristian Versari', 'John Klein']",http://arxiv.org/pdf/2205.10840v3.pdf,2022-05-22,"['cs.cv', 'cs.lg']","  Background: Accurate segmentation of microscopic structures such asbio-artificial capsules in microscopy imaging is a prerequisite to thecomputer-aided understanding of important biomechanical phenomenons.State-of-the-art segmentation performances are achieved by deep neural networksand related data-driven approaches. Training these networks from only a fewannotated examples is challenging while producing manually annotated imagesthat provide supervision is tedious.  Method: Recently, self-supervision, i.e. designing a neural pipelineproviding synthetic or indirect supervision, has proved to significantlyincrease generalization performances of models trained on few shots. Theobjective of this paper is to introduce one such neural pipeline in the contextof micro-capsule image segmentation. Our method leverages the rather simplecontent of these images so that a trainee network can be mentored by a refereenetwork which has been previously trained on synthetically generated pairs ofcorrupted/correct region masks.  Results: Challenging experimental setups are investigated. They involve fromonly 3 to 10 annotated images along with moderately large amounts ofunannotated images. In a bio-artificial capsule dataset, our approachconsistently and drastically improves accuracy. We also show that the learntreferee network is transferable to another Glioblastoma cell dataset and thatit can be efficiently coupled with data augmentation strategies.  Conclusions: Experimental results show that very significant accuracyincrements are obtained by the proposed pipeline, leading to the conclusionthat the self-supervision mechanism introduced in this paper has the potentialto replace human annotations."
What Makes Data-to-Text Generation Hard for Pretrained Language Models?,"['Moniba Keymanesh', 'Adrian Benton', 'Mark Dredze']",http://arxiv.org/pdf/2205.11505v1.pdf,2022-05-23,"['cs.cl', 'cs.ai', 'cs.ir', 'cs.lg']","  Expressing natural language descriptions of structured facts or relations --data-to-text generation (D2T) -- increases the accessibility of structuredknowledge repositories. Previous work shows that pre-trained languagemodels(PLMs) perform remarkably well on this task after fine-tuning on asignificant amount of task-specific training data. On the other hand, whileauto-regressive PLMs can generalize from a few task examples, their efficacy atD2T is largely unexplored. Furthermore, we have an incomplete understanding ofthe limits of PLMs on D2T.  In this work, we conduct an empirical study of both fine-tuned andauto-regressive PLMs on the DART multi-domain D2T dataset. We consider theirperformance as a function of the amount of task-specific data and how thesedata are incorporated into the models: zero and few-shot learning, andfine-tuning of model weights. In addition, we probe the limits of PLMs bymeasuring performance on subsets of the evaluation data: novel predicates andabstractive test examples. To improve the performance on these subsets, weinvestigate two techniques: providing predicate descriptions in the context andre-ranking generated candidates by information reflected in the source.Finally, we conduct a human evaluation of model errors and show that D2Tgeneration tasks would benefit from datasets with more careful manual curation."
ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures  of Soft Prompts,"['Akari Asai', 'Mohammadreza Salehi', 'Matthew E. Peters', 'Hannaneh Hajishirzi']",http://arxiv.org/pdf/2205.11961v2.pdf,2022-05-24,['cs.cl'],"  This work introduces a new multi-task, parameter-efficient language model(LM) tuning method that learns to transfer knowledge across different tasks viaa mixture of soft prompts-small prefix embedding vectors pre-trained fordifferent tasks. Our method, called ATTEMPT (ATTEntional Mixtures of PromptTuning), obtains source prompts as encodings of large-scale source tasks into asmall number of parameters and trains an attention module to interpolate thesource prompts and a newly initialized target prompt for every instance in thetarget task. During training, only the target task prompt and the attentionweights, which are shared between tasks in multi-task training, are updated,while the original LM and source prompts are intact. ATTEMPT is highlyparameter-efficient (e.g., updates 2,300 times fewer parameters than fullfine-tuning) while achieving high task performance using knowledge fromhigh-resource tasks. Moreover, it is modular using pre-trained soft prompts,and can flexibly add or remove source prompts for effective knowledge transfer.Our experimental results across 21 diverse NLP datasets show that ATTEMPTsignificantly outperforms prompt tuning and outperforms or matches fullyfine-tuned or other parameter-efficient tuning approaches that use over tentimes more parameters. Finally, ATTEMPT outperforms previous work in few-shotlearning settings."
Representing Brain Anatomical Regularity and Variability by Few-Shot  Embedding,"['Lu Zhang', 'Xiaowei Yu', 'Yanjun Lyu', 'Zhengwang Wu', 'Haixing Dai', 'Lin Zhao', 'Li Wang', 'Gang Li', 'Tianming Liu', 'Dajiang Zhu']",http://arxiv.org/pdf/2205.13644v1.pdf,2022-05-26,['q-bio.nc'],"  Effective representation of brain anatomical architecture is fundamental inunderstanding brain regularity and variability. Despite numerous efforts, it isstill difficult to infer reliable anatomical correspondence at finer scale,given the tremendous individual variability in cortical folding patterns. It iseven more challenging to disentangle common and individual patterns whencomparing brains at different neuro-developmental stages. In this work, wedeveloped a novel learning-based few-shot embedding framework to encode thecortical folding patterns into a latent space represented by a group ofanatomically meaningful embedding vectors. Specifically, we adopted 3-hinge(3HG) network as the substrate and designed an autoencoder-based embeddingframework to learn a common embedding vector for each 3HG's multi-hop feature:each 3HG can be represented as a combination of these feature embeddings via aset of individual specific coefficients to characterize individualizedanatomical information. That is, the regularity of folding patterns is encodedinto the embeddings, while the individual variations are preserved by themulti=hop combination coefficients. To effectively learn the embeddings for thepopulation with very limited samples, few-shot learning was adopted. We appliedour method on adult HCP and pediatric datasets with 1,000+ brains (from 34gestational weeks to young adult). Our experimental results show that: 1) thelearned embedding vectors can quantitatively encode the commonality andindividuality of cortical folding patterns; 2) with the embeddings we canrobustly infer the complicated many-to-many anatomical correspondences amongdifferent brains and 3) our model can be successfully transferred to newpopulations with very limited training samples."
Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object  Interactions,"['Huaizu Jiang', 'Xiaojian Ma', 'Weili Nie', 'Zhiding Yu', 'Yuke Zhu', 'Song-Chun Zhu', 'Anima Anandkumar']",http://arxiv.org/pdf/2205.13803v2.pdf,2022-05-27,"['cs.cv', 'cs.ai', 'cs.lg']","  A significant gap remains between today's visual pattern recognition modelsand human-level visual cognition especially when it comes to few-shot learningand compositional reasoning of novel concepts. We introduce Bongard-HOI, a newvisual reasoning benchmark that focuses on compositional learning ofhuman-object interactions (HOIs) from natural images. It is inspired by twodesirable characteristics from the classical Bongard problems (BPs): 1)few-shot concept learning, and 2) context-dependent reasoning. We carefullycurate the few-shot instances with hard negatives, where positive and negativeimages only disagree on action labels, making mere recognition of objectcategories insufficient to complete our benchmarks. We also design multipletest sets to systematically study the generalization of visual learning models,where we vary the overlap of the HOI concepts between the training and testsets of few-shot instances, from partial to no overlaps. Bongard-HOI presents asubstantial challenge to today's visual recognition models. Thestate-of-the-art HOI detection model achieves only 62% accuracy on few-shotbinary prediction while even amateur human testers on MTurk have 91% accuracy.With the Bongard-HOI benchmark, we hope to further advance research efforts invisual reasoning, especially in holistic perception-reasoning systems andbetter representation learning."
The Spike Gating Flow: A Hierarchical Structure Based Spiking Neural  Network for Online Gesture Recognition,"['Zihao Zhao', 'Yanhong Wang', 'Qiaosha Zou', 'Tie Xu', 'Fangbo Tao', 'Jiansong Zhang', 'Xiaoan Wang', 'C. -J. Richard Shi', 'Junwen Luo', 'Yuan Xie']",http://arxiv.org/pdf/2206.01910v2.pdf,2022-06-04,"['cs.cv', 'cs.ai']","  Action recognition is an exciting research avenue for artificial intelligencesince it may be a game changer in the emerging industrial fields such asrobotic visions and automobiles. However, current deep learning faces majorchallenges for such applications because of the huge computational cost and theinefficient learning. Hence, we develop a novel brain-inspired Spiking NeuralNetwork (SNN) based system titled Spiking Gating Flow (SGF) for online actionlearning. The developed system consists of multiple SGF units which assembledin a hierarchical manner. A single SGF unit involves three layers: a featureextraction layer, an event-driven layer and a histogram-based training layer.To demonstrate the developed system capabilities, we employ a standard DynamicVision Sensor (DVS) gesture classification as a benchmark. The results indicatethat we can achieve 87.5% accuracy which is comparable with Deep Learning (DL),but at smaller training/inference data number ratio 1.5:1. And only a singletraining epoch is required during the learning process. Meanwhile, to the bestof our knowledge, this is the highest accuracy among the non-backpropagationalgorithm based SNNs. At last, we conclude the few-shot learning paradigm ofthe developed network: 1) a hierarchical structure-based network designinvolves human prior knowledge; 2) SNNs for content based global dynamicfeature detection."
Robust Meta-learning with Sampling Noise and Label Noise via  Eigen-Reptile,"['Dong Chen', 'Lingfei Wu', 'Siliang Tang', 'Xiao Yun', 'Bo Long', 'Yueting Zhuang']",http://arxiv.org/pdf/2206.01944v1.pdf,2022-06-04,"['cs.lg', 'cs.ai']","  Recent years have seen a surge of interest in meta-learning techniques fortackling the few-shot learning (FSL) problem. However, the meta-learner isprone to overfitting since there are only a few available samples, which can beidentified as sampling noise on a clean dataset. Moreover, when handling thedata with noisy labels, the meta-learner could be extremely sensitive to labelnoise on a corrupted dataset. To address these two challenges, we presentEigen-Reptile (ER) that updates the meta-parameters with the main direction ofhistorical task-specific parameters to alleviate sampling and label noise.Specifically, the main direction is computed in a fast way, where the scale ofthe calculated matrix is related to the number of gradient steps instead of thenumber of parameters. Furthermore, to obtain a more accurate main direction forEigen-Reptile in the presence of many noisy labels, we further proposeIntrospective Self-paced Learning (ISPL). We have theoretically andexperimentally demonstrated the soundness and effectiveness of the proposedEigen-Reptile and ISPL. Particularly, our experiments on different tasks showthat the proposed method is able to outperform or achieve highly competitiveperformance compared with other gradient-based methods with or without noisylabels. The code and data for the proposed method are provided for researchpurposes https://github.com/Anfeather/Eigen-Reptile."
Guided Deep Metric Learning,"['Jorge Gonzalez-Zapata', 'Ivan Reyes-Amezcua', 'Daniel Flores-Araiza', 'Mauricio Mendez-Ruiz', 'Gilberto Ochoa-Ruiz', 'Andres Mendez-Vazquez']",http://arxiv.org/pdf/2206.02029v1.pdf,2022-06-04,"['cs.cv', 'cs.ai', 'cs.lg']","  Deep Metric Learning (DML) methods have been proven relevant for visualsimilarity learning. However, they sometimes lack generalization propertiesbecause they are trained often using an inappropriate sample selection strategyor due to the difficulty of the dataset caused by a distributional shift in thedata. These represent a significant drawback when attempting to learn theunderlying data manifold. Therefore, there is a pressing need to develop betterways of obtaining generalization and representation of the underlying manifold.In this paper, we propose a novel approach to DML that we call Guided DeepMetric Learning, a novel architecture oriented to learning more compactclusters, improving generalization under distributional shifts in DML. Thisnovel architecture consists of two independent models: A multi-branch mastermodel, inspired from a Few-Shot Learning (FSL) perspective, generates a reducedhypothesis space based on prior knowledge from labeled data, which guides orregularizes the decision boundary of a student model during training under anoffline knowledge distillation scheme. Experiments have shown that the proposedmethod is capable of a better manifold generalization and representation to upto 40% improvement (Recall@1, CIFAR10), using guidelines suggested by Musgraveet al. to perform a more fair and realistic comparison, which is currentlyabsent in the literature"
Making Large Language Models Better Reasoners with Step-Aware Verifier,"['Yifei Li', 'Zeqi Lin', 'Shizhuo Zhang', 'Qiang Fu', 'Bei Chen', 'Jian-Guang Lou', 'Weizhu Chen']",http://arxiv.org/pdf/2206.02336v3.pdf,2022-06-06,"['cs.cl', 'cs.ai']","  Few-shot learning is a challenging task that requires language models togeneralize from limited examples. Large language models like GPT-3 and PaLMhave made impressive progress in this area, but they still face difficulties inreasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improvetheir reasoning skills, previous work has proposed to guide the language modelwith prompts that elicit a series of reasoning steps before giving the finalanswer, achieving a significant improvement on GSM8K from 17.9% to 58.1% inproblem-solving rate. In this paper, we present DIVERSE (Diverse Verifier onReasoning Step), a novel approach that further enhances the reasoningcapability of language models. DIVERSE has three main components: first, itgenerates diverse prompts to explore different reasoning paths for the samequestion; second, it uses a verifier to filter out incorrect answers based on aweighted voting scheme; and third, it verifies each reasoning step individuallyinstead of the whole chain. We evaluate DIVERSE on the latest language modelcode-davinci-002 and show that it achieves new state-of-the-art results on sixof eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%)."
Real-time Hyper-Dimensional Reconfiguration at the Edge using Hardware  Accelerators,"['Indhumathi Kandaswamy', 'Saurabh Farkya', 'Zachary Daniels', 'Gooitzen van der Wal', 'Aswin Raghavan', 'Yuzheng Zhang', 'Jun Hu', 'Michael Lomnitz', 'Michael Isnardi', 'David Zhang', 'Michael Piacentino']",http://arxiv.org/pdf/2206.05128v1.pdf,2022-06-10,"['cs.cv', 'cs.ar']","  In this paper we present Hyper-Dimensional Reconfigurable Analytics at theTactical Edge (HyDRATE) using low-SWaP embedded hardware that can performreal-time reconfiguration at the edge leveraging non-MAC (free offloating-point MultiplyACcumulate operations) deep neural nets (DNN) combinedwith hyperdimensional (HD) computing accelerators. We describe the algorithm,trained quantized model generation, and simulated performance of a featureextractor free of multiply-accumulates feeding a hyperdimensional logic-basedclassifier. Then we show how performance increases with the number ofhyperdimensions. We describe the realized low-SWaP FPGA hardware and embeddedsoftware system compared to traditional DNNs and detail the implementedhardware accelerators. We discuss the measured system latency and power, noiserobustness due to use of learnable quantization and HD computing, actual versussimulated system performance for a video activity classification task anddemonstration of reconfiguration on this same dataset. We show thatreconfigurability in the field is achieved by retraining only the feed-forwardHD classifier without gradient descent backpropagation (gradient-free), usingfew-shot learning of new classes at the edge. Initial work performed used LRCNDNN and is currently extended to use Two-stream DNN with improved performance."
Language Models are General-Purpose Interfaces,"['Yaru Hao', 'Haoyu Song', 'Li Dong', 'Shaohan Huang', 'Zewen Chi', 'Wenhui Wang', 'Shuming Ma', 'Furu Wei']",http://arxiv.org/pdf/2206.06336v1.pdf,2022-06-13,['cs.cl'],"  Foundation models have received much attention due to their effectivenessacross a broad range of downstream applications. Though there is a bigconvergence in terms of architecture, most pretrained models are typicallystill developed for specific tasks or modalities. In this work, we propose touse language models as a general-purpose interface to various foundationmodels. A collection of pretrained encoders perceive diverse modalities (suchas vision, and language), and they dock with a language model that plays therole of a universal task layer. We propose a semi-causal language modelingobjective to jointly pretrain the interface and the modular encoders. Wesubsume the advantages and capabilities from both causal and non-causalmodeling, thereby combining the best of two worlds. Specifically, the proposedmethod not only inherits the capabilities of in-context learning and open-endedgeneration from causal language modeling, but also is conducive to finetuningbecause of the bidirectional encoders. More importantly, our approachseamlessly unlocks the combinations of the above capabilities, e.g., enablingin-context learning or instruction following with finetuned encoders.Experimental results across various language-only and vision-languagebenchmarks show that our model outperforms or is competitive with specializedmodels on finetuning, zero-shot generalization, and few-shot learning."
"NatGen: Generative pre-training by ""Naturalizing"" source code","['Saikat Chakraborty', 'Toufique Ahmed', 'Yangruibo Ding', 'Premkumar Devanbu', 'Baishakhi Ray']",http://arxiv.org/pdf/2206.07585v2.pdf,2022-06-15,"['cs.pl', 'cs.ai', 'cs.lg', 'cs.se']","  Pre-trained Generative Language models (e.g. PLBART, CodeT5, SPT-Code) forsource code yielded strong results on several tasks in the past few years,including code generation and translation. These models have adopted varyingpre-training objectives to learn statistics of code construction from verylarge-scale corpora in a self-supervised fashion; the success of pre-trainedmodels largely hinges on these pre-training objectives. This paper proposes anew pre-training objective, ""Naturalizing"" of source code, exploiting code'sbimodal, dual-channel (formal & natural channels) nature. Unlike naturallanguage, code's bimodal, dual-channel nature allows us to generatesemantically equivalent code at scale. We introduce six classes of semanticpreserving transformations to introduce un-natural forms of code, and thenforce our model to produce more natural original programs written bydevelopers. Learning to generate equivalent, but more natural code, at scale,over large corpora of open-source code, without explicit manual supervision,helps the model learn to both ingest & generate code. We fine-tune our model inthree generative Software Engineering tasks: code generation, code translation,and code refinement with limited human-curated labeled data and achievestate-of-the-art performance rivaling CodeT5. We show that our pre-trainedmodel is especially competitive at zero-shot and few-shot learning, and betterat learning code properties (e.g., syntax, data flow)."
FiT: Parameter Efficient Few-shot Transfer Learning for Personalized and  Federated Image Classification,"['Aliaksandra Shysheya', 'John Bronskill', 'Massimiliano Patacchiola', 'Sebastian Nowozin', 'Richard E Turner']",http://arxiv.org/pdf/2206.08671v2.pdf,2022-06-17,"['stat.ml', 'cs.cv', 'cs.lg']","  Modern deep learning systems are increasingly deployed in situations such aspersonalization and federated learning where it is necessary to support i)learning on small amounts of data, and ii) communication efficient distributedtraining protocols. In this work, we develop FiLM Transfer (FiT) which fulfillsthese requirements in the image classification setting by combining ideas fromtransfer learning (fixed pretrained backbones and fine-tuned FiLM adapterlayers) and meta-learning (automatically configured Naive Bayes classifiers andepisodic training) to yield parameter efficient models with superiorclassification accuracy at low-shot. The resulting parameter efficiency is keyfor enabling few-shot learning, inexpensive model updates for personalization,and communication efficient federated learning. We experiment with FiT on awide range of downstream datasets and show that it achieves betterclassification accuracy than the leading Big Transfer (BiT) algorithm atlow-shot and achieves state-of-the art accuracy on the challenging VTAB-1kbenchmark, with fewer than 1% of the updateable parameters. Finally, wedemonstrate the parameter efficiency and superior accuracy of FiT indistributed low-shot applications including model personalization and federatedlearning where model update size is an important performance metric."
Model-Agnostic Few-Shot Open-Set Recognition,"['Malik Boudiaf', 'Etienne Bennequin', 'Myriam Tami', 'Celine Hudelot', 'Antoine Toubhans', 'Pablo Piantanida', 'Ismail Ben Ayed']",http://arxiv.org/pdf/2206.09236v1.pdf,2022-06-18,['cs.lg'],"  We tackle the Few-Shot Open-Set Recognition (FSOSR) problem, i.e. classifyinginstances among a set of classes for which we only have few labeled samples,while simultaneously detecting instances that do not belong to any known class.Departing from existing literature, we focus on developing model-agnosticinference methods that can be plugged into any existing model, regardless ofits architecture or its training procedure. Through evaluating the embedding'squality of a variety of models, we quantify the intrinsic difficulty ofmodel-agnostic FSOSR. Furthermore, a fair empirical evaluation suggests thatthe naive combination of a kNN detector and a prototypical classifier ranksbefore specialized or complex methods in the inductive setting of FSOSR. Theseobservations motivated us to resort to transduction, as a popular and practicalrelaxation of standard few-shot learning problems. We introduce an Open SetTransductive Information Maximization method OSTIM, which hallucinates anoutlier prototype while maximizing the mutual information between extractedfeatures and assignments. Through extensive experiments spanning 5 datasets, weshow that OSTIM surpasses both inductive and existing transductive methods indetecting open-set instances while competing with the strongest transductivemethods in classifying closed-set instances. We further show that OSTIM's modelagnosticity allows it to successfully leverage the strong expressive abilitiesof the latest architectures and training strategies without any hyperparametermodification, a promising sign that architectural advances to come willcontinue to positively impact OSTIM's performances."
Task-Adaptive Few-shot Node Classification,"['Song Wang', 'Kaize Ding', 'Chuxu Zhang', 'Chen Chen', 'Jundong Li']",http://arxiv.org/pdf/2206.11972v1.pdf,2022-06-23,['cs.lg'],"  Node classification is of great importance among various graph mining tasks.In practice, real-world graphs generally follow the long-tail distribution,where a large number of classes only consist of limited labeled nodes. AlthoughGraph Neural Networks (GNNs) have achieved significant improvements in nodeclassification, their performance decreases substantially in such a few-shotscenario. The main reason can be attributed to the vast generalization gapbetween meta-training and meta-test due to the task variance caused bydifferent node/class distributions in meta-tasks (i.e., node-level andclass-level variance). Therefore, to effectively alleviate the impact of taskvariance, we propose a task-adaptive node classification framework under thefew-shot learning setting. Specifically, we first accumulate meta-knowledgeacross classes with abundant labeled nodes. Then we transfer such knowledge tothe classes with limited labeled nodes via our proposed task-adaptive modules.In particular, to accommodate the different node/class distributions amongmeta-tasks, we propose three essential modules to perform \emph{node-level},\emph{class-level}, and \emph{task-level} adaptations in each meta-task,respectively. In this way, our framework can conduct adaptations to differentmeta-tasks and thus advance the model generalization performance on meta-testtasks. Extensive experiments on four prevalent node classification datasetsdemonstrate the superiority of our framework over the state-of-the-artbaselines. Our code is provided at https://github.com/SongW-SW/TENT."
Dynamic Sub-Cluster-Aware Network for Few-Shot Skin Disease  Classification,"['Shuhan LI', 'Xiaomeng Li', 'Xiaowei Xu', 'Kwang-Ting Cheng']",http://arxiv.org/pdf/2207.01072v2.pdf,2022-07-03,['cs.cv'],"  This paper addresses the problem of few-shot skin disease classification byintroducing a novel approach called the Sub-Cluster-Aware Network (SCAN) thatenhances accuracy in diagnosing rare skin diseases. The key insight motivatingthe design of SCAN is the observation that skin disease images within a classoften exhibit multiple sub-clusters, characterized by distinct variations inappearance. To improve the performance of few-shot learning, we focus onlearning a high-quality feature encoder that captures the unique sub-clusteredrepresentations within each disease class, enabling better characterization offeature distributions. Specifically, SCAN follows a dual-branch framework,where the first branch learns class-wise features to distinguish different skindiseases, and the second branch aims to learn features which can effectivelypartition each class into several groups so as to preserve the sub-clusteredstructure within each class. To achieve the objective of the second branch, wepresent a cluster loss to learn image similarities via unsupervised clustering.To ensure that the samples in each sub-cluster are from the same class, wefurther design a purity loss to refine the unsupervised clustering results. Weevaluate the proposed approach on two public datasets for few-shot skin diseaseclassification. The experimental results validate that our frameworkoutperforms the state-of-the-art methods by around 2% to 5% in terms ofsensitivity, specificity, accuracy, and F1-score on the SD-198 and Derm7ptdatasets."
DCT-Net: Domain-Calibrated Translation for Portrait Stylization,"['Yifang Men', 'Yuan Yao', 'Miaomiao Cui', 'Zhouhui Lian', 'Xuansong Xie']",http://arxiv.org/pdf/2207.02426v1.pdf,2022-07-06,['cs.cv'],"  This paper introduces DCT-Net, a novel image translation architecture forfew-shot portrait stylization. Given limited style exemplars ($\sim$100), thenew architecture can produce high-quality style transfer results with advancedability to synthesize high-fidelity contents and strong generality to handlecomplicated scenes (e.g., occlusions and accessories). Moreover, it enablesfull-body image translation via one elegant evaluation network trained bypartial observations (i.e., stylized heads). Few-shot learning based styletransfer is challenging since the learned model can easily become overfitted inthe target domain, due to the biased distribution formed by only a few trainingexamples. This paper aims to handle the challenge by adopting the key idea of""calibration first, translation later"" and exploring the augmented globalstructure with locally-focused translation. Specifically, the proposed DCT-Netconsists of three modules: a content adapter borrowing the powerful prior fromsource photos to calibrate the content distribution of target samples; ageometry expansion module using affine transformations to release spatiallysemantic constraints; and a texture translation module leveraging samplesproduced by the calibrated distribution to learn a fine-grained conversion.Experimental results demonstrate the proposed method's superiority over thestate of the art in head stylization and its effectiveness on full imagetranslation with adaptive deformations."
Segment-level Metric Learning for Few-shot Bioacoustic Event Detection,"['Haohe Liu', 'Xubo Liu', 'Xinhao Mei', 'Qiuqiang Kong', 'Wenwu Wang', 'Mark D. Plumbley']",http://arxiv.org/pdf/2207.07773v1.pdf,2022-07-15,"['eess.as', 'cs.ai', 'cs.sd', 'eess.sp']","  Few-shot bioacoustic event detection is a task that detects the occurrencetime of a novel sound given a few examples. Previous methods employ metriclearning to build a latent space with the labeled part of different soundclasses, also known as positive events. In this study, we propose asegment-level few-shot learning framework that utilizes both the positive andnegative events during model optimization. Training with negative events, whichare larger in volume than positive events, can increase the generalizationability of the model. In addition, we use transductive inference on thevalidation set during training for better adaptation to novel classes. Weconduct ablation studies on our proposed method with different setups on inputfeatures, training data, and hyper-parameters. Our final system achieves anF-measure of 62.73 on the DCASE 2022 challenge task 5 (DCASE2022-T5) validationset, outperforming the performance of the baseline prototypical network 34.02by a large margin. Using the proposed method, our submitted system ranks 2nd inDCASE2022-T5. The code of this paper is fully open-sourced athttps://github.com/haoheliu/DCASE_2022_Task_5."
A Reinforcement Learning-based Offensive semantics Censorship System for  Chatbots,"['Shaokang Cai', 'Dezhi Han', 'Zibin Zheng', 'Dun Li', ' NoelCrespi']",http://arxiv.org/pdf/2207.10569v1.pdf,2022-07-13,['cs.cl'],"  The rapid development of artificial intelligence (AI) technology has enabledlarge-scale AI applications to land in the market and practice. However, whileAI technology has brought many conveniences to people in the productizationprocess, it has also exposed many security issues. Especially, attacks againstonline learning vulnerabilities of chatbots occur frequently. Therefore, thispaper proposes a semantics censorship chatbot system based on reinforcementlearning, which is mainly composed of two parts: the Offensive semanticscensorship model and the semantics purification model. Offensive semanticsreview can combine the context of user input sentences to detect the rapidevolution of Offensive semantics and respond to Offensive semantics responses.The semantics purification model For the case of chatting robot models, it hasbeen contaminated by large numbers of offensive semantics, by strengthening theoffensive reply learned by the learning algorithm, rather than rolling back tothe early versions. In addition, by integrating a once-through learningapproach, the speed of semantics purification is accelerated while reducing theimpact on the quality of replies. The experimental results show that ourproposed approach reduces the probability of the chat model generatingoffensive replies and that the integration of the few-shot learning algorithmimproves the training speed rapidly while effectively slowing down the declinein BLEU values."
Boosting Point-BERT by Multi-choice Tokens,"['Kexue Fu', 'Mingzhi Yuan', 'Manning Wang']",http://arxiv.org/pdf/2207.13226v2.pdf,2022-07-27,['cs.cv'],"  Masked language modeling (MLM) has become one of the most successfulself-supervised pre-training task. Inspired by its success, Point-BERT, as apioneer work in point cloud, proposed masked point modeling (MPM) to pre-trainpoint transformer on large scale unanotated dataset. Despite its greatperformance, we find the inherent difference between language and point cloudtends to cause ambiguous tokenization for point cloud. For point cloud, theredoesn't exist a gold standard for point cloud tokenization. Point-BERT use adiscrete Variational AutoEncoder (dVAE) as tokenizer, but it might generatedifferent token ids for semantically-similar patches and generate the sametoken ids for semantically-dissimilar patches. To tackle above problem, wepropose our McP-BERT, a pre-training framework with multi-choice tokens.Specifically, we ease the previous single-choice constraint on patch token idsin Point-BERT, and provide multi-choice token ids for each patch assupervision. Moreover, we utilitze the high-level semantics learned bytransformer to further refine our supervision signals. Extensive experiments onpoint cloud classification, few-shot classification and part segmentation tasksdemonstrate the superiority of our method, e.g., the pre-trained transformerachieves 94.1% accuracy on ModelNet40, 84.28% accuracy on the hardest settingof ScanObjectNN and new state-of-the-art performance on few-shot learning. Wealso demonstrate that our method not only improves the performance ofPoint-BERT on all downstream tasks, but also incurs almost no extracomputational overhead. The code will be released inhttps://github.com/fukexue/McP-BERT."
BeCAPTCHA-Type: Biometric Keystroke Data Generation for Improved Bot  Detection,"['Daniel DeAlcala', 'Aythami Morales', 'Ruben Tolosana', 'Alejandro Acien', 'Julian Fierrez', 'Santiago Hernandez', 'Miguel A. Ferrer', 'Moises Diaz']",http://arxiv.org/pdf/2207.13394v3.pdf,2022-07-27,"['cs.lg', 'cs.cv']","  This work proposes a data driven learning model for the synthesis ofkeystroke biometric data. The proposed method is compared with two statisticalapproaches based on Universal and User-dependent models. These approaches arevalidated on the bot detection task, using the keystroke synthetic data toimprove the training process of keystroke-based bot detection systems. Ourexperimental framework considers a dataset with 136 million keystroke eventsfrom 168 thousand subjects. We have analyzed the performance of the threesynthesis approaches through qualitative and quantitative experiments.Different bot detectors are considered based on several supervised classifiers(Support Vector Machine, Random Forest, Gaussian Naive Bayes and a LongShort-Term Memory network) and a learning framework including human andsynthetic samples. The experiments demonstrate the realism of the syntheticsamples. The classification results suggest that in scenarios with largelabeled data, these synthetic samples can be detected with high accuracy.However, in few-shot learning scenarios it represents an important challenge.Furthermore, these results show the great potential of the presented models."
AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq  Model,"['Saleh Soltan', 'Shankar Ananthakrishnan', 'Jack FitzGerald', 'Rahul Gupta', 'Wael Hamza', 'Haidar Khan', 'Charith Peris', 'Stephen Rawls', 'Andy Rosenbaum', 'Anna Rumshisky', 'Chandana Satya Prakash', 'Mukund Sridhar', 'Fabian Triefenbach', 'Apurv Verma', 'Gokhan Tur', 'Prem Natarajan']",http://arxiv.org/pdf/2208.01448v2.pdf,2022-08-02,"['cs.cl', 'cs.lg']","  In this work, we demonstrate that multilingual large-scalesequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoisingand Causal Language Modeling (CLM) tasks, are more efficient few-shot learnersthan decoder-only models on various tasks. In particular, we train a 20 billionparameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B)and show that it achieves state-of-the-art (SOTA) performance on 1-shotsummarization tasks, outperforming a much larger 540B PaLM decoder model.AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially forlow-resource languages, across almost all language pairs supported by the model(Arabic, English, French, German, Hindi, Italian, Japanese, Marathi,Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show inzero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2datasets and provides SOTA performance on multilingual tasks such as XNLI,XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling casefor seq2seq models as a powerful alternative to decoder-only models forLarge-scale Language Model (LLM) training."
A Game-Theoretic Perspective of Generalization in Reinforcement Learning,"['Chang Yang', 'Ruiyu Wang', 'Xinrun Wang', 'Zhen Wang']",http://arxiv.org/pdf/2208.03650v2.pdf,2022-08-07,"['cs.lg', 'cs.gt']","  Generalization in reinforcement learning (RL) is of importance for realdeployment of RL algorithms. Various schemes are proposed to address thegeneralization issues, including transfer learning, multi-task learning andmeta learning, as well as the robust and adversarial reinforcement learning.However, there is not a unified formulation of the various schemes, as well asthe comprehensive comparisons of methods across different schemes. In thiswork, we propose a game-theoretic framework for the generalization inreinforcement learning, named GiRL, where an RL agent is trained against anadversary over a set of tasks, where the adversary can manipulate thedistributions over tasks within a given threshold. With differentconfigurations, GiRL can reduce the various schemes mentioned above. To solveGiRL, we adapt the widely-used method in game theory, policy space responseoracle (PSRO) with the following three important modifications: i) we usemodel-agnostic meta learning (MAML) as the best-response oracle, ii) we proposea modified projected replicated dynamics, i.e., R-PRD, which ensures thecomputed meta-strategy of the adversary fall in the threshold, and iii) we alsopropose a protocol for the few-shot learning of the multiple strategies duringtesting. Extensive experiments on MuJoCo environments demonstrate that ourproposed methods can outperform existing baselines, e.g., MAML."
"Designing, Modeling, and Optimizing Data-Intensive Computing Systems",['Gagandeep Singh'],http://arxiv.org/pdf/2208.08886v1.pdf,2022-08-18,['cs.ar'],"  The cost of moving data between the memory units and the compute units is amajor contributor to the execution time and energy consumption of modernworkloads in computing systems. At the same time, we are witnessing an enormousamount of data being generated across multiple application domains. Thesetrends suggest a need for a paradigm shift towards a data-centric approachwhere computation is performed close to where the data resides. Further, adata-centric approach can enable a data-driven view where we take advantage ofvast amounts of available data to improve architectural decisions.  As a step towards modern architectures, this dissertation contributes tovarious aspects of the data-centric approach and proposes several data-drivenmechanisms.  First, we design NERO, a data-centric accelerator for a real-world weatherprediction application. Second, we explore the applicability of differentnumber formats, including fixed-point, floating-point, and posit, for differentstencil kernels. Third, we propose NAPEL, an ML-based application performanceand energy prediction framework for data-centric architectures. Fourth, wepresent LEAPER, the first use of few-shot learning to transfer FPGA-basedcomputing models across different hardware platforms and applications. Fifth,we propose Sibyl, the first reinforcement learning-based mechanism for dataplacement in hybrid storage systems.  Overall, this thesis provides two key conclusions: (1) hardware accelerationon an FPGA+HBM fabric is a promising solution to overcome the data movementbottleneck of our current computing systems; (2) data should drive system anddesign decisions by leveraging inherent data characteristics to make ourcomputing systems more efficient."
Unsupervisedly Prompting AlphaFold2 for Few-Shot Learning of Accurate  Folding Landscape and Protein Structure Prediction,"['Jun Zhang', 'Sirui Liu', 'Mengyun Chen', 'Haotian Chu', 'Min Wang', 'Zidong Wang', 'Jialiang Yu', 'Ningxi Ni', 'Fan Yu', 'Diqing Chen', 'Yi Isaac Yang', 'Boxin Xue', 'Lijiang Yang', 'Yuan Liu', 'Yi Qin Gao']",http://arxiv.org/pdf/2208.09652v2.pdf,2022-08-20,"['cs.lg', 'cs.ai', 'physics.bio-ph']","  Data-driven predictive methods which can efficiently and accurately transformprotein sequences into biologically active structures are highly valuable forscientific research and medical development. Determining accurate foldinglandscape using co-evolutionary information is fundamental to the success ofmodern protein structure prediction methods. As the state of the art,AlphaFold2 has dramatically raised the accuracy without performing explicitco-evolutionary analysis. Nevertheless, its performance still shows strongdependence on available sequence homologs. Based on the interrogation on thecause of such dependence, we presented EvoGen, a meta generative model, toremedy the underperformance of AlphaFold2 for poor MSA targets. By promptingthe model with calibrated or virtually generated homologue sequences, EvoGenhelps AlphaFold2 fold accurately in low-data regime and even achieveencouraging performance with single-sequence predictions. Being able to makeaccurate predictions with few-shot MSA not only generalizes AlphaFold2 betterfor orphan sequences, but also democratizes its use for high-throughputapplications. Besides, EvoGen combined with AlphaFold2 yields a probabilisticstructure generation method which could explore alternative conformations ofprotein sequences, and the task-aware differentiable algorithm for sequencegeneration will benefit other related tasks including protein design."
MetaRF: Differentiable Random Forest for Reaction Yield Prediction with  a Few Trails,"['Kexin Chen', 'Guangyong Chen', 'Junyou Li', 'Yuansheng Huang', 'Pheng-Ann Heng']",http://arxiv.org/pdf/2208.10083v1.pdf,2022-08-22,"['cs.lg', 'q-bio.qm']","  Artificial intelligence has deeply revolutionized the field of medicinalchemistry with many impressive applications, but the success of theseapplications requires a massive amount of training samples with high-qualityannotations, which seriously limits the wide usage of data-driven methods. Inthis paper, we focus on the reaction yield prediction problem, which assistschemists in selecting high-yield reactions in a new chemical space only with afew experimental trials. To attack this challenge, we first put forth MetaRF,an attention-based differentiable random forest model specially designed forthe few-shot yield prediction, where the attention weight of a random forest isautomatically optimized by the meta-learning framework and can be quicklyadapted to predict the performance of new reagents while given a few additionalsamples. To improve the few-shot learning performance, we further introduce adimension-reduction based sampling method to determine valuable samples to beexperimentally tested and then learned. Our methodology is evaluated on threedifferent datasets and acquires satisfactory performance on few-shotprediction. In high-throughput experimentation (HTE) datasets, the averageyield of our methodology's top 10 high-yield reactions is relatively close tothe results of ideal yield selection."
Robust Prototypical Few-Shot Organ Segmentation with Regularized  Neural-ODEs,"['Prashant Pandey', 'Mustafa Chasmai', 'Tanuj Sur', 'Brejesh Lall']",http://arxiv.org/pdf/2208.12428v3.pdf,2022-08-26,"['cs.cv', 'cs.ai']","  Despite the tremendous progress made by deep learning models in imagesemantic segmentation, they typically require large annotated examples, andincreasing attention is being diverted to problem settings like Few-ShotLearning (FSL) where only a small amount of annotation is needed forgeneralisation to novel classes. This is especially seen in medical domainswhere dense pixel-level annotations are expensive to obtain. In this paper, wepropose Regularized Prototypical Neural Ordinary Differential Equation(R-PNODE), a method that leverages intrinsic properties of Neural-ODEs,assisted and enhanced by additional cluster and consistency losses to performFew-Shot Segmentation (FSS) of organs. R-PNODE constrains support and queryfeatures from the same classes to lie closer in the representation spacethereby improving the performance over the existing Convolutional NeuralNetwork (CNN) based FSS methods. We further demonstrate that while manyexisting Deep CNN based methods tend to be extremely vulnerable to adversarialattacks, R-PNODE exhibits increased adversarial robustness for a wide array ofthese attacks. We experiment with three publicly available multi-organsegmentation datasets in both in-domain and cross-domain FSS settings todemonstrate the efficacy of our method. In addition, we perform experimentswith seven commonly used adversarial attacks in various settings to demonstrateR-PNODE's robustness. R-PNODE outperforms the baselines for FSS by significantmargins and also shows superior performance for a wide array of attacks varyingin intensity and design."
Disentangle and Remerge: Interventional Knowledge Distillation for  Few-Shot Object Detection from A Conditional Causal Perspective,"['Jiangmeng Li', 'Yanan Zhang', 'Wenwen Qiang', 'Lingyu Si', 'Chengbo Jiao', 'Xiaohui Hu', 'Changwen Zheng', 'Fuchun Sun']",http://arxiv.org/pdf/2208.12681v2.pdf,2022-08-26,['cs.cv'],"  Few-shot learning models learn representations with limited humanannotations, and such a learning paradigm demonstrates practicability invarious tasks, e.g., image classification, object detection, etc. However,few-shot object detection methods suffer from an intrinsic defect that thelimited training data makes the model cannot sufficiently explore semanticinformation. To tackle this, we introduce knowledge distillation to thefew-shot object detection learning paradigm. We further run a motivatingexperiment, which demonstrates that in the process of knowledge distillation,the empirical error of the teacher model degenerates the prediction performanceof the few-shot object detection model as the student. To understand thereasons behind this phenomenon, we revisit the learning paradigm of knowledgedistillation on the few-shot object detection task from the causal theoreticstandpoint, and accordingly, develop a Structural Causal Model. Following thetheoretical guidance, we propose a backdoor adjustment-based knowledgedistillation method for the few-shot object detection task, namely Disentangleand Remerge (D&R), to perform conditional causal intervention toward thecorresponding Structural Causal Model. Empirically, the experiments onbenchmarks demonstrate that D&R can yield significant performance boosts infew-shot object detection. Code is available athttps://github.com/ZYN-1101/DandR.git."
NeurIPS'22 Cross-Domain MetaDL competition: Design and baseline results,"['Dustin Carrión-Ojeda', 'Hong Chen', 'Adrian El Baz', 'Sergio Escalera', 'Chaoyu Guan', 'Isabelle Guyon', 'Ihsan Ullah', 'Xin Wang', 'Wenwu Zhu']",http://arxiv.org/pdf/2208.14686v1.pdf,2022-08-31,"['cs.lg', 'cs.ai', 'cs.cv', 'cs.ne']","  We present the design and baseline results for a new challenge in theChaLearn meta-learning series, accepted at NeurIPS'22, focusing on""cross-domain"" meta-learning. Meta-learning aims to leverage experience gainedfrom previous tasks to solve new tasks efficiently (i.e., with betterperformance, little training data, and/or modest computational resources).While previous challenges in the series focused on within-domain few-shotlearning problems, with the aim of learning efficiently N-way k-shot tasks(i.e., N class classification problems with k training examples), thiscompetition challenges the participants to solve ""any-way"" and ""any-shot""problems drawn from various domains (healthcare, ecology, biology,manufacturing, and others), chosen for their humanitarian and societal impact.To that end, we created Meta-Album, a meta-dataset of 40 image classificationdatasets from 10 domains, from which we carve out tasks with any number of""ways"" (within the range 2-20) and any number of ""shots"" (within the range1-20). The competition is with code submission, fully blind-tested on theCodaLab challenge platform. The code of the winners will be open-sourced,enabling the deployment of automated machine learning solutions for few-shotimage classification across several domains."
Data-Driven Target Localization Using Adaptive Radar Processing and  Convolutional Neural Networks,"['Shyam Venkatasubramanian', 'Sandeep Gogineni', 'Bosung Kang', 'Ali Pezeshki', 'Muralidhar Rangaswamy', 'Vahid Tarokh']",http://arxiv.org/pdf/2209.02890v5.pdf,2022-09-07,"['cs.cv', 'eess.sp']","  Leveraging the advanced functionalities of modern radio frequency (RF)modeling and simulation tools, specifically designed for adaptive radarprocessing applications, this paper presents a data-driven approach to improveaccuracy in radar target localization post adaptive radar detection. To thisend, we generate a large number of radar returns by randomly placing targets ofvariable strengths in a predefined area, using RFView, a high-fidelity,site-specific, RF modeling & simulation tool. We produce heatmap tensors fromthe radar returns, in range, azimuth [and Doppler], of the normalized adaptivematched filter (NAMF) test statistic. We then train a regression convolutionalneural network (CNN) to estimate target locations from these heatmap tensors,and we compare the target localization accuracy of this approach with that ofpeak-finding and local search methods. This empirical study shows that ourregression CNN achieves a considerable improvement in target locationestimation accuracy. The regression CNN offers significant gains and reasonableaccuracy even at signal-to-clutter-plus-noise ratio (SCNR) regimes that areclose to the breakdown threshold SCNR of the NAMF. We also study the robustnessof our trained CNN to mismatches in the radar data, where the CNN is tested onheatmap tensors collected from areas that it was not trained on. We show thatour CNN can be made robust to mismatches in the radar data through few-shotlearning, using a relatively small number of new training samples."
Adaptive Dimension Reduction and Variational Inference for Transductive  Few-Shot Classification,"['Yuqing Hu', 'Stéphane Pateux', 'Vincent Gripon']",http://arxiv.org/pdf/2209.08527v1.pdf,2022-09-18,"['cs.lg', 'cs.cv']","  Transductive Few-Shot learning has gained increased attention nowadaysconsidering the cost of data annotations along with the increased accuracyprovided by unlabelled samples in the domain of few shot. Especially inFew-Shot Classification (FSC), recent works explore the feature distributionsaiming at maximizing likelihoods or posteriors with respect to the unknownparameters. Following this vein, and considering the parallel between FSC andclustering, we seek for better taking into account the uncertainty inestimation due to lack of data, as well as better statistical properties of theclusters associated with each class. Therefore in this paper we propose a newclustering method based on Variational Bayesian inference, further improved byAdaptive Dimension Reduction based on Probabilistic Linear DiscriminantAnalysis. Our proposed method significantly improves accuracy in the realisticunbalanced transductive setting on various Few-Shot benchmarks when applied tofeatures used in previous studies, with a gain of up to $6\%$ in accuracy. Inaddition, when applied to balanced setting, we obtain very competitive resultswithout making use of the class-balance artefact which is disputable forpractical use cases. We also provide the performance of our method on a highperforming pretrained backbone, with the reported results further surpassingthe current state-of-the-art accuracy, suggesting the genericity of theproposed method."
Automatic Label Sequence Generation for Prompting Sequence-to-sequence  Models,"['Zichun Yu', 'Tianyu Gao', 'Zhengyan Zhang', 'Yankai Lin', 'Zhiyuan Liu', 'Maosong Sun', 'Jie Zhou']",http://arxiv.org/pdf/2209.09401v1.pdf,2022-09-20,"['cs.cl', 'cs.lg']","  Prompting, which casts downstream applications as language modeling tasks,has shown to be sample efficient compared to standard fine-tuning withpre-trained models. However, one pitfall of prompting is the need ofmanually-designed patterns, whose outcome can be unintuitive and requires largevalidation sets to tune. To tackle the challenge, we propose AutoSeq, a fullyautomatic prompting method: (1) We adopt natural language prompts onsequence-to-sequence models, enabling free-form generation and larger labelsearch space; (2) We propose label sequences -- phrases with indefinite lengthsto verbalize the labels -- which eliminate the need of manual templates and aremore expressive than single label words; (3) We use beam search toautomatically generate a large amount of label sequence candidates and proposecontrastive re-ranking to get the best combinations. AutoSeq significantlyoutperforms other no-manual-design methods, such as soft prompt tuning, adaptertuning, and automatic search on single label words; the generated labelsequences are even better than curated manual ones on a variety of tasks. Ourmethod reveals the potential of sequence-to-sequence models in few-shotlearning and sheds light on a path to generic and automatic prompting. Thesource code of this paper can be obtained fromhttps://github.com/thunlp/Seq2Seq-Prompt."
AirFi: Empowering WiFi-based Passive Human Gesture Recognition to Unseen  Environment via Domain Generalization,"['Dazhuo Wang', 'Jianfei Yang', 'Wei Cui', 'Lihua Xie', 'Sumei Sun']",http://arxiv.org/pdf/2209.10285v2.pdf,2022-09-21,"['cs.cv', 'cs.hc']","  WiFi-based smart human sensing technology enabled by Channel StateInformation (CSI) has received great attention in recent years. However,CSI-based sensing systems suffer from performance degradation when deployed indifferent environments. Existing works solve this problem by domain adaptationusing massive unlabeled high-quality data from the new environment, which isusually unavailable in practice. In this paper, we propose a novel augmentedenvironment-invariant robust WiFi gesture recognition system named AirFi thatdeals with the issue of environment dependency from a new perspective. TheAirFi is a novel domain generalization framework that learns the critical partof CSI regardless of different environments and generalizes the model to unseenscenarios, which does not require collecting any data for adaptation to the newenvironment. AirFi extracts the common features from several trainingenvironment settings and minimizes the distribution differences among them. Thefeature is further augmented to be more robust to environments. Moreover, thesystem can be further improved by few-shot learning techniques. Compared tostate-of-the-art methods, AirFi is able to work in different environmentsettings without acquiring any CSI data from the new environment. Theexperimental results demonstrate that our system remains robust in the newenvironment and outperforms the compared systems."
Collaboration of Pre-trained Models Makes Better Few-shot Learner,"['Renrui Zhang', 'Bohao Li', 'Wei Zhang', 'Hao Dong', 'Hongsheng Li', 'Peng Gao', 'Yu Qiao']",http://arxiv.org/pdf/2209.12255v2.pdf,2022-09-25,['cs.cv'],"  Few-shot classification requires deep neural networks to learn generalizedrepresentations only from limited training images, which is challenging butsignificant in low-data regimes. Recently, CLIP-based methods have shownpromising few-shot performance benefited from the contrastive language-imagepre-training. Based on this point, we question if the large-scale pre-trainingcan alleviate the few-shot data deficiency and also assist the representationlearning by the pre-learned knowledge. In this paper, we propose CoMo, aCollaboration of pre-trained Models that incorporates diverse prior knowledgefrom various pre-training paradigms for better few-shot learning. Our CoMoincludes: CLIP's language-contrastive knowledge, DINO's vision-contrastiveknowledge, and DALL-E's language-generative knowledge. Specifically, CoMo worksin two aspects: few-shot data expansion and diverse knowledge ensemble. Forone, we generate synthetic images via zero-shot DALL-E to enrich the few-shottraining data without any manpower. For the other, we introduce a learnableMulti-Knowledge Adapter (MK-Adapter) to adaptively blend the predictions fromCLIP and DINO. By such collaboration, CoMo can fully unleash the potential ofdifferent pre-training methods and unify them to perform state-of-the-art forfew-shot classification. We conduct extensive experiments on 11 datasets todemonstrate the superiority and generalization ability of our approach."
Learnable Distribution Calibration for Few-Shot Class-Incremental  Learning,"['Binghao Liu', 'Boyu Yang', 'Lingxi Xie', 'Ren Wang', 'Qi Tian', 'Qixiang Ye']",http://arxiv.org/pdf/2210.00232v1.pdf,2022-10-01,['cs.cv'],"  Few-shot class-incremental learning (FSCIL) faces challenges of memorizingold class distributions and estimating new class distributions given fewtraining samples. In this study, we propose a learnable distributioncalibration (LDC) approach, with the aim to systematically solve these twochallenges using a unified framework. LDC is built upon a parameterizedcalibration unit (PCU), which initializes biased distributions for all classesbased on classifier vectors (memory-free) and a single covariance matrix. Thecovariance matrix is shared by all classes, so that the memory costs are fixed.During base training, PCU is endowed with the ability to calibrate biaseddistributions by recurrently updating sampled features under the supervision ofreal distributions. During incremental learning, PCU recovers distributions forold classes to avoid `forgetting', as well as estimating distributions andaugmenting samples for new classes to alleviate `over-fitting' caused by thebiased distributions of few-shot samples. LDC is theoretically plausible byformatting a variational inference procedure. It improves FSCIL's flexibilityas the training procedure requires no class similarity priori. Experiments onCUB200, CIFAR100, and mini-ImageNet datasets show that LDC outperforms thestate-of-the-arts by 4.64%, 1.98%, and 3.97%, respectively. LDC's effectivenessis also validated on few-shot learning scenarios."
CLIP2Point: Transfer CLIP to Point Cloud Classification with Image-Depth  Pre-training,"['Tianyu Huang', 'Bowen Dong', 'Yunhan Yang', 'Xiaoshui Huang', 'Rynson W. H. Lau', 'Wanli Ouyang', 'Wangmeng Zuo']",http://arxiv.org/pdf/2210.01055v3.pdf,2022-10-03,['cs.cv'],"  Pre-training across 3D vision and language remains under development becauseof limited training data. Recent works attempt to transfer vision-languagepre-training models to 3D vision. PointCLIP converts point cloud data tomulti-view depth maps, adopting CLIP for shape classification. However, itsperformance is restricted by the domain gap between rendered depth maps andimages, as well as the diversity of depth distributions. To address this issue,we propose CLIP2Point, an image-depth pre-training method by contrastivelearning to transfer CLIP to the 3D domain, and adapt it to point cloudclassification. We introduce a new depth rendering setting that forms a bettervisual effect, and then render 52,460 pairs of images and depth maps fromShapeNet for pre-training. The pre-training scheme of CLIP2Point combinescross-modality learning to enforce the depth features for capturing expressivevisual and textual features and intra-modality learning to enhance theinvariance of depth aggregation. Additionally, we propose a novel Dual-PathAdapter (DPA) module, i.e., a dual-path structure with simplified adapters forfew-shot learning. The dual-path structure allows the joint use of CLIP andCLIP2Point, and the simplified adapter can well fit few-shot tasks withoutpost-search. Experimental results show that CLIP2Point is effective intransferring CLIP knowledge to 3D vision. Our CLIP2Point outperforms PointCLIPand other self-supervised 3D networks, achieving state-of-the-art results onzero-shot and few-shot classification."
Boosting Few-shot Fine-grained Recognition with Background Suppression  and Foreground Alignment,"['Zican Zha', 'Hao Tang', 'Yunlian Sun', 'Jinhui Tang']",http://arxiv.org/pdf/2210.01439v2.pdf,2022-10-04,['cs.cv'],"  Few-shot fine-grained recognition (FS-FGR) aims to recognize novelfine-grained categories with the help of limited available samples.Undoubtedly, this task inherits the main challenges from both few-shot learningand fine-grained recognition. First, the lack of labeled samples makes thelearned model easy to overfit. Second, it also suffers from high intra-classvariance and low inter-class differences in the datasets. To address thischallenging task, we propose a two-stage background suppression and foregroundalignment framework, which is composed of a background activation suppression(BAS) module, a foreground object alignment (FOA) module, and a local-to-local(L2L) similarity metric. Specifically, the BAS is introduced to generate aforeground mask for localization to weaken background disturbance and enhancedominative foreground objects. The FOA then reconstructs the feature map ofeach support sample according to its correction to the query ones, whichaddresses the problem of misalignment between support-query image pairs. Toenable the proposed method to have the ability to capture subtle differences inconfused samples, we present a novel L2L similarity metric to further measurethe local similarity between a pair of aligned spatial features in theembedding space. What's more, considering that background interference bringspoor robustness, we infer the pairwise similarity of feature maps using boththe raw image and the refined image. Extensive experiments conducted onmultiple popular fine-grained benchmarks demonstrate that our methodoutperforms the existing state of the art by a large margin. The source codesare available at: https://github.com/CSer-Tang-hao/BSFA-FSFG."
Uncertainty-Aware Meta-Learning for Multimodal Task Distributions,"['Cesar Almecija', 'Apoorva Sharma', 'Navid Azizan']",http://arxiv.org/pdf/2210.01881v1.pdf,2022-10-04,"['cs.lg', 'cs.ai']","  Meta-learning or learning to learn is a popular approach for learning newtasks with limited data (i.e., few-shot learning) by leveraging thecommonalities among different tasks. However, meta-learned models can performpoorly when context data is limited, or when data is drawn from anout-of-distribution (OoD) task. Especially in safety-critical settings, thisnecessitates an uncertainty-aware approach to meta-learning. In addition, theoften multimodal nature of task distributions can pose unique challenges tometa-learning methods. In this work, we present UnLiMiTD (uncertainty-awaremeta-learning for multimodal task distributions), a novel method formeta-learning that (1) makes probabilistic predictions on in-distribution tasksefficiently, (2) is capable of detecting OoD context data at test time, and (3)performs on heterogeneous, multimodal task distributions. To achieve this goal,we take a probabilistic perspective and train a parametric, tuneabledistribution over tasks on the meta-dataset. We construct this distribution byperforming Bayesian inference on a linearized neural network, leveragingGaussian process theory. We demonstrate that UnLiMiTD's predictions comparefavorably to, and outperform in most cases, the standard baselines, especiallyin the low-data regime. Furthermore, we show that UnLiMiTD is effective indetecting data from OoD tasks. Finally, we confirm that both of these findingscontinue to hold in the multimodal task-distribution setting."
Adversarially Robust Prototypical Few-shot Segmentation with Neural-ODEs,"['Prashant Pandey', 'Aleti Vardhan', 'Mustafa Chasmai', 'Tanuj Sur', 'Brejesh Lall']",http://arxiv.org/pdf/2210.03429v1.pdf,2022-10-07,['cs.cv'],"  Few-shot Learning (FSL) methods are being adopted in settings where data isnot abundantly available. This is especially seen in medical domains where theannotations are expensive to obtain. Deep Neural Networks have been shown to bevulnerable to adversarial attacks. This is even more severe in the case of FSLdue to the lack of a large number of training examples. In this paper, weprovide a framework to make few-shot segmentation models adversarially robustin the medical domain where such attacks can severely impact the decisions madeby clinicians who use them. We propose a novel robust few-shot segmentationframework, Prototypical Neural Ordinary Differential Equation (PNODE), thatprovides defense against gradient-based adversarial attacks. We show that ourframework is more robust compared to traditional adversarial defense mechanismssuch as adversarial training. Adversarial training involves increased trainingtime and shows robustness to limited types of attacks depending on the type ofadversarial examples seen during training. Our proposed framework generaliseswell to common adversarial attacks like FGSM, PGD and SMIA while having themodel parameters comparable to the existing few-shot segmentation models. Weshow the effectiveness of our proposed approach on three publicly availablemulti-organ segmentation datasets in both in-domain and cross-domain settingsby attacking the support and query sets without the need for ad-hoc adversarialtraining."
Adaptive Distribution Calibration for Few-Shot Learning with  Hierarchical Optimal Transport,"['Dandan Guo', 'Long Tian', 'He Zhao', 'Mingyuan Zhou', 'Hongyuan Zha']",http://arxiv.org/pdf/2210.04144v1.pdf,2022-10-09,"['cs.lg', 'cs.cv']","  Few-shot classification aims to learn a classifier to recognize unseenclasses during training, where the learned model can easily become over-fittedbased on the biased distribution formed by only a few training examples. Arecent solution to this problem is calibrating the distribution of these fewsample classes by transferring statistics from the base classes with sufficientexamples, where how to decide the transfer weights from base classes to novelclasses is the key. However, principled approaches for learning the transferweights have not been carefully studied. To this end, we propose a noveldistribution calibration method by learning the adaptive weight matrix betweennovel samples and base classes, which is built upon a hierarchical OptimalTransport (H-OT) framework. By minimizing the high-level OT distance betweennovel samples and base classes, we can view the learned transport plan as theadaptive weight information for transferring the statistics of base classes.The learning of the cost function between a base class and novel class in thehigh-level OT leads to the introduction of the low-level OT, which considersthe weights of all the data samples in the base class. Experimental results onstandard benchmarks demonstrate that our proposed plug-and-play modeloutperforms competing approaches and owns desired cross-domain generalizationability, indicating the effectiveness of the learned adaptive weights."
Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis,"['Siddharth Varia', 'Shuai Wang', 'Kishaloy Halder', 'Robert Vacareanu', 'Miguel Ballesteros', 'Yassine Benajiba', 'Neha Anna John', 'Rishita Anubhai', 'Smaranda Muresan', 'Dan Roth']",http://arxiv.org/pdf/2210.06629v2.pdf,2022-10-12,['cs.cl'],"  Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment analysistask which involves four elements from user-generated texts: aspect term,aspect category, opinion term, and sentiment polarity. Most computationalapproaches focus on some of the ABSA sub-tasks such as tuple (aspect term,sentiment polarity) or triplet (aspect term, opinion term, sentiment polarity)extraction using either pipeline or joint modeling approaches. Recently,generative approaches have been proposed to extract all four elements as (oneor more) quadruplets from text as a single task. In this work, we take a stepfurther and propose a unified framework for solving ABSA, and the associatedsub-tasks to improve the performance in few-shot scenarios. To this end, wefine-tune a T5 model with instructional prompts in a multi-task learningfashion covering all the sub-tasks, as well as the entire quadruple predictiontask. In experiments with multiple benchmark datasets, we show that theproposed multi-task prompting approach brings performance boost (by absolute8.29 F1) in the few-shot learning setting."
Few-Shot Visual Question Generation: A Novel Task and Benchmark Datasets,"['Anurag Roy', 'David Johnson Ekka', 'Saptarshi Ghosh', 'Abir Das']",http://arxiv.org/pdf/2210.07076v2.pdf,2022-10-13,['cs.cv'],"  Generating natural language questions from visual scenes, known as VisualQuestion Generation (VQG), has been explored in the recent past where largeamounts of meticulously labeled data provide the training corpus. However, inpractice, it is not uncommon to have only a few images with questionannotations corresponding to a few types of answers. In this paper, we proposea new and challenging Few-Shot Visual Question Generation (FS-VQG) task andprovide a comprehensive benchmark to it. Specifically, we evaluate variousexisting VQG approaches as well as popular few-shot solutions based onmeta-learning and self-supervised strategies for the FS-VQG task. We conductexperiments on two popular existing datasets VQG and Visual7w. In addition, wehave also cleaned and extended the VQG dataset for use in a few-shot scenario,with additional image-question pairs as well as additional answer categories.We call this new dataset VQG-23. Several important findings emerge from ourexperiments, that shed light on the limits of current models in few-shot visionand language generation tasks. We find that trivially extending existing VQGapproaches with transfer learning or meta-learning may not be enough to tacklethe inherent challenges in few-shot VQG. We believe that this work willcontribute to accelerating the progress in few-shot learning research."
"RARR: Researching and Revising What Language Models Say, Using Language  Models","['Luyu Gao', 'Zhuyun Dai', 'Panupong Pasupat', 'Anthony Chen', 'Arun Tejasvi Chaganty', 'Yicheng Fan', 'Vincent Y. Zhao', 'Ni Lao', 'Hongrae Lee', 'Da-Cheng Juan', 'Kelvin Guu']",http://arxiv.org/pdf/2210.08726v3.pdf,2022-10-17,"['cs.cl', 'cs.ai', 'cs.ir', 'cs.lg']","  Language models (LMs) now excel at many tasks such as few-shot learning,question answering, reasoning, and dialog. However, they sometimes generateunsupported or misleading content. A user cannot easily determine whether theiroutputs are trustworthy or not, because most LMs do not have any built-inmechanism for attribution to external evidence. To enable attribution whilestill preserving all the powerful advantages of recent generation models, wepropose RARR (Retrofit Attribution using Research and Revision), a system that1) automatically finds attribution for the output of any text generation modeland 2) post-edits the output to fix unsupported content while preserving theoriginal output as much as possible. When applied to the output of severalstate-of-the-art LMs on a diverse set of generation tasks, we find that RARRsignificantly improves attribution while otherwise preserving the originalinput to a much greater degree than previously explored edit models.Furthermore, the implementation of RARR requires only a handful of trainingexamples, a large language model, and standard web search."
HCL-TAT: A Hybrid Contrastive Learning Method for Few-shot Event  Detection with Task-Adaptive Threshold,"['Ruihan Zhang', 'Wei Wei', 'Xian-Ling Mao', 'Rui Fang', 'Dangyang Chen']",http://arxiv.org/pdf/2210.08806v2.pdf,2022-10-17,['cs.cl'],"  Conventional event detection models under supervised learning settings sufferfrom the inability of transfer to newly-emerged event types owing to lack ofsufficient annotations. A commonly-adapted solution is to follow aidentify-then-classify manner, which first identifies the triggers and thenconverts the classification task via a few-shot learning paradigm. However,these methods still fall far short of expectations due to: (i) insufficientlearning of discriminative representations in low-resource scenarios, and (ii)trigger misidentification caused by the overlap of the learned representationsof triggers and non-triggers. To address the problems, in this paper, wepropose a novel Hybrid Contrastive Learning method with a Task-AdaptiveThreshold (abbreviated as HCLTAT), which enables discriminative representationlearning with a two-view contrastive loss (support-support andprototype-query), and devises a easily-adapted threshold to alleviatemisidentification of triggers. Extensive experiments on the benchmark datasetFewEvent demonstrate the superiority of our method to achieve better resultscompared to the state-of-the-arts. All the code and data of this paper will beavailable for online public access."
TAPE: Assessing Few-shot Russian Language Understanding,"['Ekaterina Taktasheva', 'Tatiana Shavrina', 'Alena Fenogenova', 'Denis Shevelev', 'Nadezhda Katricheva', 'Maria Tikhonova', 'Albina Akhmetgareeva', 'Oleg Zinkevich', 'Anastasiia Bashmakova', 'Svetlana Iordanskaia', 'Alena Spiridonova', 'Valentina Kurenshchikova', 'Ekaterina Artemova', 'Vladislav Mikhailov']",http://arxiv.org/pdf/2210.12813v1.pdf,2022-10-23,['cs.cl'],"  Recent advances in zero-shot and few-shot learning have shown promise for ascope of research and practical purposes. However, this fast-growing area lacksstandardized evaluation suites for non-English languages, hindering progressoutside the Anglo-centric paradigm. To address this line of research, wepropose TAPE (Text Attack and Perturbation Evaluation), a novel benchmark thatincludes six more complex NLU tasks for Russian, covering multi-hop reasoning,ethical concepts, logic and commonsense knowledge. The TAPE's design focuses onsystematic zero-shot and few-shot NLU evaluation: (i) linguistic-orientedadversarial attacks and perturbations for analyzing robustness, and (ii)subpopulations for nuanced interpretation. The detailed analysis of testing theautoregressive baselines indicates that simple spelling-based perturbationsaffect the performance the most, while paraphrasing the input has a morenegligible effect. At the same time, the results demonstrate a significant gapbetween the neural and human baselines for most tasks. We publicly release TAPE(tape-benchmark.com) to foster research on robust LMs that can generalize tonew tasks when little to no supervision is available."
Learning New Tasks from a Few Examples with Soft-Label Prototypes,"['Avyav Kumar Singh', 'Ekaterina Shutova', 'Helen Yannakoudakis']",http://arxiv.org/pdf/2210.17437v2.pdf,2022-10-31,"['cs.lg', 'cs.cl']","  It has been experimentally demonstrated that humans are able to learn in amanner that allows them to make predictions on categories for which they havenot seen any examples (Malaviya et al., 2022). Sucholutsky and Schonlau (2020)have recently presented a machine learning approach that aims to do the same.They utilise synthetically generated data and demonstrate that it is possibleto achieve sub-linear scaling and develop models that can learn to recognise Nclasses from M training samples where M is less than N - aka less-than-one shotlearning. Their method was, however, defined for univariate or simplemultivariate data (Sucholutsky et al., 2021). We extend it to work on large,high-dimensional and real-world datasets and empirically validate it in thisnew and challenging setting. We apply this method to learn previously unseenNLP tasks from very few examples (4, 8 or 16). We first generate compact,sophisticated less-than-one shot representations called soft-label prototypeswhich are fitted on training data, capturing the distribution of differentclasses across the input domain space. We then use a modified k-NearestNeighbours classifier to demonstrate that soft-label prototypes can classifydata competitively, even outperforming much more computationally complexfew-shot learning methods."
SAGE: Saliency-Guided Mixup with Optimal Rearrangements,"['Avery Ma', 'Nikita Dvornik', 'Ran Zhang', 'Leila Pishdad', 'Konstantinos G. Derpanis', 'Afsaneh Fazly']",http://arxiv.org/pdf/2211.00113v1.pdf,2022-10-31,"['cs.lg', 'cs.cv']","  Data augmentation is a key element for training accurate models by reducingoverfitting and improving generalization. For image classification, the mostpopular data augmentation techniques range from simple photometric andgeometrical transformations, to more complex methods that use visual saliencyto craft new training examples. As augmentation methods get more complex, theirability to increase the test accuracy improves, yet, such methods becomecumbersome, inefficient and lead to poor out-of-domain generalization, as weshow in this paper. This motivates a new augmentation technique that allows forhigh accuracy gains while being simple, efficient (i.e., minimal computationoverhead) and generalizable. To this end, we introduce Saliency-Guided Mixupwith Optimal Rearrangements (SAGE), which creates new training examples byrearranging and mixing image pairs using visual saliency as guidance. Byexplicitly leveraging saliency, SAGE promotes discriminative foreground objectsand produces informative new images useful for training. We demonstrate onCIFAR-10 and CIFAR-100 that SAGE achieves better or comparable performance tothe state of the art while being more efficient. Additionally, evaluations inthe out-of-distribution setting, and few-shot learning on mini-ImageNet, showthat SAGE achieves improved generalization performance without trading offrobustness."
On the Informativeness of Supervision Signals,"['Ilia Sucholutsky', 'Ruairidh M. Battleday', 'Katherine M. Collins', 'Raja Marjieh', 'Joshua C. Peterson', 'Pulkit Singh', 'Umang Bhatt', 'Nori Jacoby', 'Adrian Weller', 'Thomas L. Griffiths']",http://arxiv.org/pdf/2211.01407v3.pdf,2022-11-02,"['cs.lg', 'cs.ai']","  Supervised learning typically focuses on learning transferablerepresentations from training examples annotated by humans. While richannotations (like soft labels) carry more information than sparse annotations(like hard labels), they are also more expensive to collect. For example, whilehard labels only provide information about the closest class an object belongsto (e.g., ""this is a dog""), soft labels provide information about the object'srelationship with multiple classes (e.g., ""this is most likely a dog, but itcould also be a wolf or a coyote""). We use information theory to compare how anumber of commonly-used supervision signals contribute torepresentation-learning performance, as well as how their capacity is affectedby factors such as the number of labels, classes, dimensions, and noise. Ourframework provides theoretical justification for using hard labels in thebig-data regime, but richer supervision signals for few-shot learning andout-of-distribution generalization. We validate these results empirically in aseries of experiments with over 1 million crowdsourced image annotations andconduct a cost-benefit analysis to establish a tradeoff curve that enablesusers to optimize the cost of supervising representation learning on their owndatasets."
Point-DAE: Denoising Autoencoders for Self-supervised Point Cloud  Learning,"['Yabin Zhang', 'Jiehong Lin', 'Ruihuang Li', 'Kui Jia', 'Lei Zhang']",http://arxiv.org/pdf/2211.06841v3.pdf,2022-11-13,"['cs.cv', 'cs.ai']","  Masked autoencoder has demonstrated its effectiveness in self-supervisedpoint cloud learning. Considering that masking is a kind of corruption, in thiswork we explore a more general denoising autoencoder for point cloud learning(Point-DAE) by investigating more types of corruptions beyond masking.Specifically, we degrade the point cloud with certain corruptions as input, andlearn an encoder-decoder model to reconstruct the original point cloud from itscorrupted version. Three corruption families (\ie, density/masking, noise, andaffine transformation) and a total of fourteen corruption types areinvestigated with traditional non-Transformer encoders. Besides the popularmasking corruption, we identify another effective corruption family, \ie,affine transformation. The affine transformation disturbs all points globally,which is complementary to the masking corruption where some local regions aredropped. We also validate the effectiveness of affine transformation corruptionwith the Transformer backbones, where we decompose the reconstruction of thecomplete point cloud into the reconstructions of detailed local patches andrough global shape, alleviating the position leakage problem in thereconstruction. Extensive experiments on tasks of object classification,few-shot learning, robustness testing, part segmentation, and 3D objectdetection validate the effectiveness of the proposed method. The codes areavailable at \url{https://github.com/YBZh/Point-DAE}."
QAmeleon: Multilingual QA with Only 5 Examples,"['Priyanka Agrawal', 'Chris Alberti', 'Fantine Huot', 'Joshua Maynez', 'Ji Ma', 'Sebastian Ruder', 'Kuzman Ganchev', 'Dipanjan Das', 'Mirella Lapata']",http://arxiv.org/pdf/2211.08264v2.pdf,2022-11-15,['cs.cl'],"  The availability of large, high-quality datasets has been one of the maindrivers of recent progress in question answering (QA). Such annotated datasetshowever are difficult and costly to collect, and rarely exist in languagesother than English, rendering QA technology inaccessible to underrepresentedlanguages. An alternative to building large monolingual training datasets is toleverage pre-trained language models (PLMs) under a few-shot learning setting.Our approach, QAmeleon, uses a PLM to automatically generate multilingual dataupon which QA models are trained, thus avoiding costly annotation. Prompttuning the PLM for data synthesis with only five examples per language deliversaccuracy superior to translation-based baselines, bridges nearly 60% of the gapbetween an English-only baseline and a fully supervised upper bound trained onalmost 50,000 hand labeled examples, and always leads to substantialimprovements compared to fine-tuning a QA model directly on labeled examples inlow resource settings. Experiments on the TyDiQA-GoldP and MLQA benchmarks showthat few-shot prompt tuning for data synthesis scales across languages and is aviable alternative to large-scale annotation."
Explicit Knowledge Transfer for Weakly-Supervised Code Generation,"['Zhangir Azerbayev', 'Ansong Ni', 'Hailey Schoelkopf', 'Dragomir Radev']",http://arxiv.org/pdf/2211.16740v3.pdf,2022-11-30,['cs.cl'],"  Large language models (LLMs) can acquire strong code-generation capabilitiesthrough few-shot learning. In contrast, supervised fine-tuning is still neededfor smaller models to achieve good performance. Such fine-tuning demands alarge number of task-specific NL-code pairs, which are expensive to obtain. Inthis paper, we attempt to transfer the code generation ability of an LLM to asmaller model with the aid of weakly-supervised data. More specifically, wepropose explicit knowledge transfer (EKT), which uses the few-shot capabilitiesof a teacher LLM to create NL-code pairs that we then filter for correctnessand fine-tune the student on. We evaluate EKT on the task of generating codesolutions to math word problems from the GSM8k dataset. We find that EKT notonly yields better performance than training with expert iteration, but alsooutperforms knowledge distillation, another form of knowledge transfer. AGPT-Neo 1.3B model trained using EKT with a GPT-J teacher achieves a 12.4%pass@100 on GSM8k, while the same student and teacher trained with knowledgedistillation yield only a 3.7% pass@100. We also show that it is possible for astudent model to outperform the teacher using EKT."
Bi-directional Feature Reconstruction Network for Fine-Grained Few-Shot  Image Classification,"['Jijie Wu', 'Dongliang Chang', 'Aneeshan Sain', 'Xiaoxu Li', 'Zhanyu Ma', 'Jie Cao', 'Jun Guo', 'Yi-Zhe Song']",http://arxiv.org/pdf/2211.17161v2.pdf,2022-11-30,['cs.cv'],"  The main challenge for fine-grained few-shot image classification is to learnfeature representations with higher inter-class and lower intra-classvariations, with a mere few labelled samples. Conventional few-shot learningmethods however cannot be naively adopted for this fine-grained setting -- aquick pilot study reveals that they in fact push for the opposite (i.e., lowerinter-class variations and higher intra-class variations). To alleviate thisproblem, prior works predominately use a support set to reconstruct the queryimage and then utilize metric learning to determine its category. Upon carefulinspection, we further reveal that such unidirectional reconstruction methodsonly help to increase inter-class variations and are not effective in tacklingintra-class variations. In this paper, we for the first time introduce abi-reconstruction mechanism that can simultaneously accommodate for inter-classand intra-class variations. In addition to using the support set to reconstructthe query set for increasing inter-class variations, we further use the queryset to reconstruct the support set for reducing intra-class variations. Thisdesign effectively helps the model to explore more subtle and discriminativefeatures which is key for the fine-grained problem in hand. Furthermore, wealso construct a self-reconstruction module to work alongside thebi-directional module to make the features even more discriminative.Experimental results on three widely used fine-grained image classificationdatasets consistently show considerable improvements compared with othermethods. Codes are available at: https://github.com/PRIS-CV/Bi-FRN."
Can In-context Learners Learn a Reasoning Concept from Demonstrations?,"['Michal Štefánik', 'Marek Kadlčík']",http://arxiv.org/pdf/2212.01692v4.pdf,2022-12-03,"['cs.cl', 'cs.ai', 'cs.lg']","  Language models exhibit an emergent ability to learn a new task from a smallnumber of input-output demonstrations. However, recent work shows thatin-context learners largely rely on their pre-trained knowledge, such as thesentiment of the labels, instead of learning new associations from the input.We argue that the commonly-used few-shot evaluation using a random selection ofin-context demonstrations can not disentangle models' reliance on such biases,as most of the randomly-selected demonstrations do not present relationsinformative for prediction beyond exposing the task's input-outputdistribution.  Therefore, to evaluate models' in-context learning ability independent ofmodels' memory, we introduce a Concept-sharing few-shot learning methodchoosing the demonstrations that share an underlying concept with the predictedsample. We extract a set of such concepts from available human explanations andmeasure how much models can benefit from presenting these concepts in few-shotdemonstrations.  We find that most of the recent in-context learners can not consistentlybenefit from the demonstrated concepts, irrespective of the model size.However, we note that T0 models are more sensitive to exhibited concepts,benefiting from concept-sharing demonstrations in 7 out of 8 evaluationscenarios."
"Towards Automatic Cetacean Photo-Identification: A Framework for  Fine-Grain, Few-Shot Learning in Marine Ecology","['Cameron Trotter', 'Nick Wright', 'A. Stephen McGough', 'Matt Sharpe', 'Barbara Cheney', 'Mònica Arso Civil', 'Reny Tyson Moore', 'Jason Allen', 'Per Berggren']",http://arxiv.org/pdf/2212.03646v1.pdf,2022-12-07,"['cs.cv', 'cs.ai', 'cs.lg']","  Photo-identification (photo-id) is one of the main non-invasivecapture-recapture methods utilised by marine researchers for monitoringcetacean (dolphin, whale, and porpoise) populations. This method hashistorically been performed manually resulting in high workload and cost due tothe vast number of images collected. Recently automated aids have beendeveloped to help speed-up photo-id, although they are often disjoint in theirprocessing and do not utilise all available identifying information. Workpresented in this paper aims to create a fully automatic photo-id aid capableof providing most likely matches based on all available information without theneed for data pre-processing such as cropping. This is achieved through apipeline of computer vision models and post-processing techniques aimed atdetecting cetaceans in unedited field imagery before passing them downstreamfor individual level catalogue matching. The system is capable of handlingpreviously uncatalogued individuals and flagging these for investigation thanksto catalogue similarity comparison. We evaluate the system against multiplereal-life photo-id catalogues, achieving mAP@IOU[0.5] = 0.91, 0.96 for the taskof dorsal fin detection on catalogues from Tanzania and the UK respectively and83.1, 97.5% top-10 accuracy for the task of individual classification oncatalogues from the UK and USA."
The BeMi Stardust: a Structured Ensemble of Binarized Neural Networks,"['Ambrogio Maria Bernardelli', 'Stefano Gualandi', 'Hoong Chuin Lau', 'Simone Milanesi']",http://arxiv.org/pdf/2212.03659v1.pdf,2022-12-07,"['math.oc', 'cs.lg']","  Binarized Neural Networks (BNNs) are receiving increasing attention due totheir lightweight architecture and ability to run on low-power devices. Thestate-of-the-art for training classification BNNs restricted to few-shotlearning is based on a Mixed Integer Programming (MIP) approach. This paperproposes the BeMi ensemble, a structured architecture of BNNs based on traininga single BNN for each possible pair of classes and applying a majority votingscheme to predict the final output. The training of a single BNN discriminatingbetween two classes is achieved by a MIP model that optimizes a lexicographicmulti-objective function according to robustness and simplicity principles.This approach results in training networks whose output is not affected bysmall perturbations on the input and whose number of active weights is as smallas possible, while good accuracy is preserved. We computationally validate ourmodel using the MNIST and Fashion-MNIST datasets using up to 40 training imagesper class. Our structured ensemble outperforms both BNNs trained by stochasticgradient descent and state-of-the-art MIP-based approaches. While the previousapproaches achieve an average accuracy of 51.1% on the MNIST dataset, the BeMiensemble achieves an average accuracy of 61.7% when trained with 10 images perclass and 76.4% when trained with 40 images per class."
EPCL: Frozen CLIP Transformer is An Efficient Point Cloud Encoder,"['Xiaoshui Huang', 'Zhou Huang', 'Sheng Li', 'Wentao Qu', 'Tong He', 'Yuenan Hou', 'Yifan Zuo', 'Wanli Ouyang']",http://arxiv.org/pdf/2212.04098v3.pdf,2022-12-08,['cs.cv'],"  The pretrain-finetune paradigm has achieved great success in NLP and 2D imagefields because of the high-quality representation ability and transferabilityof their pretrained models. However, pretraining such a strong model isdifficult in the 3D point cloud field due to the limited amount of point cloudsequences. This paper introduces \textbf{E}fficient \textbf{P}oint\textbf{C}loud \textbf{L}earning (EPCL), an effective and efficient point cloudlearner for directly training high-quality point cloud models with a frozenCLIP transformer. Our EPCL connects the 2D and 3D modalities by semanticallyaligning the image features and point cloud features without paired 2D-3D data.Specifically, the input point cloud is divided into a series of local patches,which are converted to token embeddings by the designed point cloud tokenizer.These token embeddings are concatenated with a task token and fed into thefrozen CLIP transformer to learn point cloud representation. The intuition isthat the proposed point cloud tokenizer projects the input point cloud into aunified token space that is similar to the 2D images. Comprehensive experimentson 3D detection, semantic segmentation, classification and few-shot learningdemonstrate that the CLIP transformer can serve as an efficient point cloudencoder and our method achieves promising performance on both indoor andoutdoor benchmarks. In particular, performance gains brought by our EPCL are$\textbf{19.7}$ AP$_{50}$ on ScanNet V2 detection, $\textbf{4.4}$ mIoU on S3DISsegmentation and $\textbf{1.2}$ mIoU on SemanticKITTI segmentation compared tocontemporary pretrained models. Code is available at\url{https://github.com/XiaoshuiHuang/EPCL}."
Federated Few-Shot Learning for Mobile NLP,"['Dongqi Cai', 'Shangguang Wang', 'Yaozong Wu', 'Felix Xiaozhu Lin', 'Mengwei Xu']",http://arxiv.org/pdf/2212.05974v2.pdf,2022-12-12,"['cs.lg', 'cs.cl']","  Natural language processing (NLP) sees rich mobile applications. To supportvarious language understanding tasks, a foundation NLP model is oftenfine-tuned in a federated, privacy-preserving setting (FL). This processcurrently relies on at least hundreds of thousands of labeled training samplesfrom mobile clients; yet mobile users often lack willingness or knowledge tolabel their data. Such an inadequacy of data labels is known as a few-shotscenario; it becomes the key blocker for mobile NLP applications.  For the first time, this work investigates federated NLP in the few-shotscenario (FedFSL). By retrofitting algorithmic advances of pseudo labeling andprompt learning, we first establish a training pipeline that deliverscompetitive accuracy when only 0.05% (fewer than 100) of the training data islabeled and the remaining is unlabeled. To instantiate the workflow, we furtherpresent a system FeS, addressing the high execution cost with novel designs.(1) Curriculum pacing, which injects pseudo labels to the training workflow ata rate commensurate to the learning progress; (2) Representational diversity, amechanism for selecting the most learnable data, only for which pseudo labelswill be generated; (3) Co-planning of a model's training depth and layercapacity. Together, these designs reduce the training delay, client energy, andnetwork traffic by up to 46.0$\times$, 41.2$\times$ and 3000.0$\times$,respectively. Through algorithm/system co-design, FFNLP demonstrates that FLcan apply to challenging settings where most training samples are unlabeled."
FewFedWeight: Few-shot Federated Learning Framework across Multiple NLP  Tasks,"['Weilong Dong', 'Xinwei Wu', 'Junzhuo Li', 'Shuangzhi Wu', 'Chao Bian', 'Deyi Xiong']",http://arxiv.org/pdf/2212.08354v1.pdf,2022-12-16,['cs.cl'],"  Massively multi-task learning with large language models has recently madesubstantial progress on few-shot generalization. However, this is usuallyperformed in a centralized learning fashion, ignoring the privacy sensitivityissue of (annotated) data used in multiple tasks. To mitigate this issue, wepropose FewFedWeight, a few-shot federated learning framework across multipletasks, to achieve the best of both worlds: privacy preservation and cross-taskgeneralization. FewFedWeight trains client models in isolated devices withoutsharing data. It broadcasts the global model in the server to each client andproduces pseudo data for clients so that knowledge from the global model can beexplored to enhance few-shot learning of each client model. An energy-basedalgorithm is further proposed to weight pseudo samples in order to reduce thenegative impact of noise from the generated pseudo data. Adaptive model weightsof client models are also tuned according to their performance. We use thesemodel weights to dynamically aggregate client models to update the globalmodel. Experiments on 118 NLP tasks show that FewFedWeight can significantlyimprove the performance of client models on 61% tasks with an averageperformance improvement rate of 30.5% over the baseline and substantiallyoutperform FedAvg and other decentralized learning methods."
Contrastive Distillation Is a Sample-Efficient Self-Supervised Loss  Policy for Transfer Learning,"['Chris Lengerich', 'Gabriel Synnaeve', 'Amy Zhang', 'Hugh Leather', 'Kurt Shuster', 'François Charton', 'Charysse Redwood']",http://arxiv.org/pdf/2212.11353v1.pdf,2022-12-21,"['cs.cl', 'cs.lg']","  Traditional approaches to RL have focused on learning decision policiesdirectly from episodic decisions, while slowly and implicitly learning thesemantics of compositional representations needed for generalization. Whilesome approaches have been adopted to refine representations via auxiliaryself-supervised losses while simultaneously learning decision policies,learning compositional representations from hand-designed andcontext-independent self-supervised losses (multi-view) still adapts relativelyslowly to the real world, which contains many non-IID subspaces requiring rapiddistribution shift in both time and spatial attention patterns at varyinglevels of abstraction. In contrast, supervised language model cascades haveshown the flexibility to adapt to many diverse manifolds, and hints ofself-learning needed for autonomous task transfer. However, to date, transfermethods for language models like few-shot learning and fine-tuning stillrequire human supervision and transfer learning using self-learning methods hasbeen underexplored. We propose a self-supervised loss policy called contrastivedistillation which manifests latent variables with high mutual information withboth source and target tasks from weights to tokens. We show how thisoutperforms common methods of transfer learning and suggests a useful designaxis of trading off compute for generalizability for online transfer.Contrastive distillation is improved through sampling from memory and suggestsa simple algorithm for more efficiently sampling negative examples forcontrastive losses than random sampling."
Robust Meta-Representation Learning via Global Label Inference and  Classification,"['Ruohan Wang', 'Isak Falk', 'Massimiliano Pontil', 'Carlo Ciliberto']",http://arxiv.org/pdf/2212.11702v2.pdf,2022-12-22,"['cs.lg', 'stat.ml']","  Few-shot learning (FSL) is a central problem in meta-learning, where learnersmust efficiently learn from few labeled examples. Within FSL, featurepre-training has recently become an increasingly popular strategy tosignificantly improve generalization performance. However, the contribution ofpre-training is often overlooked and understudied, with limited theoreticalunderstanding of its impact on meta-learning performance. Further, pre-trainingrequires a consistent set of global labels shared across training tasks, whichmay be unavailable in practice. In this work, we address the above issues byfirst showing the connection between pre-training and meta-learning. We discusswhy pre-training yields more robust meta-representation and connect thetheoretical analysis to existing works and empirical results. Secondly, weintroduce Meta Label Learning (MeLa), a novel meta-learning algorithm thatlearns task relations by inferring global labels across tasks. This allows usto exploit pre-training for FSL even when global labels are unavailable orill-defined. Lastly, we introduce an augmented pre-training procedure thatfurther improves the learned meta-representation. Empirically, MeLa outperformsexisting methods across a diverse range of benchmarks, in particular under amore challenging setting where the number of training tasks is limited andlabels are task-specific. We also provide extensive ablation study to highlightits key properties."
Few-shot human motion prediction for heterogeneous sensors,"['Rafael Rego Drumond', 'Lukas Brinkmeyer', 'Lars Schmidt-Thieme']",http://arxiv.org/pdf/2212.11771v2.pdf,2022-12-22,"['cs.lg', '68', 'i.2.6']","  Human motion prediction is a complex task as it involves forecastingvariables over time on a graph of connected sensors. This is especially true inthe case of few-shot learning, where we strive to forecast motion sequences forpreviously unseen actions based on only a few examples. Despite this, almostall related approaches for few-shot motion prediction do not incorporate theunderlying graph, while it is a common component in classical motionprediction. Furthermore, state-of-the-art methods for few-shot motionprediction are restricted to motion tasks with a fixed output space meaningthese tasks are all limited to the same sensor graph. In this work, we proposeto extend recent works on few-shot time-series forecasting with heterogeneousattributes with graph neural networks to introduce the first few-shot motionapproach that explicitly incorporates the spatial graph while also generalizingacross motion tasks with heterogeneous sensors. In our experiments on motiontasks with heterogeneous sensors, we demonstrate significant performanceimprovements with lifts from 10.4% up to 39.3% compared to beststate-of-the-art models. Moreover, we show that our model can perform on parwith the best approach so far when evaluating on tasks with a fixed outputspace while maintaining two magnitudes fewer parameters."
Generalization Bounds for Few-Shot Transfer Learning with Pretrained  Classifiers,"['Tomer Galanti', 'András György', 'Marcus Hutter']",http://arxiv.org/pdf/2212.12532v2.pdf,2022-12-23,['cs.lg'],"  We study the ability of foundation models to learn representations forclassification that are transferable to new, unseen classes. Recent results inthe literature show that representations learned by a single classifier overmany classes are competitive on few-shot learning problems with representationslearned by special-purpose algorithms designed for such problems. We offer atheoretical explanation for this behavior based on the recently discoveredphenomenon of class-feature-variability collapse, that is, that during thetraining of deep classification networks the feature embeddings of samplesbelonging to the same class tend to concentrate around their class means. Morespecifically, we show that the few-shot error of the learned feature map on newclasses (defined as the classification error of the nearest class-centerclassifier using centers learned from a small number of random samples fromeach new class) is small in case of class-feature-variability collapse, underthe assumption that the classes are selected independently from a fixeddistribution. This suggests that foundation models can provide feature mapsthat are transferable to new downstream tasks, even with very few samples; toour knowledge, this is the first performance bound for transfer-learning thatis non-vacuous in the few-shot setting."
High-level semantic feature matters few-shot unsupervised domain  adaptation,"['Lei Yu', 'Wanqi Yang', 'Shengqi Huang', 'Lei Wang', 'Ming Yang']",http://arxiv.org/pdf/2301.01956v2.pdf,2023-01-05,['cs.cv'],"  In few-shot unsupervised domain adaptation (FS-UDA), most existing methodsfollowed the few-shot learning (FSL) methods to leverage the low-level localfeatures (learned from conventional convolutional models, e.g., ResNet) forclassification. However, the goal of FS-UDA and FSL are relevant yet distinct,since FS-UDA aims to classify the samples in target domain rather than sourcedomain. We found that the local features are insufficient to FS-UDA, whichcould introduce noise or bias against classification, and not be used toeffectively align the domains. To address the above issues, we aim to refinethe local features to be more discriminative and relevant to classification.Thus, we propose a novel task-specific semantic feature learning method (TSECS)for FS-UDA. TSECS learns high-level semantic features for image-to-classsimilarity measurement. Based on the high-level features, we design across-domain self-training strategy to leverage the few labeled samples insource domain to build the classifier in target domain. In addition, weminimize the KL divergence of the high-level feature distributions betweensource and target domains to shorten the distance of the samples between thetwo domains. Extensive experiments on DomainNet show that the proposed methodsignificantly outperforms SOTA methods in FS-UDA by a large margin (i.e., 10%)."
Exploring Efficient Few-shot Adaptation for Vision Transformers,"['Chengming Xu', 'Siqian Yang', 'Yabiao Wang', 'Zhanxiong Wang', 'Yanwei Fu', 'Xiangyang Xue']",http://arxiv.org/pdf/2301.02419v1.pdf,2023-01-06,['cs.cv'],"  The task of Few-shot Learning (FSL) aims to do the inference on novelcategories containing only few labeled examples, with the help of knowledgelearned from base categories containing abundant labeled training samples.While there are numerous works into FSL task, Vision Transformers (ViTs) haverarely been taken as the backbone to FSL with few trials focusing on naivefinetuning of whole backbone or classification layer.} Essentially, despiteViTs have been shown to enjoy comparable or even better performance on othervision tasks, it is still very nontrivial to efficiently finetune the ViTs inreal-world FSL scenarios. To this end, we propose a novel efficient TransformerTuning (eTT) method that facilitates finetuning ViTs in the FSL tasks. The keynovelties come from the newly presented Attentive Prefix Tuning (APT) andDomain Residual Adapter (DRA) for the task and backbone tuning, individually.Specifically, in APT, the prefix is projected to new key and value pairs thatare attached to each self-attention layer to provide the model withtask-specific information. Moreover, we design the DRA in the form of learnableoffset vectors to handle the potential domain gaps between base and novel data.To ensure the APT would not deviate from the initial task-specific informationmuch, we further propose a novel prototypical regularization, which maximizesthe similarity between the projected distribution of prefix and initialprototypes, regularizing the update procedure. Our method receives outstandingperformance on the challenging Meta-Dataset. We conduct extensive experimentsto show the efficacy of our model."
Unleashing the Power of Shared Label Structures for Human Activity  Recognition,"['Xiyuan Zhang', 'Ranak Roy Chowdhury', 'Jiayun Zhang', 'Dezhi Hong', 'Rajesh K. Gupta', 'Jingbo Shang']",http://arxiv.org/pdf/2301.03462v2.pdf,2023-01-01,"['cs.lg', 'cs.ai', 'eess.sp']","  Current human activity recognition (HAR) techniques regard activity labels asinteger class IDs without explicitly modeling the semantics of class labels. Weobserve that different activity names often have shared structures. Forexample, ""open door"" and ""open fridge"" both have ""open"" as the action; ""kickingsoccer ball"" and ""playing tennis ball"" both have ""ball"" as the object. Suchshared structures in label names can be translated to the similarity in sensorydata and modeling common structures would help uncover knowledge acrossdifferent activities, especially for activities with limited samples. In thispaper, we propose SHARE, a HAR framework that takes into account sharedstructures of label names for different activities. To exploit the sharedstructures, SHARE comprises an encoder for extracting features from inputsensory time series and a decoder for generating label names as a tokensequence. We also propose three label augmentation techniques to help the modelmore effectively capture semantic structures across activities, including abasic token-level augmentation, and two enhanced embedding-level andsequence-level augmentations utilizing the capabilities of pre-trained models.SHARE outperforms state-of-the-art HAR models in extensive experiments on sevenHAR benchmark datasets. We also evaluate in few-shot learning and labelimbalance settings and observe even more significant performance gap."
Few-shot Learning for Cross-Target Stance Detection by Aggregating  Multimodal Embeddings,"['Parisa Jamadi Khiabani', 'Arkaitz Zubiaga']",http://arxiv.org/pdf/2301.04535v2.pdf,2023-01-11,"['cs.cl', 'cs.si']","  Despite the increasing popularity of the stance detection task, existingapproaches are predominantly limited to using the textual content of socialmedia posts for the classification, overlooking the social nature of the task.The stance detection task becomes particularly challenging in cross-targetclassification scenarios, where even in few-shot training settings the modelneeds to predict the stance towards new targets for which the model has onlyseen few relevant samples during training. To address the cross-target stancedetection in social media by leveraging the social nature of the task, weintroduce CT-TN, a novel model that aggregates multimodal embeddings derivedfrom both textual and network features of the data. We conduct experiments in afew-shot cross-target scenario on six different combinations ofsource-destination target pairs. By comparing CT-TN with state-of-the-artcross-target stance detection models, we demonstrate the effectiveness of ourmodel by achieving average performance improvements ranging from 11% to 21%across different baseline models. Experiments with different numbers of shotsshow that CT-TN can outperform other models after seeing 300 instances of thedestination target. Further, ablation experiments demonstrate the positivecontribution of each of the components of CT-TN towards the final performance.We further analyse the network interactions between social media users, whichreveal the potential of using social features for cross-target stancedetection."
"See, Think, Confirm: Interactive Prompting Between Vision and Language  Models for Knowledge-based Visual Reasoning","['Zhenfang Chen', 'Qinhong Zhou', 'Yikang Shen', 'Yining Hong', 'Hao Zhang', 'Chuang Gan']",http://arxiv.org/pdf/2301.05226v1.pdf,2023-01-12,"['cs.cv', 'cs.ai', 'cs.cl', 'cs.lg']","  Large pre-trained vision and language models have demonstrated remarkablecapacities for various tasks. However, solving the knowledge-based visualreasoning tasks remains challenging, which requires a model to comprehensivelyunderstand image content, connect the external world knowledge, and performstep-by-step reasoning to answer the questions correctly. To this end, wepropose a novel framework named Interactive Prompting Visual Reasoner (IPVR)for few-shot knowledge-based visual reasoning. IPVR contains three stages, see,think and confirm. The see stage scans the image and grounds the visual conceptcandidates with a visual perception model. The think stage adopts a pre-trainedlarge language model (LLM) to attend to the key concepts from candidatesadaptively. It then transforms them into text context for prompting with avisual captioning model and adopts the LLM to generate the answer. The confirmstage further uses the LLM to generate the supporting rationale to the answer,verify the generated rationale with a cross-modality classifier and ensure thatthe rationale can infer the predicted output consistently. We conductexperiments on a range of knowledge-based visual reasoning datasets. We foundour IPVR enjoys several benefits, 1). it achieves better performance than theprevious few-shot learning baselines; 2). it enjoys the total transparency andtrustworthiness of the whole reasoning process by providing rationales for eachreasoning step; 3). it is computation-efficient compared with other fine-tuningbaselines."
From Ember to Blaze: Swift Interactive Video Adaptation via  Meta-Reinforcement Learning,"['Xuedou Xiao', 'Mingxuan Yan', 'Yingying Zuo', 'Boxi Liu', 'Paul Ruan', 'Yang Cao', 'Wei Wang']",http://arxiv.org/pdf/2301.05541v1.pdf,2023-01-13,['cs.mm'],"  Maximizing quality of experience (QoE) for interactive video streaming hasbeen a long-standing challenge, as its delay-sensitive nature makes it morevulnerable to bandwidth fluctuations. While reinforcement learning (RL) hasdemonstrated great potential, existing works are either limited by fixed modelsor require enormous data/time for online adaptation, which struggle to fittime-varying and diverse network states. Driven by these practical concerns, weperform large-scale measurements on WeChat for Business's interactive videoservice to study real-world network fluctuations. Surprisingly, our analysisshows that, compared to time-varying network metrics, network sequences exhibitnoticeable short-term continuity, sufficient for few-shot learningrequirements. We thus propose Fiammetta, the first meta-RL-based bitrateadaptation algorithm for interactive video streaming. Building on theshort-term continuity, Fiammetta accumulates learning experiences throughoffline meta-training and enables fast online adaptation to changing networkstates through a few gradient updates. Moreover, Fiammetta innovativelyincorporates a probing mechanism for real-time monitoring of network states,and proposes an adaptive meta-testing mechanism for seamless adaptation. Weimplement Fiammetta on a testbed whose end-to-end network follows thereal-world WeChat for Business traces. The results show that Fiammettaoutperforms prior algorithms significantly, improving video bitrate by3.6%-16.2% without increasing stalling rate."
Open-Set Likelihood Maximization for Few-Shot Learning,"['Malik Boudiaf', 'Etienne Bennequin', 'Myriam Tami', 'Antoine Toubhans', 'Pablo Piantanida', 'Céline Hudelot', 'Ismail Ben Ayed']",http://arxiv.org/pdf/2301.08390v2.pdf,2023-01-20,"['cs.cv', 'cs.lg']","  We tackle the Few-Shot Open-Set Recognition (FSOSR) problem, i.e. classifyinginstances among a set of classes for which we only have a few labeled samples,while simultaneously detecting instances that do not belong to any known class.We explore the popular transductive setting, which leverages the unlabelledquery instances at inference. Motivated by the observation that existingtransductive methods perform poorly in open-set scenarios, we propose ageneralization of the maximum likelihood principle, in which latent scoresdown-weighing the influence of potential outliers are introduced alongside theusual parametric model. Our formulation embeds supervision constraints from thesupport set and additional penalties discouraging overconfident predictions onthe query set. We proceed with a block-coordinate descent, with the latentscores and parametric model co-optimized alternately, thereby benefiting fromeach other. We call our resulting formulation \textit{Open-Set LikelihoodOptimization} (OSLO). OSLO is interpretable and fully modular; it can beapplied on top of any pre-trained model seamlessly. Through extensiveexperiments, we show that our method surpasses existing inductive andtransductive methods on both aspects of open-set recognition, namely inlierclassification and outlier detection."
Self-Supervised Learning for Data Scarcity in a Fatigue Damage  Prognostic Problem,"['Anass Akrim', 'Christian Gogu', 'Rob Vingerhoeds', 'Michel Salaün']",http://arxiv.org/pdf/2301.08441v1.pdf,2023-01-20,"['stat.ml', 'cs.ai', 'cs.lg']","  With the increasing availability of data for Prognostics and HealthManagement (PHM), Deep Learning (DL) techniques are now the subject ofconsiderable attention for this application, often achieving more accurateRemaining Useful Life (RUL) predictions. However, one of the major challengesfor DL techniques resides in the difficulty of obtaining large amounts oflabelled data on industrial systems. To overcome this lack of labelled data, anemerging learning technique is considered in our work: Self-SupervisedLearning, a sub-category of unsupervised learning approaches. This paper aimsto investigate whether pre-training DL models in a self-supervised way onunlabelled sensors data can be useful for RUL estimation with only Few-ShotsLearning, i.e. with scarce labelled data. In this research, a fatigue damageprognostics problem is addressed, through the estimation of the RUL of aluminumalloy panels (typical of aerospace structures) subject to fatigue cracks fromstrain gauge data. Synthetic datasets composed of strain data are used allowingto extensively investigate the influence of the dataset size on the predictiveperformance. Results show that the self-supervised pre-trained models are ableto significantly outperform the non-pre-trained models in downstream RULprediction task, and with less computational expense, showing promising resultsin prognostic tasks when only limited labelled data is available."
Large Language Models Are Latent Variable Models: Explaining and Finding  Good Demonstrations for In-Context Learning,"['Xinyi Wang', 'Wanrong Zhu', 'Michael Saxon', 'Mark Steyvers', 'William Yang Wang']",http://arxiv.org/pdf/2301.11916v3.pdf,2023-01-27,"['cs.cl', 'cs.ai', 'cs.lg']","  In recent years, pre-trained large language models (LLMs) have demonstratedremarkable efficiency in achieving an inference-time few-shot learningcapability known as in-context learning. However, existing literature hashighlighted the sensitivity of this capability to the selection of few-shotdemonstrations. Current understandings of the underlying mechanisms by whichthis capability arises from regular language model pretraining objectivesremain disconnected from the real-world LLMs. This study aims to examine thein-context learning phenomenon through a Bayesian lens, viewing real-world LLMsas latent variable models. On this premise, we propose an algorithm to selectoptimal demonstrations from a set of annotated data with a small LM, and thendirectly generalize the selected demonstrations to larger LMs. We demonstratesignificant improvement over baselines, averaged over eight GPT models on eightreal-world text classification datasets. We also demonstrate the real-worldusefulness of our algorithm on GSM8K, a math word problem dataset. Ourempirical findings support our hypothesis that LLMs implicitly infer a latentvariable containing task information."
Language Quantized AutoEncoders: Towards Unsupervised Text-Image  Alignment,"['Hao Liu', 'Wilson Yan', 'Pieter Abbeel']",http://arxiv.org/pdf/2302.00902v2.pdf,2023-02-02,"['cs.lg', 'cs.cl', 'cs.cv']","  Recent progress in scaling up large language models has shown impressivecapabilities in performing few-shot learning across a wide range of text-basedtasks. However, a key limitation is that these language models fundamentallylack visual perception - a crucial attribute needed to extend these models tobe able to interact with the real world and solve vision tasks, such as invisual-question answering and robotics. Prior works have largely connectedimage to text through pretraining and/or fine-tuning on curated image-textdatasets, which can be a costly and expensive process. In order to resolve thislimitation, we propose a simple yet effective approach calledLanguage-Quantized AutoEncoder (LQAE), a modification of VQ-VAE that learns toalign text-image data in an unsupervised manner by leveraging pretrainedlanguage models (e.g., BERT, RoBERTa). Our main idea is to encode image assequences of text tokens by directly quantizing image embeddings using apretrained language codebook. We then apply random masking followed by a BERTmodel, and have the decoder reconstruct the original image from BERT predictedtext token embeddings. By doing so, LQAE learns to represent similar imageswith similar clusters of text tokens, thereby aligning these two modalitieswithout the use of aligned text-image pairs. This enables few-shot imageclassification with large language models (e.g., GPT-3) as well as linearclassification of images based on BERT text features. To the best of ourknowledge, our work is the first work that uses unaligned images for multimodaltasks by leveraging the power of pretrained language models."
The unreasonable effectiveness of few-shot learning for machine  translation,"['Xavier Garcia', 'Yamini Bansal', 'Colin Cherry', 'George Foster', 'Maxim Krikun', 'Fangxiaoyu Feng', 'Melvin Johnson', 'Orhan Firat']",http://arxiv.org/pdf/2302.01398v1.pdf,2023-02-02,['cs.cl'],"  We demonstrate the potential of few-shot translation systems, trained withunpaired language data, for both high and low-resource language pairs. We showthat with only 5 examples of high-quality translation data shown at inference,a transformer decoder-only model trained solely with self-supervised learning,is able to match specialized supervised state-of-the-art models as well as moregeneral commercial translation systems. In particular, we outperform the bestperforming system on the WMT'21 English - Chinese news translation task by onlyusing five examples of English - Chinese parallel data at inference. Moreover,our approach in building these models does not necessitate joint multilingualtraining or back-translation, is conceptually simple and shows the potential toextend to the multilingual setting. Furthermore, the resulting models are twoorders of magnitude smaller than state-of-the-art language models. We thenanalyze the factors which impact the performance of few-shot translationsystems, and highlight that the quality of the few-shot demonstrations heavilydetermines the quality of the translations generated by our models. Finally, weshow that the few-shot paradigm also provides a way to control certainattributes of the translation -- we show that we are able to control forregional varieties and formality using only a five examples at inference,paving the way towards controllable machine translation systems."
CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code  Models,"['Changan Niu', 'Chuanyi Li', 'Vincent Ng', 'Bin Luo']",http://arxiv.org/pdf/2302.04030v2.pdf,2023-02-08,"['cs.se', 'cs.ai']","  Despite the recent advances showing that a model pre-trained on large-scalesource code data is able to gain appreciable generalization capability, itstill requires a sizeable amount of data on the target task for fine-tuning.And the effectiveness of the model generalization is largely affected by thesize and quality of the fine-tuning data, which is detrimental for target taskswith limited or unavailable resources. Therefore, cross-task generalization,with the goal of improving the generalization of the model to unseen tasks thathave not been seen before, is of strong research and application value.  In this paper, we propose a large-scale benchmark that includes 216 existingcode-related tasks. Then, we annotate each task with the corresponding metainformation such as task description and instruction, which contains detailedinformation about the task and a solution guide. This also helps us to easilycreate a wide variety of ``training/evaluation'' task splits to evaluate thevarious cross-task generalization capabilities of the model. Then we performsome preliminary experiments to demonstrate that the cross-task generalizationof models can be largely improved by in-context learning methods such asfew-shot learning and learning from task instructions, which shows thepromising prospects of conducting cross-task learning research on ourbenchmark. We hope that the collection of the datasets and our benchmark willfacilitate future work that is not limited to cross-task generalization."
"A Systematic Evaluation and Benchmark for Embedding-Aware Generative  Models: Features, Models, and Any-shot Scenarios","['Liangjun Feng', 'Jiancheng Zhao', 'Chunhui Zhao']",http://arxiv.org/pdf/2302.04060v3.pdf,2023-02-08,['cs.cv'],"  Embedding-aware generative model (EAGM) addresses the data insufficiencyproblem for zero-shot learning (ZSL) by constructing a generator betweensemantic and visual feature spaces. Thanks to the predefined benchmark andprotocols, the number of proposed EAGMs for ZSL is increasing rapidly. We arguethat it is time to take a step back and reconsider the embedding-awaregenerative paradigm. The main work of this paper is two-fold. First, theembedding features in benchmark datasets are somehow overlooked, whichpotentially limits the performance of EAGMs, while most researchers focus onhow to improve EAGMs. Therefore, we conduct a systematic evaluation of tenrepresentative EAGMs and prove that even embarrassedly simple modifications onthe embedding features can improve the performance of EAGMs for ZSL remarkably.So it's time to pay more attention to the current embedding features inbenchmark datasets. Second, based on five benchmark datasets, each with sixany-shot learning scenarios, we systematically compare the performance of tentypical EAGMs for the first time, and we give a strong baseline for zero-shotlearning (ZSL) and few-shot learning (FSL). Meanwhile, a comprehensivegenerative model repository, namely, generative any-shot learning (GASL)repository, is provided, which contains the models, features, parameters, andscenarios of EAGMs for ZSL and FSL. Any results in this paper can be readilyreproduced with only one command line based on GASL."
Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot  Image Captioning,"['Zhuolin Yang', 'Wei Ping', 'Zihan Liu', 'Vijay Korthikanti', 'Weili Nie', 'De-An Huang', 'Linxi Fan', 'Zhiding Yu', 'Shiyi Lan', 'Bo Li', 'Ming-Yu Liu', 'Yuke Zhu', 'Mohammad Shoeybi', 'Bryan Catanzaro', 'Chaowei Xiao', 'Anima Anandkumar']",http://arxiv.org/pdf/2302.04858v2.pdf,2023-02-09,"['cs.cv', 'cs.ai', 'cs.cl', 'cs.ir', 'cs.lg']","  Augmenting pretrained language models (LMs) with a vision encoder (e.g.,Flamingo) has obtained the state-of-the-art results in image-to-textgeneration. However, these models store all the knowledge within theirparameters, thus often requiring enormous model parameters to model theabundant visual concepts and very rich textual descriptions. Additionally, theyare inefficient in incorporating new data, requiring a computational-expensivefine-tuning process. In this work, we introduce a Retrieval-augmented VisualLanguage Model, Re-ViLM, built upon the Flamingo, that supports retrieving therelevant knowledge from the external database for zero and in-context few-shotimage-to-text generations. By storing certain knowledge explicitly in theexternal database, our approach reduces the number of model parameters and caneasily accommodate new data during evaluation by simply updating the database.We also construct an interleaved image and text data that facilitatesin-context few-shot learning capabilities. We demonstrate that Re-ViLMsignificantly boosts performance for image-to-text generation tasks, especiallyfor zero-shot and few-shot generation in out-of-domain settings with 4 timesless parameters compared with baseline methods."
Mask-guided BERT for Few Shot Text Classification,"['Wenxiong Liao', 'Zhengliang Liu', 'Haixing Dai', 'Zihao Wu', 'Yiyang Zhang', 'Xiaoke Huang', 'Yuzhong Chen', 'Xi Jiang', 'Wei Liu', 'Dajiang Zhu', 'Tianming Liu', 'Sheng Li', 'Xiang Li', 'Hongmin Cai']",http://arxiv.org/pdf/2302.10447v3.pdf,2023-02-21,"['cs.cl', 'cs.ai']","  Transformer-based language models have achieved significant success invarious domains. However, the data-intensive nature of the transformerarchitecture requires much labeled data, which is challenging in low-resourcescenarios (i.e., few-shot learning (FSL)). The main challenge of FSL is thedifficulty of training robust models on small amounts of samples, whichfrequently leads to overfitting. Here we present Mask-BERT, a simple andmodular framework to help BERT-based architectures tackle FSL. The proposedapproach fundamentally differs from existing FSL strategies such as prompttuning and meta-learning. The core idea is to selectively apply masks on textinputs and filter out irrelevant information, which guides the model to focuson discriminative tokens that influence prediction results. In addition, tomake the text representations from different categories more separable and thetext representations from the same category more compact, we introduce acontrastive learning loss function. Experimental results on public-domainbenchmark datasets demonstrate the effectiveness of Mask-BERT."
Layer Grafted Pre-training: Bridging Contrastive Learning And Masked  Image Modeling For Label-Efficient Representations,"['Ziyu Jiang', 'Yinpeng Chen', 'Mengchen Liu', 'Dongdong Chen', 'Xiyang Dai', 'Lu Yuan', 'Zicheng Liu', 'Zhangyang Wang']",http://arxiv.org/pdf/2302.14138v1.pdf,2023-02-27,['cs.cv'],"  Recently, both Contrastive Learning (CL) and Mask Image Modeling (MIM)demonstrate that self-supervision is powerful to learn good representations.However, naively combining them is far from success. In this paper, we start bymaking the empirical observation that a naive joint optimization of CL and MIMlosses leads to conflicting gradient directions - more severe as the layers godeeper. This motivates us to shift the paradigm from combining loss at the end,to choosing the proper learning method per network layer. Inspired byexperimental observations, we find that MIM and CL are suitable to lower andhigher layers, respectively. We hence propose to combine them in a surprisinglysimple, ""sequential cascade"" fashion: early layers are first trained under oneMIM loss, on top of which latter layers continue to be trained under another CLloss. The proposed Layer Grafted Pre-training learns good visualrepresentations that demonstrate superior label efficiency in downstreamapplications, in particular yielding strong few-shot performance besides linearevaluation. For instance, on ImageNet-1k, Layer Grafted Pre-training yields65.5% Top-1 accuracy in terms of 1% few-shot learning with ViT-B/16, whichimproves MIM and CL baselines by 14.4% and 2.1% with no bells and whistles. Thecode is available athttps://github.com/VITA-Group/layerGraftedPretraining_ICLR23.git."
Meta-Learning with Adaptive Weighted Loss for Imbalanced Cold-Start  Recommendation,"['Minchang Kim', 'Yongjin Yang', 'Jung Hyun Ryu', 'Taesup Kim']",http://arxiv.org/pdf/2302.14640v2.pdf,2023-02-28,"['cs.ir', 'cs.lg']","  Sequential recommenders have made great strides in capturing a user'spreferences. Nevertheless, the cold-start recommendation remains a fundamentalchallenge as they typically involve limited user-item interactions forpersonalization. Recently, gradient-based meta-learning approaches have emergedin the sequential recommendation field due to their fast adaptation andeasy-to-integrate abilities. The meta-learning algorithms formulate thecold-start recommendation as a few-shot learning problem, where each user isrepresented as a task to be adapted. While meta-learning algorithms generallyassume that task-wise samples are evenly distributed over classes or values,user-item interactions in real-world applications do not conform to such adistribution (e.g., watching favorite videos multiple times, leaving onlypositive ratings without any negative ones). Consequently, imbalanced userfeedback, which accounts for the majority of task training data, may dominatethe user adaptation process and prevent meta-learning algorithms from learningmeaningful meta-knowledge for personalized recommendations. To alleviate thislimitation, we propose a novel sequential recommendation framework based ongradient-based meta-learning that captures the imbalanced rating distributionof each user and computes adaptive loss for user-specific learning. Our work isthe first to tackle the impact of imbalanced ratings in cold-start sequentialrecommendation scenarios. Through extensive experiments conducted on real-worlddatasets, we demonstrate the effectiveness of our framework."
"Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong  Few-shot Learners","['Renrui Zhang', 'Xiangfei Hu', 'Bohao Li', 'Siyuan Huang', 'Hanqiu Deng', 'Hongsheng Li', 'Yu Qiao', 'Peng Gao']",http://arxiv.org/pdf/2303.02151v1.pdf,2023-03-03,"['cs.cv', 'cs.cl']","  Visual recognition in low-data regimes requires deep neural networks to learngeneralized representations from limited training samples. Recently, CLIP-basedmethods have shown promising few-shot performance benefited from thecontrastive language-image pre-training. We then question, if the more diversepre-training knowledge can be cascaded to further assist few-shotrepresentation learning. In this paper, we propose CaFo, a Cascade ofFoundation models that incorporates diverse prior knowledge of variouspre-training paradigms for better few-shot learning. Our CaFo incorporatesCLIP's language-contrastive knowledge, DINO's vision-contrastive knowledge,DALL-E's vision-generative knowledge, and GPT-3's language-generativeknowledge. Specifically, CaFo works by 'Prompt, Generate, then Cache'. Firstly,we leverage GPT-3 to produce textual inputs for prompting CLIP with richdownstream linguistic semantics. Then, we generate synthetic images via DALL-Eto expand the few-shot training data without any manpower. At last, weintroduce a learnable cache model to adaptively blend the predictions from CLIPand DINO. By such collaboration, CaFo can fully unleash the potential ofdifferent pre-training methods and unify them to perform state-of-the-art forfew-shot classification. Code is available athttps://github.com/ZrrSkywalker/CaFo."
DINet: Deformation Inpainting Network for Realistic Face Visually  Dubbing on High Resolution Video,"['Zhimeng Zhang', 'Zhipeng Hu', 'Wenjin Deng', 'Changjie Fan', 'Tangjie Lv', 'Yu Ding']",http://arxiv.org/pdf/2303.03988v1.pdf,2023-03-07,['cs.cv'],"  For few-shot learning, it is still a critical challenge to realizephoto-realistic face visually dubbing on high-resolution videos. Previous worksfail to generate high-fidelity dubbing results. To address the above problem,this paper proposes a Deformation Inpainting Network (DINet) forhigh-resolution face visually dubbing. Different from previous works relying onmultiple up-sample layers to directly generate pixels from latent embeddings,DINet performs spatial deformation on feature maps of reference images tobetter preserve high-frequency textural details. Specifically, DINet consistsof one deformation part and one inpainting part. In the first part, fivereference facial images adaptively perform spatial deformation to createdeformed feature maps encoding mouth shapes at each frame, in order to alignwith the input driving audio and also the head poses of the input sourceimages. In the second part, to produce face visually dubbing, a feature decoderis responsible for adaptively incorporating mouth movements from the deformedfeature maps and other attributes (i.e., head pose and upper facial expression)from the source feature maps together. Finally, DINet achieves face visuallydubbing with rich textural details. We conduct qualitative and quantitativecomparisons to validate our DINet on high-resolution videos. The experimentalresults show that our method outperforms state-of-the-art works."
MaskDiff: Modeling Mask Distribution with Diffusion Probabilistic Model  for Few-Shot Instance Segmentation,"['Minh-Quan Le', 'Tam V. Nguyen', 'Trung-Nghia Le', 'Thanh-Toan Do', 'Minh N. Do', 'Minh-Triet Tran']",http://arxiv.org/pdf/2303.05105v1.pdf,2023-03-09,['cs.cv'],"  Few-shot instance segmentation extends the few-shot learning paradigm to theinstance segmentation task, which tries to segment instance objects from aquery image with a few annotated examples of novel categories. Conventionalapproaches have attempted to address the task via prototype learning, known aspoint estimation. However, this mechanism is susceptible to noise and suffersfrom bias due to a significant scarcity of data. To overcome the disadvantagesof the point estimation mechanism, we propose a novel approach, dubbedMaskDiff, which models the underlying conditional distribution of a binarymask, which is conditioned on an object region and $K$-shot information.Inspired by augmentation approaches that perturb data with Gaussian noise forpopulating low data density regions, we model the mask distribution with adiffusion probabilistic model. In addition, we propose to utilizeclassifier-free guided mask sampling to integrate category information into thebinary mask generation process. Without bells and whistles, our proposed methodconsistently outperforms state-of-the-art methods on both base and novelclasses of the COCO dataset while simultaneously being more stable thanexisting methods."
Knowledge-augmented Few-shot Visual Relation Detection,"['Tianyu Yu', 'Yangning Li', 'Jiaoyan Chen', 'Yinghui Li', 'Hai-Tao Zheng', 'Xi Chen', 'Qingbin Liu', 'Wenqiang Liu', 'Dongxiao Huang', 'Bei Wu', 'Yexin Wang']",http://arxiv.org/pdf/2303.05342v1.pdf,2023-03-09,"['cs.cv', 'cs.ai']","  Visual Relation Detection (VRD) aims to detect relationships between objectsfor image understanding. Most existing VRD methods rely on thousands oftraining samples of each relationship to achieve satisfactory performance. Somerecent papers tackle this problem by few-shot learning with elaboratelydesigned pipelines and pre-trained word vectors. However, the performance ofexisting few-shot VRD models is severely hampered by the poor generalizationcapability, as they struggle to handle the vast semantic diversity of visualrelationships. Nonetheless, humans have the ability to learn new relationshipswith just few examples based on their knowledge. Inspired by this, we devise aknowledge-augmented, few-shot VRD framework leveraging both textual knowledgeand visual relation knowledge to improve the generalization ability of few-shotVRD. The textual knowledge and visual relation knowledge are acquired from apre-trained language model and an automatically constructed visual relationknowledge graph, respectively. We extensively validate the effectiveness of ourframework. Experiments conducted on three benchmarks from the commonly usedVisual Genome dataset show that our performance surpasses existingstate-of-the-art models with a large improvement."
Gradient-Regulated Meta-Prompt Learning for Generalizable  Vision-Language Models,"['Juncheng Li', 'Minghe Gao', 'Longhui Wei', 'Siliang Tang', 'Wenqiao Zhang', 'Mengze Li', 'Wei Ji', 'Qi Tian', 'Tat-Seng Chua', 'Yueting Zhuang']",http://arxiv.org/pdf/2303.06571v2.pdf,2023-03-12,['cs.cv'],"  Prompt tuning, a recently emerging paradigm, enables the powerfulvision-language pre-training models to adapt to downstream tasks in a parameter-- and data -- efficient way, by learning the ``soft prompts'' to conditionfrozen pre-training models. Though effective, it is particularly problematic inthe few-shot scenario, where prompt tuning performance is sensitive to theinitialization and requires a time-consuming process to find a goodinitialization, thus restricting the fast adaptation ability of thepre-training models. In addition, prompt tuning could undermine thegeneralizability of the pre-training models, because the learnable prompttokens are easy to overfit to the limited training samples. To address theseissues, we introduce a novel Gradient-RegulAted Meta-prompt learning (GRAM)framework that jointly meta-learns an efficient soft prompt initialization forbetter adaptation and a lightweight gradient regulating function for strongcross-domain generalizability in a meta-learning paradigm using only theunlabeled image-text pre-training data. Rather than designing a specific prompttuning method, our GRAM can be easily incorporated into various prompt tuningmethods in a model-agnostic way, and comprehensive experiments show that GRAMbrings about consistent improvement for them in several settings (i.e.,few-shot learning, cross-domain generalization, cross-dataset generalization,etc.) over 11 datasets. Further, experiments show that GRAM enables theorthogonal methods of textual and visual prompt tuning to work in amutually-enhanced way, offering better generalizability beyond the uni-modalprompt tuning methods."
Instance-Conditioned GAN Data Augmentation for Representation Learning,"['Pietro Astolfi', 'Arantxa Casanova', 'Jakob Verbeek', 'Pascal Vincent', 'Adriana Romero-Soriano', 'Michal Drozdzal']",http://arxiv.org/pdf/2303.09677v1.pdf,2023-03-16,['cs.cv'],"  Data augmentation has become a crucial component to train state-of-the-artvisual representation models. However, handcrafting combinations oftransformations that lead to improved performances is a laborious task, whichcan result in visually unrealistic samples. To overcome these limitations,recent works have explored the use of generative models as learnable dataaugmentation tools, showing promising results in narrow application domains,e.g., few-shot learning and low-data medical imaging. In this paper, weintroduce a data augmentation module, called DA_IC-GAN, which leveragesinstance-conditioned GAN generations and can be used off-the-shelf inconjunction with most state-of-the-art training recipes. We showcase thebenefits of DA_IC-GAN by plugging it out-of-the-box into the supervisedtraining of ResNets and DeiT models on the ImageNet dataset, and achievingaccuracy boosts up to between 1%p and 2%p with the highest capacity models.Moreover, the learnt representations are shown to be more robust than thebaselines when transferred to a handful of out-of-distribution datasets, andexhibit increased invariance to variations of instance and viewpoints. Weadditionally couple DA_IC-GAN with a self-supervised training recipe and showthat we can also achieve an improvement of 1%p in accuracy in some settings.With this work, we strengthen the evidence on the potential of learnable dataaugmentations to improve visual representation learning, paving the roadtowards non-handcrafted augmentations in model training."
BeamSense: Rethinking Wireless Sensing with MU-MIMO Wi-Fi Beamforming  Feedback,"['Khandaker Foysal Haque', 'Milin Zhang', 'Francesca Meneghello', 'Francesco Restuccia']",http://arxiv.org/pdf/2303.09687v1.pdf,2023-03-16,"['cs.ni', 'eess.sp']","  In this paper, we propose BeamSense, a completely novel approach to implementstandard-compliant Wi-Fi sensing applications. Wi-Fi sensing enablesgame-changing applications in remote healthcare, home entertainment, and homesurveillance, among others. However, existing work leverages the manualextraction of channel state information (CSI) from Wi-Fi chips to classifyactivities, which is not supported by the Wi-Fi standard and hence requires theusage of specialized equipment. On the contrary, BeamSense leverages thestandard-compliant beamforming feedback information (BFI) to characterize thepropagation environment. Conversely from CSI, the BFI (i) can be easilyrecorded without any firmware modification, and (ii) captures the multiplechannels between the access point and the stations, thus providing much bettersensitivity. BeamSense includes a novel cross-domain few-shot learning (FSL)algorithm to handle unseen environments and subjects with few additional datapoints. We evaluate BeamSense through an extensive data collection campaignwith three subjects performing twenty different activities in three differentenvironments. We show that our BFI-based approach achieves about 10% moreaccuracy when compared to CSI-based prior work, while our FSL strategy improvesaccuracy by up to 30% and 80% when compared with state-of-the-art cross-domainalgorithms."
A general-purpose AI assistant embedded in an open-source radiology  information system,"['Saptarshi Purkayastha', 'Rohan Isaac', 'Sharon Anthony', 'Shikhar Shukla', 'Elizabeth A. Krupinski', 'Joshua A. Danish', 'Judy W. Gichoya']",http://arxiv.org/pdf/2303.10338v1.pdf,2023-03-18,"['cs.ai', 'cs.hc']","  Radiology AI models have made significant progress in near-human performanceor surpassing it. However, AI model's partnership with human radiologistremains an unexplored challenge due to the lack of health informationstandards, contextual and workflow differences, and data labeling variations.To overcome these challenges, we integrated an AI model service that uses DICOMstandard SR annotations into the OHIF viewer in the open-source LibreHealthRadiology Information Systems (RIS). In this paper, we describe the novelHuman-AI partnership capabilities of the platform, including few-shot learningand swarm learning approaches to retrain the AI models continuously. Buildingon the concept of machine teaching, we developed an active learning strategywithin the RIS, so that the human radiologist can enable/disable AI annotationsas well as ""fix""/relabel the AI annotations. These annotations are then used toretrain the models. This helps establish a partnership between the radiologistuser and a user-specific AI model. The weights of these user-specific modelsare then finally shared between multiple models in a swarm learning approach."
Decomposed Prototype Learning for Few-Shot Scene Graph Generation,"['Xingchen Li', 'Long Chen', 'Guikun Chen', 'Yinfu Feng', 'Yi Yang', 'Jun Xiao']",http://arxiv.org/pdf/2303.10863v1.pdf,2023-03-20,['cs.cv'],"  Today's scene graph generation (SGG) models typically require abundant manualannotations to learn new predicate types. Thus, it is difficult to apply themto real-world applications with a long-tailed distribution of predicates. Inthis paper, we focus on a new promising task of SGG: few-shot SGG (FSSGG).FSSGG encourages models to be able to quickly transfer previous knowledge andrecognize novel predicates well with only a few examples. Although manyadvanced approaches have achieved great success on few-shot learning (FSL)tasks, straightforwardly extending them into FSSGG is not applicable due to twointrinsic characteristics of predicate concepts: 1) Each predicate categorycommonly has multiple semantic meanings under different contexts. 2) The visualappearance of relation triplets with the same predicate differs greatly underdifferent subject-object pairs. Both issues make it hard to model conventionallatent representations for predicate categories with state-of-the-art FSLmethods. To this end, we propose a novel Decomposed Prototype Learning (DPL).Specifically, we first construct a decomposable prototype space to captureintrinsic visual patterns of subjects and objects for predicates, and enhancetheir feature representations with these decomposed prototypes. Then, we devisean intelligent metric learner to assign adaptive weights to each support sampleby considering the relevance of their subject-object pairs. We further re-splitthe VG dataset and compare DPL with various FSL methods to benchmark this task.Extensive results show that DPL achieves excellent performance in both base andnovel categories."
Few Shot Medical Image Segmentation with Cross Attention Transformer,"['Yi Lin', 'Yufan Chen', 'Kwang-Ting Cheng', 'Hao Chen']",http://arxiv.org/pdf/2303.13867v3.pdf,2023-03-24,['cs.cv'],"  Medical image segmentation has made significant progress in recent years.Deep learning-based methods are recognized as data-hungry techniques, requiringlarge amounts of data with manual annotations. However, manual annotation isexpensive in the field of medical image analysis, which requiresdomain-specific expertise. To address this challenge, few-shot learning has thepotential to learn new classes from only a few examples. In this work, wepropose a novel framework for few-shot medical image segmentation, termedCAT-Net, based on cross masked attention Transformer. Our proposed networkmines the correlations between the support image and query image, limiting themto focus only on useful foreground information and boosting the representationcapacity of both the support prototype and query features. We further design aniterative refinement framework that refines the query image segmentationiteratively and promotes the support feature in turn. We validated the proposedmethod on three public datasets: Abd-CT, Abd-MRI, and Card-MRI. Experimentalresults demonstrate the superior performance of our method compared tostate-of-the-art methods and the effectiveness of each component. Code:https://github.com/hust-linyi/CAT-Net."
SPEC: Summary Preference Decomposition for Low-Resource Abstractive  Summarization,"['Yi-Syuan Chen', 'Yun-Zhu Song', 'Hong-Han Shuai']",http://arxiv.org/pdf/2303.14011v1.pdf,2023-03-24,"['cs.cl', 'cs.ai']","  Neural abstractive summarization has been widely studied and achieved greatsuccess with large-scale corpora. However, the considerable cost of annotatingdata motivates the need for learning strategies under low-resource settings. Inthis paper, we investigate the problems of learning summarizers with only fewexamples and propose corresponding methods for improvements. First, typicaltransfer learning methods are prone to be affected by data properties andlearning objectives in the pretext tasks. Therefore, based on pretrainedlanguage models, we further present a meta learning framework to transferfew-shot learning processes from source corpora to the target corpus. Second,previous methods learn from training examples without decomposing the contentand preference. The generated summaries could therefore be constrained by thepreference bias in the training set, especially under low-resource settings. Assuch, we propose decomposing the contents and preferences during learningthrough the parameter modulation, which enables control over preferences duringinference. Third, given a target application, specifying required preferencescould be non-trivial because the preferences may be difficult to derive throughobservations. Therefore, we propose a novel decoding method to automaticallyestimate suitable preferences and generate corresponding summary candidatesfrom the few training examples. Extensive experiments demonstrate that ourmethods achieve state-of-the-art performance on six diverse corpora with30.11%/33.95%/27.51% and 26.74%/31.14%/24.48% average improvements onROUGE-1/2/L under 10- and 100-example settings."
VERN: Vegetation-aware Robot Navigation in Dense Unstructured Outdoor  Environments,"['Adarsh Jagan Sathyamoorthy', 'Kasun Weerakoon', 'Tianrui Guan', 'Mason Russell', 'Damon Conover', 'Jason Pusey', 'Dinesh Manocha']",http://arxiv.org/pdf/2303.14502v1.pdf,2023-03-25,['cs.ro'],"  We propose a novel method for autonomous legged robot navigation in denselyvegetated environments with a variety of pliable/traversable andnon-pliable/untraversable vegetation. We present a novel few-shot learningclassifier that can be trained on a few hundred RGB images to differentiateflora that can be navigated through, from the ones that must be circumvented.Using the vegetation classification and 2D lidar scans, our method constructs avegetation-aware traversability cost map that accurately represents the pliableand non-pliable obstacles with lower, and higher traversability costs,respectively. Our cost map construction accounts for misclassifications of thevegetation and further lowers the risk of collisions, freezing and entrapmentin vegetation during navigation. Furthermore, we propose holonomic recoverybehaviors for the robot for scenarios where it freezes, or gets physicallyentrapped in dense, pliable vegetation. We demonstrate our method on a BostonDynamics Spot robot in real-world unstructured environments with sparse anddense tall grass, bushes, trees, etc. We observe an increase of 25-90% insuccess rates, 10-90% decrease in freezing rate, and up to 65% decrease in thefalse positive rate compared to existing methods."
Supervised Masked Knowledge Distillation for Few-Shot Transformers,"['Han Lin', 'Guangxing Han', 'Jiawei Ma', 'Shiyuan Huang', 'Xudong Lin', 'Shih-Fu Chang']",http://arxiv.org/pdf/2303.15466v2.pdf,2023-03-25,"['cs.cv', 'cs.ai']","  Vision Transformers (ViTs) emerge to achieve impressive performance on manydata-abundant computer vision tasks by capturing long-range dependencies amonglocal features. However, under few-shot learning (FSL) settings on smalldatasets with only a few labeled data, ViT tends to overfit and suffers fromsevere performance degradation due to its absence of CNN-alike inductive bias.Previous works in FSL avoid such problem either through the help ofself-supervised auxiliary losses, or through the dextile uses of labelinformation under supervised settings. But the gap between self-supervised andsupervised few-shot Transformers is still unfilled. Inspired by recent advancesin self-supervised knowledge distillation and masked image modeling (MIM), wepropose a novel Supervised Masked Knowledge Distillation model (SMKD) forfew-shot Transformers which incorporates label information intoself-distillation frameworks. Compared with previous self-supervised methods,we allow intra-class knowledge distillation on both class and patch tokens, andintroduce the challenging task of masked patch tokens reconstruction acrossintra-class images. Experimental results on four few-shot classificationbenchmark datasets show that our method with simple design outperforms previousmethods by a large margin and achieves a new start-of-the-art. Detailedablation studies confirm the effectiveness of each component of our model. Codefor this paper is available here: https://github.com/HL-hanlin/SMKD."
"Augmenting Iterative Trajectory for Bilevel Optimization: Methodology,  Analysis and Extensions","['Risheng Liu', 'Yaohua Liu', 'Shangzhi Zeng', 'Jin Zhang']",http://arxiv.org/pdf/2303.16397v1.pdf,2023-03-29,['math.oc'],"  In recent years, there has been a surge of machine learning applicationsdeveloped with hierarchical structure, which can be approached from Bi-LevelOptimization (BLO) perspective. However, most existing gradient-based methodsoverlook the interdependence between hyper-gradient calculation and Lower-Level(LL) iterative trajectory, focusing solely on the former. Consequently,convergence theory is constructed with restrictive LL assumptions, which areoften challenging to satisfy in real-world scenarios. In this work, wethoroughly analyze the constructed iterative trajectory, and highlight twodeficiencies, including empirically chosen initialization and default use ofentire trajectory for hyper-gradient calculation. To address these issues, weincrementally introduce two augmentation techniques including InitializationAuxiliary (IA) and Pessimistic Trajectory Truncation (PTT), and investigatevarious extension strategies such as prior regularization, different iterativemapping schemes and acceleration dynamics to construct Augmented IterativeTrajectory (AIT) for corresponding BLO scenarios (e.g., LL convexity and LLnon-convexity). Theoretically, we provide convergence analysis for AIT and itsvariations under different LL assumptions, and establish the first convergenceanalysis for BLOs with non-convex LL subproblem. Finally, we demonstrate theeffectiveness of AIT through three numerical examples, typical learning andvision applications (e.g., data hyper-cleaning and few-shot learning) and morechallenging tasks such as neural architecture search."
Channel Phase Processing in Wireless Networks for Human Activity  Recognition,"['Guillermo Diaz', 'Iker Sobron', 'Iñaki Eizmendi', 'Iratxe landa', 'Manuel Velez']",http://arxiv.org/pdf/2303.16873v2.pdf,2023-03-29,['eess.sp'],"  The phase of the channel state information (CSI) is underutilized as a sourceof information in wireless sensing due to its sensitivity to synchronizationerrors of the signal reception. A linear transformation of the phase iscommonly applied to correct linear offsets and, in a few cases, some filteringin time or frequency is carried out to smooth the data. This paper presents anovel processing method of the CSI phase to improve the accuracy of humanactivity recognition (HAR) in indoor environments. This new method, coined TimeSmoothing and Frequency Rebuild (TSFR), consists of performing a CSI phasesanitization method to remove phase impairments based on a linear regressiontransformation method, then a time domain filtering stage with a Savitzy-Golay(SG) filter for denoising purposes and, finally, the phase is rebuilt,eliminating distortions in frequency caused by SG filtering. The TSFR methodhas been tested on five datasets obtained from experimental measurements, usingthree different deep learning algorithms, and compared against five other typesof CSI phase processing. The results show an accuracy improvement using TSFR inall the cases. Concretely, accuracy performance higher than 90\% in most of thestudied scenarios has been achieved with the proposed solution. In few-shotlearning strategies, TSFR outperforms the state-of-the-art performance from 35%to 85%."
A Closer Look at Few-Shot 3D Point Cloud Classification,"['Chuangguan Ye', 'Hongyuan Zhu', 'Bo Zhang', 'Tao Chen']",http://arxiv.org/pdf/2303.18210v1.pdf,2023-03-31,['cs.cv'],"  In recent years, research on few-shot learning (FSL) has been fast-growing inthe 2D image domain due to the less requirement for labeled training data andgreater generalization for novel classes. However, its application in 3D pointcloud data is relatively under-explored. Not only need to distinguish unseenclasses as in the 2D domain, 3D FSL is more challenging in terms of irregularstructures, subtle inter-class differences, and high intra-class variances{when trained on a low number of data.} Moreover, different architectures andlearning algorithms make it difficult to study the effectiveness of existing 2DFSL algorithms when migrating to the 3D domain. In this work, for the firsttime, we perform systematic and extensive investigations of directly applyingrecent 2D FSL works to 3D point cloud related backbone networks and thussuggest a strong learning baseline for few-shot 3D point cloud classification.Furthermore, we propose a new network, Point-cloud Correlation Interaction(PCIA), with three novel plug-and-play components called Salient-Part Fusion(SPF) module, Self-Channel Interaction Plus (SCI+) module, and Cross-InstanceFusion Plus (CIF+) module to obtain more representative embeddings and improvethe feature distinction. These modules can be inserted into most FSL algorithmswith minor changes and significantly improve the performance. Experimentalresults on three benchmark datasets, ModelNet40-FS, ShapeNet70-FS, andScanObjectNN-FS, demonstrate that our method achieves state-of-the-artperformance for the 3D FSL task. Code and datasets are available athttps://github.com/cgye96/A_Closer_Look_At_3DFSL."
Improving Few-Shot Inductive Learning on Temporal Knowledge Graphs using  Confidence-Augmented Reinforcement Learning,"['Zifeng Ding', 'Jingpei Wu', 'Zongyue Li', 'Yunpu Ma', 'Volker Tresp']",http://arxiv.org/pdf/2304.00613v2.pdf,2023-04-02,"['cs.lg', 'cs.ai']","  Temporal knowledge graph completion (TKGC) aims to predict the missing linksamong the entities in a temporal knwoledge graph (TKG). Most previous TKGCmethods only consider predicting the missing links among the entities seen inthe training set, while they are unable to achieve great performance in linkprediction concerning newly-emerged unseen entities. Recently, a new task,i.e., TKG few-shot out-of-graph (OOG) link prediction, is proposed, where TKGCmodels are required to achieve great link prediction performance concerningnewly-emerged entities that only have few-shot observed examples. In this work,we propose a TKGC method FITCARL that combines few-shot learning withreinforcement learning to solve this task. In FITCARL, an agent traversesthrough the whole TKG to search for the prediction answer. A policy network isdesigned to guide the search process based on the traversed path. To betteraddress the data scarcity problem in the few-shot setting, we introduce amodule that computes the confidence of each candidate action and integrate itinto the policy for action selection. We also exploit the entity conceptinformation with a novel concept regularizer to boost model performance.Experimental results show that FITCARL achieves stat-of-the-art performance onTKG few-shot OOG link prediction."
Meta-Learning with a Geometry-Adaptive Preconditioner,"['Suhyun Kang', 'Duhun Hwang', 'Moonjung Eo', 'Taesup Kim', 'Wonjong Rhee']",http://arxiv.org/pdf/2304.01552v2.pdf,2023-04-04,"['cs.cv', 'cs.ai', 'cs.lg']","  Model-agnostic meta-learning (MAML) is one of the most successfulmeta-learning algorithms. It has a bi-level optimization structure where theouter-loop process learns a shared initialization and the inner-loop processoptimizes task-specific weights. Although MAML relies on the standard gradientdescent in the inner-loop, recent studies have shown that controlling theinner-loop's gradient descent with a meta-learned preconditioner can bebeneficial. Existing preconditioners, however, cannot simultaneously adapt in atask-specific and path-dependent way. Additionally, they do not satisfy theRiemannian metric condition, which can enable the steepest descent learningwith preconditioned gradient. In this study, we propose Geometry-AdaptivePreconditioned gradient descent (GAP) that can overcome the limitations inMAML; GAP can efficiently meta-learn a preconditioner that is dependent ontask-specific parameters, and its preconditioner can be shown to be aRiemannian metric. Thanks to the two properties, the geometry-adaptivepreconditioner is effective for improving the inner-loop optimization.Experiment results show that GAP outperforms the state-of-the-art MAML familyand preconditioned gradient descent-MAML (PGD-MAML) family in a variety offew-shot learning tasks. Code is available at:https://github.com/Suhyun777/CVPR23-GAP."
Simulated Annealing in Early Layers Leads to Better Generalization,"['Amirmohammad Sarfi', 'Zahra Karimpour', 'Muawiz Chaudhary', 'Nasir M. Khalid', 'Mirco Ravanelli', 'Sudhir Mudur', 'Eugene Belilovsky']",http://arxiv.org/pdf/2304.04858v1.pdf,2023-04-10,"['cs.lg', 'cs.cv']","  Recently, a number of iterative learning methods have been introduced toimprove generalization. These typically rely on training for longer periods oftime in exchange for improved generalization. LLF (later-layer-forgetting) is astate-of-the-art method in this category. It strengthens learning in earlylayers by periodically re-initializing the last few layers of the network. Ourprincipal innovation in this work is to use Simulated annealing in EArly Layers(SEAL) of the network in place of re-initialization of later layers.Essentially, later layers go through the normal gradient descent process, whilethe early layers go through short stints of gradient ascent followed bygradient descent. Extensive experiments on the popular Tiny-ImageNet datasetbenchmark and a series of transfer learning and few-shot learning tasks showthat we outperform LLF by a significant margin. We further show that, comparedto normal training, LLF features, although improving on the target task,degrade the transfer learning performance across all datasets we explored. Incomparison, our method outperforms LLF across the same target datasets by alarge margin. We also show that the prediction depth of our method issignificantly lower than that of LLF and normal training, indicating on averagebetter prediction performance."
SMAE: Few-shot Learning for HDR Deghosting with Saturation-Aware Masked  Autoencoders,"['Qingsen Yan', 'Song Zhang', 'Weiye Chen', 'Hao Tang', 'Yu Zhu', 'Jinqiu Sun', 'Luc Van Gool', 'Yanning Zhang']",http://arxiv.org/pdf/2304.06914v1.pdf,2023-04-14,['cs.cv'],"  Generating a high-quality High Dynamic Range (HDR) image from dynamic sceneshas recently been extensively studied by exploiting Deep Neural Networks(DNNs). Most DNNs-based methods require a large amount of training data withground truth, requiring tedious and time-consuming work. Few-shot HDR imagingaims to generate satisfactory images with limited data. However, it isdifficult for modern DNNs to avoid overfitting when trained on only a fewimages. In this work, we propose a novel semi-supervised approach to realizefew-shot HDR imaging via two stages of training, called SSHDR. Unlikelyprevious methods, directly recovering content and removing ghostssimultaneously, which is hard to achieve optimum, we first generate content ofsaturated regions with a self-supervised mechanism and then address ghosts viaan iterative semi-supervised learning framework. Concretely, considering thatsaturated regions can be regarded as masking Low Dynamic Range (LDR) inputregions, we design a Saturated Mask AutoEncoder (SMAE) to learn a robustfeature representation and reconstruct a non-saturated HDR image. We alsopropose an adaptive pseudo-label selection strategy to pick high-quality HDRpseudo-labels in the second stage to avoid the effect of mislabeled samples.Experiments demonstrate that SSHDR outperforms state-of-the-art methodsquantitatively and qualitatively within and across different datasets,achieving appealing HDR visualization with few labeled samples."
"Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with  Text","['Wanrong Zhu', 'Jack Hessel', 'Anas Awadalla', 'Samir Yitzhak Gadre', 'Jesse Dodge', 'Alex Fang', 'Youngjae Yu', 'Ludwig Schmidt', 'William Yang Wang', 'Yejin Choi']",http://arxiv.org/pdf/2304.06939v3.pdf,2023-04-14,"['cs.cv', 'cs.cl']","  In-context vision and language models like Flamingo support arbitrarilyinterleaved sequences of images and text as input. This format not only enablesfew-shot learning via interleaving independent supervised (image, text)examples, but also, more complex prompts involving interaction between images,e.g., ""What do image A and image B have in common?"" To support this interface,pretraining occurs over web corpora that similarly contain interleavedimages+text. To date, however, large-scale data of this form have not beenpublicly available.  We release Multimodal C4, an augmentation of the popular text-only C4 corpuswith images interleaved. We use a linear assignment algorithm to place imagesinto longer bodies of text using CLIP features, a process that we showoutperforms alternatives. Multimodal C4 spans everyday topics like cooking,travel, technology, etc. A manual inspection of a random sample of documentsshows that a vast majority (88%) of images are topically relevant, and thatlinear assignment frequently selects individual sentences specificallywell-aligned with each image (80%). After filtering NSFW images, ads, etc., theresulting corpus consists of 101.2M documents with 571M images interleaved in43B English tokens."
A Survey on Few-Shot Class-Incremental Learning,"['Songsong Tian', 'Lusi Li', 'Weijun Li', 'Hang Ran', 'Xin Ning', 'Prayag Tiwari']",http://arxiv.org/pdf/2304.08130v2.pdf,2023-04-17,['cs.cv'],"  Large deep learning models are impressive, but they struggle when real-timedata is not available. Few-shot class-incremental learning (FSCIL) poses asignificant challenge for deep neural networks to learn new tasks from just afew labeled samples without forgetting the previously learned ones. This setupeasily leads to catastrophic forgetting and overfitting problems, severelyaffecting model performance. Studying FSCIL helps overcome deep learning modellimitations on data volume and acquisition time, while improving practicalityand adaptability of machine learning models. This paper provides acomprehensive survey on FSCIL. Unlike previous surveys, we aim to synthesizefew-shot learning and incremental learning, focusing on introducing FSCIL fromtwo perspectives, while reviewing over 30 theoretical research studies and morethan 20 applied research studies. From the theoretical perspective, we providea novel categorization approach that divides the field into five subcategories,including traditional machine learning methods, meta-learning based methods,feature and feature space-based methods, replay-based methods, and dynamicnetwork structure-based methods. We also evaluate the performance of recenttheoretical research on benchmark datasets of FSCIL. From the applicationperspective, FSCIL has achieved impressive achievements in various fields ofcomputer vision such as image classification, object detection, and imagesegmentation, as well as in natural language processing and graph. We summarizethe important applications. Finally, we point out potential future researchdirections, including applications, problem setups, and theory development.Overall, this paper offers a comprehensive analysis of the latest advances inFSCIL from a methodological, performance, and application perspective."
Clustered-patch Element Connection for Few-shot Learning,"['Jinxiang Lai', 'Siqian Yang', 'Junhong Zhou', 'Wenlong Wu', 'Xiaochen Chen', 'Jun Liu', 'Bin-Bin Gao', 'Chengjie Wang']",http://arxiv.org/pdf/2304.10093v2.pdf,2023-04-20,['cs.cv'],"  Weak feature representation problem has influenced the performance offew-shot classification task for a long time. To alleviate this problem, recentresearchers build connections between support and query instances throughembedding patch features to generate discriminative representations. However,we observe that there exists semantic mismatches (foreground/ background) amongthese local patches, because the location and size of the target object are notfixed. What is worse, these mismatches result in unreliable similarityconfidences, and complex dense connection exacerbates the problem. According tothis, we propose a novel Clustered-patch Element Connection (CEC) layer tocorrect the mismatch problem. The CEC layer leverages Patch Cluster and ElementConnection operations to collect and establish reliable connections with highsimilarity patch features, respectively. Moreover, we propose a CECNet,including CEC layer based attention module and distance metric. The former isutilized to generate a more discriminative representation benefiting from theglobal clustered-patch features, and the latter is introduced to reliablymeasure the similarity between pair-features. Extensive experiments demonstratethat our CECNet outperforms the state-of-the-art methods on classificationbenchmark. Furthermore, our CEC approach can be extended into few-shotsegmentation and detection tasks, which achieves competitive performances."
Unified Quantum State Tomography and Hamiltonian Learning Using  Transformer Models: A Language-Translation-Like Approach for Quantum Systems,"['Zheng An', 'Jiahui Wu', 'Muchun Yang', 'D. L. Zhou', 'Bei Zeng']",http://arxiv.org/pdf/2304.12010v1.pdf,2023-04-24,['quant-ph'],"  Schr\""odinger's equation serves as a fundamental component in characterizingquantum systems, wherein both quantum state tomography and Hamiltonian learningare instrumental in comprehending and interpreting quantum systems. Whilenumerous techniques exist for carrying out state tomography and learningHamiltonians individually, no method has been developed to combine these twoaspects. In this study, we introduce a new approach that employs the attentionmechanism in transformer models to effectively merge quantum state tomographyand Hamiltonian learning. By carefully choosing and preparing the trainingdata, our method integrates both tasks without altering the model'sarchitecture, allowing the model to effectively learn the intricaterelationships between quantum states and Hamiltonian. We also demonstrate theeffectiveness of our approach across various quantum systems, ranging fromsimple 2-qubit cases to more involved 2D antiferromagnetic Heisenbergstructures. The data collection process is streamlined, as it only necessitatesa one-way generation process beginning with state tomography. Furthermore, thescalability and few-shot learning capabilities of our method could potentiallyminimize the resources required for characterizing and optimizing quantumsystems. Our research provides valuable insights into the relationship betweenHamiltonian structure and quantum system behavior, fostering opportunities foradditional studies on quantum systems and the advancement of quantumcomputation and associated technologies."
Meta-tuning Loss Functions and Data Augmentation for Few-shot Object  Detection,"['Berkan Demirel', 'Orhun Buğra Baran', 'Ramazan Gokberk Cinbis']",http://arxiv.org/pdf/2304.12161v1.pdf,2023-04-24,['cs.cv'],"  Few-shot object detection, the problem of modelling novel object detectioncategories with few training instances, is an emerging topic in the area offew-shot learning and object detection. Contemporary techniques can be dividedinto two groups: fine-tuning based and meta-learning based approaches. Whilemeta-learning approaches aim to learn dedicated meta-models for mapping samplesto novel class models, fine-tuning approaches tackle few-shot detection in asimpler manner, by adapting the detection model to novel classes throughgradient based optimization. Despite their simplicity, fine-tuning basedapproaches typically yield competitive detection results. Based on thisobservation, we focus on the role of loss functions and augmentations as theforce driving the fine-tuning process, and propose to tune their dynamicsthrough meta-learning principles. The proposed training scheme, therefore,allows learning inductive biases that can boost few-shot detection, whilekeeping the advantages of fine-tuning based approaches. In addition, theproposed approach yields interpretable loss functions, as opposed to highlyparametric and complex few-shot meta-models. The experimental results highlightthe merits of the proposed scheme, with significant improvements over thestrong fine-tuning based few-shot detection baselines on benchmark Pascal VOCand MS-COCO datasets, in terms of both standard and generalized few-shotperformance metrics."
Analogy-Forming Transformers for Few-Shot 3D Parsing,"['Nikolaos Gkanatsios', 'Mayank Singh', 'Zhaoyuan Fang', 'Shubham Tulsiani', 'Katerina Fragkiadaki']",http://arxiv.org/pdf/2304.14382v2.pdf,2023-04-27,"['cs.cv', 'cs.ai', 'cs.lg']","  We present Analogical Networks, a model that encodes domain knowledgeexplicitly, in a collection of structured labelled 3D scenes, in addition toimplicitly, as model parameters, and segments 3D object scenes with analogicalreasoning: instead of mapping a scene to part segments directly, our modelfirst retrieves related scenes from memory and their corresponding partstructures, and then predicts analogous part structures for the input scene,via an end-to-end learnable modulation mechanism. By conditioning on more thanone retrieved memories, compositions of structures are predicted, that mix andmatch parts across the retrieved memories. One-shot, few-shot or many-shotlearning are treated uniformly in Analogical Networks, by conditioning on theappropriate set of memories, whether taken from a single, few or many memoryexemplars, and inferring analogous parses. We show Analogical Networks arecompetitive with state-of-the-art 3D segmentation transformers in many-shotsettings, and outperform them, as well as existing paradigms of meta-learningand few-shot learning, in few-shot settings. Analogical Networks successfullysegment instances of novel object categories simply by expanding their memory,without any weight updates. Our code and models are publicly available in theproject webpage: http://analogicalnets.github.io/."
HQP: A Human-Annotated Dataset for Detecting Online Propaganda,"['Abdurahman Maarouf', 'Dominik Bär', 'Dominique Geissler', 'Stefan Feuerriegel']",http://arxiv.org/pdf/2304.14931v2.pdf,2023-04-28,['cs.cl'],"  Online propaganda poses a severe threat to the integrity of societies.However, existing datasets for detecting online propaganda have a keylimitation: they were annotated using weak labels that can be noisy and evenincorrect. To address this limitation, our work makes the followingcontributions: (1) We present HQP: a novel dataset (N=30,000) for detectingonline propaganda with high-quality labels. To the best of our knowledge, HQPis the first dataset for detecting online propaganda that was created throughhuman annotation. (2) We show empirically that state-of-the-art language modelsfail in detecting online propaganda when trained with weak labels (AUC: 64.03).In contrast, state-of-the-art language models can accurately detect onlinepropaganda when trained with our high-quality labels (AUC: 92.25), which is animprovement of ~44%. (3) To address the cost of labeling, we extend our work tofew-shot learning. Specifically, we show that prompt-based learning using asmall sample of high-quality labels can still achieve a reasonable performance(AUC: 80.27). Finally, we discuss implications for the NLP community to balancethe cost and quality of labeling. Crucially, our work highlights the importanceof high-quality labels for sensitive NLP tasks such as propaganda detection."
DocLangID: Improving Few-Shot Training to Identify the Language of  Historical Documents,"['Furkan Simsek', 'Brian Pfitzmann', 'Hendrik Raetz', 'Jona Otholt', 'Haojin Yang', 'Christoph Meinel']",http://arxiv.org/pdf/2305.02208v1.pdf,2023-05-03,['cs.cv'],"  Language identification describes the task of recognizing the language ofwritten text in documents. This information is crucial because it can be usedto support the analysis of a document's vocabulary and context. Supervisedlearning methods in recent years have advanced the task of languageidentification. However, these methods usually require large labeled datasets,which often need to be included for various domains of images, such asdocuments or scene images. In this work, we propose DocLangID, a transferlearning approach to identify the language of unlabeled historical documents.We achieve this by first leveraging labeled data from a different but relateddomain of historical documents. Secondly, we implement a distance-basedfew-shot learning approach to adapt a convolutional neural network to newlanguages of the unlabeled dataset. By introducing small amounts of manuallylabeled examples from the set of unlabeled images, our feature extractordevelops a better adaptability towards new and different data distributions ofhistorical documents. We show that such a model can be effectively fine-tunedfor the unlabeled set of images by only reusing the same few-shot examples. Weshowcase our work across 10 languages that mostly use the Latin script. Ourexperiments on historical documents demonstrate that our combined approachimproves the language identification performance, achieving 74% recognitionaccuracy on the four unseen languages of the unlabeled dataset."
AttenWalker: Unsupervised Long-Document Question Answering via  Attention-based Graph Walking,"['Yuxiang Nie', 'Heyan Huang', 'Wei Wei', 'Xian-Ling Mao']",http://arxiv.org/pdf/2305.02235v1.pdf,2023-05-03,['cs.cl'],"  Annotating long-document question answering (long-document QA) pairs istime-consuming and expensive. To alleviate the problem, it might be possible togenerate long-document QA pairs via unsupervised question answering (UQA)methods. However, existing UQA tasks are based on short documents, and canhardly incorporate long-range information. To tackle the problem, we propose anew task, named unsupervised long-document question answering (ULQA), aiming togenerate high-quality long-document QA instances in an unsupervised manner.Besides, we propose AttenWalker, a novel unsupervised method to aggregate andgenerate answers with long-range dependency so as to construct long-document QApairs. Specifically, AttenWalker is composed of three modules, i.e., spancollector, span linker and answer aggregator. Firstly, the span collector takesadvantage of constituent parsing and reconstruction loss to select informativecandidate spans for constructing answers. Secondly, by going through theattention graph of a pre-trained long-document model, potentially interrelatedtext spans (that might be far apart) could be linked together via anattention-walking algorithm. Thirdly, in the answer aggregator, linked spansare aggregated into the final answer via the mask-filling ability of apre-trained model. Extensive experiments show that AttenWalker outperformsprevious methods on Qasper and NarrativeQA. In addition, AttenWalker also showsstrong performance in the few-shot learning setting."
Joint Graph Learning and Model Fitting in Laplacian Regularized  Stratified Models,"['Ziheng Cheng', 'Junzi Zhang', 'Akshay Agrawal', 'Stephen Boyd']",http://arxiv.org/pdf/2305.02573v1.pdf,2023-05-04,"['stat.ml', 'cs.lg', 'math.oc']","  Laplacian regularized stratified models (LRSM) are models that utilize theexplicit or implicit network structure of the sub-problems as defined by thecategorical features called strata (e.g., age, region, time, forecast horizon,etc.), and draw upon data from neighboring strata to enhance the parameterlearning of each sub-problem. They have been widely applied in machine learningand signal processing problems, including but not limited to time seriesforecasting, representation learning, graph clustering, max-marginclassification, and general few-shot learning. Nevertheless, existing works onLRSM have either assumed a known graph or are restricted to specificapplications. In this paper, we start by showing the importance and sensitivityof graph weights in LRSM, and provably show that the sensitivity can bearbitrarily large when the parameter scales and sample sizes are heavilyimbalanced across nodes. We then propose a generic approach to jointly learnthe graph while fitting the model parameters by solving a single optimizationproblem. We interpret the proposed formulation from both a graph connectivityviewpoint and an end-to-end Bayesian perspective, and propose an efficientalgorithm to solve the problem. Convergence guarantees of the proposedoptimization algorithm is also provided despite the lack of global stronglysmoothness of the Laplacian regularization term typically required in theexisting literature, which may be of independent interest. Finally, weillustrate the efficiency of our approach compared to existing methods byvarious real-world numerical examples."
Parameter-Efficient Cross-lingual Transfer of Vision and Language Models  via Translation-based Alignment,"['Zhen Zhang', 'Jialu Wang', 'Xin Eric Wang']",http://arxiv.org/pdf/2305.03510v2.pdf,2023-05-02,"['cs.cl', 'cs.ai']","  Pre-trained vision and language models such as CLIP have witnessed remarkablesuccess in connecting images and texts with a primary focus on English texts.Despite recent efforts to extend CLIP to support other languages, disparitiesin performance among different languages have been observed due to unevenresource availability. Additionally, current cross-lingual transfer methods ofthose pre-trained models would consume excessive resources for a large numberof languages. Therefore, we propose a new parameter-efficient cross-lingualtransfer learning framework that utilizes a translation-based alignment methodto mitigate multilingual disparities and explores parameter-efficientfine-tuning methods for parameter-efficient cross-lingual transfer. Extensiveexperiments on XTD and Multi30K datasets, covering 11 languages underzero-shot, few-shot, and full-dataset learning scenarios, show that ourframework significantly reduces the multilingual disparities among languagesand improves cross-lingual transfer results, especially in low-resourcescenarios, while only keeping and fine-tuning an extremely small number ofparameters compared to the full model (e.g., Our framework only requires 0.16\%additional parameters of a full-model for each language in the few-shotlearning scenario). The codes are available at\url{https://github.com/eric-ai-lab/PECTVLM}. The codes are available at\url{https://github.com/eric-ai-lab/PECTVLM}."
CodeIE: Large Code Generation Models are Better Few-Shot Information  Extractors,"['Peng Li', 'Tianxiang Sun', 'Qiong Tang', 'Hang Yan', 'Yuanbin Wu', 'Xuanjing Huang', 'Xipeng Qiu']",http://arxiv.org/pdf/2305.05711v2.pdf,2023-05-09,"['cs.cl', 'cs.ai']","  Large language models (LLMs) pre-trained on massive corpora have demonstratedimpressive few-shot learning ability on many NLP tasks. A common practice is torecast the task into a text-to-text format such that generative LLMs of naturallanguage (NL-LLMs) like GPT-3 can be prompted to solve it. However, it isnontrivial to perform information extraction (IE) tasks with NL-LLMs since theoutput of the IE task is usually structured and therefore is hard to beconverted into plain text. In this paper, we propose to recast the structuredoutput in the form of code instead of natural language and utilize generativeLLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular,named entity recognition and relation extraction. In contrast to NL-LLMs, weshow that Code-LLMs can be well-aligned with these IE tasks by designingcode-style prompts and formulating these IE tasks as code generation tasks.Experiment results on seven benchmarks show that our method consistentlyoutperforms fine-tuning moderate-size pre-trained models specially designed forIE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We furtherconduct a series of in-depth analyses to demonstrate the merits of leveragingCode-LLMs for IE tasks."
RAMario: Experimental Approach to Reptile Algorithm -- Reinforcement  Learning for Mario,['Sanyam Jain'],http://arxiv.org/pdf/2305.09655v1.pdf,2023-05-16,"['cs.lg', 'cs.ma']","  This research paper presents an experimental approach to using the Reptilealgorithm for reinforcement learning to train a neural network to play SuperMario Bros. We implement the Reptile algorithm using the Super Mario Bros Gymlibrary and TensorFlow in Python, creating a neural network model with a singleconvolutional layer, a flatten layer, and a dense layer. We define theoptimizer and use the Reptile class to create an instance of the Reptilemeta-learning algorithm. We train the model using multiple tasks and episodes,choosing actions using the current weights of the neural network model, takingthose actions in the environment, and updating the model weights using theReptile algorithm. We evaluate the performance of the algorithm by printing thetotal reward for each episode. In addition, we compare the performance of theReptile algorithm approach to two other popular reinforcement learningalgorithms, Proximal Policy Optimization (PPO) and Deep Q-Network (DQN),applied to the same Super Mario Bros task. Our results demonstrate that theReptile algorithm provides a promising approach to few-shot learning in videogame AI, with comparable or even better performance than the other twoalgorithms, particularly in terms of moves vs distance that agent performs for1M episodes of training. The results shows that best total distance for world1-2 in the game environment were ~1732 (PPO), ~1840 (DQN) and ~2300 (RAMario).Full code is available at https://github.com/s4nyam/RAMario."
Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced  Generative Pre-training Model,"['Jiageng Wu', 'Xian Wu', 'Zhaopeng Qiu', 'Minghui Li', 'Yefeng Zheng', 'Jie Yang']",http://arxiv.org/pdf/2305.10163v2.pdf,2023-05-17,"['cs.cl', 'cs.ai', 'cs.cy']","  Generative Pre-Training (GPT) models like ChatGPT have demonstratedexceptional performance in various Natural Language Processing (NLP) tasks.Although ChatGPT has been integrated into the overall workflow to boostefficiency in many domains, the lack of flexibility in the finetuning processhinders its applications in areas that demand extensive domain expertise andsemantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT onthe China National Medical Licensing Examination (CNMLE) and propose a novelapproach to improve ChatGPT from two perspectives: integrating medical domainknowledge and enabling few-shot learning. By using a simple but effectiveretrieval method, medical background knowledge is extracted as semanticinstructions to guide the inference of ChatGPT. Similarly, relevant medicalquestions are identified and fed as demonstrations to ChatGPT. Experimentalresults show that directly applying ChatGPT fails to qualify the CNMLE at ascore of 51 (i.e., only 51\% of questions are answered correctly). While ourknowledge-enhanced model achieves a high score of 70 on CNMLE-2022 which notonly passes the qualification but also surpasses the average score of humans(61). This research demonstrates the potential of knowledge-enhanced ChatGPT toserve as versatile medical assistants, capable of analyzing real-world medicalproblems in a more accessible, user-friendly, and adaptable manner."
Exploring the Space of Key-Value-Query Models with Intention,"['Marta Garnelo', 'Wojciech Marian Czarnecki']",http://arxiv.org/pdf/2305.10203v1.pdf,2023-05-17,"['cs.lg', 'cs.ne']","  Attention-based models have been a key element of many recent breakthroughsin deep learning. Two key components of Attention are the structure of itsinput (which consists of keys, values and queries) and the computations bywhich these three are combined. In this paper we explore the space of modelsthat share said input structure but are not restricted to the computations ofAttention. We refer to this space as Keys-Values-Queries (KVQ) Space. Our goalis to determine whether there are any other stackable models in KVQ Space thatAttention cannot efficiently approximate, which we can implement with ourcurrent deep learning toolbox and that solve problems that are interesting tothe community. Maybe surprisingly, the solution to the standard least squaresproblem satisfies these properties. A neural network module that is able tocompute this solution not only enriches the set of computations that a neuralnetwork can represent but is also provably a strict generalisation of LinearAttention. Even more surprisingly the computational complexity of this moduleis exactly the same as that of Attention, making it a suitable drop inreplacement. With this novel connection between classical machine learning(least squares) and modern deep learning (Attention) established we justify avariation of our model which generalises regular Attention in the same way.Both new modules are put to the test an a wide spectrum of tasks ranging fromfew-shot learning to policy distillation that confirm their real-worldsapplicability."
PointGPT: Auto-regressively Generative Pre-training from Point Clouds,"['Guangyan Chen', 'Meiling Wang', 'Yi Yang', 'Kai Yu', 'Li Yuan', 'Yufeng Yue']",http://arxiv.org/pdf/2305.11487v2.pdf,2023-05-19,['cs.cv'],"  Large language models (LLMs) based on the generative pre-training transformer(GPT) have demonstrated remarkable effectiveness across a diverse range ofdownstream tasks. Inspired by the advancements of the GPT, we present PointGPT,a novel approach that extends the concept of GPT to point clouds, addressingthe challenges associated with disorder properties, low information density,and task gaps. Specifically, a point cloud auto-regressive generation task isproposed to pre-train transformer models. Our method partitions the input pointcloud into multiple point patches and arranges them in an ordered sequencebased on their spatial proximity. Then, an extractor-generator basedtransformer decoder, with a dual masking strategy, learns latentrepresentations conditioned on the preceding point patches, aiming to predictthe next one in an auto-regressive manner. Our scalable approach allows forlearning high-capacity models that generalize well, achieving state-of-the-artperformance on various downstream tasks. In particular, our approach achievesclassification accuracies of 94.9% on the ModelNet40 dataset and 93.4% on theScanObjectNN dataset, outperforming all other transformer models. Furthermore,our method also attains new state-of-the-art accuracies on all four few-shotlearning benchmarks."
A Survey of Diffusion Models in Natural Language Processing,"['Hao Zou', 'Zae Myung Kim', 'Dongyeop Kang']",http://arxiv.org/pdf/2305.14671v2.pdf,2023-05-24,['cs.cl'],"  This survey paper provides a comprehensive review of the use of diffusionmodels in natural language processing (NLP). Diffusion models are a class ofmathematical models that aim to capture the diffusion of information or signalsacross a network or manifold. In NLP, diffusion models have been used in avariety of applications, such as natural language generation, sentimentanalysis, topic modeling, and machine translation. This paper discusses thedifferent formulations of diffusion models used in NLP, their strengths andlimitations, and their applications. We also perform a thorough comparisonbetween diffusion models and alternative generative models, specificallyhighlighting the autoregressive (AR) models, while also examining how diversearchitectures incorporate the Transformer in conjunction with diffusion models.Compared to AR models, diffusion models have significant advantages forparallel generation, text interpolation, token-level controls such as syntacticstructures and semantic contents, and robustness. Exploring furtherpermutations of integrating Transformers into diffusion models would be avaluable pursuit. Also, the development of multimodal diffusion models andlarge-scale diffusion language models with notable capabilities for few-shotlearning would be important directions for the future advance of diffusionmodels in NLP."
Benchmarking Arabic AI with Large Language Models,"['Ahmed Abdelali', 'Hamdy Mubarak', 'Shammur Absar Chowdhury', 'Maram Hasanain', 'Basel Mousi', 'Sabri Boughorbel', 'Yassine El Kheir', 'Daniel Izham', 'Fahim Dalvi', 'Majd Hawasly', 'Nizi Nazar', 'Yousseif Elshahawy', 'Ahmed Ali', 'Nadir Durrani', 'Natasa Milic-Frayling', 'Firoj Alam']",http://arxiv.org/pdf/2305.14982v1.pdf,2023-05-24,"['cs.cl', 'cs.ai', '68t50', 'f.2.2; i.2.7']","  With large Foundation Models (FMs), language technologies (AI in general) areentering a new paradigm: eliminating the need for developing large-scaletask-specific datasets and supporting a variety of tasks through set-upsranging from zero-shot to few-shot learning. However, understanding FMscapabilities requires a systematic benchmarking effort by comparing FMsperformance with the state-of-the-art (SOTA) task-specific models. With thatgoal, past work focused on the English language and included a few efforts withmultiple languages. Our study contributes to ongoing research by evaluating FMsperformance for standard Arabic NLP and Speech processing, including a range oftasks from sequence tagging to content classification across diverse domains.We start with zero-shot learning using GPT-3.5-turbo, Whisper, and USM,addressing 33 unique tasks using 59 publicly available datasets resulting in 96test setups. For a few tasks, FMs performs on par or exceeds the performance ofthe SOTA models but for the majority it under-performs. Given the importance ofprompt for the FMs performance, we discuss our prompt strategies in detail andelaborate on our findings. Our future work on Arabic AI will explore few-shotprompting, expand the range of tasks, and investigate additional open-sourcemodels."
Sentiment Analysis in the Era of Large Language Models: A Reality Check,"['Wenxuan Zhang', 'Yue Deng', 'Bing Liu', 'Sinno Jialin Pan', 'Lidong Bing']",http://arxiv.org/pdf/2305.15005v1.pdf,2023-05-24,['cs.cl'],"  Sentiment analysis (SA) has been a long-standing research area in naturallanguage processing. It can offer rich insights into human sentiments andopinions and has thus seen considerable interest from both academia andindustry. With the advent of large language models (LLMs) such as ChatGPT,there is a great potential for their employment on SA problems. However, theextent to which existing LLMs can be leveraged for different sentiment analysistasks remains unclear. This paper aims to provide a comprehensive investigationinto the capabilities of LLMs in performing various sentiment analysis tasks,from conventional sentiment classification to aspect-based sentiment analysisand multifaceted analysis of subjective texts. We evaluate performance across13 tasks on 26 datasets and compare the results against small language models(SLMs) trained on domain-specific datasets. Our study reveals that while LLMsdemonstrate satisfactory performance in simpler tasks, they lag behind in morecomplex tasks requiring deeper understanding or structured sentimentinformation. However, LLMs significantly outperform SLMs in few-shot learningsettings, suggesting their potential when annotation resources are limited. Wealso highlight the limitations of current evaluation practices in assessingLLMs' SA abilities and propose a novel benchmark, \textsc{SentiEval}, for amore comprehensive and realistic evaluation. Data and code during ourinvestigations are available at\url{https://github.com/DAMO-NLP-SG/LLM-Sentiment}."
On convex decision regions in deep network representations,"['Lenka Tětková', 'Thea Brüsch', 'Teresa Karen Scheidt', 'Fabian Martin Mager', 'Rasmus Ørtoft Aagaard', 'Jonathan Foldager', 'Tommy Sonne Alstrøm', 'Lars Kai Hansen']",http://arxiv.org/pdf/2305.17154v2.pdf,2023-05-26,"['cs.lg', 'cs.ai']","  Current work on human-machine alignment aims at understanding machine-learnedlatent spaces and their correspondence to human representations.G{\""a}rdenfors' conceptual spaces is a prominent framework for understandinghuman representations. Convexity of object regions in conceptual spaces isargued to promote generalizability, few-shot learning, and interpersonalalignment. Based on these insights, we investigate the notion of convexity ofconcept regions in machine-learned latent spaces. We develop a set of tools formeasuring convexity in sampled data and evaluate emergent convexity in layeredrepresentations of state-of-the-art deep networks. We show that convexity isrobust to basic re-parametrization and, hence, meaningful as a quality ofmachine-learned latent spaces. We find that approximate convexity is pervasivein neural representations in multiple application domains, including models ofimages, audio, human activity, text, and medical images. Generally, we observethat fine-tuning increases the convexity of label regions. We find evidencethat pretraining convexity of class label regions predicts subsequentfine-tuning performance."
Toward Understanding Generative Data Augmentation,"['Chenyu Zheng', 'Guoqiang Wu', 'Chongxuan Li']",http://arxiv.org/pdf/2305.17476v1.pdf,2023-05-27,"['cs.lg', 'stat.ml']","  Generative data augmentation, which scales datasets by obtaining fake labeledexamples from a trained conditional generative model, boosts classificationperformance in various learning tasks including (semi-)supervised learning,few-shot learning, and adversarially robust learning. However, little work hastheoretically investigated the effect of generative data augmentation. To fillthis gap, we establish a general stability bound in this not independently andidentically distributed (non-i.i.d.) setting, where the learned distribution isdependent on the original train set and generally not the same as the truedistribution. Our theoretical result includes the divergence between thelearned distribution and the true distribution. It shows that generative dataaugmentation can enjoy a faster learning rate when the order of divergence termis $o(\max\left( \log(m)\beta_m, 1 / \sqrt{m})\right)$, where $m$ is the trainset size and $\beta_m$ is the corresponding stability constant. We furtherspecify the learning setup to the Gaussian mixture model and generativeadversarial nets. We prove that in both cases, though generative dataaugmentation does not enjoy a faster learning rate, it can improve the learningguarantees at a constant level when the train set is small, which issignificant when the awful overfitting occurs. Simulation results on theGaussian mixture model and empirical results on generative adversarial netssupport our theoretical conclusions. Our code is available athttps://github.com/ML-GSAI/Understanding-GDA."
Catalysis distillation neural network for the few shot open catalyst  challenge,['Bowen Deng'],http://arxiv.org/pdf/2305.19545v1.pdf,2023-05-31,"['physics.chem-ph', 'cs.ce', 'cs.lg']","  The integration of artificial intelligence and science has resulted insubstantial progress in computational chemistry methods for the design anddiscovery of novel catalysts. Nonetheless, the challenges of electrocatalyticreactions and developing a large-scale language model in catalysis persist, andthe recent success of ChatGPT's (Chat Generative Pre-trained Transformer)few-shot methods surpassing BERT (Bidirectional Encoder Representation fromTransformers) underscores the importance of addressing limited data, expensivecomputations, time constraints and structure-activity relationship in research.Hence, the development of few-shot techniques for catalysis is critical andessential, regardless of present and future requirements. This paper introducesthe Few-Shot Open Catalyst Challenge 2023, a competition aimed at advancing theapplication of machine learning technology for predicting catalytic reactionson catalytic surfaces, with a specific focus on dual-atom catalysts in hydrogenperoxide electrocatalysis. To address the challenge of limited data incatalysis, we propose a machine learning approach based on MLP-Like and aframework called Catalysis Distillation Graph Neural Network (CDGNN). Ourresults demonstrate that CDGNN effectively learns embeddings from catalyticstructures, enabling the capture of structure-adsorption relationships. Thisaccomplishment has resulted in the utmost advanced and efficient determinationof the reaction pathway for hydrogen peroxide, surpassing the current graphneural network approach by 16.1%.. Consequently, CDGNN presents a promisingapproach for few-shot learning in catalysis."
Impact of Large Language Models on Generating Software Specifications,"['Danning Xie', 'Byungwoo Yoo', 'Nan Jiang', 'Mijung Kim', 'Lin Tan', 'Xiangyu Zhang', 'Judy S. Lee']",http://arxiv.org/pdf/2306.03324v2.pdf,2023-06-06,['cs.se'],"  Software specifications are essential for ensuring the reliability ofsoftware systems. Existing specification extraction approaches, however, sufferfrom limited generalizability and require manual efforts. The recent emergenceof Large Language Models (LLMs), which have been successfully applied tonumerous software engineering tasks, offers a promising avenue for automatingthis process. In this paper, we conduct the first empirical study to evaluatethe capabilities of LLMs for generating software specifications from softwarecomments or documentation. We evaluate LLMs' performance with Few Shot Learning(FSL), enabling LLMs to generalize from a small number of examples, as well asdifferent prompt construction strategies, and compare the performance of LLMswith traditional approaches. Additionally, we conduct a comparative diagnosisof the failure cases from both LLMs and traditional methods, identifying theirunique strengths and weaknesses. Lastly, we conduct extensive experiments on 15state of the art LLMs, evaluating their performance and cost effectiveness forgenerating software specifications.  Our results show that with FSL, LLMs outperform traditional methods (by5.6%), and more sophisticated prompt construction strategies can furtherenlarge this performance gap (up to 5.1 to 10.0%). Yet, LLMs suffer from theirunique challenges, such as ineffective prompts and the lack of domainknowledge, which together account for 53 to 60% of LLM unique failures. Thestrong performance of open source models (e.g., StarCoder) makes closed sourcemodels (e.g., GPT 3 Davinci) less desirable due to size and cost. Our studyoffers valuable insights for future research to improve specificationgeneration."
One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning,"['Arnav Chavan', 'Zhuang Liu', 'Deepak Gupta', 'Eric Xing', 'Zhiqiang Shen']",http://arxiv.org/pdf/2306.07967v2.pdf,2023-06-13,"['cs.lg', 'cs.ai', 'cs.cv']","  We present Generalized LoRA (GLoRA), an advanced approach for universalparameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA),GLoRA employs a generalized prompt module to optimize pre-trained model weightsand adjust intermediate activations, providing more flexibility and capabilityacross diverse tasks and datasets. Moreover, GLoRA facilitates efficientparameter adaptation by employing a scalable, modular, layer-wise structuresearch that learns individual adapter of each layer. Originating from a unifiedmathematical formulation, GLoRA exhibits strong transfer learning, few-shotlearning and domain generalization abilities, as it adapts to new tasks throughnot only weights but also additional dimensions like activations. Comprehensiveexperiments demonstrate that GLoRA outperforms all previous methods in natural,specialized, and structured vision benchmarks, achieving superior accuracy withfewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2also show considerable enhancements compared to the original LoRA in thelanguage domain. Furthermore, our structural re-parameterization design ensuresthat GLoRA incurs no extra inference cost, rendering it a practical solutionfor resource-limited applications. Code and models are available at:https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA."
Domain-Aware Few-Shot Learning for Optical Coherence Tomography Noise  Reduction,['Deborah Pereg'],http://arxiv.org/pdf/2306.08102v2.pdf,2023-06-13,"['eess.iv', 'cs.cv', 'cs.lg']","  Speckle noise has long been an extensively studied problem in medicalimaging. In recent years, there have been significant advances in leveragingdeep learning methods for noise reduction. Nevertheless, adaptation ofsupervised learning models to unseen domains remains a challenging problem.Specifically, deep neural networks (DNNs) trained for computational imagingtasks are vulnerable to changes in the acquisition system's physicalparameters, such as: sampling space, resolution, and contrast. Even within thesame acquisition system, performance degrades across datasets of differentbiological tissues. In this work, we propose a few-shot supervised learningframework for optical coherence tomography (OCT) noise reduction, that offers adramatic increase in training speed and requires only a single image, or partof an image, and a corresponding speckle suppressed ground truth, for training.Furthermore, we formulate the domain shift problem for OCT diverse imagingsystems, and prove that the output resolution of a despeckling trained model isdetermined by the source domain resolution. We also provide possible remedies.We propose different practical implementations of our approach, verify andcompare their applicability, robustness, and computational efficiency. Ourresults demonstrate significant potential for generally improving samplecomplexity, generalization, and time efficiency, for coherent and non-coherentnoise reduction via supervised learning models, that can also be leveraged forother real-time computer vision applications."
Improving Generalization in Meta-Learning via Meta-Gradient Augmentation,"['Ren Wang', 'Haoliang Sun', 'Qi Wei', 'Xiushan Nie', 'Yuling Ma', 'Yilong Yin']",http://arxiv.org/pdf/2306.08460v1.pdf,2023-06-14,"['cs.lg', 'cs.ai']","  Meta-learning methods typically follow a two-loop framework, where each looppotentially suffers from notorious overfitting, hindering rapid adaptation andgeneralization to new tasks. Existing schemes solve it by enhancing themutual-exclusivity or diversity of training samples, but these datamanipulation strategies are data-dependent and insufficiently flexible. Thiswork alleviates overfitting in meta-learning from the perspective of gradientregularization and proposes a data-independent \textbf{M}eta-\textbf{G}radient\textbf{Aug}mentation (\textbf{MGAug}) method. The key idea is to first breakthe rote memories by network pruning to address memorization overfitting in theinner loop, and then the gradients of pruned sub-networks naturally form thehigh-quality augmentation of the meta-gradient to alleviate learner overfittingin the outer loop. Specifically, we explore three pruning strategies, including\textit{random width pruning}, \textit{random parameter pruning}, and a newlyproposed \textit{catfish pruning} that measures a Meta-Memorization CarryingAmount (MMCA) score for each parameter and prunes high-score ones to break rotememories as much as possible. The proposed MGAug is theoretically guaranteed bythe generalization bound from the PAC-Bayes framework. In addition, we extend alightweight version, called MGAug-MaxUp, as a trade-off between performancegains and resource overhead. Extensive experiments on multiple few-shotlearning benchmarks validate MGAug's effectiveness and significant improvementover various meta-baselines. The code is publicly available at\url{https://github.com/xxLifeLover/Meta-Gradient-Augmentation}."
DocumentNet: Bridging the Data Gap in Document Pre-Training,"['Lijun Yu', 'Jin Miao', 'Xiaoyu Sun', 'Jiayi Chen', 'Alexander G. Hauptmann', 'Hanjun Dai', 'Wei Wei']",http://arxiv.org/pdf/2306.08937v3.pdf,2023-06-15,"['cs.cl', 'cs.ir']","  Document understanding tasks, in particular, Visually-rich Document EntityRetrieval (VDER), have gained significant attention in recent years thanks totheir broad applications in enterprise AI. However, publicly available datahave been scarce for these tasks due to strict privacy constraints and highannotation costs. To make things worse, the non-overlapping entity spaces fromdifferent datasets hinder the knowledge transfer between document types. Inthis paper, we propose a method to collect massive-scale and weakly labeleddata from the web to benefit the training of VDER models. The collecteddataset, named DocumentNet, does not depend on specific document types orentity sets, making it universally applicable to all VDER tasks. The currentDocumentNet consists of 30M documents spanning nearly 400 document typesorganized in a four-level ontology. Experiments on a set of broadly adoptedVDER tasks show significant improvements when DocumentNet is incorporated intothe pre-training for both classic and few-shot learning settings. With therecent emergence of large language models (LLMs), DocumentNet provides a largedata source to extend their multi-modal capabilities for VDER."
Few-shot bioacoustic event detection at the DCASE 2023 challenge,"['Ines Nolasco', 'Burooj Ghani', 'Shubhr Singh', 'Ester Vidaña-Vila', 'Helen Whitehead', 'Emily Grout', 'Michael Emmerson', 'Frants Jensen', 'Ivan Kiskin', 'Joe Morford', 'Ariana Strandburg-Peshkin', 'Lisa Gill', 'Hanna Pamuła', 'Vincent Lostanlen', 'Dan Stowell']",http://arxiv.org/pdf/2306.09223v1.pdf,2023-06-15,"['cs.sd', 'cs.lg', 'eess.as']","  Few-shot bioacoustic event detection consists in detecting sound events ofspecified types, in varying soundscapes, while having access to only a fewexamples of the class of interest. This task ran as part of the DCASE challengefor the third time this year with an evaluation set expanded to include newanimal species, and a new rule: ensemble models were no longer allowed. The2023 few shot task received submissions from 6 different teams with F-scoresreaching as high as 63% on the evaluation set. Here we describe the task,focusing on describing the elements that differed from previous years. We alsotake a look back at past editions to describe how the task has evolved. Notonly have the F-score results steadily improved (40% to 60% to 63%), but thetype of systems proposed have also become more complex. Sound event detectionsystems are no longer simple variations of the baselines provided: multiplefew-shot learning methodologies are still strong contenders for the task."
FewSAR: A Few-shot SAR Image Classification Benchmark,"['Rui Zhang', 'Ziqi Wang', 'Yang Li', 'Jiabao Wang', 'Zhiteng Wang']",http://arxiv.org/pdf/2306.09592v1.pdf,2023-06-16,"['cs.cv', 'cs.lg']","  Few-shot learning (FSL) is one of the significant and hard problems in thefield of image classification. However, in contrast to the rapid development ofthe visible light dataset, the progress in SAR target image classification ismuch slower. The lack of unified benchmark is a key reason for this phenomenon,which may be severely overlooked by the current literature. The researchers ofSAR target image classification always report their new results on their owndatasets and experimental setup. It leads to inefficiency in result comparisonand impedes the further progress of this area. Motivated by this observation,we propose a novel few-shot SAR image classification benchmark (FewSAR) toaddress this issue. FewSAR consists of an open-source Python code library of 15classic methods in three categories for few-shot SAR image classification. Itprovides an accessible and customizable testbed for different few-shot SARimage classification task. To further understanding the performance ofdifferent few-shot methods, we establish evaluation protocols and conductextensive experiments within the benchmark. By analyzing the quantitativeresults and runtime under the same setting, we observe that the accuracy ofmetric learning methods can achieve the best results. Meta-learning methods andfine-tuning methods perform poorly on few-shot SAR images, which is primarilydue to the bias of existing datasets. We believe that FewSAR will open up a newavenue for future research and development, on real-world challenges at theintersection of SAR image classification and few-shot deep learning. We willprovide our code for the proposed FewSAR at https://github.com/solarlee/FewSAR."
Democratizing LLMs for Low-Resource Languages by Leveraging their  English Dominant Abilities with Linguistically-Diverse Prompts,"['Xuan-Phi Nguyen', 'Sharifah Mahani Aljunied', 'Shafiq Joty', 'Lidong Bing']",http://arxiv.org/pdf/2306.11372v1.pdf,2023-06-20,"['cs.cl', 'cs.ai']","  Large language models (LLMs) are known to effectively perform tasks by simplyobserving few exemplars. However, in low-resource languages, obtaining suchhand-picked exemplars can still be challenging, where unsupervised techniquesmay be necessary. Moreover, competent generative capabilities of LLMs areobserved only in high-resource languages, while their performances amongunder-represented languages fall behind due to pre-training data imbalance. Toelicit LLMs' ability onto low-resource languages without any supervised data,we propose to assemble synthetic exemplars from a diverse set of high-resourcelanguages to prompt the LLMs to translate from any language into English. Theseprompts are then used to create intra-lingual exemplars to perform tasks in thetarget languages. Our unsupervised prompting method performs on par withsupervised few-shot learning in LLMs of different sizes for translationsbetween English and 13 Indic and 21 African low-resource languages. We alsoshow that fine-tuning a 7B model on data generated from our method helps itperform competitively with a 175B model. In non-English translation tasks, ourmethod even outperforms supervised prompting by up to 3 chrF++ in manylow-resource languages. When evaluated on zero-shot multilingual summarization,our method surpasses other English-pivoting baselines by up to 4 ROUGE-L and isalso favored by GPT-4."
ProtoDiff: Learning to Learn Prototypical Networks by Task-Guided  Diffusion,"['Yingjun Du', 'Zehao Xiao', 'Shengcai Liao', 'Cees Snoek']",http://arxiv.org/pdf/2306.14770v2.pdf,2023-06-26,"['cs.lg', 'cs.ai']","  Prototype-based meta-learning has emerged as a powerful technique foraddressing few-shot learning challenges. However, estimating a deterministicprototype using a simple average function from a limited number of examplesremains a fragile process. To overcome this limitation, we introduce ProtoDiff,a novel framework that leverages a task-guided diffusion model during themeta-training phase to gradually generate prototypes, thereby providingefficient class representations. Specifically, a set of prototypes is optimizedto achieve per-task prototype overfitting, enabling accurately obtaining theoverfitted prototypes for individual tasks. Furthermore, we introduce atask-guided diffusion process within the prototype space, enabling themeta-learning of a generative process that transitions from a vanilla prototypeto an overfitted prototype. ProtoDiff gradually generates task-specificprototypes from random noise during the meta-test stage, conditioned on thelimited samples available for the new task. Furthermore, to expedite trainingand enhance ProtoDiff's performance, we propose the utilization of residualprototype learning, which leverages the sparsity of the residual prototype. Weconduct thorough ablation studies to demonstrate its ability to accuratelycapture the underlying prototype distribution and enhance generalization. Thenew state-of-the-art performance on within-domain, cross-domain, and few-taskfew-shot classification further substantiates the benefit of ProtoDiff."
Effective Transfer of Pretrained Large Visual Model for Fabric Defect  Segmentation via Specifc Knowledge Injection,"['Zhewei Chen', 'Wai Keung Wong', 'Zuofeng Zhong', 'Jinpiao Liao', 'Ying Qu']",http://arxiv.org/pdf/2306.16186v1.pdf,2023-06-28,"['cs.cv', 'cs.ai', 'i.2.10; i.4.9; i.5.4']","  Fabric defect segmentation is integral to textile quality control. Despitethis, the scarcity of high-quality annotated data and the diversity of fabricdefects present significant challenges to the application of deep learning inthis field. These factors limit the generalization and segmentation performanceof existing models, impeding their ability to handle the complexity of diversefabric types and defects. To overcome these obstacles, this study introduces aninnovative method to infuse specialized knowledge of fabric defects into theSegment Anything Model (SAM), a large-scale visual model. By introducing andtraining a unique set of fabric defect-related parameters, this approachseamlessly integrates domain-specific knowledge into SAM without the need forextensive modifications to the pre-existing model parameters. The revamped SAMmodel leverages generalized image understanding learned from large-scalenatural image datasets while incorporating fabric defect-specific knowledge,ensuring its proficiency in fabric defect segmentation tasks. The experimentalresults reveal a significant improvement in the model's segmentationperformance, attributable to this novel amalgamation of generic andfabric-specific knowledge. When benchmarking against popular existingsegmentation models across three datasets, our proposed model demonstrates asubstantial leap in performance. Its impressive results in cross-datasetcomparisons and few-shot learning experiments further demonstrate its potentialfor practical applications in textile quality control."
Black-Box Prediction of Flaky Test Fix Categories Using Language Models,"['Sakina Fatima', 'Hadi Hemmati', 'Lionel Briand']",http://arxiv.org/pdf/2307.00012v1.pdf,2023-06-21,"['cs.se', 'cs.ai', 'cs.lg']","  Flaky tests are problematic because they non-deterministically pass or failfor the same software version under test, causing confusion and wastingdeveloper time. While machine learning models have been used to predictflakiness and its root causes, there is less work on providing support to fixthe problem. To address this gap, we propose a framework that automaticallygenerates labeled datasets for 13 fix categories and train models to predictthe fix category of a flaky test by analyzing the test code only. Though it isunrealistic at this stage to accurately predict the fix itself, the categoriesprovide precise guidance about what part of the test code to look at. Ourapproach is based on language models, namely CodeBERT and UniXcoder, whoseoutput is fine-tuned with a Feed Forward Neural Network (FNN) or a SiameseNetwork-based Few Shot Learning (FSL). Our experimental results show thatUniXcoder outperforms CodeBERT, in correctly predicting most of the categoriesof fixes a developer should apply. Furthermore, FSL does not appear to have anysignificant effect. Given the high accuracy obtained for most fix categories,our proposed framework has the potential to help developers to fix flaky testsquickly and accurately.To aid future research, we make our automated labelingtool, dataset, prediction models, and experimental infrastructure publiclyavailable."
Prompting classes: Exploring the Power of Prompt Class Learning in  Weakly Supervised Semantic Segmentation,"['Balamurali Murugesan', 'Rukhshanda Hussain', 'Rajarshi Bhattacharya', 'Ismail Ben Ayed', 'Jose Dolz']",http://arxiv.org/pdf/2307.00097v2.pdf,2023-06-30,['cs.cv'],"  Recently, CLIP-based approaches have exhibited remarkable performance ongeneralization and few-shot learning tasks, fueled by the power of contrastivelanguage-vision pre-training. In particular, prompt tuning has emerged as aneffective strategy to adapt the pre-trained language-vision models todownstream tasks by employing task-related textual tokens. Motivated by thisprogress, in this work we question whether other fundamental problems, such asweakly supervised semantic segmentation (WSSS), can benefit from prompt tuning.Our findings reveal two interesting observations that shed light on the impactof prompt tuning on WSSS. First, modifying only the class token of the textprompt results in a greater impact on the Class Activation Map (CAM), comparedto arguably more complex strategies that optimize the context. And second, theclass token associated with the image ground truth does not necessarilycorrespond to the category that yields the best CAM. Motivated by theseobservations, we introduce a novel approach based on a PrOmpt cLass lEarning(POLE) strategy. Through extensive experiments we demonstrate that our simple,yet efficient approach achieves SOTA performance in a well-known WSSSbenchmark. These results highlight not only the benefits of language-visionmodels in WSSS but also the potential of prompt learning for this problem. Thecode is available at https://github.com/rB080/WSS_POLE."
Meta-training with Demonstration Retrieval for Efficient Few-shot  Learning,"['Aaron Mueller', 'Kanika Narang', 'Lambert Mathias', 'Qifan Wang', 'Hamed Firooz']",http://arxiv.org/pdf/2307.00119v1.pdf,2023-06-30,['cs.cl'],"  Large language models show impressive results on few-shot NLP tasks. However,these models are memory and computation-intensive. Meta-training allows one toleverage smaller models for few-shot generalization in a domain-general andtask-agnostic manner; however, these methods alone results in models that maynot have sufficient parameterization or knowledge to adapt quickly to a largevariety of tasks. To overcome this issue, we propose meta-training withdemonstration retrieval, where we use a dense passage retriever to retrievesemantically similar labeled demonstrations to each example for more variedsupervision. By separating external knowledge from model parameters, we can usemeta-training to train parameter-efficient models that generalize well on alarger variety of tasks. We construct a meta-training set from UnifiedQA andCrossFit, and propose a demonstration bank based on UnifiedQA tasks. To ourknowledge, our work is the first to combine retrieval with meta-training, touse DPR models to retrieve demonstrations, and to leverage demonstrations frommany tasks simultaneously, rather than randomly sampling demonstrations fromthe training set of the target task. Our approach outperforms a variety oftargeted parameter-efficient and retrieval-augmented few-shot methods on QA,NLI, and text classification tasks (including SQuAD, QNLI, and TREC). Ourapproach can be meta-trained and fine-tuned quickly on a single GPU."
Task-Specific Alignment and Multiple Level Transformer for Few-Shot  Action Recognition,"['Fei Guo', 'Li Zhu', 'YiWang Wang', 'Jing Sun']",http://arxiv.org/pdf/2307.01985v2.pdf,2023-07-05,"['cs.cv', 'cs.dm']","  In the research field of few-shot learning, the main difference betweenimage-based and video-based is the additional temporal dimension. In recentyears, some works have used the Transformer to deal with frames, then get theattention feature and the enhanced prototype, and the results are competitive.However, some video frames may relate little to the action, and only usingsingle frame-level or segment-level features may not mine enough information.We address these problems sequentially through an end-to-end method named""Task-Specific Alignment and Multiple-level Transformer Network (TSA-MLT)"". Thefirst module (TSA) aims at filtering the action-irrelevant frames for actionduration alignment. Affine Transformation for frame sequence in the timedimension is used for linear sampling. The second module (MLT) focuses on theMultiple-level feature of the support prototype and query sample to mine moreinformation for the alignment, which operates on different level features. Weadopt a fusion loss according to a fusion distance that fuses the L2 sequencedistance, which focuses on temporal order alignment, and the Optimal Transportdistance, which focuses on measuring the gap between the appearance andsemantics of the videos. Extensive experiments show our method achievesstate-of-the-art results on the HMDB51 and UCF101 datasets and a competitiveresult on the benchmark of Kinetics and something 2-something V2 datasets. Ourcode is available at the URL: https://github.com/cofly2014/tsa-mlt.git"
TablEye: Seeing small Tables through the Lens of Images,"['Seung-eon Lee', 'Sang-Chul Lee']",http://arxiv.org/pdf/2307.02491v1.pdf,2023-07-04,"['cs.lg', 'cs.ai']","  The exploration of few-shot tabular learning becomes imperative. Tabular datais a versatile representation that captures diverse information, yet it is notexempt from limitations, property of data and model size. Labeling extensivetabular data can be challenging, and it may not be feasible to capture everyimportant feature. Few-shot tabular learning, however, remains relativelyunexplored, primarily due to scarcity of shared information among independentdatasets and the inherent ambiguity in defining boundaries within tabular data.To the best of our knowledge, no meaningful and unrestricted few-shot tabularlearning techniques have been developed without imposing constraints on thedataset. In this paper, we propose an innovative framework called TablEye,which aims to overcome the limit of forming prior knowledge for tabular data byadopting domain transformation. It facilitates domain transformation bygenerating tabular images, which effectively conserve the intrinsic semanticsof the original tabular data. This approach harnesses rigorously testedfew-shot learning algorithms and embedding functions to acquire and apply priorknowledge. Leveraging shared data domains allows us to utilize this priorknowledge, originally learned from the image domain. Specifically, TablEyedemonstrated a superior performance by outstripping the TabLLM in a 4-shot taskwith a maximum 0.11 AUC and a STUNT in a 1- shot setting, where it led onaverage by 3.17% accuracy."
Text Descriptions are Compressive and Invariant Representations for  Visual Learning,"['Zhili Feng', 'Anna Bair', 'J. Zico Kolter']",http://arxiv.org/pdf/2307.04317v2.pdf,2023-07-10,"['cs.cv', 'cs.lg']","  Modern image classification is based upon directly predicting classes vialarge discriminative networks, which do not directly contain information aboutthe intuitive visual features that may constitute a classification decision.Recently, work in vision-language models (VLM) such as CLIP has provided waysto specify natural language descriptions of image classes, but typicallyfocuses on providing single descriptions for each class. In this work, wedemonstrate that an alternative approach, in line with humans' understanding ofmultiple visual features per class, can also provide compelling performance inthe robust few-shot learning setting. In particular, we introduce a novelmethod, \textit{SLR-AVD (Sparse Logistic Regression using Augmented VisualDescriptors)}. This method first automatically generates multiple visualdescriptions of each class via a large language model (LLM), then uses a VLM totranslate these descriptions to a set of visual feature embeddings of eachimage, and finally uses sparse logistic regression to select a relevant subsetof these features to classify each image. Core to our approach is the factthat, information-theoretically, these descriptive features are more invariantto domain shift than traditional image embeddings, even though the VLM trainingprocess is not explicitly designed for invariant representation learning. Theseinvariant descriptive features also compose a better input compression scheme.When combined with finetuning, we show that SLR-AVD is able to outperformexisting state-of-the-art finetuning approaches on both in-distribution andout-of-distribution performance."
SAR-NeRF: Neural Radiance Fields for Synthetic Aperture Radar Multi-View  Representation,"['Zhengxin Lei', 'Feng Xu', 'Jiangtao Wei', 'Feng Cai', 'Feng Wang', 'Ya-Qiu Jin']",http://arxiv.org/pdf/2307.05087v1.pdf,2023-07-11,"['cs.cv', 'eess.iv']","  SAR images are highly sensitive to observation configurations, and theyexhibit significant variations across different viewing angles, making itchallenging to represent and learn their anisotropic features. As a result,deep learning methods often generalize poorly across different view angles.Inspired by the concept of neural radiance fields (NeRF), this study combinesSAR imaging mechanisms with neural networks to propose a novel NeRF model forSAR image generation. Following the mapping and projection pinciples, a set ofSAR images is modeled implicitly as a function of attenuation coefficients andscattering intensities in the 3D imaging space through a differentiablerendering equation. SAR-NeRF is then constructed to learn the distribution ofattenuation coefficients and scattering intensities of voxels, where thevectorized form of 3D voxel SAR rendering equation and the samplingrelationship between the 3D space voxels and the 2D view ray grids areanalytically derived. Through quantitative experiments on various datasets, wethoroughly assess the multi-view representation and generalization capabilitiesof SAR-NeRF. Additionally, it is found that SAR-NeRF augumented dataset cansignificantly improve SAR target classification performance under few-shotlearning setup, where a 10-type classification accuracy of 91.6\% can beachieved by using only 12 images per class."
TinyMetaFed: Efficient Federated Meta-Learning for TinyML,"['Haoyu Ren', 'Xue Li', 'Darko Anicic', 'Thomas A. Runkler']",http://arxiv.org/pdf/2307.06822v3.pdf,2023-07-13,"['cs.lg', 'cs.ai', 'cs.dc']","  The field of Tiny Machine Learning (TinyML) has made substantial advancementsin democratizing machine learning on low-footprint devices, such asmicrocontrollers. The prevalence of these miniature devices raises the questionof whether aggregating their knowledge can benefit TinyML applications.Federated meta-learning is a promising answer to this question, as it addressesthe scarcity of labeled data and heterogeneous data distribution across devicesin the real world. However, deploying TinyML hardware faces unique resourceconstraints, making existing methods impractical due to energy, privacy, andcommunication limitations. We introduce TinyMetaFed, a model-agnosticmeta-learning framework suitable for TinyML. TinyMetaFed facilitatescollaborative training of a neural network initialization that can be quicklyfine-tuned on new devices. It offers communication savings and privacyprotection through partial local reconstruction and Top-P% selectivecommunication, computational efficiency via online learning, and robustness toclient heterogeneity through few-shot learning. The evaluations on three TinyMLuse cases demonstrate that TinyMetaFed can significantly reduce energyconsumption and communication overhead, accelerate convergence, and stabilizethe training process."
DialogStudio: Towards Richest and Most Diverse Unified Dataset  Collection for Conversational AI,"['Jianguo Zhang', 'Kun Qian', 'Zhiwei Liu', 'Shelby Heinecke', 'Rui Meng', 'Ye Liu', 'Zhou Yu', 'Huan Wang', 'Silvio Savarese', 'Caiming Xiong']",http://arxiv.org/pdf/2307.10172v2.pdf,2023-07-19,"['cs.cl', 'cs.ai']","  Despite advancements in conversational AI, language models encounterchallenges to handle diverse conversational tasks, and existing dialoguedataset collections often lack diversity and comprehensiveness. To tackle theseissues, we introduce DialogStudio: the largest and most diverse collection ofdialogue datasets, unified under a consistent format while preserving theiroriginal information. Our collection encompasses data from open-domaindialogues, task-oriented dialogues, natural language understanding,conversational recommendation, dialogue summarization, and knowledge-groundeddialogues, making it an incredibly rich and diverse resource for dialogueresearch and model training. To further enhance the utility of DialogStudio, weidentify the licenses for each dataset and design domain-aware prompts forselected dialogues to facilitate instruction-aware fine-tuning. Furthermore, wedevelop conversational AI models using the dataset collection, and ourexperiments in both zero-shot and few-shot learning scenarios demonstrate thesuperiority of DialogStudio. To improve transparency and support dataset andtask-based research, as well as language model pre-training, all datasets,licenses, codes, and models associated with DialogStudio are made publiclyaccessible at https://github.com/salesforce/DialogStudio"
Mutual Reinforcement Effects in Japanese Sentence Classification and  Named Entity Recognition Tasks,"['Chengguang Gan', 'Qinghao Zhang', 'Tatsunori Mori']",http://arxiv.org/pdf/2307.10291v2.pdf,2023-07-18,['cs.cl'],"  Information extraction(IE) is a crucial subfield within natural languageprocessing. However, for the traditionally segmented approach to sentenceclassification and Named Entity Recognition, the intricate interactions betweenthese individual subtasks remain largely uninvestigated. In this study, wepropose an integrative analysis, converging sentence classification with NamedEntity Recognition, with the objective to unveil and comprehend the mutualreinforcement effect within these two information extraction subtasks. Toachieve this, we introduce a Sentence Classification and Named EntityRecognition Multi-task (SCNM) approach that combines Sentence Classification(SC) and Named Entity Recognition (NER). We develop a Sentence-to-LabelGeneration (SLG) framework for SCNM and construct a Wikipedia datasetcontaining both SC and NER. Using a format converter, we unify input formatsand employ a generative model to generate SC-labels, NER-labels, and associatedtext segments. We propose a Constraint Mechanism (CM) to improve generatedformat accuracy. Our results show SC accuracy increased by 1.13 points and NERby 1.06 points in SCNM compared to standalone tasks, with CM raising formataccuracy from 63.61 to 100. The findings indicate mutual reinforcement effectsbetween SC and NER, and integration enhances both tasks' performance. Weadditionally implemented the SLG framework on single SC task. It yieldedsuperior accuracies compared to the baseline on two distinct Japanese SCdatasets. Notably, in the experiment of few-shot learning, SLG framework showsmuch better performance than fine-tune method. These empirical findingscontribute additional evidence to affirm the efficacy of the SLG framework."
CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study,"['Zihan Guan', 'Zihao Wu', 'Zhengliang Liu', 'Dufan Wu', 'Hui Ren', 'Quanzheng Li', 'Xiang Li', 'Ninghao Liu']",http://arxiv.org/pdf/2307.11346v1.pdf,2023-07-21,"['cs.cl', 'cs.ai']","  Participant recruitment based on unstructured medical texts such as clinicalnotes and radiology reports has been a challenging yet important task for thecohort establishment in clinical research. Recently, Large Language Models(LLMs) such as ChatGPT have achieved tremendous success in various downstreamtasks thanks to their promising performance in language understanding,inference, and generation. It is then natural to test their feasibility insolving the cohort recruitment task, which involves the classification of agiven paragraph of medical text into disease label(s). However, when applied toknowledge-intensive problem settings such as medical text classification, wherethe LLMs are expected to understand the decision made by human experts andaccurately identify the implied disease labels, the LLMs show a mediocreperformance. A possible explanation is that, by only using the medical text,the LLMs neglect to use the rich context of additional information thatlanguages afford. To this end, we propose to use a knowledge graph as auxiliaryinformation to guide the LLMs in making predictions. Moreover, to further boostthe LLMs adapt to the problem setting, we apply a chain-of-thought (CoT) sampleselection strategy enhanced by reinforcement learning, which selects a set ofCoT samples given each individual medical report. Experimental results andvarious ablation studies show that our few-shot learning method achievessatisfactory performance compared with fine-tuning strategies and gains superbadvantages when the available data is limited. The code and sample dataset ofthe proposed CohortGPT model is available at:https://anonymous.4open.science/r/CohortGPT-4872/"
Identifying Misinformation on YouTube through Transcript Contextual  Analysis with Transformer Models,"['Christos Christodoulou', 'Nikos Salamanos', 'Pantelitsa Leonidou', 'Michail Papadakis', 'Michael Sirivianos']",http://arxiv.org/pdf/2307.12155v1.pdf,2023-07-22,['cs.cl'],"  Misinformation on YouTube is a significant concern, necessitating robustdetection strategies. In this paper, we introduce a novel methodology for videoclassification, focusing on the veracity of the content. We convert theconventional video classification task into a text classification task byleveraging the textual content derived from the video transcripts. We employadvanced machine learning techniques like transfer learning to solve theclassification challenge. Our approach incorporates two forms of transferlearning: (a) fine-tuning base transformer models such as BERT, RoBERTa, andELECTRA, and (b) few-shot learning using sentence-transformers MPNet andRoBERTa-large. We apply the trained models to three datasets: (a) YouTubeVaccine-misinformation related videos, (b) YouTube Pseudoscience videos, and(c) Fake-News dataset (a collection of articles). Including the Fake-Newsdataset extended the evaluation of our approach beyond YouTube videos. Usingthese datasets, we evaluated the models distinguishing valid information frommisinformation. The fine-tuned models yielded Matthews CorrelationCoefficient>0.81, accuracy>0.90, and F1 score>0.90 in two of three datasets.Interestingly, the few-shot models outperformed the fine-tuned ones by 20% inboth Accuracy and F1 score for the YouTube Pseudoscience dataset, highlightingthe potential utility of this approach -- especially in the context of limitedtraining data."
Sparse annotation strategies for segmentation of short axis cardiac MRI,"['Josh Stein', 'Maxime Di Folco', 'Julia Schnabel']",http://arxiv.org/pdf/2307.12619v1.pdf,2023-07-24,"['eess.iv', 'cs.cv']","  Short axis cardiac MRI segmentation is a well-researched topic, withexcellent results achieved by state-of-the-art models in a supervised setting.However, annotating MRI volumes is time-consuming and expensive. Many differentapproaches (e.g. transfer learning, data augmentation, few-shot learning, etc.)have emerged in an effort to use fewer annotated data and still achieve similarperformance as a fully supervised model. Nevertheless, to the best of ourknowledge, none of these works focus on which slices of MRI volumes are mostimportant to annotate for yielding the best segmentation results. In thispaper, we investigate the effects of training with sparse volumes, i.e.reducing the number of cases annotated, and sparse annotations, i.e. reducingthe number of slices annotated per case. We evaluate the segmentationperformance using the state-of-the-art nnU-Net model on two public datasets toidentify which slices are the most important to annotate. We have shown thattraining on a significantly reduced dataset (48 annotated volumes) can give aDice score greater than 0.85 and results comparable to using the full dataset(160 and 240 volumes for each dataset respectively). In general, training onmore slice annotations provides more valuable information compared to trainingon more volumes. Further, annotating slices from the middle of volumes yieldsthe most beneficial results in terms of segmentation performance, and theapical region the worst. When evaluating the trade-off between annotatingvolumes against slices, annotating as many slices as possible instead ofannotating more volumes is a better strategy."
Self-supervised Few-shot Learning for Semantic Segmentation: An  Annotation-free Approach,"['Sanaz Karimijafarbigloo', 'Reza Azad', 'Dorit Merhof']",http://arxiv.org/pdf/2307.14446v1.pdf,2023-07-26,['cs.cv'],"  Few-shot semantic segmentation (FSS) offers immense potential in the field ofmedical image analysis, enabling accurate object segmentation with limitedtraining data. However, existing FSS techniques heavily rely on annotatedsemantic classes, rendering them unsuitable for medical images due to thescarcity of annotations. To address this challenge, multiple contributions areproposed: First, inspired by spectral decomposition methods, the problem ofimage decomposition is reframed as a graph partitioning task. The eigenvectorsof the Laplacian matrix, derived from the feature affinity matrix ofself-supervised networks, are analyzed to estimate the distribution of theobjects of interest from the support images. Secondly, we propose a novelself-supervised FSS framework that does not rely on any annotation. Instead, itadaptively estimates the query mask by leveraging the eigenvectors obtainedfrom the support images. This approach eliminates the need for manualannotation, making it particularly suitable for medical images with limitedannotated data. Thirdly, to further enhance the decoding of the query imagebased on the information provided by the support image, we introduce amulti-scale large kernel attention module. By selectively emphasizing relevantfeatures and details, this module improves the segmentation process andcontributes to better object delineation. Evaluations on both natural andmedical image datasets demonstrate the efficiency and effectiveness of ourmethod. Moreover, the proposed approach is characterized by its generality andmodel-agnostic nature, allowing for seamless integration with various deeparchitectures. The code is publicly available at\href{https://github.com/mindflow-institue/annotation_free_fewshot}{\textcolor{magenta}{GitHub}}."
A Weakly Supervised Segmentation Network Embedding Cross-scale Attention  Guidance and Noise-sensitive Constraint for Detecting Tertiary Lymphoid  Structures of Pancreatic Tumors,"['Bingxue Wang', 'Liwen Zou', 'Jun Chen', 'Yingying Cao', 'Zhenghua Cai', 'Yudong Qiu', 'Liang Mao', 'Zhongqiu Wang', 'Jingya Chen', 'Luying Gui', 'Xiaoping Yang']",http://arxiv.org/pdf/2307.14603v1.pdf,2023-07-27,"['eess.iv', 'cs.cv']","  The presence of tertiary lymphoid structures (TLSs) on pancreaticpathological images is an important prognostic indicator of pancreatic tumors.Therefore, TLSs detection on pancreatic pathological images plays a crucialrole in diagnosis and treatment for patients with pancreatic tumors. However,fully supervised detection algorithms based on deep learning usually require alarge number of manual annotations, which is time-consuming andlabor-intensive. In this paper, we aim to detect the TLSs in a manner offew-shot learning by proposing a weakly supervised segmentation network. Wefirstly obtain the lymphocyte density maps by combining a pretrained model fornuclei segmentation and a domain adversarial network for lymphocyte nucleirecognition. Then, we establish a cross-scale attention guidance mechanism byjointly learning the coarse-scale features from the original histopathologyimages and fine-scale features from our designed lymphocyte density attention.A noise-sensitive constraint is introduced by an embedding signed distancefunction loss in the training procedure to reduce tiny prediction errors.Experimental results on two collected datasets demonstrate that our proposedmethod significantly outperforms the state-of-the-art segmentation-basedalgorithms in terms of TLSs detection accuracy. Additionally, we apply ourmethod to study the congruent relationship between the density of TLSs andperipancreatic vascular invasion and obtain some clinically statisticalresults."
GeneMask: Fast Pretraining of Gene Sequences to Enable Few-Shot Learning,"['Soumyadeep Roy', 'Jonas Wallat', 'Sowmya S Sundaram', 'Wolfgang Nejdl', 'Niloy Ganguly']",http://arxiv.org/pdf/2307.15933v1.pdf,2023-07-29,['cs.cl'],"  Large-scale language models such as DNABert and LOGO aim to learn optimalgene representations and are trained on the entire Human Reference Genome.However, standard tokenization schemes involve a simple sliding window oftokens like k-mers that do not leverage any gene-based semantics and thus maylead to (trivial) masking of easily predictable sequences and subsequentlyinefficient Masked Language Modeling (MLM) training. Therefore, we propose anovel masking algorithm, GeneMask, for MLM training of gene sequences, where werandomly identify positions in a gene sequence as mask centers and locallyselect the span around the mask center with the highest Normalized PointwiseMutual Information (NPMI) to mask. We observe that in the absence ofhuman-understandable semantics in the genomics domain (in contrast, semanticunits like words and phrases are inherently available in NLP), GeneMask-basedmodels substantially outperform the SOTA models (DNABert and LOGO) over fourbenchmark gene sequence classification datasets in five few-shot settings (10to 1000-shot). More significantly, the GeneMask-based DNABert model is trainedfor less than one-tenth of the number of epochs of the original SOTA model. Wealso observe a strong correlation between top-ranked PMI tokens and conservedDNA sequence motifs, which may indicate the incorporation of latent genomicinformation. The codes (including trained models) and datasets are madepublicly available at https://github.com/roysoumya/GeneMask."
ChatGPT for Arabic Grammatical Error Correction,"['Sang Yun Kwon', 'Gagan Bhatia', 'El Moatez Billah Nagoud', 'Muhammad Abdul-Mageed']",http://arxiv.org/pdf/2308.04492v1.pdf,2023-08-08,['cs.ai'],"  Recently, large language models (LLMs) fine-tuned to follow human instructionhave exhibited significant capabilities in various English NLP tasks. However,their performance in grammatical error correction (GEC) tasks, particularly innon-English languages, remains significantly unexplored. In this paper, wedelve into abilities of instruction fine-tuned LLMs in Arabic GEC, a task madecomplex due to Arabic's rich morphology. Our findings suggest that variousprompting methods, coupled with (in-context) few-shot learning, demonstrateconsiderable effectiveness, with GPT-4 achieving up to $65.49$F\textsubscript{1} score under expert prompting (approximately $5$ pointshigher than our established baseline). This highlights the potential of LLMs inlow-resource settings, offering a viable approach for generating usefulsynthetic data for model training. Despite these positive results, we find thatinstruction fine-tuned models, regardless of their size, significantlyunderperform compared to fully fine-tuned models of significantly smallersizes. This disparity highlights a substantial room for improvements for LLMs.Inspired by methods from low-resource machine translation, we also develop amethod exploiting synthetic data that significantly outperforms previous modelson two standard Arabic benchmarks. Our work sets new SoTA for Arabic GEC, with$72.19\%$ and $73.26$ F$_{1}$ on the 2014 and 2015 QALB datasets, respectively."
LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking,"['Fahim Dalvi', 'Maram Hasanain', 'Sabri Boughorbel', 'Basel Mousi', 'Samir Abdaljalil', 'Nizi Nazar', 'Ahmed Abdelali', 'Shammur Absar Chowdhury', 'Hamdy Mubarak', 'Ahmed Ali', 'Majd Hawasly', 'Nadir Durrani', 'Firoj Alam']",http://arxiv.org/pdf/2308.04945v1.pdf,2023-08-09,"['cs.cl', 'cs.ai', '68t50', 'f.2.2; i.2.7']","  The recent development and success of Large Language Models (LLMs)necessitate an evaluation of their performance across diverse NLP tasks indifferent languages. Although several frameworks have been developed and madepublicly available, their customization capabilities for specific tasks anddatasets are often complex for different users. In this study, we introduce theLLMeBench framework. Initially developed to evaluate Arabic NLP tasks usingOpenAI's GPT and BLOOM models; it can be seamlessly customized for any NLP taskand model, regardless of language. The framework also features zero- andfew-shot learning settings. A new custom dataset can be added in less than 10minutes, and users can use their own model API keys to evaluate the task athand. The developed framework has been already tested on 31 unique NLP tasksusing 53 publicly available datasets within 90 experimental setups, involvingapproximately 296K data points. We plan to open-source the framework for thecommunity (https://github.com/qcri/LLMeBench/). A video demonstrating theframework is available online (https://youtu.be/FkQn4UjYA0s)."
Global in Local: A Convolutional Transformer for SAR ATR FSL,"['Chenwei Wang', 'Yulin Huang', 'Xiaoyu Liu', 'Jifang Pei', 'Yin Zhang', 'Jianyu Yang']",http://arxiv.org/pdf/2308.05464v1.pdf,2023-08-10,['eess.iv'],"  Convolutional neural networks (CNNs) have dominated the synthetic apertureradar (SAR) automatic target recognition (ATR) for years. However, under thelimited SAR images, the width and depth of the CNN-based models are limited,and the widening of the received field for global features in images ishindered, which finally leads to the low performance of recognition. To addressthese challenges, we propose a Convolutional Transformer (ConvT) for SAR ATRfew-shot learning (FSL). The proposed method focuses on constructing ahierarchical feature representation and capturing global dependencies of localfeatures in each layer, named global in local. A novel hybrid loss is proposedto interpret the few SAR images in the forms of recognition labels andcontrastive image pairs, construct abundant anchor-positive and anchor-negativeimage pairs in one batch and provide sufficient loss for the optimization ofthe ConvT to overcome the few sample effect. An auto augmentation is proposedto enhance and enrich the diversity and amount of the few training samples toexplore the hidden feature in a few SAR images and avoid the over-fitting inSAR ATR FSL. Experiments conducted on the Moving and Stationary TargetAcquisition and Recognition dataset (MSTAR) have shown the effectiveness of ourproposed ConvT for SAR ATR FSL. Different from existing SAR ATR FSL methodsemploying additional training datasets, our method achieved pioneeringperformance without other SAR target images in training."
Link-Context Learning for Multimodal LLMs,"['Yan Tai', 'Weichen Fan', 'Zhao Zhang', 'Feng Zhu', 'Rui Zhao', 'Ziwei Liu']",http://arxiv.org/pdf/2308.07891v1.pdf,2023-08-15,"['cs.cv', 'cs.cl']","  The ability to learn from context with novel concepts, and deliverappropriate responses are essential in human conversations. Despite currentMultimodal Large Language Models (MLLMs) and Large Language Models (LLMs) beingtrained on mega-scale datasets, recognizing unseen images or understandingnovel concepts in a training-free manner remains a challenge. In-ContextLearning (ICL) explores training-free few-shot learning, where models areencouraged to ``learn to learn"" from limited tasks and generalize to unseentasks. In this work, we propose link-context learning (LCL), which emphasizes""reasoning from cause and effect"" to augment the learning capabilities ofMLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causalrelationship between the support set and the query set. By providingdemonstrations with causal links, LCL guides the model to discern not only theanalogy but also the underlying causal associations between data points, whichempowers MLLMs to recognize unseen images and understand novel concepts moreeffectively. To facilitate the evaluation of this novel approach, we introducethe ISEKAI dataset, comprising exclusively of unseen generated image-labelpairs designed for link-context learning. Extensive experiments show that ourLCL-MLLM exhibits strong link-context learning capabilities to novel conceptsover vanilla MLLMs. Code and data will be released athttps://github.com/isekai-portal/Link-Context-Learning."
CodeCoT and Beyond: Learning to Program and Test like a Developer,"['Dong Huang', 'Qingwen Bu', 'Heming Cui']",http://arxiv.org/pdf/2308.08784v1.pdf,2023-08-17,"['cs.se', 'cs.ai']","  In natural language processing, transformer-based large language models(LLMs) like GPT-x models developed by OpenAI have revolutionized the landscape.Despite their impressive capabilities, these models often encounter challengeswhen handling tasks that differ from their training data, resulting incompromised performance. To address this, few-shot learning has emerged as avaluable technique, allowing LLMs to adapt with minimal task-specific data. Oneinnovative strategy, known as Chain-of-Thought Prompting (CoT), has beenintroduced to guide LLMs in revealing cognitive processes during multi-stepreasoning. In this paper, we propose Code Chain-of-Thought~(CodeCoT), whichconsists of two components: the Vanilla CodeCoT and the Self-exam CodeCoT. Thelatter incorporates self-examination, empowering the model to iterativelygenerate code, formulate test cases, and refine its outputs. Specifically, theprocess entails the generation of test examples by the model corresponding tothe code it is tasked to implement. If it fails on the test examples, then itregenerates the code based on the erroneous code and associated error types.Through comprehensive experiments, we observed that both techniquessignificantly enhance code generation accuracy across various LLM variants. Ourevaluation results reveal that CodeCoT improves the code generationeffectiveness, including an unprecedented pass@1 accuracy of 79.27\% using theSelf-exam CodeCoT approach on the gpt-3.5-turbo-0613 model in the HumanEvaldataset."
Measuring the Effect of Causal Disentanglement on the Adversarial  Robustness of Neural Network Models,"['Preben M. Ness', 'Dusica Marijan', 'Sunanda Bose']",http://arxiv.org/pdf/2308.10708v1.pdf,2023-08-21,['cs.lg'],"  Causal Neural Network models have shown high levels of robustness toadversarial attacks as well as an increased capacity for generalisation taskssuch as few-shot learning and rare-context classification compared totraditional Neural Networks. This robustness is argued to stem from thedisentanglement of causal and confounder input signals. However, noquantitative study has yet measured the level of disentanglement achieved bythese types of causal models or assessed how this relates to their adversarialrobustness.  Existing causal disentanglement metrics are not applicable to deterministicmodels trained on real-world datasets. We, therefore, utilise metrics ofcontent/style disentanglement from the field of Computer Vision to measuredifferent aspects of the causal disentanglement for four state-of-the-artcausal Neural Network models. By re-implementing these models with a commonResNet18 architecture we are able to fairly measure their adversarialrobustness on three standard image classification benchmarking datasets underseven common white-box attacks. We find a strong association (r=0.820, p=0.001)between the degree to which models decorrelate causal and confounder signalsand their adversarial robustness. Additionally, we find a moderate negativeassociation between the pixel-level information content of the confoundersignal and adversarial robustness (r=-0.597, p=0.040)."
Few-shot Anomaly Detection in Text with Deviation Learning,"['Anindya Sundar Das', 'Aravind Ajay', 'Sriparna Saha', 'Monowar Bhuyan']",http://arxiv.org/pdf/2308.11780v1.pdf,2023-08-22,"['cs.lg', 'cs.cl']","  Most current methods for detecting anomalies in text concentrate onconstructing models solely relying on unlabeled data. These models operate onthe presumption that no labeled anomalous examples are available, whichprevents them from utilizing prior knowledge of anomalies that are typicallypresent in small numbers in many real-world applications. Furthermore, thesemodels prioritize learning feature embeddings rather than optimizing anomalyscores directly, which could lead to suboptimal anomaly scoring and inefficientuse of data during the learning process. In this paper, we introduce FATE, adeep few-shot learning-based framework that leverages limited anomaly examplesand learns anomaly scores explicitly in an end-to-end method using deviationlearning. In this approach, the anomaly scores of normal examples are adjustedto closely resemble reference scores obtained from a prior distribution.Conversely, anomaly samples are forced to have anomalous scores thatconsiderably deviate from the reference score in the upper tail of the prior.Additionally, our model is optimized to learn the distinct behavior ofanomalies by utilizing a multi-head self-attention layer and multiple instancelearning approaches. Comprehensive experiments on several benchmark datasetsdemonstrate that our proposed approach attains a new level of state-of-the-artperformance."
Cabrita: closing the gap for foreign languages,"['Celio Larcher', 'Marcos Piau', 'Paulo Finardi', 'Pedro Gengo', 'Piero Esposito', 'Vinicius Caridá']",http://arxiv.org/pdf/2308.11878v1.pdf,2023-08-23,"['cs.cl', 'cs.ai', 'cs.lg']","  The strategy of training the model from scratch in a specific language ordomain serves two essential purposes: i) enhancing performance in theparticular linguistic or domain context, and ii) ensuring effectivetokenization. The main limitation inherent to this approach lies in theassociated cost, which can reach six to seven-digit dollar values, depending onthe model size and the number of parameters involved.  The main solution to overcome the cost challenge is to rely on availablepre-trained models, which, despite recent advancements such as the LLaMA andLLaMA-2 models, still demonstrate inefficiency for certain specific domainproblems or prove ineffective in scenarios involving conversational memoryresources, given the large number of tokens required to represent text.  To overcome this issue, we present a methodology named Cabrita, which, as ourresearch demonstrates, successfully addresses the performance and efficienttokenization problem, all at an affordable cost. We believe that thismethodology can be applied to any transformer-like architecture model. Tovalidate the study, we conducted continuous pre-training exclusively usingPortuguese text on a 3-billion-parameter model known as OpenLLaMA, resulting ina model named openCabrita 3B. The openCabrita 3B also features a new tokenizerthat results in a significant reduction in the number of tokens required torepresent the text. In our assessment, for few-shot learning tasks, we achievedsimilar results with this 3B model compared to a traditional continuouspre-training approach as well as to 7B models English pre-trained models."
Large Language Models Vote: Prompting for Rare Disease Identification,"['David Oniani', 'Jordan Hilsman', 'Hang Dong', 'Fengyi Gao', 'Shiven Verma', 'Yanshan Wang']",http://arxiv.org/pdf/2308.12890v2.pdf,2023-08-24,"['cs.cl', 'cs.ai']","  The emergence of generative Large Language Models (LLMs) emphasizes the needfor accurate and efficient prompting approaches. LLMs are often applied inFew-Shot Learning (FSL) contexts, where tasks are executed with minimaltraining data. FSL has become popular in many Artificial Intelligence (AI)subdomains, including AI for health. Rare diseases affect a small fraction ofthe population. Rare disease identification from clinical notes inherentlyrequires FSL techniques due to limited data availability. Manual datacollection and annotation is both expensive and time-consuming. In this paper,we propose Models-Vote Prompting (MVP), a flexible prompting approach forimproving the performance of LLM queries in FSL settings. MVP works byprompting numerous LLMs to perform the same tasks and then conducting amajority vote on the resulting outputs. This method achieves improved resultsto any one model in the ensemble on one-shot rare disease identification andclassification tasks. We also release a novel rare disease dataset for FSL,available to those who signed the MIMIC-IV Data Use Agreement (DUA).Furthermore, in using MVP, each model is prompted multiple times, substantiallyincreasing the time needed for manual annotation, and to address this, weassess the feasibility of using JSON for automating generative LLM evaluation."
Diagnosing Infeasible Optimization Problems Using Large Language Models,"['Hao Chen', 'Gonzalo E. Constante-Flores', 'Can Li']",http://arxiv.org/pdf/2308.12923v1.pdf,2023-08-23,"['cs.hc', 'cs.cl', 'cs.lg', 'math.oc']","  Decision-making problems can be represented as mathematical optimizationmodels, finding wide applications in fields such as economics, engineering andmanufacturing, transportation, and health care. Optimization models aremathematical abstractions of the problem of making the best decision whilesatisfying a set of requirements or constraints. One of the primary barriers todeploying these models in practice is the challenge of helping practitionersunderstand and interpret such models, particularly when they are infeasible,meaning no decision satisfies all the constraints. Existing methods fordiagnosing infeasible optimization models often rely on expert systems,necessitating significant background knowledge in optimization. In this paper,we introduce OptiChat, a first-of-its-kind natural language-based systemequipped with a chatbot GUI for engaging in interactive conversations aboutinfeasible optimization models. OptiChat can provide natural languagedescriptions of the optimization model itself, identify potential sources ofinfeasibility, and offer suggestions to make the model feasible. Theimplementation of OptiChat is built on GPT-4, which interfaces with anoptimization solver to identify the minimal subset of constraints that renderthe entire optimization problem infeasible, also known as the IrreducibleInfeasible Subset (IIS). We utilize few-shot learning, expert chain-of-thought,key-retrieve, and sentiment prompts to enhance OptiChat's reliability. Ourexperiments demonstrate that OptiChat assists both expert and non-expert usersin improving their understanding of the optimization models, enabling them toquickly identify the sources of infeasibility."
Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via  Training-free Networks,"['Xiangyang Zhu', 'Renrui Zhang', 'Bowei He', 'Ziyu Guo', 'Jiaming Liu', 'Hao Dong', 'Peng Gao']",http://arxiv.org/pdf/2308.12961v1.pdf,2023-08-24,['cs.cv'],"  To reduce the reliance on large-scale datasets, recent works in 3Dsegmentation resort to few-shot learning. Current 3D few-shot semanticsegmentation methods first pre-train the models on `seen' classes, and thenevaluate their generalization performance on `unseen' classes. However, theprior pre-training stage not only introduces excessive time overhead, but alsoincurs a significant domain gap on `unseen' classes. To tackle these issues, wepropose an efficient Training-free Few-shot 3D Segmentation netwrok, TFS3D, anda further training-based variant, TFS3D-T. Without any learnable parameters,TFS3D extracts dense representations by trigonometric positional encodings, andachieves comparable performance to previous training-based methods. Due to theelimination of pre-training, TFS3D can alleviate the domain gap issue and savea substantial amount of time. Building upon TFS3D, TFS3D-T only requires totrain a lightweight query-support transferring attention (QUEST), whichenhances the interaction between the few-shot query and support data.Experiments demonstrate TFS3D-T improves previous state-of-the-art methods by+6.93% and +17.96% mIoU respectively on S3DIS and ScanNet, while reducing thetraining time by -90%, indicating superior effectiveness and efficiency."
Transfer Learning for Microstructure Segmentation with CS-UNet: A Hybrid  Algorithm with Transformer and CNN Encoders,"['Khaled Alrfou', 'Tian Zhao', 'Amir Kordijazi']",http://arxiv.org/pdf/2308.13917v1.pdf,2023-08-26,"['cs.cv', 'cond-mat.mtrl-sci']","  Transfer learning improves the performance of deep learning models byinitializing them with parameters pre-trained on larger datasets. Intuitively,transfer learning is more effective when pre-training is on the in-domaindatasets. A recent study by NASA has demonstrated that the microstructuresegmentation with encoder-decoder algorithms benefits more from CNN encoderspre-trained on microscopy images than from those pre-trained on natural images.However, CNN models only capture the local spatial relations in images. Inrecent years, attention networks such as Transformers are increasingly used inimage analysis to capture the long-range relations between pixels. In thisstudy, we compare the segmentation performance of Transformer and CNN modelspre-trained on microscopy images with those pre-trained on natural images. Ourresult partially confirms the NASA study that the segmentation performance ofout-of-distribution images (taken under different imaging and sampleconditions) is significantly improved when pre-training on microscopy images.However, the performance gain for one-shot and few-shot learning is more modestwith Transformers. We also find that for image segmentation, the combination ofpre-trained Transformers and CNN encoders are consistently better thanpre-trained CNN encoders alone. Our dataset (of about 50,000 images) combinesthe public portion of the NASA dataset with additional images we collected.Even with much less training data, our pre-trained models have significantlybetter performance for image segmentation. This result suggests thatTransformers and CNN complement each other and when pre-trained on microscopyimages, they are more beneficial to the downstream tasks."
Fair Few-shot Learning with Auxiliary Sets,"['Song Wang', 'Jing Ma', 'Lu Cheng', 'Jundong Li']",http://arxiv.org/pdf/2308.14338v1.pdf,2023-08-28,"['cs.lg', 'cs.ai']","  Recently, there has been a growing interest in developing machine learning(ML) models that can promote fairness, i.e., eliminating biased predictionstowards certain populations (e.g., individuals from a specific demographicgroup). Most existing works learn such models based on well-designed fairnessconstraints in optimization. Nevertheless, in many practical ML tasks, onlyvery few labeled data samples can be collected, which can lead to inferiorfairness performance. This is because existing fairness constraints aredesigned to restrict the prediction disparity among different sensitive groups,but with few samples, it becomes difficult to accurately measure the disparity,thus rendering ineffective fairness optimization. In this paper, we define thefairness-aware learning task with limited training samples as the \emph{fairfew-shot learning} problem. To deal with this problem, we devise a novelframework that accumulates fairness-aware knowledge across differentmeta-training tasks and then generalizes the learned knowledge to meta-testtasks. To compensate for insufficient training samples, we propose an essentialstrategy to select and leverage an auxiliary set for each meta-test task. Theseauxiliary sets contain several labeled training samples that can enhance themodel performance regarding fairness in meta-test tasks, thereby allowing forthe transfer of learned useful fairness-oriented knowledge to meta-test tasks.Furthermore, we conduct extensive experiments on three real-world datasets tovalidate the superiority of our framework against the state-of-the-artbaselines."
"LongBench: A Bilingual, Multitask Benchmark for Long Context  Understanding","['Yushi Bai', 'Xin Lv', 'Jiajie Zhang', 'Hongchang Lyu', 'Jiankai Tang', 'Zhidian Huang', 'Zhengxiao Du', 'Xiao Liu', 'Aohan Zeng', 'Lei Hou', 'Yuxiao Dong', 'Jie Tang', 'Juanzi Li']",http://arxiv.org/pdf/2308.14508v1.pdf,2023-08-28,['cs.cl'],"  Although large language models (LLMs) demonstrate impressive performance formany language tasks, most of them can only handle texts a few thousand tokenslong, limiting their applications on longer sequence inputs, such as books,reports, and codebases. Recent works have proposed methods to improve LLMs'long context capabilities by extending context windows and more sophisticatedmemory mechanisms. However, comprehensive benchmarks tailored for evaluatinglong context understanding are lacking. In this paper, we introduce LongBench,the first bilingual, multi-task benchmark for long context understanding,enabling a more rigorous evaluation of long context understanding. LongBenchcomprises 21 datasets across 6 task categories in both English and Chinese,with an average length of 6,711 words (English) and 13,386 characters(Chinese). These tasks cover key long-text application areas includingsingle-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,and code completion. All datasets in LongBench are standardized into a unifiedformat, allowing for effortless automatic evaluation of LLMs. Uponcomprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercialmodel (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but stillstruggles on longer contexts. (2) Scaled position embedding and fine-tuning onlonger sequences lead to substantial improvement on long context understanding.(3) Context compression technique such as retrieval brings improvement formodel with weak ability on long contexts, but the performance still lags behindmodels that have strong long context understanding capability. The code anddatasets are available at https://github.com/THUDM/LongBench."
When hard negative sampling meets supervised contrastive learning,"['Zijun Long', 'George Killick', 'Richard McCreadie', 'Gerardo Aragon Camarasa', 'Zaiqiao Meng']",http://arxiv.org/pdf/2308.14893v1.pdf,2023-08-28,"['cs.cv', 'cs.ai', 'cs.lg']","  State-of-the-art image models predominantly follow a two-stage strategy:pre-training on large datasets and fine-tuning with cross-entropy loss. Manystudies have shown that using cross-entropy can result in sub-optimalgeneralisation and stability. While the supervised contrastive loss addressessome limitations of cross-entropy loss by focusing on intra-class similaritiesand inter-class differences, it neglects the importance of hard negativemining. We propose that models will benefit from performance improvement byweighting negative samples based on their dissimilarity to positivecounterparts. In this paper, we introduce a new supervised contrastive learningobjective, SCHaNe, which incorporates hard negative sampling during thefine-tuning phase. Without requiring specialized architectures, additionaldata, or extra computational resources, experimental results indicate thatSCHaNe outperforms the strong baseline BEiT-3 in Top-1 accuracy across variousbenchmarks, with significant gains of up to $3.32\%$ in few-shot learningsettings and $3.41\%$ in full dataset fine-tuning. Importantly, our proposedobjective sets a new state-of-the-art for base models on ImageNet-1k, achievingan 86.14\% accuracy. Furthermore, we demonstrate that the proposed objectiveyields better embeddings and explains the improved effectiveness observed inour experiments."
TransPrompt v2: A Transferable Prompting Framework for Cross-task Text  Classification,"['Jianing Wang', 'Chengyu Wang', 'Cen Chen', 'Ming Gao', 'Jun Huang', 'Aoying Zhou']",http://arxiv.org/pdf/2308.15010v1.pdf,2023-08-29,['cs.cl'],"  Text classification is one of the most imperative tasks in natural languageprocessing (NLP). Recent advances with pre-trained language models (PLMs) haveshown remarkable success on this task. However, the satisfying results obtainedby PLMs heavily depend on the large amounts of task-specific labeled data,which may not be feasible in many application scenarios due to data access andprivacy constraints. The recently-proposed prompt-based fine-tuning paradigmimproves the performance of PLMs for few-shot text classification withtask-specific templates. Yet, it is unclear how the prompting knowledge can betransferred across tasks, for the purpose of mutual reinforcement. We proposeTransPrompt v2, a novel transferable prompting framework for few-shot learningacross similar or distant text classification tasks. For learning acrosssimilar tasks, we employ a multi-task meta-knowledge acquisition (MMA)procedure to train a meta-learner that captures the cross-task transferableknowledge. For learning across distant tasks, we further inject the task typedescriptions into the prompt, and capture the intra-type and inter-type promptembeddings among multiple distant tasks. Additionally, two de-biasingtechniques are further designed to make the trained meta-learner moretask-agnostic and unbiased towards any tasks. After that, the meta-learner canbe adapted to each specific task with better parameters initialization.Extensive experiments show that TransPrompt v2 outperforms single-task andcross-task strong baselines over multiple NLP tasks and datasets. We furthershow that the meta-learner can effectively improve the performance of PLMs onpreviously unseen tasks. In addition, TransPrompt v2 also outperforms strongfine-tuning baselines when learning with full training sets."
AskIt: Unified Programming Interface for Programming with Large Language  Models,"['Katsumi Okuda', 'Saman Amarasinghe']",http://arxiv.org/pdf/2308.15645v1.pdf,2023-08-29,"['cs.pl', 'cs.ai', 'cs.se']","  In the evolving landscape of software development, Large Language Models(LLMs) exhibit a unique phenomenon known as emergent abilities, demonstratingadeptness across numerous tasks, from text summarization to code generation.While these abilities open up novel avenues in software design and crafting,their incorporation presents substantial challenges. Developers grapple withdecisions surrounding the direct embedding of LLMs within applications versusemploying them for code generation. Moreover, effective prompt design becomes acritical concern, given the necessity of data extraction from natural languageoutputs. To address these intricacies, this paper introduces AskIt, adomain-specific language (DSL) specifically designed for LLMs. AskIt simplifiesLLM integration, offering type-guided output control, template-based functiondefinitions, and a unified interface that diminishes the distinction betweenLLM-based code generation and application integration. Furthermore, throughProgramming by Example (PBE), AskIt harnesses the power of few-shot learning atthe programming language level. Our evaluations underscore AskIt's potency.Across 50 tasks, AskIt generated concise prompts for the given tasks, achievinga 16.14% reduction in prompt length relative to benchmarks. Additionally, byenabling the transition from direct LLM application usage to functiongeneration, AskIt achieved significant speedups, as observed in our GSM8Kbenchmark experiments. Through these advancements, AskIt streamlines theintegration of LLMs in software development, offering a more efficient,versatile approach for leveraging emergent abilities. The implementations ofAskIt in TypeScript and Python are available athttps://github.com/katsumiok/ts-askit and https://github.com/katsumiok/pyaskit,respectively."
Self-Sampling Meta SAM: Enhancing Few-shot Medical Image Segmentation  with Meta-Learning,"['Yiming Zhang', 'Tianang Leng', 'Kun Han', 'Xiaohui Xie']",http://arxiv.org/pdf/2308.16466v3.pdf,2023-08-31,['cs.cv'],"  While the Segment Anything Model (SAM) excels in semantic segmentation forgeneral-purpose images, its performance significantly deteriorates when appliedto medical images, primarily attributable to insufficient representation ofmedical images in its training dataset. Nonetheless, gathering comprehensivedatasets and training models that are universally applicable is particularlychallenging due to the long-tail problem common in medical images. To addressthis gap, here we present a Self-Sampling Meta SAM (SSM-SAM) framework forfew-shot medical image segmentation. Our innovation lies in the design of threekey modules: 1) An online fast gradient descent optimizer, further optimized bya meta-learner, which ensures swift and robust adaptation to new tasks. 2) ASelf-Sampling module designed to provide well-aligned visual prompts forimproved attention allocation; and 3) A robust attention-based decoderspecifically designed for medical few-shot learning to capture relationshipbetween different slices. Extensive experiments on a popular abdominal CTdataset and an MRI dataset demonstrate that the proposed method achievessignificant improvements over state-of-the-art methods in few-shotsegmentation, with an average improvements of 10.21% and 1.80% in terms of DSC,respectively. In conclusion, we present a novel approach for rapid onlineadaptation in interactive image segmentation, adapting to a new organ in just0.83 minutes. Code is publicly available on GitHub upon acceptance."
Pretraining Representations for Bioacoustic Few-shot Detection using  Supervised Contrastive Learning,"['Ilyass Moummad', 'Romain Serizel', 'Nicolas Farrugia']",http://arxiv.org/pdf/2309.00878v1.pdf,2023-09-02,"['cs.sd', 'cs.lg', 'eess.as']","  Deep learning has been widely used recently for sound event detection andclassification. Its success is linked to the availability of sufficiently largedatasets, possibly with corresponding annotations when supervised learning isconsidered. In bioacoustic applications, most tasks come with few labelledtraining data, because annotating long recordings is time consuming and costly.Therefore supervised learning is not the best suited approach to solvebioacoustic tasks. The bioacoustic community recasted the problem of soundevent detection within the framework of few-shot learning, i.e. training asystem with only few labeled examples. The few-shot bioacoustic sound eventdetection task in the DCASE challenge focuses on detecting events in long audiorecordings given only five annotated examples for each class of interest. Inthis paper, we show that learning a rich feature extractor from scratch can beachieved by leveraging data augmentation using a supervised contrastivelearning framework. We highlight the ability of this framework to transfer wellfor five-shot event detection on previously unseen classes in the trainingdata. We obtain an F-score of 63.46\% on the validation set and 42.7\% on thetest set, ranking second in the DCASE challenge. We provide an ablation studyfor the critical choices of data augmentation techniques as well as for thelearning strategy applied on the training set."
Prompt-based Node Feature Extractor for Few-shot Learning on  Text-Attributed Graphs,"['Xuanwen Huang', 'Kaiqiao Han', 'Dezheng Bao', 'Quanjin Tao', 'Zhisheng Zhang', 'Yang Yang', 'Qi Zhu']",http://arxiv.org/pdf/2309.02848v1.pdf,2023-09-06,['cs.si'],"  Text-attributed Graphs (TAGs) are commonly found in the real world, such associal networks and citation networks, and consist of nodes represented bytextual descriptions. Currently, mainstream machine learning methods on TAGsinvolve a two-stage modeling approach: (1) unsupervised node feature extractionwith pre-trained language models (PLMs); and (2) supervised learning usingGraph Neural Networks (GNNs). However, we observe that these representations,which have undergone large-scale pre-training, do not significantly improveperformance with a limited amount of training samples. The main issue is thatexisting methods have not effectively integrated information from the graph anddownstream tasks simultaneously. In this paper, we propose a novel frameworkcalled G-Prompt, which combines a graph adapter and task-specific prompts toextract node features. First, G-Prompt introduces a learnable GNN layer(\emph{i.e.,} adaptor) at the end of PLMs, which is fine-tuned to bettercapture the masked tokens considering graph neighborhood information. After theadapter is trained, G-Prompt incorporates task-specific prompts to obtain\emph{interpretable} node representations for the downstream task. Ourexperiment results demonstrate that our proposed method outperforms currentstate-of-the-art (SOTA) methods on few-shot node classification. Moreimportantly, in zero-shot settings, the G-Prompt embeddings can not onlyprovide better task interpretability than vanilla PLMs but also achievecomparable performance with fully-supervised baselines."
Cross-Image Context Matters for Bongard Problems,"['Nikhil Raghuraman', 'Adam W. Harley', 'Leonidas Guibas']",http://arxiv.org/pdf/2309.03468v1.pdf,2023-09-07,"['cs.cv', 'cs.ai', 'cs.lg']","  Current machine learning methods struggle to solve Bongard problems, whichare a type of IQ test that requires deriving an abstract ""concept"" from a setof positive and negative ""support"" images, and then classifying whether or nota new query image depicts the key concept. On Bongard-HOI, a benchmark fornatural-image Bongard problems, existing methods have only reached 66% accuracy(where chance is 50%). Low accuracy is often attributed to neural nets' lack ofability to find human-like symbolic rules. In this work, we point out that manyexisting methods are forfeiting accuracy due to a much simpler problem: they donot incorporate information contained in the support set as a whole, and relyinstead on information extracted from individual supports. This is a criticalissue, because unlike in few-shot learning tasks concerning objectclassification, the ""key concept"" in a typical Bongard problem can only bedistinguished using multiple positives and multiple negatives. We explore avariety of simple methods to take this cross-image context into account, anddemonstrate substantial gains over prior methods, leading to newstate-of-the-art performance on Bongard-LOGO (75.3%) and Bongard-HOI (72.45%)and strong performance on the original Bongard problem set (60.84%)."
Few-Shot Learning of Force-Based Motions From Demonstration Through  Pre-training of Haptic Representation,"['Marina Y. Aoyama', 'João Moura', 'Namiko Saito', 'Sethu Vijayakumar']",http://arxiv.org/pdf/2309.04640v1.pdf,2023-09-08,"['cs.ro', 'cs.ai', 'cs.lg']","  In many contact-rich tasks, force sensing plays an essential role in adaptingthe motion to the physical properties of the manipulated object. To enablerobots to capture the underlying distribution of object properties necessaryfor generalising learnt manipulation tasks to unseen objects, existing Learningfrom Demonstration (LfD) approaches require a large number of costly humandemonstrations. Our proposed semi-supervised LfD approach decouples the learntmodel into an haptic representation encoder and a motion generation decoder.This enables us to pre-train the first using large amount of unsupervised data,easily accessible, while using few-shot LfD to train the second, leveraging thebenefits of learning skills from humans. We validate the approach on the wipingtask using sponges with different stiffness and surface friction. Our resultsdemonstrate that pre-training significantly improves the ability of the LfDmodel to recognise physical properties and generate desired wiping motions forunseen sponges, outperforming the LfD method without pre-training. We validatethe motion generated by our semi-supervised LfD model on the physical robothardware using the KUKA iiwa robot arm. We also validate that the hapticrepresentation encoder, pre-trained in simulation, captures the properties ofreal objects, explaining its contribution to improving the generalisation ofthe downstream task."
DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning,"['Zhengxiang Shi', 'Aldo Lipani']",http://arxiv.org/pdf/2309.05173v2.pdf,2023-09-11,"['cs.cl', 'cs.ai', 'cs.cv', 'cs.lg']","  Prompt tuning (PT), where a small amount of trainable soft (continuous)prompt vectors is affixed to the input of language models (LM), has shownpromising results across various tasks and models for parameter-efficientfine-tuning (PEFT). PT stands out from other PEFT approaches because itmaintains competitive performance with fewer trainable parameters and does notdrastically scale up its parameters as the model size expands. However, PTintroduces additional soft prompt tokens, leading to longer input sequences,which significantly impacts training and inference time and memory usage due tothe Transformer's quadratic complexity. Particularly concerning for LargeLanguage Models (LLMs) that face heavy daily querying. To address this issue,we propose Decomposed Prompt Tuning (DePT), which decomposes the soft promptinto a shorter soft prompt and a pair of low-rank matrices that are thenoptimised with two different learning rates. This allows DePT to achieve betterperformance while saving over 20% memory and time costs compared to vanilla PTand its variants, without changing trainable parameter sizes. Through extensiveexperiments on 23 natural language processing (NLP) and vision-language (VL)tasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches,including the full fine-tuning baseline in some scenarios. Additionally, weempirically show that DEPT grows more efficient as the model size increases.Our further study reveals that DePT integrates seamlessly withparameter-efficient transfer learning in the few-shot learning setting andhighlights its adaptability to various model architectures and sizes."
Zero-shot Learning with Minimum Instruction to Extract Social  Determinants and Family History from Clinical Notes using GPT Model,"['Neel Bhate', 'Ansh Mittal', 'Zhe He', 'Xiao Luo']",http://arxiv.org/pdf/2309.05475v2.pdf,2023-09-11,['cs.cl'],"  Demographics, Social determinants of health, and family history documented inthe unstructured text within the electronic health records are increasinglybeing studied to understand how this information can be utilized with thestructured data to improve healthcare outcomes. After the GPT models werereleased, many studies have applied GPT models to extract this information fromthe narrative clinical notes. Different from the existing work, our researchfocuses on investigating the zero-shot learning on extracting this informationtogether by providing minimum information to the GPT model. We utilizede-identified real-world clinical notes annotated for demographics, varioussocial determinants, and family history information. Given that the GPT modelmight provide text different from the text in the original data, we explore twosets of evaluation metrics, including the traditional NER evaluation metricsand semantic similarity evaluation metrics, to completely understand theperformance. Our results show that the GPT-3.5 method achieved an average of0.975 F1 on demographics extraction, 0.615 F1 on social determinantsextraction, and 0.722 F1 on family history extraction. We believe these resultscan be further improved through model fine-tuning or few-shots learning.Through the case studies, we also identified the limitations of the GPT models,which need to be addressed in future research."
Self-Correlation and Cross-Correlation Learning for Few-Shot Remote  Sensing Image Semantic Segmentation,"['Linhan Wang', 'Shuo Lei', 'Jianfeng He', 'Shengkun Wang', 'Min Zhang', 'Chang-Tien Lu']",http://arxiv.org/pdf/2309.05840v2.pdf,2023-09-11,['cs.cv'],"  Remote sensing image semantic segmentation is an important problem for remotesensing image interpretation. Although remarkable progress has been achieved,existing deep neural network methods suffer from the reliance on massivetraining data. Few-shot remote sensing semantic segmentation aims at learningto segment target objects from a query image using only a few annotated supportimages of the target class. Most existing few-shot learning methods stemprimarily from their sole focus on extracting information from support images,thereby failing to effectively address the large variance in appearance andscales of geographic objects. To tackle these challenges, we propose aSelf-Correlation and Cross-Correlation Learning Network for the few-shot remotesensing image semantic segmentation. Our model enhances the generalization byconsidering both self-correlation and cross-correlation between support andquery images to make segmentation predictions. To further explore theself-correlation with the query image, we propose to adopt a classical spectralmethod to produce a class-agnostic segmentation mask based on the basic visualinformation of the image. Extensive experiments on two remote sensing imagedatasets demonstrate the effectiveness and superiority of our model in few-shotremote sensing image semantic segmentation. Code and models will be accessed athttps://github.com/linhanwang/SCCNet."
GLAD: Content-aware Dynamic Graphs For Log Anomaly Detection,"['Yufei Li', 'Yanchi Liu', 'Haoyu Wang', 'Zhengzhang Chen', 'Wei Cheng', 'Yuncong Chen', 'Wenchao Yu', 'Haifeng Chen', 'Cong Liu']",http://arxiv.org/pdf/2309.05953v1.pdf,2023-09-12,"['cs.lg', 'cs.ir']","  Logs play a crucial role in system monitoring and debugging by recordingvaluable system information, including events and states. Although variousmethods have been proposed to detect anomalies in log sequences, they oftenoverlook the significance of considering relations among system components,such as services and users, which can be identified from log contents.Understanding these relations is vital for detecting anomalies and theirunderlying causes. To address this issue, we introduce GLAD, a Graph-based LogAnomaly Detection framework designed to detect relational anomalies in systemlogs. GLAD incorporates log semantics, relational patterns, and sequentialpatterns into a unified framework for anomaly detection. Specifically, GLADfirst introduces a field extraction module that utilizes prompt-based few-shotlearning to identify essential fields from log contents. Then GLAD constructsdynamic log graphs for sliding windows by interconnecting extracted fields andlog events parsed from the log parser. These graphs represent events and fieldsas nodes and their relations as edges. Subsequently, GLAD utilizes atemporal-attentive graph edge anomaly detection model for identifying anomalousrelations in these dynamic log graphs. This model employs a Graph NeuralNetwork (GNN)-based encoder enhanced with transformers to capture content,structural and temporal features. We evaluate our proposed method on threedatasets, and the results demonstrate the effectiveness of GLAD in detectinganomalies indicated by varying relational patterns."
Using Large Language Model to Solve and Explain Physics Word Problems  Approaching Human Level,"['Jingzhe Ding', 'Yan Cen', 'Xinyuan Wei']",http://arxiv.org/pdf/2309.08182v2.pdf,2023-09-15,"['cs.cl', 'cs.ai', 'i.2.7']","  Our work demonstrates that large language model (LLM) pre-trained on textscan not only solve pure math word problems, but also physics word problems,whose solution requires calculation and inference based on prior physicalknowledge. We collect and annotate the first physics word problemdataset-PhysQA, which contains over 1000 junior high school physics wordproblems (covering Kinematics, Mass&Density, Mechanics, Heat, Electricity).Then we use OpenAI' s GPT3.5 to generate the answer of these problems and foundthat GPT3.5 could automatically solve 49.3% of the problems through zero-shotlearning and 73.2% through few-shot learning. This result demonstrates that byusing similar problems and their answers as prompt, LLM could solve elementaryphysics word problems approaching human level performance. In addition tosolving problems, GPT3.5 can also summarize the knowledge or topics covered bythe problems, provide relevant explanations, and generate new physics wordproblems based on the input. Our work is the first research to focus on theautomatic solving, explanation, and generation of physics word problems acrossvarious types and scenarios, and we achieve an acceptable and state-of-the-artaccuracy. This underscores the potential of LLMs for further applications insecondary education."
SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient  Channels,"['Henry Hengyuan Zhao', 'Pichao Wang', 'Yuyang Zhao', 'Hao Luo', 'Fan Wang', 'Mike Zheng Shou']",http://arxiv.org/pdf/2309.08513v2.pdf,2023-09-15,"['cs.cv', 'cs.ai']","  Pre-trained vision transformers have strong representation benefits tovarious downstream tasks. Recently, many parameter-efficient fine-tuning (PEFT)methods have been proposed, and their experiments demonstrate that tuning only1% of extra parameters could surpass full fine-tuning in low-data resourcescenarios. However, these methods overlook the task-specific information whenfine-tuning diverse downstream tasks. In this paper, we propose a simple yeteffective method called ""Salient Channel Tuning"" (SCT) to leverage thetask-specific information by forwarding the model with the task images toselect partial channels in a feature map that enables us to tune only 1/8channels leading to significantly lower parameter costs. Experiments outperformfull fine-tuning on 18 out of 19 tasks in the VTAB-1K benchmark by adding only0.11M parameters of the ViT-B, which is 780$\times$ fewer than its fullfine-tuning counterpart. Furthermore, experiments on domain generalization andfew-shot learning surpass other PEFT methods with lower parameter costs,demonstrating our proposed tuning technique's strong capability andeffectiveness in the low-data regime."
Analog Content-Addressable Memory from Complementary FeFETs,"['Xiwen Liu', 'Keshava Katti', 'Yunfei He', 'Paul Jacob', 'Claudia Richter', 'Uwe Schroeder', 'Santosh Kurinec', 'Pratik Chaudhari', 'Deep Jariwala']",http://arxiv.org/pdf/2309.09165v1.pdf,2023-09-17,"['cs.et', 'cs.ar', 'physics.app-ph']","  To address the increasing computational demands of artificial intelligence(AI) and big data, compute-in-memory (CIM) integrates memory and processingunits into the same physical location, reducing the time and energy overhead ofthe system. Despite advancements in non-volatile memory (NVM) for matrixmultiplication, other critical data-intensive operations, like parallel search,have been overlooked. Current parallel search architectures, namelycontent-addressable memory (CAM), often use binary, which restricts density andfunctionality. We present an analog CAM (ACAM) cell, built on two complementaryferroelectric field-effect transistors (FeFETs), that performs parallel searchin the analog domain with over 40 distinct match windows. We then deploy it tocalculate similarity between vectors, a building block in the following twomachine learning problems. ACAM outperforms ternary CAM (TCAM) when applied tosimilarity search for few-shot learning on the Omniglot dataset, yieldingprojected simulation results with improved inference accuracy by 5%, 3x densermemory architecture, and more than 100x faster speed compared to centralprocessing unit (CPU) and graphics processing unit (GPU) per similarity searchon scaled CMOS nodes. We also demonstrate 1-step inference on a kernelregression model by combining non-linear kernel computation and matrixmultiplication in ACAM, with simulation estimates indicating 1,000x fasterinference than CPU and GPU."
Replication: Contrastive Learning and Data Augmentation in Traffic  Classification Using a Flowpic Input Representation,"['Alessandro Finamore', 'Chao Wang', 'Jonatan Krolikowski', 'Jose M. Navarro', 'Fuxing Chen', 'Dario Rossi']",http://arxiv.org/pdf/2309.09733v2.pdf,2023-09-18,"['cs.lg', 'cs.ni']","  Over the last years we witnessed a renewed interest toward TrafficClassification (TC) captivated by the rise of Deep Learning (DL). Yet, the vastmajority of TC literature lacks code artifacts, performance assessments acrossdatasets and reference comparisons against Machine Learning (ML) methods. Amongthose works, a recent study from IMC22 [16] is worth of attention since itadopts recent DL methodologies (namely, few-shot learning, self-supervision viacontrastive learning and data augmentation) appealing for networking as theyenable to learn from a few samples and transfer across datasets. The mainresult of [16] on the UCDAVIS19, ISCX-VPN and ISCX-Tor datasets is that, withsuch DL methodologies, 100 input samples are enough to achieve very highaccuracy using an input representation called ""flowpic"" (i.e., a per-flow 2dhistograms of the packets size evolution over time). In this paper (i) wereproduce [16] on the same datasets and (ii) we replicate its most salientaspect (the importance of data augmentation) on three additional publicdatasets (MIRAGE19, MIRAGE22 and UTMOBILENET21). While we confirm most of theoriginal results, we also found a 20% accuracy drop on some of the investigatedscenarios due to a data shift in the original dataset that we uncovered.Additionally, our study validates that the data augmentation strategies studiedin [16] perform well on other datasets too. In the spirit of reproducibilityand replicability we make all artifacts (code and data) available to theresearch community at https://tcbenchstack.github.io/tcbench/"
MAD: Meta Adversarial Defense Benchmark,"['X. Peng', 'D. Zhou', 'G. Sun', 'J. Shi', 'L. Wu']",http://arxiv.org/pdf/2309.09776v1.pdf,2023-09-18,['eess.iv'],"  Adversarial training (AT) is a prominent technique employed by deep learningmodels to defend against adversarial attacks, and to some extent, enhance modelrobustness. However, there are three main drawbacks of the existing AT-baseddefense methods: expensive computational cost, low generalization ability, andthe dilemma between the original model and the defense model. To this end, wepropose a novel benchmark called meta adversarial defense (MAD). The MADbenchmark consists of two MAD datasets, along with a MAD evaluation protocol.The two large-scale MAD datasets were generated through experiments using 30kinds of attacks on MNIST and CIFAR-10 datasets. In addition, we introduce ameta-learning based adversarial training (Meta-AT) algorithm as the baseline,which features high robustness to unseen adversarial attacks through few-shotlearning. Experimental results demonstrate the effectiveness of our Meta-ATalgorithm compared to the state-of-the-art methods. Furthermore, the modelafter Meta-AT maintains a relatively high clean-samples classification accuracy(CCA). It is worth noting that Meta-AT addresses all three aforementionedlimitations, leading to substantial improvements. This benchmark ultimatelyachieved breakthroughs in investigating the transferability of adversarialdefense methods to new attacks and the ability to learn from a limited numberof adversarial examples. Our codes and attacked datasets address will beavailable at https://github.com/PXX1110/Meta_AT."
Domain Adaptive Few-Shot Open-Set Learning,"['Debabrata Pal', 'Deeptej More', 'Sai Bhargav', 'Dipesh Tamboli', 'Vaneet Aggarwal', 'Biplab Banerjee']",http://arxiv.org/pdf/2309.12814v1.pdf,2023-09-22,['cs.cv'],"  Few-shot learning has made impressive strides in addressing the crucialchallenges of recognizing unknown samples from novel classes in target querysets and managing visual shifts between domains. However, existing techniquesfall short when it comes to identifying target outliers under domain shifts bylearning to reject pseudo-outliers from the source domain, resulting in anincomplete solution to both problems. To address these challengescomprehensively, we propose a novel approach called Domain Adaptive Few-ShotOpen Set Recognition (DA-FSOS) and introduce a meta-learning-based architecturenamed DAFOSNET. During training, our model learns a shared and discriminativeembedding space while creating a pseudo open-space decision boundary, given afully-supervised source domain and a label-disjoint few-shot target domain. Toenhance data density, we use a pair of conditional adversarial networks withtunable noise variances to augment both domains closed and pseudo-open spaces.Furthermore, we propose a domain-specific batch-normalized class prototypesalignment strategy to align both domains globally while ensuringclass-discriminativeness through novel metric objectives. Our training approachensures that DAFOS-NET can generalize well to new scenarios in the targetdomain. We present three benchmarks for DA-FSOS based on the Office-Home,mini-ImageNet/CUB, and DomainNet datasets and demonstrate the efficacy ofDAFOS-NET through extensive experimentation"
HAVE-Net: Hallucinated Audio-Visual Embeddings for Few-Shot  Classification with Unimodal Cues,"['Ankit Jha', 'Debabrata Pal', 'Mainak Singha', 'Naman Agarwal', 'Biplab Banerjee']",http://arxiv.org/pdf/2309.13470v1.pdf,2023-09-23,['cs.cv'],"  Recognition of remote sensing (RS) or aerial images is currently of greatinterest, and advancements in deep learning algorithms added flavor to it inrecent years. Occlusion, intra-class variance, lighting, etc., might arisewhile training neural networks using unimodal RS visual input. Even thoughjoint training of audio-visual modalities improves classification performancein a low-data regime, it has yet to be thoroughly investigated in the RSdomain. Here, we aim to solve a novel problem where both the audio and visualmodalities are present during the meta-training of a few-shot learning (FSL)classifier; however, one of the modalities might be missing during themeta-testing stage. This problem formulation is pertinent in the RS domain,given the difficulties in data acquisition or sensor malfunctioning. Tomitigate, we propose a novel few-shot generative framework, HallucinatedAudio-Visual Embeddings-Network (HAVE-Net), to meta-train cross-modal featuresfrom limited unimodal data. Precisely, these hallucinated features aremeta-learned from base classes and used for few-shot classification on novelclasses during the inference phase. The experimental results on the benchmarkADVANCE and AudioSetZSL datasets show that our hallucinated modalityaugmentation strategy for few-shot classification outperforms the classifierperformance trained with the real multimodal information at least by 0.8-2%."
nnSAM: Plug-and-play Segment Anything Model Improves nnUNet Performance,"['Yunxiang Li', 'Bowen Jing', 'Zihan Li', 'Jing Wang', 'You Zhang']",http://arxiv.org/pdf/2309.16967v2.pdf,2023-09-29,"['cs.cv', 'eess.iv']","  The recent developments of foundation models in computer vision, especiallythe Segment Anything Model (SAM), allow scalable and domain-agnostic imagesegmentation to serve as a general-purpose segmentation tool. In parallel, thefield of medical image segmentation has benefited significantly fromspecialized neural networks like the nnUNet, which is trained ondomain-specific datasets and can automatically configure the network to tailorto specific segmentation challenges. To combine the advantages of foundationmodels and domain-specific models, we present nnSAM, which synergisticallyintegrates the SAM model with the nnUNet model to achieve more accurate androbust medical image segmentation. The nnSAM model leverages the powerful androbust feature extraction capabilities of SAM, while harnessing the automaticconfiguration capabilities of nnUNet to promote dataset-tailored learning. Ourcomprehensive evaluation of nnSAM model on different sizes of training samplesshows that it allows few-shot learning, which is highly relevant for medicalimage segmentation where high-quality, annotated data can be scarce and costlyto obtain. By melding the strengths of both its predecessors, nnSAM positionsitself as a potential new benchmark in medical image segmentation, offering atool that combines broad applicability with specialized efficiency. The code isavailable at https://github.com/Kent0n-Li/Medical-Image-Segmentation."
Mitigating the Effect of Incidental Correlations on Part-based Learning,"['Gaurav Bhatt', 'Deepayan Das', 'Leonid Sigal', 'Vineeth N Balasubramanian']",http://arxiv.org/pdf/2310.00377v1.pdf,2023-09-30,['cs.lg'],"  Intelligent systems possess a crucial characteristic of breaking complicatedproblems into smaller reusable components or parts and adjusting to new tasksusing these part representations. However, current part-learners encounterdifficulties in dealing with incidental correlations resulting from the limitedobservations of objects that may appear only in specific arrangements or withspecific backgrounds. These incidental correlations may have a detrimentalimpact on the generalization and interpretability of learned partrepresentations. This study asserts that part-based representations could bemore interpretable and generalize better with limited data, employing twoinnovative regularization methods. The first regularization separatesforeground and background information's generative process via a uniquemixture-of-parts formulation. Structural constraints are imposed on the partsusing a weakly-supervised loss, guaranteeing that the mixture-of-parts forforeground and background entails soft, object-agnostic masks. The secondregularization assumes the form of a distillation loss, ensuring the invarianceof the learned parts to the incidental background correlations. Furthermore, weincorporate sparse and orthogonal constraints to facilitate learninghigh-quality part representations. By reducing the impact of incidentalbackground correlations on the learned parts, we exhibit state-of-the-art(SoTA) performance on few-shot learning tasks on benchmark datasets, includingMiniImagenet, TieredImageNet, and FC100. We also demonstrate that thepart-based representations acquired through our approach generalize better thanexisting techniques, even under domain shifts of the background and common datacorruption on the ImageNet-9 dataset. The implementation is available onGitHub: https://github.com/GauravBh1010tt/DPViT.git"
RA-DIT: Retrieval-Augmented Dual Instruction Tuning,"['Xi Victoria Lin', 'Xilun Chen', 'Mingda Chen', 'Weijia Shi', 'Maria Lomeli', 'Rich James', 'Pedro Rodriguez', 'Jacob Kahn', 'Gergely Szilvasy', 'Mike Lewis', 'Luke Zettlemoyer', 'Scott Yih']",http://arxiv.org/pdf/2310.01352v3.pdf,2023-10-02,"['cs.cl', 'cs.ai']","  Retrieval-augmented language models (RALMs) improve performance by accessinglong-tail and up-to-date knowledge from external data stores, but arechallenging to build. Existing approaches require either expensiveretrieval-specific modifications to LM pre-training or use post-hoc integrationof the data store that leads to suboptimal performance. We introduceRetrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuningmethodology that provides a third option by retrofitting any LLM with retrievalcapabilities. Our approach operates in two distinct fine-tuning steps: (1) oneupdates a pre-trained LM to better use retrieved information, while (2) theother updates the retriever to return more relevant results, as preferred bythe LM. By fine-tuning over tasks that require both knowledge utilization andcontextual awareness, we demonstrate that each stage yields significantperformance improvements, and using both leads to additional gains. Our bestmodel, RA-DIT 65B, achieves state-of-the-art performance across a range ofknowledge-intensive zero- and few-shot learning benchmarks, significantlyoutperforming existing in-context RALM approaches by up to +8.9% in 0-shotsetting and +1.4% in 5-shot setting on average."
SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified  Pre-training,"['Kazem Meidani', 'Parshin Shojaee', 'Chandan K. Reddy', 'Amir Barati Farimani']",http://arxiv.org/pdf/2310.02227v2.pdf,2023-10-03,"['cs.lg', 'cs.ai']","  In an era where symbolic mathematical equations are indispensable formodeling complex natural phenomena, scientific inquiry often involvescollecting observations and translating them into mathematical expressions.Recently, deep learning has emerged as a powerful tool for extracting insightsfrom data. However, existing models typically specialize in either numeric orsymbolic domains, and are usually trained in a supervised manner tailored tospecific tasks. This approach neglects the substantial benefits that couldarise from a task-agnostic unified understanding between symbolic equations andtheir numeric counterparts. To bridge the gap, we introduce SNIP, aSymbolic-Numeric Integrated Pre-training, which employs joint contrastivelearning between symbolic and numeric domains, enhancing their mutualsimilarities in the pre-trained embeddings. By performing latent spaceanalysis, we observe that SNIP provides cross-domain insights into therepresentations, revealing that symbolic supervision enhances the embeddings ofnumeric data and vice versa. We evaluate SNIP across diverse tasks, includingsymbolic-to-numeric mathematical property prediction and numeric-to-symbolicequation discovery, commonly known as symbolic regression. Results show thatSNIP effectively transfers to various tasks, consistently outperforming fullysupervised baselines and competing strongly with established task-specificmethods, especially in few-shot learning scenarios where available data islimited."
UniPredict: Large Language Models are Universal Tabular Predictors,"['Ruiyu Wang', 'Zifeng Wang', 'Jimeng Sun']",http://arxiv.org/pdf/2310.03266v1.pdf,2023-10-05,['cs.lg'],"  Tabular data prediction is a fundamental machine learning task for manyapplications. Existing methods predominantly employ discriminative modeling andoperate under the assumption of a fixed target column, necessitatingre-training for every new predictive task. Inspired by the generative power oflarge language models (LLMs), this paper exploits the idea of buildinguniversal tabular data predictors based on generative modeling, namelyUniPredict. Here, we show that scaling up an LLM to extensive tabular datasetswith the capability of comprehending diverse tabular inputs and predicting fortarget variables following the input instructions. Specifically, we train asingle LLM on an aggregation of 169 tabular datasets with diverse targets andcompare its performance against baselines that are trained on each datasetseparately. We observe this versatile UniPredict model demonstrates anadvantage over other models, ranging from 5.4% to 13.4%, when compared with thebest tree-boosting baseline and the best neural network baseline, respectively.We further test UniPredict in few-shot learning settings on another 62 tabulardatasets. Our method achieves strong performance in quickly adapting to newtasks, where our method outperforms XGBoost over 100% on the low-resource setupand shows a significant margin over all baselines. We envision that UniPredictsheds light on developing a universal tabular data prediction system thatlearns from data at scale and serves a wide range of prediction tasks."
LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios  via Prompt Compression,"['Huiqiang Jiang', 'Qianhui Wu', 'Xufang Luo', 'Dongsheng Li', 'Chin-Yew Lin', 'Yuqing Yang', 'Lili Qiu']",http://arxiv.org/pdf/2310.06839v1.pdf,2023-10-10,"['cs.cl', 'cs.lg']","  In long context scenarios, large language models (LLMs) face three mainchallenges: higher computational/financial cost, longer latency, and inferiorperformance. Some studies reveal that the performance of LLMs depends on boththe density and the position of the key information (question relevant) in theinput prompt. Inspired by these findings, we propose LongLLMLingua for promptcompression towards improving LLMs' perception of the key information tosimultaneously address the three challenges. We conduct evaluation on a widerange of long context scenarios including single-/multi-document QA, few-shotlearning, summarization, synthetic tasks, and code completion. The experimentalresults show that LongLLMLingua compressed prompt can derive higher performancewith much less cost. The latency of the end-to-end system is also reduced. Forexample, on NaturalQuestions benchmark, LongLLMLingua gains a performance boostof up to 17.1% over the original prompt with ~4x fewer tokens as input toGPT-3.5-Turbo. It can derive cost savings of \$28.5 and \$27.4 per 1,000samples from the LongBench and ZeroScrolls benchmark, respectively.Additionally, when compressing prompts of ~10k tokens at a compression rate of2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x. Ourcode is available at https://aka.ms/LLMLingua."
Leveraging Twitter Data for Sentiment Analysis of Transit User Feedback:  An NLP Framework,"['Adway Das', 'Abhishek Kumar Prajapati', 'Pengxiang Zhang', 'Mukund Srinath', 'Andisheh Ranjbari']",http://arxiv.org/pdf/2310.07086v1.pdf,2023-10-11,"['cs.ai', 'cs.si']","  Traditional methods of collecting user feedback through transit surveys areoften time-consuming, resource intensive, and costly. In this paper, we proposea novel NLP-based framework that harnesses the vast, abundant, and inexpensivedata available on social media platforms like Twitter to understand users'perceptions of various service issues. Twitter, being a microblogging platform,hosts a wealth of real-time user-generated content that often includes valuablefeedback and opinions on various products, services, and experiences. Theproposed framework streamlines the process of gathering and analyzing userfeedback without the need for costly and time-consuming user feedback surveysusing two techniques. First, it utilizes few-shot learning for tweetclassification within predefined categories, allowing effective identificationof the issues described in tweets. It then employs a lexicon-based sentimentanalysis model to assess the intensity and polarity of the tweet sentiments,distinguishing between positive, negative, and neutral tweets. Theeffectiveness of the framework was validated on a subset of manually labeledTwitter data and was applied to the NYC subway system as a case study. Theframework accurately classifies tweets into predefined categories related tosafety, reliability, and maintenance of the subway system and effectivelymeasured sentiment intensities within each category. The general findings werecorroborated through a comparison with an agency-run customer survey conductedin the same year. The findings highlight the effectiveness of the proposedframework in gauging user feedback through inexpensive social media data tounderstand the pain points of the transit system and plan for targetedimprovements."
Empower Text-Attributed Graphs Learning with Large Language Models  (LLMs),"['Jianxiang Yu', 'Yuxiang Ren', 'Chenghua Gong', 'Jiaqi Tan', 'Xiang Li', 'Xuecang Zhang']",http://arxiv.org/pdf/2310.09872v1.pdf,2023-10-15,['cs.lg'],"  Text-attributed graphs have recently garnered significant attention due totheir wide range of applications in web domains. Existing methodologies employword embedding models for acquiring text representations as node features,which are subsequently fed into Graph Neural Networks (GNNs) for training.Recently, the advent of Large Language Models (LLMs) has introduced theirpowerful capabilities in information retrieval and text generation, which cangreatly enhance the text attributes of graph data. Furthermore, the acquisitionand labeling of extensive datasets are both costly and time-consumingendeavors. Consequently, few-shot learning has emerged as a crucial problem inthe context of graph learning tasks. In order to tackle this challenge, wepropose a lightweight paradigm called ENG, which adopts a plug-and-playapproach to empower text-attributed graphs through node generation using LLMs.Specifically, we utilize LLMs to extract semantic information from the labelsand generate samples that belong to these categories as exemplars.Subsequently, we employ an edge predictor to capture the structural informationinherent in the raw dataset and integrate the newly generated samples into theoriginal graph. This approach harnesses LLMs for enhancing class-levelinformation and seamlessly introduces labeled nodes and edges without modifyingthe raw dataset, thereby facilitating the node classification task in few-shotscenarios. Extensive experiments demonstrate the outstanding performance of ourproposed paradigm, particularly in low-shot scenarios. For instance, in the1-shot setting of the ogbn-arxiv dataset, ENG achieves a 76% improvement overthe baseline model."
In-Context Learning with Iterative Demonstration Selection,"['Chengwei Qin', 'Aston Zhang', 'Anirudh Dagar', 'Wenming Ye']",http://arxiv.org/pdf/2310.09881v2.pdf,2023-10-15,"['cs.cl', 'cs.ai']","  Spurred by advancements in scale, large language models (LLMs) havedemonstrated strong few-shot learning ability via in-context learning (ICL).However, the performance of ICL has been shown to be highly sensitive to theselection of few-shot demonstrations. Selecting the most suitable examples ascontext remains an ongoing challenge and an open problem. Existing literaturehas highlighted the importance of selecting examples that are diverse orsemantically similar to the test sample while ignoring the fact that theoptimal selection dimension, i.e., diversity or similarity, is task-specific.Leveraging the merits of both dimensions, we propose Iterative DemonstrationSelection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT),IDS iteratively selects examples that are diverse but still strongly correlatedwith the test sample as ICL demonstrations. Specifically, IDS appliesZero-shot-CoT to the test sample before demonstration selection. The outputreasoning path is then used to choose demonstrations that are prepended to thetest sample for inference. The generated answer is accompanied by itscorresponding reasoning path for extracting a new set of demonstrations in thenext iteration. After several iterations, IDS adopts majority voting to obtainthe final result. Through extensive experiments on tasks including commonsensereasoning, question answering, topic classification, and sentiment analysis, wedemonstrate that IDS can consistently outperform existing ICL demonstrationselection methods."
AutoMix: Automatically Mixing Language Models,"['Aman Madaan', 'Pranjal Aggarwal', 'Ankit Anand', 'Srividya Pranavi Potharaju', 'Swaroop Mishra', 'Pei Zhou', 'Aditya Gupta', 'Dheeraj Rajagopal', 'Karthik Kappaganthu', 'Yiming Yang', 'Shyam Upadhyay', ' Mausam', 'Manaal Faruqui']",http://arxiv.org/pdf/2310.12963v2.pdf,2023-10-19,"['cs.cl', 'cs.ai']","  Large language models (LLMs) are now available in various sizes andconfigurations from cloud API providers. While this diversity offers a broadspectrum of choices, effectively leveraging the options to optimizecomputational cost and performance remains challenging. In this work, wepresent AutoMix, an approach that strategically routes queries to larger LMs,based on the approximate correctness of outputs from a smaller LM. Central toAutoMix is a few-shot self-verification mechanism, which estimates thereliability of its own outputs without requiring training. Given thatverifications can be noisy, we employ a meta verifier in AutoMix to refine theaccuracy of these assessments. Our experiments using LLAMA2-13/70B, on fivecontext-grounded reasoning datasets demonstrate that AutoMix surpassesestablished baselines, improving the incremental benefit per cost by up to 89%.Our code and data are available at https://github.com/automix-llm/automix."
The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64  Languages,"['Chiyu Zhang', 'Khai Duy Doan', 'Qisheng Liao', 'Muhammad Abdul-Mageed']",http://arxiv.org/pdf/2310.14557v1.pdf,2023-10-23,['cs.cl'],"  Instruction tuned large language models (LLMs), such as ChatGPT, demonstrateremarkable performance in a wide range of tasks. Despite numerous recentstudies that examine the performance of instruction-tuned LLMs on various NLPbenchmarks, there remains a lack of comprehensive investigation into theirability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaningembedded within social and interactive contexts. This deficiency arises partlyfrom SM not being adequately represented in any of the existing benchmarks. Toaddress this gap, we present SPARROW, an extensive multilingual benchmarkspecifically designed for SM understanding. SPARROW comprises 169 datasetscovering 13 task types across six primary categories (e.g., anti-sociallanguage detection, emotion recognition). SPARROW datasets encompass 64different languages originating from 12 language families representing 16writing scripts. We evaluate the performance of various multilingual pretrainedlanguage models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT)on SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Ourcomprehensive analysis reveals that existing open-source instruction tuned LLMsstill struggle to understand SM across various languages, performing close to arandom baseline in some cases. We also find that although ChatGPT outperformsmany LLMs, it still falls behind task-specific finetuned models with a gap of12.19 SPARROW score. Our benchmark is available at:https://github.com/UBC-NLP/SPARROW"
PAC-tuning:Fine-tuning Pretrained Language Models with PAC-driven  Perturbed Gradient Descent,"['Guangliang Liu', 'Zhiyu Xue', 'Xitong Zhang', 'Kristen Marie Johnson', 'Rongrong Wang']",http://arxiv.org/pdf/2310.17588v1.pdf,2023-10-26,"['cs.lg', 'cs.cl']","  Fine-tuning pretrained language models (PLMs) for downstream tasks is alarge-scale optimization problem, in which the choice of the training algorithmcritically determines how well the trained model can generalize to unseen testdata, especially in the context of few-shot learning. To achieve goodgeneralization performance and avoid overfitting, techniques such as dataaugmentation and pruning are often applied. However, adding theseregularizations necessitates heavy tuning of the hyperparameters ofoptimization algorithms, such as the popular Adam optimizer. In this paper, wepropose a two-stage fine-tuning method, PAC-tuning, to address thisoptimization challenge. First, based on PAC-Bayes training, PAC-tuning directlyminimizes the PAC-Bayes generalization bound to learn proper parameterdistribution. Second, PAC-tuning modifies the gradient by injecting noise withthe variance learned in the first stage into the model parameters duringtraining, resulting in a variant of perturbed gradient descent (PGD). In thepast, the few-shot scenario posed difficulties for PAC-Bayes training becausethe PAC-Bayes bound, when applied to large models with limited training data,might not be stringent. Our experimental results across 5 GLUE benchmark tasksdemonstrate that PAC-tuning successfully handles the challenges of fine-tuningtasks and outperforms strong baseline methods by a visible margin, furtherconfirming the potential to apply PAC training for any other settings where theAdam optimizer is currently used for training."
STDA-Meta: A Meta-Learning Framework for Few-Shot Traffic Prediction,"['Maoxiang Sun', 'Weilong Ding', 'Tianpu Zhang', 'Zijian Liu', 'Mengda Xing']",http://arxiv.org/pdf/2310.20223v1.pdf,2023-10-31,['cs.lg'],"  As the development of cities, traffic congestion becomes an increasinglypressing issue, and traffic prediction is a classic method to relieve thatissue. Traffic prediction is one specific application of spatio-temporalprediction learning, like taxi scheduling, weather prediction, and shiptrajectory prediction. Against these problems, classical spatio-temporalprediction learning methods including deep learning, require large amounts oftraining data. In reality, some newly developed cities with insufficientsensors would not hold that assumption, and the data scarcity makes predictiveperformance worse. In such situation, the learning method on insufficient datais known as few-shot learning (FSL), and the FSL of traffic prediction remainschallenges. On the one hand, graph structures' irregularity and dynamic natureof graphs cannot hold the performance of spatio-temporal learning method. Onthe other hand, conventional domain adaptation methods cannot work well oninsufficient training data, when transferring knowledge from different domainsto the intended target domain.To address these challenges, we propose a novelspatio-temporal domain adaptation (STDA) method that learns transferablespatio-temporal meta-knowledge from data-sufficient cities in an adversarialmanner. This learned meta-knowledge can improve the prediction performance ofdata-scarce cities. Specifically, we train the STDA model using aModel-Agnostic Meta-Learning (MAML) based episode learning process, which is amodel-agnostic meta-learning framework that enables the model to solve newlearning tasks using only a small number of training samples. We conductnumerous experiments on four traffic prediction datasets, and our results showthat the prediction performance of our model has improved by 7\% compared tobaseline models on the two metrics of MAE and RMSE."
Unleashing the Power of Pre-trained Language Models for Offline  Reinforcement Learning,"['Ruizhe Shi', 'Yuyao Liu', 'Yanjie Ze', 'Simon S. Du', 'Huazhe Xu']",http://arxiv.org/pdf/2310.20587v4.pdf,2023-10-31,['cs.lg'],"  Offline reinforcement learning (RL) aims to find a near-optimal policy usingpre-collected datasets. In real-world scenarios, data collection could becostly and risky; therefore, offline RL becomes particularly challenging whenthe in-domain data is limited. Given recent advances in Large Language Models(LLMs) and their few-shot learning prowess, this paper introduces$\textbf{La}$nguage Models for $\textbf{Mo}$tion Control ($\textbf{LaMo}$), ageneral framework based on Decision Transformers to effectively use pre-trainedLanguage Models (LMs) for offline RL. Our framework highlights four crucialcomponents: (1) Initializing Decision Transformers with sequentiallypre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast tofull-weight fine-tuning, to combine the pre-trained knowledge from LMs andin-domain knowledge effectively, (3) using the non-linear MLP transformationinstead of linear projections, to generate embeddings, and (4) integrating anauxiliary language prediction loss during fine-tuning to stabilize the LMs andretain their original abilities on languages. Empirical results indicate$\textbf{LaMo}$ achieves state-of-the-art performance in sparse-reward tasksand closes the gap between value-based offline RL methods and decisiontransformers in dense-reward tasks. In particular, our method demonstratessuperior performance in scenarios with limited data samples."
On Task-personalized Multimodal Few-shot Learning for Visually-rich  Document Entity Retrieval,"['Jiayi Chen', 'Hanjun Dai', 'Bo Dai', 'Aidong Zhang', 'Wei Wei']",http://arxiv.org/pdf/2311.00693v2.pdf,2023-11-01,['cs.ai'],"  Visually-rich document entity retrieval (VDER), which extracts keyinformation (e.g. date, address) from document images like invoices andreceipts, has become an important topic in industrial NLP applications. Theemergence of new document types at a constant pace, each with its unique entitytypes, presents a unique challenge: many documents contain unseen entity typesthat occur only a couple of times. Addressing this challenge requires models tohave the ability of learning entities in a few-shot manner. However, priorworks for Few-shot VDER mainly address the problem at the document level with apredefined global entity space, which doesn't account for the entity-levelfew-shot scenario: target entity types are locally personalized by each taskand entity occurrences vary significantly among documents. To address thisunexplored scenario, this paper studies a novel entity-level few-shot VDERtask. The challenges lie in the uniqueness of the label space for each task andthe increased complexity of out-of-distribution (OOD) contents. To tackle thisnovel task, we present a task-aware meta-learning based framework, with acentral focus on achieving effective task personalization that distinguishesbetween in-task and out-of-task distribution. Specifically, we adopt ahierarchical decoder (HC) and employ contrastive learning (ContrastProtoNet) toachieve this goal. Furthermore, we introduce a new dataset, FewVEX, to boostfuture research in the field of entity-level few-shot VDER. Experimentalresults demonstrate our approaches significantly improve the robustness ofpopular meta-learning baselines."
LLM4Drive: A Survey of Large Language Models for Autonomous Driving,"['Zhenjie Yang', 'Xiaosong Jia', 'Hongyang Li', 'Junchi Yan']",http://arxiv.org/pdf/2311.01043v2.pdf,2023-11-02,['cs.ai'],"  Autonomous driving technology, a catalyst for revolutionizing transportationand urban mobility, has the tend to transition from rule-based systems todata-driven strategies. Traditional module-based systems are constrained bycumulative errors among cascaded modules and inflexible pre-set rules. Incontrast, end-to-end autonomous driving systems have the potential to avoiderror accumulation due to their fully data-driven training process, althoughthey often lack transparency due to their ""black box"" nature, complicating thevalidation and traceability of decisions. Recently, large language models(LLMs) have demonstrated abilities including understanding context, logicalreasoning, and generating answers. A natural thought is to utilize theseabilities to empower autonomous driving. By combining LLM with foundationvision models, it could open the door to open-world understanding, reasoning,and few-shot learning, which current autonomous driving systems are lacking. Inthis paper, we systematically review a research line about \textit{LargeLanguage Models for Autonomous Driving (LLM4AD)}. This study evaluates thecurrent state of technological advancements, distinctly outlining the principalchallenges and prospective directions for the field. For the convenience ofresearchers in academia and industry, we provide real-time updates on thelatest advances in the field as well as relevant open-source resources via thedesignated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD."
Robust Fine-Tuning of Vision-Language Models for Domain Generalization,"['Kevin Vogt-Lowell', 'Noah Lee', 'Theodoros Tsiligkaridis', 'Marc Vaillant']",http://arxiv.org/pdf/2311.02236v1.pdf,2023-11-03,"['cs.cv', 'cs.ai', 'cs.cl', 'cs.lg']","  Transfer learning enables the sharing of common knowledge among models for avariety of downstream tasks, but traditional methods suffer in limited trainingdata settings and produce narrow models incapable of effectively generalizingunder distribution shifts. Foundation models have recently demonstratedimpressive zero-shot inference capabilities and robustness under distributionshifts. However, zero-shot evaluation for these models has been predominantlyconfined to benchmarks with simple distribution shifts, limiting ourunderstanding of their effectiveness under the more realistic shifts found inpractice. Moreover, common fine-tuning methods for these models have yet to beevaluated against vision models in few-shot scenarios where training data islimited. To address these gaps, we present a new recipe for few-shotfine-tuning of the popular vision-language foundation model CLIP and evaluateits performance on challenging benchmark datasets with realistic distributionshifts from the WILDS collection. Our experimentation demonstrates that, whilezero-shot CLIP fails to match performance of trained vision models on morecomplex benchmarks, few-shot CLIP fine-tuning outperforms its vision-onlycounterparts in terms of in-distribution and out-of-distribution accuracy atall levels of training data availability. This provides a strong incentive foradoption of foundation models within few-shot learning applications operatingwith real-world data. Code is available athttps://github.com/mit-ll/robust-vision-language-finetuning"
Relation Extraction in underexplored biomedical domains: A  diversity-optimised sampling and synthetic data generation approach,"['Maxime Delmas', 'Magdalena Wysocka', 'André Freitas']",http://arxiv.org/pdf/2311.06364v1.pdf,2023-11-10,['cs.cl'],"  The sparsity of labelled data is an obstacle to the development of RelationExtraction models and the completion of databases in various biomedical areas.While being of high interest in drug-discovery, the natural-productsliterature, reporting the identification of potential bioactive compounds fromorganisms, is a concrete example of such an overlooked topic. To mark the startof this new task, we created the first curated evaluation dataset and extractedliterature items from the LOTUS database to build training sets. To this end,we developed a new sampler inspired by diversity metrics in ecology, namedGreedy Maximum Entropy sampler, or GME-sampler(https://github.com/idiap/gme-sampler). The strategic optimization of bothbalance and diversity of the selected items in the evaluation set is importantgiven the resource-intensive nature of manual curation. After quantifying thenoise in the training set, in the form of discrepancies between the inputabstracts text and the expected output labels, we explored different strategiesaccordingly. Framing the task as an end-to-end Relation Extraction, weevaluated the performance of standard fine-tuning as a generative task andfew-shot learning with open Large Language Models (LLaMA 7B-65B). In additionto their evaluation in few-shot settings, we explore the potential of openLarge Language Models (Vicuna-13B) as synthetic data generator and propose anew workflow for this purpose. All evaluated models exhibited substantialimprovements when fine-tuned on synthetic abstracts rather than the originalnoisy data. We provide our best performing (f1-score=59.0) BioGPT-Large modelfor end-to-end RE of natural-products relationships along with all thegenerated synthetic data and the evaluation dataset. See more details athttps://github.com/idiap/abroad-re."
Learning Knowledge-Enhanced Contextual Language Representations for  Domain Natural Language Understanding,"['Ruyao Xu', 'Taolin Zhang', 'Chengyu Wang', 'Zhongjie Duan', 'Cen Chen', 'Minghui Qiu', 'Dawei Cheng', 'Xiaofeng He', 'Weining Qian']",http://arxiv.org/pdf/2311.06761v1.pdf,2023-11-12,['cs.cl'],"  Knowledge-Enhanced Pre-trained Language Models (KEPLMs) improve theperformance of various downstream NLP tasks by injecting knowledge facts fromlarge-scale Knowledge Graphs (KGs). However, existing methods for pre-trainingKEPLMs with relational triples are difficult to be adapted to close domains dueto the lack of sufficient domain graph semantics. In this paper, we propose aKnowledge-enhanced lANGuAge Representation learning framework for variousclOsed dOmains (KANGAROO) via capturing the implicit graph structure among theentities. Specifically, since the entity coverage rates of closed-domain KGscan be relatively low and may exhibit the global sparsity phenomenon forknowledge injection, we consider not only the shallow relationalrepresentations of triples but also the hyperbolic embeddings of deephierarchical entity-class structures for effective knowledge fusion.Moreover,as two closed-domain entities under the same entity-class often have locallydense neighbor subgraphs counted by max point biconnected component, we furtherpropose a data augmentation strategy based on contrastive learning oversubgraphs to construct hard negative samples of higher quality. It makes theunderlying KELPMs better distinguish the semantics of these neighboringentities to further complement the global semantic sparsity. In theexperiments, we evaluate KANGAROO over various knowledge-aware and general NLPtasks in both full and few-shot learning settings, outperforming various KEPLMtraining paradigms performance in closed-domains significantly."
Dual-channel Prototype Network for few-shot Classification of  Pathological Images,"['Hao Quan', 'Xinjia Li', 'Dayu Hu', 'Tianhang Nan', 'Xiaoyu Cui']",http://arxiv.org/pdf/2311.07871v1.pdf,2023-11-14,['cs.cv'],"  In pathology, the rarity of certain diseases and the complexity in annotatingpathological images significantly hinder the creation of extensive,high-quality datasets. This limitation impedes the progress of deeplearning-assisted diagnostic systems in pathology. Consequently, it becomesimperative to devise a technology that can discern new disease categories froma minimal number of annotated examples. Such a technology would substantiallyadvance deep learning models for rare diseases. Addressing this need, weintroduce the Dual-channel Prototype Network (DCPN), rooted in the few-shotlearning paradigm, to tackle the challenge of classifying pathological imageswith limited samples. DCPN augments the Pyramid Vision Transformer (PVT)framework for few-shot classification via self-supervised learning andintegrates it with convolutional neural networks. This combination forms adual-channel architecture that extracts multi-scale, highly precisepathological features. The approach enhances the versatility of prototyperepresentations and elevates the efficacy of prototype networks in few-shotpathological image classification tasks. We evaluated DCPN using three publiclyavailable pathological datasets, configuring small-sample classification tasksthat mirror varying degrees of clinical scenario domain shifts. Ourexperimental findings robustly affirm DCPN's superiority in few-shotpathological image classification, particularly in tasks within the samedomain, where it achieves the benchmarks of supervised learning."
Cross-dataset domain adaptation for the classification COVID-19 using  chest computed tomography images,"['Ridha Ouni', 'Haikel Alhichri']",http://arxiv.org/pdf/2311.08524v1.pdf,2023-11-14,"['eess.iv', 'cs.cv', 'cs.lg']","  Detecting COVID-19 patients using Computed Tomography (CT) images of thelungs is an active area of research. Datasets of CT images from COVID-19patients are becoming available. Deep learning (DL) solutions and in particularConvolutional Neural Networks (CNN) have achieved impressive results for theclassification of COVID-19 CT images, but only when the training and testingtake place within the same dataset. Work on the cross-dataset problem is stilllimited and the achieved results are low. Our work tackles the cross-datasetproblem through a Domain Adaptation (DA) technique with deep learning. Ourproposed solution, COVID19-DANet, is based on pre-trained CNN backbone forfeature extraction. For this task, we select the pre-trained Efficientnet-B3CNN because it has achieved impressive classification accuracy in previouswork. The backbone CNN is followed by a prototypical layer which is a conceptborrowed from prototypical networks in few-shot learning (FSL). It computes acosine distance between given samples and the class prototypes and thenconverts them to class probabilities using the Softmax function. To train theCOVID19-DANet model, we propose a combined loss function that is composed ofthe standard cross-entropy loss for class discrimination and another entropyloss computed over the unlabelled target set only. This so-called unlabelledtarget entropy loss is minimized and maximized in an alternative fashion, toreach the two objectives of class discrimination and domain invariance.COVID19-DANet is tested under four cross-dataset scenarios using theSARS-CoV-2-CT and COVID19-CT datasets and has achieved encouraging resultscompared to recent work in the literature."
CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models,"['Wenhong Zhu', 'Hongkun Hao', 'Zhiwei He', 'Yunze Song', 'Yumeng Zhang', 'Hanxu Hu', 'Yiran Wei', 'Rui Wang', 'Hongyuan Lu']",http://arxiv.org/pdf/2311.09154v1.pdf,2023-11-15,['cs.cl'],"  We are currently in an era of fierce competition among various large languagemodels (LLMs) continuously pushing the boundaries of benchmark performance.However, genuinely assessing the capabilities of these LLMs has become achallenging and critical issue due to potential data contamination, and itwastes dozens of time and effort for researchers and engineers to download andtry those contaminated models. To save our precious time, we propose a noveland useful method, Clean-Eval, which mitigates the issue of data contaminationand evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM toparaphrase and back-translate the contaminated data into a candidate set,generating expressions with the same meaning but in different surface forms. Asemantic detector is then used to filter the generated low-quality samples tonarrow down this candidate set. The best candidate is finally selected fromthis set based on the BLEURT score. According to human assessment, this bestcandidate is semantically similar to the original contamination data butexpressed differently. All candidates can form a new benchmark to evaluate themodel. Our experiments illustrate that Clean-Eval substantially restores theactual evaluation results on contaminated LLMs under both few-shot learning andfine-tuning scenarios."
A Language Agent for Autonomous Driving,"['Jiageng Mao', 'Junjie Ye', 'Yuxi Qian', 'Marco Pavone', 'Yue Wang']",http://arxiv.org/pdf/2311.10813v3.pdf,2023-11-17,"['cs.cv', 'cs.ai', 'cs.cl', 'cs.ro']","  Human-level driving is an ultimate goal of autonomous driving. Conventionalapproaches formulate autonomous driving as a perception-prediction-planningframework, yet their systems do not capitalize on the inherent reasoningability and experiential knowledge of humans. In this paper, we propose afundamental paradigm shift from current pipelines, exploiting Large LanguageModels (LLMs) as a cognitive agent to integrate human-like intelligence intoautonomous driving systems. Our approach, termed Agent-Driver, transforms thetraditional autonomous driving pipeline by introducing a versatile tool libraryaccessible via function calls, a cognitive memory of common sense andexperiential knowledge for decision-making, and a reasoning engine capable ofchain-of-thought reasoning, task planning, motion planning, andself-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitivecommon sense and robust reasoning capabilities, thus enabling a more nuanced,human-like approach to autonomous driving. We evaluate our approach on thelarge-scale nuScenes benchmark, and extensive experiments substantiate that ourAgent-Driver significantly outperforms the state-of-the-art driving methods bya large margin. Our approach also demonstrates superior interpretability andfew-shot learning ability to these methods. Code will be released."
Point Cloud Self-supervised Learning via 3D to Multi-view Masked  Autoencoder,"['Zhimin Chen', 'Yingwei Li', 'Longlong Jing', 'Liang Yang', 'Bing Li']",http://arxiv.org/pdf/2311.10887v1.pdf,2023-11-17,"['cs.cv', 'cs.ai']","  In recent years, the field of 3D self-supervised learning has witnessedsignificant progress, resulting in the emergence of Multi-Modality MaskedAutoEncoders (MAE) methods that leverage both 2D images and 3D point clouds forpre-training. However, a notable limitation of these approaches is that they donot fully utilize the multi-view attributes inherent in 3D point clouds, whichis crucial for a deeper understanding of 3D structures. Building upon thisinsight, we introduce a novel approach employing a 3D to multi-view maskedautoencoder to fully harness the multi-modal attributes of 3D point clouds. Tobe specific, our method uses the encoded tokens from 3D masked point clouds togenerate original point clouds and multi-view depth images across variousposes. This approach not only enriches the model's comprehension of geometricstructures but also leverages the inherent multi-modal properties of pointclouds. Our experiments illustrate the effectiveness of the proposed method fordifferent tasks and under different settings. Remarkably, our methodoutperforms state-of-the-art counterparts by a large margin in a variety ofdownstream tasks, including 3D object classification, few-shot learning, partsegmentation, and 3D object detection. Code will be available at:https://github.com/Zhimin-C/Multiview-MAE"
High-performance cVEP-BCI under minimal calibration,"['Yining Miao', 'Nanlin Shi', 'Changxing Huang', 'Yonghao Song', 'Xiaogang Chen', 'Yijun Wang', 'Xiaorong Gao']",http://arxiv.org/pdf/2311.11596v1.pdf,2023-11-20,"['cs.hc', 'cs.it', 'eess.sp', 'math.it', 'q-bio.nc']","  The ultimate goal of brain-computer interfaces (BCIs) based on visualmodulation paradigms is to achieve high-speed performance without the burden ofextensive calibration. Code-modulated visual evoked potential-based BCIs(cVEP-BCIs) modulated by broadband white noise (WN) offer various advantages,including increased communication speed, expanded encoding target capabilities,and enhanced coding flexibility. However, the complexity of thespatial-temporal patterns under broadband stimuli necessitates extensivecalibration for effective target identification in cVEP-BCIs. Consequently, theinformation transfer rate (ITR) of cVEP-BCI under limited calibration usuallystays around 100 bits per minute (bpm), significantly lagging behindstate-of-the-art steady-state visual evoked potential-based BCIs (SSVEP-BCIs),which achieve rates above 200 bpm. To enhance the performance of cVEP-BCIs withminimal calibration, we devised an efficient calibration stage involving abrief single-target flickering, lasting less than a minute, to extractgeneralizable spatial-temporal patterns. Leveraging the calibration data, wedeveloped two complementary methods to construct cVEP temporal patterns: thelinear modeling method based on the stimulus sequence and the transfer learningtechniques using cross-subject data. As a result, we achieved the highest ITRof 250 bpm under a minute of calibration, which has been shown to be comparableto the state-of-the-art SSVEP paradigms. In summary, our work significantlyimproved the cVEP performance under few-shot learning, which is expected toexpand the practicality and usability of cVEP-BCIs."
Using Guided Transfer Learning to Predispose AI Agent to Learn  Efficiently from Small RNA-sequencing Datasets,"['Kevin Li', 'Danko Nikolić', 'Vjekoslav Nikolić', 'Davor Andrić', 'Lauren M. Sanders', 'Sylvain V. Costes']",http://arxiv.org/pdf/2311.12045v1.pdf,2023-11-17,['q-bio.gn'],"  Given the increasing availability of RNA-seq data and its complex andheterogeneous nature, there has been growing interest in applying AI/machinelearning methodologies to work with such data modalities. However, becauseomics data is characterized by high dimensionality and low sample size (HDLSS),current attempts at integrating AI in this domain require significant humanguidance and expertise to mitigate overfitting. In this work we look at howtransfer learning can be improved to learn from small RNA-seq sample sizeswithout significant human interference. The strategy is to gain general priorknowledge about a particular domain of data (e.g. RNA-seq data) by pre-trainingon a general task with a large aggregate of data, then fine-tuning to variousspecific, downstream target tasks in the same domain. Because previous attemptshave shown traditional transfer learning failing on HLDSS, we propose toimprove performance by using Guided Transfer Learning (GTL). Collaborating withRobots Go Mental, the AI we deploy here not only learns good initial parametersduring pre-training, but also learns inductive biases that affect how the AIlearns downstream tasks. In this approach, we first pre-trained on recount3data, a collection of over 400,000 mouse RNA-seq samples sourced from thousandsof individual studies. With such a large collection, patterns of expressionbetween the ~30,000 genes in mammalian systems were pre-determined. Suchpatterns were sufficient for the pre-trained AI agent to efficiently learn newdownstream tasks involving RNA-seq datasets with very low sample sizes andperformed notably better on few-shot learning tasks compared to the same modelwithout pre-training."
Modeling Political Orientation of Social Media Posts: An Extended  Analysis,"['Sadia Kamal', 'Brenner Little', 'Jade Gullic', 'Trevor Harms', 'Kristin Olofsson', 'Arunkumar Bagavathi']",http://arxiv.org/pdf/2311.12323v1.pdf,2023-11-21,"['cs.si', 'cs.cl', 'cs.lg']","  Developing machine learning models to characterize political polarization ononline social media presents significant challenges. These challenges mainlystem from various factors such as the lack of annotated data, presence of noisein social media datasets, and the sheer volume of data. The common researchpractice typically examines the biased structure of online user communities fora given topic or qualitatively measuring the impacts of polarized topics onsocial media. However, there is limited work focusing on analyzing polarizationat the ground-level, specifically in the social media posts themselves. Suchexisting analysis heavily relies on annotated data, which often requireslaborious human labeling, offers labels only to specific problems, and lacksthe ability to determine the near-future bias state of a social mediaconversations. Understanding the degree of political orientation conveyed insocial media posts is crucial for quantifying the bias of online usercommunities and investigating the spread of polarized content. In this work, wefirst introduce two heuristic methods that leverage on news media bias and postcontent to label social media posts. Next, we compare the efficacy and qualityof heuristically labeled dataset with a randomly sampled human-annotateddataset. Additionally, we demonstrate that current machine learning models canexhibit improved performance in predicting political orientation of socialmedia posts, employing both traditional supervised learning and few-shotlearning setups. We conduct experiments using the proposed heuristic methodsand machine learning approaches to predict the political orientation of postscollected from two social media forums with diverse political ideologies: Gaband Twitter."
NERIF: GPT-4V for Automatic Scoring of Drawn Models,"['Gyeong-Geon Lee', 'Xiaoming Zhai']",http://arxiv.org/pdf/2311.12990v1.pdf,2023-11-21,['cs.ai'],"  Scoring student-drawn models is time-consuming. Recently released GPT-4Vprovides a unique opportunity to advance scientific modeling practices byleveraging the powerful image processing capability. To test this abilityspecifically for automatic scoring, we developed a method NERIF(Notation-Enhanced Rubric Instruction for Few-shot Learning) employinginstructional note and rubrics to prompt GPT-4V to score students' drawn modelsfor science phenomena. We randomly selected a set of balanced data (N = 900)that includes student-drawn models for six modeling assessment tasks. Eachmodel received a score from GPT-4V ranging at three levels: 'Beginning,''Developing,' or 'Proficient' according to scoring rubrics. GPT-4V scores werecompared with human experts' scores to calculate scoring accuracy. Results showthat GPT-4V's average scoring accuracy was mean =.51, SD = .037. Specifically,average scoring accuracy was .64 for the 'Beginning' class, .62 for the'Developing' class, and .26 for the 'Proficient' class, indicating that moreproficient models are more challenging to score. Further qualitative studyreveals how GPT-4V retrieves information from image input, including problemcontext, example evaluations provided by human coders, and students' drawingmodels. We also uncovered how GPT-4V catches the characteristics ofstudent-drawn models and narrates them in natural language. At last, wedemonstrated how GPT-4V assigns scores to student-drawn models according to thegiven scoring rubric and instructional notes. Our findings suggest that theNERIF is an effective approach for employing GPT-4V to score drawn models. Eventhough there is space for GPT-4V to improve scoring accuracy, some mis-assignedscores seemed interpretable to experts. The results of this study show thatutilizing GPT-4V for automatic scoring of student-drawn models is promising."
Descriptor and Word Soups: Overcoming the Parameter Efficiency Accuracy  Tradeoff for Out-of-Distribution Few-shot Learning,"['Christopher Liao', 'Theodoros Tsiligkaridis', 'Brian Kulis']",http://arxiv.org/pdf/2311.13612v1.pdf,2023-11-21,['cs.cv'],"  Over the past year, a large body of multimodal research has emerged aroundzero-shot evaluation using GPT descriptors. These studies boost the zero-shotaccuracy of pretrained VL models with an ensemble of label-specific textgenerated by GPT. A recent study, WaffleCLIP, demonstrated that similarzero-shot accuracy can be achieved with an ensemble of random descriptors.However, both zero-shot methods are un-trainable and consequently sub-optimalwhen some few-shot out-of-distribution (OOD) training data is available.Inspired by these prior works, we present two more flexible methods calleddescriptor and word soups, which do not require an LLM at test time and canleverage training data to increase OOD target accuracy. Descriptor soupgreedily selects a small set of textual descriptors using generic few-shottraining data, then calculates robust class embeddings using the selecteddescriptors. Word soup greedily assembles a chain of words in a similar manner.Compared to existing few-shot soft prompt tuning methods, word soup requiresfewer parameters by construction and less GPU memory, since it does not requirebackpropagation. Both soups outperform current published few-shot methods, evenwhen combined with SoTA zero-shot methods, on cross-dataset and domaingeneralization benchmarks. Compared with SoTA prompt and descriptor ensemblingmethods, such as ProDA and WaffleCLIP, word soup achieves higher OOD accuracywith fewer ensemble members. Please checkout our code:github.com/Chris210634/word_soups"
One Fits All: Universal Time Series Analysis by Pretrained LM and  Specially Designed Adaptors,"['Tian Zhou', 'Peisong Niu', 'Xue Wang', 'Liang Sun', 'Rong Jin']",http://arxiv.org/pdf/2311.14782v1.pdf,2023-11-24,['cs.lg'],"  Despite the impressive achievements of pre-trained models in the fields ofnatural language processing (NLP) and computer vision (CV), progress in thedomain of time series analysis has been limited. In contrast to NLP and CV,where a single model can handle various tasks, time series analysis stillrelies heavily on task-specific methods for activities such as classification,anomaly detection, forecasting, and few-shot learning. The primary obstacle todeveloping a pre-trained model for time series analysis is the scarcity ofsufficient training data. In our research, we overcome this obstacle byutilizing pre-trained models from language or CV, which have been trained onbillions of data points, and apply them to time series analysis. We assess theeffectiveness of the pre-trained transformer model in two ways. Initially, wemaintain the original structure of the self-attention and feedforward layers inthe residual blocks of the pre-trained language or image model, using theFrozen Pre-trained Transformer (FPT) for time series analysis with the additionof projection matrices for input and output. Additionally, we introduce fourunique adapters, designed specifically for downstream tasks based on thepre-trained model, including forecasting and anomaly detection. These adaptersare further enhanced with efficient parameter tuning, resulting in superiorperformance compared to all state-of-the-art methods.Our comprehensiveexperimental studies reveal that (a) the simple FPT achieves top-tierperformance across various time series analysis tasks; and (b) fine-tuning theFPT with the custom-designed adapters can further elevate its performance,outshining specialized task-specific models."
ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection,"['Yichen Bai', 'Zongbo Han', 'Changqing Zhang', 'Bing Cao', 'Xiaoheng Jiang', 'Qinghua Hu']",http://arxiv.org/pdf/2311.15243v2.pdf,2023-11-26,"['cs.cv', 'cs.ai', 'cs.lg']","  Out-of-distribution (OOD) detection methods often exploit auxiliary outliersto train model identifying OOD samples, especially discovering challengingoutliers from auxiliary outliers dataset to improve OOD detection. However,they may still face limitations in effectively distinguishing between the mostchallenging OOD samples that are much like in-distribution (ID) data, i.e.,ID-like samples. To this end, we propose a novel OOD detection framework thatdiscovers ID-like outliers using CLIP from the vicinity space of the IDsamples, thus helping to identify these most challenging OOD samples. Then aprompt learning framework is proposed that utilizes the identified ID-likeoutliers to further leverage the capabilities of CLIP for OOD detection.Benefiting from the powerful CLIP, we only need a small number of ID samples tolearn the prompts of the model without exposing other auxiliary outlierdatasets. By focusing on the most challenging ID-like OOD samples and elegantlyexploiting the capabilities of CLIP, our method achieves superior few-shotlearning performance on various real-world image datasets (e.g., in 4-shot OODdetection on the ImageNet-1k dataset, our method reduces the average FPR95 by12.16% and improves the average AUROC by 2.76%, compared to state-of-the-artmethods)."
Explaining CLIP's performance disparities on data from blind/low vision  users,"['Daniela Massiceti', 'Camilla Longden', 'Agnieszka Słowik', 'Samuel Wills', 'Martin Grayson', 'Cecily Morrison']",http://arxiv.org/pdf/2311.17315v2.pdf,2023-11-29,['cs.cv'],"  Large multi-modal models (LMMs) hold the potential to usher in a new era ofautomated visual assistance for people who are blind or low vision (BLV). Yet,these models have not been systematically evaluated on data captured by BLVusers. We address this by empirically assessing CLIP, a widely-used LMM likelyto underpin many assistive technologies. Testing 25 CLIP variants in azero-shot classification task, we find that their accuracy is 15 percentagepoints lower on average for images captured by BLV users than web-crawledimages. This disparity stems from CLIP's sensitivities to 1) image content(e.g. not recognizing disability objects as well as other objects); 2) imagequality (e.g. not being robust to lighting variation); and 3) text content(e.g. not recognizing objects described by tactile adjectives as well as visualones). We delve deeper with a textual analysis of three common pre-trainingdatasets: LAION-400M, LAION-2B and DataComp-1B, showing that disability contentis rarely mentioned. We then provide three examples that illustrate how theperformance disparities extend to three downstream models underpinned by CLIP:OWL-ViT, CLIPSeg and DALL-E2. We find that few-shot learning with as few as 5images can mitigate CLIP's quality-of-service disparities for BLV users in somescenarios, which we discuss alongside a set of other possible mitigations."
Generalizing Political Leaning Inference to Multi-Party Systems:  Insights from the UK Political Landscape,"['Joseba Fernandez de Landa', 'Arkaitz Zubiaga', 'Rodrigo Agerri']",http://arxiv.org/pdf/2312.01738v1.pdf,2023-12-04,"['cs.si', 'cs.cy']","  An ability to infer the political leaning of social media users can help ingathering opinion polls thereby leading to a better understanding of publicopinion. While there has been a body of research attempting to infer thepolitical leaning of social media users, this has been typically simplified asa binary classification problem (e.g. left vs right) and has been limited to asingle location, leading to a dearth of investigation into more complex,multiclass classification and its generalizability to different locations,particularly those with multi-party systems. Our work performs the first sucheffort by studying political leaning inference in three of the UK's nations(Scotland, Wales and Northern Ireland), each of which has a different politicallandscape composed of multiple parties. To do so, we collect and release adataset comprising users labelled by their political leaning as well asinteractions with one another. We investigate the ability to predict thepolitical leaning of users by leveraging these interactions in challengingscenarios such as few-shot learning, where training data is scarce, as well asassessing the applicability to users with different levels of politicalengagement. We show that interactions in the form of retweets between users canbe a very powerful feature to enable political leaning inference, leading toconsistent and robust results across different regions with multi-partysystems. However, we also see that there is room for improvement in predictingthe political leaning of users who are less engaged in politics."
Diversified in-domain synthesis with efficient fine-tuning for few-shot  classification,"['Victor G. Turrisi da Costa', ""Nicola Dall'Asen"", 'Yiming Wang', 'Nicu Sebe', 'Elisa Ricci']",http://arxiv.org/pdf/2312.03046v2.pdf,2023-12-05,['cs.cv'],"  Few-shot image classification aims to learn an image classifier using only asmall set of labeled examples per class. A recent research direction forimproving few-shot classifiers involves augmenting the labelled samples withsynthetic images created by state-of-the-art text-to-image generation models.Following this trend, we propose Diversified In-domain Synthesis with EfficientFine-tuning (DISEF), a novel approach which addresses the generalizationchallenge in few-shot learning using synthetic data. DISEF consists of two maincomponents. First, we propose a novel text-to-image augmentation pipeline that,by leveraging the real samples and their rich semantics coming from an advancedcaptioning model, promotes in-domain sample diversity for bettergeneralization. Second, we emphasize the importance of effective modelfine-tuning in few-shot recognition, proposing to use Low-Rank Adaptation(LoRA) for joint adaptation of the text and image encoders in a Vision LanguageModel. We validate our method in ten different benchmarks, consistentlyoutperforming baselines and establishing a new state-of-the-art for few-shotclassification. Code is available at https://github.com/vturrisi/disef."
Clinical Risk Prediction Using Language Models: Benefits And  Considerations,"['Angeela Acharya', 'Sulabh Shrestha', 'Anyi Chen', 'Joseph Conte', 'Sanja Avramovic', 'Siddhartha Sikdar', 'Antonios Anastasopoulos', 'Sanmay Das']",http://arxiv.org/pdf/2312.03742v1.pdf,2023-11-29,"['cs.cl', 'cs.lg']","  The utilization of Electronic Health Records (EHRs) for clinical riskprediction is on the rise. However, strict privacy regulations limit access tocomprehensive health records, making it challenging to apply standard machinelearning algorithms in practical real-world scenarios. Previous research hasaddressed this data limitation by incorporating medical ontologies andemploying transfer learning methods. In this study, we investigate thepotential of leveraging language models (LMs) as a means to incorporatesupplementary domain knowledge for improving the performance of variousEHR-based risk prediction tasks. Unlike applying LMs to unstructured EHR datasuch as clinical notes, this study focuses on using textual descriptions withinstructured EHR to make predictions exclusively based on that information. Weextensively compare against previous approaches across various data types andsizes. We find that employing LMs to represent structured EHRs, such asdiagnostic histories, leads to improved or at least comparable performance indiverse risk prediction tasks. Furthermore, LM-based approaches offer numerousadvantages, including few-shot learning, the capability to handle previouslyunseen medical concepts, and adaptability to various medical vocabularies.Nevertheless, we underscore, through various experiments, the importance ofbeing cautious when employing such models, as concerns regarding thereliability of LMs persist."
Few-Shot Class-Incremental Learning via Training-Free Prototype  Calibration,"['Qi-Wei Wang', 'Da-Wei Zhou', 'Yi-Kai Zhang', 'De-Chuan Zhan', 'Han-Jia Ye']",http://arxiv.org/pdf/2312.05229v1.pdf,2023-12-08,"['cs.cv', 'cs.lg']","  Real-world scenarios are usually accompanied by continuously appearingclasses with scare labeled samples, which require the machine learning model toincrementally learn new classes and maintain the knowledge of base classes. Inthis Few-Shot Class-Incremental Learning (FSCIL) scenario, existing methodseither introduce extra learnable components or rely on a frozen featureextractor to mitigate catastrophic forgetting and overfitting problems.However, we find a tendency for existing methods to misclassify the samples ofnew classes into base classes, which leads to the poor performance of newclasses. In other words, the strong discriminability of base classes distractsthe classification of new classes. To figure out this intriguing phenomenon, weobserve that although the feature extractor is only trained on base classes, itcan surprisingly represent the semantic similarity between the base and unseennew classes. Building upon these analyses, we propose a simple yet effectiveTraining-frEE calibratioN (TEEN) strategy to enhance the discriminability ofnew classes by fusing the new prototypes (i.e., mean features of a class) withweighted base prototypes. In addition to standard benchmarks in FSCIL, TEENdemonstrates remarkable performance and consistent improvements over baselinemethods in the few-shot learning scenario. Code is available at:https://github.com/wangkiw/TEEN"
Self-supervised Adaptive Pre-training of Multilingual Speech Models for  Language and Dialect Identification,"['Mohammed Maqsood Shaik', 'Dietrich Klakow', 'Badr M. Abdullah']",http://arxiv.org/pdf/2312.07338v1.pdf,2023-12-12,"['cs.cl', 'cs.sd', 'eess.as']","  Pre-trained Transformer-based speech models have shown striking performancewhen fine-tuned on various downstream tasks such as automatic speechrecognition and spoken language identification (SLID). However, the problem ofdomain mismatch remains a challenge in this area, where the domain of thepre-training data might differ from that of the downstream labeled data usedfor fine-tuning. In multilingual tasks such as SLID, the pre-trained speechmodel may not support all the languages in the downstream task. To address thischallenge, we propose self-supervised adaptive pre-training (SAPT) to adapt thepre-trained model to the target domain and languages of the downstream task. Weapply SAPT to the XLSR-128 model and investigate the effectiveness of thisapproach for the SLID task. First, we demonstrate that SAPT improves XLSRperformance on the FLEURS benchmark with substantial gains up to 40.1% forunder-represented languages. Second, we apply SAPT on four different datasetsin a few-shot learning setting, showing that our approach improves the sampleefficiency of XLSR during fine-tuning. Our experiments provide strong empiricalevidence that continual adaptation via self-supervision improves downstreamperformance for multilingual speech models."
Beyond English: Evaluating LLMs for Arabic Grammatical Error Correction,"['Sang Yun Kwon', 'Gagan Bhatia', 'El Moatez Billah Nagoudi', 'Muhammad Abdul-Mageed']",http://arxiv.org/pdf/2312.08400v1.pdf,2023-12-13,"['cs.cl', 'cs.ai']","  Large language models (LLMs) finetuned to follow human instruction haverecently exhibited significant capabilities in various English NLP tasks.However, their performance in grammatical error correction (GEC), especially onlanguages other than English, remains significantly unexplored. In this work,we evaluate the abilities of instruction finetuned LLMs in Arabic GEC, acomplex task due to Arabic's rich morphology. Our findings suggest that variousprompting methods, coupled with (in-context) few-shot learning, demonstrateconsiderable effectiveness, with GPT-4 achieving up to $65.49$ F$_{1}$ scoreunder expert prompting (approximately $5$ points higher than our establishedbaseline). Despite these positive results, we find that instruction finetunedmodels, regardless of their size, are still outperformed by fully finetunedones, even if they are significantly smaller in size. This disparity highlightssubstantial room for improvements for LLMs. Inspired by methods used inlow-resource machine translation, we also develop a method exploiting syntheticdata that significantly outperforms previous models on two standard Arabicbenchmarks. Our best model achieves a new SOTA on Arabic GEC, with $73.29$ and$73.26$ F$_{1}$ on the 2014 and 2015 QALB datasets, respectively, compared topeer-reviewed published baselines."
A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs  for Financial Sentiment Analysis,"['Sorouralsadat Fatemi', 'Yuheng Hu']",http://arxiv.org/pdf/2312.08725v1.pdf,2023-12-14,"['cs.lg', 'cs.cl']","  Financial sentiment analysis plays a crucial role in uncovering latentpatterns and detecting emerging trends, enabling individuals to makewell-informed decisions that may yield substantial advantages within theconstantly changing realm of finance. Recently, Large Language Models (LLMs)have demonstrated their effectiveness in diverse domains, showcasing remarkablecapabilities even in zero-shot and few-shot in-context learning for variousNatural Language Processing (NLP) tasks. Nevertheless, their potential andapplicability in the context of financial sentiment analysis have not beenthoroughly explored yet. To bridge this gap, we employ two approaches:in-context learning (with a focus on gpt-3.5-turbo model) and fine-tuning LLMson a finance-domain dataset. Given the computational costs associated withfine-tuning LLMs with large parameter sizes, our focus lies on smaller LLMs,spanning from 250M to 3B parameters for fine-tuning. We then compare theperformances with state-of-the-art results to evaluate their effectiveness inthe finance-domain. Our results demonstrate that fine-tuned smaller LLMs canachieve comparable performance to state-of-the-art fine-tuned LLMs, even withmodels having fewer parameters and a smaller training dataset. Additionally,the zero-shot and one-shot performance of LLMs produces comparable results withfine-tuned smaller LLMs and state-of-the-art outcomes. Furthermore, ouranalysis demonstrates that there is no observed enhancement in performance forfinance-domain sentiment analysis when the number of shots for in-contextlearning is increased."
Continual Adversarial Defense,"['Qian Wang', 'Yaoyao Liu', 'Hefei Ling', 'Yingwei Li', 'Qihao Liu', 'Ping Li', 'Jiazhong Chen', 'Alan Yuille', 'Ning Yu']",http://arxiv.org/pdf/2312.09481v1.pdf,2023-12-15,"['cs.cv', 'cs.cr', 'cs.lg']","  In response to the rapidly evolving nature of adversarial attacks on amonthly basis, numerous defenses have been proposed to generalize against asmany known attacks as possible. However, designing a defense method that cangeneralize to all types of attacks, including unseen ones, is not realisticbecause the environment in which defense systems operate is dynamic andcomprises various unique attacks used by many attackers. The defense systemneeds to upgrade itself by utilizing few-shot defense feedback and efficientmemory. Therefore, we propose the first continual adversarial defense (CAD)framework that adapts to any attacks in a dynamic scenario, where variousattacks emerge stage by stage. In practice, CAD is modeled under fourprinciples: (1) continual adaptation to new attacks without catastrophicforgetting, (2) few-shot adaptation, (3) memory-efficient adaptation, and (4)high accuracy on both clean and adversarial images. We leverage cutting-edgecontinual learning, few-shot learning, and ensemble learning techniques toqualify the principles. Experiments conducted on CIFAR-10 and ImageNet-100validate the effectiveness of our approach against multiple stages of 10 modernadversarial attacks and significant improvements over 10 baseline methods. Inparticular, CAD is capable of quickly adapting with minimal feedback and a lowcost of defense failure, while maintaining good performance against oldattacks. Our research sheds light on a brand-new paradigm for continual defenseadaptation against dynamic and evolving attacks."
Transferrable Feature and Projection Learning with Class Hierarchy for  Zero-Shot Learning,"['Aoxue Li', 'Zhiwu Lu', 'Jiechao Guan', 'Tao Xiang', 'Liwei Wang', 'Ji-Rong Wen']",http://arxiv.org/pdf/1810.08329v1.pdf,2018-10-19,"['cs.cv', 'cs.lg']","  Zero-shot learning (ZSL) aims to transfer knowledge from seen classes tounseen ones so that the latter can be recognised without any training samples.This is made possible by learning a projection function between a feature spaceand a semantic space (e.g. attribute space). Considering the seen and unseenclasses as two domains, a big domain gap often exists which challenges ZSL.Inspired by the fact that an unseen class is not exactly `unseen' if it belongsto the same superclass as a seen class, we propose a novel inductive ZSL modelthat leverages superclasses as the bridge between seen and unseen classes tonarrow the domain gap. Specifically, we first build a class hierarchy ofmultiple superclass layers and a single class layer, where the superclasses areautomatically generated by data-driven clustering over the semanticrepresentations of all seen and unseen class names. We then exploit thesuperclasses from the class hierarchy to tackle the domain gap challenge in twoaspects: deep feature learning and projection function learning. First, tonarrow the domain gap in the feature space, we integrate a recurrent neuralnetwork (RNN) defined with the superclasses into a convolutional neural network(CNN), in order to enforce the superclass hierarchy. Second, to further learn atransferrable projection function for ZSL, a novel projection function learningmethod is proposed by exploiting the superclasses to align the two domains.Importantly, our transferrable feature and projection learning methods can beeasily extended to a closely related task -- few-shot learning (FSL). Extensiveexperiments show that the proposed model significantly outperforms thestate-of-the-art alternatives in both ZSL and FSL tasks."
Interpreting and Understanding Graph Convolutional Neural Network using  Gradient-based Attribution Method,"['Shangsheng Xie', 'Mingming Lu']",http://arxiv.org/pdf/1903.03768v2.pdf,2019-03-09,['cs.lg'],"  To solve the problem that convolutional neural networks (CNNs) are difficultto process non-grid type relational data like graphs, Kipf et al. proposed agraph convolutional neural network (GCN). The core idea of the GCN is toperform two-fold informational fusion for each node in a given graph duringeach iteration: the fusion of graph structure information and the fusion ofnode feature dimensions. Because of the characteristic of the combinatorialgeneralizations, GCN has been widely used in the fields of scene semanticrelationship analysis, natural language processing and few-shot learning etc.However, due to its two-fold informational fusion involves mathematicalirreversible calculations, it is hard to explain the decision reason for theprediction of the each node classification. Unfortunately, most of the existingattribution analysis methods concentrate on the models like CNNs, which areutilized to process grid-like data. It is difficult to apply those analysismethods to the GCN directly. It is because compared with the independence amongCNNs input data, there is correlation between the GCN input data. Thisresulting in the existing attribution analysis methods can only obtain thepartial model contribution from the central node features to the final decisionof the GCN, but ignores the other model contribution from central node featuresand its neighbor nodes features to that decision. To this end, we propose agradient attribution analysis method for the GCN called Node Attribution Method(NAM), which can get the model contribution from not only the central node butalso its neighbor nodes to the GCN output. We also propose the Node ImportanceVisualization (NIV) method to visualize the central node and its neighbor nodesbased on the value of the contribution..."
MetAdapt: Meta-Learned Task-Adaptive Architecture for Few-Shot  Classification,"['Sivan Doveh', 'Eli Schwartz', 'Chao Xue', 'Rogerio Feris', 'Alex Bronstein', 'Raja Giryes', 'Leonid Karlinsky']",http://arxiv.org/pdf/1912.00412v3.pdf,2019-12-01,"['cs.cv', 'cs.lg']","  Few-Shot Learning (FSL) is a topic of rapidly growing interest. Typically, inFSL a model is trained on a dataset consisting of many small tasks (meta-tasks)and learns to adapt to novel tasks that it will encounter during test time.This is also referred to as meta-learning. Another topic closely related tometa-learning with a lot of interest in the community is Neural ArchitectureSearch (NAS), automatically finding optimal architecture instead of engineeringit manually. In this work, we combine these two aspects of meta-learning. Sofar, meta-learning FSL methods have focused on optimizing parameters ofpre-defined network architectures, in order to make them easily adaptable tonovel tasks. Moreover, it was observed that, in general, larger architecturesperform better than smaller ones up to a certain saturation point (where theystart to degrade due to over-fitting). However, little attention has been givento explicitly optimizing the architectures for FSL, nor to an adaptation of thearchitecture at test time to particular novel tasks. In this work, we proposeto employ tools inspired by the Differentiable Neural Architecture Search(D-NAS) literature in order to optimize the architecture for FSL withoutover-fitting. Additionally, to make the architecture task adaptive, we proposethe concept of `MetAdapt Controller' modules. These modules are added to themodel and are meta-trained to predict the optimal network connections for agiven novel task. Using the proposed approach we observe state-of-the-artresults on two popular few-shot benchmarks: miniImageNet and FC100."
A Revision of Neural Tangent Kernel-based Approaches for Neural Networks,"['Kyung-Su Kim', 'Aurélie C. Lozano', 'Eunho Yang']",http://arxiv.org/pdf/2007.00884v2.pdf,2020-07-02,"['cs.lg', 'stat.ml']","  Recent theoretical works based on the neural tangent kernel (NTK) have shedlight on the optimization and generalization of over-parameterized networks,and partially bridge the gap between their practical success and classicallearning theory. Especially, using the NTK-based approach, the following threerepresentative results were obtained: (1) A training error bound was derived toshow that networks can fit any finite training sample perfectly by reflecting atighter characterization of training speed depending on the data complexity.(2) A generalization error bound invariant of network size was derived by usinga data-dependent complexity measure (CMD). It follows from this CMD bound thatnetworks can generalize arbitrary smooth functions. (3) A simple and analytickernel function was derived as indeed equivalent to a fully-trained network.This kernel outperforms its corresponding network and the existing goldstandard, Random Forests, in few shot learning. For all of these results tohold, the network scaling factor $\kappa$ should decrease w.r.t. sample size n.In this case of decreasing $\kappa$, however, we prove that the aforementionedresults are surprisingly erroneous. It is because the output value of trainednetwork decreases to zero when $\kappa$ decreases w.r.t. n. To solve thisproblem, we tighten key bounds by essentially removing $\kappa$-affectedvalues. Our tighter analysis resolves the scaling problem and enables thevalidation of the original NTK-based results."
Top-Related Meta-Learning Method for Few-Shot Object Detection,"['Qian Li', 'Nan Guo', 'Xiaochun Ye', 'Duo Wang', 'Dongrui Fan', 'Zhimin Tang']",http://arxiv.org/pdf/2007.06837v6.pdf,2020-07-14,"['cs.cv', 'cs.lg']","  Many meta-learning methods are proposed for few-shot detection. However,previous most methods have two main problems, poor detection APs, and strongbias because of imbalance and insufficient datasets. Previous works mainlyalleviate these issues by additional datasets, multi-relation attentionmechanisms and sub-modules. However, they require more cost. In this work, formeta-learning, we find that the main challenges focus on related or irrelevantsemantic features between categories. Therefore, based on semantic features, wepropose a Top-C classification loss (i.e., TCL-C) for classification task and acategory-based grouping mechanism for category-based meta-features obtained bythe meta-model. The TCL-C exploits the true-label prediction and the mostlikely C-1 false classification predictions to improve detection performance onfew-shot classes. According to similar appearance (i.e., visual appearance,shape, and limbs etc.) and environment in which objects often appear, thecategory-based grouping mechanism splits categories into disjoint groups tomake similar semantic features more compact between categories within a groupand obtain more significant difference between groups, alleviating the strongbias problem and further improving detection APs. The whole training consistsof the base model and the fine-tuning phases. According to grouping mechanism,we group the meta-features vectors obtained by meta-model, so that thedistribution difference between groups is obvious, and the one within eachgroup is less. Extensive experiments on Pascal VOC dataset demonstrate thatours which combines the TCL-C with category-based grouping significantlyoutperforms previous state-of-the-art methods for few-shot detection. Comparedwith previous competitive baseline, ours improves detection APs by almost 4%for few-shot detection."
Lesion2Vec: Deep Metric Learning for Few-Shot Multiple Lesions  Recognition in Wireless Capsule Endoscopy Video,"['Sodiq Adewole', 'Philip Fernandez', 'Michelle Yeghyayan', 'James Jablonski', 'Andrew Copland', 'Michael Porter', 'Sana Syed', 'Donald Brown']",http://arxiv.org/pdf/2101.04240v2.pdf,2021-01-11,['cs.cv'],"  Effective and rapid detection of lesions in the Gastrointestinal tract iscritical to gastroenterologist's response to some life-threatening diseases.Wireless Capsule Endoscopy (WCE) has revolutionized traditional endoscopyprocedure by allowing gastroenterologists visualize the entire GI tractnon-invasively. Once the tiny capsule is swallowed, it sequentially captureimages of the GI tract at about 2 to 6 frames per second (fps). A single videocan last up to 8 hours producing between 30,000 to 100,000 images. Automatingthe detection of frames containing specific lesion in WCE video would relievegastroenterologists the arduous task of reviewing the entire video beforemaking diagnosis. While the WCE produces large volume of images, only about 5\%of the frames contain lesions that aid the diagnosis process. ConvolutionalNeural Network (CNN) based models have been very successful in various imageclassification tasks. However, they suffer excessive parameters, are sampleinefficient and rely on very large amount of training data. Deploying a CNNclassifier for lesion detection task will require time-to-time fine-tuning togeneralize to any unforeseen category. In this paper, we propose a metric-basedlearning framework followed by a few-shot lesion recognition in WCE data.Metric-based learning is a meta-learning framework designed to establishsimilarity or dissimilarity between concepts while few-shot learning (FSL) aimsto identify new concepts from only a small number of examples. We train afeature extractor to learn a representation for different small bowel lesionsusing metric-based learning. At the testing stage, the category of an unseensample is predicted from only a few support examples, thereby allowing themodel to generalize to a new category that has never been seen before. Wedemonstrated the efficacy of this method on real patient capsule endoscopydata."
'Squeeze & Excite' Guided Few-Shot Segmentation of Volumetric Images,"['Abhijit Guha Roy', 'Shayan Siddiqui', 'Sebastian Pölsterl', 'Nassir Navab', 'Christian Wachinger']",http://arxiv.org/pdf/1902.01314v2.pdf,2019-02-04,['cs.cv'],"  Deep neural networks enable highly accurate image segmentation, but requirelarge amounts of manually annotated data for supervised training. Few-shotlearning aims to address this shortcoming by learning a new class from a fewannotated support examples. We introduce, a novel few-shot framework, for thesegmentation of volumetric medical images with only a few annotated slices.Compared to other related works in computer vision, the major challenges arethe absence of pre-trained networks and the volumetric nature of medical scans.We address these challenges by proposing a new architecture for few-shotsegmentation that incorporates 'squeeze & excite' blocks. Our two-armedarchitecture consists of a conditioner arm, which processes the annotatedsupport input and generates a task-specific representation. This representationis passed on to the segmenter arm that uses this information to segment the newquery image. To facilitate efficient interaction between the conditioner andthe segmenter arm, we propose to use 'channel squeeze & spatial excitation'blocks - a light-weight computational module - that enables heavy interactionbetween both the arms with negligible increase in model complexity. Thiscontribution allows us to perform image segmentation without relying on apre-trained model, which generally is unavailable for medical scans.Furthermore, we propose an efficient strategy for volumetric segmentation byoptimally pairing a few slices of the support volume to all the slices of thequery volume. We perform experiments for organ segmentation on whole-bodycontrast-enhanced CT scans from the Visceral Dataset. Our proposed modeloutperforms multiple baselines and existing approaches with respect to thesegmentation accuracy by a significant margin. The source code is available athttps://github.com/abhi4ssj/few-shot-segmentation."
"Intelligence, physics and information -- the tradeoff between accuracy  and simplicity in machine learning",['Tailin Wu'],http://arxiv.org/pdf/2001.03780v2.pdf,2020-01-11,"['cs.lg', 'physics.data-an', 'stat.ml']","  How can we enable machines to make sense of the world, and become better atlearning? To approach this goal, I believe viewing intelligence in terms ofmany integral aspects, and also a universal two-term tradeoff between taskperformance and complexity, provides two feasible perspectives. In this thesis,I address several key questions in some aspects of intelligence, and study thephase transitions in the two-term tradeoff, using strategies and tools fromphysics and information. Firstly, how can we make the learning models moreflexible and efficient, so that agents can learn quickly with fewer examples?Inspired by how physicists model the world, we introduce a paradigm and an AIPhysicist agent for simultaneously learning many small specialized models(theories) and the domain they are accurate, which can then be simplified,unified and stored, facilitating few-shot learning in a continual way.Secondly, for representation learning, when can we learn a good representation,and how does learning depend on the structure of the dataset? We approach thisquestion by studying phase transitions when tuning the tradeoff hyperparameter.In the information bottleneck, we theoretically show that these phasetransitions are predictable and reveal structure in the relationships betweenthe data, the model, the learned representation and the loss landscape.Thirdly, how can agents discover causality from observations? We address partof this question by introducing an algorithm that combines prediction andminimizing information from the input, for exploratory causal discovery fromobservational time series. Fourthly, to make models more robust to label noise,we introduce Rank Pruning, a robust algorithm for classification with noisylabels. I believe that building on the work of my thesis we will be one stepcloser to enable more intelligent machines that can make sense of the world."
Deep Learning Algorithms for Rotating Machinery Intelligent Diagnosis:  An Open Source Benchmark Study,"['Zhibin Zhao', 'Tianfu Li', 'Jingyao Wu', 'Chuang Sun', 'Shibin Wang', 'Ruqiang Yan', 'Xuefeng Chen']",http://arxiv.org/pdf/2003.03315v3.pdf,2020-03-06,"['eess.sp', 'cs.lg']","  With the development of deep learning (DL) techniques, rotating machineryintelligent diagnosis has gone through tremendous progress with verifiedsuccess and the classification accuracies of many DL-based intelligentdiagnosis algorithms are tending to 100\%. However, different datasets,configurations, and hyper-parameters are often recommended to be used inperformance verification for different types of models, and few open sourcecodes are made public for evaluation and comparisons. Therefore, unfaircomparisons and ineffective improvement may exist in rotating machineryintelligent diagnosis, which limits the advancement of this field. To addressthese issues, we perform an extensive evaluation of four kinds of models,including multi-layer perception (MLP), auto-encoder (AE), convolutional neuralnetwork (CNN), and recurrent neural network (RNN), with various datasets toprovide a benchmark study within the same framework. We first gather most ofthe publicly available datasets and give the complete benchmark study ofDL-based intelligent algorithms under two data split strategies, five inputformats, three normalization methods, and four augmentation methods. Second, weintegrate the whole evaluation codes into a code library and release this codelibrary to the public for better development of this field. Third, we usespecific-designed cases to point out the existing issues, including classimbalance, generalization ability, interpretability, few-shot learning, andmodel selection. By these works, we release a unified code framework forcomparing and testing models fairly and quickly, emphasize the importance ofopen source codes, provide the baseline accuracy (a lower bound) to avoiduseless improvement, and discuss potential future directions in this field. Thecode library is available athttps://github.com/ZhaoZhibin/DL-based-Intelligent-Diagnosis-Benchmark."
Adaptive-Step Graph Meta-Learner for Few-Shot Graph Classification,"['Ning Ma', 'Jiajun Bu', 'Jieyu Yang', 'Zhen Zhang', 'Chengwei Yao', 'Zhi Yu', 'Sheng Zhou', 'Xifeng Yan']",http://arxiv.org/pdf/2003.08246v2.pdf,2020-03-18,"['cs.lg', 'stat.ml']","  Graph classification aims to extract accurate information fromgraph-structured data for classification and is becoming more and moreimportant in graph learning community. Although Graph Neural Networks (GNNs)have been successfully applied to graph classification tasks, most of themoverlook the scarcity of labeled graph data in many applications. For example,in bioinformatics, obtaining protein graph labels usually needs laboriousexperiments. Recently, few-shot learning has been explored to alleviate thisproblem with only given a few labeled graph samples of test classes. The sharedsub-structures between training classes and test classes are essential infew-shot graph classification. Exiting methods assume that the test classesbelong to the same set of super-classes clustered from training classes.However, according to our observations, the label spaces of training classesand test classes usually do not overlap in real-world scenario. As a result,the existing methods don't well capture the local structures of unseen testclasses. To overcome the limitation, in this paper, we propose a direct methodto capture the sub-structures with well initialized meta-learner within a fewadaptation steps. More specifically, (1) we propose a novel frameworkconsisting of a graph meta-learner, which uses GNNs based modules for fastadaptation on graph data, and a step controller for the robustness andgeneralization of meta-learner; (2) we provide quantitative analysis for theframework and give a graph-dependent upper bound of the generalization errorbased on our framework; (3) the extensive experiments on real-world datasetsdemonstrate that our framework gets state-of-the-art results on severalfew-shot graph classification tasks compared to baselines."
Towards Data-Efficient Learning: A Benchmark for COVID-19 CT Lung and  Infection Segmentation,"['Jun Ma', 'Yixin Wang', 'Xingle An', 'Cheng Ge', 'Ziqi Yu', 'Jianan Chen', 'Qiongjie Zhu', 'Guoqiang Dong', 'Jian He', 'Zhiqiang He', 'Yuntao Zhu', 'Ziwei Nie', 'Xiaoping Yang']",http://arxiv.org/pdf/2004.12537v2.pdf,2020-04-27,"['eess.iv', 'cs.cv', 'cs.lg']","  Purpose: Accurate segmentation of lung and infection in COVID-19 CT scansplays an important role in the quantitative management of patients. Most of theexisting studies are based on large and private annotated datasets that areimpractical to obtain from a single institution, especially when radiologistsare busy fighting the coronavirus disease. Furthermore, it is hard to comparecurrent COVID-19 CT segmentation methods as they are developed on differentdatasets, trained in different settings, and evaluated with different metrics.Methods: To promote the development of data-efficient deep learning methods, inthis paper, we built three benchmarks for lung and infection segmentation basedon 70 annotated COVID-19 cases, which contain current active research areas,e.g., few-shot learning, domain generalization, and knowledge transfer. For afair comparison among different segmentation methods, we also provide standardtraining, validation and testing splits, evaluation metrics and, thecorresponding code. Results: Based on the state-of-the-art network, we providemore than 40 pre-trained baseline models, which not only serve asout-of-the-box segmentation tools but also save computational time forresearchers who are interested in COVID-19 lung and infection segmentation. Weachieve average Dice Similarity Coefficient (DSC) scores of 97.3\%, 97.7\%, and67.3\% and average Normalized Surface Dice (NSD) scores of 90.6\%, 91.4\%, and70.0\% for left lung, right lung, and infection, respectively. Conclusions: Tothe best of our knowledge, this work presents the first data-efficient learningbenchmark for medical image segmentation and the largest number of pre-trainedmodels up to now. All these resources are publicly available, and our work laysthe foundation for promoting the development of deep learning methods forefficient COVID-19 CT segmentation with limited data."
Zero-Shot Learning and its Applications from Autonomous Vehicles to  COVID-19 Diagnosis: A Review,"['Mahdi Rezaei', 'Mahsa Shahidi']",http://arxiv.org/pdf/2004.14143v3.pdf,2020-04-29,"['cs.cv', 'cs.lg', 'stat.ml']","  The challenge of learning a new concept, object, or a new medical diseaserecognition without receiving any examples beforehand is called Zero-ShotLearning (ZSL). One of the major issues in deep learning based methodologiessuch as in Medical Imaging and other real-world applications is the requirementof large annotated datasets prepared by clinicians or experts to train themodel. ZSL is known for having minimal human intervention by relying only onpreviously known or trained concepts plus currently existing auxiliaryinformation. This makes the ZSL applicable in many real-world scenarios, fromunknown object detection in autonomous vehicles to medical imaging andunforeseen diseases such as COVID-19 Chest X-Ray (CXR) based diagnosis. Weintroduce a novel and broaden solution called Few/one-shot learning, andpresent the definition of the ZSL problem as an extreme case of the few-shotlearning. We review over fundamentals and the challenging steps of Zero-ShotLearning, including state-of-the-art categories of solutions, as well as ourrecommended solution, motivations behind each approach, their advantages overeach category to guide both clinicians and AI researchers to proceed with thebest techniques and practices based on their applications. We then reviewthrough different datasets inducing medical and non-medical images, the varietyof splits, and the evaluation protocols proposed so far. Finally, we discussthe recent applications and future directions of ZSL. We aim to convey a usefulintuition through this paper towards the goal of handling complex learningtasks more similar to the way humans learn. We mainly focus on two applicationsin the current modern yet challenging era: coping with an early and fastdiagnosis of COVID-19 cases, and also encouraging the readers to develop othersimilar AI-based automated detection/recognition systems using ZSL."
Compositional Few-Shot Recognition with Primitive Discovery and  Enhancing,"['Yixiong Zou', 'Shanghang Zhang', 'Ke Chen', 'Yonghong Tian', 'Yaowei Wang', 'José M. F. Moura']",http://arxiv.org/pdf/2005.06047v3.pdf,2020-05-12,"['cs.cv', 'cs.mm']","  Few-shot learning (FSL) aims at recognizing novel classes given only fewtraining samples, which still remains a great challenge for deep learning.However, humans can easily recognize novel classes with only few samples. A keycomponent of such ability is the compositional recognition that human canperform, which has been well studied in cognitive science but is not wellexplored in FSL. Inspired by such capability of humans, to imitate humans'ability of learning visual primitives and composing primitives to recognizenovel classes, we propose an approach to FSL to learn a feature representationcomposed of important primitives, which is jointly trained with two parts, i.e.primitive discovery and primitive enhancing. In primitive discovery, we focuson learning primitives related to object parts by self-supervision from theorder of image splits, avoiding extra laborious annotations and alleviating theeffect of semantic gaps. In primitive enhancing, inspired by current studies onthe interpretability of deep networks, we provide our composition view for theFSL baseline model. To modify this model for effective composition, inspired byboth mathematical deduction and biological studies (the Hebbian Learning ruleand the Winner-Take-All mechanism), we propose a soft composition mechanism byenlarging the activation of important primitives while reducing that of others,so as to enhance the influence of important primitives and better utilize theseprimitives to compose novel classes. Extensive experiments on public benchmarksare conducted on both the few-shot image classification and video recognitiontasks. Our method achieves the state-of-the-art performance on all thesedatasets and shows better interpretability."
Many-Class Few-Shot Learning on Multi-Granularity Class Hierarchy,"['Lu Liu', 'Tianyi Zhou', 'Guodong Long', 'Jing Jiang', 'Chengqi Zhang']",http://arxiv.org/pdf/2006.15479v1.pdf,2020-06-28,"['cs.lg', 'stat.ml']","  We study many-class few-shot (MCFS) problem in both supervised learning andmeta-learning settings. Compared to the well-studied many-class many-shot andfew-class few-shot problems, the MCFS problem commonly occurs in practicalapplications but has been rarely studied in previous literature. It brings newchallenges of distinguishing between many classes given only a few trainingsamples per class. In this paper, we leverage the class hierarchy as a priorknowledge to train a coarse-to-fine classifier that can produce accuratepredictions for MCFS problem in both settings. The propose model,""memory-augmented hierarchical-classification network (MahiNet)"", performscoarse-to-fine classification where each coarse class can cover multiple fineclasses. Since it is challenging to directly distinguish a variety of fineclasses given few-shot data per class, MahiNet starts from learning aclassifier over coarse-classes with more training data whose labels are muchcheaper to obtain. The coarse classifier reduces the searching range over thefine classes and thus alleviates the challenges from ""many classes"". Onarchitecture, MahiNet firstly deploys a convolutional neural network (CNN) toextract features. It then integrates a memory-augmented attention module and amulti-layer perceptron (MLP) together to produce the probabilities over coarseand fine classes. While the MLP extends the linear classifier, the attentionmodule extends the KNN classifier, both together targeting the ""few-shot""problem. We design several training strategies of MahiNet for supervisedlearning and meta-learning. In addition, we propose two novel benchmarkdatasets ""mcfsImageNet"" and ""mcfsOmniglot"" specially designed for MCFS problem.In experiments, we show that MahiNet outperforms several state-of-the-artmodels on MCFS problems in both supervised learning and meta-learning."
A Primal-Dual Subgradient Approachfor Fair Meta Learning,"['Chen Zhao', 'Feng Chen', 'Zhuoyi Wang', 'Latifur Khan']",http://arxiv.org/pdf/2009.12675v3.pdf,2020-09-26,"['cs.lg', 'stat.ml']","  The problem of learning to generalize to unseen classes during training,known as few-shot classification, has attracted considerable attention.Initialization based methods, such as the gradient-based model agnosticmeta-learning (MAML), tackle the few-shot learning problem by ""learning tofine-tune"". The goal of these approaches is to learn proper modelinitialization, so that the classifiers for new classes can be learned from afew labeled examples with a small number of gradient update steps. Few shotmeta-learning is well-known with its fast-adapted capability and accuracygeneralization onto unseen tasks. Learning fairly with unbiased outcomes isanother significant hallmark of human intelligence, which is rarely touched infew-shot meta-learning. In this work, we propose a Primal-Dual FairMeta-learning framework, namely PDFM, which learns to train fair machinelearning models using only a few examples based on data from related tasks. Thekey idea is to learn a good initialization of a fair model's primal and dualparameters so that it can adapt to a new fair learning task via a few gradientupdate steps. Instead of manually tuning the dual parameters as hyperparametersvia a grid search, PDFM optimizes the initialization of the primal and dualparameters jointly for fair meta-learning via a subgradient primal-dualapproach. We further instantiate examples of bias controlling using meandifference and decision boundary covariance as fairness constraints to eachtask for supervised regression and classification, respectively. We demonstratethe versatility of our proposed approach by applying our approach to variousreal-world datasets. Our experiments show substantial improvements over thebest prior work for this setting."
Meta-Generating Deep Attentive Metric for Few-shot Classification,"['Lei Zhang', 'Fei Zhou', 'Wei Wei', 'Yanning Zhang']",http://arxiv.org/pdf/2012.01641v1.pdf,2020-12-03,['cs.cv'],"  Learning to generate a task-aware base learner proves a promising directionto deal with few-shot learning (FSL) problem. Existing methods mainly focus ongenerating an embedding model utilized with a fixed metric (eg, cosinedistance) for nearest neighbour classification or directly generating a linearclassier. However, due to the limited discriminative capacity of such a simplemetric or classifier, these methods fail to generalize to challenging casesappropriately. To mitigate this problem, we present a novel deep metricmeta-generation method that turns to an orthogonal direction, ie, learning toadaptively generate a specific metric for a new FSL task based on the taskdescription (eg, a few labelled samples). In this study, we structure themetric using a three-layer deep attentive network that is flexible enough toproduce a discriminative metric for each task. Moreover, different fromexisting methods that utilize an uni-modal weight distribution conditioned onlabelled samples for network generation, the proposed meta-learner establishesa multi-modal weight distribution conditioned on cross-class sample pairs usinga tailored variational autoencoder, which can separately capture the specificinter-class discrepancy statistics for each class and jointly embed thestatistics for all classes into metric generation. By doing this, the generatedmetric can be appropriately adapted to a new FSL task with pleasinggeneralization performance. To demonstrate this, we test the proposed method onfour benchmark FSL datasets and gain surprisingly obvious performanceimprovement over state-of-the-art competitors, especially in the challengingcases, eg, improve the accuracy from 26.14% to 46.69% in the 20-way 1-shot taskon miniImageNet, while improve the accuracy from 45.2% to 68.72% in the 5-way1-shot task on FC100. Code is available: https://github.com/NWPUZhoufei/DAM."
Batch Group Normalization,"['Xiao-Yun Zhou', 'Jiacheng Sun', 'Nanyang Ye', 'Xu Lan', 'Qijun Luo', 'Bo-Lin Lai', 'Pedro Esperanca', 'Guang-Zhong Yang', 'Zhenguo Li']",http://arxiv.org/pdf/2012.02782v2.pdf,2020-12-04,"['cs.lg', 'cs.cv']","  Deep Convolutional Neural Networks (DCNNs) are hard and time-consuming totrain. Normalization is one of the effective solutions. Among previousnormalization methods, Batch Normalization (BN) performs well at medium andlarge batch sizes and is with good generalizability to multiple vision tasks,while its performance degrades significantly at small batch sizes. In thispaper, we find that BN saturates at extreme large batch sizes, i.e., 128 imagesper worker, i.e., GPU, as well and propose that the degradation/saturation ofBN at small/extreme large batch sizes is caused by noisy/confused statisticcalculation. Hence without adding new trainable parameters, usingmultiple-layer or multi-iteration information, or introducing extracomputation, Batch Group Normalization (BGN) is proposed to solve thenoisy/confused statistic calculation of BN at small/extreme large batch sizeswith introducing the channel, height and width dimension to compensate. Thegroup technique in Group Normalization (GN) is used and a hyper-parameter G isused to control the number of feature instances used for statistic calculation,hence to offer neither noisy nor confused statistic for different batch sizes.We empirically demonstrate that BGN consistently outperforms BN, InstanceNormalization (IN), Layer Normalization (LN), GN, and Positional Normalization(PN), across a wide spectrum of vision tasks, including image classification,Neural Architecture Search (NAS), adversarial learning, Few Shot Learning (FSL)and Unsupervised Domain Adaptation (UDA), indicating its good performance,robust stability to batch size and wide generalizability. For example, fortraining ResNet-50 on ImageNet with a batch size of 2, BN achieves Top1accuracy of 66.512% while BGN achieves 76.096% with notable improvement."
Personalized Adaptive Meta Learning for Cold-start User Preference  Prediction,"['Runsheng Yu', 'Yu Gong', 'Xu He', 'Bo An', 'Yu Zhu', 'Qingwen Liu', 'Wenwu Ou']",http://arxiv.org/pdf/2012.11842v1.pdf,2020-12-22,"['cs.ir', 'cs.lg']","  A common challenge in personalized user preference prediction is thecold-start problem. Due to the lack of user-item interactions, directlylearning from the new users' log data causes serious over-fitting problem.Recently, many existing studies regard the cold-start personalized preferenceprediction as a few-shot learning problem, where each user is the task andrecommended items are the classes, and the gradient-based meta learning method(MAML) is leveraged to address this challenge. However, in real-worldapplication, the users are not uniformly distributed (i.e., different users mayhave different browsing history, recommended items, and user profiles. Wedefine the major users as the users in the groups with large numbers of userssharing similar user information, and other users are the minor users),existing MAML approaches tend to fit the major users and ignore the minorusers. To address this cold-start task-overfitting problem, we propose a novelpersonalized adaptive meta learning approach to consider both the major and theminor users with three key contributions: 1) We are the first to present apersonalized adaptive learning rate meta-learning approach to improve theperformance of MAML by focusing on both the major and minor users. 2) Toprovide better personalized learning rates for each user, we introduce asimilarity-based method to find similar users as a reference and a tree-basedmethod to store users' features for fast search. 3) To reduce the memory usage,we design a memory agnostic regularizer to further reduce the space complexityto constant while maintain the performance. Experiments on MovieLens,BookCrossing, and real-world production datasets reveal that our methodoutperforms the state-of-the-art methods dramatically for both the minor andmajor users."
Hyperspherical embedding for novel class classification,"['Rafael S. Pereira', 'Alexis Joly', 'Patrick Valduriez', 'Fabio Porto']",http://arxiv.org/pdf/2102.03243v2.pdf,2021-02-05,"['cs.cv', 'cs.ai', 'cs.lg, cs.ai, cs:cv']","  Deep learning models have become increasingly useful in many differentindustries. On the domain of image classification, convolutional neuralnetworks proved the ability to learn robust features for the closed setproblem, as shown in many different datasets, such as MNIST FASHIONMNIST,CIFAR10, CIFAR100, and IMAGENET. These approaches use deep neural networks withdense layers with softmax activation functions in order to learn features thatcan separate classes in a latent space. However, this traditional approach isnot useful for identifying classes unseen on the training set, known as theopen set problem. A similar problem occurs in scenarios involving learning onsmall data. To tackle both problems, few-shot learning has been proposed. Inparticular, metric learning learns features that obey constraints of a metricdistance in the latent space in order to perform classification. However, whilethis approach proves to be useful for the open set problem, currentimplementation requires pair-wise training, where both positive and negativeexamples of similar images are presented during the training phase, whichlimits the applicability of these approaches in large data or large classscenarios given the combinatorial nature of the possible inputs.In this paper,we present a constraint-based approach applied to the representations in thelatent space under the normalized softmax loss, proposed by[18]. Weexperimentally validate the proposed approach for the classification of unseenclasses on different datasets using both metric learning and the normalizedsoftmax loss, on disjoint and joint scenarios. Our results show that not onlyour proposed strategy can be efficiently trained on larger set of classes, asit does not require pairwise learning, but also present better classificationresults than the metric learning strategies surpassing its accuracy by asignificant margin."
"A Minimalist Dataset for Systematic Generalization of Perception,  Syntax, and Semantics","['Qing Li', 'Siyuan Huang', 'Yining Hong', 'Yixin Zhu', 'Ying Nian Wu', 'Song-Chun Zhu']",http://arxiv.org/pdf/2103.01403v3.pdf,2021-03-02,"['cs.lg', 'cs.ai', 'cs.cv']","  Inspired by humans' exceptional ability to master arithmetic and generalizeto new problems, we present a new dataset, Handwritten arithmetic with INTegers(HINT), to examine machines' capability of learning generalizable concepts atthree levels: perception, syntax, and semantics. In HINT, machines are taskedwith learning how concepts are perceived from raw signals such as images (i.e.,perception), how multiple concepts are structurally combined to form a validexpression (i.e., syntax), and how concepts are realized to afford variousreasoning tasks (i.e., semantics), all in a weakly supervised manner. Focusingon systematic generalization, we carefully design a five-fold test set toevaluate both the interpolation and the extrapolation of learned conceptsw.r.t. the three levels. Further, we design a few-shot learning split todetermine whether or not models can rapidly learn new concepts and generalizethem to more complex scenarios. To comprehend existing models' limitations, weundertake extensive experiments with various sequence-to-sequence models,including RNNs, Transformers, and GPT-3 (with the chain of thought prompting).The results indicate that current models struggle to extrapolate to long-rangesyntactic dependency and semantics. Models exhibit a considerable gap towardhuman-level generalization when evaluated with new concepts in a few-shotsetting. Moreover, we discover that it is infeasible to solve HINT by merelyscaling up the dataset and the model size; this strategy contributes little tothe extrapolation of syntax and semantics. Finally, in zero-shot GPT-3experiments, the chain of thought prompting exhibits impressive results andsignificantly boosts the test accuracy. We believe the HINT dataset and theexperimental findings are of great interest to the learning community onsystematic generalization."
SiT: Self-supervised vIsion Transformer,"['Sara Atito', 'Muhammad Awais', 'Josef Kittler']",http://arxiv.org/pdf/2104.03602v3.pdf,2021-04-08,"['cs.cv', 'cs.lg']","  Self-supervised learning methods are gaining increasing traction in computervision due to their recent success in reducing the gap with supervisedlearning. In natural language processing (NLP) self-supervised learning andtransformers are already the methods of choice. The recent literature suggeststhat the transformers are becoming increasingly popular also in computervision. So far, the vision transformers have been shown to work well whenpretrained either using a large scale supervised data or with some kind ofco-supervision, e.g. in terms of teacher network. These supervised pretrainedvision transformers achieve very good results in downstream tasks with minimalchanges. In this work we investigate the merits of self-supervised learning forpretraining image/vision transformers and then using them for downstreamclassification tasks. We propose Self-supervised vIsion Transformers (SiT) anddiscuss several self-supervised training mechanisms to obtain a pretext model.The architectural flexibility of SiT allows us to use it as an autoencoder andwork with multiple self-supervised tasks seamlessly. We show that a pretrainedSiT can be finetuned for a downstream classification task on small scaledatasets, consisting of a few thousand images rather than several millions. Theproposed approach is evaluated on standard datasets using common protocols. Theresults demonstrate the strength of the transformers and their suitability forself-supervised learning. We outperformed existing self-supervised learningmethods by large margin. We also observed that SiT is good for few shotlearning and also showed that it is learning useful representation by simplytraining a linear classifier on top of the learned features from SiT.Pretraining, finetuning, and evaluation codes will be available under:https://github.com/Sara-Ahmed/SiT."
Relational Learning with Gated and Attentive Neighbor Aggregator for  Few-Shot Knowledge Graph Completion,"['Guanglin Niu', 'Yang Li', 'Chengguang Tang', 'Ruiying Geng', 'Jian Dai', 'Qiao Liu', 'Hao Wang', 'Jian Sun', 'Fei Huang', 'Luo Si']",http://arxiv.org/pdf/2104.13095v2.pdf,2021-04-27,"['cs.ai', 'cs.cl']","  Aiming at expanding few-shot relations' coverage in knowledge graphs (KGs),few-shot knowledge graph completion (FKGC) has recently gained more researchinterests. Some existing models employ a few-shot relation's multi-hop neighborinformation to enhance its semantic representation. However, noise neighborinformation might be amplified when the neighborhood is excessively sparse andno neighbor is available to represent the few-shot relation. Moreover, modelingand inferring complex relations of one-to-many (1-N), many-to-one (N-1), andmany-to-many (N-N) by previous knowledge graph completion approaches requireshigh model complexity and a large amount of training instances. Thus, inferringcomplex relations in the few-shot scenario is difficult for FKGC models due tolimited training instances. In this paper, we propose a few-shot relationallearning with global-local framework to address the above issues. At the globalstage, a novel gated and attentive neighbor aggregator is built for accuratelyintegrating the semantics of a few-shot relation's neighborhood, which helpsfiltering the noise neighbors even if a KG contains extremely sparseneighborhoods. For the local stage, a meta-learning based TransH (MTransH)method is designed to model complex relations and train our model in a few-shotlearning fashion. Extensive experiments show that our model outperforms thestate-of-the-art FKGC approaches on the frequently-used benchmark datasetsNELL-One and Wiki-One. Compared with the strong baseline model MetaR, our modelachieves 5-shot FKGC performance improvements of 8.0% on NELL-One and 2.8% onWiki-One by the metric Hits@10."
Distribution Matching for Heterogeneous Multi-Task Learning: a  Large-scale Face Study,"['Dimitrios Kollias', 'Viktoriia Sharmanska', 'Stefanos Zafeiriou']",http://arxiv.org/pdf/2105.03790v1.pdf,2021-05-08,['cs.cv'],"  Multi-Task Learning has emerged as a methodology in which multiple tasks arejointly learned by a shared learning algorithm, such as a DNN. MTL is based onthe assumption that the tasks under consideration are related; therefore itexploits shared knowledge for improving performance on each individual task.Tasks are generally considered to be homogeneous, i.e., to refer to the sametype of problem. Moreover, MTL is usually based on ground truth annotationswith full, or partial overlap across tasks. In this work, we deal withheterogeneous MTL, simultaneously addressing detection, classification &regression problems. We explore task-relatedness as a means for co-training, ina weakly-supervised way, tasks that contain little, or even non-overlappingannotations. Task-relatedness is introduced in MTL, either explicitly throughprior expert knowledge, or through data-driven studies. We propose a noveldistribution matching approach, in which knowledge exchange is enabled betweentasks, via matching of their predictions' distributions. Based on thisapproach, we build FaceBehaviorNet, the first framework for large-scale faceanalysis, by jointly learning all facial behavior tasks. We develop casestudies for: i) continuous affect estimation, action unit detection, basicemotion recognition; ii) attribute detection, face identification.  We illustrate that co-training via task relatedness alleviates negativetransfer. Since FaceBehaviorNet learns features that encapsulate all aspects offacial behavior, we conduct zero-/few-shot learning to perform tasks beyond theones that it has been trained for, such as compound emotion recognition. Byconducting a very large experimental study, utilizing 10 databases, weillustrate that our approach outperforms, by large margins, thestate-of-the-art in all tasks and in all databases, even in these which havenot been used in its training."
Intriguing Properties of Vision Transformers,"['Muzammal Naseer', 'Kanchana Ranasinghe', 'Salman Khan', 'Munawar Hayat', 'Fahad Shahbaz Khan', 'Ming-Hsuan Yang']",http://arxiv.org/pdf/2105.10497v3.pdf,2021-05-21,"['cs.cv', 'cs.ai', 'cs.lg']","  Vision transformers (ViT) have demonstrated impressive performance acrossvarious machine vision problems. These models are based on multi-headself-attention mechanisms that can flexibly attend to a sequence of imagepatches to encode contextual cues. An important question is how suchflexibility in attending image-wide context conditioned on a given patch canfacilitate handling nuisances in natural images e.g., severe occlusions, domainshifts, spatial permutations, adversarial and natural perturbations. Wesystematically study this question via an extensive set of experimentsencompassing three ViT families and comparisons with a high-performingconvolutional neural network (CNN). We show and analyze the followingintriguing properties of ViT: (a) Transformers are highly robust to severeocclusions, perturbations and domain shifts, e.g., retain as high as 60% top-1accuracy on ImageNet even after randomly occluding 80% of the image content.(b) The robust performance to occlusions is not due to a bias towards localtextures, and ViTs are significantly less biased towards textures compared toCNNs. When properly trained to encode shape-based features, ViTs demonstrateshape recognition capability comparable to that of human visual system,previously unmatched in the literature. (c) Using ViTs to encode shaperepresentation leads to an interesting consequence of accurate semanticsegmentation without pixel-level supervision. (d) Off-the-shelf features from asingle ViT model can be combined to create a feature ensemble, leading to highaccuracy rates across a range of classification datasets in both traditionaland few-shot learning paradigms. We show effective features of ViTs are due toflexible and dynamic receptive fields possible via the self-attentionmechanism."
Reinforced Few-Shot Acquisition Function Learning for Bayesian  Optimization,"['Bing-Jing Hsieh', 'Ping-Chun Hsieh', 'Xi Liu']",http://arxiv.org/pdf/2106.04335v1.pdf,2021-06-08,"['cs.lg', 'cs.ai', 'stat.ml']","  Bayesian optimization (BO) conventionally relies on handcrafted acquisitionfunctions (AFs) to sequentially determine the sample points. However, it hasbeen widely observed in practice that the best-performing AF in terms of regretcan vary significantly under different types of black-box functions. It hasremained a challenge to design one AF that can attain the best performance overa wide variety of black-box functions. This paper aims to attack this challengethrough the perspective of reinforced few-shot AF learning (FSAF).Specifically, we first connect the notion of AFs with Q-functions and view adeep Q-network (DQN) as a surrogate differentiable AF. While it serves as anatural idea to combine DQN and an existing few-shot learning method, weidentify that such a direct combination does not perform well due to severeoverfitting, which is particularly critical in BO due to the need of aversatile sampling policy. To address this, we present a Bayesian variant ofDQN with the following three features: (i) It learns a distribution ofQ-networks as AFs based on the Kullback-Leibler regularization framework. Thisinherently provides the uncertainty required in sampling for BO and mitigatesoverfitting. (ii) For the prior of the Bayesian DQN, we propose to use a demopolicy induced by an off-the-shelf AF for better training stability. (iii) Onthe meta-level, we leverage the meta-loss of Bayesian model-agnosticmeta-learning, which serves as a natural companion to the proposed FSAF.Moreover, with the proper design of the Q-networks, FSAF is general-purpose inthat it is agnostic to the dimension and the cardinality of the input domain.Through extensive experiments, we demonstrate that the FSAF achieves comparableor better regrets than the state-of-the-art benchmarks on a wide variety ofsynthetic and real-world test functions."
NDPNet: A novel non-linear data projection network for few-shot  fine-grained image classification,"['Weichuan Zhang', 'Xuefang Liu', 'Zhe Xue', 'Yongsheng Gao', 'Changming Sun']",http://arxiv.org/pdf/2106.06988v3.pdf,2021-06-13,"['cs.cv', 'cs.lg']","  Metric-based few-shot fine-grained image classification (FSFGIC) aims tolearn a transferable feature embedding network by estimating the similaritiesbetween query images and support classes from very few examples. In this work,we propose, for the first time, to introduce the non-linear data projectionconcept into the design of FSFGIC architecture in order to address the limitedsample problem in few-shot learning and at the same time to increase thediscriminability of the model for fine-grained image classification.Specifically, we first design a feature re-abstraction embedding network thathas the ability to not only obtain the required semantic features for effectivemetric learning but also re-enhance such features with finer details from inputimages. Then the descriptors of the query images and the support classes areprojected into different non-linear spaces in our proposed similarity metriclearning network to learn discriminative projection factors. This design caneffectively operate in the challenging and restricted condition of a FSFGICtask for making the distance between the samples within the same class smallerand the distance between samples from different classes larger and for reducingthe coupling relationship between samples from different categories.Furthermore, a novel similarity measure based on the proposed non-linear dataproject is presented for evaluating the relationships of feature informationbetween a query image and a support set. It is worth to note that our proposedarchitecture can be easily embedded into any episodic training mechanisms forend-to-end training from scratch. Extensive experiments on FSFGIC tasksdemonstrate the superiority of the proposed methods over the state-of-the-artbenchmarks."
High-dimensional separability for one- and few-shot learning,"['Alexander N. Gorban', 'Bogdan Grechuk', 'Evgeny M. Mirkes', 'Sergey V. Stasenko', 'Ivan Y. Tyukin']",http://arxiv.org/pdf/2106.15416v2.pdf,2021-06-28,"['cs.lg', 'cs.ai', 'stat.ml']","  This work is driven by a practical question: corrections of ArtificialIntelligence (AI) errors. These corrections should be quick and non-iterative.To solve this problem without modification of a legacy AI system, we proposespecial `external' devices, correctors. Elementary correctors consist of twoparts, a classifier that separates the situations with high risk of error fromthe situations in which the legacy AI system works well and a new decision forsituations with potential errors. Input signals for the correctors can be theinputs of the legacy AI system, its internal signals, and outputs. If theintrinsic dimensionality of data is high enough then the classifiers forcorrection of small number of errors can be very simple. According to theblessing of dimensionality effects, even simple and robust Fisher'sdiscriminants can be used for one-shot learning of AI correctors. Stochasticseparation theorems provide the mathematical basis for this one-short learning.However, as the number of correctors needed grows, the cluster structure ofdata becomes important and a new family of stochastic separation theorems isrequired. We refuse the classical hypothesis of the regularity of the datadistribution and assume that the data can have a fine-grained structure withmany clusters and peaks in the probability density. New stochastic separationtheorems for data with fine-grained structure are formulated and proved. Themulti-correctors for granular data are proposed. The advantages of themulti-corrector technology were demonstrated by examples of correcting errorsand learning new classes of objects by a deep convolutional neural network onthe CIFAR-10 dataset. The key problems of the non-classical high-dimensionaldata analysis are reviewed together with the basic preprocessing stepsincluding supervised, semi-supervised and domain adaptation Principal ComponentAnalysis."
Next-item Recommendations in Short Sessions,"['Wenzhuo Song', 'Shoujin Wang', 'Yan Wang', 'Shengsheng Wang']",http://arxiv.org/pdf/2107.07453v2.pdf,2021-07-15,['cs.ir'],"  The changing preferences of users towards items trigger the emergence ofsession-based recommender systems (SBRSs), which aim to model the dynamicpreferences of users for next-item recommendations. However, most of theexisting studies on SBRSs are based on long sessions only for recommendations,ignoring short sessions, though short sessions, in fact, account for a largeproportion in most of the real-world datasets. As a result, the applicabilityof existing SBRSs solutions is greatly reduced. In a short session, quitelimited contextual information is available, making the next-itemrecommendation very challenging. To this end, in this paper, inspired by thesuccess of few-shot learning (FSL) in effectively learning a model with limitedinstances, we formulate the next-item recommendation as an FSL problem.Accordingly, following the basic idea of a representative approach for FSL,i.e., meta-learning, we devise an effective SBRS called INter-SEssioncollaborative Recommender netTwork (INSERT) for next-item recommendations inshort sessions. With the carefully devised local module and global module,INSERT is able to learn an optimal preference representation of the currentuser in a given short session. In particular, in the global module, a similarsession retrieval network (SSRN) is designed to find out the sessions similarto the current short session from the historical sessions of both the currentuser and other users, respectively. The obtained similar sessions are thenutilized to complement and optimize the preference representation learned fromthe current short session by the local module for more accurate next-itemrecommendations in this short session. Extensive experiments conducted on tworeal-world datasets demonstrate the superiority of our proposed INSERT over thestate-of-the-art SBRSs when making next-item recommendations in short sessions."
Trip-ROMA: Self-Supervised Learning with Triplets and Random Mappings,"['Wenbin Li', 'Xuesong Yang', 'Meihao Kong', 'Lei Wang', 'Jing Huo', 'Yang Gao', 'Jiebo Luo']",http://arxiv.org/pdf/2107.10419v3.pdf,2021-07-22,['cs.cv'],"  Contrastive self-supervised learning (SSL) methods, such as MoCo and SimCLR,have achieved great success in unsupervised visual representation learning.They rely on a large number of negative pairs and thus require either largememory banks or large batches. Some recent non-contrastive SSL methods, such asBYOL and SimSiam, attempt to discard negative pairs and have also shownremarkable performance. To avoid collapsed solutions caused by not usingnegative pairs, these methods require non-trivial asymmetry designs. However,in small data regimes, we can not obtain a sufficient number of negative pairsor effectively avoid the over-fitting problem when negatives are not used atall. To address this situation, we argue that negative pairs are stillimportant but one is generally sufficient for each positive pair. We show thata simple Triplet-based loss (Trip) can achieve surprisingly good performancewithout requiring large batches or asymmetry designs. Moreover, to alleviatethe over-fitting problem in small data regimes and further enhance the effectof Trip, we propose a simple plug-and-play RandOm MApping (ROMA) strategy byrandomly mapping samples into other spaces and requiring these randomlyprojected samples to satisfy the same relationship indicated by the triplets.Integrating the triplet-based loss with random mapping, we obtain the proposedmethod Trip-ROMA. Extensive experiments, including unsupervised representationlearning and unsupervised few-shot learning, have been conducted on ImageNet-1Kand seven small datasets. They successfully demonstrate the effectiveness ofTrip-ROMA and consistently show that ROMA can further effectively boost otherSSL methods. Code is available at https://github.com/WenbinLee/Trip-ROMA."
Few-shot Unsupervised Domain Adaptation with Image-to-class Sparse  Similarity Encoding,"['Shengqi Huang', 'Wanqi Yang', 'Lei Wang', 'Luping Zhou', 'Ming Yang']",http://arxiv.org/pdf/2108.02953v1.pdf,2021-08-06,"['cs.cv', 'cs.mm']","  This paper investigates a valuable setting called few-shot unsuperviseddomain adaptation (FS-UDA), which has not been sufficiently studied in theliterature. In this setting, the source domain data are labelled, but withfew-shot per category, while the target domain data are unlabelled. To addressthe FS-UDA setting, we develop a general UDA model to solve the following twokey issues: the few-shot labeled data per category and the domain adaptationbetween support and query sets. Our model is general in that once trained itwill be able to be applied to various FS-UDA tasks from the same source andtarget domains. Inspired by the recent local descriptor based few-shot learning(FSL), our general UDA model is fully built upon local descriptors (LDs) forimage classification and domain adaptation. By proposing a novel concept calledsimilarity patterns (SPs), our model not only effectively considers the spatialrelationship of LDs that was ignored in previous FSL methods, but also makesthe learned image similarity better serve the required domain alignment.Specifically, we propose a novel IMage-to-class sparse Similarity Encoding(IMSE) method. It learns SPs to extract the local discriminative informationfor classification and meanwhile aligns the covariance matrix of the SPs fordomain adaptation. Also, domain adversarial training and multi-scale localfeature matching are performed upon LDs. Extensive experiments conducted on amulti-domain benchmark dataset DomainNet demonstrates the state-of-the-artperformance of our IMSE for the novel setting of FS-UDA. In addition, for FSL,our IMSE can also show better performance than most of recent FSL methods onminiImageNet."
Program Synthesis with Large Language Models,"['Jacob Austin', 'Augustus Odena', 'Maxwell Nye', 'Maarten Bosma', 'Henryk Michalewski', 'David Dohan', 'Ellen Jiang', 'Carrie Cai', 'Michael Terry', 'Quoc Le', 'Charles Sutton']",http://arxiv.org/pdf/2108.07732v1.pdf,2021-08-16,"['cs.pl', 'cs.lg']","  This paper explores the limits of the current generation of large languagemodels for program synthesis in general purpose programming languages. Weevaluate a collection of such models (with between 244M and 137B parameters) ontwo new benchmarks, MBPP and MathQA-Python, in both the few-shot andfine-tuning regimes. Our benchmarks are designed to measure the ability ofthese models to synthesize short Python programs from natural languagedescriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974programming tasks, designed to be solvable by entry-level programmers. TheMathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914problems that evaluate the ability of the models to synthesize code from morecomplex text. On both datasets, we find that synthesis performance scaleslog-linearly with model size. Our largest models, even without finetuning on acode dataset, can synthesize solutions to 59.6 percent of the problems fromMBPP using few-shot learning with a well-designed prompt. Fine-tuning on aheld-out portion of the dataset improves performance by about 10 percentagepoints across most model sizes. On the MathQA-Python dataset, the largestfine-tuned model achieves 83.8 percent accuracy. Going further, we study themodel's ability to engage in dialog about code, incorporating human feedback toimprove its solutions. We find that natural language feedback from a humanhalves the error rate compared to the model's initial prediction. Additionally,we conduct an error analysis to shed light on where these models fall short andwhat types of programs are most difficult to generate. Finally, we explore thesemantic grounding of these models by fine-tuning them to predict the resultsof program execution. We find that even our best models are generally unable topredict the output of a program given a specific input."
Adaptive Transfer Learning: a simple but effective transfer learning,"['Jung H Lee', 'Henry J Kvinge', 'Scott Howland', 'Zachary New', 'John Buckheit', 'Lauren A. Phillips', 'Elliott Skomski', 'Jessica Hibler', 'Courtney D. Corley', 'Nathan O. Hodas']",http://arxiv.org/pdf/2111.10937v1.pdf,2021-11-22,"['cs.lg', 'cs.cv']","  Transfer learning (TL) leverages previously obtained knowledge to learn newtasks efficiently and has been used to train deep learning (DL) models withlimited amount of data. When TL is applied to DL, pretrained (teacher) modelsare fine-tuned to build domain specific (student) models. This fine-tuningrelies on the fact that DL model can be decomposed to classifiers and featureextractors, and a line of studies showed that the same feature extractors canbe used to train classifiers on multiple tasks. Furthermore, recent studiesproposed multiple algorithms that can fine-tune teacher models' featureextractors to train student models more efficiently. We note that regardless ofthe fine-tuning of feature extractors, the classifiers of student models aretrained with final outputs of feature extractors (i.e., the outputs ofpenultimate layers). However, a recent study suggested that feature maps inResNets across layers could be functionally equivalent, raising the possibilitythat feature maps inside the feature extractors can also be used to trainstudent models' classifiers. Inspired by this study, we tested if feature mapsin the hidden layers of the teacher models can be used to improve the studentmodels' accuracy (i.e., TL's efficiency). Specifically, we developed 'adaptivetransfer learning (ATL)', which can choose an optimal set of feature maps forTL, and tested it in the few-shot learning setting. Our empirical evaluationssuggest that ATL can help DL models learn more efficiently, especially whenavailable examples are limited."
Unsupervised Law Article Mining based on Deep Pre-Trained Language  Representation Models with Application to the Italian Civil Code,"['Andrea Tagarelli', 'Andrea Simeri']",http://arxiv.org/pdf/2112.03033v1.pdf,2021-12-02,"['cs.cl', 'cs.ai', 'cs.ir', 'physics.soc-ph']","  Modeling law search and retrieval as prediction problems has recently emergedas a predominant approach in law intelligence. Focusing on the law articleretrieval task, we present a deep learning framework named LamBERTa, which isdesigned for civil-law codes, and specifically trained on the Italian civilcode. To our knowledge, this is the first study proposing an advanced approachto law article prediction for the Italian legal system based on a BERT(Bidirectional Encoder Representations from Transformers) learning framework,which has recently attracted increased attention among deep learningapproaches, showing outstanding effectiveness in several natural languageprocessing and learning tasks. We define LamBERTa models by fine-tuning anItalian pre-trained BERT on the Italian civil code or its portions, for lawarticle retrieval as a classification task. One key aspect of our LamBERTaframework is that we conceived it to address an extreme classificationscenario, which is characterized by a high number of classes, the few-shotlearning problem, and the lack of test query benchmarks for Italian legalprediction tasks. To solve such issues, we define different methods for theunsupervised labeling of the law articles, which can in principle be applied toany law article code system. We provide insights into the explainability andinterpretability of our LamBERTa models, and we present an extensiveexperimental analysis over query sets of different type, for single-label aswell as multi-label evaluation tasks. Empirical evidence has shown theeffectiveness of LamBERTa, and also its superiority against widely useddeep-learning text classifiers and a few-shot learner conceived for anattribute-aware prediction task."
"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A  Large-Scale Generative Language Model","['Shaden Smith', 'Mostofa Patwary', 'Brandon Norick', 'Patrick LeGresley', 'Samyam Rajbhandari', 'Jared Casper', 'Zhun Liu', 'Shrimai Prabhumoye', 'George Zerveas', 'Vijay Korthikanti', 'Elton Zhang', 'Rewon Child', 'Reza Yazdani Aminabadi', 'Julie Bernauer', 'Xia Song', 'Mohammad Shoeybi', 'Yuxiong He', 'Michael Houston', 'Saurabh Tiwary', 'Bryan Catanzaro']",http://arxiv.org/pdf/2201.11990v3.pdf,2022-01-28,['cs.cl'],"  Pretrained general-purpose language models can achieve state-of-the-artaccuracies in various natural language processing domains by adapting todownstream tasks via zero-shot, few-shot and fine-tuning techniques. Because oftheir success, the size of these models has increased rapidly, requiringhigh-performance hardware, software, and algorithmic techniques to enabletraining such large models. As the result of a joint effort between Microsoftand NVIDIA, we present details on the training of the largest monolithictransformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530billion parameters. In this paper, we first focus on the infrastructure as wellas the 3D parallelism methodology used to train this model using DeepSpeed andMegatron. Next, we detail the training process, the design of our trainingcorpus, and our data curation techniques, which we believe is a key ingredientto the success of the model. Finally, we discuss various evaluation results, aswell as other interesting observations and new properties exhibited by MT-NLG.We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learningaccuracies on several NLP benchmarks and establishes new state-of-the-artresults. We believe that our contributions will help further the development oflarge-scale training infrastructures, large-scale language models, and naturallanguage generations."
CampNet: Context-Aware Mask Prediction for End-to-End Text-Based Speech  Editing,"['Tao Wang', 'Jiangyan Yi', 'Ruibo Fu', 'Jianhua Tao', 'Zhengqi Wen']",http://arxiv.org/pdf/2202.09950v2.pdf,2022-02-21,"['cs.sd', 'cs.cl', 'eess.as']","  The text-based speech editor allows the editing of speech through intuitivecutting, copying, and pasting operations to speed up the process of editingspeech. However, the major drawback of current systems is that edited speechoften sounds unnatural due to cut-copy-paste operation. In addition, it is notobvious how to synthesize records according to a new word not appearing in thetranscript. This paper proposes a novel end-to-end text-based speech editingmethod called context-aware mask prediction network (CampNet). The model cansimulate the text-based speech editing process by randomly masking part ofspeech and then predicting the masked region by sensing the speech context. Itcan solve unnatural prosody in the edited region and synthesize the speechcorresponding to the unseen words in the transcript. Secondly, for the possibleoperation of text-based speech editing, we design three text-based operationsbased on CampNet: deletion, insertion, and replacement. These operations cancover various situations of speech editing. Thirdly, to synthesize the speechcorresponding to long text in insertion and replacement operations, aword-level autoregressive generation method is proposed. Fourthly, we propose aspeaker adaptation method using only one sentence for CampNet and explore theability of few-shot learning based on CampNet, which provides a new idea forspeech forgery tasks. The subjective and objective experiments on VCTK andLibriTTS datasets show that the speech editing results based on CampNet arebetter than TTS technology, manual editing, and VoCo method. We also conductdetailed ablation experiments to explore the effect of the CampNet structure onits performance. Finally, the experiment shows that speaker adaptation withonly one sentence can further improve the naturalness of speech. Examples ofgenerated speech can be found at https://hairuo55.github.io/CampNet."
Data Distributional Properties Drive Emergent In-Context Learning in  Transformers,"['Stephanie C. Y. Chan', 'Adam Santoro', 'Andrew K. Lampinen', 'Jane X. Wang', 'Aaditya Singh', 'Pierre H. Richemond', 'Jay McClelland', 'Felix Hill']",http://arxiv.org/pdf/2205.05055v6.pdf,2022-04-22,"['cs.lg', 'cs.ai', 'cs.cl']","  Large transformer-based models are able to perform in-context few-shotlearning, without being explicitly trained for it. This observation raises thequestion: what aspects of the training regime lead to this emergent behavior?Here, we show that this behavior is driven by the distributions of the trainingdata itself. In-context learning emerges when the training data exhibitsparticular distributional properties such as burstiness (items appear inclusters rather than being uniformly distributed over time) and having largenumbers of rarely occurring classes. In-context learning also emerges morestrongly when item meanings or interpretations are dynamic rather than fixed.These properties are exemplified by natural language, but are also inherent tonaturalistic data in a wide range of other domains. They also departsignificantly from the uniform, i.i.d. training distributions typically usedfor standard supervised learning. In our initial experiments, we found thatin-context learning traded off against more conventional weight-based learning,and models were unable to achieve both simultaneously. However, our laterexperiments uncovered that the two modes of learning could co-exist in a singlemodel when it was trained on data following a skewed Zipfian distribution --another common property of naturalistic data, including language. In furtherexperiments, we found that naturalistic data distributions were only able toelicit in-context learning in transformers, and not in recurrent models. Insum, our findings indicate how the transformer architecture works together withparticular properties of the training data to drive the intriguing emergentin-context learning behaviour of large language models, and how future workmight encourage both in-context and in-weights learning in domains beyondlanguage."
Large Language Models are Zero-Shot Reasoners,"['Takeshi Kojima', 'Shixiang Shane Gu', 'Machel Reid', 'Yutaka Matsuo', 'Yusuke Iwasawa']",http://arxiv.org/pdf/2205.11916v4.pdf,2022-05-24,"['cs.cl', 'cs.ai', 'cs.lg']","  Pretrained large language models (LLMs) are widely used in many sub-fields ofnatural language processing (NLP) and generally known as excellent few-shotlearners with task-specific exemplars. Notably, chain of thought (CoT)prompting, a recent technique for eliciting complex multi-step reasoningthrough step-by-step answer examples, achieved the state-of-the-artperformances in arithmetics and symbolic reasoning, difficult system-2 tasksthat do not follow the standard scaling laws for LLMs. While these successesare often attributed to LLMs' ability for few-shot learning, we show that LLMsare decent zero-shot reasoners by simply adding ""Let's think step by step""before each answer. Experimental results demonstrate that our Zero-shot-CoT,using the same single prompt template, significantly outperforms zero-shot LLMperformances on diverse benchmark reasoning tasks including arithmetics(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, CoinFlip), and other logical reasoning tasks (Date Understanding, Tracking ShuffledObjects), without any hand-crafted few-shot examples, e.g. increasing theaccuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% withlarge InstructGPT model (text-davinci-002), as well as similar magnitudes ofimprovements with another off-the-shelf large model, 540B parameter PaLM. Theversatility of this single prompt across very diverse reasoning tasks hints atuntapped and understudied fundamental zero-shot capabilities of LLMs,suggesting high-level, multi-task broad cognitive capabilities may be extractedby simple prompting. We hope our work not only serves as the minimal strongestzero-shot baseline for the challenging reasoning benchmarks, but alsohighlights the importance of carefully exploring and analyzing the enormouszero-shot knowledge hidden inside LLMs before crafting finetuning datasets orfew-shot exemplars."
Lessons learned from the NeurIPS 2021 MetaDL challenge: Backbone  fine-tuning without episodic meta-learning dominates for few-shot learning  image classification,"['Adrian El Baz', 'Ihsan Ullah', 'Edesio Alcobaça', 'André C. P. L. F. Carvalho', 'Hong Chen', 'Fabio Ferreira', 'Henry Gouk', 'Chaoyu Guan', 'Isabelle Guyon', 'Timothy Hospedales', 'Shell Hu', 'Mike Huisman', 'Frank Hutter', 'Zhengying Liu', 'Felix Mohr', 'Ekrem Öztürk', 'Jan N. van Rijn', 'Haozhe Sun', 'Xin Wang', 'Wenwu Zhu']",http://arxiv.org/pdf/2206.08138v2.pdf,2022-06-15,"['cs.lg', 'cs.ai', 'cs.cv', 'cs.ne']","  Although deep neural networks are capable of achieving performance superiorto humans on various tasks, they are notorious for requiring large amounts ofdata and computing resources, restricting their success to domains where suchresources are available. Metalearning methods can address this problem bytransferring knowledge from related tasks, thus reducing the amount of data andcomputing resources needed to learn new tasks. We organize the MetaDLcompetition series, which provide opportunities for research groups all overthe world to create and experimentally assess new meta-(deep)learning solutionsfor real problems. In this paper, authored collaboratively between thecompetition organizers and the top-ranked participants, we describe the designof the competition, the datasets, the best experimental results, as well as thetop-ranked methods in the NeurIPS 2021 challenge, which attracted 15 activeteams who made it to the final phase (by outperforming the baseline), makingover 100 code submissions during the feedback phase. The solutions of the topparticipants have been open-sourced. The lessons learned include that learninggood representations is essential for effective transfer learning."
Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement  Learning,"['Yuanpei Chen', 'Tianhao Wu', 'Shengjie Wang', 'Xidong Feng', 'Jiechuang Jiang', 'Stephen Marcus McAleer', 'Yiran Geng', 'Hao Dong', 'Zongqing Lu', 'Song-Chun Zhu', 'Yaodong Yang']",http://arxiv.org/pdf/2206.08686v2.pdf,2022-06-17,"['cs.ro', 'cs.ai', 'cs.lg', 'cs.ma']","  Achieving human-level dexterity is an important open problem in robotics.However, tasks of dexterous hand manipulation, even at the baby level, arechallenging to solve through reinforcement learning (RL). The difficulty liesin the high degrees of freedom and the required cooperation among heterogeneousagents (e.g., joints of fingers). In this study, we propose the BimanualDexterous Hands Benchmark (Bi-DexHands), a simulator that involves twodexterous hands with tens of bimanual manipulation tasks and thousands oftarget objects. Specifically, tasks in Bi-DexHands are designed to matchdifferent levels of human motor skills according to cognitive scienceliterature. We built Bi-DexHands in the Issac Gym; this enables highlyefficient RL training, reaching 30,000+ FPS by only one single NVIDIA RTX 3090.We provide a comprehensive benchmark for popular RL algorithms under differentsettings; this includes Single-agent/Multi-agent RL, Offline RL, Multi-task RL,and Meta RL. Our results show that the PPO type of on-policy algorithms canmaster simple manipulation tasks that are equivalent up to 48-month humanbabies (e.g., catching a flying object, opening a bottle), while multi-agent RLcan further help to master manipulations that require skilled bimanualcooperation (e.g., lifting a pot, stacking blocks). Despite the success on eachsingle task, when it comes to acquiring multiple manipulation skills, existingRL algorithms fail to work in most of the multi-task and the few-shot learningsettings, which calls for more substantial development from the RL community.Our project is open sourced at https://github.com/PKU-MARL/DexterousHands."
"A Survey of Learning on Small Data: Generalization, Optimization, and  Challenge","['Xiaofeng Cao', 'Weixin Bu', 'Shengjun Huang', 'Minling Zhang', 'Ivor W. Tsang', 'Yew Soon Ong', 'James T. Kwok']",http://arxiv.org/pdf/2207.14443v2.pdf,2022-07-29,['cs.lg'],"  Learning on big data brings success for artificial intelligence (AI), but theannotation and training costs are expensive. In future, learning on small datathat approximates the generalization ability of big data is one of the ultimatepurposes of AI, which requires machines to recognize objectives and scenariosrelying on small data as humans. A series of learning topics is going on thisway such as active learning and few-shot learning. However, there are fewtheoretical guarantees for their generalization performance. Moreover, most oftheir settings are passive, that is, the label distribution is explicitlycontrolled by finite training resources from known distributions. This surveyfollows the agnostic active sampling theory under a PAC (Probably ApproximatelyCorrect) framework to analyze the generalization error and label complexity oflearning on small data in model-agnostic supervised and unsupervised fashion.Considering multiple learning communities could produce small datarepresentation and related topics have been well surveyed, we thus subjoinnovel geometric representation perspectives for small data: the Euclidean andnon-Euclidean (hyperbolic) mean, where the optimization solutions including theEuclidean gradients, non-Euclidean gradients, and Stein gradient are presentedand discussed. Later, multiple learning communities that may be improved bylearning on small data are summarized, which yield data-efficientrepresentations, such as transfer learning, contrastive learning, graphrepresentation learning. Meanwhile, we find that the meta-learning may provideeffective parameter update policies for learning on small data. Then, weexplore multiple challenging scenarios for small data, such as the weaksupervision and multi-label. Finally, multiple data applications that maybenefit from efficient small data representation are surveyed."
Improving Meta-Learning Generalization with Activation-Based  Early-Stopping,"['Simon Guiroy', 'Christopher Pal', 'Gonçalo Mordido', 'Sarath Chandar']",http://arxiv.org/pdf/2208.02377v1.pdf,2022-08-03,"['cs.lg', 'cs.ai', 'stat.ml']","  Meta-Learning algorithms for few-shot learning aim to train neural networkscapable of generalizing to novel tasks using only a few examples.Early-stopping is critical for performance, halting model training when itreaches optimal generalization to the new task distribution. Early-stoppingmechanisms in Meta-Learning typically rely on measuring the model performanceon labeled examples from a meta-validation set drawn from the training (source)dataset. This is problematic in few-shot transfer learning settings, where themeta-test set comes from a different target dataset (OOD) and can potentiallyhave a large distributional shift with the meta-validation set. In this work,we propose Activation Based Early-stopping (ABE), an alternative to usingvalidation-based early-stopping for meta-learning. Specifically, we analyze theevolution, during meta-training, of the neural activations at each hiddenlayer, on a small set of unlabelled support examples from a single task of thetarget tasks distribution, as this constitutes a minimal and justifiablyaccessible information from the target problem. Our experiments show thatsimple, label agnostic statistics on the activations offer an effective way toestimate how the target generalization evolves over time. At each hidden layer,we characterize the activation distributions, from their first and second ordermoments, then further summarized along the feature dimensions, resulting in acompact yet intuitive characterization in a four-dimensional space. Detectingwhen, throughout training time, and at which layer, the target activationtrajectory diverges from the activation trajectory of the source data, allowsus to perform early-stopping and improve generalization in a large array offew-shot transfer learning settings, across different algorithms, source andtarget datasets."
Generative Transfer Learning: Covid-19 Classification with a few Chest  X-ray Images,"['Suvarna Kadam', 'Vinay G. Vaidya']",http://arxiv.org/pdf/2208.05305v1.pdf,2022-08-10,"['eess.iv', 'cs.cv', 'cs.lg']","  Detection of diseases through medical imaging is preferred due to itsnon-invasive nature. Medical imaging supports multiple modalities of data thatenable a thorough and quick look inside a human body. However, interpretingimaging data is often time-consuming and requires a great deal of humanexpertise. Deep learning models can expedite interpretation and alleviate thework of human experts. However, these models are data-intensive and requiresignificant labeled images for training. During novel disease outbreaks such asCovid-19, we often do not have the required labeled imaging data, especially atthe start of the epidemic. Deep Transfer Learning addresses this problem byusing a pretrained model in the public domain, e.g. any variant of eitherVGGNet, ResNet, Inception, DenseNet, etc., as a feature learner to quicklyadapt the target task from fewer samples. Most pretrained models are deep withcomplex architectures. They are trained with large multi-class datasets such asImageNet, with significant human efforts in architecture design and hyperparameters tuning. We presented 1 a simpler generative source model, pretrainedon a single but related concept, can perform as effectively as existing largerpretrained models. We demonstrate the usefulness of generative transferlearning that requires less compute and training data, for Few Shot Learning(FSL) with a Covid-19 binary classification use case. We compare classic deeptransfer learning with our approach and also report FSL results with threesettings of 84, 20, and 10 training samples. The model implementation ofgenerative FSL for Covid-19 classification is available publicly athttps://github.com/suvarnak/GenerativeFSLCovid.git."
Open Long-Tailed Recognition in a Dynamic World,"['Ziwei Liu', 'Zhongqi Miao', 'Xiaohang Zhan', 'Jiayun Wang', 'Boqing Gong', 'Stella X. Yu']",http://arxiv.org/pdf/2208.08349v1.pdf,2022-08-17,"['cs.cv', 'cs.lg']","  Real world data often exhibits a long-tailed and open-ended (with unseenclasses) distribution. A practical recognition system must balance betweenmajority (head) and minority (tail) classes, generalize across thedistribution, and acknowledge novelty upon the instances of unseen classes(open classes). We define Open Long-Tailed Recognition++ (OLTR++) as learningfrom such naturally distributed data and optimizing for the classificationaccuracy over a balanced test set which includes both known and open classes.OLTR++ handles imbalanced classification, few-shot learning, open-setrecognition, and active learning in one integrated algorithm, whereas existingclassification approaches often focus only on one or two aspects and deliverpoorly over the entire spectrum. The key challenges are: 1) how to share visualknowledge between head and tail classes, 2) how to reduce confusion betweentail and open classes, and 3) how to actively explore open classes with learnedknowledge. Our algorithm, OLTR++, maps images to a feature space such thatvisual concepts can relate to each other through a memory association mechanismand a learned metric (dynamic meta-embedding) that both respects the closedworld classification of seen classes and acknowledges the novelty of openclasses. Additionally, we propose an active learning scheme based on visualmemory, which learns to recognize open classes in a data-efficient manner forfuture expansions. On three large-scale open long-tailed datasets we curatedfrom ImageNet (object-centric), Places (scene-centric), and MS1M (face-centric)data, as well as three standard benchmarks (CIFAR-10-LT, CIFAR-100-LT, andiNaturalist-18), our approach, as a unified framework, consistentlydemonstrates competitive performance. Notably, our approach also shows strongpotential for the active exploration of open classes and the fairness analysisof minority groups."
Prototypical few-shot segmentation for cross-institution male pelvic  structures with spatial registration,"['Yiwen Li', 'Yunguan Fu', 'Iani Gayo', 'Qianye Yang', 'Zhe Min', 'Shaheer Saeed', 'Wen Yan', 'Yipei Wang', 'J. Alison Noble', 'Mark Emberton', 'Matthew J. Clarkson', 'Henkjan Huisman', 'Dean Barratt', 'Victor Adrian Prisacariu', 'Yipeng Hu']",http://arxiv.org/pdf/2209.05160v3.pdf,2022-09-12,"['eess.iv', 'cs.cv']","  The prowess that makes few-shot learning desirable in medical image analysisis the efficient use of the support image data, which are labelled to classifyor segment new classes, a task that otherwise requires substantially moretraining images and expert annotations. This work describes a fully 3Dprototypical few-shot segmentation algorithm, such that the trained networkscan be effectively adapted to clinically interesting structures that are absentin training, using only a few labelled images from a different institute.First, to compensate for the widely recognised spatial variability betweeninstitutions in episodic adaptation of novel classes, a novel spatialregistration mechanism is integrated into prototypical learning, consisting ofa segmentation head and an spatial alignment module. Second, to assist thetraining with observed imperfect alignment, support mask conditioning module isproposed to further utilise the annotation available from the support images.Extensive experiments are presented in an application of segmenting eightanatomical structures important for interventional planning, using a data setof 589 pelvic T2-weighted MR images, acquired at seven institutes. The resultsdemonstrate the efficacy in each of the 3D formulation, the spatialregistration, and the support mask conditioning, all of which made positivecontributions independently or collectively. Compared with the previouslyproposed 2D alternatives, the few-shot segmentation performance was improvedwith statistical significance, regardless whether the support data come fromthe same or different institutes."
Enabling ISP-less Low-Power Computer Vision,"['Gourav Datta', 'Zeyu Liu', 'Zihan Yin', 'Linyu Sun', 'Akhilesh R. Jaiswal', 'Peter A. Beerel']",http://arxiv.org/pdf/2210.05451v1.pdf,2022-10-11,"['cs.cv', 'eess.iv']","  In order to deploy current computer vision (CV) models onresource-constrained low-power devices, recent works have proposed in-sensorand in-pixel computing approaches that try to partly/fully bypass the imagesignal processor (ISP) and yield significant bandwidth reduction between theimage sensor and the CV processing unit by downsampling the activation maps inthe initial convolutional neural network (CNN) layers. However, directinference on the raw images degrades the test accuracy due to the difference incovariance of the raw images captured by the image sensors compared to theISP-processed images used for training. Moreover, it is difficult to train deepCV models on raw images, because most (if not all) large-scale open-sourcedatasets consist of RGB images. To mitigate this concern, we propose to invertthe ISP pipeline, which can convert the RGB images of any dataset to its rawcounterparts, and enable model training on raw images. We release the rawversion of the COCO dataset, a large-scale benchmark for generic high-levelvision tasks. For ISP-less CV systems, training on these raw images result in a7.1% increase in test accuracy on the visual wake works (VWW) dataset comparedto relying on training with traditional ISP-processed RGB datasets. To furtherimprove the accuracy of ISP-less CV models and to increase the energy andbandwidth benefits obtained by in-sensor/in-pixel computing, we propose anenergy-efficient form of analog in-pixel demosaicing that may be coupled within-pixel CNN computations. When evaluated on raw images captured by realsensors from the PASCALRAW dataset, our approach results in a 8.1% increase inmAP. Lastly, we demonstrate a further 20.5% increase in mAP by using a novelapplication of few-shot learning with thirty shots each for the novel PASCALRAWdataset, constituting 3 classes."
Enhancing Few-shot Image Classification with Cosine Transformer,"['Quang-Huy Nguyen', 'Cuong Q. Nguyen', 'Dung D. Le', 'Hieu H. Pham']",http://arxiv.org/pdf/2211.06828v3.pdf,2022-11-13,['cs.cv'],"  This paper addresses the few-shot image classification problem, where theclassification task is performed on unlabeled query samples given a smallamount of labeled support samples only. One major challenge of the few-shotlearning problem is the large variety of object visual appearances thatprevents the support samples to represent that object comprehensively. Thismight result in a significant difference between support and query samples,therefore undermining the performance of few-shot algorithms. In this paper, wetackle the problem by proposing Few-shot Cosine Transformer (FS-CT), where therelational map between supports and queries is effectively obtained for thefew-shot tasks. The FS-CT consists of two parts, a learnable prototypicalembedding network to obtain categorical representations from support sampleswith hard cases, and a transformer encoder to effectively achieve therelational map from two different support and query samples. We introduceCosine Attention, a more robust and stable attention module that enhances thetransformer module significantly and therefore improves FS-CT performance from5% to over 20% in accuracy compared to the default scaled dot-productmechanism. Our method performs competitive results in mini-ImageNet, CUB-200,and CIFAR-FS on 1-shot learning and 5-shot learning tasks across backbones andfew-shot configurations. We also developed a custom few-shot dataset for Yogapose recognition to demonstrate the potential of our algorithm for practicalapplication. Our FS-CT with cosine attention is a lightweight, simple few-shotalgorithm that can be applied for a wide range of applications, such ashealthcare, medical, and security surveillance. The official implementationcode of our Few-shot Cosine Transformer is available athttps://github.com/vinuni-vishc/Few-Shot-Cosine-Transformer"
Karyotype AI for Precision Oncology,"['Zahra Shamsi', 'Drew Bryant', 'Jacob Wilson', 'Xiaoyu Qu', 'Avinava Dubey', 'Konik Kothari', 'Mostafa Dehghani', 'Mariya Chavarha', 'Valerii Likhosherstov', 'Brian Williams', 'Michael Frumkin', 'Fred Appelbaum', 'Krzysztof Choromanski', 'Ali Bashir', 'Min Fang']",http://arxiv.org/pdf/2211.14312v3.pdf,2022-11-20,"['q-bio.qm', 'cs.cv', 'cs.lg', 'eess.iv']","  Chromosome analysis is essential for diagnosing genetic disorders. Forhematologic malignancies, identification of somatic clonal aberrations bykaryotype analysis remains the standard of care. However, karyotyping is costlyand time-consuming because of the largely manual process and the expertiserequired in identifying and annotating aberrations. Efforts to automatekaryotype analysis to date fell short in aberration detection. Using a trainingset of ~10k patient specimens and ~50k karyograms from over 5 years from theFred Hutchinson Cancer Center, we created a labeled set of images representingindividual chromosomes. These individual chromosomes were used to train andassess deep learning models for classifying the 24 human chromosomes andidentifying chromosomal aberrations. The top-accuracy models utilized therecently introduced Topological Vision Transformers (TopViTs) with2-level-block-Toeplitz masking, to incorporate structural inductive bias.TopViT outperformed CNN (Inception) models with >99.3% accuracy for chromosomeidentification, and exhibited accuracies >99% for aberration detection in mostaberrations. Notably, we were able to show high-quality performance even in""few shot"" learning scenarios. Incorporating the definition of clonalitysubstantially improved both precision and recall (sensitivity). When applied to""zero shot"" scenarios, the model captured aberrations without training, withperfect precision at >50% recall. Together these results show that modern deeplearning models can approach expert-level performance for chromosome aberrationdetection. To our knowledge, this is the first study demonstrating thedownstream effectiveness of TopViTs. These results open up excitingopportunities for not only expediting patient results but providing a scalabletechnology for early screening of low-abundance chromosomal lesions."
Hungry Hungry Hippos: Towards Language Modeling with State Space Models,"['Daniel Y. Fu', 'Tri Dao', 'Khaled K. Saab', 'Armin W. Thomas', 'Atri Rudra', 'Christopher Ré']",http://arxiv.org/pdf/2212.14052v3.pdf,2022-12-28,"['cs.lg', 'cs.cl']","  State space models (SSMs) have demonstrated state-of-the-art sequencemodeling performance in some modalities, but underperform attention in languagemodeling. Moreover, despite scaling nearly linearly in sequence length insteadof quadratically, SSMs are still slower than Transformers due to poor hardwareutilization. In this paper, we make progress on understanding the expressivitygap between SSMs and attention in language modeling, and on reducing thehardware barrier between SSMs and attention. First, we use synthetic languagemodeling tasks to understand the gap between SSMs and attention. We find thatexisting SSMs struggle with two capabilities: recalling earlier tokens in thesequence and comparing tokens across the sequence. To understand the impact onlanguage modeling, we propose a new SSM layer, H3, that is explicitly designedfor these abilities. H3 matches attention on the synthetic languages and comeswithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid125M-parameter H3-attention model that retains two attention layerssurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, toimprove the efficiency of training SSMs on modern hardware, we proposeFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency onsequences up to 8K, and introduces a novel state passing algorithm thatexploits the recurrent properties of SSMs to scale to longer sequences.FlashConv yields 2$\times$ speedup on the long-range arena benchmark and allowshybrid language models to generate text 2.4$\times$ faster than Transformers.Using FlashConv, we scale hybrid H3-attention language models up to 2.7Bparameters on the Pile and find promising initial results, achieving lowerperplexity than Transformers and outperforming Transformers in zero- andfew-shot learning on a majority of tasks in the SuperGLUE benchmark."
CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,"['Runnan Chen', 'Youquan Liu', 'Lingdong Kong', 'Xinge Zhu', 'Yuexin Ma', 'Yikang Li', 'Yuenan Hou', 'Yu Qiao', 'Wenping Wang']",http://arxiv.org/pdf/2301.04926v2.pdf,2023-01-12,['cs.cv'],"  Contrastive Language-Image Pre-training (CLIP) achieves promising results in2D zero-shot and few-shot learning. Despite the impressive performance in 2D,applying CLIP to help the learning in 3D scene understanding has yet to beexplored. In this paper, we make the first attempt to investigate how CLIPknowledge benefits 3D scene understanding. We propose CLIP2Scene, a simple yeteffective framework that transfers CLIP knowledge from 2D image-textpre-trained models to a 3D point cloud network. We show that the pre-trained 3Dnetwork yields impressive performance on various downstream tasks, i.e.,annotation-free and fine-tuning with labelled data for semantic segmentation.Specifically, built upon CLIP, we design a Semantic-driven Cross-modalContrastive Learning framework that pre-trains a 3D network via semantic andspatial-temporal consistency regularization. For the former, we first leverageCLIP's text semantics to select the positive and negative point samples andthen employ the contrastive loss to train the 3D network. In terms of thelatter, we force the consistency between the temporally coherent point cloudfeatures and their corresponding image features. We conduct experiments onSemanticKITTI, nuScenes, and ScanNet. For the first time, our pre-trainednetwork achieves annotation-free 3D semantic segmentation with 20.8% and 25.08%mIoU on nuScenes and ScanNet, respectively. When fine-tuned with 1% or 100%labelled data, our method significantly outperforms other self-supervisedmethods, with improvements of 8% and 1% mIoU, respectively. Furthermore, wedemonstrate the generalizability for handling cross-domain datasets. Code ispublicly available https://github.com/runnanchen/CLIP2Scene."
AI of Brain and Cognitive Sciences: From the Perspective of First  Principles,"['Luyao Chen', 'Zhiqiang Chen', 'Longsheng Jiang', 'Xiang Liu', 'Linlu Xu', 'Bo Zhang', 'Xiaolong Zou', 'Jinying Gao', 'Yu Zhu', 'Xizi Gong', 'Shan Yu', 'Sen Song', 'Liangyi Chen', 'Fang Fang', 'Si Wu', 'Jia Liu']",http://arxiv.org/pdf/2301.08382v1.pdf,2023-01-20,['q-bio.nc'],"  Nowadays, we have witnessed the great success of AI in various applications,including image classification, game playing, protein structure analysis,language translation, and content generation. Despite these powerfulapplications, there are still many tasks in our daily life that are rathersimple to humans but pose great challenges to AI. These include image andlanguage understanding, few-shot learning, abstract concepts, and low-energycost computing. Thus, learning from the brain is still a promising way that canshed light on the development of next-generation AI. The brain is arguably theonly known intelligent machine in the universe, which is the product ofevolution for animals surviving in the natural environment. At the behaviorlevel, psychology and cognitive sciences have demonstrated that human andanimal brains can execute very intelligent high-level cognitive functions. Atthe structure level, cognitive and computational neurosciences have unveiledthat the brain has extremely complicated but elegant network forms to supportits functions. Over years, people are gathering knowledge about the structureand functions of the brain, and this process is accelerating recently alongwith the initiation of giant brain projects worldwide. Here, we argue that thegeneral principles of brain functions are the most valuable things to inspirethe development of AI. These general principles are the standard rules of thebrain extracting, representing, manipulating, and retrieving information, andhere we call them the first principles of the brain. This paper collects sixsuch first principles. They are attractor network, criticality, random network,sparse coding, relational memory, and perceptual learning. On each topic, wereview its biological background, fundamental property, potential applicationto AI, and future development."
An Empirical Evaluation of Using Large Language Models for Automated  Unit Test Generation,"['Max Schäfer', 'Sarah Nadi', 'Aryaz Eghbali', 'Frank Tip']",http://arxiv.org/pdf/2302.06527v4.pdf,2023-02-13,"['cs.se', 'cs.ai']","  Unit tests play a key role in ensuring the correctness of software. However,manually creating unit tests is a laborious task, motivating the need forautomation. Large Language Models (LLMs) have recently been applied to thisproblem, utilizing additional training or few-shot learning on examples ofexisting tests. This paper presents a large-scale empirical evaluation on theeffectiveness of LLMs for automated unit test generation without additionaltraining or manual effort, providing the LLM with the signature andimplementation of the function under test, along with usage examples extractedfrom documentation. We also attempt to repair failed generated tests byre-prompting the model with the failing test and error message. We implementour approach in TestPilot, a test generation tool for JavaScript thatautomatically generates unit tests for all API functions in an npm package. Weevaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with atotal of 1,684 API functions. The generated tests achieve a median statementcoverage of 70.2% and branch coverage of 52.8%, significantly improving onNessie, a recent feedback-directed JavaScript test generation technique, whichachieves only 51.3% statement coverage and 25.6% branch coverage. We also findthat 92.8% of TestPilot's generated tests have no more than 50% similarity withexisting tests (as measured by normalized edit distance), with none of thembeing exact copies. Finally, we run TestPilot with two additional LLMs,OpenAI's older code-cushman-002 LLM and the open LLM StarCoder. Overall, weobserved similar results with the former (68.2% median statement coverage), andsomewhat worse results with the latter (54.0% median statement coverage),suggesting that the effectiveness of the approach is influenced by the size andtraining set of the LLM, but does not fundamentally depend on the specificmodel."
One Fits All:Power General Time Series Analysis by Pretrained LM,"['Tian Zhou', 'PeiSong Niu', 'Xue Wang', 'Liang Sun', 'Rong Jin']",http://arxiv.org/pdf/2302.11939v6.pdf,2023-02-23,"['cs.lg', 'cs.ai']","  Although we have witnessed great success of pre-trained models in naturallanguage processing (NLP) and computer vision (CV), limited progress has beenmade for general time series analysis. Unlike NLP and CV where a unified modelcan be used to perform different tasks, specially designed approach stilldominates in each time series analysis task such as classification, anomalydetection, forecasting, and few-shot learning. The main challenge that blocksthe development of pre-trained model for time series analysis is the lack of alarge amount of data for training. In this work, we address this challenge byleveraging language or CV models, pre-trained from billions of tokens, for timeseries analysis. Specifically, we refrain from altering the self-attention andfeedforward layers of the residual blocks in the pre-trained language or imagemodel. This model, known as the Frozen Pretrained Transformer (FPT), isevaluated through fine-tuning on all major types of tasks involving timeseries. Our results demonstrate that pre-trained models on natural language orimages can lead to a comparable or state-of-the-art performance in all maintime series analysis tasks, as illustrated in Figure 1. We also found boththeoretically and empirically that the self-attention module behaviorssimilarly to principle component analysis (PCA), an observation that helpsexplains how transformer bridges the domain gap and a crucial step towardsunderstanding the universality of a pre-trained transformer.The code ispublicly available at https://github.com/DAMO-DI-ML/One_Fits_All."
SiMWiSense: Simultaneous Multi-Subject Activity Classification Through  Wi-Fi Signals,"['Khandaker Foysal Haque', 'Milin Zhang', 'Francesco Restuccia']",http://arxiv.org/pdf/2304.00057v1.pdf,2023-03-31,['cs.ni'],"  Recent advances in Wi-Fi sensing have ushered in a plethora of pervasiveapplications in home surveillance, remote healthcare, road safety, and homeentertainment, among others. Most of the existing works are limited to theactivity classification of a single human subject at a given time. Conversely,a more realistic scenario is to achieve simultaneous, multi-subject activityclassification. The first key challenge in that context is that the number ofclasses grows exponentially with the number of subjects and activities.Moreover, it is known that Wi-Fi sensing systems struggle to adapt to newenvironments and subjects. To address both issues, we propose SiMWiSense, thefirst framework for simultaneous multi-subject activity classification based onWi-Fi that generalizes to multiple environments and subjects. We address thescalability issue by using the Channel State Information (CSI) computed fromthe device positioned closest to the subject. We experimentally prove thisintuition by confirming that the best accuracy is experienced when the CSIcomputed by the transceiver positioned closest to the subject is used forclassification. To address the generalization issue, we develop a brand-newfew-shot learning algorithm named Feature Reusable Embedding Learning (FREL).Through an extensive data collection campaign in 3 different environments and 3subjects performing 20 different activities simultaneously, we demonstrate thatSiMWiSense achieves classification accuracy of up to 97%, while FREL improvesthe accuracy by 85% in comparison to a traditional Convolutional Neural Network(CNN) and up to 20% when compared to the state-of-the-art few-shot embeddinglearning (FSEL), by using only 15 seconds of additional data for each class.For reproducibility purposes, we share our 1TB dataset and code repository."
On the Opportunities and Challenges of Foundation Models for Geospatial  Artificial Intelligence,"['Gengchen Mai', 'Weiming Huang', 'Jin Sun', 'Suhang Song', 'Deepak Mishra', 'Ninghao Liu', 'Song Gao', 'Tianming Liu', 'Gao Cong', 'Yingjie Hu', 'Chris Cundy', 'Ziyuan Li', 'Rui Zhu', 'Ni Lao']",http://arxiv.org/pdf/2304.06798v1.pdf,2023-04-13,"['cs.ai', 'cs.cl', 'cs.cv', 'i.2.0; i.2.4; i.2.7; i.2.10; i.5.1']","  Large pre-trained models, also known as foundation models (FMs), are trainedin a task-agnostic manner on large-scale data and can be adapted to a widerange of downstream tasks by fine-tuning, few-shot, or even zero-shot learning.Despite their successes in language and vision tasks, we have yet seen anattempt to develop foundation models for geospatial artificial intelligence(GeoAI). In this work, we explore the promises and challenges of developingmultimodal foundation models for GeoAI. We first investigate the potential ofmany existing FMs by testing their performances on seven tasks across multiplegeospatial subdomains including Geospatial Semantics, Health Geography, UrbanGeography, and Remote Sensing. Our results indicate that on several geospatialtasks that only involve text modality such as toponym recognition, locationdescription recognition, and US state-level/county-level dementia time seriesforecasting, these task-agnostic LLMs can outperform task-specificfully-supervised models in a zero-shot or few-shot learning setting. However,on other geospatial tasks, especially tasks that involve multiple datamodalities (e.g., POI-based urban function classification, street viewimage-based urban noise intensity classification, and remote sensing imagescene classification), existing foundation models still underperformtask-specific models. Based on these observations, we propose that one of themajor challenges of developing a FM for GeoAI is to address the multimodalitynature of geospatial tasks. After discussing the distinct challenges of eachgeospatial data modality, we suggest the possibility of a multimodal foundationmodel which can reason over various types of geospatial data through geospatialalignments. We conclude this paper by discussing the unique risks andchallenges to develop such a model for GeoAI."
Semantic-Aware Graph Matching Mechanism for Multi-Label Image  Recognition,"['Yanan Wu', 'Songhe Feng', 'Yang Wang']",http://arxiv.org/pdf/2304.11275v1.pdf,2023-04-21,['cs.cv'],"  Multi-label image recognition aims to predict a set of labels that present inan image. The key to deal with such problem is to mine the associations betweenimage contents and labels, and further obtain the correct assignments betweenimages and their labels. In this paper, we treat each image as a bag ofinstances, and formulate the task of multi-label image recognition as aninstance-label matching selection problem. To model such problem, we propose aninnovative Semantic-aware Graph Matching framework for Multi-Label imagerecognition (ML-SGM), in which Graph Matching mechanism is introduced owing toits good performance of excavating the instance and label relationship. Theframework explicitly establishes category correlations and instance-labelcorrespondences by modeling the relation among content-aware (instance) andsemantic-aware (label) category representations, to facilitate multi-labelimage understanding and reduce the dependency of large amounts of trainingsamples for each category. Specifically, we first construct an instance spatialgraph and a label semantic graph respectively and then incorporate them into aconstructed assignment graph by connecting each instance to all labels.Subsequently, the graph network block is adopted to aggregate and update allnodes and edges state on the assignment graph to form structuredrepresentations for each instance and label. Our network finally derives aprediction score for each instance-label correspondence and optimizes suchcorrespondence with a weighted cross-entropy loss. Empirical results conductedon generic multi-label image recognition demonstrate the superiority of ourproposed method. Moreover, the proposed method also shows advantages inmulti-label recognition with partial labels and multi-label few-shot learning,as well as outperforms current state-of-the-art methods with a clear margin."
Towards Addressing Training Data Scarcity Challenge in Emerging Radio  Access Networks: A Survey and Framework,"['Haneya Naeem Qureshi', 'Usama Masood', 'Marvin Manalastas', 'Syed Muhammad Asad Zaidi', 'Hasan Farooq', 'Julien Forgeat', 'Maxime Bouton', 'Shruti Bothe', 'Per Karlsson', 'Ali Rizwan', 'Ali Imran']",http://arxiv.org/pdf/2304.12480v1.pdf,2023-04-24,"['eess.sy', 'cs.sy']","  The future of cellular networks is contingent on artificial intelligence (AI)based automation, particularly for radio access network (RAN) operation,optimization, and troubleshooting. To achieve such zero-touch automation, amyriad of AI-based solutions are being proposed in literature for modeling andoptimizing network behavior to achieve the zero-touch automation goal. However,to work reliably, AI based automation, requires a deluge of training data.Consequently, the success of AI solutions is limited by a fundamental challengefaced by cellular network research community: scarcity of training data. Wepresent an extensive review of classic and emerging techniques to address thischallenge. We first identify the common data types in RAN and their knownuse-cases. We then present a taxonomized survey of techniques to addresstraining data scarcity for various data types. This is followed by a frameworkto address the training data scarcity. The framework builds on availableinformation and combination of techniques including interpolation,domain-knowledge based, generative adversarial neural networks, transferlearning, autoencoders, few-shot learning, simulators, and testbeds. Potentialnew techniques to enrich scarce data in cellular networks are also proposed,such as by matrix completion theory, and domain knowledge-based techniquesleveraging different network parameters and geometries. An overview ofstate-of-the art simulators and testbeds is also presented to make readersaware of current and emerging platforms for real data access. The extensivesurvey of training data scarcity addressing techniques combined with proposedframework to select a suitable technique for given type of data, can assistresearchers and network operators in choosing appropriate methods to overcomethe data scarcity challenge in leveraging AI to radio access networkautomation."
Learning to detect an animal sound from five examples,"['Inês Nolasco', 'Shubhr Singh', 'Veronica Morfi', 'Vincent Lostanlen', 'Ariana Strandburg-Peshkin', 'Ester Vidaña-Vila', 'Lisa Gill', 'Hanna Pamuła', 'Helen Whitehead', 'Ivan Kiskin', 'Frants H. Jensen', 'Joe Morford', 'Michael G. Emmerson', 'Elisabetta Versace', 'Emily Grout', 'Haohe Liu', 'Dan Stowell']",http://arxiv.org/pdf/2305.13210v1.pdf,2023-05-22,"['cs.sd', 'eess.as', 'q-bio.qm']","  Automatic detection and classification of animal sounds has many applicationsin biodiversity monitoring and animal behaviour. In the past twenty years, thevolume of digitised wildlife sound available has massively increased, andautomatic classification through deep learning now shows strong results.However, bioacoustics is not a single task but a vast range of small-scaletasks (such as individual ID, call type, emotional indication) with widevariety in data characteristics, and most bioacoustic tasks do not come withstrongly-labelled training data. The standard paradigm of supervised learning,focussed on a single large-scale dataset and/or a generic pre-trainedalgorithm, is insufficient. In this work we recast bioacoustic sound eventdetection within the AI framework of few-shot learning. We adapt this frameworkto sound event detection, such that a system can be given the annotatedstart/end times of as few as 5 events, and can then detect events inlong-duration audio -- even when the sound category was not known at the timeof algorithm training. We introduce a collection of open datasets designed tostrongly test a system's ability to perform few-shot sound event detections,and we present the results of a public contest to address the task. We showthat prototypical networks are a strong-performing method, when enhanced withadaptations for general characteristics of animal sounds. We demonstrate thatwidely-varying sound event durations are an important factor in performance, aswell as non-stationarity, i.e. gradual changes in conditions throughout theduration of a recording. For fine-grained bioacoustic recognition tasks withoutmassive annotated training data, our results demonstrate that few-shot soundevent detection is a powerful new method, strongly outperforming traditionalsignal-processing detection methods in the fully automated scenario."
Prototype Adaption and Projection for Few- and Zero-shot 3D Point Cloud  Semantic Segmentation,"['Shuting He', 'Xudong Jiang', 'Wei Jiang', 'Henghui Ding']",http://arxiv.org/pdf/2305.14335v1.pdf,2023-05-23,['cs.cv'],"  In this work, we address the challenging task of few-shot and zero-shot 3Dpoint cloud semantic segmentation. The success of few-shot semanticsegmentation in 2D computer vision is mainly driven by the pre-training onlarge-scale datasets like imagenet. The feature extractor pre-trained onlarge-scale 2D datasets greatly helps the 2D few-shot learning. However, thedevelopment of 3D deep learning is hindered by the limited volume and instancemodality of datasets due to the significant cost of 3D data collection andannotation. This results in less representative features and large intra-classfeature variation for few-shot 3D point cloud segmentation. As a consequence,directly extending existing popular prototypical methods of 2D few-shotclassification/segmentation into 3D point cloud segmentation won't work as wellas in 2D domain. To address this issue, we propose a Query-Guided PrototypeAdaption (QGPA) module to adapt the prototype from support point clouds featurespace to query point clouds feature space. With such prototype adaption, wegreatly alleviate the issue of large feature intra-class variation in pointcloud and significantly improve the performance of few-shot 3D segmentation.Besides, to enhance the representation of prototypes, we introduce aSelf-Reconstruction (SR) module that enables prototype to reconstruct thesupport mask as well as possible. Moreover, we further consider zero-shot 3Dpoint cloud semantic segmentation where there is no support sample. To thisend, we introduce category words as semantic information and propose asemantic-visual projection model to bridge the semantic and visual spaces. Ourproposed method surpasses state-of-the-art algorithms by a considerable 7.90%and 14.82% under the 2-way 1-shot setting on S3DIS and ScanNet benchmarks,respectively. Code is available at https://github.com/heshuting555/PAP-FZS3D."
The Rise of AI Language Pathologists: Exploring Two-level Prompt  Learning for Few-shot Weakly-supervised Whole Slide Image Classification,"['Linhao Qu', 'Xiaoyuan Luo', 'Kexue Fu', 'Manning Wang', 'Zhijian Song']",http://arxiv.org/pdf/2305.17891v1.pdf,2023-05-29,['cs.cv'],"  This paper introduces the novel concept of few-shot weakly supervisedlearning for pathology Whole Slide Image (WSI) classification, denoted as FSWC.A solution is proposed based on prompt learning and the utilization of a largelanguage model, GPT-4. Since a WSI is too large and needs to be divided intopatches for processing, WSI classification is commonly approached as a MultipleInstance Learning (MIL) problem. In this context, each WSI is considered a bag,and the obtained patches are treated as instances. The objective of FSWC is toclassify both bags and instances with only a limited number of labeled bags.Unlike conventional few-shot learning problems, FSWC poses additionalchallenges due to its weak bag labels within the MIL framework. Drawinginspiration from the recent achievements of vision-language models (V-L models)in downstream few-shot classification tasks, we propose a two-level promptlearning MIL framework tailored for pathology, incorporating language priorknowledge. Specifically, we leverage CLIP to extract instance features for eachpatch, and introduce a prompt-guided pooling strategy to aggregate theseinstance features into a bag feature. Subsequently, we employ a small number oflabeled bags to facilitate few-shot prompt learning based on the bag features.Our approach incorporates the utilization of GPT-4 in a question-and-answermode to obtain language prior knowledge at both the instance and bag levels,which are then integrated into the instance and bag level language prompts.Additionally, a learnable component of the language prompts is trained usingthe available few-shot labeled data. We conduct extensive experiments on threereal WSI datasets encompassing breast cancer, lung cancer, and cervical cancer,demonstrating the notable performance of the proposed method in bag andinstance classification. All codes will be made publicly accessible."
A metric learning approach for endoscopic kidney stone identification,"['Jorge Gonzalez-Zapata', 'Francisco Lopez-Tiro', 'Elias Villalvazo-Avila', 'Daniel Flores-Araiza', 'Jacques Hubert', 'Andres Mendez-Vazquez', 'Gilberto Ochoa-Ruiz', 'Christian Daul']",http://arxiv.org/pdf/2307.07046v1.pdf,2023-07-13,"['cs.cv', 'cs.ai']","  Several Deep Learning (DL) methods have recently been proposed for anautomated identification of kidney stones during an ureteroscopy to enablerapid therapeutic decisions. Even if these DL approaches led to promisingresults, they are mainly appropriate for kidney stone types for which numerouslabelled data are available. However, only few labelled images are availablefor some rare kidney stone types. This contribution exploits Deep MetricLearning (DML) methods i) to handle such classes with few samples, ii) togeneralize well to out of distribution samples, and iii) to cope better withnew classes which are added to the database. The proposed Guided Deep MetricLearning approach is based on a novel architecture which was designed to learndata representations in an improved way. The solution was inspired by Few-ShotLearning (FSL) and makes use of a teacher-student approach. The teacher model(GEMINI) generates a reduced hypothesis space based on prior knowledge from thelabeled data, and is used it as a guide to a student model (i.e., ResNet50)through a Knowledge Distillation scheme. Extensive tests were first performedon two datasets separately used for the recognition, namely a set of imagesacquired for the surfaces of the kidney stone fragments, and a set of images ofthe fragment sections. The proposed DML-approach improved the identificationaccuracy by 10% and 12% in comparison to DL-methods and other DML-approaches,respectively. Moreover, model embeddings from the two dataset types were mergedin an organized way through a multi-view scheme to simultaneously exploit theinformation of surface and section fragments. Test with the resulting mixedmodel improves the identification accuracy by at least 3% and up to 30% withrespect to DL-models and shallow machine learning methods, respectively."
Complex Facial Expression Recognition Using Deep Knowledge Distillation  of Basic Features,"['Angus Maiden', 'Bahareh Nakisa']",http://arxiv.org/pdf/2308.06197v2.pdf,2023-08-11,"['cs.cv', 'cs.ai', 'cs.lg', '68t45 (primary) 68t07, 68t01, 68t05 (secondary)', 'i.2.1; i.2.10; i.2.6; i.2.0']","  Complex emotion recognition is a cognitive task that has so far eluded thesame excellent performance of other tasks that are at or above the level ofhuman cognition. Emotion recognition through facial expressions is particularlydifficult due to the complexity of emotions expressed by the human face. For amachine to approach the same level of performance in complex facial expressionrecognition as a human, it may need to synthesise knowledge and understand newconcepts in real-time, as humans do. Humans are able to learn new conceptsusing only few examples by distilling important information from memories.Inspired by human cognition and learning, we propose a novel continual learningmethod for complex facial expression recognition that can accurately recognisenew compound expression classes using few training samples, by building on andretaining its knowledge of basic expression classes. In this work, we also useGradCAM visualisations to demonstrate the relationship between basic andcompound facial expressions. Our method leverages this relationship throughknowledge distillation and a novel Predictive Sorting Memory Replay, to achievethe current state-of-the-art in continual learning for complex facialexpression recognition, with 74.28% Overall Accuracy on new classes. We alsodemonstrate that using continual learning for complex facial expressionrecognition achieves far better performance than non-continual learningmethods, improving on state-of-the-art non-continual learning methods by13.95%. Our work is also the first to apply few-shot learning to complex facialexpression recognition, achieving the state-of-the-art with 100% accuracy usingonly a single training sample per class."
Effective Test Generation Using Pre-trained Large Language Models and  Mutation Testing,"['Arghavan Moradi Dakhel', 'Amin Nikanjam', 'Vahid Majdinasab', 'Foutse Khomh', 'Michel C. Desmarais']",http://arxiv.org/pdf/2308.16557v1.pdf,2023-08-31,['cs.se'],"  One of the critical phases in software development is software testing.Testing helps with identifying potential bugs and reducing maintenance costs.The goal of automated test generation tools is to ease the development of testsby suggesting efficient bug-revealing tests. Recently, researchers haveleveraged Large Language Models (LLMs) of code to generate unit tests. Whilethe code coverage of generated tests was usually assessed, the literature hasacknowledged that the coverage is weakly correlated with the efficiency oftests in bug detection. To improve over this limitation, in this paper, weintroduce MuTAP for improving the effectiveness of test cases generated by LLMsin terms of revealing bugs by leveraging mutation testing. Our goal is achievedby augmenting prompts with surviving mutants, as those mutants highlight thelimitations of test cases in detecting bugs. MuTAP is capable of generatingeffective test cases in the absence of natural language descriptions of theProgram Under Test (PUTs). We employ different LLMs within MuTAP and evaluatetheir performance on different benchmarks. Our results show that our proposedmethod is able to detect up to 28% more faulty human-written code snippets.Among these, 17% remained undetected by both the current state-of-the-art fullyautomated test generation tool (i.e., Pynguin) and zero-shot/few-shot learningapproaches on LLMs. Furthermore, MuTAP achieves a Mutation Score (MS) of 93.57%on synthetic buggy code, outperforming all other approaches in our evaluation.Our findings suggest that although LLMs can serve as a useful tool to generatetest cases, they require specific post-processing steps to enhance theeffectiveness of the generated test cases which may suffer from syntactic orfunctional errors and may be ineffective in detecting certain types of bugs andtesting corner cases PUTs."
An evaluation of GPT models for phenotype concept recognition,"['Tudor Groza', 'Harry Caufield', 'Dylan Gration', 'Gareth Baynam', 'Melissa A Haendel', 'Peter N Robinson', 'Christopher J Mungall', 'Justin T Reese']",http://arxiv.org/pdf/2309.17169v2.pdf,2023-09-29,"['cs.cl', 'cs.ai']","  Objective: Clinical deep phenotyping and phenotype annotation play a criticalrole in both the diagnosis of patients with rare disorders as well as inbuilding computationally-tractable knowledge in the rare disorders field. Theseprocesses rely on using ontology concepts, often from the Human PhenotypeOntology, in conjunction with a phenotype concept recognition task (supportedusually by machine learning methods) to curate patient profiles or existingscientific literature. With the significant shift in the use of large languagemodels (LLMs) for most NLP tasks, we examine the performance of the latestGenerative Pre-trained Transformer (GPT) models underpinning ChatGPT as afoundation for the tasks of clinical phenotyping and phenotype annotation.Materials and Methods: The experimental setup of the study included sevenprompts of various levels of specificity, two GPT models (gpt-3.5-turbo andgpt-4.0) and two established gold standard corpora for phenotype recognition,one consisting of publication abstracts and the other clinical observations.Results: Our results show that, with an appropriate setup, these models canachieve state of the art performance. The best run, using few-shot learning,achieved 0.58 macro F1 score on publication abstracts and 0.75 macro F1 scoreon clinical observations, the former being comparable with the state of theart, while the latter surpassing the current best in class tool. Conclusion:While the results are promising, the non-deterministic nature of the outcomes,the high cost and the lack of concordance between different runs using the sameprompt and input make the use of these LLMs challenging for this particulartask."
LLM4SGG: Large Language Model for Weakly Supervised Scene Graph  Generation,"['Kibum Kim', 'Kanghoon Yoon', 'Jaehyeong Jeon', 'Yeonjun In', 'Jinyoung Moon', 'Donghyun Kim', 'Chanyoung Park']",http://arxiv.org/pdf/2310.10404v5.pdf,2023-10-16,['cs.cv'],"  Weakly-Supervised Scene Graph Generation (WSSGG) research has recentlyemerged as an alternative to the fully-supervised approach that heavily relieson costly annotations. In this regard, studies on WSSGG have utilized imagecaptions to obtain unlocalized triplets while primarily focusing on groundingthe unlocalized triplets over image regions. However, they have overlooked thetwo issues involved in the triplet formation process from the captions: 1)Semantic over-simplification issue arises when extracting triplets fromcaptions, where fine-grained predicates in captions are undesirably convertedinto coarse-grained predicates, resulting in a long-tailed predicatedistribution, and 2) Low-density scene graph issue arises when aligning thetriplets in the caption with entity/predicate classes of interest, where manytriplets are discarded and not used in training, leading to insufficientsupervision. To tackle the two issues, we propose a new approach, i.e., LargeLanguage Model for weakly-supervised SGG (LLM4SGG), where we mitigate the twoissues by leveraging the LLM's in-depth understanding of language and reasoningability during the extraction of triplets from captions and alignment ofentity/predicate classes with target data. To further engage the LLM in theseprocesses, we adopt the idea of Chain-of-Thought and the in-context few-shotlearning strategy. To validate the effectiveness of LLM4SGG, we conductextensive experiments on Visual Genome and GQA datasets, showing significantimprovements in both Recall@K and mean Recall@K compared to thestate-of-the-art WSSGG methods. A further appeal is that LLM4SGG isdata-efficient, enabling effective model training with a small amount oftraining images."
Analysis and Applications of Deep Learning with Finite Samples in Full  Life-Cycle Intelligence of Nuclear Power Generation,"['Chenwei Tang', 'Wenqiang Zhou', 'Dong Wang', 'Caiyang Yu', 'Zhenan He', 'Jizhe Zhou', 'Shudong Huang', 'Yi Gao', 'Jianming Chen', 'Wentao Feng', 'Jiancheng Lv']",http://arxiv.org/pdf/2311.04247v1.pdf,2023-11-07,"['cs.lg', 'cs.ai']","  The advent of Industry 4.0 has precipitated the incorporation of ArtificialIntelligence (AI) methods within industrial contexts, aiming to realizeintelligent manufacturing, operation as well as maintenance, also known asindustrial intelligence. However, intricate industrial milieus, particularlythose relating to energy exploration and production, frequently encompass datacharacterized by long-tailed class distribution, sample imbalance, and domainshift. These attributes pose noteworthy challenges to data-centric DeepLearning (DL) techniques, crucial for the realization of industrialintelligence. The present study centers on the intricate and distinctiveindustrial scenarios of Nuclear Power Generation (NPG), meticulouslyscrutinizing the application of DL techniques under the constraints of finitedata samples. Initially, the paper expounds on potential employment scenariosfor AI across the full life-cycle of NPG. Subsequently, we delve into anevaluative exposition of DL's advancement, grounded in the finite sampleperspective. This encompasses aspects such as small-sample learning, few-shotlearning, zero-shot learning, and open-set recognition, also referring to theunique data characteristics of NPG. The paper then proceeds to present twospecific case studies. The first revolves around the automatic recognition ofzirconium alloy metallography, while the second pertains to open-setrecognition for signal diagnosis of machinery sensors. These cases, spanningthe entirety of NPG's life-cycle, are accompanied by constructive outcomes andinsightful deliberations. By exploring and applying DL methodologies within theconstraints of finite sample availability, this paper not only furnishes arobust technical foundation but also introduces a fresh perspective toward thesecure and efficient advancement and exploitation of this advanced energysource."
On the Out of Distribution Robustness of Foundation Models in Medical  Image Segmentation,"['Duy Minh Ho Nguyen', 'Tan Ngoc Pham', 'Nghiem Tuong Diep', 'Nghi Quoc Phan', 'Quang Pham', 'Vinh Tong', 'Binh T. Nguyen', 'Ngan Hoang Le', 'Nhat Ho', 'Pengtao Xie', 'Daniel Sonntag', 'Mathias Niepert']",http://arxiv.org/pdf/2311.11096v1.pdf,2023-11-18,"['eess.iv', 'cs.cv']","  Constructing a robust model that can effectively generalize to test samplesunder distribution shifts remains a significant challenge in the field ofmedical imaging. The foundational models for vision and language, pre-trainedon extensive sets of natural image and text data, have emerged as a promisingapproach. It showcases impressive learning abilities across different taskswith the need for only a limited amount of annotated samples. While numeroustechniques have focused on developing better fine-tuning strategies to adaptthese models for specific domains, we instead examine their robustness todomain shifts in the medical image segmentation task. To this end, we comparethe generalization performance to unseen domains of various pre-trained modelsafter being fine-tuned on the same in-distribution dataset and show thatfoundation-based models enjoy better robustness than other architectures. Fromhere, we further developed a new Bayesian uncertainty estimation for frozenmodels and used them as an indicator to characterize the model's performance onout-of-distribution (OOD) data, proving particularly beneficial for real-worldapplications. Our experiments not only reveal the limitations of currentindicators like accuracy on the line or agreement on the line commonly used innatural image applications but also emphasize the promise of the introducedBayesian uncertainty. Specifically, lower uncertainty predictions usually tendto higher out-of-distribution (OOD) performance."
Optimal Strategies to Perform Multilingual Analysis of Social Content  for a Novel Dataset in the Tourism Domain,"['Maxime Masson', 'Rodrigo Agerri', 'Christian Sallaberry', 'Marie-Noelle Bessagnet', 'Annig Le Parc Lacayrelle', 'Philippe Roose']",http://arxiv.org/pdf/2311.14727v1.pdf,2023-11-20,"['cs.cl', 'cs.lg']","  The rising influence of social media platforms in various domains, includingtourism, has highlighted the growing need for efficient and automated naturallanguage processing (NLP) approaches to take advantage of this valuableresource. However, the transformation of multilingual, unstructured, andinformal texts into structured knowledge often poses significant challenges.  In this work, we evaluate and compare few-shot, pattern-exploiting andfine-tuning machine learning techniques on large multilingual language models(LLMs) to establish the best strategy to address the lack of annotated data for3 common NLP tasks in the tourism domain: (1) Sentiment Analysis, (2) NamedEntity Recognition, and (3) Fine-grained Thematic Concept Extraction (linked toa semantic resource). Furthermore, we aim to ascertain the quantity ofannotated examples required to achieve good performance in those 3 tasks,addressing a common challenge encountered by NLP researchers in theconstruction of domain-specific datasets.  Extensive experimentation on a newly collected and annotated multilingual(French, English, and Spanish) dataset composed of tourism-related tweets showsthat current few-shot learning techniques allow us to obtain competitiveresults for all three tasks with very little annotation data: 5 tweets perlabel (15 in total) for Sentiment Analysis, 10% of the tweets for locationdetection (around 160) and 13% (200 approx.) of the tweets annotated withthematic concepts, a highly fine-grained sequence labeling task based on aninventory of 315 classes.  This comparative analysis, grounded in a novel dataset, paves the way forapplying NLP to new domain-specific applications, reducing the need for manualannotations and circumventing the complexities of rule-based, ad hoc solutions."
Generalized Label-Efficient 3D Scene Parsing via Hierarchical Feature  Aligned Pre-Training and Region-Aware Fine-tuning,"['Kangcheng Liu', 'Yong-Jin Liu', 'Kai Tang', 'Ming Liu', 'Baoquan Chen']",http://arxiv.org/pdf/2312.00663v1.pdf,2023-12-01,"['cs.cv', 'cs.ro']","  Deep neural network models have achieved remarkable progress in 3D sceneunderstanding while trained in the closed-set setting and with full labels.However, the major bottleneck for current 3D recognition approaches is thatthey do not have the capacity to recognize any unseen novel classes beyond thetraining categories in diverse kinds of real-world applications. In themeantime, current state-of-the-art 3D scene understanding approaches primarilyrequire high-quality labels to train neural networks, which merely perform wellin a fully supervised manner. This work presents a generalized and simpleframework for dealing with 3D scene understanding when the labeled scenes arequite limited. To extract knowledge for novel categories from the pre-trainedvision-language models, we propose a hierarchical feature-aligned pre-trainingand knowledge distillation strategy to extract and distill meaningfulinformation from large-scale vision-language models, which helps benefit theopen-vocabulary scene understanding tasks. To leverage the boundaryinformation, we propose a novel energy-based loss with boundary awarenessbenefiting from the region-level boundary predictions. To encourage latentinstance discrimination and to guarantee efficiency, we propose theunsupervised region-level semantic contrastive learning scheme for pointclouds, using confident predictions of the neural network to discriminate theintermediate feature embeddings at multiple stages. Extensive experiments withboth indoor and outdoor scenes demonstrated the effectiveness of our approachin both data-efficient learning and open-world few-shot learning. All codes,models, and data are made publicly available at:https://drive.google.com/drive/folders/1M58V-PtR8DBEwD296zJkNg_m2qq-MTAP?usp=sharing."
Towards General Purpose Vision Foundation Models for Medical Image  Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks,"['Mohammed Baharoon', 'Waseem Qureshi', 'Jiahong Ouyang', 'Yanwu Xu', 'Abdulrhman Aljouie', 'Wei Peng']",http://arxiv.org/pdf/2312.02366v2.pdf,2023-12-04,"['cs.cv', 'cs.ai']","  The integration of deep learning systems into the medical domain has beenhindered by the resource-intensive process of data annotation and the inabilityof these systems to generalize to different data distributions. Foundationmodels, which are models pre-trained on large datasets, have emerged as asolution to reduce reliance on annotated data and enhance modelgeneralizability and robustness. DINOv2, an open-source foundation modelpre-trained with self-supervised learning on 142 million curated naturalimages, excels in extracting general-purpose visual representations, exhibitingpromising capabilities across various vision tasks. Nevertheless, a criticalquestion remains unanswered regarding DINOv2's adaptability to radiologicalimaging, and the clarity on whether its features are sufficiently general tobenefit radiology image analysis is yet to be established. Therefore, thisstudy comprehensively evaluates DINOv2 for radiology, conducting over 100experiments across diverse modalities (X-ray, CT, and MRI). Tasks includedisease classification and organ segmentation on both 2D and 3D images,evaluated under different settings like kNN, few-shot learning, linear-probing,end-to-end fine-tuning, and parameter-efficient fine-tuning, to measure theeffectiveness and generalizability of the DINOv2 feature embeddings.Comparative analyses with established medical image analysis models, U-Net andTransUnet for segmentation, and CNN and ViT models pre-trained via supervised,weakly supervised, and self-supervised learning for classification, revealDINOv2's superior performance in segmentation tasks and competitive results indisease classification. The findings contribute insights to potential avenuesfor optimizing pre-training strategies for medical imaging and enhancing thebroader understanding of DINOv2's role in bridging the gap between natural andradiological image analysis."
Near-real-time Earthquake-induced Fatality Estimation using Crowdsourced  Data and Large-Language Models,"['Chenguang Wang', 'Davis Engler', 'Xuechun Li', 'James Hou', 'David J. Wald', 'Kishor Jaiswal', 'Susu Xu']",http://arxiv.org/pdf/2312.03755v1.pdf,2023-12-04,"['cs.cl', 'cs.ai', 'cs.cy', 'cs.lg']","  When a damaging earthquake occurs, immediate information about casualties iscritical for time-sensitive decision-making by emergency response and aidagencies in the first hours and days. Systems such as Prompt Assessment ofGlobal Earthquakes for Response (PAGER) by the U.S. Geological Survey (USGS)were developed to provide a forecast within about 30 minutes of any significantearthquake globally. Traditional systems for estimating human loss in disastersoften depend on manually collected early casualty reports from global media, aprocess that's labor-intensive and slow with notable time delays. Recently,some systems have employed keyword matching and topic modeling to extractrelevant information from social media. However, these methods struggle withthe complex semantics in multilingual texts and the challenge of interpretingever-changing, often conflicting reports of death and injury numbers fromvarious unverified sources on social media platforms. In this work, weintroduce an end-to-end framework to significantly improve the timeliness andaccuracy of global earthquake-induced human loss forecasting usingmulti-lingual, crowdsourced social media. Our framework integrates (1) ahierarchical casualty extraction model built upon large language models, promptdesign, and few-shot learning to retrieve quantitative human loss claims fromsocial media, (2) a physical constraint-aware, dynamic-truth discovery modelthat discovers the truthful human loss from massive noisy and potentiallyconflicting human loss claims, and (3) a Bayesian updating loss projectionmodel that dynamically updates the final loss estimation using discoveredtruths. We test the framework in real-time on a series of global earthquakeevents in 2021 and 2022 and show that our framework streamlines casualty dataretrieval, achieving speed and accuracy comparable to manual methods by USGS."
Language Models are Few-Shot Learners,"['Tom B. Brown', 'Benjamin Mann', 'Nick Ryder', 'Melanie Subbiah', 'Jared Kaplan', 'Prafulla Dhariwal', 'Arvind Neelakantan', 'Pranav Shyam', 'Girish Sastry', 'Amanda Askell', 'Sandhini Agarwal', 'Ariel Herbert-Voss', 'Gretchen Krueger', 'Tom Henighan', 'Rewon Child', 'Aditya Ramesh', 'Daniel M. Ziegler', 'Jeffrey Wu', 'Clemens Winter', 'Christopher Hesse', 'Mark Chen', 'Eric Sigler', 'Mateusz Litwin', 'Scott Gray', 'Benjamin Chess', 'Jack Clark', 'Christopher Berner', 'Sam McCandlish', 'Alec Radford', 'Ilya Sutskever', 'Dario Amodei']",http://arxiv.org/pdf/2005.14165v4.pdf,2020-05-28,['cs.cl'],"  Recent work has demonstrated substantial gains on many NLP tasks andbenchmarks by pre-training on a large corpus of text followed by fine-tuning ona specific task. While typically task-agnostic in architecture, this methodstill requires task-specific fine-tuning datasets of thousands or tens ofthousands of examples. By contrast, humans can generally perform a new languagetask from only a few examples or from simple instructions - something whichcurrent NLP systems still largely struggle to do. Here we show that scaling uplanguage models greatly improves task-agnostic, few-shot performance, sometimeseven reaching competitiveness with prior state-of-the-art fine-tuningapproaches. Specifically, we train GPT-3, an autoregressive language model with175 billion parameters, 10x more than any previous non-sparse language model,and test its performance in the few-shot setting. For all tasks, GPT-3 isapplied without any gradient updates or fine-tuning, with tasks and few-shotdemonstrations specified purely via text interaction with the model. GPT-3achieves strong performance on many NLP datasets, including translation,question-answering, and cloze tasks, as well as several tasks that requireon-the-fly reasoning or domain adaptation, such as unscrambling words, using anovel word in a sentence, or performing 3-digit arithmetic. At the same time,we also identify some datasets where GPT-3's few-shot learning still struggles,as well as some datasets where GPT-3 faces methodological issues related totraining on large web corpora. Finally, we find that GPT-3 can generate samplesof news articles which human evaluators have difficulty distinguishing fromarticles written by humans. We discuss broader societal impacts of this findingand of GPT-3 in general."
MasakhaNEWS: News Topic Classification for African languages,"['David Ifeoluwa Adelani', 'Marek Masiak', 'Israel Abebe Azime', 'Jesujoba Alabi', 'Atnafu Lambebo Tonja', 'Christine Mwase', 'Odunayo Ogundepo', 'Bonaventure F. P. Dossou', 'Akintunde Oladipo', 'Doreen Nixdorf', 'Chris Chinenye Emezue', 'sana al-azzawi', 'Blessing Sibanda', 'Davis David', 'Lolwethu Ndolela', 'Jonathan Mukiibi', 'Tunde Ajayi', 'Tatiana Moteu', 'Brian Odhiambo', 'Abraham Owodunni', 'Nnaemeka Obiefuna', 'Muhidin Mohamed', 'Shamsuddeen Hassan Muhammad', 'Teshome Mulugeta Ababu', 'Saheed Abdullahi Salahudeen', 'Mesay Gemeda Yigezu', 'Tajuddeen Gwadabe', 'Idris Abdulmumin', 'Mahlet Taye', 'Oluwabusayo Awoyomi', 'Iyanuoluwa Shode', 'Tolulope Adelani', 'Habiba Abdulganiyu', 'Abdul-Hakeem Omotayo', 'Adetola Adeeko', 'Abeeb Afolabi', 'Anuoluwapo Aremu', 'Olanrewaju Samuel', 'Clemencia Siro', 'Wangari Kimotho', 'Onyekachi Ogbu', 'Chinedu Mbonu', 'Chiamaka Chukwuneke', 'Samuel Fanijo', 'Jessica Ojo', 'Oyinkansola Awosan', 'Tadesse Kebede', 'Toadoum Sari Sakayo', 'Pamela Nyatsine', 'Freedmore Sidume', 'Oreen Yousuf', 'Mardiyyah Oduwole', 'Tshinu Tshinu', 'Ussen Kimanuka', 'Thina Diko', 'Siyanda Nxakama', 'Sinodos Nigusse', 'Abdulmejid Johar', 'Shafie Mohamed', 'Fuad Mire Hassan', 'Moges Ahmed Mehamed', 'Evrard Ngabire', 'Jules Jules', 'Ivan Ssenkungu', 'Pontus Stenetorp']",http://arxiv.org/pdf/2304.09972v2.pdf,2023-04-19,['cs.cl'],"  African languages are severely under-represented in NLP research due to lackof datasets covering several NLP tasks. While there are individual languagespecific datasets that are being expanded to different tasks, only a handful ofNLP tasks (e.g. named entity recognition and machine translation) havestandardized benchmark datasets covering several geographical andtypologically-diverse African languages. In this paper, we develop MasakhaNEWS-- a new benchmark dataset for news topic classification covering 16 languageswidely spoken in Africa. We provide an evaluation of baseline models bytraining classical machine learning models and fine-tuning several languagemodels. Furthermore, we explore several alternatives to full fine-tuning oflanguage models that are better suited for zero-shot and few-shot learning suchas cross-lingual parameter-efficient fine-tuning (like MAD-X), patternexploiting training (PET), prompting language models (like ChatGPT), andprompt-free sentence transformer fine-tuning (SetFit and Cohere Embedding API).Our evaluation in zero-shot setting shows the potential of prompting ChatGPTfor news topic classification in low-resource African languages, achieving anaverage performance of 70 F1 points without leveraging additional supervisionlike MAD-X. In few-shot setting, we show that with as little as 10 examples perlabel, we achieved more than 90\% (i.e. 86.0 F1 points) of the performance offull supervised training (92.6 F1 points) leveraging the PET approach."
Exploring Effectiveness of GPT-3 in Grammatical Error Correction: A  Study on Performance and Controllability in Prompt-Based Methods,"['Mengsay Loem', 'Masahiro Kaneko', 'Sho Takase', 'Naoaki Okazaki']",http://arxiv.org/pdf/2305.18156v1.pdf,2023-05-29,"['cs.cl', 'cs.ai']","  Large-scale pre-trained language models such as GPT-3 have shown remarkableperformance across various natural language processing tasks. However, applyingprompt-based methods with GPT-3 for Grammatical Error Correction (GEC) tasksand their controllability remains underexplored. Controllability in GEC iscrucial for real-world applications, particularly in educational settings,where the ability to tailor feedback according to learner levels and specificerror types can significantly enhance the learning process. This paperinvestigates the performance and controllability of prompt-based methods withGPT-3 for GEC tasks using zero-shot and few-shot setting. We explore the impactof task instructions and examples on GPT-3's output, focusing on controllingaspects such as minimal edits, fluency edits, and learner levels. Our findingsdemonstrate that GPT-3 could effectively perform GEC tasks, outperformingexisting supervised and unsupervised approaches. We also showed that GPT-3could achieve controllability when appropriate task instructions and examplesare given."
Causal Intervention-based Prompt Debiasing for Event Argument Extraction,"['Jiaju Lin', 'Jie Zhou', 'Qin Chen']",http://arxiv.org/pdf/2210.01561v1.pdf,2022-10-04,"['cs.cl', 'cs.ai']","  Prompt-based methods have become increasingly popular among informationextraction tasks, especially in low-data scenarios. By formatting a finetunetask into a pre-training objective, prompt-based methods resolve the datascarce problem effectively. However, seldom do previous research investigatethe discrepancy among different prompt formulating strategies. In this work, wecompare two kinds of prompts, name-based prompt and ontology-base prompt, andreveal how ontology-base prompt methods exceed its counterpart in zero-shotevent argument extraction (EAE) . Furthermore, we analyse the potential risk inontology-base prompts via a causal view and propose a debias method by causalintervention. Experiments on two benchmarks demonstrate that modified by ourdebias method, the baseline model becomes both more effective and robust, withsignificant improvement in the resistance to adversarial attacks."
When Prompt-based Incremental Learning Does Not Meet Strong Pretraining,"['Yu-Ming Tang', 'Yi-Xing Peng', 'Wei-Shi Zheng']",http://arxiv.org/pdf/2308.10445v1.pdf,2023-08-21,['cs.cv'],"  Incremental learning aims to overcome catastrophic forgetting when learningdeep networks from sequential tasks. With impressive learning efficiency andperformance, prompt-based methods adopt a fixed backbone to sequential tasks bylearning task-specific prompts. However, existing prompt-based methods heavilyrely on strong pretraining (typically trained on ImageNet-21k), and we findthat their models could be trapped if the potential gap between the pretrainingtask and unknown future tasks is large. In this work, we develop a learnableAdaptive Prompt Generator (APG). The key is to unify the prompt retrieval andprompt learning processes into a learnable prompt generator. Hence, the wholeprompting process can be optimized to reduce the negative effects of the gapbetween tasks effectively. To make our APG avoid learning ineffectiveknowledge, we maintain a knowledge pool to regularize APG with the featuredistribution of each class. Extensive experiments show that our methodsignificantly outperforms advanced methods in exemplar-free incrementallearning without (strong) pretraining. Besides, under strong retraining, ourmethod also has comparable performance to existing prompt-based models, showingthat our method can still benefit from pretraining. Codes can be found athttps://github.com/TOM-tym/APG"
Zero-shot Domain Adaptation for Neural Machine Translation with  Retrieved Phrase-level Prompts,"['Zewei Sun', 'Qingnan Jiang', 'Shujian Huang', 'Jun Cao', 'Shanbo Cheng', 'Mingxuan Wang']",http://arxiv.org/pdf/2209.11409v1.pdf,2022-09-23,['cs.cl'],"  Domain adaptation is an important challenge for neural machine translation.However, the traditional fine-tuning solution requires multiple extra trainingand yields a high cost. In this paper, we propose a non-tuning paradigm,resolving domain adaptation with a prompt-based method. Specifically, weconstruct a bilingual phrase-level database and retrieve relevant pairs from itas a prompt for the input sentences. By utilizing Retrieved Phrase-levelPrompts (RePP), we effectively boost the translation quality. Experiments showthat our method improves domain-specific machine translation for 6.2 BLEUscores and improves translation constraints for 11.5% accuracy withoutadditional training."
NSP-BERT: A Prompt-based Few-Shot Learner Through an Original  Pre-training Task--Next Sentence Prediction,"['Yi Sun', 'Yu Zheng', 'Chao Hao', 'Hangping Qiu']",http://arxiv.org/pdf/2109.03564v2.pdf,2021-09-08,"['cs.cl', 'cs.ai']","  Using prompts to utilize language models to perform various downstream tasks,also known as prompt-based learning or prompt-learning, has lately gainedsignificant success in comparison to the pre-train and fine-tune paradigm.Nonetheless, virtually all prompt-based methods are token-level, meaning theyall utilize GPT's left-to-right language model or BERT's masked language modelto perform cloze-style tasks. In this paper, we attempt to accomplish severalNLP tasks in the zero-shot scenario using a BERT original pre-training taskabandoned by RoBERTa and other models--Next Sentence Prediction (NSP). Unliketoken-level techniques, our sentence-level prompt-based method NSP-BERT doesnot need to fix the length of the prompt or the position to be predicted,allowing it to handle tasks such as entity linking with ease. Based on thecharacteristics of NSP-BERT, we offer several quick building templates forvarious downstream tasks. We suggest a two-stage prompt method for word sensedisambiguation tasks in particular. Our strategies for mapping the labelssignificantly enhance the model's performance on sentence pair tasks. On theFewCLUE benchmark, our NSP-BERT outperforms other zero-shot methods on most ofthese tasks and comes close to the few-shot methods."
Introducing Language Guidance in Prompt-based Continual Learning,"['Muhammad Gul Zain Ali Khan', 'Muhammad Ferjad Naeem', 'Luc Van Gool', 'Didier Stricker', 'Federico Tombari', 'Muhammad Zeshan Afzal']",http://arxiv.org/pdf/2308.15827v1.pdf,2023-08-30,['cs.cv'],"  Continual Learning aims to learn a single model on a sequence of taskswithout having access to data from previous tasks. The biggest challenge in thedomain still remains catastrophic forgetting: a loss in performance on seenclasses of earlier tasks. Some existing methods rely on an expensive replaybuffer to store a chunk of data from previous tasks. This, while promising,becomes expensive when the number of tasks becomes large or data can not bestored for privacy reasons. As an alternative, prompt-based methods have beenproposed that store the task information in a learnable prompt pool. Thisprompt pool instructs a frozen image encoder on how to solve each task. Whilethe model faces a disjoint set of classes in each task in this setting, weargue that these classes can be encoded to the same embedding space of apre-trained language encoder. In this work, we propose Language Guidance forPrompt-based Continual Learning (LGCL) as a plug-in for prompt-based methods.LGCL is model agnostic and introduces language guidance at the task level inthe prompt pool and at the class level on the output feature of the visionencoder. We show with extensive experimentation that LGCL consistently improvesthe performance of prompt-based continual learning methods to set a newstate-of-the art. LGCL achieves these performance improvements without needingany additional learnable parameters."
Enable Language Models to Implicitly Learn Self-Improvement From Data,"['Ziqi Wang', 'Le Hou', 'Tianjian Lu', 'Yuexin Wu', 'Yunxuan Li', 'Hongkun Yu', 'Heng Ji']",http://arxiv.org/pdf/2310.00898v2.pdf,2023-10-02,['cs.cl'],"  Large Language Models (LLMs) have demonstrated remarkable capabilities inopen-ended text generation tasks. However, the inherent open-ended nature ofthese tasks implies that there is always room for improvement in the quality ofmodel responses. To address this challenge, various approaches have beenproposed to enhance the performance of LLMs. There has been a growing focus onenabling LLMs to self-improve their response quality, thereby reducing thereliance on extensive human annotation efforts for collecting diverse andhigh-quality training data. Recently, prompting-based methods have been widelyexplored among self-improvement methods owing to their effectiveness,efficiency, and convenience. However, those methods usually require explicitlyand thoroughly written rubrics as inputs to LLMs. It is expensive andchallenging to manually derive and provide all necessary rubrics with areal-world complex goal for improvement (e.g., being more helpful and lessharmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) frameworkthat implicitly learns the improvement goal from human preference data. PITonly requires preference data that are used to train reward models withoutextra human efforts. Specifically, we reformulate the training objective ofreinforcement learning from human feedback (RLHF) -- instead of maximizingresponse quality for a given input, we maximize the quality gap of the responseconditioned on a reference response. In this way, PIT is implicitly trainedwith the improvement goal of better aligning with human preferences.Experiments on two real-world datasets and one synthetic dataset show that ourmethod significantly outperforms prompting-based methods."
MEmoBERT: Pre-training Model with Prompt-based Learning for Multimodal  Emotion Recognition,"['Jinming Zhao', 'Ruichen Li', 'Qin Jin', 'Xinchao Wang', 'Haizhou Li']",http://arxiv.org/pdf/2111.00865v1.pdf,2021-10-27,"['cs.cv', 'eess.iv']","  Multimodal emotion recognition study is hindered by the lack of labelledcorpora in terms of scale and diversity, due to the high annotation cost andlabel ambiguity. In this paper, we propose a pre-training model\textbf{MEmoBERT} for multimodal emotion recognition, which learns multimodaljoint representations through self-supervised learning from large-scaleunlabeled video data that come in sheer volume. Furthermore, unlike theconventional ""pre-train, finetune"" paradigm, we propose a prompt-based methodthat reformulates the downstream emotion classification task as a masked textprediction one, bringing the downstream task closer to the pre-training.Extensive experiments on two benchmark datasets, IEMOCAP and MSP-IMPROV, showthat our proposed MEmoBERT significantly enhances emotion recognitionperformance."
PSG: Prompt-based Sequence Generation for Acronym Extraction,"['Bin Li', 'Fei Xia', 'Yixuan Weng', 'Xiusheng Huang', 'Bin Sun', 'Shutao Li']",http://arxiv.org/pdf/2111.14301v2.pdf,2021-11-29,"['cs.cl', 'cs.ai']","  Acronym extraction aims to find acronyms (i.e., short-forms) and theirmeanings (i.e., long-forms) from the documents, which is important forscientific document understanding (SDU@AAAI-22) tasks. Previous works aredevoted to modeling this task as a paragraph-level sequence labeling problem.However, it lacks the effective use of the external knowledge, especially whenthe datasets are in a low-resource setting. Recently, the prompt-based methodwith the vast pre-trained language model can significantly enhance theperformance of the low-resourced downstream tasks. In this paper, we propose aPrompt-based Sequence Generation (PSG) method for the acronym extraction task.Specifically, we design a template for prompting the extracted acronym textswith auto-regression. A position extraction algorithm is designed forextracting the position of the generated answers. The results on the acronymextraction of Vietnamese and Persian in a low-resource setting show that theproposed method outperforms all other competitive state-of-the-art (SOTA)methods."
Chemical Identification and Indexing in PubMed Articles via BERT and  Text-to-Text Approaches,"['Virginia Adams', 'Hoo-Chang Shin', 'Carol Anderson', 'Bo Liu', 'Anas Abidin']",http://arxiv.org/pdf/2111.15622v1.pdf,2021-11-30,['cs.cl'],"  The Biocreative VII Track-2 challenge consists of named entity recognition,entity-linking (or entity-normalization), and topic indexing tasks -- withentities and topics limited to chemicals for this challenge. Named entityrecognition is a well-established problem and we achieve our best performancewith BERT-based BioMegatron models. We extend our BERT-based approach to theentity linking task. After the second stage of pretraining BioBERT with ametric-learning loss strategy called self-alignment pretraining (SAP), we linkentities based on the cosine similarity between their SAP-BioBERT wordembeddings. Despite the success of our named entity recognition experiments, wefind the chemical indexing task generally more challenging.  In addition to conventional NER methods, we attempt both named entityrecognition and entity linking with a novel text-to-text or ""prompt"" basedmethod that uses generative language models such as T5 and GPT. We achieveencouraging results with this new approach."
AdaPrompt: Adaptive Model Training for Prompt-based NLP,"['Yulong Chen', 'Yang Liu', 'Li Dong', 'Shuohang Wang', 'Chenguang Zhu', 'Michael Zeng', 'Yue Zhang']",http://arxiv.org/pdf/2202.04824v2.pdf,2022-02-10,['cs.cl'],"  Prompt-based learning, with its capability to tackle zero-shot and few-shotNLP tasks, has gained much attention in community. The main idea is to bridgethe gap between NLP downstream tasks and language modeling (LM), by mappingthese tasks into natural language prompts, which are then filled by pre-trainedlanguage models (PLMs). However, for prompt learning, there are still twosalient gaps between NLP tasks and pretraining. First, prompt information isnot necessarily sufficiently present during LM pretraining. Second,task-specific data are not necessarily well represented during pretraining. Weaddress these two issues by proposing AdaPrompt, adaptively retrieving externaldata for continual pretraining of PLMs by making use of both task and promptcharacteristics. In addition, we make use of knowledge in Natural LanguageInference models for deriving adaptive verbalizers. Experimental results onfive NLP benchmarks show that AdaPrompt can improve over standard PLMs infew-shot settings. In addition, in zero-shot settings, our method outperformsstandard prompt-based methods by up to 26.35\% relative error reduction."
Prompting to Distill: Boosting Data-Free Knowledge Distillation via  Reinforced Prompt,"['Xinyin Ma', 'Xinchao Wang', 'Gongfan Fang', 'Yongliang Shen', 'Weiming Lu']",http://arxiv.org/pdf/2205.07523v1.pdf,2022-05-16,['cs.cl'],"  Data-free knowledge distillation (DFKD) conducts knowledge distillation viaeliminating the dependence of original training data, and has recently achievedimpressive results in accelerating pre-trained language models. At the heart ofDFKD is to reconstruct a synthetic dataset by inverting the parameters of theuncompressed model. Prior DFKD approaches, however, have largely relied onhand-crafted priors of the target data distribution for the reconstruction,which can be inevitably biased and often incompetent to capture the intrinsicdistributions. To address this problem, we propose a prompt-based method,termed as PromptDFD, that allows us to take advantage of learned languagepriors, which effectively harmonizes the synthetic sentences to be semanticallyand grammatically correct. Specifically, PromptDFD leverages a pre-trainedgenerative model to provide language priors and introduces a reinforced topicprompter to control data synthesis, making the generated samples thematicallyrelevant and semantically plausible, and thus friendly to downstream tasks. Asshown in our experiments, the proposed method substantially improves thesynthesis quality and achieves considerable improvements on distillationperformance. In some cases, PromptDFD even gives rise to results on par withthose from the data-driven knowledge distillation with access to the originaltraining data."
"Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender  Bias","['Yarden Tal', 'Inbal Magar', 'Roy Schwartz']",http://arxiv.org/pdf/2206.09860v1.pdf,2022-06-20,['cs.cl'],"  The size of pretrained models is increasing, and so is their performance on avariety of NLP tasks. However, as their memorization capacity grows, they mightpick up more social biases. In this work, we examine the connection betweenmodel size and its gender bias (specifically, occupational gender bias). Wemeasure bias in three masked language model families (RoBERTa, DeBERTa, and T5)in two setups: directly using prompt based method, and using a downstream task(Winogender). We find on the one hand that larger models receive higher biasscores on the former task, but when evaluated on the latter, they make fewergender errors. To examine these potentially conflicting results, we carefullyinvestigate the behavior of the different models on Winogender. We find thatwhile larger models outperform smaller ones, the probability that theirmistakes are caused by gender bias is higher. Moreover, we find that theproportion of stereotypical errors compared to anti-stereotypical ones growswith the model size. Our findings highlight the potential risks that can arisefrom increasing model size."
PromptAttack: Prompt-based Attack for Language Models via Gradient  Search,"['Yundi Shi', 'Piji Li', 'Changchun Yin', 'Zhaoyang Han', 'Lu Zhou', 'Zhe Liu']",http://arxiv.org/pdf/2209.01882v1.pdf,2022-09-05,"['cs.cl', 'cs.ai', 'cs.cr']","  As the pre-trained language models (PLMs) continue to grow, so do thehardware and data requirements for fine-tuning PLMs. Therefore, the researchershave come up with a lighter method called \textit{Prompt Learning}. However,during the investigations, we observe that the prompt learning methods arevulnerable and can easily be attacked by some illegally constructed prompts,resulting in classification errors, and serious security problems for PLMs.Most of the current research ignores the security issue of prompt-basedmethods. Therefore, in this paper, we propose a malicious prompt templateconstruction method (\textbf{PromptAttack}) to probe the security performanceof PLMs. Several unfriendly template construction approaches are investigatedto guide the model to misclassify the task. Extensive experiments on threedatasets and three PLMs prove the effectiveness of our proposed approachPromptAttack. We also conduct experiments to verify that our method isapplicable in few-shot scenarios."
ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational  Finance Question Answering,"['Zhiyu Chen', 'Shiyang Li', 'Charese Smiley', 'Zhiqiang Ma', 'Sameena Shah', 'William Yang Wang']",http://arxiv.org/pdf/2210.03849v1.pdf,2022-10-07,['cs.cl'],"  With the recent advance in large pre-trained language models, researchershave achieved record performances in NLP tasks that mostly focus on languagepattern matching. The community is experiencing the shift of the challenge fromhow to model language to the imitation of complex reasoning abilities likehuman beings. In this work, we investigate the application domain of financethat involves real-world, complex numerical reasoning. We propose a newlarge-scale dataset, ConvFinQA, aiming to study the chain of numericalreasoning in conversational question answering. Our dataset poses greatchallenge in modeling long-range, complex numerical reasoning paths inreal-world conversations. We conduct comprehensive experiments and analyseswith both the neural symbolic methods and the prompting-based methods, toprovide insights into the reasoning mechanisms of these two divisions. Webelieve our new dataset should serve as a valuable resource to push forward theexploration of real-world, complex reasoning tasks as the next research focus.Our dataset and code is publicly available athttps://github.com/czyssrs/ConvFinQA."
Can Language Models Be Specific? How?,"['Jie Huang', 'Kevin Chen-Chuan Chang', 'Jinjun Xiong', 'Wen-mei Hwu']",http://arxiv.org/pdf/2210.05159v2.pdf,2022-10-11,"['cs.cl', 'cs.ai']","  ""He is a person"", ""Paris is located on the earth"". Both statements arecorrect but meaningless - due to lack of specificity. In this paper, we proposeto measure how specific the language of pre-trained language models (PLMs) is.To achieve this, we introduce a novel approach to build a benchmark forspecificity testing by forming masked token prediction tasks with prompts. Forinstance, given ""Toronto is located in [MASK]."", we want to test whether a morespecific answer will be better filled in by PLMs, e.g., Ontario instead ofCanada. From our evaluations, we show that existing PLMs have only a slightpreference for more specific answers. We identify underlying factors affectingthe specificity and design two prompt-based methods to improve the specificity.Results show that the specificity of the models can be improved by the proposedmethods without additional training. We hope this work can bring to awarenessthe notion of specificity of language models and encourage the researchcommunity to further explore this important but understudied problem."
Multilingual Relation Classification via Efficient and Effective  Prompting,"['Yuxuan Chen', 'David Harbecke', 'Leonhard Hennig']",http://arxiv.org/pdf/2210.13838v2.pdf,2022-10-25,"['cs.cl', 'cs.lg']","  Prompting pre-trained language models has achieved impressive performance onvarious NLP tasks, especially in low data regimes. Despite the success ofprompting in monolingual settings, applying prompt-based methods inmultilingual scenarios has been limited to a narrow set of tasks, due to thehigh cost of handcrafting multilingual prompts. In this paper, we present thefirst work on prompt-based multilingual relation classification (RC), byintroducing an efficient and effective method that constructs prompts fromrelation triples and involves only minimal translation for the class labels. Weevaluate its performance in fully supervised, few-shot and zero-shot scenarios,and analyze its effectiveness across 14 languages, prompt variants, andEnglish-task training in cross-lingual settings. We find that in both fullysupervised and few-shot scenarios, our prompt method beats competitivebaselines: fine-tuning XLM-R_EM and null prompts. It also outperforms therandom baseline by a large margin in zero-shot experiments. Our method requireslittle in-language knowledge and can be used as a strong baseline for similarmultilingual classification tasks."
Steps towards prompt-based creation of virtual worlds,"['Jasmine Roberts', 'Andrzej Banburski-Fahey', 'Jaron Lanier']",http://arxiv.org/pdf/2211.05875v1.pdf,2022-11-10,"['cs.hc', 'cs.ai', 'cs.lg', 'cs.mm']","  Large language models trained for code generation can be applied to speakingvirtual worlds into existence (creating virtual worlds). In this work we showthat prompt-based methods can both accelerate in-VR level editing, as well ascan become part of gameplay rather than just part of game development. As anexample, we present Codex VR Pong which shows non-deterministic game mechanicsusing generative processes to not only create static content but alsonon-trivial interactions between 3D objects. This demonstration naturally leadsto an integral discussion on how one would evaluate and benchmark experiencescreated by generative models - as there are no qualitative or quantitativemetrics that apply in these scenarios. We conclude by discussing impendingchallenges of AI-assisted co-creation in VR."
SPE: Symmetrical Prompt Enhancement for Fact Probing,"['Yiyuan Li', 'Tong Che', 'Yezhen Wang', 'Zhengbao Jiang', 'Caiming Xiong', 'Snigdha Chaturvedi']",http://arxiv.org/pdf/2211.07078v1.pdf,2022-11-14,"['cs.cl', 'cs.ai', 'cs.lg']","  Pretrained language models (PLMs) have been shown to accumulate factualknowledge during pretrainingng (Petroni et al., 2019). Recent works probe PLMsfor the extent of this knowledge through prompts either in discrete orcontinuous forms. However, these methods do not consider symmetry of the task:object prediction and subject prediction. In this work, we propose SymmetricalPrompt Enhancement (SPE), a continuous prompt-based method for factual probingin PLMs that leverages the symmetry of the task by constructing symmetricalprompts for subject and object prediction. Our results on a popular factualprobing dataset, LAMA, show significant improvement of SPE over previousprobing methods."
Interactive-Chain-Prompting: Ambiguity Resolution for Crosslingual  Conditional Generation with Interaction,"['Jonathan Pilault', 'Xavier Garcia', 'Arthur Bražinskas', 'Orhan Firat']",http://arxiv.org/pdf/2301.10309v1.pdf,2023-01-24,"['cs.lg', 'cs.ai', 'cs.cl']","  Crosslingual conditional generation (e.g., machine translation) has longenjoyed the benefits of scaling. Nonetheless, there are still issues that scalealone may not overcome. A source query in one language, for instance, may yieldseveral translation options in another language without any extra context. Onlyone translation could be acceptable however, depending on the translator'spreferences and goals. Choosing the incorrect option might significantly affecttranslation usefulness and quality. We propose a novel method interactive-chainprompting -- a series of question, answering and generation intermediate stepsbetween a Translator model and a User model -- that reduces translations into alist of subproblems addressing ambiguities and then resolving such subproblemsbefore producing the final text to be translated. To check ambiguity resolutioncapabilities and evaluate translation quality, we create a dataset exhibitingdifferent linguistic phenomena which leads to ambiguities at inference for fourlanguages. To encourage further exploration in this direction, we release alldatasets. We note that interactive-chain prompting, using eight interactions asexemplars, consistently surpasses prompt-based methods with direct access tobackground information to resolve ambiguities."
Evaluating the Robustness of Discrete Prompts,"['Yoichi Ishibashi', 'Danushka Bollegala', 'Katsuhito Sudoh', 'Satoshi Nakamura']",http://arxiv.org/pdf/2302.05619v1.pdf,2023-02-11,"['cs.cl', 'cs.ai']","  Discrete prompts have been used for fine-tuning Pre-trained Language Modelsfor diverse NLP tasks. In particular, automatic methods that generate discreteprompts from a small set of training instances have reported superiorperformance. However, a closer look at the learnt prompts reveals that theycontain noisy and counter-intuitive lexical constructs that would not beencountered in manually-written prompts. This raises an important yetunderstudied question regarding the robustness of automatically learnt discreteprompts when used in downstream tasks. To address this question, we conduct asystematic study of the robustness of discrete prompts by applying carefullydesigned perturbations into an application using AutoPrompt and then measuretheir performance in two Natural Language Inference (NLI) datasets. Ourexperimental results show that although the discrete prompt-based methodremains relatively robust against perturbations to NLI inputs, they are highlysensitive to other types of perturbations such as shuffling and deletion ofprompt tokens. Moreover, they generalize poorly across different NLI datasets.We hope our findings will inspire future work on robust discrete promptlearning."
Stabilized In-Context Learning with Pre-trained Language Models for Few  Shot Dialogue State Tracking,"['Derek Chen', 'Kun Qian', 'Zhou Yu']",http://arxiv.org/pdf/2302.05932v1.pdf,2023-02-12,['cs.cl'],"  Prompt-based methods with large pre-trained language models (PLMs) have shownimpressive unaided performance across many NLP tasks. These models improve evenfurther with the addition of a few labeled in-context exemplars to guide outputgeneration. However, for more complex tasks such as dialogue state tracking(DST), designing prompts that reliably convey the desired intent is nontrivial,leading to unstable results. Furthermore, building in-context exemplars fordialogue tasks is difficult because conversational contexts are long whilemodel input lengths are relatively short. To overcome these issues we firstadapt a meta-learning scheme to the dialogue domain which stabilizes theability of the model to perform well under various prompts. We additionallydesign a novel training method to improve upon vanilla retrieval mechanisms tofind ideal in-context examples. Finally, we introduce a saliency model to limitdialogue text length, allowing us to include more exemplars per query. Ineffect, we are able to achieve highly competitive results for few-shot DST onMultiWOZ."
Zero-Shot Information Extraction via Chatting with ChatGPT,"['Xiang Wei', 'Xingyu Cui', 'Ning Cheng', 'Xiaobin Wang', 'Xin Zhang', 'Shen Huang', 'Pengjun Xie', 'Jinan Xu', 'Yufeng Chen', 'Meishan Zhang', 'Yong Jiang', 'Wenjuan Han']",http://arxiv.org/pdf/2302.10205v1.pdf,2023-02-20,['cs.cl'],"  Zero-shot information extraction (IE) aims to build IE systems from theunannotated text. It is challenging due to involving little human intervention.Challenging but worthwhile, zero-shot IE reduces the time and effort that datalabeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3,ChatGPT) show promising performance on zero-shot settings, thus inspiring us toexplore prompt-based methods. In this work, we ask whether strong IE models canbe constructed by directly prompting LLMs. Specifically, we transform thezero-shot IE task into a multi-turn question-answering problem with a two-stageframework (ChatIE). With the power of ChatGPT, we extensively evaluate ourframework on three IE tasks: entity-relation triple extract, named entityrecognition, and event extraction. Empirical results on six datasets across twolanguages show that ChatIE achieves impressive performance and even surpassessome full-shot models on several datasets (e.g., NYT11-HRL). We believe thatour work could shed light on building IE models with limited resources."
Divide and Prompt: Chain of Thought Prompting for Text-to-SQL,"['Xiping Liu', 'Zhao Tan']",http://arxiv.org/pdf/2304.11556v1.pdf,2023-04-23,"['cs.cl', 'cs.ai']","  Chain-of-thought (CoT) prompting combined with large language models (LLMs)have achieved encouraging results on complex reasoning tasks. Text-to-SQL is acritical semantic parsing task that converts natural language questions intoSQL statements, involving a complex reasoning process. However, there is littlework about using CoT prompting to activate LLM's reasoning capabilities onText-to-SQL tasks. In this work, we propose a new paradigm for promptingText-to-SQL tasks, called Divide-and-Prompt, which first divides the task intosubtasks, and then approach each subtask through CoT. We present 3prompting-based methods to enhance the Text-to-SQL ability of LLMs. Experimentsshow that these prompts guide LLMs to generate Text-to-SQL with higherexecution accuracy."
Few-shot Event Detection: An Empirical Study and a Unified View,"['Yubo Ma', 'Zehao Wang', 'Yixin Cao', 'Aixin Sun']",http://arxiv.org/pdf/2305.01901v2.pdf,2023-05-03,"['cs.cl', 'cs.ai']","  Few-shot event detection (ED) has been widely studied, while this bringsnoticeable discrepancies, e.g., various motivations, tasks, and experimentalsettings, that hinder the understanding of models for future progress.Thispaper presents a thorough empirical study, a unified view of ED models, and abetter unified baseline. For fair evaluation, we compare 12 representativemethods on three datasets, which are roughly grouped into prompt-based andprototype-based models for detailed analysis. Experiments consistentlydemonstrate that prompt-based methods, including ChatGPT, still significantlytrail prototype-based methods in terms of overall performance. To investigatetheir superior performance, we break down their design elements along severaldimensions and build a unified framework on prototype-based methods. Under suchunified view, each prototype-method can be viewed a combination of differentmodules from these design elements. We further combine all advantageous modulesand propose a simple yet effective baseline, which outperforms existing methodsby a large margin (e.g., 2.7% F1 gains under low-resource setting)."
PURR: Efficiently Editing Language Model Hallucinations by Denoising  Language Model Corruptions,"['Anthony Chen', 'Panupong Pasupat', 'Sameer Singh', 'Hongrae Lee', 'Kelvin Guu']",http://arxiv.org/pdf/2305.14908v1.pdf,2023-05-24,['cs.cl'],"  The remarkable capabilities of large language models have been accompanied bya persistent drawback: the generation of false and unsubstantiated claimscommonly known as ""hallucinations"". To combat this issue, recent research hasintroduced approaches that involve editing and attributing the outputs oflanguage models, particularly through prompt-based editing. However, theinference cost and speed of using large language models for editing currentlybottleneck prompt-based methods. These bottlenecks motivate the training ofcompact editors, which is challenging due to the scarcity of training data forthis purpose. To overcome these challenges, we exploit the power of largelanguage models to introduce corruptions (i.e., noise) into text andsubsequently fine-tune compact editors to denoise the corruptions byincorporating relevant evidence. Our methodology is entirely unsupervised andprovides us with faux hallucinations for training in any domain. Our PetiteUnsupervised Research and Revision model, PURR, not only improves attributionover existing editing methods based on fine-tuning and prompting, but alsoachieves faster execution times by orders of magnitude."
Syntax-aware Hybrid prompt model for Few-shot multi-modal sentiment  analysis,"['Zikai Zhou', 'Haisong Feng', 'Baiyou Qiao', 'Gang Wu', 'Donghong Han']",http://arxiv.org/pdf/2306.01312v2.pdf,2023-06-02,['cs.cl'],"  Multimodal Sentiment Analysis (MSA) has been a popular topic in naturallanguage processing nowadays, at both sentence and aspect level. However, theexisting approaches almost require large-size labeled datasets, which bringabout large consumption of time and resources. Therefore, it is practical toexplore the method for few-shot sentiment analysis in cross-modalities.Previous works generally execute on textual modality, using the prompt-basedmethods, mainly two types: hand-crafted prompts and learnable prompts. Theexisting approach in few-shot multi-modality sentiment analysis task hasutilized both methods, separately. We further design a hybrid pattern that cancombine one or more fixed hand-crafted prompts and learnable prompts andutilize the attention mechanisms to optimize the prompt encoder. Theexperiments on both sentence-level and aspect-level datasets prove that we geta significant outperformance."
Scaling Sentence Embeddings with Large Language Models,"['Ting Jiang', 'Shaohan Huang', 'Zhongzhi Luan', 'Deqing Wang', 'Fuzhen Zhuang']",http://arxiv.org/pdf/2307.16645v1.pdf,2023-07-31,['cs.cl'],"  Large language models (LLMs) have recently garnered significant interest.With in-context learning, LLMs achieve impressive results in various naturallanguage tasks. However, the application of LLMs to sentence embeddings remainsan area of ongoing research. In this work, we propose an in-contextlearning-based method aimed at improving sentence embeddings performance. Ourapproach involves adapting the previous prompt-based representation method forautoregressive models, constructing a demonstration set that enables LLMs toperform in-context learning, and scaling up the LLMs to different model sizes.Through extensive experiments, in-context learning enables LLMs to generatehigh-quality sentence embeddings without any fine-tuning. It helps LLMs achieveperformance comparable to current contrastive learning methods. By scalingmodel size, we find scaling to more than tens of billion parameters harms theperformance on semantic textual similarity (STS) tasks. However, the largestmodel outperforms other counterparts and achieves the new state-of-the-artresult on transfer tasks. We also fine-tune LLMs with current contrastivelearning approach, and the 2.7B OPT model, incorporating our prompt-basedmethod, surpasses the performance of 4.8B ST5, achieving the newstate-of-the-art results on STS tasks. Our code is available athttps://github.com/kongds/scaling_sentemb."
Analyzing Modular Approaches for Visual Question Decomposition,"['Apoorv Khandelwal', 'Ellie Pavlick', 'Chen Sun']",http://arxiv.org/pdf/2311.06411v1.pdf,2023-11-10,"['cs.cv', 'cs.cl']","  Modular neural networks without additional training have recently been shownto surpass end-to-end neural networks on challenging vision-language tasks. Thelatest such methods simultaneously introduce LLM-based code generation to buildprograms and a number of skill-specific, task-oriented modules to execute them.In this paper, we focus on ViperGPT and ask where its additional performancecomes from and how much is due to the (state-of-art, end-to-end) BLIP-2 modelit subsumes vs. additional symbolic components. To do so, we conduct acontrolled study (comparing end-to-end, modular, and prompting-based methodsacross several VQA benchmarks). We find that ViperGPT's reported gains overBLIP-2 can be attributed to its selection of task-specific modules, and when werun ViperGPT using a more task-agnostic selection of modules, these gains goaway. Additionally, ViperGPT retains much of its performance if we makeprominent alterations to its selection of modules: e.g. removing or retainingonly BLIP-2. Finally, we compare ViperGPT against a prompting-baseddecomposition strategy and find that, on some benchmarks, modular approachessignificantly benefit by representing subtasks with natural language, insteadof code."
Personalized Jargon Identification for Enhanced Interdisciplinary  Communication,"['Yue Guo', 'Joseph Chee Chang', 'Maria Antoniak', 'Erin Bransom', 'Trevor Cohen', 'Lucy Lu Wang', 'Tal August']",http://arxiv.org/pdf/2311.09481v1.pdf,2023-11-16,['cs.cl'],"  Scientific jargon can impede researchers when they read materials from otherdomains. Current methods of jargon identification mainly use corpus-levelfamiliarity indicators (e.g., Simple Wikipedia represents plain language).However, researchers' familiarity of a term can vary greatly based on their ownbackground. We collect a dataset of over 10K term familiarity annotations from11 computer science researchers for terms drawn from 100 paper abstracts.Analysis of this data reveals that jargon familiarity and information needsvary widely across annotators, even within the same sub-domain (e.g., NLP). Weinvestigate features representing individual, sub-domain, and domain knowledgeto predict individual jargon familiarity. We compare supervised andprompt-based approaches, finding that prompt-based methods including personalpublications yields the highest accuracy, though zero-shot prompting provides astrong baseline. This research offers insight into features and methods tointegrate personal data into scientific jargon identification."
SYNC-CLIP: Synthetic Data Make CLIP Generalize Better in Data-Limited  Scenarios,"['Mushui Liu', 'Weijie He', 'Ziqian Lu', 'Yunlong Yu']",http://arxiv.org/pdf/2312.03805v1.pdf,2023-12-06,['cs.cv'],"  Prompt learning is a powerful technique for transferring Vision-LanguageModels (VLMs) such as CLIP to downstream tasks. However, the prompt-basedmethods that are fine-tuned solely with base classes may struggle to generalizeto novel classes in open-vocabulary scenarios, especially when data arelimited. To address this issue, we propose an innovative approach calledSYNC-CLIP that leverages SYNthetiC data for enhancing the generalizationcapability of CLIP. Based on the observation of the distribution shift betweenthe real and synthetic samples, we treat real and synthetic samples as distinctdomains and propose to optimize separate domain prompts to capturedomain-specific information, along with the shared visual prompts to preservethe semantic consistency between two domains. By aligning the cross-domainfeatures, the synthetic data from novel classes can provide implicit guidanceto rebalance the decision boundaries. Experimental results on three modelgeneralization tasks demonstrate that our method performs very competitivelyacross various benchmarks. Notably, SYNC-CLIP outperforms the state-of-the-artcompetitor PromptSRC by an average improvement of 3.0% on novel classes across11 datasets in open-vocabulary scenarios."
The ICL Consistency Test,"['Lucas Weber', 'Elia Bruni', 'Dieuwke Hupkes']",http://arxiv.org/pdf/2312.04945v1.pdf,2023-12-08,"['cs.cl', 'cs.ai', 'cs.lg']","  Just like the previous generation of task-tuned models, large language models(LLMs) that are adapted to tasks via prompt-based methods likein-context-learning (ICL) perform well in some setups but not in others. Thislack of consistency in prompt-based learning hints at a lack of robustgeneralisation. We here introduce the ICL consistency test -- a contribution tothe GenBench collaborative benchmark task (CBT) -- which evaluates howconsistent a model makes predictions across many different setups while usingthe same data. The test is based on different established natural languageinference tasks. We provide preprocessed data constituting 96 different'setups' and a metric that estimates model consistency across these setups. Themetric is provided on a fine-grained level to understand what properties of asetup render predictions unstable and on an aggregated level to compare overallmodel consistency. We conduct an empirical analysis of eight state-of-the-artmodels, and our consistency metric reveals how all tested LLMs lack robustgeneralisation."
Unified Multimodal Pre-training and Prompt-based Tuning for  Vision-Language Understanding and Generation,"['Tianyi Liu', 'Zuxuan Wu', 'Wenhan Xiong', 'Jingjing Chen', 'Yu-Gang Jiang']",http://arxiv.org/pdf/2112.05587v2.pdf,2021-12-10,"['cs.cv', 'cs.cl', 'cs.lg']","  Most existing vision-language pre-training methods focus on understandingtasks and use BERT-like objectives (masked language modeling and image-textmatching) during pretraining. Although they perform well in many understandingdownstream tasks, e.g., visual question answering, image-text retrieval andvisual entailment, they do not possess the ability to generate. To tackle thisproblem, we propose Unified multimodal pre-training for both Vision-Languageunderstanding and generation (UniVL). The proposed UniVL is capable of handlingboth understanding tasks and generative tasks. We augment existing pretrainingparadigms that only use random masks with causal masks, i.e., triangular masksthat mask out future tokens, such that the pre-trained models can haveautoregressive generation abilities by design. We formulate several previousunderstanding tasks as a text generation task and propose to use prompt-basedmethod for fine-tuning on different downstream tasks. Our experiments show thatthere is a trade-off between understanding tasks and generation tasks whileusing the same model, and a feasible way to improve both tasks is to use moredata. Our UniVL framework attains comparable performance to recentvision-language pre-training methods on both understanding tasks and generationtasks. Moreover, we demostrate that prompt-based finetuning is moredata-efficient - it outperforms discriminative methods in few-shot scenarios."
Learning to Transfer Prompts for Text Generation,"['Junyi Li', 'Tianyi Tang', 'Jian-Yun Nie', 'Ji-Rong Wen', 'Wayne Xin Zhao']",http://arxiv.org/pdf/2205.01543v2.pdf,2022-05-03,['cs.cl'],"  Pretrained language models (PLMs) have made remarkable progress in textgeneration tasks via fine-tuning. While, it is challenging to fine-tune PLMs ina data-scarce situation. Therefore, it is non-trivial to develop a general andlightweight model that can adapt to various text generation tasks based onPLMs. To fulfill this purpose, the recent prompt-based learning offers apotential solution. In this paper, we improve this technique and propose anovel prompt-based method (PTG) for text generation in a transferable setting.First, PTG learns a set of source prompts for various source generation tasksand then transfers these prompts as target prompts to perform target generationtasks. To consider both task- and instance-level information, we design anadaptive attention mechanism to derive the target prompts. For each datainstance, PTG learns a specific target prompt by attending to highly relevantsource prompts. In extensive experiments, PTG yields competitive or betterresults than fine-tuning methods. We release our source prompts as an openresource, where users can add or reuse them to improve new text generationtasks for future research. Code and data can be available athttps://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation."
On the Robustness of Dialogue History Representation in Conversational  Question Answering: A Comprehensive Study and a New Prompt-based Method,"['Zorik Gekhman', 'Nadav Oved', 'Orgad Keller', 'Idan Szpektor', 'Roi Reichart']",http://arxiv.org/pdf/2206.14796v2.pdf,2022-06-29,"['cs.cl', 'cs.ai', 'cs.lg']","  Most works on modeling the conversation history in Conversational QuestionAnswering (CQA) report a single main result on a common CQA benchmark. Whileexisting models show impressive results on CQA leaderboards, it remains unclearwhether they are robust to shifts in setting (sometimes to more realisticones), training data size (e.g. from large to small sets) and domain. In thiswork, we design and conduct the first large-scale robustness study of historymodeling approaches for CQA. We find that high benchmark scores do notnecessarily translate to strong robustness, and that various methods canperform extremely differently under different settings. Equipped with theinsights from our study, we design a novel prompt-based history modelingapproach, and demonstrate its strong robustness across various settings. Ourapproach is inspired by existing methods that highlight historic answers in thepassage. However, instead of highlighting by modifying the passage tokenembeddings, we add textual prompts directly in the passage text. Our approachis simple, easy-to-plug into practically any model, and highly effective, thuswe recommend it as a starting point for future model developers. We also hopethat our study and insights will raise awareness to the importance ofrobustness-focused evaluation, in addition to obtaining high leaderboardscores, leading to better CQA systems."
GPTs at Factify 2022: Prompt Aided Fact-Verification,"['Pawan Kumar Sahu', 'Saksham Aggarwal', 'Taneesh Gupta', 'Gyanendra Das']",http://arxiv.org/pdf/2206.14913v1.pdf,2022-06-29,['cs.cl'],"  One of the most pressing societal issues is the fight against false news. Thefalse claims, as difficult as they are to expose, create a lot of damage. Totackle the problem, fact verification becomes crucial and thus has been a topicof interest among diverse research communities. Using only the textual form ofdata we propose our solution to the problem and achieve competitive resultswith other approaches. We present our solution based on two approaches - PLM(pre-trained language model) based method and Prompt based method. ThePLM-based approach uses the traditional supervised learning, where the model istrained to take 'x' as input and output prediction 'y' as P(y|x). Whereas,Prompt-based learning reflects the idea to design input to fit the model suchthat the original objective may be re-framed as a problem of (masked) languagemodeling. We may further stimulate the rich knowledge provided by PLMs tobetter serve downstream tasks by employing extra prompts to fine-tune PLMs. Ourexperiments showed that the proposed method performs better than justfine-tuning PLMs. We achieved an F1 score of 0.6946 on the FACTIFY dataset anda 7th position on the competition leader-board."
Towards Realistic Low-resource Relation Extraction: A Benchmark with  Empirical Baseline Study,"['Xin Xu', 'Xiang Chen', 'Ningyu Zhang', 'Xin Xie', 'Xi Chen', 'Huajun Chen']",http://arxiv.org/pdf/2210.10678v3.pdf,2022-10-19,"['cs.cl', 'cs.ai', 'cs.ir', 'cs.lg']","  This paper presents an empirical study to build relation extraction systemsin low-resource settings. Based upon recent pre-trained language models, wecomprehensively investigate three schemes to evaluate the performance inlow-resource settings: (i) different types of prompt-based methods withfew-shot labeled data; (ii) diverse balancing methods to address thelong-tailed distribution issue; (iii) data augmentation technologies andself-training to generate more labeled in-domain data. We create a benchmarkwith 8 relation extraction (RE) datasets covering different languages, domainsand contexts and perform extensive comparisons over the proposed schemes withcombinations. Our experiments illustrate: (i) Though prompt-based tuning isbeneficial in low-resource RE, there is still much potential for improvement,especially in extracting relations from cross-sentence contexts with multiplerelational triples; (ii) Balancing methods are not always helpful for RE withlong-tailed distribution; (iii) Data augmentation complements existingbaselines and can bring much performance gain, while self-training may notconsistently achieve advancement to low-resource RE. Code and datasets are inhttps://github.com/zjunlp/LREBench."
PromptFusion: Decoupling Stability and Plasticity for Continual Learning,"['Haoran Chen', 'Zuxuan Wu', 'Xintong Han', 'Menglin Jia', 'Yu-Gang Jiang']",http://arxiv.org/pdf/2303.07223v1.pdf,2023-03-13,['cs.cv'],"  Continual learning refers to the capability of continuously learning from astream of data. Current research mainly focuses on relieving catastrophicforgetting, and most of their success is at the cost of limiting theperformance of newly incoming tasks. Such a trade-off is referred to as thestabilityplasticity dilemma and is a more general and challenging problem forcontinual learning. However, the inherent conflict between these two conceptsmakes it seemingly impossible to devise a satisfactory solution to both of themsimultaneously. Therefore, we ask, ""is it possible to divide them into twoproblems to conquer independently?"" To this end, we propose aprompt-tuning-based method termed PromptFusion to enable the decoupling ofstability and plasticity. Specifically, PromptFusion consists of a carefullydesigned Stabilizer module that deals with catastrophic forgetting and aBooster module to learn new knowledge concurrently. During training,PromptFusion first passes an input image to the two modules separately. Thenthe resulting logits are further fused with a learnable weight parameter.Finally, a weight mask is applied to the derived logits to balance between oldand new classes. Extensive experiments show that our method achieves promisingresults on popular continual learning datasets for both class-incremental anddomain incremental settings. Especially on Split-Imagenet-R, one of the mostchallenging datasets for class-incremental learning, our method exceedsstate-of-the-art prompt-based methods L2P and DualPrompt by more than 10%."
Progressive Visual Prompt Learning with Contrastive Feature Re-formation,"['Chen Xu', 'Haocheng Shen', 'Fengyuan Shi', 'Boheng Chen', 'Yixuan Liao', 'Xiaoxin Chen', 'Limin Wang']",http://arxiv.org/pdf/2304.08386v1.pdf,2023-04-17,['cs.cv'],"  Prompt learning has been designed as an alternative to fine-tuning foradapting Vision-language (V-L) models to the downstream tasks. Previous worksmainly focus on text prompt while visual prompt works are limited for V-Lmodels. The existing visual prompt methods endure either mediocre performanceor unstable training process, indicating the difficulty of visual promptlearning. In this paper, we propose a new Progressive Visual Prompt (ProVP)structure to strengthen the interactions among prompts of different layers.More importantly, our ProVP could effectively propagate the image embeddings todeep layers and behave partially similar to an instance adaptive prompt method.To alleviate generalization deterioration, we further propose a new contrastivefeature re-formation, which prevents the serious deviation of the promptedvisual feature from the fixed CLIP visual feature distribution. Combining both,our method (ProVP-Ref) is evaluated on 11 image benchmark datasets and achieves7/11 state-of-theart results on both few-shot and base-to-novel settings. Tothe best of our knowledge, we are the first to demonstrate the superiorperformance of visual prompts in V-L models to previous prompt-based methods indownstream tasks. Meanwhile, it implies that our ProVP-Ref shows the bestcapability to adapt and to generalize."
SelfEvolve: A Code Evolution Framework via Large Language Models,"['Shuyang Jiang', 'Yuhao Wang', 'Yu Wang']",http://arxiv.org/pdf/2306.02907v1.pdf,2023-06-05,"['cs.cl', 'cs.se']","  Large language models (LLMs) have already revolutionized code generation,after being pretrained on publicly available code data. However, while variousmethods have been proposed to augment LLMs with retrieved knowledge and enhancethe quality of code generation, the performance of these retrieval-basedmethods is limited by the strength of the retrievers used. In addition, whileLLMs show great emergent ability, they still struggle to produce the correctcode in one turn. To address these challenges, we propose a novel two-steppipeline, called \autoknow, that leverages LLMs as both knowledge providers andself-reflective programmers. Unlike retrieval-based methods, \autoknow~obtainsthe knowledge from input prompts and generates intermediate code based on thegenerated knowledge. After that, \autoknow~asks LLM to act as an expertprogrammer to perform debugging for the generated code. This is achieved byreceiving the error message from the interpreter, without requiring specialtest cases for correctness verification. We evaluate \autoknow~on three codegeneration datasets, including DS-1000 for data science code, HumanEval forsoftware engineering code, and TransCoder for C++-to-Python translation. Ourempirical experiments show that \autoknow~outperforms strong baselines by asignificant margin on all datasets. We also conduct exhaustive analyticalexperiments to validate the effectiveness of the two stages of \autoknow, andfind that both are superior to other prompting-based methods. Furtherscalability analysis demonstrates that \autoknow~can be adapted to other moreadvanced models, such as GPT-4, and bring consistent efficacy improvement."
Quantifying Language Models' Sensitivity to Spurious Features in Prompt  Design or: How I learned to start worrying about prompt formatting,"['Melanie Sclar', 'Yejin Choi', 'Yulia Tsvetkov', 'Alane Suhr']",http://arxiv.org/pdf/2310.11324v1.pdf,2023-10-17,"['cs.cl', 'cs.ai', 'cs.lg']","  As large language models (LLMs) are adopted as a fundamental component oflanguage technologies, it is crucial to accurately characterize theirperformance. Because choices in prompt design can strongly influence modelbehavior, this design process is critical in effectively using any modernpre-trained generative language model. In this work, we focus on LLMsensitivity to a quintessential class of meaning-preserving design choices:prompt formatting. We find that several widely used open-source LLMs areextremely sensitive to subtle changes in prompt formatting in few-shotsettings, with performance differences of up to 76 accuracy points whenevaluated using LLaMA-2-13B. Sensitivity remains even when increasing modelsize, the number of few-shot examples, or performing instruction tuning. Ouranalysis suggests that work evaluating LLMs with prompting-based methods wouldbenefit from reporting a range of performance across plausible prompt formats,instead of the currently-standard practice of reporting performance on a singleformat. We also show that format performance only weakly correlates betweenmodels, which puts into question the methodological validity of comparingmodels with an arbitrarily chosen, fixed prompt format. To facilitatesystematic analysis we propose FormatSpread, an algorithm that rapidlyevaluates a sampled set of plausible prompt formats for a given task, andreports the interval of expected performance without accessing model weights.Furthermore, we present a suite of analyses that characterize the nature ofthis sensitivity, including exploring the influence of particular atomicperturbations and the internal representation of particular formats."
GPT-3-driven pedagogical agents for training children's curious  question-asking skills,"['Rania Abdelghani', 'Yen-Hsiang Wang', 'Xingdi Yuan', 'Tong Wang', 'Pauline Lucas', 'Hélène Sauzéon', 'Pierre-Yves Oudeyer']",http://arxiv.org/pdf/2211.14228v6.pdf,2022-11-25,"['cs.cl', 'cs.hc']","  In order to train children's ability to ask curiosity-driven questions,previous research has explored designing specific exercises relying onproviding semantic and linguistic cues to help formulate such questions. Butdespite showing pedagogical efficiency, this method is still limited as itrelies on generating the said cues by hand, which can be a very costly process.In this context, we propose to leverage advances in the natural languageprocessing field (NLP) and investigate the efficiency of using a large languagemodel (LLM) for automating the production of the pedagogical content of acurious question-asking (QA) training. We study generating the said contentusing the ""prompt-based"" method that consists of explaining the task to the LLMin natural text. We evaluate the output using human experts annotations andcomparisons with hand-generated content. Results suggested indeed the relevanceand usefulness of this content. We also conduct a field study in primary school(75 children aged 9-10), where we evaluate children's QA performance whenhaving this training. We compare 3 types of content : 1) hand-generated contentthat proposes ""closed"" cues leading to predefined questions; 2) GPT-3-generatedcontent that proposes the same type of cues; 3) GPT-3-generated content thatproposes ""open"" cues leading to several possible questions. We see a similar QAperformance between the two ""closed"" trainings (showing the scalability of theapproach using GPT-3), and a better one for participants with the ""open""training. These results suggest the efficiency of using LLMs to supportchildren in generating more curious questions, using a natural languageprompting approach that affords usability by teachers and other users notspecialists of AI techniques. Furthermore, results also show that open-endedcontent may be more suitable for training curious question-asking skills."
Towards using Few-Shot Prompt Learning for Automating Model Completion,"['Meriem Ben Chaaben', 'Lola Burgueño', 'Houari Sahraoui']",http://arxiv.org/pdf/2212.03404v1.pdf,2022-12-07,"['cs.se', 'cs.cl']",  We propose a simple yet a novel approach to improve completion in domainmodeling activities. Our approach exploits the power of large language modelsby using few-shot prompt learning without the need to train or fine-tune thosemodels with large datasets that are scarce in this field. We implemented ourapproach and tested it on the completion of static and dynamic domain diagrams.Our initial evaluation shows that such an approach is effective and can beintegrated in different ways during the modeling activities.
Are Prompt-based Models Clueless?,"['Pride Kavumba', 'Ryo Takahashi', 'Yusuke Oda']",http://arxiv.org/pdf/2205.09295v2.pdf,2022-05-19,['cs.cl'],"  Finetuning large pre-trained language models with a task-specific head hasadvanced the state-of-the-art on many natural language understandingbenchmarks. However, models with a task-specific head require a lot of trainingdata, making them susceptible to learning and exploiting dataset-specificsuperficial cues that do not generalize to other datasets. Prompting hasreduced the data requirement by reusing the language model head and formattingthe task input to match the pre-training objective. Therefore, it is expectedthat few-shot prompt-based models do not exploit superficial cues. This paperpresents an empirical examination of whether few-shot prompt-based models alsoexploit superficial cues. Analyzing few-shot prompt-based models on MNLI, SNLI,HANS, and COPA has revealed that prompt-based models also exploit superficialcues. While the models perform well on instances with superficial cues, theyoften underperform or only marginally outperform random accuracy on instanceswithout superficial cues."
Decomposed Prompting for Machine Translation Between Related Languages  using Large Language Models,"['Ratish Puduppully', 'Anoop Kunchukuttan', 'Raj Dabre', 'Ai Ti Aw', 'Nancy F. Chen']",http://arxiv.org/pdf/2305.13085v2.pdf,2023-05-22,['cs.cl'],"  This study investigates machine translation between related languages i.e.,languages within the same family that share linguistic characteristics such asword order and lexical similarity. Machine translation through few-shotprompting leverages a small set of translation pair examples to generatetranslations for test sentences. This procedure requires the model to learn howto generate translations while simultaneously ensuring that token ordering ismaintained to produce a fluent and accurate translation. We propose that forrelated languages, the task of machine translation can be simplified byleveraging the monotonic alignment characteristic of such languages. Weintroduce DecoMT, a novel approach of few-shot prompting that decomposes thetranslation process into a sequence of word chunk translations. Throughautomatic and human evaluation conducted on multiple related language pairsacross various language families, we demonstrate that our proposed approach ofdecomposed prompting surpasses multiple established few-shot baselineapproaches. For example, DecoMT outperforms the strong few-shot prompting BLOOMmodel with an average improvement of 8 chrF++ scores across the examinedlanguages."
Multilingual Large Language Models Are Not (Yet) Code-Switchers,"['Ruochen Zhang', 'Samuel Cahyawijaya', 'Jan Christian Blaise Cruz', 'Genta Indra Winata', 'Alham Fikri Aji']",http://arxiv.org/pdf/2305.14235v2.pdf,2023-05-23,"['cs.cl', 'cs.ai']","  Multilingual Large Language Models (LLMs) have recently shown greatcapabilities in a wide range of tasks, exhibiting state-of-the-art performancethrough zero-shot or few-shot prompting methods. While there have beenextensive studies on their abilities in monolingual tasks, the investigation oftheir potential in the context of code-switching (CSW), the practice ofalternating languages within an utterance, remains relatively uncharted. Inthis paper, we provide a comprehensive empirical analysis of variousmultilingual LLMs, benchmarking their performance across four tasks: sentimentanalysis, machine translation, summarization and word-level languageidentification. Our results indicate that despite multilingual LLMs exhibitingpromising outcomes in certain tasks using zero or few-shot prompting, theystill underperform in comparison to fine-tuned models of much smaller scales.We argue that current ""multilingualism"" in LLMs does not inherently implyproficiency with code-switching texts, calling for future research to bridgethis discrepancy."
"Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango","['Aman Madaan', 'Amir Yazdanbakhsh']",http://arxiv.org/pdf/2209.07686v2.pdf,2022-09-16,"['cs.cl', 'cs.ai', 'cs.lg']","  The past decade has witnessed dramatic gains in natural language processingand an unprecedented scaling of large language models. These developments havebeen accelerated by the advent of few-shot techniques such as chain of thought(CoT) prompting. Specifically, CoT pushes the performance of large languagemodels in a few-shot setup by augmenting the prompts with intermediate steps.Despite impressive results across various tasks, the reasons behind theirsuccess have not been explored. This work uses counterfactual prompting todevelop a deeper understanding of CoT-based few-shot prompting mechanisms inlarge language models. We first systematically identify and define the keycomponents of a prompt: symbols, patterns, and text. Then, we devise andconduct an exhaustive set of experiments across four different tasks, byquerying the model with counterfactual prompts where only one of thesecomponents is altered. Our experiments across three models (PaLM, GPT-3, andCODEX) reveal several surprising findings and brings into question theconventional wisdom around few-shot prompting. First, the presence of factualpatterns in a prompt is practically immaterial to the success of CoT. Second,our results conclude that the primary role of intermediate steps may not be tofacilitate learning how to solve a task. The intermediate steps are rather abeacon for the model to realize what symbols to replicate in the output to forma factual answer. Further, text imbues patterns with commonsense knowledge andmeaning. Our empirical and qualitative analysis reveals that a symbioticrelationship between text and patterns explains the success of few-shotprompting: text helps extract commonsense from the question to help patterns,and patterns enforce task understanding and direct text generation."
Understanding How Model Size Affects Few-shot Instruction Prompting,"['Ayrton San Joaquin', 'Ardy Haroen']",http://arxiv.org/pdf/2212.01907v1.pdf,2022-12-04,"['cs.cl', 'cs.lg', 'stat.ml']","  Large Language Models are affected by the phenomena of memorizing andforgetting their training data. But how do these vary by model size? We worktowards this question by investigating how the model size affects the model'sability to discriminate a word's meaning in a given context. We introduce adataset called DeltaWords, which evaluates a model's ability to followinstructions to select a sentence which replaces the target word with itsantonym. We show a weak inverse scaling trend, where task accuracy degrades asmodel size increase, under extremely few-shot prompting regimes. We show thatincreasing the number of examples tend to disproportionately benefit largermodels than smaller models."
Prompted LLMs as Chatbot Modules for Long Open-domain Conversation,"['Gibbeum Lee', 'Volker Hartmann', 'Jongho Park', 'Dimitris Papailiopoulos', 'Kangwook Lee']",http://arxiv.org/pdf/2305.04533v1.pdf,2023-05-08,"['cs.cl', 'cs.ai', 'cs.lg']","  In this paper, we propose MPC (Modular Prompted Chatbot), a new approach forcreating high-quality conversational agents without the need for fine-tuning.Our method utilizes pre-trained large language models (LLMs) as individualmodules for long-term consistency and flexibility, by using techniques such asfew-shot prompting, chain-of-thought (CoT), and external memory. Our humanevaluation results show that MPC is on par with fine-tuned chatbot models inopen-domain conversations, making it an effective solution for creatingconsistent and engaging chatbots."
Internet-augmented language models through few-shot prompting for  open-domain question answering,"['Angeliki Lazaridou', 'Elena Gribovskaya', 'Wojciech Stokowiec', 'Nikolai Grigorev']",http://arxiv.org/pdf/2203.05115v2.pdf,2022-03-10,"['cs.cl', 'cs.lg']","  In this work, we aim to capitalize on the unique few-shot capabilities oflarge-scale language models (LSLMs) to overcome some of their challenges withrespect to grounding to factual and up-to-date information. Motivated bysemi-parametric language models (LMs), which ground their decisions in externalretrieved evidence, we use few-shot prompting to learn to condition LMs oninformation returned from the web using Google Search, a broad and constantlyupdated knowledge source. Our approach does not involve fine-tuning or learningadditional parameters, thus making it applicable to any LM, offering thereforea strong baseline. Indeed, we find that LMs conditioned on the web surpassperformance of closed-book models of similar, or even larger, model sizes inopen-domain question answering. Finally, we find that increasing theinference-time compute of models, achieved via using multiple retrievedevidences to generate multiple answers followed by a reranking stage that usesscores generated by the same LMs, leads to better performance and alleviateslower performance of smaller few-shot LMs. All in all, our findings suggestthat it might be beneficial to slow down the race towards the biggest model andinstead shift attention towards finding more effective ways to use models,including but not limited to, better prompting or increasing inference-timecompute."
Decomposed Prompting: A Modular Approach for Solving Complex Tasks,"['Tushar Khot', 'Harsh Trivedi', 'Matthew Finlayson', 'Yao Fu', 'Kyle Richardson', 'Peter Clark', 'Ashish Sabharwal']",http://arxiv.org/pdf/2210.02406v2.pdf,2022-10-05,['cs.cl'],"  Few-shot prompting is a surprisingly powerful way to use Large LanguageModels (LLMs) to solve various tasks. However, this approach struggles as thetask complexity increases or when the individual reasoning steps of the taskthemselves are hard to learn, especially when embedded in more complex tasks.To address this, we propose Decomposed Prompting, a new approach to solvecomplex tasks by decomposing them (via prompting) into simpler sub-tasks thatcan be delegated to a library of prompting-based LLMs dedicated to thesesub-tasks. This modular structure allows each prompt to be optimized for itsspecific sub-task, further decomposed if necessary, and even easily replacedwith more effective prompts, trained models, or symbolic functions if desired.We show that the flexibility and modularity of Decomposed Prompting allows itto outperform prior work on few-shot prompting using GPT3. On symbolicreasoning tasks, we can further decompose sub-tasks that are hard for LLMs intoeven simpler solvable sub-tasks. When the complexity comes from the inputlength, we can recursively decompose the task into the same task but withsmaller inputs. We also evaluate our approach on textual multi-step reasoningtasks: on long-context multi-hop QA task, we can more effectively teach thesub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,we can incorporate a symbolic information retrieval within our decompositionframework, leading to improved performance on both tasks. Datasets, Code andPrompts available at https://github.com/allenai/DecomP."
Language Model Crossover: Variation through Few-Shot Prompting,"['Elliot Meyerson', 'Mark J. Nelson', 'Herbie Bradley', 'Adam Gaier', 'Arash Moradi', 'Amy K. Hoover', 'Joel Lehman']",http://arxiv.org/pdf/2302.12170v2.pdf,2023-02-23,['cs.ne'],"  This paper pursues the insight that language models naturally enable anintelligent variation operator similar in spirit to evolutionary crossover. Inparticular, language models of sufficient scale demonstrate in-contextlearning, i.e. they can learn from associations between a small number of inputpatterns to generate outputs incorporating such associations (also calledfew-shot prompting). This ability can be leveraged to form a simple butpowerful variation operator, i.e. to prompt a language model with a fewtext-based genotypes (such as code, plain-text sentences, or equations), and toparse its corresponding output as those genotypes' offspring. The promise ofsuch language model crossover (which is simple to implement and can leveragemany different open-source language models) is that it enables a simplemechanism to evolve semantically-rich text representations (with fewdomain-specific tweaks), and naturally benefits from current progress inlanguage models. Experiments in this paper highlight the versatility oflanguage-model crossover, through evolving binary bit-strings, sentences,equations, text-to-image prompts, and Python code. The conclusion is thatlanguage model crossover is a promising method for evolving genomesrepresentable as text."
Distilling Step-by-Step! Outperforming Larger Language Models with Less  Training Data and Smaller Model Sizes,"['Cheng-Yu Hsieh', 'Chun-Liang Li', 'Chih-Kuan Yeh', 'Hootan Nakhost', 'Yasuhisa Fujii', 'Alexander Ratner', 'Ranjay Krishna', 'Chen-Yu Lee', 'Tomas Pfister']",http://arxiv.org/pdf/2305.02301v2.pdf,2023-05-03,"['cs.cl', 'cs.ai', 'cs.lg']","  Deploying large language models (LLMs) is challenging because they are memoryinefficient and compute-intensive for practical applications. In reaction,researchers train smaller task-specific models by either finetuning with humanlabels or distilling using LLM-generated labels. However, finetuning anddistillation require large amounts of training data to achieve comparableperformance to LLMs. We introduce Distilling step-by-step, a new mechanism that(a) trains smaller models that outperform LLMs, and (b) achieves so byleveraging less training data needed by finetuning or distillation. Our methodextracts LLM rationales as additional supervision for training small modelswithin a multi-task framework. We present three findings across 4 NLPbenchmarks: First, compared to both finetuning and distillation, our mechanismachieves better performance with much fewer labeled/unlabeled trainingexamples. Second, compared to few-shot prompted LLMs, we achieve betterperformance using substantially smaller model sizes. Third, we reduce both themodel size and the amount of data required to outperform LLMs; our finetuned770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80%of available data on a benchmark, whereas standard finetuning the same T5 modelstruggles to match even by using 100% of the dataset. We release the code at:https://github.com/google-research/distilling-step-by-step ."
Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning,"['Zhanming Jie', 'Wei Lu']",http://arxiv.org/pdf/2305.18170v2.pdf,2023-05-29,['cs.cl'],"  Chain-of-thought (CoT) prompting with large language models has proveneffective in numerous natural language processing tasks, but designing promptsthat generalize well to diverse problem types can be challenging, especially inthe context of math word problem (MWP) solving. Additionally, it is common tohave a large amount of training data that have a better diversity coverage butCoT annotations are not available, which limits the use of supervised learningtechniques. To address these issues, we investigate two approaches to leveragethe training data in a few-shot prompting scenario: dynamic program promptingand program distillation. Our approach is largely inspired by Gao et al.,(2022), where they proposed to replace the CoT with the programs as theintermediate reasoning step. Such a prompting strategy allows us to accuratelyverify the answer correctness through program execution in MWP solving. Ourdynamic program prompting involves annotating the training data by samplingcorrect programs from a large language model, while program distillationinvolves adapting a smaller model to the program-annotated training data. Ourexperiments on three standard MWP datasets demonstrate the effectiveness ofthese approaches, yielding significant improvements over previous baselines forprompting and fine-tuning. Our results suggest that leveraging a large amountof training data can improve the generalization ability of prompts and boostthe performance of fine-tuned small models in MWP solving."
Zero- and Few-Shot Prompting with LLMs: A Comparative Study with  Fine-tuned Models for Bangla Sentiment Analysis,"['Md. Arid Hasan', 'Shudipta Das', 'Afiyat Anjum', 'Firoj Alam', 'Anika Anjum', 'Avijit Sarker', 'Sheak Rashed Haider Noori']",http://arxiv.org/pdf/2308.10783v1.pdf,2023-08-21,"['cs.cl', 'cs.lg', '68t50', 'i.2.7']","  The rapid expansion of the digital world has propelled sentiment analysisinto a critical tool across diverse sectors such as marketing, politics,customer service, and healthcare. While there have been significantadvancements in sentiment analysis for widely spoken languages, low-resourcelanguages, such as Bangla, remain largely under-researched due to resourceconstraints. Furthermore, the recent unprecedented performance of LargeLanguage Models (LLMs) in various applications highlights the need to evaluatethem in the context of low-resource languages. In this study, we present asizeable manually annotated dataset encompassing 33,605 Bangla news tweets andFacebook comments. We also investigate zero- and few-shot in-context learningwith several language models, including Flan-T5, GPT-4, and Bloomz, offering acomparative analysis against fine-tuned models. Our findings suggest thatmonolingual transformer-based models consistently outperform other models, evenin zero and few-shot scenarios. To foster continued exploration, we intend tomake this dataset and our research tools publicly available to the broaderresearch community. In the spirit of further research, we plan to make thisdataset and our experimental resources publicly accessible to the widerresearch community."
FOLIO: Natural Language Reasoning with First-Order Logic,"['Simeng Han', 'Hailey Schoelkopf', 'Yilun Zhao', 'Zhenting Qi', 'Martin Riddell', 'Luke Benson', 'Lucy Sun', 'Ekaterina Zubova', 'Yujie Qiao', 'Matthew Burtell', 'David Peng', 'Jonathan Fan', 'Yixin Liu', 'Brian Wong', 'Malcolm Sailor', 'Ansong Ni', 'Linyong Nan', 'Jungo Kasai', 'Tao Yu', 'Rui Zhang', 'Shafiq Joty', 'Alexander R. Fabbri', 'Wojciech Kryscinski', 'Xi Victoria Lin', 'Caiming Xiong', 'Dragomir Radev']",http://arxiv.org/pdf/2209.00840v1.pdf,2022-09-02,['cs.cl'],"  We present FOLIO, a human-annotated, open-domain, and logically complex anddiverse dataset for reasoning in natural language (NL), equipped with firstorder logic (FOL) annotations. FOLIO consists of 1,435 examples (uniqueconclusions), each paired with one of 487 sets of premises which serve as rulesto be used to deductively reason for the validity of each conclusion. Thelogical correctness of premises and conclusions is ensured by their parallelFOL annotations, which are automatically verified by our FOL inference engine.In addition to the main NL reasoning task, NL-FOL pairs in FOLIO automaticallyconstitute a new NL-FOL translation dataset using FOL as the logical form. Ourexperiments on FOLIO systematically evaluate the FOL reasoning ability ofsupervised fine-tuning on medium-sized language models (BERT, RoBERTa) andfew-shot prompting on large language models (GPT-NeoX, OPT, GPT-3, Codex). ForNL-FOL translation, we experiment with GPT-3 and Codex. Our results show thatone of the most capable Large Language Model (LLM) publicly available, GPT-3davinci, achieves only slightly better than random results with few-shotprompting on a subset of FOLIO, and the model is especially bad at predictingthe correct truth values for False and Unknown conclusions. Our dataset andcode are available at https://github.com/Yale-LILY/FOLIO."
Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"['Mirac Suzgun', 'Nathan Scales', 'Nathanael Schärli', 'Sebastian Gehrmann', 'Yi Tay', 'Hyung Won Chung', 'Aakanksha Chowdhery', 'Quoc V. Le', 'Ed H. Chi', 'Denny Zhou', 'Jason Wei']",http://arxiv.org/pdf/2210.09261v1.pdf,2022-10-17,"['cs.cl', 'cs.ai']","  BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite thatfocuses on tasks believed to be beyond the capabilities of current languagemodels. Language models have already made good progress on this benchmark, withthe best model in the BIG-Bench paper outperforming average reportedhuman-rater results on 65% of the BIG-Bench tasks via few-shot prompting. Buton what tasks do language models fall short of average human-rater performance,and are those tasks actually unsolvable by current language models?  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which wecall BIG-Bench Hard (BBH). These are the task for which prior language modelevaluations did not outperform the average human-rater. We find that applyingchain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass theaverage human-rater performance on 10 of the 23 tasks, and Codex(code-davinci-002) to surpass the average human-rater performance on 17 of the23 tasks. Since many tasks in BBH require multi-step reasoning, few-shotprompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al.,2022), substantially underestimates the best performance and capabilities oflanguage models, which is better captured via CoT prompting. As furtheranalysis, we explore the interaction between CoT and model scale on BBH,finding that CoT enables emergent task performance on several BBH tasks withotherwise flat scaling curves."
Mental-LLM: Leveraging Large Language Models for Mental Health  Prediction via Online Text Data,"['Xuhai Xu', 'Bingsheng Yao', 'Yuanzhe Dong', 'Saadia Gabriel', 'Hong Yu', 'James Hendler', 'Marzyeh Ghassemi', 'Anind K. Dey', 'Dakuo Wang']",http://arxiv.org/pdf/2307.14385v3.pdf,2023-07-26,"['cs.cl', '68u35', 'h.5.2; i.2.m']","  Advances in large language models (LLMs) have empowered a variety ofapplications. However, there is still a significant gap in research when itcomes to understanding and enhancing the capabilities of LLMs in the field ofmental health. In this work, we present the first comprehensive evaluation ofmultiple LLMs, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4, onvarious mental health prediction tasks via online text data. We conduct a broadrange of experiments, covering zero-shot prompting, few-shot prompting, andinstruction fine-tuning. The results indicate a promising yet limitedperformance of LLMs with zero-shot and few-shot prompt designs for the mentalhealth tasks. More importantly, our experiments show that instructionfinetuning can significantly boost the performance of LLMs for all taskssimultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5,outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9%on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%.They further perform on par with the state-of-the-art task-specific languagemodel. We also conduct an exploratory case study on LLMs' capability on themental health reasoning tasks, illustrating the promising capability of certainmodels such as GPT-4. We summarize our findings into a set of action guidelinesfor potential methods to enhance LLMs' capability for mental health tasks.Meanwhile, we also emphasize the important limitations before achievingdeployability in real-world mental health settings, such as known racial andgender bias. We highlight the important ethical risks accompanying this line ofresearch."
Prompt Programming for Large Language Models: Beyond the Few-Shot  Paradigm,"['Laria Reynolds', 'Kyle McDonell']",http://arxiv.org/pdf/2102.07350v1.pdf,2021-02-15,"['cs.cl', 'cs.ai']","  Prevailing methods for mapping large generative language models to supervisedtasks may fail to sufficiently probe models' novel capabilities. Using GPT-3 asa case study, we show that 0-shot prompts can significantly outperform few-shotprompts. We suggest that the function of few-shot examples in these cases isbetter described as locating an already learned task rather than meta-learning.This analysis motivates rethinking the role of prompts in controlling andevaluating powerful language models. In this work, we discuss methods of promptprogramming, emphasizing the usefulness of considering prompts through the lensof natural language. We explore techniques for exploiting the capacity ofnarratives and cultural anchors to encode nuanced intentions and techniques forencouraging deconstruction of a problem into components before producing averdict. Informed by this more encompassing theory of prompt programming, wealso introduce the idea of a metaprompt that seeds the model to generate itsown natural language prompts for a range of tasks. Finally, we discuss howthese more general methods of interacting with language models can beincorporated into existing and future benchmarks and practical applications."
Fantastically Ordered Prompts and Where to Find Them: Overcoming  Few-Shot Prompt Order Sensitivity,"['Yao Lu', 'Max Bartolo', 'Alastair Moore', 'Sebastian Riedel', 'Pontus Stenetorp']",http://arxiv.org/pdf/2104.08786v2.pdf,2021-04-18,"['cs.cl', 'cs.ai']","  When primed with only a handful of training samples, very large, pretrainedlanguage models such as GPT-3 have shown competitive results when compared tofully-supervised, fine-tuned, large, pretrained language models. We demonstratethat the order in which the samples are provided can make the differencebetween near state-of-the-art and random guess performance: essentially somepermutations are ""fantastic"" and some not. We analyse this phenomenon indetail, establishing that: it is present across model sizes (even for thelargest current models), it is not related to a specific subset of samples, andthat a given good permutation for one model is not transferable to another.While one could use a development set to determine which permutations areperformant, this would deviate from the true few-shot setting as it requiresadditional annotated data. Instead, we use the generative nature of languagemodels to construct an artificial development set and based on entropystatistics of the candidate permutations on this set, we identify performantprompts. Our method yields a 13% relative improvement for GPT-family modelsacross eleven different established text classification tasks."
Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning,"['Prasetya Ajie Utama', 'Nafise Sadat Moosavi', 'Victor Sanh', 'Iryna Gurevych']",http://arxiv.org/pdf/2109.04144v1.pdf,2021-09-09,"['cs.cl', 'cs.ai']","  Recent prompt-based approaches allow pretrained language models to achievestrong performances on few-shot finetuning by reformulating downstream tasks asa language modeling problem. In this work, we demonstrate that, despite itsadvantages on low data regimes, finetuned prompt-based models for sentence pairclassification tasks still suffer from a common pitfall of adopting inferenceheuristics based on lexical overlap, e.g., models incorrectly assuming asentence pair is of the same meaning because they consist of the same set ofwords. Interestingly, we find that this particular inference heuristic issignificantly less present in the zero-shot evaluation of the prompt-basedmodel, indicating how finetuning can be destructive to useful knowledge learnedduring the pretraining. We then show that adding a regularization thatpreserves pretraining weights is effective in mitigating this destructivetendency of few-shot finetuning. Our evaluation on three datasets demonstratespromising improvements on the three corresponding challenge datasets used todiagnose the inference heuristics."
Towards Zero-Label Language Learning,"['Zirui Wang', 'Adams Wei Yu', 'Orhan Firat', 'Yuan Cao']",http://arxiv.org/pdf/2109.09193v1.pdf,2021-09-19,"['cs.cl', 'cs.lg']","  This paper explores zero-label learning in Natural Language Processing (NLP),whereby no human-annotated data is used anywhere during training and models aretrained purely on synthetic data. At the core of our framework is a novelapproach for better leveraging the powerful pretrained language models.Specifically, inspired by the recent success of few-shot inference on GPT-3, wepresent a training data creation procedure named Unsupervised Data Generation(UDG), which leverages few-shot prompts to synthesize high-quality trainingdata without real human annotations. Our method enables zero-label learning aswe train task-specific models solely on the synthetic data, yet we achievebetter or comparable results from strong baseline models trained onhuman-labeled data. Furthermore, when mixed with labeled data, our approachserves as a highly effective data augmentation procedure, achieving newstate-of-the-art results on the SuperGLUE benchmark."
P4E: Few-Shot Event Detection as Prompt-Guided Identification and  Localization,"['Sha Li', 'Liyuan Liu', 'Yiqing Xie', 'Heng Ji', 'Jiawei Han']",http://arxiv.org/pdf/2202.07615v3.pdf,2022-02-15,['cs.cl'],"  We propose P4E, an identify-and-localize event detection framework thatintegrates the best of few-shot prompting and structured prediction. Ourframework decomposes event detection into an identification task and alocalization task. For the identification task, which we formulate asmulti-label classification, we leverage cloze-based prompting to align ourobjective with the pre-training task of language models, allowing our model toquickly adapt to new event types. We then employ an event type-agnosticsequence labeling model to localize the event trigger conditioned on theidentification output. This heterogeneous model design allows P4E to quicklylearn new event types without sacrificing the ability to make structuredpredictions. Our experiments demonstrate the effectiveness of our proposeddesign, and P4E shows superior performance for few-shot event detection onbenchmark datasets FewEvent and MAVEN and comparable performance to SOTA forfully-supervised event detection on ACE."
Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual  Style Transfer with Small Language Models,"['Mirac Suzgun', 'Luke Melas-Kyriazi', 'Dan Jurafsky']",http://arxiv.org/pdf/2205.11503v1.pdf,2022-05-23,['cs.cl'],"  We propose a method for arbitrary textual style transfer (TST)--the task oftransforming a text into any given style--utilizing general-purpose pre-trainedlanguage models. Our method, Prompt-and-Rerank, is based on a mathematicalformulation of the TST task, decomposing it into three constituent components:textual similarity, target style strength, and fluency. Specifically, ourmethod first uses zero-shot or few-shot prompting to obtain a set of candidategenerations in the target style, and then re-ranks these candidates accordingto a combination of the three components above. Empirically, our method enablessmall pre-trained language models to perform on par with state-of-the-artlarge-scale models while consuming two orders of magnitude less compute andmemory. Finally, we conduct a systematic investigation of the effect of modelsize and prompt design (e.g., prompt paraphrasing and delimiter-pair choice) onstyle transfer quality across seven diverse textual style transfer datasets."
Bootstrapping Multilingual Semantic Parsers using Large Language Models,"['Abhijeet Awasthi', 'Nitish Gupta', 'Bidisha Samanta', 'Shachi Dave', 'Sunita Sarawagi', 'Partha Talukdar']",http://arxiv.org/pdf/2210.07313v2.pdf,2022-10-13,"['cs.cl', 'cs.lg']","  Despite cross-lingual generalization demonstrated by pre-trained multilingualmodels, the translate-train paradigm of transferring English datasets acrossmultiple languages remains to be a key mechanism for training task-specificmultilingual models. However, for many low-resource languages, the availabilityof a reliable translation service entails significant amounts of costlyhuman-annotated translation pairs. Further, translation services may continueto be brittle due to domain mismatch between task-specific input text andgeneral-purpose text used for training translation models. For multilingualsemantic parsing, we demonstrate the effectiveness and flexibility offered bylarge language models (LLMs) for translating English datasets into severallanguages via few-shot prompting. Through extensive comparisons on two publicdatasets, MTOP and MASSIVE, spanning 50 languages and several domains, we showthat our method of translating data using LLMs outperforms a strongtranslate-train baseline on 41 out of 50 languages. We study the key designchoices that enable more effective multilingual data translation via promptedLLMs."
Prompting GPT-3 To Be Reliable,"['Chenglei Si', 'Zhe Gan', 'Zhengyuan Yang', 'Shuohang Wang', 'Jianfeng Wang', 'Jordan Boyd-Graber', 'Lijuan Wang']",http://arxiv.org/pdf/2210.09150v2.pdf,2022-10-17,['cs.cl'],"  Large language models (LLMs) show impressive abilities via few-shotprompting. Commercialized APIs such as OpenAI GPT-3 further increase their usein real-world language applications. However, the crucial problem of how toimprove the reliability of GPT-3 is still under-explored. While reliability isa broad and vaguely defined term, we decompose reliability into four mainfacets that correspond to the existing framework of ML safety and arewell-recognized to be important: generalizability, social biases, calibration,and factuality. Our core contribution is to establish simple and effectiveprompts that improve GPT-3's reliability as it: 1) generalizesout-of-distribution, 2) balances demographic distribution and uses naturallanguage instructions to reduce social biases, 3) calibrates outputprobabilities, and 4) updates the LLM's factual knowledge and reasoning chains.With appropriate prompts, GPT-3 is more reliable than smaller-scale supervisedmodels on all these facets. We release all processed datasets, evaluationscripts, and model predictions. Our systematic empirical study not only shedsnew insights on the reliability of prompting LLMs, but more importantly, ourprompting strategies can help practitioners more reliably use LLMs like GPT-3."
Exploring The Landscape of Distributional Robustness for Question  Answering Models,"['Anas Awadalla', 'Mitchell Wortsman', 'Gabriel Ilharco', 'Sewon Min', 'Ian Magnusson', 'Hannaneh Hajishirzi', 'Ludwig Schmidt']",http://arxiv.org/pdf/2210.12517v1.pdf,2022-10-22,"['cs.cl', 'cs.lg']","  We conduct a large empirical evaluation to investigate the landscape ofdistributional robustness in question answering. Our investigation spans over350 models and 16 question answering datasets, including a diverse set ofarchitectures, model sizes, and adaptation methods (e.g., fine-tuning, adaptertuning, in-context learning, etc.). We find that, in many cases, modelvariations do not affect robustness and in-distribution performance alonedetermines out-of-distribution performance. Moreover, our findings indicatethat i) zero-shot and in-context learning methods are more robust todistribution shifts than fully fine-tuned models; ii) few-shot promptfine-tuned models exhibit better robustness than few-shot fine-tuned spanprediction models; iii) parameter-efficient and robustness enhancing trainingmethods provide no significant robustness improvements. In addition, wepublicly release all evaluations to encourage researchers to further analyzerobustness trends for question answering models."
"""Covid vaccine is against Covid but Oxford vaccine is made at Oxford!""  Semantic Interpretation of Proper Noun Compounds","['Keshav Kolluru', 'Gabriel Stanovsky', ' Mausam']",http://arxiv.org/pdf/2210.13039v1.pdf,2022-10-24,['cs.cl'],"  Proper noun compounds, e.g., ""Covid vaccine"", convey information in asuccinct manner (a ""Covid vaccine"" is a ""vaccine that immunizes against theCovid disease""). These are commonly used in short-form domains, such as newsheadlines, but are largely ignored in information-seeking applications. Toaddress this limitation, we release a new manually annotated dataset, ProNCI,consisting of 22.5K proper noun compounds along with their free-form semanticinterpretations. ProNCI is 60 times larger than prior noun compound datasetsand also includes non-compositional examples, which have not been previouslyexplored. We experiment with various neural models for automatically generatingthe semantic interpretations from proper noun compounds, ranging from few-shotprompting to supervised learning, with varying degrees of knowledge about theconstituent nouns. We find that adding targeted knowledge, particularly aboutthe common noun, results in performance gains of upto 2.8%. Finally, weintegrate our model generated interpretations with an existing Open IE systemand observe an 7.5% increase in yield at a precision of 85%. The dataset andcode are available at https://github.com/dair-iitd/pronci."
Prompting PaLM for Translation: Assessing Strategies and Performance,"['David Vilar', 'Markus Freitag', 'Colin Cherry', 'Jiaming Luo', 'Viresh Ratnakar', 'George Foster']",http://arxiv.org/pdf/2211.09102v3.pdf,2022-11-16,['cs.cl'],"  Large language models (LLMs) that have been trained on multilingual but notparallel text exhibit a remarkable ability to translate between languages. Weprobe this ability in an in-depth study of the pathways language model (PaLM),which has demonstrated the strongest machine translation (MT) performance amongsimilarly-trained LLMs to date. We investigate various strategies for choosingtranslation examples for few-shot prompting, concluding that example quality isthe most important factor. Using optimized prompts, we revisit previousassessments of PaLM's MT capabilities with more recent test sets, modern MTmetrics, and human evaluation, and find that its performance, while impressive,still lags that of state-of-the-art supervised systems. We conclude byproviding an analysis of PaLM's MT output which reveals some interestingproperties and prospects for future work."
PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained  Image-Language Models,"['Minghua Liu', 'Yinhao Zhu', 'Hong Cai', 'Shizhong Han', 'Zhan Ling', 'Fatih Porikli', 'Hao Su']",http://arxiv.org/pdf/2212.01558v2.pdf,2022-12-03,"['cs.cv', 'cs.ro']","  Generalizable 3D part segmentation is important but challenging in vision androbotics. Training deep models via conventional supervised methods requireslarge-scale 3D datasets with fine-grained part annotations, which are costly tocollect. This paper explores an alternative way for low-shot part segmentationof 3D point clouds by leveraging a pretrained image-language model, GLIP, whichachieves superior performance on open-vocabulary 2D detection. We transfer therich knowledge from 2D to 3D through GLIP-based part detection on point cloudrendering and a novel 2D-to-3D label lifting algorithm. We also utilizemulti-view 3D priors and few-shot prompt tuning to boost performancesignificantly. Extensive evaluation on PartNet and PartNet-Mobility datasetsshows that our method enables excellent zero-shot 3D part segmentation. Ourfew-shot version not only outperforms existing few-shot approaches by a largemargin but also achieves highly competitive results compared to the fullysupervised counterpart. Furthermore, we demonstrate that our method can bedirectly applied to iPhone-scanned point clouds without significant domaingaps."
Natural Language to Code Generation in Interactive Data Science  Notebooks,"['Pengcheng Yin', 'Wen-Ding Li', 'Kefan Xiao', 'Abhishek Rao', 'Yeming Wen', 'Kensen Shi', 'Joshua Howland', 'Paige Bailey', 'Michele Catasta', 'Henryk Michalewski', 'Alex Polozov', 'Charles Sutton']",http://arxiv.org/pdf/2212.09248v1.pdf,2022-12-19,"['cs.cl', 'cs.se']","  Computational notebooks, such as Jupyter notebooks, are interactive computingenvironments that are ubiquitous among data scientists to perform datawrangling and analytic tasks. To measure the performance of AI pair programmersthat automatically synthesize programs for those tasks given natural language(NL) intents from users, we build ARCADE, a benchmark of 1082 code generationproblems using the pandas data analysis framework in data science notebooks.ARCADE features multiple rounds of NL-to-code problems from the same notebook.It requires a model to understand rich multi-modal contexts, such as existingnotebook cells and their execution states as well as previous turns ofinteraction. To establish a strong baseline on this challenging task, wedevelop PaChiNCo, a 62B code language model (LM) for Python computationalnotebooks, which significantly outperforms public code LMs. Finally, we explorefew-shot prompting strategies to elicit better code with step-by-stepdecomposition and NL explanation, showing the potential to improve thediversity and explainability of model predictions."
LAMBADA: Backward Chaining for Automated Reasoning in Natural Language,"['Mehran Kazemi', 'Najoung Kim', 'Deepti Bhatia', 'Xin Xu', 'Deepak Ramachandran']",http://arxiv.org/pdf/2212.13894v2.pdf,2022-12-20,"['cs.ai', 'cs.lg']","  Remarkable progress has been made on automated reasoning with natural text,by using Language Models (LMs) and methods such as Chain-of-Thought andSelection-Inference. These techniques search for proofs in the forwarddirection from axioms to the conclusion, which suffers from a combinatorialexplosion of the search space, and thus high failure rates for problemsrequiring longer chains of reasoning. The classical automated reasoningliterature has shown that reasoning in the backward direction (i.e. from theintended conclusion to supporting axioms) is significantly more efficient atproof-finding. Importing this intuition into the LM setting, we develop aBackward Chaining algorithm, called LAMBADA, that decomposes reasoning intofour sub-modules. These sub-modules are simply implemented by few-shot promptedLM inference. We show that LAMBADA achieves sizable accuracy boosts overstate-of-the-art forward reasoning methods on challenging logical reasoningdatasets, particularly when deep and accurate proof chains are required."
Can GPT-3 Perform Statutory Reasoning?,"['Andrew Blair-Stanek', 'Nils Holzenberger', 'Benjamin Van Durme']",http://arxiv.org/pdf/2302.06100v2.pdf,2023-02-13,"['cs.cl', 'cs.ai']","  Statutory reasoning is the task of reasoning with facts and statutes, whichare rules written in natural language by a legislature. It is a basic legalskill. In this paper we explore the capabilities of the most capable GPT-3model, text-davinci-003, on an established statutory-reasoning dataset calledSARA. We consider a variety of approaches, including dynamic few-shotprompting, chain-of-thought prompting, and zero-shot prompting. While weachieve results with GPT-3 that are better than the previous best publishedresults, we also identify several types of clear errors it makes. Weinvestigate why these errors happen. We discover that GPT-3 has imperfect priorknowledge of the actual U.S. statutes on which SARA is based. More importantly,we create simple synthetic statutes, which GPT-3 is guaranteed not to have seenduring training. We find GPT-3 performs poorly at answering straightforwardquestions about these simple synthetic statutes."
STREET: A Multi-Task Structured Reasoning and Explanation Benchmark,"['Danilo Ribeiro', 'Shen Wang', 'Xiaofei Ma', 'Henry Zhu', 'Rui Dong', 'Deguang Kong', 'Juliette Burger', 'Anjelica Ramos', 'William Wang', 'Zhiheng Huang', 'George Karypis', 'Bing Xiang', 'Dan Roth']",http://arxiv.org/pdf/2302.06729v1.pdf,2023-02-13,"['cs.cl', 'cs.ai', 'i.2.7; i.2.6']","  We introduce STREET, a unified multi-task and multi-domain natural languagereasoning and explanation benchmark. Unlike most existing question-answering(QA) datasets, we expect models to not only answer questions, but also producestep-by-step structured explanations describing how premises in the questionare used to produce intermediate conclusions that can prove the correctness ofa certain answer. We perform extensive evaluation with popular language modelssuch as few-shot prompting GPT-3 and fine-tuned T5. We find that these modelsstill lag behind human performance when producing such structured reasoningsteps. We believe this work will provide a way for the community to bettertrain and test systems on multi-step reasoning and explanations in naturallanguage."
ADELT: Transpilation Between Deep Learning Frameworks,"['Linyuan Gong', 'Jiayi Wang', 'Alvin Cheung']",http://arxiv.org/pdf/2303.03593v1.pdf,2023-03-07,"['cs.cl', 'cs.lg']","  We propose Adversarial DEep Learning Transpiler (ADELT) for source-to-sourcetranspilation between deep learning frameworks. Unlike prior approaches, wedecouple the transpilation of code skeletons and the mapping of API keywords(an API function name or a parameter name). ADELT transpile code skeletonsusing few-shot prompting on big language models. Based on contextual embeddingsextracted by a BERT for code, we train aligned API embeddings in adomain-adversarial setup, upon which we generate a dictionary for keywordtranslation. The model is trained on our unlabeled DL corpus from web crawldata, without using any hand-crafted rules and parallel data. Our methodoutperforms state-of-the-art transpilers on multiple transpilation pairsincluding PyTorch-Keras and PyTorch-MXNet by 15.9pts and 12.0pts in exact matchscores respectively."
Query2doc: Query Expansion with Large Language Models,"['Liang Wang', 'Nan Yang', 'Furu Wei']",http://arxiv.org/pdf/2303.07678v2.pdf,2023-03-14,"['cs.ir', 'cs.cl']","  This paper introduces a simple yet effective query expansion approach,denoted as query2doc, to improve both sparse and dense retrieval systems. Theproposed method first generates pseudo-documents by few-shot prompting largelanguage models (LLMs), and then expands the query with generatedpseudo-documents. LLMs are trained on web-scale text corpora and are adept atknowledge memorization. The pseudo-documents from LLMs often contain highlyrelevant information that can aid in query disambiguation and guide theretrievers. Experimental results demonstrate that query2doc boosts theperformance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO andTREC DL, without any model fine-tuning. Furthermore, our method also benefitsstate-of-the-art dense retrievers in terms of both in-domain and out-of-domainresults."
How to Design Translation Prompts for ChatGPT: An Empirical Study,"['Yuan Gao', 'Ruili Wang', 'Feng Hou']",http://arxiv.org/pdf/2304.02182v2.pdf,2023-04-05,['cs.cl'],"  The recently released ChatGPT has demonstrated surprising abilities innatural language understanding and natural language generation. Machinetranslation relies heavily on the abilities of language understanding andgeneration. Thus, in this paper, we explore how to assist machine translationwith ChatGPT. We adopt several translation prompts on a wide range oftranslations. Our experimental results show that ChatGPT with designedtranslation prompts can achieve comparable or better performance overcommercial translation systems for high-resource language translations. Wefurther evaluate the translation quality using multiple references, and ChatGPTachieves superior performance compared to commercial systems. We also conductexperiments on domain-specific translations, the final results show thatChatGPT is able to comprehend the provided domain keyword and adjustaccordingly to output proper translations. At last, we perform few-shot promptsthat show consistent improvement across different base prompts. Our workprovides empirical evidence that ChatGPT still has great potential intranslations."
Boosted Prompt Ensembles for Large Language Models,"['Silviu Pitis', 'Michael R. Zhang', 'Andrew Wang', 'Jimmy Ba']",http://arxiv.org/pdf/2304.05970v1.pdf,2023-04-12,"['cs.cl', 'cs.lg']","  Methods such as chain-of-thought prompting and self-consistency have pushedthe frontier of language model reasoning performance with no additionaltraining. To further improve performance, we propose a prompt ensembling methodfor large language models, which uses a small dataset to construct a set of fewshot prompts that together comprise a ``boosted prompt ensemble''. The few shotexamples for each prompt are chosen in a stepwise fashion to be ``hard''examples on which the previous step's ensemble is uncertain. We show that thisoutperforms single-prompt output-space ensembles and bagged prompt-spaceensembles on the GSM8k and AQuA datasets, among others. We propose bothtrain-time and test-time versions of boosted prompting that use differentlevels of available annotation and conduct a detailed empirical study of ouralgorithm."
Multi-Party Chat: Conversational Agents in Group Settings with Humans  and Models,"['Jimmy Wei', 'Kurt Shuster', 'Arthur Szlam', 'Jason Weston', 'Jack Urbanek', 'Mojtaba Komeili']",http://arxiv.org/pdf/2304.13835v3.pdf,2023-04-26,"['cs.cl', 'cs.lg']","  Current dialogue research primarily studies pairwise (two-party)conversations, and does not address the everyday setting where more than twospeakers converse together. In this work, we both collect and evaluatemulti-party conversations to study this more general case. We use the LIGHTenvironment to construct grounded conversations, where each participant has anassigned character to role-play. We thus evaluate the ability of languagemodels to act as one or more characters in such conversations. Models requiretwo skills that pairwise-trained models appear to lack: (1) being able todecide when to talk; (2) producing coherent utterances grounded on multiplecharacters. We compare models trained on our new dataset to existingpairwise-trained dialogue models, as well as large language models withfew-shot prompting. We find that our new dataset, MultiLIGHT, which we willpublicly release, can help bring significant improvements in the group setting."
Transferring Procedural Knowledge across Commonsense Tasks,"['Yifan Jiang', 'Filip Ilievski', 'Kaixin Ma']",http://arxiv.org/pdf/2304.13867v3.pdf,2023-04-26,['cs.cl'],"  Stories about everyday situations are an essential part of humancommunication, motivating the need to develop AI agents that can reliablyunderstand these stories. Despite the long list of supervised methods for storycompletion and procedural understanding, current AI has no mechanisms toautomatically track and explain procedures in unseen stories. To bridge thisgap, we study the ability of AI models to transfer procedural knowledge tonovel narrative tasks in a transparent manner. We design LEAP: a comprehensiveframework that integrates state-of-the-art modeling architectures, trainingregimes, and augmentation strategies based on both natural and syntheticstories. To address the lack of densely annotated training data, we devise arobust automatic labeler based on few-shot prompting to enhance the augmenteddata. Our experiments with in- and out-of-domain tasks reveal insights into theinterplay of different architectures, training regimes, and augmentationstrategies. LEAP's labeler has a clear positive impact on out-of-domaindatasets, while the resulting dense annotation provides native explainability."
Explainable Verbal Reasoner Plus (EVR+): A Natural Language Reasoning  Framework that Supports Diverse Compositional Reasoning,"['Zhengzhong Liang', 'Zeyu Zhang', 'Steven Bethard', 'Mihai Surdeanu']",http://arxiv.org/pdf/2305.00061v1.pdf,2023-04-28,"['cs.cl', 'cs.ai']","  Languages models have been successfully applied to a variety of reasoningtasks in NLP, yet the language models still suffer from compositionalgeneralization. In this paper we present Explainable Verbal Reasoner Plus(EVR+), a reasoning framework that enhances language models' compositionalreasoning ability by (1) allowing the model to explicitly generate and executesymbolic operators, and (2) allowing the model to decompose a complex task intoseveral simpler ones in a flexible manner. Compared with its predecessorExplainable Verbal Reasoner (EVR) and other previous approaches adoptingsimilar ideas, our framework supports more diverse types of reasoning such asnested loops and different types of recursion. To evaluate our reasoningframework, we build a synthetic dataset with five tasks that requirecompositional reasoning. Results show that our reasoning framework can enhancethe language model's compositional generalization performance on the fivetasks, using a fine-tuned language model. We also discussed the possibility andthe challenges to combine our reasoning framework with a few-shot promptedlanguage model."
Revisiting Relation Extraction in the era of Large Language Models,"['Somin Wadhwa', 'Silvio Amir', 'Byron C. Wallace']",http://arxiv.org/pdf/2305.05003v1.pdf,2023-05-08,['cs.cl'],"  Relation extraction (RE) is the core NLP task of inferring semanticrelationships between entities from text. Standard supervised RE techniquesentail training modules to tag tokens comprising entity spans and then predictthe relationship between them. Recent work has instead treated the problem as a\emph{sequence-to-sequence} task, linearizing relations between entities astarget strings to be generated conditioned on the input. Here we push thelimits of this approach, using larger language models (GPT-3 and Flan-T5 large)than considered in prior work and evaluating their performance on standard REtasks under varying levels of supervision. We address issues inherent toevaluating generative approaches to RE by doing human evaluations, in lieu ofrelying on exact matching. Under this refined evaluation, we find that: (1)Few-shot prompting with GPT-3 achieves near SOTA performance, i.e., roughlyequivalent to existing fully supervised models; (2) Flan-T5 is not as capablein the few-shot setting, but supervising and fine-tuning it withChain-of-Thought (CoT) style explanations (generated via GPT-3) yields SOTAresults. We release this model as a new baseline for RE tasks."
Generating medically-accurate summaries of patient-provider dialogue: A  multi-stage approach using large language models,"['Varun Nair', 'Elliot Schumacher', 'Anitha Kannan']",http://arxiv.org/pdf/2305.05982v1.pdf,2023-05-10,"['cs.cl', 'cs.ai', 'cs.lg']","  A medical provider's summary of a patient visit serves several criticalpurposes, including clinical decision-making, facilitating hand-offs betweenproviders, and as a reference for the patient. An effective summary is requiredto be coherent and accurately capture all the medically relevant information inthe dialogue, despite the complexity of patient-generated language. Even minorinaccuracies in visit summaries (for example, summarizing ""patient does nothave a fever"" when a fever is present) can be detrimental to the outcome ofcare for the patient.  This paper tackles the problem of medical conversation summarization bydiscretizing the task into several smaller dialogue-understanding tasks thatare sequentially built upon. First, we identify medical entities and theiraffirmations within the conversation to serve as building blocks. We studydynamically constructing few-shot prompts for tasks by conditioning on relevantpatient information and use GPT-3 as the backbone for our experiments. We alsodevelop GPT-derived summarization metrics to measure performance againstreference summaries quantitatively. Both our human evaluation study and metricsfor medical correctness show that summaries generated using this approach areclinically accurate and outperform the baseline approach of summarizing thedialog in a zero-shot, single-prompt setting."
ZARA: Improving Few-Shot Self-Rationalization for Small Language Models,"['Wei-Lin Chen', 'An-Zi Yen', 'Cheng-Kuang Wu', 'Hen-Hsen Huang', 'Hsin-Hsi Chen']",http://arxiv.org/pdf/2305.07355v2.pdf,2023-05-12,['cs.cl'],"  Language models (LMs) that jointly generate end-task answers as well asfree-text rationales are known as self-rationalization models. Recent worksdemonstrate great performance gain for self-rationalization by few-shotprompting LMs with rationale-augmented exemplars. However, the ability tobenefit from explanations only emerges with large-scale LMs, which have pooraccessibility. In this work, we explore the less-studied setting of leveragingexplanations for small LMs to improve few-shot self-rationalization. We firstrevisit the relationship between rationales and answers. Inspired by theimplicit mental process of how human beings assess explanations, we present anovel approach, Zero-shot Augmentation of Rationale-Answer pairs (ZARA), toautomatically construct pseudo-parallel data for self-training by reducing theproblem of plausibility judgement to natural language inference. Experimentalresults show ZARA achieves SOTA performance on the FEB benchmark, for both thetask accuracy and the explanation metric. In addition, we conduct human andquantitative evaluation validating ZARA's ability to automatically identifyplausible and accurate rationale-answer pairs."
Natural Language Decomposition and Interpretation of Complex Utterances,"['Harsh Jhamtani', 'Hao Fang', 'Patrick Xia', 'Eran Levy', 'Jacob Andreas', 'Ben Van Durme']",http://arxiv.org/pdf/2305.08677v1.pdf,2023-05-15,['cs.cl'],"  Natural language interfaces often require supervised data to translate userrequests into programs, database queries, or other structured intentrepresentations. During data collection, it can be difficult to anticipate andformalize the full range of user needs -- for example, in a system designed tohandle simple requests (like $\textit{find my meetings tomorrow}$ or$\textit{move my meeting with my manager to noon})$, users may also expressmore elaborate requests (like $\textit{swap all my calls on Monday andTuesday}$). We introduce an approach for equipping a simple language-to-codemodel to handle complex utterances via a process of hierarchical naturallanguage decomposition. Our approach uses a pre-trained language model todecompose a complex utterance into a sequence of smaller natural languagesteps, then interprets each step using the language-to-code model. To test ourapproach, we collect and release DeCU -- a new NL-to-program benchmark toevaluate Decomposition of Complex Utterances. Experiments show that theproposed approach enables the interpretation of complex utterances with almostno complex training data, while outperforming standard few-shot promptingapproaches."
Visualizing Linguistic Diversity of Text Datasets Synthesized by Large  Language Models,"['Emily Reif', 'Minsuk Kahng', 'Savvas Petridis']",http://arxiv.org/pdf/2305.11364v2.pdf,2023-05-19,"['cs.cl', 'cs.ai']","  Large language models (LLMs) can be used to generate smaller, more refineddatasets via few-shot prompting for benchmarking, fine-tuning or other usecases. However, understanding and evaluating these datasets is difficult, andthe failure modes of LLM-generated data are still not well understood.Specifically, the data can be repetitive in surprising ways, not onlysemantically but also syntactically and lexically. We present LinguisticLens, anovel inter-active visualization tool for making sense of and analyzingsyntactic diversity of LLM-generated datasets. LinguisticLens clusters textalong syntactic, lexical, and semantic axes. It supports hierarchicalvisualization of a text dataset, allowing users to quickly scan for an overviewand inspect individual examples. The live demo is available atshorturl.at/zHOUV."
Improved Compositional Generalization by Generating Demonstrations for  Meta-Learning,"['Sam Spilsbury', 'Alexander Ilin']",http://arxiv.org/pdf/2305.13092v1.pdf,2023-05-22,['cs.cl'],"  Meta-learning and few-shot prompting are viable methods to induce certaintypes of compositional behaviour. However, these methods can be very sensitiveto the choice of support examples used. Choosing good supports from thetraining data for a given test query is already a difficult problem, but insome cases solving this may not even be enough. We consider a grounded languagelearning problem (gSCAN) where good support examples for certain test splitsmight not even exist in the training data, or would be infeasible to searchfor. We design an agent which instead generates possible supports which arerelevant to the test query and current state of the world, then uses thesesupports via meta-learning to solve the test query. We show substantiallyimproved performance on a previously unsolved compositional behaviour splitwithout a loss of performance on other splits. Further experiments show that inthis case, searching for relevant demonstrations even with an oracle functionis not sufficient to attain good performance when using meta-learning."
SPARSEFIT: Few-shot Prompting with Sparse Fine-tuning for Jointly  Generating Predictions and Natural Language Explanations,"['Jesus Solano', 'Oana-Maria Camburu', 'Pasquale Minervini']",http://arxiv.org/pdf/2305.13235v2.pdf,2023-05-22,"['cs.cl', 'cs.ai']","  Explaining the decisions of neural models is crucial for ensuring theirtrustworthiness at deployment time. Using Natural Language Explanations (NLEs)to justify a model's predictions has recently gained increasing interest.However, this approach usually demands large datasets of human-written NLEs forthe ground-truth answers, which are expensive and potentially infeasible forsome applications. For models to generate high-quality NLEs when only a fewNLEs are available, the fine-tuning of Pre-trained Language Models (PLMs) inconjunction with prompt-based learning recently emerged. However, PLMstypically have billions of parameters, making fine-tuning expensive. We proposeSparseFit, a sparse few-shot fine-tuning strategy that leverages discreteprompts to jointly generate predictions and NLEs. We experiment with SparseFiton the T5 model and four datasets and compare it against state-of-the-artparameter-efficient fine-tuning techniques. We perform automatic and humanevaluations to assess the quality of the model-generated NLEs, finding thatfine-tuning only 6.8% of the model parameters leads to competitive results forboth the task performance and the quality of the NLEs."
Towards Legally Enforceable Hate Speech Detection for Public Forums,"['Chu Fei Luo', 'Rohan Bhambhoria', 'Xiaodan Zhu', 'Samuel Dahan']",http://arxiv.org/pdf/2305.13677v2.pdf,2023-05-23,['cs.cl'],"  Hate speech causes widespread and deep-seated societal issues. Properenforcement of hate speech laws is key for protecting groups of people againstharmful and discriminatory language. However, determining what constitutes hatespeech is a complex task that is highly open to subjective interpretations.Existing works do not align their systems with enforceable definitions of hatespeech, which can make their outputs inconsistent with the goals of regulators.This research introduces a new perspective and task for enforceable hate speechdetection centred around legal definitions, and a dataset annotated onviolations of eleven possible definitions by legal experts. Given the challengeof identifying clear, legally enforceable instances of hate speech, we augmentthe dataset with expert-generated samples and an automatically mined challengeset. We experiment with grounding the model decision in these definitions usingzero-shot and few-shot prompting. We then report results on several largelanguage models (LLMs). With this task definition, automatic hate speechdetection can be more closely aligned to enforceable laws, and hence assist inmore rigorous enforcement of legal protections against harmful speech in publicforums."
ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain  Readability Assessment,"['Tarek Naous', 'Michael J. Ryan', 'Anton Lavrouk', 'Mohit Chandra', 'Wei Xu']",http://arxiv.org/pdf/2305.14463v2.pdf,2023-05-23,"['cs.cl', 'cs.ai', 'cs.lg']","  We present a systematic study and comprehensive evaluation of large languagemodels for automatic multilingual readability assessment. In particular, weconstruct ReadMe++, a multilingual multi-domain dataset with human annotationsof 9757 sentences in Arabic, English, French, Hindi, and Russian collected from112 different data sources. ReadMe++ offers more domain and language diversitythan existing readability datasets, making it ideal for benchmarkingmultilingual and non-English language models (including mBERT, XLM-R, mT5,Llama-2, GPT-4, etc.) in the supervised, unsupervised, and few-shot promptingsettings. Our experiments reveal that models fine-tuned on ReadMe++ outperformthose trained on single-domain datasets, showcasing superior performance onmulti-domain readability assessment and cross-lingual transfer capabilities. Wealso compare to traditional readability metrics (such as Flesch-Kincaid GradeLevel and Open Source Metric for Measuring Arabic Narratives), as well as thestate-of-the-art unsupervised metric RSRS (Martinc et al., 2021). We will makeour data and code publicly available at: https://github.com/tareknaous/readme."
PEARL: Prompting Large Language Models to Plan and Execute Actions Over  Long Documents,"['Simeng Sun', 'Yang Liu', 'Shuohang Wang', 'Chenguang Zhu', 'Mohit Iyyer']",http://arxiv.org/pdf/2305.14564v1.pdf,2023-05-23,['cs.cl'],"  Strategies such as chain-of-thought prompting improve the performance oflarge language models (LLMs) on complex reasoning tasks by decomposing inputexamples into intermediate steps. However, it remains unclear how to apply suchmethods to reason over long input documents, in which both the decompositionand the output of each intermediate step are non-trivial to obtain. In thiswork, we propose PEARL, a prompting framework to improve reasoning over longdocuments, which consists of three stages: action mining, plan formulation, andplan execution. More specifically, given a question about a long document,PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE,FIND_EVENT, FIND_RELATION) and then executes them over the document to obtainthe answer. Each stage of PEARL is implemented via zero-shot or few-shotprompting of LLMs (in our work, GPT-4) with minimal human input. We evaluatePEARL on a challenging subset of the QuALITY dataset, which contains questionsthat require complex reasoning over long narrative texts. PEARL outperformszero-shot and chain-of-thought prompting on this dataset, and ablationexperiments show that each stage of PEARL is critical to its performance.Overall, PEARL is a first step towards leveraging LLMs to reason over longdocuments."
Revisiting non-English Text Simplification: A Unified Multilingual  Benchmark,"['Michael J. Ryan', 'Tarek Naous', 'Wei Xu']",http://arxiv.org/pdf/2305.15678v1.pdf,2023-05-25,"['cs.cl', 'cs.ai']","  Recent advancements in high-quality, large-scale English resources havepushed the frontier of English Automatic Text Simplification (ATS) research.However, less work has been done on multilingual text simplification due to thelack of a diverse evaluation benchmark that covers complex-simple sentencepairs in many languages. This paper introduces the MultiSim benchmark, acollection of 27 resources in 12 distinct languages containing over 1.7 millioncomplex-simple sentence pairs. This benchmark will encourage research indeveloping more effective multilingual text simplification models andevaluation metrics. Our experiments using MultiSim with pre-trainedmultilingual language models reveal exciting performance improvements frommultilingual training in non-English settings. We observe strong performancefrom Russian in zero-shot cross-lingual transfer to low-resource languages. Wefurther show that few-shot prompting with BLOOM-176b achieves comparablequality to reference simplifications outperforming fine-tuned models in mostlanguages. We validate these findings through human evaluation."
Do GPTs Produce Less Literal Translations?,"['Vikas Raunak', 'Arul Menezes', 'Matt Post', 'Hany Hassan Awadalla']",http://arxiv.org/pdf/2305.16806v4.pdf,2023-05-26,"['cs.cl', 'cs.ai']","  Large Language Models (LLMs) such as GPT-3 have emerged as general-purposelanguage models capable of addressing many natural language generation orunderstanding tasks. On the task of Machine Translation (MT), multiple workshave investigated few-shot prompting mechanisms to elicit better translationsfrom LLMs. However, there has been relatively little investigation on how suchtranslations differ qualitatively from the translations generated by standardNeural Machine Translation (NMT) models. In this work, we investigate thesedifferences in terms of the literalness of translations produced by the twosystems. Using literalness measures involving word alignment and monotonicity,we find that translations out of English (E-X) from GPTs tend to be lessliteral, while exhibiting similar or better scores on MT quality metrics. Wedemonstrate that this finding is borne out in human evaluations as well. Wethen show that these differences are especially pronounced when translatingsentences that contain idiomatic expressions."
Log Parsing: How Far Can ChatGPT Go?,"['Van-Hoang Le', 'Hongyu Zhang']",http://arxiv.org/pdf/2306.01590v2.pdf,2023-06-02,"['cs.se', 'cs.ai']","  Software logs play an essential role in ensuring the reliability andmaintainability of large-scale software systems, as they are often the solesource of runtime information. Log parsing, which converts raw log messagesinto structured data, is an important initial step towards downstream loganalytics. In recent studies, ChatGPT, the current cutting-edge large languagemodel (LLM), has been widely applied to a wide range of software engineeringtasks. However, its performance in automated log parsing remains unclear. Inthis paper, we evaluate ChatGPT's ability to undertake log parsing byaddressing two research questions. (1) Can ChatGPT effectively parse logs? (2)How does ChatGPT perform with different prompting methods? Our results showthat ChatGPT can achieve promising results for log parsing with appropriateprompts, especially with few-shot prompting. Based on our findings, we outlineseveral challenges and opportunities for ChatGPT-based log parsing."
Large Language Model Augmented Narrative Driven Recommendations,"['Sheshera Mysore', 'Andrew McCallum', 'Hamed Zamani']",http://arxiv.org/pdf/2306.02250v2.pdf,2023-06-04,"['cs.ir', 'cs.cl']","  Narrative-driven recommendation (NDR) presents an information access problemwhere users solicit recommendations with verbose descriptions of theirpreferences and context, for example, travelers soliciting recommendations forpoints of interest while describing their likes/dislikes and travelcircumstances. These requests are increasingly important with the rise ofnatural language-based conversational interfaces for search and recommendationsystems. However, NDR lacks abundant training data for models, and currentplatforms commonly do not support these requests. Fortunately, classicaluser-item interaction datasets contain rich textual data, e.g., reviews, whichoften describe user preferences and context - this may be used to bootstraptraining for NDR models. In this work, we explore using large language models(LLMs) for data augmentation to train NDR models. We use LLMs for authoringsynthetic narrative queries from user-item interactions with few-shot promptingand train retrieval models for NDR on synthetic queries and user-iteminteraction data. Our experiments demonstrate that this is an effectivestrategy for training small-parameter retrieval models that outperform otherretrieval and LLM baselines for narrative-driven recommendation."
Enhancing In-Context Learning with Answer Feedback for Multi-Span  Question Answering,"['Zixian Huang', 'Jiaying Zhou', 'Gengyang Xiao', 'Gong Cheng']",http://arxiv.org/pdf/2306.04508v1.pdf,2023-06-07,"['cs.cl', 'cs.ai']","  Whereas the recent emergence of large language models (LLMs) like ChatGPT hasexhibited impressive general performance, it still has a large gap withfully-supervised models on specific tasks such as multi-span questionanswering. Previous researches found that in-context learning is an effectiveapproach to exploiting LLM, by using a few task-related labeled data asdemonstration examples to construct a few-shot prompt for answering newquestions. A popular implementation is to concatenate a few questions and theircorrect answers through simple templates, informing LLM of the desired output.In this paper, we propose a novel way of employing labeled data such that italso informs LLM of some undesired output, by extending demonstration exampleswith feedback about answers predicted by an off-the-shelf model, e.g., correct,incorrect, or incomplete. Experiments on three multi-span question answeringdatasets as well as a keyphrase extraction dataset show that our new promptingstrategy consistently improves LLM's in-context learning performance."
Product Information Extraction using ChatGPT,"['Alexander Brinkmann', 'Roee Shraga', 'Reng Chiz Der', 'Christian Bizer']",http://arxiv.org/pdf/2306.14921v1.pdf,2023-06-23,"['cs.cl', 'cs.ir']","  Structured product data in the form of attribute/value pairs is thefoundation of many e-commerce applications such as faceted product search,product comparison, and product recommendation. Product offers often onlycontain textual descriptions of the product attributes in the form of titles orfree text. Hence, extracting attribute/value pairs from textual productdescriptions is an essential enabler for e-commerce applications. In order toexcel, state-of-the-art product information extraction methods require largequantities of task-specific training data. The methods also struggle withgeneralizing to out-of-distribution attributes and attribute values that werenot a part of the training data. Due to being pre-trained on huge amounts oftext as well as due to emergent effects resulting from the model size, LargeLanguage Models like ChatGPT have the potential to address both of theseshortcomings. This paper explores the potential of ChatGPT for extractingattribute/value pairs from product descriptions. We experiment with differentzero-shot and few-shot prompt designs. Our results show that ChatGPT achieves aperformance similar to a pre-trained language model but requires much smalleramounts of training data and computation for fine-tuning."
SummQA at MEDIQA-Chat 2023:In-Context Learning with GPT-4 for Medical  Summarization,"['Yash Mathur', 'Sanketh Rangreji', 'Raghav Kapoor', 'Medha Palavalli', 'Amanda Bertsch', 'Matthew R. Gormley']",http://arxiv.org/pdf/2306.17384v1.pdf,2023-06-30,['cs.cl'],"  Medical dialogue summarization is challenging due to the unstructured natureof medical conversations, the use of medical terminology in gold summaries, andthe need to identify key information across multiple symptom sets. We present anovel system for the Dialogue2Note Medical Summarization tasks in the MEDIQA2023 Shared Task. Our approach for section-wise summarization (Task A) is atwo-stage process of selecting semantically similar dialogues and using thetop-k similar dialogues as in-context examples for GPT-4. For full-notesummarization (Task B), we use a similar solution with k=1. We achieved 3rdplace in Task A (2nd among all teams), 4th place in Task B Division WiseSummarization (2nd among all teams), 15th place in Task A Section HeaderClassification (9th among all teams), and 8th place among all teams in Task B.Our results highlight the effectiveness of few-shot prompting for this task,though we also identify several weaknesses of prompting-based approaches. Wecompare GPT-4 performance with several finetuned baselines. We find that GPT-4summaries are more abstractive and shorter. We make our code publiclyavailable."
Building Cooperative Embodied Agents Modularly with Large Language  Models,"['Hongxin Zhang', 'Weihua Du', 'Jiaming Shan', 'Qinhong Zhou', 'Yilun Du', 'Joshua B. Tenenbaum', 'Tianmin Shu', 'Chuang Gan']",http://arxiv.org/pdf/2307.02485v1.pdf,2023-07-05,"['cs.ai', 'cs.cl', 'cs.cv']","  Large Language Models (LLMs) have demonstrated impressive planning abilitiesin single-agent embodied tasks across various domains. However, their capacityfor planning and communication in multi-agent cooperation remains unclear, eventhough these are crucial skills for intelligent embodied agents. In this paper,we present a novel framework that utilizes LLMs for multi-agent cooperation andtests it in various embodied environments. Our framework enables embodiedagents to plan, communicate, and cooperate with other embodied agents or humansto accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs,such as GPT-4, can surpass strong planning-based methods and exhibit emergenteffective communication using our framework without requiring fine-tuning orfew-shot prompting. We also discover that LLM-based agents that communicate innatural language can earn more trust and cooperate more effectively withhumans. Our research underscores the potential of LLMs for embodied AI and laysthe foundation for future research in multi-agent cooperation. Videos can befound on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/."
MultiQG-TI: Towards Question Generation from Multi-modal Sources,"['Zichao Wang', 'Richard Baraniuk']",http://arxiv.org/pdf/2307.04643v1.pdf,2023-07-07,"['cs.cl', 'cs.ai']","  We study the new problem of automatic question generation (QG) frommulti-modal sources containing images and texts, significantly expanding thescope of most of the existing work that focuses exclusively on QG from onlytextual sources. We propose a simple solution for our new problem, calledMultiQG-TI, which enables a text-only question generator to process visualinput in addition to textual input. Specifically, we leverage an image-to-textmodel and an optical character recognition model to obtain the textualdescription of the image and extract any texts in the image, respectively, andthen feed them together with the input texts to the question generator. We onlyfine-tune the question generator while keeping the other components fixed. Onthe challenging ScienceQA dataset, we demonstrate that MultiQG-TI significantlyoutperforms ChatGPT with few-shot prompting, despite having hundred-times lesstrainable parameters. Additional analyses empirically confirm the necessity ofboth visual and textual signals for QG and show the impact of various modelingchoices."
Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?,"['Cheng-En Wu', 'Yu Tian', 'Haichao Yu', 'Heng Wang', 'Pedro Morgado', 'Yu Hen Hu', 'Linjie Yang']",http://arxiv.org/pdf/2307.11978v1.pdf,2023-07-22,"['cs.cv', 'cs.ai', 'cs.lg']","  Vision-language models such as CLIP learn a generic text-image embedding fromlarge-scale training data. A vision-language model can be adapted to a newclassification task through few-shot prompt tuning. We find that such a prompttuning process is highly robust to label noises. This intrigues us to study thekey reasons contributing to the robustness of the prompt tuning paradigm. Weconducted extensive experiments to explore this property and find the keyfactors are: 1) the fixed classname tokens provide a strong regularization tothe optimization of the model, reducing gradients induced by the noisy samples;2) the powerful pre-trained image-text embedding that is learned from diverseand generic web data provides strong prior knowledge for image classification.Further, we demonstrate that noisy zero-shot predictions from CLIP can be usedto tune its own prompt, significantly enhancing prediction accuracy in theunsupervised setting. The code is available at https://github.com/CEWu/PTNL."
Analyzing Chain-of-Thought Prompting in Large Language Models via  Gradient-based Feature Attributions,"['Skyler Wu', 'Eric Meng Shen', 'Charumathi Badrinath', 'Jiaqi Ma', 'Himabindu Lakkaraju']",http://arxiv.org/pdf/2307.13339v1.pdf,2023-07-25,"['cs.cl', 'cs.ai']","  Chain-of-thought (CoT) prompting has been shown to empirically improve theaccuracy of large language models (LLMs) on various question answering tasks.While understanding why CoT prompting is effective is crucial to ensuring thatthis phenomenon is a consequence of desired model behavior, little work hasaddressed this; nonetheless, such an understanding is a critical prerequisitefor responsible model deployment. We address this question by leveraginggradient-based feature attribution methods which produce saliency scores thatcapture the influence of input tokens on model output. Specifically, we probeseveral open-source LLMs to investigate whether CoT prompting affects therelative importances they assign to particular input tokens. Our resultsindicate that while CoT prompting does not increase the magnitude of saliencyscores attributed to semantically relevant tokens in the prompt compared tostandard few-shot prompting, it increases the robustness of saliency scores toquestion perturbations and variations in model output."
Low-Parameter Federated Learning with Large Language Models,"['Jingang Jiang', 'Xiangyang Liu', 'Chenyou Fan']",http://arxiv.org/pdf/2307.13896v1.pdf,2023-07-26,['cs.dc'],"  We study few-shot Natural Language Understanding (NLU) tasks with LargeLanguage Models (LLMs) in federated learning (FL) scenarios. It is achallenging task due to limited labeled data and communication capacities inFL, especially with mobile devices. Recent studies show LLMs can be prompted toperform few-shot NLU tasks like sentiment analysis and arithmetic reasoning.However, the huge sizes of LLMs result in high computation and communicationcosts, making classical FL schemes impractical. To address these challenges, wepropose Low-Parameter Federated Learning (LP-FL). LP-FL combines few-shotprompt learning from LLMs with efficient communication and federatingtechniques. Our approach enables federated clients to assign soft labels tounlabeled data using gradually learned knowledge from the global model. Throughiterative soft-label assigning, we continually expand the labeled set duringthe FL process. Additionally, to reduce computation and communication costs,LP-FL utilizes the Low-Rank Adaptation (LoRA) technique for compact learnableparameter construction, efficient local model fine-tuning, and affordableglobal model federation. LP-FL consistently outperforms Full-ParameterFederated Learning (FP-FL) in sentiment analysis tasks across various FLsettings. Its resistance to overfitting allows LP-FL to equal or surpasscentralized training in few-shot scenarios."
Large Language Model Prompt Chaining for Long Legal Document  Classification,['Dietrich Trautmann'],http://arxiv.org/pdf/2308.04138v1.pdf,2023-08-08,['cs.cl'],"  Prompting is used to guide or steer a language model in generating anappropriate response that is consistent with the desired outcome. Chaining is astrategy used to decompose complex tasks into smaller, manageable components.In this study, we utilize prompt chaining for extensive legal documentclassification tasks, which present difficulties due to their intricatedomain-specific language and considerable length. Our approach begins with thecreation of a concise summary of the original document, followed by a semanticsearch for related exemplar texts and their corresponding annotations from atraining corpus. Finally, we prompt for a label - based on the task - toassign, by leveraging the in-context learning from the few-shot prompt. Wedemonstrate that through prompt chaining, we can not only enhance theperformance over zero-shot, but also surpass the micro-F1 score achieved bylarger models, such as ChatGPT zero-shot, using smaller models."
FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for  Large Language Models,"['Liwen Zhang', 'Weige Cai', 'Zhaowei Liu', 'Zhi Yang', 'Wei Dai', 'Yujie Liao', 'Qianru Qin', 'Yifei Li', 'Xingyu Liu', 'Zhiqiang Liu', 'Zhoufan Zhu', 'Anbo Wu', 'Xin Guo', 'Yun Chen']",http://arxiv.org/pdf/2308.09975v1.pdf,2023-08-19,['cs.cl'],"  Large language models (LLMs) have demonstrated exceptional performance invarious natural language processing tasks, yet their efficacy in morechallenging and domain-specific tasks remains largely unexplored. This paperpresents FinEval, a benchmark specifically designed for the financial domainknowledge in the LLMs. FinEval is a collection of high-quality multiple-choicequestions covering Finance, Economy, Accounting, and Certificate. It includes4,661 questions spanning 34 different academic subjects. To ensure acomprehensive model performance evaluation, FinEval employs a range of prompttypes, including zero-shot and few-shot prompts, as well as answer-only andchain-of-thought prompts. Evaluating state-of-the-art Chinese and English LLMson FinEval, the results show that only GPT-4 achieved an accuracy close to 70%in different prompt settings, indicating significant growth potential for LLMsin the financial domain knowledge. Our work offers a more comprehensivefinancial knowledge evaluation benchmark, utilizing data of mock exams andcovering a wide range of evaluated LLMs."
Diversity Measures: Domain-Independent Proxies for Failure in Language  Model Queries,"['Noel Ngu', 'Nathaniel Lee', 'Paulo Shakarian']",http://arxiv.org/pdf/2308.11189v1.pdf,2023-08-22,"['cs.cl', 'cs.ai', 'cs.lg']","  Error prediction in large language models often relies on domain-specificinformation. In this paper, we present measures for quantification of error inthe response of a large language model based on the diversity of responses to agiven prompt - hence independent of the underlying application. We describe howthree such measures - based on entropy, Gini impurity, and centroid distance -can be employed. We perform a suite of experiments on multiple datasets andtemperature settings to demonstrate that these measures strongly correlate withthe probability of failure. Additionally, we present empirical resultsdemonstrating how these measures can be applied to few-shot prompting,chain-of-thought reasoning, and error detection."
Evaluating Large Language Models on Graphs: Performance Insights and  Comparative Analysis,"['Chang Liu', 'Bo Wu']",http://arxiv.org/pdf/2308.11224v2.pdf,2023-08-22,"['cs.ai', 'cs.cl']","  Large Language Models (LLMs) have garnered considerable interest within bothacademic and industrial. Yet, the application of LLMs to graph data remainsunder-explored. In this study, we evaluate the capabilities of four LLMs inaddressing several analytical problems with graph data. We employ four distinctevaluation metrics: Comprehension, Correctness, Fidelity, and Rectification.Our results show that: 1) LLMs effectively comprehend graph data in naturallanguage and reason with graph topology. 2) GPT models can generate logical andcoherent results, outperforming alternatives in correctness. 3) All examinedLLMs face challenges in structural reasoning, with techniques like zero-shotchain-of-thought and few-shot prompting showing diminished efficacy. 4) GPTmodels often produce erroneous answers in multi-answer tasks, raising concernsin fidelity. 5) GPT models exhibit elevated confidence in their outputs,potentially hindering their rectification capacities. Notably, GPT-4 hasdemonstrated the capacity to rectify responses from GPT-3.5-turbo and its ownprevious iterations. The code is available at:https://github.com/Ayame1006/LLMtoGraph."
Prompt2Model: Generating Deployable Models from Natural Language  Instructions,"['Vijay Viswanathan', 'Chenyang Zhao', 'Amanda Bertsch', 'Tongshuang Wu', 'Graham Neubig']",http://arxiv.org/pdf/2308.12261v1.pdf,2023-08-23,['cs.cl'],"  Large language models (LLMs) enable system builders today to create competentNLP systems through prompting, where they only need to describe the task innatural language and provide a few examples. However, in other ways, LLMs are astep backward from traditional special-purpose NLP models; they requireextensive computational resources for deployment and can be gated behind APIs.In this paper, we propose Prompt2Model, a general-purpose method that takes anatural language task description like the prompts provided to LLMs, and usesit to train a special-purpose model that is conducive to deployment. This isdone through a multi-step process of retrieval of existing datasets andpretrained models, dataset generation using LLMs, and supervised fine-tuning onthese retrieved and generated datasets. Over three tasks, we demonstrate thatgiven the same few-shot prompt as input, Prompt2Model trains models thatoutperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20%while being up to 700 times smaller. We also show that this data can be used toobtain reliable performance estimates of model performance, enabling modeldevelopers to assess model reliability before deployment. Prompt2Model isavailable open-source at https://github.com/neulab/prompt2model."
Prompt a Robot to Walk with Large Language Models,"['Yen-Jen Wang', 'Bike Zhang', 'Jianyu Chen', 'Koushil Sreenath']",http://arxiv.org/pdf/2309.09969v2.pdf,2023-09-18,"['cs.ro', 'cs.lg', 'cs.sy', 'eess.sy']","  Large language models (LLMs) pre-trained on vast internet-scale data haveshowcased remarkable capabilities across diverse domains. Recently, there hasbeen escalating interest in deploying LLMs for robotics, aiming to harness thepower of foundation models in real-world settings. However, this approach facessignificant challenges, particularly in grounding these models in the physicalworld and in generating dynamic robot motions. To address these issues, weintroduce a novel paradigm in which we use few-shot prompts collected from thephysical environment, enabling the LLM to autoregressively generate low-levelcontrol commands for robots without task-specific fine-tuning. Experimentsacross various robots and environments validate that our method can effectivelyprompt a robot to walk. We thus illustrate how LLMs can proficiently functionas low-level feedback controllers for dynamic motion control even inhigh-dimensional robotic systems. The project website and source code can befound at: https://prompt2walk.github.io/ ."
SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language  Models,"['Shyam Sundar Kannan', 'Vishnunandan L. N. Venkatesh', 'Byung-Cheol Min']",http://arxiv.org/pdf/2309.10062v1.pdf,2023-09-18,['cs.ro'],"  In this work, we introduce SMART-LLM, an innovative framework designed forembodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot TaskPlanning using Large Language Models (LLMs), harnesses the power of LLMs toconvert high-level task instructions provided as input into a multi-robot taskplan. It accomplishes this by executing a series of stages, including taskdecomposition, coalition formation, and task allocation, all guided byprogrammatic LLM prompts within the few-shot prompting paradigm. We create abenchmark dataset designed for validating the multi-robot task planningproblem, encompassing four distinct categories of high-level instructions thatvary in task complexity. Our evaluation experiments span both simulation andreal-world scenarios, demonstrating that the proposed model can achievepromising results for generating multi-robot task plans. The experimentalvideos, code, and datasets from the work can be found athttps://sites.google.com/view/smart-llm/."
EchoPrompt: Instructing the Model to Rephrase Queries for Improved  In-context Learning,"['Rajasekhar Reddy Mekala', 'Yasaman Razeghi', 'Sameer Singh']",http://arxiv.org/pdf/2309.10687v2.pdf,2023-09-16,['cs.cl'],"  Language models are achieving impressive performance on various tasks byaggressively adopting inference-time prompting techniques, such as zero-shotand few-shot prompting. In this work, we introduce EchoPrompt, a simple yeteffective approach that prompts the model to rephrase its queries beforeanswering them. EchoPrompt is adapted for both zero-shot and few-shotin-context learning with standard and chain-of-thought prompting. Experimentalresults show that EchoPrompt yields substantial improvements across all thesesettings for four families of causal language models. These improvements areobserved across various numerical reasoning (e.g. GSM8K, SVAMP), readingcomprehension (e.g. DROP), and logical reasoning (e.g. Coin Flipping) tasks. Onaverage, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002by 5% in numerical tasks and 13% in reading comprehension tasks. We investigatethe factors contributing to EchoPrompt's effectiveness through ablationstudies, which reveal that both the original query and the model-generatedrephrased version are instrumental in its performance gains. Our empiricalresults indicate that EchoPrompt is an effective technique that enhancesin-context learning performance. We recommend incorporating EchoPrompt intovarious baseline prompting strategies to achieve performance boosts."
Self-Explanation Prompting Improves Dialogue Understanding in Large  Language Models,"['Haoyu Gao', 'Ting-En Lin', 'Hangyu Li', 'Min Yang', 'Yuchuan Wu', 'Wentao Ma', 'Yongbin Li']",http://arxiv.org/pdf/2309.12940v1.pdf,2023-09-22,"['cs.cl', 'cs.ai']","  Task-oriented dialogue (TOD) systems facilitate users in executing variousactivities via multi-turn dialogues, but Large Language Models (LLMs) oftenstruggle to comprehend these intricate contexts. In this study, we propose anovel ""Self-Explanation"" prompting strategy to enhance the comprehensionabilities of LLMs in multi-turn dialogues. This task-agnostic approach requiresthe model to analyze each dialogue utterance before task execution, therebyimproving performance across various dialogue-centric tasks. Experimentalresults from six benchmark datasets confirm that our method consistentlyoutperforms other zero-shot prompts and matches or exceeds the efficacy offew-shot prompts, demonstrating its potential as a powerful tool in enhancingLLMs' comprehension in complex dialogue tasks."
Language Models as Knowledge Bases for Visual Word Sense Disambiguation,"['Anastasia Kritharoula', 'Maria Lymperaiou', 'Giorgos Stamou']",http://arxiv.org/pdf/2310.01960v1.pdf,2023-10-03,"['cs.cl', 'cs.ai']","  Visual Word Sense Disambiguation (VWSD) is a novel challenging task that liesbetween linguistic sense disambiguation and fine-grained multimodal retrieval.The recent advancements in the development of visiolinguistic (VL) transformerssuggest some off-the-self implementations with encouraging results, whichhowever we argue that can be further improved. To this end, we propose someknowledge-enhancement techniques towards improving the retrieval performance ofVL transformers via the usage of Large Language Models (LLMs) as KnowledgeBases. More specifically, knowledge stored in LLMs is retrieved with the helpof appropriate prompts in a zero-shot manner, achieving performanceadvancements. Moreover, we convert VWSD to a purely textual question-answering(QA) problem by considering generated image captions as multiple-choicecandidate answers. Zero-shot and few-shot prompting strategies are leveraged toexplore the potential of such a transformation, while Chain-of-Thought (CoT)prompting in the zero-shot setting is able to reveal the internal reasoningsteps an LLM follows to select the appropriate candidate. In total, ourpresented approach is the first one to analyze the merits of exploitingknowledge stored in LLMs in different ways to solve WVSD."
Can Large Language Models be Good Path Planners? A Benchmark and  Investigation on Spatial-temporal Reasoning,"['Mohamed Aghzal', 'Erion Plaku', 'Ziyu Yao']",http://arxiv.org/pdf/2310.03249v1.pdf,2023-10-05,['cs.cl'],"  Large language models (LLMs) have achieved remarkable success across a widespectrum of tasks; however, they still face limitations in scenarios thatdemand long-term planning and spatial reasoning. To facilitate this line ofresearch, in this work, we propose a new benchmark, termed $\textbf{P}$ath$\textbf{P}$lanning from $\textbf{N}$atural $\textbf{L}$anguage($\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning byformulating ''path planning'' tasks that require an LLM to navigate to targetlocations while avoiding obstacles and adhering to constraints. Leveraging thisbenchmark, we systematically investigate LLMs including GPT-4 via differentfew-shot prompting methodologies and BART and T5 of various sizes viafine-tuning. Our experimental results show the promise of few-shot GPT-4 inspatial reasoning, when it is prompted to reason and act interleavedly,although it still fails to make long-term temporal reasoning. In contrast,while fine-tuned LLMs achieved impressive results on in-distribution reasoningtasks, they struggled to generalize to larger environments or environments withmore obstacles."
Towards Informative Few-Shot Prompt with Maximum Information Gain for  In-Context Learning,"['Hongfu Liu', 'Ye Wang']",http://arxiv.org/pdf/2310.08923v1.pdf,2023-10-13,['cs.cl'],"  Large Language models (LLMs) possess the capability to engage In-contextLearning (ICL) by leveraging a few demonstrations pertaining to a newdownstream task as conditions. However, this particular learning paradigmsuffers from high instability stemming from substantial variances induced byfactors such as the input distribution of selected examples, their ordering,and prompt formats. In this work, we demonstrate that even when all thesefactors are held constant, the random selection of examples still results inhigh variance. Consequently, we aim to explore the informative ability of dataexamples by quantifying the Information Gain (IG) obtained in prediction afterobserving a given example candidate. Then we propose to sample those withmaximum IG. Additionally, we identify the presence of template bias, which canlead to unfair evaluations of IG during the sampling process. To mitigate thisbias, we introduce Calibration Before Sampling strategy. The experimentalresults illustrate that our proposed method can yield an average relativeimprovement of 14.3% across six classification tasks using three LLMs."
Ecologically Valid Explanations for Label Variation in NLI,"['Nan-Jiang Jiang', 'Chenhao Tan', 'Marie-Catherine de Marneffe']",http://arxiv.org/pdf/2310.13850v1.pdf,2023-10-20,['cs.cl'],"  Human label variation, or annotation disagreement, exists in many naturallanguage processing (NLP) tasks, including natural language inference (NLI). Togain direct evidence of how NLI label variation arises, we build LiveNLI, anEnglish dataset of 1,415 ecologically valid explanations (annotators explainthe NLI labels they chose) for 122 MNLI items (at least 10 explanations peritem). The LiveNLI explanations confirm that people can systematically vary ontheir interpretation and highlight within-label variation: annotators sometimeschoose the same label for different reasons. This suggests that explanationsare crucial for navigating label interpretations in general. We few-shot promptlarge language models to generate explanations but the results areinconsistent: they sometimes produces valid and informative explanations, butit also generates implausible ones that do not support the label, highlightingdirections for improvement."
API-Assisted Code Generation for Question Answering on Varied Table  Structures,"['Yihan Cao', 'Shuyi Chen', 'Ryan Liu', 'Zhiruo Wang', 'Daniel Fried']",http://arxiv.org/pdf/2310.14687v1.pdf,2023-10-23,"['cs.cl', 'cs.ai']","  A persistent challenge to table question answering (TableQA) by generatingexecutable programs has been adapting to varied table structures, typicallyrequiring domain-specific logical forms. In response, this paper introduces aunified TableQA framework that: (1) provides a unified representation forstructured tables as multi-index Pandas data frames, (2) uses Python as apowerful querying language, and (3) uses few-shot prompting to translate NLquestions into Python programs, which are executable on Pandas data frames.Furthermore, to answer complex relational questions with extended programfunctionality and external knowledge, our framework allows customized APIs thatPython programs can call. We experiment with four TableQA datasets that involvetables of different structures -- relational, multi-table, and hierarchicalmatrix shapes -- and achieve prominent improvements over past state-of-the-artsystems. In ablation studies, we (1) show benefits from our multi-indexrepresentation and APIs over baselines that use only an LLM, and (2)demonstrate that our approach is modular and can incorporate additional APIs."
Tree of Clarifications: Answering Ambiguous Questions with  Retrieval-Augmented Large Language Models,"['Gangwoo Kim', 'Sungdong Kim', 'Byeongguk Jeon', 'Joonsuk Park', 'Jaewoo Kang']",http://arxiv.org/pdf/2310.14696v1.pdf,2023-10-23,['cs.cl'],"  Questions in open-domain question answering are often ambiguous, allowingmultiple interpretations. One approach to handling them is to identify allpossible interpretations of the ambiguous question (AQ) and to generate along-form answer addressing them all, as suggested by Stelmakh et al., (2022).While it provides a comprehensive response without bothering the user forclarification, considering multiple dimensions of ambiguity and gatheringcorresponding knowledge remains a challenge. To cope with the challenge, wepropose a novel framework, Tree of Clarifications (ToC): It recursivelyconstructs a tree of disambiguations for the AQ -- via few-shot promptingleveraging external knowledge -- and uses it to generate a long-form answer.ToC outperforms existing baselines on ASQA in a few-shot setup across themetrics, while surpassing fully-supervised baselines trained on the wholetraining set in terms of Disambig-F1 and Disambig-ROUGE. Code is available athttps://github.com/gankim/tree-of-clarifications."
Dissecting In-Context Learning of Translations in GPTs,"['Vikas Raunak', 'Hany Hassan Awadalla', 'Arul Menezes']",http://arxiv.org/pdf/2310.15987v1.pdf,2023-10-24,"['cs.cl', 'cs.ai']","  Most of the recent work in leveraging Large Language Models (LLMs) such asGPT-3 for Machine Translation (MT) has focused on selecting the few-shotsamples for prompting. In this work, we try to better understand the role ofdemonstration attributes for the in-context learning of translations throughperturbations of high-quality, in-domain demonstrations. We find thatasymmetric perturbation of the source-target mappings yield vastly differentresults. We show that the perturbation of the source side has surprisinglylittle impact, while target perturbation can drastically reduce translationquality, suggesting that it is the output text distribution that provides themost important learning signal during in-context learning of translations. Wepropose a method named Zero-Shot-Context to add this signal automatically inZero-Shot prompting. We demonstrate that it improves upon the zero-shottranslation performance of GPT-3, even making it competitive with few-shotprompted translations."
Extraction of Atypical Aspects from Customer Reviews: Datasets and  Experiments with Language Models,"['Smita Nannaware', 'Erfan Al-Hossami', 'Razvan Bunescu']",http://arxiv.org/pdf/2311.02702v1.pdf,2023-11-05,"['cs.cl', 'cs.ai']","  A restaurant dinner may become a memorable experience due to an unexpectedaspect enjoyed by the customer, such as an origami-making station in thewaiting area. If aspects that are atypical for a restaurant experience wereknown in advance, they could be leveraged to make recommendations that have thepotential to engender serendipitous experiences, further increasing usersatisfaction. Although relatively rare, whenever encountered, atypical aspectsoften end up being mentioned in reviews due to their memorable quality.Correspondingly, in this paper we introduce the task of detecting atypicalaspects in customer reviews. To facilitate the development of extractionmodels, we manually annotate benchmark datasets of reviews in three domains -restaurants, hotels, and hair salons, which we use to evaluate a number oflanguage models, ranging from fine-tuning the instruction-based text-to-texttransformer Flan-T5 to zero-shot and few-shot prompting of GPT-3.5."
SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data,"['Ruoxi Sun', 'Sercan Ö. Arik', 'Rajarishi Sinha', 'Hootan Nakhost', 'Hanjun Dai', 'Pengcheng Yin', 'Tomas Pfister']",http://arxiv.org/pdf/2311.02883v1.pdf,2023-11-06,['cs.cl'],"  Text-to-SQL aims to automate the process of generating SQL queries on adatabase from natural language text. In this work, we propose ""SQLPrompt"",tailored to improve the few-shot prompting capabilities of Text-to-SQL forLarge Language Models (LLMs). Our methods include innovative prompt design,execution-based consistency decoding strategy which selects the SQL with themost consistent execution outcome among other SQL proposals, and a method thataims to improve performance by diversifying the SQL proposals duringconsistency selection with different prompt designs (""MixPrompt"") andfoundation models (""MixLLMs""). We show that \emph{SQLPrompt} outperformsprevious approaches for in-context learning with few labeled data by a largemargin, closing the gap with finetuning state-of-the-art with thousands oflabeled data."
OLaLa: Ontology Matching with Large Language Models,"['Sven Hertling', 'Heiko Paulheim']",http://arxiv.org/pdf/2311.03837v1.pdf,2023-11-07,"['cs.ir', 'cs.cl']","  Ontology (and more generally: Knowledge Graph) Matching is a challenging taskwhere information in natural language is one of the most important signals toprocess. With the rise of Large Language Models, it is possible to incorporatethis knowledge in a better way into the matching pipeline. A number ofdecisions still need to be taken, e.g., how to generate a prompt that is usefulto the model, how information in the KG can be formulated in prompts, whichLarge Language Model to choose, how to provide existing correspondences to themodel, how to generate candidates, etc. In this paper, we present a prototypethat explores these questions by applying zero-shot and few-shot prompting withmultiple open Large Language Models to different tasks of the OntologyAlignment Evaluation Initiative (OAEI). We show that with only a handful ofexamples and a well-designed prompt, it is possible to achieve results that areen par with supervised matching systems which use a much larger portion of theground truth."
Language Models can be Logical Solvers,"['Jiazhan Feng', 'Ruochen Xu', 'Junheng Hao', 'Hiteshi Sharma', 'Yelong Shen', 'Dongyan Zhao', 'Weizhu Chen']",http://arxiv.org/pdf/2311.06158v1.pdf,2023-11-10,"['cs.cl', 'cs.ai']","  Logical reasoning is a fundamental aspect of human intelligence and a keycomponent of tasks like problem-solving and decision-making. Recentadvancements have enabled Large Language Models (LLMs) to potentially exhibitreasoning capabilities, but complex logical reasoning remains a challenge. Thestate-of-the-art, solver-augmented language models, use LLMs to parse naturallanguage logical questions into symbolic representations first and then adoptexternal logical solvers to take in the symbolic representations and output theanswers. Despite their impressive performance, any parsing errors willinevitably result in the failure of the execution of the external logicalsolver and no answer to the logical questions. In this paper, we introduceLoGiPT, a novel language model that directly emulates the reasoning processesof logical solvers and bypasses the parsing errors by learning to strictadherence to solver syntax and grammar. LoGiPT is fine-tuned on a newlyconstructed instruction-tuning dataset derived from revealing and refining theinvisible reasoning process of deductive solvers. Experimental results on twopublic deductive reasoning datasets demonstrate that LoGiPT outperformsstate-of-the-art solver-augmented LMs and few-shot prompting methods oncompetitive LLMs like ChatGPT or GPT-4."
Automated Parliaments: A Solution to Decision Uncertainty and  Misalignment in Language Models,"['Thomas Forster', 'Jonathan Ouwerx', 'Shak Ragoler']",http://arxiv.org/pdf/2311.10098v1.pdf,2023-10-31,['cs.ai'],"  As AI takes on a greater role in the modern world, it is essential to ensurethat AI models can overcome decision uncertainty and remain aligned with humanmorality and interests. This research paper proposes a method for improving thedecision-making of language models (LMs) via Automated Parliaments (APs) -constructs made of AI delegates each representing a certain perspective.Delegates themselves consist of three AI models: generators, modifiers, andevaluators. We specify two mechanisms for producing optimal solutions: theSimultaneous Modification mechanism for response creation and an evaluationmechanism for fairly assessing solutions. The overall process begins when eachgenerator creates a response aligned with its delegate's theory. The modifiersalter all other responses to make them more self-aligned. The evaluatorscollectively assess the best end response. Finally, the modifiers andgenerators learn from feedback from the evaluators. In our research, we testedthe evaluation mechanism, comparing the use of single-value zero-shot promptingand AP few-shot prompting in evaluating morally contentious scenarios. We foundthat the AP architecture saw a 57.3% reduction in its loss value compared tothe baseline. We conclude by discussing some potential applications of APs andspecifically their potential impact when implemented as Automated MoralParliaments."
Meta Prompting for AGI Systems,['Yifan Zhang'],http://arxiv.org/pdf/2311.11482v1.pdf,2023-11-20,"['cs.ai', 'cs.cl']","  This paper presents an in-depth exploration of Meta Prompting, a noveltechnique that revolutionizes the way large language models (LLMs), multi-modalfoundation models, and AI systems approach problem-solving and datainterpretation. Meta Prompting, rooted in type theory and category theory,prioritizes the structure and syntax of information, providing a uniqueframework that transcends traditional content-focused methods. We delve intothe formal definitions of Meta Prompting, contrasting it with Few-ShotPrompting, and highlight its applicability and superiority in various AIapplications.  Key to this exploration is the expansion of Meta Prompting into the realm ofcomplex reasoning. Here, we demonstrate how this technique adeptly breaks downintricate problems into manageable sub-problems, facilitating a step-by-step,detailed approach to problem-solving. This method proves especiallyadvantageous in terms of token efficiency and offering a fair comparison inproblem-solving scenarios, standing out against few-shot example approaches.  Furthermore, the paper breaks new ground by extending Meta Prompting intomulti-modal foundation model settings. This extension addresses the integrationof diverse data types, such as images, audio, and video, within the structuredframework of Meta Prompting, highlighting both the challenges and the vastpotential of this approach in handling complex, multi-faceted data (The code isavailable at https://github.com/meta-prompting/meta-prompting)."
Zero-Shot Question Answering over Financial Documents using Large  Language Models,"['Karmvir Singh Phogat', 'Chetan Harsha', 'Sridhar Dasaratha', 'Shashishekar Ramakrishna', 'Sai Akhil Puranam']",http://arxiv.org/pdf/2311.14722v1.pdf,2023-11-19,"['cs.cl', 'cs.ai', 'cs.lg']","  We introduce a large language model (LLM) based approach to answer complexquestions requiring multi-hop numerical reasoning over financial reports. WhileLLMs have exhibited remarkable performance on various natural language andreasoning tasks, complex reasoning problems often rely on few-shot prompts thatrequire carefully crafted examples. In contrast, our approach uses novelzero-shot prompts that guide the LLM to encode the required reasoning into aPython program or a domain specific language. The generated program is thenexecuted by a program interpreter, thus mitigating the limitations of LLM inperforming accurate arithmetic calculations.  We evaluate the proposed approach on three financial datasets using some ofthe recently developed generative pretrained transformer (GPT) models andperform comparisons with various zero-shot baselines. The experimental resultsdemonstrate that our approach significantly improves the accuracy for all theLLMs over their respective baselines. We provide a detailed analysis of theresults, generating insights to support our findings. The success of ourapproach demonstrates the enormous potential to extract complex domain specificnumerical reasoning by designing zero-shot prompts to effectively exploit theknowledge embedded in LLMs."
Steering Llama 2 via Contrastive Activation Addition,"['Nina Rimsky', 'Nick Gabrieli', 'Julian Schulz', 'Meg Tong', 'Evan Hubinger', 'Alexander Matt Turner']",http://arxiv.org/pdf/2312.06681v1.pdf,2023-12-09,"['cs.cl', 'cs.ai', 'cs.lg']","  We introduce Contrastive Activation Addition (CAA), an innovative method forsteering language models by modifying activations during their forward passes.CAA computes ``steering vectors'' by averaging the difference in residualstream activations between pairs of positive and negative examples of aparticular behavior such as factual versus hallucinatory responses. Duringinference, these steering vectors are added at all token positions after theuser's prompt with either a positive or negative coefficient, allowing precisecontrol over the degree of the targeted behavior. We evaluate CAA'seffectiveness on Llama 2 Chat using both multiple-choice behavioral questiondatasets and open-ended generation tasks. We demonstrate that CAA significantlyalters model behavior, outperforms traditional methods like finetuning andfew-shot prompting, and minimally reduces capabilities. Moreover, by employingvarious activation space interpretation methods, we gain deeper insights intoCAA's mechanisms. CAA both accurately steers model outputs and also sheds lighton how high-level concepts are represented in Large Language Models (LLMs)."
Jurassic is (almost) All You Need: Few-Shot Meaning-to-Text Generation  for Open-Domain Dialogue,"['Lena Reed', 'Cecilia Li', 'Angela Ramirez', 'Liren Wu', 'Marilyn Walker']",http://arxiv.org/pdf/2110.08094v2.pdf,2021-10-15,['cs.cl'],"  One challenge with open-domain dialogue systems is the need to producetruthful, high-quality responses on any topic. We aim to improve the qualityand coverage of Athena, an Alexa Prize dialogue system. We experiment withfew-shot prompt-based learning, comparing GPT-Neo to Jurassic-1, for themovies, music, TV, sports, and video game domains, both within andcross-domain, with different prompt set sizes (2, 3, 10), formats, and meaningrepresentations consisting of either sets of WikiData KG triples, or dialogueacts. Our evaluation uses BLEURT and human metrics, and shows that with 10-shotprompting, Athena-Jurassic's performance is significantly better for coherenceand semantic accuracy. Experiments with 2-shot cross-domain prompts results ina huge performance drop for Athena-GPT-Neo, whose semantic accuracy falls to0.41, and whose untrue hallucination rate increases to 12%. Experiments withdialogue acts for video games show that with 10-shot prompting, both modelslearn to control dialogue acts, but Athena-Jurassic has significantly highercoherence, and only 4% untrue hallucinations. Our results suggest thatAthena-Jurassic produces high enough quality outputs to be useful in livesystems with real users. To our knowledge, these are the first resultsdemonstrating that few-shot semantic prompt-based learning can create NLGs thatgeneralize to new domains, and produce high-quality, semantically-controlled,conversational responses directly from meaning representations."
Code as Policies: Language Model Programs for Embodied Control,"['Jacky Liang', 'Wenlong Huang', 'Fei Xia', 'Peng Xu', 'Karol Hausman', 'Brian Ichter', 'Pete Florence', 'Andy Zeng']",http://arxiv.org/pdf/2209.07753v4.pdf,2022-09-16,['cs.ro'],"  Large language models (LLMs) trained on code completion have been shown to becapable of synthesizing simple Python programs from docstrings [1]. We findthat these code-writing LLMs can be re-purposed to write robot policy code,given natural language commands. Specifically, policy code can expressfunctions or feedback loops that process perception outputs (e.g.,from objectdetectors [2], [3]) and parameterize control primitive APIs. When provided asinput several example language commands (formatted as comments) followed bycorresponding policy code (via few-shot prompting), LLMs can take in newcommands and autonomously re-compose API calls to generate new policy coderespectively. By chaining classic logic structures and referencing third-partylibraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this waycan write robot policies that (i) exhibit spatial-geometric reasoning, (ii)generalize to new instructions, and (iii) prescribe precise values (e.g.,velocities) to ambiguous descriptions (""faster"") depending on context (i.e.,behavioral commonsense). This paper presents code as policies: a robot-centricformulation of language model generated programs (LMPs) that can representreactive policies (e.g., impedance controllers), as well as waypoint-basedpolicies (vision-based pick and place, trajectory-based control), demonstratedacross multiple real robot platforms. Central to our approach is promptinghierarchical code-gen (recursively defining undefined functions), which canwrite more complex code and also improves state-of-the-art to solve 39.8% ofproblems on the HumanEval [1] benchmark. Code and videos are available athttps://code-as-policies.github.io"
Spotlight: Mobile UI Understanding using Vision-Language Models with a  Focus,"['Gang Li', 'Yang Li']",http://arxiv.org/pdf/2209.14927v4.pdf,2022-09-29,"['cs.cv', 'cs.hc', 'cs.lg']","  Mobile UI understanding is important for enabling various interaction taskssuch as UI automation and accessibility. Previous mobile UI modeling oftendepends on the view hierarchy information of a screen, which directly providesthe structural data of the UI, with the hope to bypass challenging tasks ofvisual modeling from screen pixels. However, view hierarchies are not alwaysavailable, and are often corrupted with missing object descriptions ormisaligned structure information. As a result, despite the use of viewhierarchies could offer short-term gains, it may ultimately hinder theapplicability and performance of the model. In this paper, we proposeSpotlight, a vision-only approach for mobile UI understanding. Specifically, weenhance a vision-language model that only takes the screenshot of the UI and aregion of interest on the screen -- the focus -- as the input. This generalarchitecture of Spotlight is easily scalable and capable of performing a rangeof UI modeling tasks. Our experiments show that our model establishes SoTAresults on several representative UI tasks and outperforms previous methodsthat use both screenshots and view hierarchies as inputs. Furthermore, weexplore multi-task learning and few-shot prompting capacities of the proposedmodels, demonstrating promising results in the multi-task learning direction."
Grounding Language with Visual Affordances over Unstructured Data,"['Oier Mees', 'Jessica Borja-Diaz', 'Wolfram Burgard']",http://arxiv.org/pdf/2210.01911v3.pdf,2022-10-04,"['cs.ro', 'cs.ai', 'cs.cl', 'cs.cv', 'cs.lg']","  Recent works have shown that Large Language Models (LLMs) can be applied toground natural language to a wide variety of robot skills. However, inpractice, learning multi-task, language-conditioned robotic skills typicallyrequires large-scale data collection and frequent human intervention to resetthe environment or help correcting the current policies. In this work, wepropose a novel approach to efficiently learn general-purposelanguage-conditioned robot skills from unstructured, offline and reset-freedata in the real world by exploiting a self-supervised visuo-lingual affordancemodel, which requires annotating as little as 1% of the total data withlanguage. We evaluate our method in extensive experiments both in simulated andreal-world robotic tasks, achieving state-of-the-art performance on thechallenging CALVIN benchmark and learning over 25 distinct visuomotormanipulation tasks with a single policy in the real world. We find that whenpaired with LLMs to break down abstract natural language instructions intosubgoals via few-shot prompting, our method is capable of completinglong-horizon, multi-tier tasks in the real world, while requiring an order ofmagnitude less data than previous approaches. Code and videos are available athttp://hulc2.cs.uni-freiburg.de"
MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for  Vision-Language Few-Shot Prompting,"['Oscar Mañas', 'Pau Rodriguez', 'Saba Ahmadi', 'Aida Nematzadeh', 'Yash Goyal', 'Aishwarya Agrawal']",http://arxiv.org/pdf/2210.07179v2.pdf,2022-10-13,"['cs.cv', 'cs.ai', 'cs.cl', 'cs.lg']","  Large pre-trained models have proved to be remarkable zero- and(prompt-based) few-shot learners in unimodal vision and language tasks. Wepropose MAPL, a simple and parameter-efficient method that reuses frozenpre-trained unimodal models and leverages their strong generalizationcapabilities in multimodal vision-language (VL) settings. MAPL learns alightweight mapping between the representation spaces of unimodal models usingaligned image-text data, and can generalize to unseen VL tasks from just a fewin-context examples. The small number of trainable parameters makes MAPLeffective at low-data and in-domain learning. Moreover, MAPL's modularityenables easy extension to other pre-trained models. Extensive experiments onseveral visual question answering and image captioning benchmarks show thatMAPL achieves superior or competitive performance compared to similar methodswhile training orders of magnitude fewer parameters. MAPL can be trained injust a few hours using modest computational resources and public datasets. Werelease our code and pre-trained model weights athttps://github.com/mair-lab/mapl."
Model ensemble instead of prompt fusion: a sample-specific knowledge  transfer method for few-shot prompt tuning,"['Xiangyu Peng', 'Chen Xing', 'Prafulla Kumar Choubey', 'Chien-Sheng Wu', 'Caiming Xiong']",http://arxiv.org/pdf/2210.12587v3.pdf,2022-10-23,['cs.cl'],"  Prompt tuning approaches, which learn task-specific soft prompts for adownstream task conditioning on frozen pre-trained models, have attractedgrowing interest due to its parameter efficiency. With large language modelsand sufficient training data, prompt tuning performs comparably to full-modeltuning. However, with limited training samples in few-shot settings, prompttuning fails to match the performance of full-model fine-tuning. In this work,we focus on improving the few-shot performance of prompt tuning by transferringknowledge from soft prompts of source tasks. Recognizing the goodgeneralization capabilities of ensemble methods in low-data regime, we firstexperiment and show that a simple ensemble of model predictions based ondifferent source prompts, outperforms existing multi-prompt knowledge transferapproaches such as source prompt fusion in the few-shot setting. Motivated bythis observation, we further investigate model ensembles and proposeSample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust thecontribution of each source model for each target sample separately whenensembling source model outputs. Through this way, SESoM inherits the superiorgeneralization of model ensemble approaches and simultaneously captures thesample-specific competence of each source prompt. We conduct experiments acrossa diverse set of eight NLP tasks using models of different scales (T5-{base,large, XL}) and find that SESoM consistently outperforms the existing models ofthe same as well as larger parametric scale by a large margin."
Are Hard Examples also Harder to Explain? A Study with Human and  Model-Generated Explanations,"['Swarnadeep Saha', 'Peter Hase', 'Nazneen Rajani', 'Mohit Bansal']",http://arxiv.org/pdf/2211.07517v1.pdf,2022-11-14,"['cs.cl', 'cs.ai']","  Recent work on explainable NLP has shown that few-shot prompting can enablelarge pretrained language models (LLMs) to generate grammatical and factualnatural language explanations for data labels. In this work, we study theconnection between explainability and sample hardness by investigating thefollowing research question - ""Are LLMs and humans equally good at explainingdata labels for both easy and hard samples?"" We answer this question by firstcollecting human-written explanations in the form of generalizable commonsenserules on the task of Winograd Schema Challenge (Winogrande dataset). We comparethese explanations with those generated by GPT-3 while varying the hardness ofthe test samples as well as the in-context samples. We observe that (1) GPT-3explanations are as grammatical as human explanations regardless of thehardness of the test samples, (2) for easy examples, GPT-3 generates highlysupportive explanations but human explanations are more generalizable, and (3)for hard examples, human explanations are significantly better than GPT-3explanations both in terms of label-supportiveness and generalizabilityjudgements. We also find that hardness of the in-context examples impacts thequality of GPT-3 explanations. Finally, we show that the supportiveness andgeneralizability aspects of human explanations are also impacted by samplehardness, although by a much smaller margin than models. Supporting code anddata are available at https://github.com/swarnaHub/ExplanationHardness"
Crowd Score: A Method for the Evaluation of Jokes using Large Language  Model AI Voters as Judges,"['Fabricio Goes', 'Zisen Zhou', 'Piotr Sawicki', 'Marek Grzes', 'Daniel G. Brown']",http://arxiv.org/pdf/2212.11214v1.pdf,2022-12-21,['cs.ai'],"  This paper presents the Crowd Score, a novel method to assess the funninessof jokes using large language models (LLMs) as AI judges. Our method relies oninducing different personalities into the LLM and aggregating the votes of theAI judges into a single score to rate jokes. We validate the votes using anauditing technique that checks if the explanation for a particular vote isreasonable using the LLM. We tested our methodology on 52 jokes in a crowd offour AI voters with different humour types: affiliative, self-enhancing,aggressive and self-defeating. Our results show that few-shot prompting leadsto better results than zero-shot for the voting question. Personality inductionshowed that aggressive and self-defeating voters are significantly moreinclined to find more jokes funny of a set of aggressive/self-defeating jokesthan the affiliative and self-enhancing voters. The Crowd Score follows thesame trend as human judges by assigning higher scores to jokes that are alsoconsidered funnier by human judges. We believe that our methodology could beapplied to other creative domains such as story, poetry, slogans, etc. It couldboth help the adoption of a flexible and accurate standard approach to comparedifferent work in the CC community under a common metric and by minimizinghuman participation in assessing creative artefacts, it could accelerate theprototyping of creative artefacts and reduce the cost of hiring humanparticipants to rate creative artefacts."
CodeLMSec Benchmark: Systematically Evaluating and Finding Security  Vulnerabilities in Black-Box Code Language Models,"['Hossein Hajipour', 'Keno Hassler', 'Thorsten Holz', 'Lea Schönherr', 'Mario Fritz']",http://arxiv.org/pdf/2302.04012v2.pdf,2023-02-08,"['cs.cr', 'cs.ai', 'cs.cl', 'cs.lg', 'cs.se']","  Large language models (LLMs) for automatic code generation have achievedbreakthroughs in several programming tasks. Their advances in competition-levelprogramming problems have made them an essential pillar of AI-assisted pairprogramming, and tools such as GitHub Copilot have emerged as part of the dailyprogramming workflow used by millions of developers. The training data forthese models is usually collected from the Internet (e.g., from open-sourcerepositories) and is likely to contain faults and security vulnerabilities.This unsanitized training data can cause the language models to learn thesevulnerabilities and propagate them during the code generation procedure. Whilethese models have been extensively assessed for their ability to producefunctionally correct programs, there remains a lack of comprehensiveinvestigations and benchmarks addressing the security aspects of these models.  In this work, we propose a method to systematically study the security issuesof code language models to assess their susceptibility to generating vulnerablecode. To this end, we introduce the first approach to automatically findgenerated code that contains vulnerabilities in black-box code generationmodels. To achieve this, we present an approach to approximate inversion of theblack-box code generation models based on few-shot prompting. We evaluate theeffectiveness of our approach by examining code language models in generatinghigh-risk security weaknesses. Furthermore, we establish a collection ofdiverse non-secure prompts for various vulnerability scenarios using ourmethod. This dataset forms a benchmark for evaluating and comparing thesecurity weaknesses in code language models."
ART: Automatic multi-step reasoning and tool-use for large language  models,"['Bhargavi Paranjape', 'Scott Lundberg', 'Sameer Singh', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer', 'Marco Tulio Ribeiro']",http://arxiv.org/pdf/2303.09014v1.pdf,2023-03-16,['cs.cl'],"  Large language models (LLMs) can perform complex reasoning in few- andzero-shot settings by generating intermediate chain of thought (CoT) reasoningsteps. Further, each reasoning step can rely on external tools to supportcomputation beyond the core LLM capabilities (e.g. search/running code). Priorwork on CoT prompting and tool use typically requires hand-craftingtask-specific demonstrations and carefully scripted interleaving of modelgenerations with tool use. We introduce Automatic Reasoning and Tool-use (ART),a framework that uses frozen LLMs to automatically generate intermediatereasoning steps as a program. Given a new task to solve, ART selectsdemonstrations of multi-step reasoning and tool use from a task library. Attest time, ART seamlessly pauses generation whenever external tools are called,and integrates their output before resuming generation. ART achieves asubstantial improvement over few-shot prompting and automatic CoT on unseentasks in the BigBench and MMLU benchmarks, and matches performance ofhand-crafted CoT prompts on a majority of these tasks. ART is also extensible,and makes it easy for humans to improve performance by correcting errors intask-specific programs or incorporating new tools, which we demonstrate bydrastically improving performance on select tasks with minimal humanintervention."
Fairness-guided Few-shot Prompting for Large Language Models,"['Huan Ma', 'Changqing Zhang', 'Yatao Bian', 'Lemao Liu', 'Zhirui Zhang', 'Peilin Zhao', 'Shu Zhang', 'Huazhu Fu', 'Qinghua Hu', 'Bingzhe Wu']",http://arxiv.org/pdf/2303.13217v3.pdf,2023-03-23,"['cs.cl', 'cs.ai']","  Large language models have demonstrated surprising ability to performin-context learning, i.e., these models can be directly applied to solvenumerous downstream tasks by conditioning on a prompt constructed by a fewinput-output examples. However, prior research has shown that in-contextlearning can suffer from high instability due to variations in trainingexamples, example order, and prompt formats. Therefore, the construction of anappropriate prompt is essential for improving the performance of in-contextlearning. In this paper, we revisit this problem from the view of predictivebias. Specifically, we introduce a metric to evaluate the predictive bias of afixed prompt against labels or a given attributes. Then we empirically showthat prompts with higher bias always lead to unsatisfactory predictive quality.Based on this observation, we propose a novel search strategy based on thegreedy search to identify the near-optimal prompt for improving the performanceof in-context learning. We perform comprehensive experiments withstate-of-the-art mainstream models such as GPT-3 on various downstream tasks.Our results indicate that our method can enhance the model's in-contextlearning performance in an effective and interpretable manner."
Is ChatGPT a Good Recommender? A Preliminary Study,"['Junling Liu', 'Chao Liu', 'Peilin Zhou', 'Renjie Lv', 'Kang Zhou', 'Yan Zhang']",http://arxiv.org/pdf/2304.10149v3.pdf,2023-04-20,['cs.ir'],"  Recommendation systems have witnessed significant advancements and have beenwidely used over the past decades. However, most traditional recommendationmethods are task-specific and therefore lack efficient generalization ability.Recently, the emergence of ChatGPT has significantly advanced NLP tasks byenhancing the capabilities of conversational models. Nonetheless, theapplication of ChatGPT in the recommendation domain has not been thoroughlyinvestigated. In this paper, we employ ChatGPT as a general-purposerecommendation model to explore its potential for transferring extensivelinguistic and world knowledge acquired from large-scale corpora torecommendation scenarios. Specifically, we design a set of prompts and evaluateChatGPT's performance on five recommendation scenarios. Unlike traditionalrecommendation methods, we do not fine-tune ChatGPT during the entireevaluation process, relying only on the prompts themselves to convertrecommendation tasks into natural language tasks. Further, we explore the useof few-shot prompting to inject interaction information that contains userpotential interest to help ChatGPT better understand user needs and interests.Comprehensive experimental results on Amazon Beauty dataset show that ChatGPThas achieved promising results in certain tasks and is capable of reaching thebaseline level in others. We conduct human evaluations on twoexplainability-oriented tasks to more accurately evaluate the quality ofcontents generated by different models. And the human evaluations show ChatGPTcan truly understand the provided information and generate clearer and morereasonable results. We hope that our study can inspire researchers to furtherexplore the potential of language models like ChatGPT to improve recommendationperformance and contribute to the advancement of the recommendation systemsfield."
Language Models Don't Always Say What They Think: Unfaithful  Explanations in Chain-of-Thought Prompting,"['Miles Turpin', 'Julian Michael', 'Ethan Perez', 'Samuel R. Bowman']",http://arxiv.org/pdf/2305.04388v2.pdf,2023-05-07,"['cs.cl', 'cs.ai']","  Large Language Models (LLMs) can achieve strong performance on many tasks byproducing step-by-step reasoning before giving a final output, often referredto as chain-of-thought reasoning (CoT). It is tempting to interpret these CoTexplanations as the LLM's process for solving a task. This level oftransparency into LLMs' predictions would yield significant safety benefits.However, we find that CoT explanations can systematically misrepresent the truereason for a model's prediction. We demonstrate that CoT explanations can beheavily influenced by adding biasing features to model inputs--e.g., byreordering the multiple-choice options in a few-shot prompt to make the answeralways ""(A)""--which models systematically fail to mention in theirexplanations. When we bias models toward incorrect answers, they frequentlygenerate CoT explanations rationalizing those answers. This causes accuracy todrop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testingwith GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task,model explanations justify giving answers in line with stereotypes withoutmentioning the influence of these social biases. Our findings indicate that CoTexplanations can be plausible yet misleading, which risks increasing our trustin LLMs without guaranteeing their safety. Building more transparent andexplainable systems will require either improving CoT faithfulness throughtargeted efforts or abandoning CoT in favor of alternative methods."
Skill-Based Few-Shot Selection for In-Context Learning,"['Shengnan An', 'Bo Zhou', 'Zeqi Lin', 'Qiang Fu', 'Bei Chen', 'Nanning Zheng', 'Weizhu Chen', 'Jian-Guang Lou']",http://arxiv.org/pdf/2305.14210v2.pdf,2023-05-23,"['cs.cl', 'cs.ai']","  In-context learning is the paradigm that adapts large language models todownstream tasks by providing a few examples. Few-shot selection -- selectingappropriate examples for each test instance separately -- is important forin-context learning. In this paper, we propose Skill-KNN, a skill-basedfew-shot selection method for in-context learning. The key advantages ofSkill-KNN include: (1) it addresses the problem that existing methods based onpre-trained embeddings can be easily biased by surface natural languagefeatures that are not important for the target task; (2) it does not requiretraining or fine-tuning of any models, making it suitable for frequentlyexpanding or changing example banks. The key insight is to optimize the inputsfed into the embedding model, rather than tuning the model itself. Technically,Skill-KNN generates the skill-based descriptions for each test case andcandidate example by utilizing a pre-processing few-shot prompting, thuseliminating unimportant surface features. Experimental results across fivecross-domain semantic parsing datasets and six backbone models show thatSkill-KNN significantly outperforms existing methods."
USB: A Unified Summarization Benchmark Across Tasks and Domains,"['Kundan Krishna', 'Prakhar Gupta', 'Sanjana Ramprasad', 'Byron C. Wallace', 'Jeffrey P. Bigham', 'Zachary C. Lipton']",http://arxiv.org/pdf/2305.14296v2.pdf,2023-05-23,"['cs.cl', 'cs.lg']","  While the NLP community has produced numerous summarization benchmarks, noneprovide the rich annotations required to simultaneously address many importantproblems related to control and reliability. We introduce a Wikipedia-derivedbenchmark, complemented by a rich set of crowd-sourced annotations, thatsupports $8$ interrelated tasks: (i) extractive summarization; (ii) abstractivesummarization; (iii) topic-based summarization; (iv) compressing selectedsentences into a one-line summary; (v) surfacing evidence for a summarysentence; (vi) predicting the factual accuracy of a summary sentence; (vii)identifying unsubstantiated spans in a summary sentence; (viii) correctingfactual errors in summaries. We compare various methods on this benchmark anddiscover that on multiple tasks, moderately-sized fine-tuned modelsconsistently outperform much larger few-shot prompted language models. Forfactuality-related tasks, we also evaluate existing heuristics to createtraining data and find that training on them results in worse performance thantraining on $20\times$ less human-labeled data. Our articles draw from $6$domains, facilitating cross-domain analysis. On some tasks, the amount oftraining data matters more than the domain where it comes from, while for othertasks training specifically on data from the target domain, even if limited, ismore beneficial."
Self-Polish: Enhance Reasoning in Large Language Models via Problem  Refinement,"['Zhiheng Xi', 'Senjie Jin', 'Yuhao Zhou', 'Rui Zheng', 'Songyang Gao', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang']",http://arxiv.org/pdf/2305.14497v1.pdf,2023-05-23,"['cs.cl', 'cs.ai']","  Prompting methods such as Chain-of-Thought (CoT) have shed new light onenhancing the reasoning capabilities of large language models, and researchershave extensively explored the generation process of rationales and answers.However, they have overlooked the potential challenges posed by the poorquality of reasoning problems, which may influence the reasoning performancesignificantly. In this work, we propose Self-Polish (SP), a novel method thatfacilitates the model's problem-solving process by prompting them toprogressively refine the given problems to be more comprehensible and solvable.Specifically, the method teaches models to eliminate irrelevant information,rearrange the logic structure and organize local conditions into new onesparallelly. SP is orthogonal to all other prompting methods, making itconvenient to integrate with state-of-the-art techniques for furtherimprovement. We conduct thorough experiments on five benchmarks to illustratethe effectiveness of the proposed method. For example, with Text-davinci-003,our method boosts the performance of standard few-shot prompting by $8.0\%$ onGSM8K and $17.8\%$ on MultiArith; it also improves the performance of CoT by$6.0\%$ on GSM8K and $6.0\%$ on MathQA, respectively. Furthermore, our methodalso showcases impressive performance on robustness evaluation."
SciFix: Outperforming GPT3 on Scientific Factual Error Correction,"['Dhananjay Ashok', 'Atharva Kulkarni', 'Hai Pham', 'Barnabás Póczos']",http://arxiv.org/pdf/2305.14707v2.pdf,2023-05-24,"['cs.cl', 'cs.ai', 'cs.lg']","  Due to the prohibitively high cost of creating error correction datasets,most Factual Claim Correction methods rely on a powerful verification model toguide the correction process. This leads to a significant drop in performancein domains like scientific claims, where good verification models do not alwaysexist. In this work, we introduce SciFix, a scientific claim correction systemthat does not require a verifier but can outperform existing methods by aconsiderable margin -- achieving correction accuracy of 84% on the SciFactdataset, 77% on SciFact-Open and 72% on the CovidFact dataset, compared to nextbest accuracies of 7%, 5%, and 15% on the same datasets respectively. Ourmethod leverages the power of prompting with LLMs during training to create arichly annotated dataset that can be used for fully supervised training andregularization. We additionally use a claim-aware decoding procedure to improvethe quality of corrected claims. Our method outperforms the very LLM that wasused to generate the annotated dataset -- with Few-Shot Prompting on GPT3.5achieving 58%, 61%, and 64% on the respective datasets, a consistently lowercorrection accuracy, despite using nearly 800 times as many parameters as ourmodel."
LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and  Unlabeled Image Collections,"['M. Jehanzeb Mirza', 'Leonid Karlinsky', 'Wei Lin', 'Mateusz Kozinski', 'Horst Possegger', 'Rogerio Feris', 'Horst Bischof']",http://arxiv.org/pdf/2305.18287v2.pdf,2023-05-29,"['cs.cv', 'cs.cl']","  Recently, large-scale pre-trained Vision and Language (VL) models have set anew state-of-the-art (SOTA) in zero-shot visual classification enablingopen-vocabulary recognition of potentially unlimited set of categories definedas simple language prompts. However, despite these great advances, theperformance of these zeroshot classifiers still falls short of the results ofdedicated (closed category set) classifiers trained with supervised finetuning. In this paper we show, for the first time, how to reduce this gapwithout any labels and without any paired VL data, using an unlabeled imagecollection and a set of texts auto-generated using a Large Language Model (LLM)describing the categories of interest and effectively substituting labeledvisual instances of those categories. Using our label-free approach, we areable to attain significant performance improvements over the zero-shotperformance of the base VL model and other contemporary methods and baselineson a wide variety of datasets, demonstrating absolute improvement of up to11.7% (3.8% on average) in the label-free setting. Moreover, despite ourapproach being label-free, we observe 1.3% average gains over leading few-shotprompting baselines that do use 5-shot supervision."
"Better patching using LLM prompting, via Self-Consistency","['Toufique Ahmed', 'Premkumar Devanbu']",http://arxiv.org/pdf/2306.00108v2.pdf,2023-05-31,"['cs.se', 'cs.lg']","  Large Language models (LLMs) can be induced to solve non-trivial problemswith ""few-shot"" prompts including illustrative problem-solution examples. Nowif the few-shots also include ""chain of thought"" (CoT) explanations, which areof the form problem-explanation-solution, LLMs will generate a ""explained""solution, and perform even better. Recently an exciting, substantially bettertechnique, self-consistency [1] (S-C) has emerged, based on the intuition thatthere are many plausible explanations for the right solution; when the LLM issampled repeatedly to generate a pool of explanation-solution pairs, for agiven problem, the most frequently occurring solutions in the pool (ignoringthe explanations) tend to be even more likely to be correct! Unfortunately, theuse of this highly-performant S-C (or even CoT) approach in softwareengineering settings is hampered by the lack of explanations; most softwaredatasets lack explanations. In this paper, we describe an application of theS-C approach to program repair, using the commit log on the fix as theexplanation, only in the illustrative few-shots. We achieve state-of-the artresults, beating previous approaches to prompting-based program repair, on theMODIT dataset; we also find evidence suggesting that the correct commitmessages are helping the LLM learn to produce better patches."
Large Language Models as Tax Attorneys: A Case Study in Legal  Capabilities Emergence,"['John J. Nay', 'David Karamardian', 'Sarah B. Lawsky', 'Wenting Tao', 'Meghana Bhat', 'Raghav Jain', 'Aaron Travis Lee', 'Jonathan H. Choi', 'Jungo Kasai']",http://arxiv.org/pdf/2306.07075v1.pdf,2023-06-12,"['cs.cl', 'cs.ai', 'cs.cy']","  Better understanding of Large Language Models' (LLMs) legal analysisabilities can contribute to improving the efficiency of legal services,governing artificial intelligence, and leveraging LLMs to identifyinconsistencies in law. This paper explores LLM capabilities in applying taxlaw. We choose this area of law because it has a structure that allows us toset up automated validation pipelines across thousands of examples, requireslogical reasoning and maths skills, and enables us to test LLM capabilities ina manner relevant to real-world economic lives of citizens and companies. Ourexperiments demonstrate emerging legal understanding capabilities, withimproved performance in each subsequent OpenAI model release. We experimentwith retrieving and utilising the relevant legal authority to assess the impactof providing additional legal context to LLMs. Few-shot prompting, presentingexamples of question-answer pairs, is also found to significantly enhance theperformance of the most advanced model, GPT-4. The findings indicate that LLMs,particularly when combined with prompting enhancements and the correct legaltexts, can perform at high levels of accuracy but not yet at expert tax lawyerlevels. As LLMs continue to advance, their ability to reason about lawautonomously could have significant implications for the legal profession andAI governance."
DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks,"['Caixin Kang', 'Yinpeng Dong', 'Zhengyi Wang', 'Shouwei Ruan', 'Yubo Chen', 'Hang Su', 'Xingxing Wei']",http://arxiv.org/pdf/2306.09124v3.pdf,2023-06-15,"['cs.cv', 'cs.ai', 'cs.cr', 'cs.lg']","  Adversarial attacks, particularly patch attacks, pose significant threats tothe robustness and reliability of deep learning models. Developing reliabledefenses against patch attacks is crucial for real-world applications, yetcurrent research in this area is unsatisfactory. In this paper, we proposeDIFFender, a novel defense method that leverages a text-guided diffusion modelto defend against adversarial patches. DIFFender includes two main stages:patch localization and patch restoration. In the localization stage, we findand exploit an intriguing property of the diffusion model to precisely identifythe locations of adversarial patches. In the restoration stage, we employ thediffusion model to reconstruct the adversarial regions in the images whilepreserving the integrity of the visual content. Thanks to the former finding,these two stages can be simultaneously guided by a unified diffusion model.Thus, we can utilize the close interaction between them to improve the wholedefense performance. Moreover, we propose a few-shot prompt-tuning algorithm tofine-tune the diffusion model, enabling the pre-trained diffusion model toadapt to the defense task easily. We conduct extensive experiments on imageclassification, face recognition, and further in the physical world,demonstrating that our proposed method exhibits superior robustness understrong adaptive attacks and generalizes well across various scenarios, diverseclassifiers, and multiple patch attack methods."
Teaching Arithmetic to Small Transformers,"['Nayoung Lee', 'Kartik Sreenivasan', 'Jason D. Lee', 'Kangwook Lee', 'Dimitris Papailiopoulos']",http://arxiv.org/pdf/2307.03381v1.pdf,2023-07-07,['cs.lg'],"  Large language models like GPT-4 exhibit emergent capabilities acrossgeneral-purpose tasks, such as basic arithmetic, when trained on extensive textdata, even though these tasks are not explicitly encoded by the unsupervised,next-token prediction objective. This study investigates how smalltransformers, trained from random initialization, can efficiently learnarithmetic operations such as addition, multiplication, and elementaryfunctions like square root, using the next-token prediction objective. We firstdemonstrate that conventional training data is not the most effective forarithmetic learning, and simple formatting changes can significantly improveaccuracy. This leads to sharp phase transitions as a function of training datascale, which, in some cases, can be explained through connections to low-rankmatrix completion. Building on prior work, we then train on chain-of-thoughtstyle data that includes intermediate step results. Even in the completeabsence of pretraining, this approach significantly and simultaneously improvesaccuracy, sample complexity, and convergence speed. We also study the interplaybetween arithmetic and text data during training and examine the effects offew-shot prompting, pretraining, and model scale. Additionally, we discusslength generalization challenges. Our work highlights the importance ofhigh-quality, instructive data that considers the particular characteristics ofthe next-word prediction objective for rapidly eliciting arithmeticcapabilities."
Controllable Generation of Dialogue Acts for Dialogue Systems via  Few-Shot Response Generation and Ranking,"['Angela Ramirez', 'Karik Agarwal', 'Juraj Juraska', 'Utkarsh Garg', 'Marilyn A. Walker']",http://arxiv.org/pdf/2307.14440v1.pdf,2023-07-26,['cs.cl'],"  Dialogue systems need to produce responses that realize multiple types ofdialogue acts (DAs) with high semantic fidelity. In the past, natural languagegenerators (NLGs) for dialogue were trained on large parallel corpora that mapfrom a domain-specific DA and its semantic attributes to an output utterance.Recent work shows that pretrained language models (LLMs) offer newpossibilities for controllable NLG using prompt-based learning. Here we developa novel few-shot overgenerate-and-rank approach that achieves the controlledgeneration of DAs. We compare eight few-shot prompt styles that include a novelmethod of generating from textual pseudo-references using a textual styletransfer approach. We develop six automatic ranking functions that identifyoutputs with both the correct DA and high semantic accuracy at generation time.We test our approach on three domains and four LLMs. To our knowledge, this isthe first work on NLG for dialogue that automatically ranks outputs using bothDA and attribute accuracy. For completeness, we compare our results tofine-tuned few-shot models trained with 5 to 100 instances per DA. Our resultsshow that several prompt settings achieve perfect DA accuracy, and near perfectsemantic accuracy (99.81%) and perform better than few-shot fine-tuning."
Contextual Biasing of Named-Entities with Large Language Models,"['Chuanneng Sun', 'Zeeshan Ahmed', 'Yingyi Ma', 'Zhe Liu', 'Lucas Kabela', 'Yutong Pang', 'Ozlem Kalinli']",http://arxiv.org/pdf/2309.00723v2.pdf,2023-09-01,"['cs.cl', 'cs.ai', 'cs.lg', 'cs.sd', 'eess.as', '68t10', 'i.2.7']","  This paper studies contextual biasing with Large Language Models (LLMs),where during second-pass rescoring additional contextual information isprovided to a LLM to boost Automatic Speech Recognition (ASR) performance. Wepropose to leverage prompts for a LLM without fine tuning during rescoringwhich incorporate a biasing list and few-shot examples to serve as additionalinformation when calculating the score for the hypothesis. In addition tofew-shot prompt learning, we propose multi-task training of the LLM to predictboth the entity class and the next token. To improve the efficiency forcontextual biasing and to avoid exceeding LLMs' maximum sequence lengths, wepropose dynamic prompting, where we select the most likely class using theclass tag prediction, and only use entities in this class as contexts for nexttoken prediction. Word Error Rate (WER) evaluation is performed on i) aninternal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopulidataset. Results indicate that biasing lists and few-shot examples can achieve17.8% and 9.6% relative improvement compared to first pass ASR, and thatmulti-task training and dynamic prompting can achieve 20.0% and 11.3% relativeWER improvement, respectively."
MindAgent: Emergent Gaming Interaction,"['Ran Gong', 'Qiuyuan Huang', 'Xiaojian Ma', 'Hoi Vo', 'Zane Durante', 'Yusuke Noda', 'Zilong Zheng', 'Song-Chun Zhu', 'Demetri Terzopoulos', 'Li Fei-Fei', 'Jianfeng Gao']",http://arxiv.org/pdf/2309.09971v2.pdf,2023-09-18,"['cs.ai', 'cs.hc', 'cs.ma']","  Large Language Models (LLMs) have the capacity of performing complexscheduling in a multi-agent system and can coordinate these agents intocompleting sophisticated tasks that require extensive collaboration. However,despite the introduction of numerous gaming frameworks, the community hasinsufficient benchmarks towards building general multi-agents collaborationinfrastructure that encompass both LLM and human-NPCs collaborations. In thiswork, we propose a novel infrastructure - MindAgent - to evaluate planning andcoordination emergent capabilities for gaming interaction. In particular, ourinfrastructure leverages existing gaming framework, to i) require understandingof the coordinator for a multi-agent system, ii) collaborate with human playersvia un-finetuned proper instructions, and iii) establish an in-context learningon few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a newgaming scenario and related benchmark that dispatch a multi-agent collaborationefficiency and supervise multiple agents playing the game simultaneously. Weconduct comprehensive evaluations with new auto-metric CoS for calculating thecollaboration efficiency. Finally, our infrastructure can be deployed intoreal-world gaming scenarios in a customized VR version of CUISINEWORLD andadapted in existing broader Minecraft gaming domain. We hope our findings onLLMs and the new infrastructure for general-purpose scheduling and coordinationcan help shed light on how such skills can be obtained by learning from largelanguage corpora."
DSPy: Compiling Declarative Language Model Calls into Self-Improving  Pipelines,"['Omar Khattab', 'Arnav Singhvi', 'Paridhi Maheshwari', 'Zhiyuan Zhang', 'Keshav Santhanam', 'Sri Vardhamanan', 'Saiful Haq', 'Ashutosh Sharma', 'Thomas T. Joshi', 'Hanna Moazam', 'Heather Miller', 'Matei Zaharia', 'Christopher Potts']",http://arxiv.org/pdf/2310.03714v1.pdf,2023-10-05,"['cs.cl', 'cs.ai', 'cs.ir', 'cs.lg']","  The ML community is rapidly exploring techniques for prompting languagemodels (LMs) and for stacking them into pipelines that solve complex tasks.Unfortunately, existing LM pipelines are typically implemented using hard-coded""prompt templates"", i.e. lengthy strings discovered via trial and error. Towarda more systematic approach for developing and optimizing LM pipelines, weintroduce DSPy, a programming model that abstracts LM pipelines as texttransformation graphs, i.e. imperative computational graphs where LMs areinvoked through declarative modules. DSPy modules are parameterized, meaningthey can learn (by creating and collecting demonstrations) how to applycompositions of prompting, finetuning, augmentation, and reasoning techniques.We design a compiler that will optimize any DSPy pipeline to maximize a givenmetric. We conduct two case studies, showing that succinct DSPy programs canexpress and optimize sophisticated LM pipelines that reason about math wordproblems, tackle multi-hop retrieval, answer complex questions, and controlagent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 andllama2-13b-chat to self-bootstrap pipelines that outperform standard few-shotprompting (generally by over 25% and 65%, respectively) and pipelines withexpert-created demonstrations (by up to 5-46% and 16-40%, respectively). On topof that, DSPy programs compiled to open and relatively small LMs like770M-parameter T5 and llama2-13b-chat are competitive with approaches that relyon expert-written prompt chains for proprietary GPT-3.5. DSPy is available athttps://github.com/stanfordnlp/dspy"
InterroLang: Exploring NLP Models and Datasets through Dialogue-based  Explanations,"['Nils Feldhus', 'Qianli Wang', 'Tatiana Anikina', 'Sahil Chopra', 'Cennet Oguz', 'Sebastian Möller']",http://arxiv.org/pdf/2310.05592v2.pdf,2023-10-09,"['cs.cl', 'cs.ai', 'cs.hc']","  While recently developed NLP explainability methods let us open the black boxin various ways (Madsen et al., 2022), a missing ingredient in this endeavor isan interactive tool offering a conversational interface. Such a dialogue systemcan help users explore datasets and models with explanations in acontextualized manner, e.g. via clarification or follow-up questions, andthrough a natural language interface. We adapt the conversational explanationframework TalkToModel (Slack et al., 2022) to the NLP domain, add newNLP-specific operations such as free-text rationalization, and illustrate itsgeneralizability on three NLP tasks (dialogue act classification, questionanswering, hate speech detection). To recognize user queries for explanations,we evaluate fine-tuned and few-shot prompting models and implement a novelAdapter-based approach. We then conduct two user studies on (1) the perceivedcorrectness and helpfulness of the dialogues, and (2) the simulatability, i.e.how objectively helpful dialogical explanations are for humans in figuring outthe model's predicted label when it's not shown. We found rationalization andfeature attribution were helpful in explaining the model behavior. Moreover,users could more reliably predict the model outcome based on an explanationdialogue rather than one-off explanations."
FireAct: Toward Language Agent Fine-tuning,"['Baian Chen', 'Chang Shu', 'Ehsan Shareghi', 'Nigel Collier', 'Karthik Narasimhan', 'Shunyu Yao']",http://arxiv.org/pdf/2310.05915v1.pdf,2023-10-09,"['cs.cl', 'cs.ai', 'cs.lg']","  Recent efforts have augmented language models (LMs) with external tools orenvironments, leading to the development of language agents that can reason andact. However, most of these agents rely on few-shot prompting techniques withoff-the-shelf LMs. In this paper, we investigate and argue for the overlookeddirection of fine-tuning LMs to obtain language agents. Using a setup ofquestion answering (QA) with a Google search API, we explore a variety of baseLMs, prompting methods, fine-tuning data, and QA tasks, and find languageagents are consistently improved after fine-tuning their backbone LMs. Forexample, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4leads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct,a novel approach to fine-tuning LMs with trajectories from multiple tasks andprompting methods, and show having more diverse fine-tuning data can furtherimprove agents. Along with other findings regarding scaling effects,robustness, generalization, efficiency and cost, our work establishescomprehensive benefits of fine-tuning LMs for agents, and provides an initialset of experimental designs, insights, as well as open questions towardlanguage agent fine-tuning."
Steering Large Language Models for Machine Translation with Finetuning  and In-Context Learning,"['Duarte M. Alves', 'Nuno M. Guerreiro', 'João Alves', 'José Pombal', 'Ricardo Rei', 'José G. C. de Souza', 'Pierre Colombo', 'André F. T. Martins']",http://arxiv.org/pdf/2310.13448v1.pdf,2023-10-20,['cs.cl'],"  Large language models (LLMs) are a promising avenue for machine translation(MT). However, current LLM-based MT systems are brittle: their effectivenesshighly depends on the choice of few-shot examples and they often require extrapost-processing due to overgeneration. Alternatives such as finetuning ontranslation instructions are computationally expensive and may weakenin-context learning capabilities, due to overspecialization. In this paper, weprovide a closer look at this problem. We start by showing that adapter-basedfinetuning with LoRA matches the performance of traditional finetuning whilereducing the number of training parameters by a factor of 50. This method alsooutperforms few-shot prompting and eliminates the need for post-processing orin-context examples. However, we show that finetuning generally degradesfew-shot performance, hindering adaptation capabilities. Finally, to obtain thebest of both worlds, we propose a simple approach that incorporates few-shotexamples during finetuning. Experiments on 10 language pairs show that ourproposed approach recovers the original few-shot capabilities while keeping theadded benefits of finetuning."
On Bilingual Lexicon Induction with Large Language Models,"['Yaoyiran Li', 'Anna Korhonen', 'Ivan Vulić']",http://arxiv.org/pdf/2310.13995v1.pdf,2023-10-21,"['cs.cl', 'cs.ai', 'cs.ir', 'cs.lg']","  Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP thatstill, to a large extent, relies on calculating cross-lingual wordrepresentations. Inspired by the global paradigm shift in NLP towards LargeLanguage Models (LLMs), we examine the potential of the latest generation ofLLMs for the development of bilingual lexicons. We ask the following researchquestion: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) forBLI, and how does this approach compare against and complement current BLIapproaches? To this end, we systematically study 1) zero-shot prompting forunsupervised BLI and 2) few-shot in-context prompting with a set of seedtranslation pairs, both without any LLM fine-tuning, as well as 3) standardBLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-sourcetext-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on twostandard BLI benchmarks covering a range of typologically diverse languages.Our work is the first to demonstrate strong BLI capabilities of text-to-textmLLMs. The results reveal that few-shot prompting with in-context examples fromnearest neighbours achieves the best performance, establishing newstate-of-the-art BLI scores for many language pairs. We also conduct a seriesof in-depth analyses and ablation studies, providing more insights on BLI with(m)LLMs, also along with their limitations."
An Early Evaluation of GPT-4V(ision),"['Yang Wu', 'Shilong Wang', 'Hao Yang', 'Tian Zheng', 'Hongbo Zhang', 'Yanyan Zhao', 'Bing Qin']",http://arxiv.org/pdf/2310.16534v1.pdf,2023-10-25,"['cs.cl', 'cs.cv']","  In this paper, we evaluate different abilities of GPT-4V including visualunderstanding, language understanding, visual puzzle solving, and understandingof other modalities such as depth, thermal, video, and audio. To estimateGPT-4V's performance, we manually construct 656 test instances and carefullyevaluate the results of GPT-4V. The highlights of our findings are as follows:(1) GPT-4V exhibits impressive performance on English visual-centric benchmarksbut fails to recognize simple Chinese texts in the images; (2) GPT-4V showsinconsistent refusal behavior when answering questions related to sensitivetraits such as gender, race, and age; (3) GPT-4V obtains worse results thanGPT-4 (API) on language understanding tasks including general languageunderstanding benchmarks and visual commonsense knowledge evaluationbenchmarks; (4) Few-shot prompting can improve GPT-4V's performance on bothvisual understanding and language understanding; (5) GPT-4V struggles to findthe nuances between two similar images and solve the easy math picture puzzles;(6) GPT-4V shows non-trivial performance on the tasks of similar modalities toimage, such as video and thermal. Our experimental results reveal the abilityand limitations of GPT-4V and we hope our paper can provide some insights intothe application and research of GPT-4V."
"""You Are An Expert Linguistic Annotator"": Limits of LLMs as Analyzers of  Abstract Meaning Representation","['Allyson Ettinger', 'Jena D. Hwang', 'Valentina Pyatkin', 'Chandra Bhagavatula', 'Yejin Choi']",http://arxiv.org/pdf/2310.17793v2.pdf,2023-10-26,"['cs.cl', 'cs.ai']","  Large language models (LLMs) show amazing proficiency and fluency in the useof language. Does this mean that they have also acquired insightful linguisticknowledge about the language, to an extent that they can serve as an ""expertlinguistic annotator""? In this paper, we examine the successes and limitationsof the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaningstructure, focusing on the Abstract Meaning Representation (AMR; Banarescu etal. 2013) parsing formalism, which provides rich graphical representations ofsentence meaning structure while abstracting away from surface forms. Wecompare models' analysis of this semantic structure across two settings: 1)direct production of AMR parses based on zero- and few-shot prompts, and 2)indirect partial reconstruction of AMR via metalinguistic natural languagequeries (e.g., ""Identify the primary event of this sentence, and the predicatecorresponding to that event.""). Across these settings, we find that models canreliably reproduce the basic format of AMR, and can often capture core event,argument, and modifier structure -- however, model outputs are prone tofrequent and major errors, and holistic analysis of parse acceptability showsthat even with few-shot demonstrations, models have virtually 0% success inproducing fully accurate parses. Eliciting natural language responses producessimilar patterns of errors. Overall, our findings indicate that these modelsout-of-the-box can capture aspects of semantic structure, but there remain keylimitations in their ability to support fully accurate semantic analyses orparses."
Style-Aware Radiology Report Generation with RadGraph and Few-Shot  Prompting,"['Benjamin Yan', 'Ruochen Liu', 'David E. Kuo', 'Subathra Adithan', 'Eduardo Pontes Reis', 'Stephen Kwak', 'Vasantha Kumar Venugopal', ""Chloe P. O'Connell"", 'Agustina Saenz', 'Pranav Rajpurkar', 'Michael Moor']",http://arxiv.org/pdf/2310.17811v2.pdf,2023-10-26,"['cs.ai', 'cs.cl']","  Automatically generated reports from medical images promise to improve theworkflow of radiologists. Existing methods consider an image-to-report modelingtask by directly generating a fully-fledged report from an image. However, thisconflates the content of the report (e.g., findings and their attributes) withits style (e.g., format and choice of words), which can lead to clinicallyinaccurate reports. To address this, we propose a two-step approach forradiology report generation. First, we extract the content from an image; then,we verbalize the extracted content into a report that matches the style of aspecific radiologist. For this, we leverage RadGraph -- a graph representationof reports -- together with large language models (LLMs). In our quantitativeevaluations, we find that our approach leads to beneficial performance. Ourhuman evaluation with clinical raters highlights that the AI-generated reportsare indistinguishably tailored to the style of individual radiologist despiteleveraging only a few examples as context."
Pachinko: Patching Interpretable QA Models through Natural Language  Feedback,"['Chaitanya Malaviya', 'Subin Lee', 'Dan Roth', 'Mark Yatskar']",http://arxiv.org/pdf/2311.09558v1.pdf,2023-11-16,['cs.cl'],"  Eliciting feedback from end users of NLP models can be beneficial forimproving models. However, how should we present model responses to users sothey are most amenable to be corrected from user feedback? Further, whatproperties do users value to understand and trust responses? We answer thesequestions by analyzing the effect of rationales generated by QA models tosupport their answers. We specifically consider decomposed question-answeringmodels that first extract an intermediate rationale based on a context and aquestion and then use solely this rationale to answer the question. A rationaleoutlines the approach followed by the model to answer the question. Our workconsiders various formats of these rationales that vary according towell-defined properties of interest. We sample these rationales from largelanguage models using few-shot prompting for two reading comprehensiondatasets, and then perform two user studies. In the first one, we present userswith incorrect answers and corresponding rationales of various formats and askthem to provide natural language feedback to revise the rationale. We thenmeasure the effectiveness of this feedback in patching these rationales throughin-context learning. The second study evaluates how well different rationaleformats enable users to understand and trust model answers, when they arecorrect. We find that rationale formats significantly affect how easy it is (1)for users to give feedback for rationales, and (2) for models to subsequentlyexecute this feedback. In addition to influencing critiquablity, certainformats significantly enhance user reported understanding and trust of modeloutputs."
Deploying and Evaluating LLMs to Program Service Mobile Robots,"['Zichao Hu', 'Francesca Lucchetti', 'Claire Schlesinger', 'Yash Saxena', 'Anders Freeman', 'Sadanand Modak', 'Arjun Guha', 'Joydeep Biswas']",http://arxiv.org/pdf/2311.11183v2.pdf,2023-11-18,['cs.ro'],"  Recent advancements in large language models (LLMs) have spurred interest inusing them for generating robot programs from natural language, with promisinginitial results. We investigate the use of LLMs to generate programs forservice mobile robots leveraging mobility, perception, and human interactionskills, and where accurate sequencing and ordering of actions is crucial forsuccess. We contribute CodeBotler, an open-source robot-agnostic tool toprogram service mobile robots from natural language, and RoboEval, a benchmarkfor evaluating LLMs' capabilities of generating programs to complete servicerobot tasks. CodeBotler performs program generation via few-shot prompting ofLLMs with an embedded domain-specific language (eDSL) in Python, and leveragesskill abstractions to deploy generated programs on any general-purpose mobilerobot. RoboEval evaluates the correctness of generated programs by checkingexecution traces starting with multiple initial states, and checking whetherthe traces satisfy temporal logic properties that encode correctness for eachtask. RoboEval also includes multiple prompts per task to test for therobustness of program generation. We evaluate several popular state-of-the-artLLMs with the RoboEval benchmark, and perform a thorough analysis of the modesof failures, resulting in a taxonomy that highlights common pitfalls of LLMs atgenerating robot programs. We release our code and benchmark athttps://amrl.cs.utexas.edu/codebotler/."
HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot  Prompt Learning,"['Xingtong Yu', 'Yuan Fang', 'Zemin Liu', 'Xinming Zhang']",http://arxiv.org/pdf/2312.01878v4.pdf,2023-12-04,['cs.lg'],"  Graph neural networks (GNNs) and heterogeneous graph neural networks (HGNNs)are prominent techniques for homogeneous and heterogeneous graph representationlearning, yet their performance in an end-to-end supervised framework greatlydepends on the availability of task-specific supervision. To reduce thelabeling cost, pre-training on self-supervised pretext tasks has become apopular paradigm,but there is often a gap between the pre-trained model anddownstream tasks, stemming from the divergence in their objectives. To bridgethe gap, prompt learning has risen as a promising direction especially infew-shot settings, without the need to fully fine-tune the pre-trained model.While there has been some early exploration of prompt-based learning on graphs,they primarily deal with homogeneous graphs, ignoring the heterogeneous graphsthat are prevalent in downstream applications. In this paper, we proposeHGPROMPT, a novel pre-training and prompting framework to unify not onlypre-training and downstream tasks but also homogeneous and heterogeneous graphsvia a dual-template design. Moreover, we propose dual-prompt in HGPROMPT toassist a downstream task in locating the most relevant prior to bridge the gapscaused by not only feature variations but also heterogeneity differences acrosstasks. Finally, we thoroughly evaluate and analyze HGPROMPT through extensiveexperiments on three public datasets."
Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations,"['Hakan Inan', 'Kartikeya Upasani', 'Jianfeng Chi', 'Rashi Rungta', 'Krithika Iyer', 'Yuning Mao', 'Michael Tontchev', 'Qing Hu', 'Brian Fuller', 'Davide Testuggine', 'Madian Khabsa']",http://arxiv.org/pdf/2312.06674v1.pdf,2023-12-07,"['cs.cl', 'cs.ai']","  We introduce Llama Guard, an LLM-based input-output safeguard model gearedtowards Human-AI conversation use cases. Our model incorporates a safety risktaxonomy, a valuable tool for categorizing a specific set of safety risks foundin LLM prompts (i.e., prompt classification). This taxonomy is alsoinstrumental in classifying the responses generated by LLMs to these prompts, aprocess we refer to as response classification. For the purpose of both promptand response classification, we have meticulously gathered a dataset of highquality. Llama Guard, a Llama2-7b model that is instruction-tuned on ourcollected dataset, albeit low in volume, demonstrates strong performance onexisting benchmarks such as the OpenAI Moderation Evaluation dataset andToxicChat, where its performance matches or exceeds that of currently availablecontent moderation tools. Llama Guard functions as a language model, carryingout multi-class classification and generating binary decision scores.Furthermore, the instruction fine-tuning of Llama Guard allows for thecustomization of tasks and the adaptation of output formats. This featureenhances the model's capabilities, such as enabling the adjustment of taxonomycategories to align with specific use cases, and facilitating zero-shot orfew-shot prompting with diverse taxonomies at the input. We are making LlamaGuard model weights available and we encourage researchers to further developand adapt them to meet the evolving needs of the community for AI safety."
Multi-lingual Evaluation of Code Generation Models,"['Ben Athiwaratkun', 'Sanjay Krishna Gouda', 'Zijian Wang', 'Xiaopeng Li', 'Yuchen Tian', 'Ming Tan', 'Wasi Uddin Ahmad', 'Shiqi Wang', 'Qing Sun', 'Mingyue Shang', 'Sujan Kumar Gonugondla', 'Hantian Ding', 'Varun Kumar', 'Nathan Fulton', 'Arash Farahani', 'Siddhartha Jain', 'Robert Giaquinto', 'Haifeng Qian', 'Murali Krishna Ramanathan', 'Ramesh Nallapati', 'Baishakhi Ray', 'Parminder Bhatia', 'Sudipta Sengupta', 'Dan Roth', 'Bing Xiang']",http://arxiv.org/pdf/2210.14868v3.pdf,2022-10-26,"['cs.lg', 'cs.cl']","  We present new benchmarks on evaluation code generation models: MBXP andMultilingual HumanEval, and MathQA-X. These datasets cover over 10 programminglanguages and are generated using a scalable conversion framework thattranspiles prompts and test cases from the original Python datasets into thecorresponding data in the target language. Using these benchmarks, we are ableto assess the performance of code generation models in a multi-lingual fashion,and discovered generalization ability of language models on out-of-domainlanguages, advantages of multi-lingual models over mono-lingual, the ability offew-shot prompting to teach the model new languages, and zero-shot translationabilities even on mono-lingual settings. Furthermore, we use our codegeneration model to perform large-scale bootstrapping to obtain syntheticcanonical solutions in several languages, which can be used for othercode-related evaluations such as code insertion, robustness, or summarizationtasks. Overall, our benchmarks represents a significant step towards a deeperunderstanding of language models' code generation abilities. We publiclyrelease our code and datasets at https://github.com/amazon-research/mxeval."
PAL: Program-aided Language Models,"['Luyu Gao', 'Aman Madaan', 'Shuyan Zhou', 'Uri Alon', 'Pengfei Liu', 'Yiming Yang', 'Jamie Callan', 'Graham Neubig']",http://arxiv.org/pdf/2211.10435v2.pdf,2022-11-18,"['cs.cl', 'cs.ai']","  Large language models (LLMs) have recently demonstrated an impressive abilityto perform arithmetic and symbolic reasoning tasks, when provided with a fewexamples at test time (""few-shot prompting""). Much of this success can beattributed to prompting methods such as ""chain-of-thought'', which employ LLMsfor both understanding the problem description by decomposing it into steps, aswell as solving each step of the problem. While LLMs seem to be adept at thissort of step-by-step decomposition, LLMs often make logical and arithmeticmistakes in the solution part, even when the problem is decomposed correctly.In this paper, we present Program-Aided Language models (PAL): a novel approachthat uses the LLM to read natural language problems and generate programs asthe intermediate reasoning steps, but offloads the solution step to a runtimesuch as a Python interpreter. With PAL, decomposing the natural languageproblem into runnable steps remains the only learning task for the LLM, whilesolving is delegated to the interpreter. We demonstrate this synergy between aneural LLM and a symbolic interpreter across 13 mathematical, symbolic, andalgorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In allthese natural language reasoning tasks, generating code using an LLM andreasoning using a Python interpreter leads to more accurate results than muchlarger models. For example, PAL using Codex achieves state-of-the-art few-shotaccuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540Bwhich uses chain-of-thought by absolute 15% top-1. Our code and data arepublicly available at http://reasonwithpal.com/ ."
Learning Performance-Improving Code Edits,"['Alexander Shypula', 'Aman Madaan', 'Yimeng Zeng', 'Uri Alon', 'Jacob Gardner', 'Milad Hashemi', 'Graham Neubig', 'Parthasarathy Ranganathan', 'Osbert Bastani', 'Amir Yazdanbakhsh']",http://arxiv.org/pdf/2302.07867v4.pdf,2023-02-15,"['cs.se', 'cs.ai', 'cs.lg', 'cs.pf']","  With the waning of Moore's law, optimizing program performance has become amajor focus of software research. However, high-level optimizations such as APIand algorithm changes remain elusive due to the difficulty of understanding thesemantics of code. Simultaneously, pretrained large language models (LLMs) havedemonstrated strong capabilities at solving a wide range of programming tasks.To that end, we introduce a framework for adapting LLMs to high-level programoptimization. First, we curate a dataset of performance-improving edits made byhuman programmers of over 77K competitive C++ programming submission pairs,accompanied by extensive unit tests. A major challenge is the significantvariability of measuring performance on commodity hardware, which can lead tospurious ""improvements"". To isolate and reliably evaluate the impact of programoptimizations, we design an environment based on the gem5 full systemsimulator, the de facto simulator used in academia and industry. Next, wepropose a broad range of adaptation strategies for code optimization; forprompting, these include retrieval-based few-shot prompting andchain-of-thought, and for finetuning, these include performance-conditionedgeneration and synthetic data augmentation based on self-play. A combination ofthese techniques achieves an average speedup of 5.65X on CodeLlama-13B and6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find ourproposed performance-conditioned generation is particularly effective atimproving performance as well as increasing the fraction of optimized programs."
Large Language Models for User Interest Journeys,"['Konstantina Christakopoulou', 'Alberto Lalama', 'Cj Adams', 'Iris Qu', 'Yifat Amir', 'Samer Chucri', 'Pierce Vollucci', 'Fabio Soldo', 'Dina Bseiso', 'Sarah Scodel', 'Lucas Dixon', 'Ed H. Chi', 'Minmin Chen']",http://arxiv.org/pdf/2305.15498v1.pdf,2023-05-24,"['cs.cl', 'cs.ai', 'cs.ir']","  Large language models (LLMs) have shown impressive capabilities in naturallanguage understanding and generation. Their potential for deeper userunderstanding and improved personalized user experience on recommendationplatforms is, however, largely untapped. This paper aims to address this gap.Recommender systems today capture users' interests through encoding theirhistorical activities on the platforms. The generated user representations arehard to examine or interpret. On the other hand, if we were to ask people aboutinterests they pursue in their life, they might talk about their hobbies, likeI just started learning the ukulele, or their relaxation routines, e.g., I liketo watch Saturday Night Live, or I want to plant a vertical garden. We argue,and demonstrate through extensive experiments, that LLMs as foundation modelscan reason through user activities, and describe their interests in nuanced andinteresting ways, similar to how a human would.  We define interest journeys as the persistent and overarching user interests,in other words, the non-transient ones. These are the interests that we believewill benefit most from the nuanced and personalized descriptions. We introducea framework in which we first perform personalized extraction of interestjourneys, and then summarize the extracted journeys via LLMs, using techniqueslike few-shot prompting, prompt-tuning and fine-tuning. Together, our resultsin prompting LLMs to name extracted user journeys in a large-scale industrialplatform demonstrate great potential of these models in providing deeper, moreinterpretable, and controllable user understanding. We believe LLM powered userunderstanding can be a stepping stone to entirely new user experiences onrecommendation platforms that are journey-aware, assistive, and enablingfrictionless conversation down the line."
Passive learning of active causal strategies in agents and language  models,"['Andrew Kyle Lampinen', 'Stephanie C Y Chan', 'Ishita Dasgupta', 'Andrew J Nam', 'Jane X Wang']",http://arxiv.org/pdf/2305.16183v2.pdf,2023-05-25,"['cs.lg', 'cs.ai', 'cs.cl']","  What can be learned about causality and experimentation from passive data?This question is salient given recent successes of passively-trained languagemodels in interactive domains such as tool use. Passive learning is inherentlylimited. However, we show that purely passive learning can in fact allow anagent to learn generalizable strategies for determining and using causalstructures, as long as the agent can intervene at test time. We formallyillustrate that learning a strategy of first experimenting, then seeking goals,can allow generalization from passive learning in principle. We then showempirically that agents trained via imitation on expert data can indeedgeneralize at test time to infer and use causal links which are never presentin the training data; these agents can also generalize experimentationstrategies to novel variable sets never observed in training. We then show thatstrategies for causal intervention and exploitation can be generalized frompassive data even in a more complex environment with high-dimensionalobservations, with the support of natural language explanations. Explanationscan even allow passive learners to generalize out-of-distribution fromperfectly-confounded training data. Finally, we show that language models,trained only on passive next-word prediction, can generalize causalintervention strategies from a few-shot prompt containing examples ofexperimentation, together with explanations and reasoning. These resultshighlight the surprising power of passive learning of active causal strategies,and may help to understand the behaviors and capabilities of language models."
Tool Documentation Enables Zero-Shot Tool-Usage with Large Language  Models,"['Cheng-Yu Hsieh', 'Si-An Chen', 'Chun-Liang Li', 'Yasuhisa Fujii', 'Alexander Ratner', 'Chen-Yu Lee', 'Ranjay Krishna', 'Tomas Pfister']",http://arxiv.org/pdf/2308.00675v1.pdf,2023-08-01,"['cs.cl', 'cs.ai', 'cs.cv', 'cs.lg']","  Today, large language models (LLMs) are taught to use new tools by providinga few demonstrations of the tool's usage. Unfortunately, demonstrations arehard to acquire, and can result in undesirable biased usage if the wrongdemonstration is chosen. Even in the rare scenario that demonstrations arereadily available, there is no principled selection protocol to determine howmany and which ones to provide. As tasks grow more complex, the selectionsearch grows combinatorially and invariably becomes intractable. Our workprovides an alternative to demonstrations: tool documentation. We advocate theuse of tool documentation, descriptions for the individual tool usage, overdemonstrations. We substantiate our claim through three main empirical findingson 6 tasks across both vision and language modalities. First, on existingbenchmarks, zero-shot prompts with only tool documentation are sufficient foreliciting proper tool usage, achieving performance on par with few-shotprompts. Second, on a newly collected realistic tool-use dataset with hundredsof available tool APIs, we show that tool documentation is significantly morevaluable than demonstrations, with zero-shot documentation significantlyoutperforming few-shot without documentation. Third, we highlight the benefitsof tool documentations by tackling image generation and video tracking usingjust-released unseen state-of-the-art models as tools. Finally, we highlightthe possibility of using tool documentation to automatically enable newapplications: by using nothing more than the documentation of GroundingDino,Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of thejust-released Grounded-SAM and Track Anything models."
MathAttack: Attacking Large Language Models Towards Math Solving Ability,"['Zihao Zhou', 'Qiufeng Wang', 'Mingyu Jin', 'Jie Yao', 'Jianan Ye', 'Wei Liu', 'Wei Wang', 'Xiaowei Huang', 'Kaizhu Huang']",http://arxiv.org/pdf/2309.01686v1.pdf,2023-09-04,['cs.cl'],"  With the boom of Large Language Models (LLMs), the research of solving MathWord Problem (MWP) has recently made great progress. However, there are fewstudies to examine the security of LLMs in math solving ability. Instead ofattacking prompts in the use of LLMs, we propose a MathAttack model to attackMWP samples which are closer to the essence of security in solving mathproblems. Compared to traditional text adversarial attack, it is essential topreserve the mathematical logic of original MWPs during the attacking. To thisend, we propose logical entity recognition to identify logical entries whichare then frozen. Subsequently, the remaining text are attacked by adopting aword-level attacker. Furthermore, we propose a new dataset RobustMath toevaluate the robustness of LLMs in math solving ability. Extensive experimentson our RobustMath and two another math benchmark datasets GSM8K and MultiAirthshow that MathAttack could effectively attack the math solving ability of LLMs.In the experiments, we observe that (1) Our adversarial samples fromhigher-accuracy LLMs are also effective for attacking LLMs with lower accuracy(e.g., transfer from larger to smaller-size LLMs, or from few-shot to zero-shotprompts); (2) Complex MWPs (such as more solving steps, longer text, morenumbers) are more vulnerable to attack; (3) We can improve the robustness ofLLMs by using our adversarial samples in few-shot prompts. Finally, we hope ourpractice and observation can serve as an important attempt towards enhancingthe robustness of LLMs in math solving ability. We will release our code anddataset."
MentaLLaMA: Interpretable Mental Health Analysis on Social Media with  Large Language Models,"['Kailai Yang', 'Tianlin Zhang', 'Ziyan Kuang', 'Qianqian Xie', 'Sophia Ananiadou', 'Jimin Huang']",http://arxiv.org/pdf/2309.13567v2.pdf,2023-09-24,['cs.cl'],"  With the development of web technology, social media texts are becoming arich source for automatic mental health analysis. As traditional discriminativemethods bear the problem of low interpretability, the recent large languagemodels have been explored for interpretable mental health analysis on socialmedia, which aims to provide detailed explanations along with predictions. Theresults show that ChatGPT can generate approaching-human explanations for itscorrect classifications. However, LLMs still achieve unsatisfactoryclassification performance in a zero-shot/few-shot manner. Domain-specificfinetuning is an effective solution, but faces 2 challenges: 1) lack ofhigh-quality training data. 2) no open-source LLMs for interpretable mentalhealth analysis were released to lower the finetuning cost. To alleviate theseproblems, we build the first multi-task and multi-source interpretable mentalhealth instruction (IMHI) dataset on social media, with 105K data samples. Theraw social media data are collected from 10 existing sources covering 8 mentalhealth analysis tasks. We use expert-written few-shot prompts and collectedlabels to prompt ChatGPT and obtain explanations from its responses. To ensurethe reliability of the explanations, we perform strict automatic and humanevaluations on the correctness, consistency, and quality of generated data.Based on the IMHI dataset and LLaMA2 foundation models, we train MentalLLaMA,the first open-source LLM series for interpretable mental health analysis withinstruction-following capability. We also evaluate the performance ofMentalLLaMA on the IMHI evaluation benchmark with 10 test sets, where theircorrectness for making predictions and the quality of explanations areexamined. The results show that MentalLLaMA approaches state-of-the-artdiscriminative methods in correctness and generates high-quality explanations."
FreshLLMs: Refreshing Large Language Models with Search Engine  Augmentation,"['Tu Vu', 'Mohit Iyyer', 'Xuezhi Wang', 'Noah Constant', 'Jerry Wei', 'Jason Wei', 'Chris Tar', 'Yun-Hsuan Sung', 'Denny Zhou', 'Quoc Le', 'Thang Luong']",http://arxiv.org/pdf/2310.03214v2.pdf,2023-10-05,['cs.cl'],"  Most large language models (LLMs) are trained once and never updated; thus,they lack the ability to dynamically adapt to our ever-changing world. In thiswork, we perform a detailed study of the factuality of LLM-generated text inthe context of answering questions that test current world knowledge.Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing adiverse range of question and answer types, including questions that requirefast-changing world knowledge as well as questions with false premises thatneed to be debunked. We benchmark a diverse array of both closed andopen-source LLMs under a two-mode evaluation procedure that allows us tomeasure both correctness and hallucination. Through human evaluations involvingmore than 50K judgments, we shed light on limitations of these models anddemonstrate significant room for improvement: for instance, all models(regardless of model size) struggle on questions that involve fast-changingknowledge and false premises. Motivated by these results, we presentFreshPrompt, a simple few-shot prompting method that substantially boosts theperformance of an LLM on FreshQA by incorporating relevant and up-to-dateinformation retrieved from a search engine into the prompt. Our experimentsshow that FreshPrompt outperforms both competing search engine-augmentedprompting methods such as Self-Ask (Press et al., 2022) as well as commercialsystems such as Perplexity.AI. Further analysis of FreshPrompt reveals thatboth the number of retrieved evidences and their order play a key role ininfluencing the correctness of LLM-generated answers. Additionally, instructingthe LLM to generate concise and direct answers helps reduce hallucinationcompared to encouraging more verbose answers. To facilitate future work, werelease FreshQA at github.com/freshllms/freshqa and commit to updating it atregular intervals."
A Comprehensive Survey on Pretrained Foundation Models: A History from  BERT to ChatGPT,"['Ce Zhou', 'Qian Li', 'Chen Li', 'Jun Yu', 'Yixin Liu', 'Guangjing Wang', 'Kai Zhang', 'Cheng Ji', 'Qiben Yan', 'Lifang He', 'Hao Peng', 'Jianxin Li', 'Jia Wu', 'Ziwei Liu', 'Pengtao Xie', 'Caiming Xiong', 'Jian Pei', 'Philip S. Yu', 'Lichao Sun']",http://arxiv.org/pdf/2302.09419v3.pdf,2023-02-18,"['cs.ai', 'cs.cl', 'cs.lg']","  Pretrained Foundation Models (PFMs) are regarded as the foundation forvarious downstream tasks with different data modalities. A PFM (e.g., BERT,ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonableparameter initialization for a wide range of downstream applications. BERTlearns bidirectional encoder representations from Transformers, which aretrained on large datasets as contextual language models. Similarly, thegenerative pretrained transformer (GPT) method employs Transformers as thefeature extractor and is trained using an autoregressive paradigm on largedatasets. Recently, ChatGPT shows promising success on large language models,which applies an autoregressive language model with zero shot or few shotprompting. The remarkable achievements of PFM have brought significantbreakthroughs to various fields of AI. Numerous studies have proposed differentmethods, raising the demand for an updated survey. This study provides acomprehensive review of recent research advancements, challenges, andopportunities for PFMs in text, image, graph, as well as other data modalities.The review covers the basic components and existing pretraining methods used innatural language processing, computer vision, and graph learning. Additionally,it explores advanced PFMs used for different data modalities and unified PFMsthat consider data quality and quantity. The review also discusses researchrelated to the fundamentals of PFMs, such as model efficiency and compression,security, and privacy. Finally, the study provides key implications, futureresearch directions, challenges, and open problems in the field of PFMs.Overall, this survey aims to shed light on the research of the PFMs onscalability, security, logical reasoning ability, cross-domain learningability, and the user-friendly interactive ability for artificial generalintelligence."
Short Answer Grading Using One-shot Prompting and Text Similarity  Scoring Model,['Su-Youn Yoon'],http://arxiv.org/pdf/2305.18638v1.pdf,2023-05-29,"['cs.cl', 'i.2.7']","  In this study, we developed an automated short answer grading (ASAG) modelthat provided both analytic scores and final holistic scores. Short answeritems typically consist of multiple sub-questions, and providing an analyticscore and the text span relevant to each sub-question can increase theinterpretability of the automated scores. Furthermore, they can be used togenerate actionable feedback for students. Despite these advantages, moststudies have focused on predicting only holistic scores due to the difficultyin constructing dataset with manual annotations. To address this difficulty, weused large language model (LLM)-based one-shot prompting and a text similarityscoring model with domain adaptation using small manually annotated dataset.The accuracy and quadratic weighted kappa of our model were 0.67 and 0.71 on asubset of the publicly available ASAG dataset. The model achieved a substantialimprovement over the majority baseline."
"Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks","['Melanie Mitchell', 'Alessandro B. Palmarini', 'Arseny Moskvichev']",http://arxiv.org/pdf/2311.09247v3.pdf,2023-11-14,"['cs.ai', 'cs.lg']","  We explore the abstract reasoning abilities of text-only and multimodalversions of GPT-4, using the ConceptARC benchmark [10], which is designed toevaluate robust understanding and reasoning with core-knowledge concepts. Weextend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed,one-shot prompting (rather than simple, zero-shot prompts) with text versionsof ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4,on zero- and one-shot prompts using image versions of the simplest tasks. Ourexperimental results support the conclusion that neither version of GPT-4 hasdeveloped robust abstraction abilities at humanlike levels."
DePlot: One-shot visual language reasoning by plot-to-table translation,"['Fangyu Liu', 'Julian Martin Eisenschlos', 'Francesco Piccinno', 'Syrine Krichene', 'Chenxi Pang', 'Kenton Lee', 'Mandar Joshi', 'Wenhu Chen', 'Nigel Collier', 'Yasemin Altun']",http://arxiv.org/pdf/2212.10505v2.pdf,2022-12-20,"['cs.cl', 'cs.ai', 'cs.cv']","  Visual language such as charts and plots is ubiquitous in the human world.Comprehending plots and charts requires strong reasoning skills. Priorstate-of-the-art (SOTA) models require at least tens of thousands of trainingexamples and their reasoning capabilities are still much limited, especially oncomplex human-written queries. This paper presents the first one-shot solutionto visual language reasoning. We decompose the challenge of visual languagereasoning into two steps: (1) plot-to-text translation, and (2) reasoning overthe translated text. The key in this method is a modality conversion module,named as DePlot, which translates the image of a plot or chart to a linearizedtable. The output of DePlot can then be directly used to prompt a pretrainedlarge language model (LLM), exploiting the few-shot reasoning capabilities ofLLMs. To obtain DePlot, we standardize the plot-to-table task by establishingunified task formats and metrics, and train DePlot end-to-end on this task.DePlot can then be used off-the-shelf together with LLMs in a plug-and-playfashion. Compared with a SOTA model finetuned on more than >28k data points,DePlot+LLM with just one-shot prompting achieves a 24.0% improvement overfinetuned SOTA on human-written queries from the task of chart QA."
GEE! Grammar Error Explanation with Large Language Models,"['Yixiao Song', 'Kalpesh Krishna', 'Rajesh Bhatt', 'Kevin Gimpel', 'Mohit Iyyer']",http://arxiv.org/pdf/2311.09517v1.pdf,2023-11-16,['cs.cl'],"  Grammatical error correction tools are effective at correcting grammaticalerrors in users' input sentences but do not provide users with \textit{naturallanguage} explanations about their errors. Such explanations are essential forhelping users learn the language by gaining a deeper understanding of itsgrammatical rules (DeKeyser, 2003; Ellis et al., 2006). To address this gap, wepropose the task of grammar error explanation, where a system needs to provideone-sentence explanations for each grammatical error in a pair of erroneous andcorrected sentences. We analyze the capability of GPT-4 in grammar errorexplanation, and find that it only produces explanations for 60.2% of theerrors using one-shot prompting. To improve upon this performance, we develop atwo-step pipeline that leverages fine-tuned and prompted large language modelsto perform structured atomic token edit extraction, followed by prompting GPT-4to generate explanations. We evaluate our pipeline on German and Chinesegrammar error correction data sampled from language learners with a wide rangeof proficiency levels. Human evaluation reveals that our pipeline produces93.9% and 98.0% correct explanations for German and Chinese data, respectively.To encourage further research in this area, we will open-source our data andcode."
CHAI-DT: A Framework for Prompting Conversational Generative AI Agents  to Actively Participate in Co-Creation,['Brandon Harwood'],http://arxiv.org/pdf/2305.03852v1.pdf,2023-05-05,"['cs.hc', 'cs.ai']","  This paper explores the potential for utilizing generative AI models ingroup-focused co-creative frameworks to enhance problem solving and ideation inbusiness innovation and co-creation contexts, and proposes a novel promptingtechnique for conversational generative AI agents which employ methods inspiredby traditional 'human-to-human' facilitation and instruction to enable activecontribution to Design Thinking, a co-creative framework. Through experimentsusing this prompting technique, we gather evidence that conversationalgenerative transformers (i.e. ChatGPT) have the capability to contributecontext-specific, useful, and creative input into Design Thinking activities.We also discuss the potential benefits, limitations, and risks associated withusing generative AI models in co-creative ideation and provide recommendationsfor future research."
Prompting in Autoregressive Large Language Models,['Prabin Bhandari'],http://arxiv.org/pdf/2312.03740v1.pdf,2023-11-28,['cs.cl'],"  Autoregressive Large Language Models have transformed the landscape ofNatural Language Processing. Pre-train and prompt paradigm has replaced theconventional approach of pre-training and fine-tuning for many downstream NLPtasks. This shift has been possible largely due to LLMs and innovativeprompting techniques. LLMs have shown great promise for a variety of downstreamtasks owing to their vast parameters and huge datasets that they arepre-trained on. However, in order to fully realize their potential, theiroutputs must be guided towards the desired outcomes. Prompting, in which aspecific input or instruction is provided to guide the LLMs toward the intendedoutput, has become a tool for achieving this goal. In this paper, we discussthe various prompting techniques that have been applied to fully harness thepower of LLMs. We present a taxonomy of existing literature on promptingtechniques and provide a concise survey based on this taxonomy. Further, weidentify some open problems in the realm of prompting in autoregressive LLMswhich could serve as a direction for future research."
AceCoder: Utilizing Existing Code to Enhance Code Generation,"['Jia Li', 'Yunfei Zhao', 'Yongmin Li', 'Ge Li', 'Zhi Jin']",http://arxiv.org/pdf/2303.17780v3.pdf,2023-03-31,"['cs.se', 'cs.ai']","  Large Language Models (LLMs) have shown great success in code generation.LLMs take as the input a prompt and output the code. A key question is how tomake prompts (i.e., Prompting Techniques). Existing prompting techniques aredesigned for natural language generation and have low accuracy in codegeneration.  In this paper, we propose a new prompting technique named AceCoder. Ourmotivation is that code generation meets two unique challenges (i.e.,requirement understanding and code implementation). AceCoder contains two novelmechanisms (i.e., guided code generation and example retrieval) to solve thesechallenges. (1) Guided code generation asks LLMs first to analyze requirementsand output an intermediate preliminary (e.g., test cases). The preliminary isused to clarify requirements and tell LLMs ""what to write"". (2) Exampleretrieval selects similar programs as examples in prompts, which provide lotsof relevant content (e.g., algorithms, APIs) and teach LLMs ""how to write"". Weapply AceCoder to three LLMs (e.g., Codex) and evaluate it on three publicbenchmarks using the Pass@k. Results show that AceCoder can significantlyimprove the performance of LLMs on code generation. (1) In terms of Pass@1,AceCoder outperforms the state-of-the-art baseline by up to 56.4% in MBPP,70.7% in MBJP, and 88.4% in MBJSP. (2) AceCoder is effective in LLMs withdifferent sizes (i.e., 6B to 13B) and different languages (i.e., Python, Java,and JavaScript). (3) Human evaluation shows human developers prefer programsfrom AceCoder."
Compositional Semantic Parsing with Large Language Models,"['Andrew Drozdov', 'Nathanael Schärli', 'Ekin Akyürek', 'Nathan Scales', 'Xinying Song', 'Xinyun Chen', 'Olivier Bousquet', 'Denny Zhou']",http://arxiv.org/pdf/2209.15003v2.pdf,2022-09-29,"['cs.cl', 'cs.ai']","  Humans can reason compositionally when presented with new tasks. Previousresearch shows that appropriate prompting techniques enable large languagemodels (LLMs) to solve artificial compositional generalization tasks such asSCAN. In this work, we identify additional challenges in more realisticsemantic parsing tasks with larger vocabulary and refine these promptingtechniques to address them. Our best method is based on least-to-mostprompting: it decomposes the problem using prompting-based syntactic parsing,then uses this decomposition to select appropriate exemplars and tosequentially generate the semantic parse. This method allows us to set a newstate of the art for CFQ while requiring only 1% of the training data used bytraditional approaches. Due to the general nature of our approach, we expectsimilar efforts will lead to new results in other tasks and domains, especiallyfor knowledge-intensive applications."
Thread of Thought Unraveling Chaotic Contexts,"['Yucheng Zhou', 'Xiubo Geng', 'Tao Shen', 'Chongyang Tao', 'Guodong Long', 'Jian-Guang Lou', 'Jianbing Shen']",http://arxiv.org/pdf/2311.08734v1.pdf,2023-11-15,['cs.cl'],"  Large Language Models (LLMs) have ushered in a transformative era in thefield of natural language processing, excelling in tasks related to textcomprehension and generation. Nevertheless, they encounter difficulties whenconfronted with chaotic contexts (e.g., distractors rather than long irrelevantcontext), leading to the inadvertent omission of certain details within thechaotic context. In response to these challenges, we introduce the ""Thread ofThought"" (ThoT) strategy, which draws inspiration from human cognitiveprocesses. ThoT systematically segments and analyzes extended contexts whileadeptly selecting pertinent information. This strategy serves as a versatile""plug-and-play"" module, seamlessly integrating with various LLMs and promptingtechniques. In the experiments, we utilize the PopQA and EntityQ datasets, aswell as a Multi-Turn Conversation Response dataset (MTCR) we collected, toillustrate that ThoT significantly improves reasoning performance compared toother prompting techniques."
Prompting LLMs with content plans to enhance the summarization of  scientific articles,"['Aldan Creo', 'Manuel Lama', 'Juan C. Vidal']",http://arxiv.org/pdf/2312.08282v2.pdf,2023-12-13,"['cs.cl', 'cs.ai']","  This paper presents novel prompting techniques to improve the performance ofautomatic summarization systems for scientific articles. Scientific articlesummarization is highly challenging due to the length and complexity of thesedocuments. We conceive, implement, and evaluate prompting techniques thatprovide additional contextual information to guide summarization systems.Specifically, we feed summarizers with lists of key terms extracted fromarticles, such as author keywords or automatically generated keywords. Ourtechniques are tested with various summarization models and input texts.Results show performance gains, especially for smaller models summarizingsections separately. This evidences that prompting is a promising approach toovercoming the limitations of less powerful systems. Our findings introduce anew research direction of using prompts to aid smaller models."
EvEntS ReaLM: Event Reasoning of Entity States via Language Models,"['Evangelia Spiliopoulou', 'Artidoro Pagnoni', 'Yonatan Bisk', 'Eduard Hovy']",http://arxiv.org/pdf/2211.05392v1.pdf,2022-11-10,['cs.cl'],"  This paper investigates models of event implications. Specifically, how wellmodels predict entity state-changes, by targeting their understanding ofphysical attributes. Nominally, Large Language models (LLM) have been exposedto procedural knowledge about how objects interact, yet our benchmarking showsthey fail to reason about the world. Conversely, we also demonstrate thatexisting approaches often misrepresent the surprising abilities of LLMs viaimproper task encodings and that proper model prompting can dramaticallyimprove performance of reported baseline results across multiple tasks. Inparticular, our results indicate that our prompting technique is especiallyuseful for unseen attributes (out-of-domain) or when only limited data isavailable."
GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4,"['Tom Kocmi', 'Christian Federmann']",http://arxiv.org/pdf/2310.13988v1.pdf,2023-10-21,['cs.cl'],"  This paper introduces GEMBA-MQM, a GPT-based evaluation metric designed todetect translation quality errors, specifically for the quality estimationsetting without the need for human reference translations. Based on the powerof large language models (LLM), GEMBA-MQM employs a fixed three-shot promptingtechnique, querying the GPT-4 model to mark error quality spans. Compared toprevious works, our method has language-agnostic prompts, thus avoiding theneed for manual prompt preparation for new languages.  While preliminary results indicate that GEMBA-MQM achieves state-of-the-artaccuracy for system ranking, we advise caution when using it in academic worksto demonstrate improvements over other methods due to its dependence on theproprietary, black-box GPT model."
Utilizing Language Models for Energy Load Forecasting,"['Hao Xue', 'Flora D. Salim']",http://arxiv.org/pdf/2310.17788v1.pdf,2023-10-26,"['cs.ai', 'cs.cl']","  Energy load forecasting plays a crucial role in optimizing resourceallocation and managing energy consumption in buildings and cities. In thispaper, we propose a novel approach that leverages language models for energyload forecasting. We employ prompting techniques to convert energy consumptiondata into descriptive sentences, enabling fine-tuning of language models. Byadopting an autoregressive generating approach, our proposed method enablespredictions of various horizons of future energy load consumption. Throughextensive experiments on real-world datasets, we demonstrate the effectivenessand accuracy of our proposed method. Our results indicate that utilizinglanguage models for energy load forecasting holds promise for enhancing energyefficiency and facilitating intelligent decision-making in energy systems."
Eliciting Topic Hierarchies from Large Language Models,"['Grace Li', 'Tao Long', 'Lydia B. Chilton']",http://arxiv.org/pdf/2310.19275v1.pdf,2023-10-30,['cs.hc'],"  Finding topics to write about can be a mentally demanding process. However,topic hierarchies can help writers explore topics of varying levels ofspecificity. In this paper, we use large language models (LLMs) to helpconstruct topic hierarchies. Although LLMs have access to such knowledge, itcan be difficult to elicit due to issues of specificity, scope, and repetition.We designed and tested three different prompting techniques to find one thatmaximized accuracy. We found that prepending the general topic area to a promptyielded the most accurate results with 85% accuracy. We discuss applications ofthis research including STEM writing, education, and content creation."
Structured Chain-of-Thought Prompting for Code Generation,"['Jia Li', 'Ge Li', 'Yongmin Li', 'Zhi Jin']",http://arxiv.org/pdf/2305.06599v3.pdf,2023-05-11,"['cs.se', 'cs.cl']","  Large Language Models (LLMs) (e.g., ChatGPT) have shown impressiveperformance in code generation. LLMs take prompts as inputs, andChain-of-Thought (CoT) prompting is the state-of-the-art prompting technique.CoT prompting asks LLMs first to generate CoTs (i.e., intermediate naturallanguage reasoning steps) and then output the code. However, CoT prompting isdesigned for natural language generation and has low accuracy in codegeneration.  In this paper, we propose Structured CoTs (SCoTs) and present a novelprompting technique for code generation, named SCoT prompting. Our motivationis source code contains rich structural information and any code can becomposed of three program structures (i.e., sequence, branch, and loopstructures). Intuitively, structured intermediate reasoning steps make forstructured source code. Thus, we ask LLMs to use program structures to buildCoTs, obtaining SCoTs. Then, LLMs generate the final code based on SCoTs.Compared to CoT prompting, SCoT prompting explicitly constrains LLMs to thinkabout how to solve requirements from the view of source code and further theperformance of LLMs in code generation. We apply SCoT prompting to two LLMs(i.e., ChatGPT and Codex) and evaluate it on three benchmarks (i.e., HumanEval,MBPP, and MBCPP). (1) SCoT prompting outperforms the state-of-the-art baseline- CoT prompting by up to 13.79% in Pass@1. (2) Human evaluation shows humandevelopers prefer programs from SCoT prompting. (3) SCoT prompting is robust toexamples and achieves substantial improvements."
The Impact of AI in Physics Education: A Comprehensive Review from GCSE  to University Levels,"['Will Yeadon', 'Tom Hardy']",http://arxiv.org/pdf/2309.05163v1.pdf,2023-09-10,['physics.ed-ph'],"  With the rapid evolution of Artificial Intelligence (AI), its potentialimplications for higher education have become a focal point of interest. Thisstudy delves into the capabilities of AI in Physics Education and offersactionable AI policy recommendations. Using a Large Language Model (LLM), weassessed its ability to answer 1337 Physics exam questions spanning GCSE,A-Level, and Introductory University curricula. We employed various AIprompting techniques: Zero Shot, In Context Learning, and ConfirmatoryChecking, which merges Chain of Thought reasoning with Reflection. The AI'sproficiency varied across academic levels: it scored an average of 83.4% onGCSE, 63.8% on A-Level, and 37.4% on university-level questions, with anoverall average of 59.9% using the most effective prompting technique. In aseparate test, the LLM's accuracy on 5000 mathematical operations was found todecrease as the number of digits increased. Furthermore, when evaluated as amarking tool, the LLM's concordance with human markers averaged at 50.8%, withnotable inaccuracies in marking straightforward questions, likemultiple-choice. Given these results, our recommendations underscore caution:while current LLMs can consistently perform well on Physics questions atearlier educational stages, their efficacy diminishes with advanced content andcomplex calculations. LLM outputs often showcase novel methods not in thesyllabus, excessive verbosity, and miscalculations in basic arithmetic. Thissuggests that at university, there's no substantial threat from LLMs fornon-invigilated Physics questions. However, given the LLMs' considerableproficiency in writing Physics essays and coding abilities, non-invigilatedexaminations of these skills in Physics are highly vulnerable to automatedcompletion by LLMs. This vulnerability also extends to Physics questionspitched at lower academic levels."
HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create  Customized Content with Models,"['Swaroop Mishra', 'Elnaz Nouri']",http://arxiv.org/pdf/2208.08232v2.pdf,2022-08-17,"['cs.cl', 'cs.ai', 'cs.cv', 'cs.hc', 'cs.lg']","  Controlling the text generated by language models and customizing the contenthas been a long-standing challenge. Existing prompting techniques proposed inpursuit of providing control are task-specific and lack generality; thisprovides overwhelming choices for non-expert users to find a suitable methodfor their task. The effort associated with those techniques, such as in writingexamples, explanations, instructions, etc. further limits their adoption amongnon-expert users. In this paper, we propose a simple prompting strategy HELP METHINK where we encourage GPT3 to help non-expert users by asking a set ofrelevant questions and leveraging user answers to execute the task. Wedemonstrate the efficacy of our technique HELP ME THINK on a variety of tasks.Specifically, we focus on tasks that are hard for average humans and requiresignificant thinking to perform. We hope our work will encourage thedevelopment of unconventional ways to harness the power of large languagemodels."
Enabling Conversational Interaction with Mobile UI using Large Language  Models,"['Bryan Wang', 'Gang Li', 'Yang Li']",http://arxiv.org/pdf/2209.08655v2.pdf,2022-09-18,"['cs.hc', 'cs.ai']","  Conversational agents show the promise to allow users to interact with mobiledevices using language. However, to perform diverse UI tasks with naturallanguage, developers typically need to create separate datasets and models foreach specific task, which is expensive and effort-consuming. Recently,pre-trained large language models (LLMs) have been shown capable ofgeneralizing to various downstream tasks when prompted with a handful ofexamples from the target task. This paper investigates the feasibility ofenabling versatile conversational interactions with mobile UIs using a singleLLM. We designed prompting techniques to adapt an LLM to mobile UIs. Weexperimented with four important modeling tasks that address various scenariosin conversational interaction. Our method achieved competitive performance onthese challenging tasks without requiring dedicated datasets and training,offering a lightweight and generalizable approach to enable language-basedmobile interaction."
Teaching Algorithmic Reasoning via In-context Learning,"['Hattie Zhou', 'Azade Nova', 'Hugo Larochelle', 'Aaron Courville', 'Behnam Neyshabur', 'Hanie Sedghi']",http://arxiv.org/pdf/2211.09066v1.pdf,2022-11-15,"['cs.lg', 'cs.ai', 'cs.cl']","  Large language models (LLMs) have shown increasing in-context learningcapabilities through scaling up model and data size. Despite this progress,LLMs are still unable to solve algorithmic reasoning problems. While providinga rationale with the final answer has led to further improvements in multi-stepreasoning problems, Anil et al. 2022 showed that even simple algorithmicreasoning tasks such as parity are far from solved. In this work, we identifyand study four key stages for successfully teaching algorithmic reasoning toLLMs: (1) formulating algorithms as skills, (2) teaching multiple skillssimultaneously (skill accumulation), (3) teaching how to combine skills (skillcomposition) and (4) teaching how to use skills as tools. We show that it ispossible to teach algorithmic reasoning to LLMs via in-context learning, whichwe refer to as algorithmic prompting. We evaluate our approach on a variety ofarithmetic and quantitative reasoning tasks, and demonstrate significant boostsin performance over existing prompting techniques. In particular, for longparity, addition, multiplication and subtraction, we achieve an error reductionof approximately 10x, 9x, 5x and 2x respectively compared to the best availablebaselines."
Understanding Stereotypes in Language Models: Towards Robust Measurement  and Zero-Shot Debiasing,"['Justus Mattern', 'Zhijing Jin', 'Mrinmaya Sachan', 'Rada Mihalcea', 'Bernhard Schölkopf']",http://arxiv.org/pdf/2212.10678v1.pdf,2022-12-20,"['cs.cl', 'cs.lg']","  Generated texts from large pretrained language models have been shown toexhibit a variety of harmful, human-like biases about various demographics.These findings prompted large efforts aiming to understand and measure sucheffects, with the goal of providing benchmarks that can guide the developmentof techniques mitigating these stereotypical associations. However, as recentresearch has pointed out, the current benchmarks lack a robust experimentalsetup, consequently hindering the inference of meaningful conclusions fromtheir evaluation metrics. In this paper, we extend these arguments anddemonstrate that existing techniques and benchmarks aiming to measurestereotypes tend to be inaccurate and consist of a high degree of experimentalnoise that severely limits the knowledge we can gain from benchmarking languagemodels based on them. Accordingly, we propose a new framework for robustlymeasuring and quantifying biases exhibited by generative language models.Finally, we use this framework to investigate GPT-3's occupational gender biasand propose prompting techniques for mitigating these biases without the needfor fine-tuning."
Image To Tree with Recursive Prompting,"['James Batten', 'Matthew Sinclair', 'Ben Glocker', 'Michiel Schaap']",http://arxiv.org/pdf/2301.00447v1.pdf,2023-01-01,"['cs.cv', 'cs.lg']","  Extracting complex structures from grid-based data is a common key step inautomated medical image analysis. The conventional solution to recoveringtree-structured geometries typically involves computing the minimal cost paththrough intermediate representations derived from segmentation masks. However,this methodology has significant limitations in the context of projectiveimaging of tree-structured 3D anatomical data such as coronary arteries, sincethere are often overlapping branches in the 2D projection. In this work, wepropose a novel approach to predicting tree connectivity structure whichreformulates the task as an optimization problem over individual steps of arecursive process. We design and train a two-stage model which leverages theUNet and Transformer architectures and introduces an image-based promptingtechnique. Our proposed method achieves compelling results on a pair ofsynthetic datasets, and outperforms a shortest-path baseline."
Large Language Models Can Be Easily Distracted by Irrelevant Context,"['Freda Shi', 'Xinyun Chen', 'Kanishka Misra', 'Nathan Scales', 'David Dohan', 'Ed Chi', 'Nathanael Schärli', 'Denny Zhou']",http://arxiv.org/pdf/2302.00093v3.pdf,2023-01-31,"['cs.cl', 'cs.ai']","  Large language models have achieved impressive performance on various naturallanguage processing tasks. However, so far they have been evaluated primarilyon benchmarks where all information in the input context is relevant forsolving the task. In this work, we investigate the distractibility of largelanguage models, i.e., how the model problem-solving accuracy can be influencedby irrelevant context. In particular, we introduce Grade-School Math withIrrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevantinformation in the problem description. We use this benchmark to measure thedistractibility of cutting-edge prompting techniques for large language models,and find that the model performance is dramatically decreased when irrelevantinformation is included. We also identify several approaches for mitigatingthis deficiency, such as decoding with self-consistency and adding to theprompt an instruction that tells the language model to ignore the irrelevantinformation."
Synthetic Prompting: Generating Chain-of-Thought Demonstrations for  Large Language Models,"['Zhihong Shao', 'Yeyun Gong', 'Yelong Shen', 'Minlie Huang', 'Nan Duan', 'Weizhu Chen']",http://arxiv.org/pdf/2302.00618v1.pdf,2023-02-01,['cs.cl'],"  Large language models can perform various reasoning tasks by usingchain-of-thought prompting, which guides them to find answers throughstep-by-step demonstrations. However, the quality of the prompts depends on thedemonstrations given to the models, and creating many of them by hand iscostly. We introduce Synthetic prompting, a method that leverages a fewhandcrafted examples to prompt the model to generate more examples by itself,and selects effective demonstrations to elicit better reasoning. Our methodalternates between a backward and forward process to generate new examples. Thebackward process generates a question that match a sampled reasoning chain, sothat the question is solvable and clear. The forward process produces a moredetailed reasoning chain for the question, improving the quality of theexample. We evaluate our method on numerical, symbolic, and algorithmicreasoning tasks, and show that it outperforms existing prompting techniques."
Language-Specific Representation of Emotion-Concept Knowledge Causally  Supports Emotion Inference,"['Ming Li', 'Yusheng Su', 'Hsiu-Yuan Huang', 'Jiali Cheng', 'Xin Hu', 'Xinmiao Zhang', 'Huadong Wang', 'Yujia Qin', 'Xiaozhi Wang', 'Zhiyuan Liu', 'Dan Zhang']",http://arxiv.org/pdf/2302.09582v4.pdf,2023-02-19,"['cs.ai', 'cs.cl']","  Understanding how language supports emotion inference remains a topic ofdebate in emotion science. The present study investigated whetherlanguage-derived emotion-concept knowledge would causally support emotioninference by manipulating the language-specific knowledge representations inlarge language models. Using the prompt technique, 14 attributes of emotionconcepts were found to be represented by distinct artificial neuronpopulations. By manipulating these attribute-related neurons, the majority ofthe emotion inference tasks showed performance deterioration compared to randommanipulations. The attribute-specific performance deterioration was related tothe importance of different attributes in human mental space. Our findingsprovide causal evidence in support of a language-based mechanism for emotioninference and highlight the contributions of emotion-concept knowledge."
MathPrompter: Mathematical Reasoning using Large Language Models,"['Shima Imani', 'Liang Du', 'Harsh Shrivastava']",http://arxiv.org/pdf/2303.05398v1.pdf,2023-03-04,"['cs.cl', 'cs.ai']","  Large Language Models (LLMs) have limited performance when solving arithmeticreasoning tasks and often provide incorrect answers. Unlike natural languageunderstanding, math problems typically have a single correct answer, making thetask of generating accurate solutions more challenging for LLMs. To the best ofour knowledge, we are not aware of any LLMs that indicate their level ofconfidence in their responses which fuels a trust deficit in these modelsimpeding their adoption. To address this deficiency, we propose `MathPrompter',a technique that improves performance of LLMs on arithmetic problems along withincreased reliance in the predictions. MathPrompter uses the Zero-shotchain-of-thought prompting technique to generate multiple Algebraic expressionsor Python functions to solve the same math problem in different ways andthereby raise the confidence level in the output results. This is in contrastto other prompt based CoT methods, where there is no check on the validity ofthe intermediate steps followed. Our technique improves over state-of-the-arton the MultiArith dataset ($78.7\%\rightarrow92.5\%$) evaluated using 175Bparameter GPT-based LLM."
Zero-shot Temporal Relation Extraction with ChatGPT,"['Chenhan Yuan', 'Qianqian Xie', 'Sophia Ananiadou']",http://arxiv.org/pdf/2304.05454v1.pdf,2023-04-11,"['cs.cl', 'cs.ai']","  The goal of temporal relation extraction is to infer the temporal relationbetween two events in the document. Supervised models are dominant in thistask. In this work, we investigate ChatGPT's ability on zero-shot temporalrelation extraction. We designed three different prompt techniques to breakdown the task and evaluate ChatGPT. Our experiments show that ChatGPT'sperformance has a large gap with that of supervised methods and can heavilyrely on the design of prompts. We further demonstrate that ChatGPT can infermore small relation classes correctly than supervised methods. The currentshortcomings of ChatGPT on temporal relation extraction are also discussed inthis paper. We found that ChatGPT cannot keep consistency during temporalinference and it fails in actively long-dependency temporal inference."
An Empirical Study on the Robustness of the Segment Anything Model (SAM),"['Yuqing Wang', 'Yun Zhao', 'Linda Petzold']",http://arxiv.org/pdf/2305.06422v2.pdf,2023-05-10,['cs.cv'],"  The Segment Anything Model (SAM) is a foundation model for general imagesegmentation. Although it exhibits impressive performance predominantly onnatural images, understanding its robustness against various imageperturbations and domains is critical for real-world applications where suchchallenges frequently arise. In this study we conduct a comprehensiverobustness investigation of SAM under diverse real-world conditions. Ourexperiments encompass a wide range of image perturbations. Our experimentalresults demonstrate that SAM's performance generally declines under perturbedimages, with varying degrees of vulnerability across different perturbations.By customizing prompting techniques and leveraging domain knowledge based onthe unique characteristics of each dataset, the model's resilience to theseperturbations can be enhanced, addressing dataset-specific challenges. Thiswork sheds light on the limitations and strengths of SAM in real-worldapplications, promoting the development of more robust and versatile imagesegmentation solutions."
SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim  Verification on Scientific Tables,"['Xinyuan Lu', 'Liangming Pan', 'Qian Liu', 'Preslav Nakov', 'Min-Yen Kan']",http://arxiv.org/pdf/2305.13186v3.pdf,2023-05-22,"['cs.cl', 'cs.ai']","  Current scientific fact-checking benchmarks exhibit several shortcomings,such as biases arising from crowd-sourced claims and an over-reliance ontext-based evidence. We present SCITAB, a challenging evaluation datasetconsisting of 1.2K expert-verified scientific claims that 1) originate fromauthentic scientific publications and 2) require compositional reasoning forverification. The claims are paired with evidence-containing scientific tablesannotated with labels. Through extensive evaluations, we demonstrate thatSCITAB poses a significant challenge to state-of-the-art models, includingtable-based pretraining models and large language models. All models exceptGPT-4 achieved performance barely above random guessing. Popular promptingtechniques, such as Chain-of-Thought, do not achieve much performance gains onSCITAB. Our analysis uncovers several unique challenges posed by SCITAB,including table grounding, claim ambiguity, and compositional reasoning. Ourcodes and data are publicly available at https://github.com/XinyuanLu00/SciTab."
Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented  Dialogues and Annotations,"['Tiziano Labruna', 'Sofia Brenna', 'Andrea Zaninello', 'Bernardo Magnini']",http://arxiv.org/pdf/2305.14556v1.pdf,2023-05-23,"['cs.cl', 'cs.ai']","  Large pre-trained language models have exhibited unprecedented capabilitiesin producing high-quality text via prompting techniques. This fact introducesnew possibilities for data collection and annotation, particularly insituations where such data is scarce, complex to gather, expensive, or evensensitive. In this paper, we explore the potential of these models to generateand annotate goal-oriented dialogues, and conduct an in-depth analysis toevaluate their quality. Our experiments employ ChatGPT, and encompass threecategories of goal-oriented dialogues (task-oriented, collaborative, andexplanatory), two generation modes (interactive and one-shot), and twolanguages (English and Italian). Based on extensive human-based evaluations, wedemonstrate that the quality of generated dialogues and annotations is on parwith those generated by humans."
OverPrompt: Enhancing ChatGPT through Efficient In-Context Learning,"['Jiazheng Li', 'Runcong Zhao', 'Yongxin Yang', 'Yulan He', 'Lin Gui']",http://arxiv.org/pdf/2305.14973v2.pdf,2023-05-24,['cs.cl'],"  The remarkable performance of pre-trained large language models hasrevolutionised various natural language processing applications. Due to hugeparametersizes and extensive running costs, companies or organisations tend totransfer the models to the target task by zero-shot prompting techniques.However, the prohibitive costs of tokens and time have hindered their adoptionin applications. We propose OverPrompt, leveraging the in-context learningcapability of LLMs to handle multiple task inputs, thereby reducing token andtime costs. This approach could potentially improve task performance during APIqueries due to better conditional distribution mapping. Evaluated acrossdiverse classification datasets, our experiments show that OverPrompt canachieve cost-efficient zero-shot classification without causing significantdetriment to task performance, and in some cases, even improving it. Anablation study conducted on various LLMs, along with an investigation into therobustness of our prompting strategy to different input ordering, offersvaluable insights into the broader applicability of our method across diversetasks. These findings also suggest a more seamless integration of our methodwith LLMs through an API."
StudentEval: A Benchmark of Student-Written Prompts for Large Language  Models of Code,"['Hannah McLean Babe', 'Sydney Nguyen', 'Yangtian Zi', 'Arjun Guha', 'Molly Q Feldman', 'Carolyn Jane Anderson']",http://arxiv.org/pdf/2306.04556v1.pdf,2023-06-07,"['cs.lg', 'cs.hc', 'cs.se']","  Code LLMs are being rapidly deployed and there is evidence that they can makeprofessional programmers more productive. Current benchmarks for codegeneration measure whether models generate correct programs given an expertprompt. In this paper, we present a new benchmark containing multiple promptsper problem, written by a specific population of non-expert prompters:beginning programmers. StudentEval contains 1,749 prompts for 48 problems,written by 80 students who have only completed one semester of Pythonprogramming. Our students wrote these prompts while working interactively witha Code LLM, and we observed very mixed success rates. We use StudentEval toevaluate 5 Code LLMs and find that StudentEval is a better discriminator ofmodel performance than existing benchmarks. We analyze the prompts and findsignificant variation in students' prompting techniques. We also find thatnondeterministic LLM sampling could mislead students into thinking that theirprompts are more (or less) effective than they actually are, which hasimplications for how to teach with Code LLMs."
Knowledge-Prompted Estimator: A Novel Approach to Explainable Machine  Translation Assessment,"['Hao Yang', 'Min Zhang', 'Shimin Tao', 'Minghan Wang', 'Daimeng Wei', 'Yanfei Jiang']",http://arxiv.org/pdf/2306.07486v1.pdf,2023-06-13,['cs.cl'],"  Cross-lingual Machine Translation (MT) quality estimation plays a crucialrole in evaluating translation performance. GEMBA, the first MT qualityassessment metric based on Large Language Models (LLMs), employs one-stepprompting to achieve state-of-the-art (SOTA) in system-level MT qualityestimation; however, it lacks segment-level analysis. In contrast,Chain-of-Thought (CoT) prompting outperforms one-step prompting by offeringimproved reasoning and explainability. In this paper, we introduceKnowledge-Prompted Estimator (KPE), a CoT prompting method that combines threeone-step prompting techniques, including perplexity, token-level similarity,and sentence-level similarity. This method attains enhanced performance forsegment-level estimation compared with previous deep learning models andone-step prompting approaches. Furthermore, supplementary experiments onword-level visualized alignment demonstrate that our KPE method significantlyimproves token alignment compared with earlier models and provides betterinterpretability for MT quality estimation. Code will be released uponpublication."
Questioning the Survey Responses of Large Language Models,"['Ricardo Dominguez-Olmedo', 'Moritz Hardt', 'Celestine Mendler-Dünner']",http://arxiv.org/pdf/2306.07951v2.pdf,2023-06-13,['cs.cl'],"  As large language models increase in capability, researchers have started toconduct surveys of all kinds on these models with varying scientificmotivations. In this work, we examine what we can learn from language models'survey responses on the basis of the well-established American Community Survey(ACS) by the U.S. Census Bureau. Using a de-facto standard multiple-choiceprompting technique and evaluating 40 different language models, hundreds ofthousands of times each on questions from the ACS, we systematically establishtwo dominant patterns. First, models have significant position and labelingbiases, for example, towards survey responses labeled with the letter ""A"".Second, when adjusting for labeling biases through randomized answer ordering,models across the board trend towards uniformly random survey responses. Infact, binary classifiers can almost perfectly differentiate between models'responses to the ACS and the responses of the US census. Taken together, ourfindings suggest caution in treating survey responses from language models asequivalent to those of human populations at present time."
Investigating Prompting Techniques for Zero- and Few-Shot Visual  Question Answering,"['Rabiul Awal', 'Le Zhang', 'Aishwarya Agrawal']",http://arxiv.org/pdf/2306.09996v1.pdf,2023-06-16,"['cs.cv', 'cs.cl']","  Visual question answering (VQA) is a challenging task that requires theability to comprehend and reason with visual information. While recentvision-language models have made strides, they continue to struggle withzero-shot VQA, particularly in handling complex compositional questions andadapting to new domains i.e. knowledge-based reasoning. This paper explores theuse of various prompting strategies, focusing on the BLIP2 model, to enhancezero-shot VQA performance. We conduct a comprehensive investigation acrossseveral VQA datasets, examining the effectiveness of different questiontemplates, the role of few-shot exemplars, the impact of chain-of-thought (CoT)reasoning, and the benefits of incorporating image captions as additionalvisual cues. Despite the varied outcomes, our findings demonstrate thatcarefully designed question templates and the integration of additional visualcues, like image captions, can contribute to improved VQA performance,especially when used in conjunction with few-shot examples. However, we alsoidentify a limitation in the use of chain-of-thought rationalization, whichnegatively affects VQA accuracy. Our study thus provides critical insights intothe potential of prompting for improving zero-shot VQA performance."
Extracting Multi-valued Relations from Language Models,"['Sneha Singhania', 'Simon Razniewski', 'Gerhard Weikum']",http://arxiv.org/pdf/2307.03122v2.pdf,2023-07-06,['cs.cl'],"  The widespread usage of latent language representations via pre-trainedlanguage models (LMs) suggests that they are a promising source of structuredknowledge. However, existing methods focus only on a single object persubject-relation pair, even though often multiple objects are correct. Toovercome this limitation, we analyze these representations for their potentialto yield materialized multi-object relational knowledge. We formulate theproblem as a rank-then-select task. For ranking candidate objects, we evaluateexisting prompting techniques and propose new ones incorporating domainknowledge. Among the selection methods, we find that choosing objects with alikelihood above a learned relation-specific threshold gives a 49.5% F1 score.Our results highlight the difficulty of employing LMs for the multi-valuedslot-filling task and pave the way for further research on extractingrelational knowledge from latent language representations."
Prompts Should not be Seen as Secrets: Systematically Measuring Prompt  Extraction Attack Success,"['Yiming Zhang', 'Daphne Ippolito']",http://arxiv.org/pdf/2307.06865v1.pdf,2023-07-13,"['cs.cl', 'cs.ai']","  The generations of large language models are commonly controlled throughprompting techniques, where a user's query to the model is prefixed with aprompt that aims to guide the model's behaviour on the query. The prompts usedby companies to guide their models are often treated as secrets, to be hiddenfrom the user making the query. They have even been treated as commodities tobe bought and sold. However, there has been anecdotal evidence showing that theprompts can be extracted by a user even when they are kept secret. In thispaper, we present a framework for systematically measuring the success ofprompt extraction attacks. In experiments with multiple sources of prompts andmultiple underlying language models, we find that simple text-based attacks canin fact reveal prompts with high probability."
Leveraging Large Language Models to Generate Answer Set Programs,"['Adam Ishay', 'Zhun Yang', 'Joohyung Lee']",http://arxiv.org/pdf/2307.07699v1.pdf,2023-07-15,"['cs.ai', 'cs.cl', 'cs.sc']","  Large language models (LLMs), such as GPT-3 and GPT-4, have demonstratedexceptional performance in various natural language processing tasks and haveshown the ability to solve certain reasoning problems. However, their reasoningcapabilities are limited and relatively shallow, despite the application ofvarious prompting techniques. In contrast, formal logic is adept at handlingcomplex reasoning, but translating natural language descriptions into formallogic is a challenging task that non-experts struggle with. This paper proposesa neuro-symbolic method that combines the strengths of large language modelsand answer set programming. Specifically, we employ an LLM to transform naturallanguage descriptions of logic puzzles into answer set programs. We carefullydesign prompts for an LLM to convert natural language descriptions into answerset programs in a step by step manner. Surprisingly, with just a few in-contextlearning examples, LLMs can generate reasonably complex answer set programs.The majority of errors made are relatively simple and can be easily correctedby humans, thus enabling LLMs to effectively assist in the creation of answerset programs."
Fixing Rust Compilation Errors using LLMs,"['Pantazis Deligiannis', 'Akash Lal', 'Nikita Mehrotra', 'Aseem Rastogi']",http://arxiv.org/pdf/2308.05177v1.pdf,2023-08-09,"['cs.se', 'cs.pl']","  The Rust programming language, with its safety guarantees, has establisheditself as a viable choice for low-level systems programming language over thetraditional, unsafe alternatives like C/C++. These guarantees come from astrong ownership-based type system, as well as primitive support for featureslike closures, pattern matching, etc., that make the code more concise andamenable to reasoning. These unique Rust features also pose a steep learningcurve for programmers.  This paper presents a tool called RustAssistant that leverages the emergentcapabilities of Large Language Models (LLMs) to automatically suggest fixes forRust compilation errors. RustAssistant uses a careful combination of promptingtechniques as well as iteration with an LLM to deliver high accuracy of fixes.RustAssistant is able to achieve an impressive peak accuracy of roughly 74% onreal-world compilation errors in popular open-source Rust repositories. We planto release our dataset of Rust compilation errors to enable further research."
The Devil is in the Errors: Leveraging Large Language Models for  Fine-grained Machine Translation Evaluation,"['Patrick Fernandes', 'Daniel Deutsch', 'Mara Finkelstein', 'Parker Riley', 'André F. T. Martins', 'Graham Neubig', 'Ankush Garg', 'Jonathan H. Clark', 'Markus Freitag', 'Orhan Firat']",http://arxiv.org/pdf/2308.07286v1.pdf,2023-08-14,"['cs.cl', 'cs.lg']","  Automatic evaluation of machine translation (MT) is a critical tool drivingthe rapid iterative development of MT systems. While considerable progress hasbeen made on estimating a single scalar quality score, current metrics lack theinformativeness of more detailed schemes that annotate individual errors, suchas Multidimensional Quality Metrics (MQM). In this paper, we help fill this gapby proposing AutoMQM, a prompting technique which leverages the reasoning andin-context learning capabilities of large language models (LLMs) and asks themto identify and categorize errors in translations. We start by evaluatingrecent LLMs, such as PaLM and PaLM-2, through simple score predictionprompting, and we study the impact of labeled data through in-context learningand finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find thatit improves performance compared to just prompting for scores (withparticularly large gains for larger models) while providing interpretabilitythrough error spans that align with human annotations."
Boosting Logical Reasoning in Large Language Models through a New  Framework: The Graph of Thought,"['Bin Lei', 'pei-Hung Lin', 'Chunhua Liao', 'Caiwen Ding']",http://arxiv.org/pdf/2308.08614v1.pdf,2023-08-16,"['cs.lg', 'cs.ai', 'cs.cl']","  Recent advancements in large-scale models, such as GPT-4, have showcasedremarkable capabilities in addressing standard queries. However, when facingcomplex problems that require multi-step logical reasoning, their accuracydramatically decreases. Current research has explored the realm of\textit{prompting engineering} to bolster the inferential capacities of thesemodels. Our paper unveils a pioneering prompting technique, dubbed\textit{Graph of Thoughts (GoT)}. Through testing on a trio of escalatingchallenges: the 24-point game, resolution of high-degree polynomial equations,and derivation of formulas for recursive sequences, our method outperformedGPT-4, achieving accuracy improvements of $89.7\%$, $86\%$, and $56\%$ for eachrespective task. Moreover, when juxtaposed with the state-of-the-art (SOTA)prompting method, \textit{Tree of Thought (ToT)}, our approach registered anaverage accuracy boost of $23\%$, $24\%$, and $15\%$."
DevGPT: Studying Developer-ChatGPT Conversations,"['Tao Xiao', 'Christoph Treude', 'Hideaki Hata', 'Kenichi Matsumoto']",http://arxiv.org/pdf/2309.03914v1.pdf,2023-08-31,['cs.se'],"  The emergence of large language models (LLMs) such as ChatGPT has disruptedthe landscape of software development. Many studies are investigating thequality of responses generated by ChatGPT, the efficacy of various promptingtechniques, and its comparative performance in programming contests, to name afew examples. Yet, we know very little about how ChatGPT is actually used bysoftware developers. What questions do developers present to ChatGPT? What arethe dynamics of these interactions? What is the backdrop against which theseconversations are held, and how do the conversations feedback into theartifacts of their work? To close this gap, we introduce DevGPT, a curateddataset which encompasses 17,913 prompts and ChatGPT's responses including11,751 code snippets, coupled with the corresponding software developmentartifacts -- ranging from source code, commits, issues, pull requests, todiscussions and Hacker News threads -- to enable the analysis of the contextand implications of these developer interactions with ChatGPT."
Generative Speech Recognition Error Correction with Large Language  Models and Task-Activating Prompting,"['Chao-Han Huck Yang', 'Yile Gu', 'Yi-Chieh Liu', 'Shalini Ghosh', 'Ivan Bulyko', 'Andreas Stolcke']",http://arxiv.org/pdf/2309.15649v2.pdf,2023-09-27,"['cs.cl', 'cs.ai', 'cs.lg', 'cs.sd', 'eess.as']","  We explore the ability of large language models (LLMs) to act as speechrecognition post-processors that perform rescoring and error correction. Ourfirst focus is on instruction prompting to let LLMs perform these task withoutfine-tuning, for which we evaluate different prompting schemes, both zero- andfew-shot in-context learning, and a novel task activation prompting method thatcombines causal instructions and demonstration to increase its context windows.Next, we show that rescoring only by in-context learning with frozen LLMsachieves results that are competitive with rescoring by domain-tuned LMs, usinga pretrained first-pass recognition system and rescoring output on twoout-of-domain tasks (ATIS and WSJ). By combining prompting techniques withfine-tuning we achieve error rates below the N-best oracle level, showcasingthe generalization power of the LLMs."
UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large  Language Model Capabilities,"['Hejia Geng', 'Boxun Xu', 'Peng Li']",http://arxiv.org/pdf/2310.01441v2.pdf,2023-09-30,"['cs.cl', 'cs.ai']","  Large Language Models (LLMs) have demonstrated impressive inferentialcapabilities, with numerous research endeavors devoted to enhancing thiscapacity through prompting. Despite these efforts, a unified epistemologicalfoundation is still conspicuously absent. Drawing inspiration from Kant's apriori philosophy, we propose the UPAR prompting framework, designed to emulatethe structure of human cognition within LLMs. The UPAR framework is delineatedinto four phases: ""Understand"", ""Plan"", ""Act"", and ""Reflect"", enabling theextraction of structured information from complex contexts, prior planning ofsolutions, execution according to plan, and self-reflection. This structuresignificantly augments the explainability and accuracy of LLM inference,producing a human-understandable and inspectable inferential trajectory.Furthermore, our work offers an epistemological foundation for existingprompting techniques, allowing for a possible systematic integration of thesemethods. With GPT-4, our approach elevates the accuracy from COT baseline of22.92% to 58.33% in a challenging subset of GSM8K, and from 67.91% to 75.40% inthe causal judgment task. Without using few-shot examples or external tools,UPAR significantly outperforms existing prompting methods on SCIBENCH, achallenging dataset containing collegiate-level mathematics, chemistry, andphysics scientific problems."
Take a Step Back: Evoking Reasoning via Abstraction in Large Language  Models,"['Huaixiu Steven Zheng', 'Swaroop Mishra', 'Xinyun Chen', 'Heng-Tze Cheng', 'Ed H. Chi', 'Quoc V Le', 'Denny Zhou']",http://arxiv.org/pdf/2310.06117v1.pdf,2023-10-09,"['cs.lg', 'cs.ai', 'cs.cl']","  We present Step-Back Prompting, a simple prompting technique that enablesLLMs to do abstractions to derive high-level concepts and first principles frominstances containing specific details. Using the concepts and principles toguide the reasoning steps, LLMs significantly improve their abilities infollowing a correct reasoning path towards the solution. We conduct experimentsof Step-Back Prompting with PaLM-2L models and observe substantial performancegains on a wide range of challenging reasoning-intensive tasks including STEM,Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Promptingimproves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%,TimeQA by 27%, and MuSiQue by 7%."
POSQA: Probe the World Models of LLMs with Size Comparisons,"['Chang Shu', 'Jiuzhou Han', 'Fangyu Liu', 'Ehsan Shareghi', 'Nigel Collier']",http://arxiv.org/pdf/2310.13394v1.pdf,2023-10-20,"['cs.cl', 'cs.ai', 'cs.cy']","  Embodied language comprehension emphasizes that language understanding is notsolely a matter of mental processing in the brain but also involvesinteractions with the physical and social environment. With the explosivegrowth of Large Language Models (LLMs) and their already ubiquitous presence inour daily lives, it is becoming increasingly necessary to verify theirreal-world understanding. Inspired by cognitive theories, we propose POSQA: aPhysical Object Size Question Answering dataset with simple size comparisonquestions to examine the extremity and analyze the potential mechanisms of theembodied comprehension of the latest LLMs.  We show that even the largest LLMs today perform poorly under the zero-shotsetting. We then push their limits with advanced prompting techniques andexternal knowledge augmentation. Furthermore, we investigate whether theirreal-world comprehension primarily derives from contextual information orinternal weights and analyse the impact of prompt formats and report bias ofdifferent objects. Our results show that real-world understanding that LLMsshaped from textual data can be vulnerable to deception and confusion by thesurface form of prompts, which makes it less aligned with human behaviours."
MuSR: Testing the Limits of Chain-of-thought with Multistep Soft  Reasoning,"['Zayne Sprague', 'Xi Ye', 'Kaj Bostrom', 'Swarat Chaudhuri', 'Greg Durrett']",http://arxiv.org/pdf/2310.16049v1.pdf,2023-10-24,['cs.cl'],"  While large language models (LLMs) equipped with techniques likechain-of-thought prompting have demonstrated impressive capabilities, theystill fall short in their ability to reason robustly in complex settings.However, evaluating LLM reasoning is challenging because system capabilitiescontinue to grow while benchmark datasets for tasks like logical deduction haveremained static. We introduce MuSR, a dataset for evaluating language models onmultistep soft reasoning tasks specified in a natural language narrative. Thisdataset has two crucial features. First, it is created through a novelneurosymbolic synthetic-to-natural generation algorithm, enabling theconstruction of complex reasoning instances that challenge GPT-4 (e.g., murdermysteries roughly 1000 words in length) and which can be scaled further as morecapable LLMs are released. Second, our dataset instances are free textnarratives corresponding to real-world domains of reasoning; this makes itsimultaneously much more challenging than other synthetically-craftedbenchmarks while remaining realistic and tractable for human annotators tosolve with high accuracy. We evaluate a range of LLMs and prompting techniqueson this dataset and characterize the gaps that remain for techniques likechain-of-thought to perform robust reasoning."
"Supercharging academic writing with generative AI: framework,  techniques, and caveats",['Zhicheng Lin'],http://arxiv.org/pdf/2310.17143v1.pdf,2023-10-26,"['cs.cy', 'cs.cl']","  Academic writing is an indispensable yet laborious part of the researchenterprise. This Perspective maps out principles and methods for usinggenerative artificial intelligence (AI), specifically large language models(LLMs), to elevate the quality and efficiency of academic writing. We introducea human-AI collaborative framework that delineates the rationale (why), process(how), and nature (what) of AI engagement in writing. The framework pinpointsboth short-term and long-term reasons for engagement and their underlyingmechanisms (e.g., cognitive offloading and imaginative stimulation). It revealsthe role of AI throughout the writing process, conceptualized through atwo-stage model for human-AI collaborative writing, and the nature of AIassistance in writing, represented through a model of writing-assistance typesand levels. Building on this framework, we describe effective promptingtechniques for incorporating AI into the writing routine (outlining, drafting,and editing) as well as strategies for maintaining rigorous scholarship,adhering to varied journal policies, and avoiding overreliance on AI.Ultimately, the prudent integration of AI into academic writing can ease thecommunication burden, empower authors, accelerate discovery, and promotediversity in science."
Little Giants: Exploring the Potential of Small LLMs as Evaluation  Metrics in Summarization in the Eval4NLP 2023 Shared Task,"['Neema Kotonya', 'Saran Krishnasamy', 'Joel Tetreault', 'Alejandro Jaimes']",http://arxiv.org/pdf/2311.00686v1.pdf,2023-11-01,['cs.cl'],"  This paper describes and analyzes our participation in the 2023 Eval4NLPshared task, which focuses on assessing the effectiveness of prompt-basedtechniques to empower Large Language Models to handle the task of qualityestimation, particularly in the context of evaluating machine translations andsummaries. We conducted systematic experiments with various promptingtechniques, including standard prompting, prompts informed by annotatorinstructions, and innovative chain-of-thought prompting. In addition, weintegrated these approaches with zero-shot and one-shot learning methods tomaximize the efficacy of our evaluation procedures. Our work reveals thatcombining these approaches using a ""small"", open source model (orca_mini_v3_7B)yields competitive results."
Can Large Language Models Design Accurate Label Functions?,"['Naiqing Guan', 'Kaiwen Chen', 'Nick Koudas']",http://arxiv.org/pdf/2311.00739v1.pdf,2023-11-01,"['cs.cl', 'cs.db', 'cs.lg', 'h.2.8; i.5.4']","  Programmatic weak supervision methodologies facilitate the expedited labelingof extensive datasets through the use of label functions (LFs) that encapsulateheuristic data sources. Nonetheless, the creation of precise LFs necessitatesdomain expertise and substantial endeavors. Recent advances in pre-trainedlanguage models (PLMs) have exhibited substantial potential across diversetasks. However, the capacity of PLMs to autonomously formulate accurate LFsremains an underexplored domain. In this research, we address this gap byintroducing DataSculpt, an interactive framework that harnesses PLMs for theautomated generation of LFs. Within DataSculpt, we incorporate an array ofprompting techniques, instance selection strategies, and LF filtration methodsto explore the expansive design landscape. Ultimately, we conduct a thoroughassessment of DataSculpt's performance on 12 real-world datasets, encompassinga range of tasks. This evaluation unveils both the strengths and limitations ofcontemporary PLMs in LF design."
Program-Aided Reasoners (better) Know What They Know,"['Anubha Kabra', 'Sanketh Rangreji', 'Yash Mathur', 'Aman Madaan', 'Emmy Liu', 'Graham Neubig']",http://arxiv.org/pdf/2311.09553v1.pdf,2023-11-16,['cs.ai'],"  Prior work shows that program-aided reasoning, in which large language models(LLMs) are combined with programs written in programming languages such asPython, can significantly improve accuracy on various reasoning tasks. However,while accuracy is essential, it is also important for such reasoners to ""knowwhat they know"", which can be quantified through the calibration of the model.In this paper, we compare the calibration of Program Aided Language Models(PAL) and text-based Chain-of-thought (COT) prompting techniques over 5datasets and 2 model types: LLaMA models and OpenAI models. Our resultsindicate that PAL leads to improved calibration in 75% of the instances. Ouranalysis uncovers that prompting styles that produce lesser diversity ingenerations also have more calibrated results, and thus we also experiment withinducing lower generation diversity using temperature scaling and find that forcertain temperatures, PAL is not only more accurate but is also more calibratedthan COT. Overall, we demonstrate that, in the majority of cases, program-aidedreasoners better know what they know than text-based counterparts."
Think Twice: Perspective-Taking Improves Large Language Models'  Theory-of-Mind Capabilities,"['Alex Wilf', 'Sihyun Shawn Lee', 'Paul Pu Liang', 'Louis-Philippe Morency']",http://arxiv.org/pdf/2311.10227v1.pdf,2023-11-16,"['cs.ai', 'cs.cl']","  Human interactions are deeply rooted in the interplay of thoughts, beliefs,and desires made possible by Theory of Mind (ToM): our cognitive ability tounderstand the mental states of ourselves and others. Although ToM may comenaturally to us, emulating it presents a challenge to even the most advancedLarge Language Models (LLMs). Recent improvements to LLMs' reasoningcapabilities from simple yet effective prompting techniques such asChain-of-Thought have seen limited applicability to ToM. In this paper, we turnto the prominent cognitive science theory ""Simulation Theory"" to bridge thisgap. We introduce SimToM, a novel two-stage prompting framework inspired bySimulation Theory's notion of perspective-taking. To implement this idea oncurrent ToM benchmarks, SimToM first filters context based on what thecharacter in question knows before answering a question about their mentalstate. Our approach, which requires no additional training and minimalprompt-tuning, shows substantial improvement over existing methods, and ouranalysis reveals the importance of perspective-taking to Theory-of-Mindcapabilities. Our findings suggest perspective-taking as a promising directionfor future research into improving LLMs' ToM capabilities."
CoIE: Chain-of-Instruct Editing for Multi-Attribute Face Manipulation,"['Zhenduo Zhang', 'Bowen Zhang', 'Guang Liu']",http://arxiv.org/pdf/2312.07879v1.pdf,2023-12-13,"['cs.cv', 'cs.ai']","  Current text-to-image editing models often encounter challenges with smoothlymanipulating multiple attributes using a single instruction. Taking inspirationfrom the Chain-of-Thought prompting technique utilized in language models, wepresent an innovative concept known as Chain-of-Instruct Editing (CoIE), whichenhances the capabilities of these models through step-by-step editing using aseries of instructions. In particular, in the context of face manipulation, weleverage the contextual learning abilities of a pretrained Large Language Model(LLM), such as GPT-4, to generate a sequence of instructions from the originalinput, utilizing a purpose-designed 1-shot template. To further improve theprecision of each editing step, we conduct fine-tuning on the editing modelsusing our self-constructed instruction-guided face editing dataset,Instruct-CelebA. And additionally, we incorporate a super-resolution module tomitigate the adverse effects of editability and quality degradation.Experimental results across various challenging cases confirm the significantboost in multi-attribute facial image manipulation using chain-of-instructediting. This is evident in enhanced editing success rates, measured by CLIPSimand Coverage metrics, improved by 17.86% and 85.45% respectively, andheightened controllability indicated by Preserve L1 and Quality metrics,improved by 11.58% and 4.93% respectively."
3DAxiesPrompts: Unleashing the 3D Spatial Task Capabilities of GPT-4V,"['Dingning Liu', 'Xiaomeng Dong', 'Renrui Zhang', 'Xu Luo', 'Peng Gao', 'Xiaoshui Huang', 'Yongshun Gong', 'Zhihui Wang']",http://arxiv.org/pdf/2312.09738v1.pdf,2023-12-15,['cs.ai'],"  In this work, we present a new visual prompting method called 3DAxiesPrompts(3DAP) to unleash the capabilities of GPT-4V in performing 3D spatial tasks.Our investigation reveals that while GPT-4V exhibits proficiency in discerningthe position and interrelations of 2D entities through current visual promptingtechniques, its abilities in handling 3D spatial tasks have yet to be explored.In our approach, we create a 3D coordinate system tailored to 3D imagery,complete with annotated scale information. By presenting images infused withthe 3DAP visual prompt as inputs, we empower GPT-4V to ascertain the spatialpositioning information of the given 3D target image with a high degree ofprecision. Through experiments, We identified three tasks that could be stablycompleted using the 3DAP method, namely, 2D to 3D Point Reconstruction, 2D to3D point matching, and 3D Object Detection. We perform experiments on ourproposed dataset 3DAP-Data, the results from these experiments validate theefficacy of 3DAP-enhanced GPT-4V inputs, marking a significant stride in 3Dspatial task execution."
Prompting as Probing: Using Language Models for Knowledge Base  Construction,"['Dimitrios Alivanistos', 'Selene Báez Santamaría', 'Michael Cochez', 'Jan-Christoph Kalo', 'Emile van Krieken', 'Thiviyan Thanapalasingam']",http://arxiv.org/pdf/2208.11057v3.pdf,2022-08-23,"['cs.cl', 'cs.ai']","  Language Models (LMs) have proven to be useful in various downstreamapplications, such as summarisation, translation, question answering and textclassification. LMs are becoming increasingly important tools in ArtificialIntelligence, because of the vast quantity of information they can store. Inthis work, we present ProP (Prompting as Probing), which utilizes GPT-3, alarge Language Model originally proposed by OpenAI in 2020, to perform the taskof Knowledge Base Construction (KBC). ProP implements a multi-step approachthat combines a variety of prompting techniques to achieve this. Our resultsshow that manual prompt curation is essential, that the LM must be encouragedto give answer sets of variable lengths, in particular including empty answersets, that true/false questions are a useful device to increase precision onsuggestions generated by the LM, that the size of the LM is a crucial factor,and that a dictionary of entity aliases improves the LM score. Our evaluationstudy indicates that these proposed techniques can substantially enhance thequality of the final predictions: ProP won track 2 of the LM-KBC competition,outperforming the baseline by 36.4 percentage points. Our implementation isavailable on https://github.com/HEmile/iswc-challenge."
Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors,"['Mohammad Reza Taesiri', 'Finlay Macklon', 'Yihe Wang', 'Hengshuo Shen', 'Cor-Paul Bezemer']",http://arxiv.org/pdf/2210.02506v1.pdf,2022-10-05,"['cs.cl', 'cs.se']","  Video game testing requires game-specific knowledge as well as common sensereasoning about the events in the game. While AI-driven agents can satisfy thefirst requirement, it is not yet possible to meet the second requirementautomatically. Therefore, video game testing often still relies on manualtesting, and human testers are required to play the game thoroughly to detectbugs. As a result, it is challenging to fully automate game testing. In thisstudy, we explore the possibility of leveraging the zero-shot capabilities oflarge language models for video game bug detection. By formulating the bugdetection problem as a question-answering task, we show that large languagemodels can identify which event is buggy in a sequence of textual descriptionsof events from a game. To this end, we introduce the GameBugDescriptionsbenchmark dataset, which consists of 167 buggy gameplay videos and a total of334 question-answer pairs across 8 games. We extensively evaluate theperformance of six models across the OPT and InstructGPT large language modelfamilies on our benchmark dataset. Our results show promising results foremploying language models to detect video game bugs. With the proper promptingtechnique, we could achieve an accuracy of 70.66%, and on some video games, upto 78.94%. Our code, evaluation data and the benchmark can be found onhttps://asgaardlab.github.io/LLMxBugs"
Boosting Low-Data Instance Segmentation by Unsupervised Pre-training  with Saliency Prompt,"['Hao Li', 'Dingwen Zhang', 'Nian Liu', 'Lechao Cheng', 'Yalun Dai', 'Chao Zhang', 'Xinggang Wang', 'Junwei Han']",http://arxiv.org/pdf/2302.01171v1.pdf,2023-02-02,"['cs.cv', 'cs.ai']","  Recently, inspired by DETR variants, query-based end-to-end instancesegmentation (QEIS) methods have outperformed CNN-based models on large-scaledatasets. Yet they would lose efficacy when only a small amount of trainingdata is available since it's hard for the crucial queries/kernels to learnlocalization and shape priors. To this end, this work offers a novelunsupervised pre-training solution for low-data regimes. Inspired by the recentsuccess of the Prompting technique, we introduce a new pre-training method thatboosts QEIS models by giving Saliency Prompt for queries/kernels. Our methodcontains three parts: 1) Saliency Masks Proposal is responsible for generatingpseudo masks from unlabeled images based on the saliency mechanism. 2)Prompt-Kernel Matching transfers pseudo masks into prompts and injects thecorresponding localization and shape priors to the best-matched kernels. 3)Kernel Supervision is applied to supply supervision at the kernel level forrobust learning. From a practical perspective, our pre-training method helpsQEIS models achieve a similar convergence speed and comparable performance withCNN-based models in low-data regimes. Experimental results show that our methodsignificantly boosts several QEIS models on three datasets. Code will be madeavailable."
One-Shot Labeling for Automatic Relevance Estimation,"['Sean MacAvaney', 'Luca Soldaini']",http://arxiv.org/pdf/2302.11266v2.pdf,2023-02-22,['cs.ir'],"  Dealing with unjudged documents (""holes"") in relevance assessments is aperennial problem when evaluating search systems with offline experiments.Holes can reduce the apparent effectiveness of retrieval systems duringevaluation and introduce biases in models trained with incomplete data. In thiswork, we explore whether large language models can help us fill such holes toimprove offline evaluations. We examine an extreme, albeit common, evaluationsetting wherein only a single known relevant document per query is availablefor evaluation. We then explore various approaches for predicting the relevanceof unjudged documents with respect to a query and the known relevant document,including nearest neighbor, supervised, and prompting techniques. We find thatalthough the predictions of these One-Shot Labelers (1SL) frequently disagreewith human assessments, the labels they produce yield a far more reliableranking of systems than the single labels do alone. Specifically, the strongestapproaches can consistently reach system ranking correlations of over 0.86 withthe full rankings over a variety of measures. Meanwhile, the approachsubstantially increases the reliability of t-tests due to filling holes inrelevance assessments, giving researchers more confidence in results they findto be significant. Alongside this work, we release an easy-to-use softwarepackage to enable the use of 1SL for evaluation of other ad-hoc collections orsystems."
Are Large Language Models Ready for Healthcare? A Comparative Study on  Clinical Language Understanding,"['Yuqing Wang', 'Yun Zhao', 'Linda Petzold']",http://arxiv.org/pdf/2304.05368v3.pdf,2023-04-09,"['cs.cl', 'cs.ai']","  Large language models (LLMs) have made significant progress in variousdomains, including healthcare. However, the specialized nature of clinicallanguage understanding tasks presents unique challenges and limitations thatwarrant further investigation. In this study, we conduct a comprehensiveevaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, withinthe realm of clinical language understanding tasks. These tasks span a diverserange, including named entity recognition, relation extraction, naturallanguage inference, semantic textual similarity, document classification, andquestion-answering. We also introduce a novel prompting strategy,self-questioning prompting (SQP), tailored to enhance LLMs' performance byeliciting informative questions and answers pertinent to the clinical scenariosat hand. Our evaluation underscores the significance of task-specific learningstrategies and prompting techniques for improving LLMs' effectiveness inhealthcare-related tasks. Additionally, our in-depth error analysis on thechallenging relation extraction task offers valuable insights into errordistribution and potential avenues for improvement using SQP. Our study shedslight on the practical implications of employing LLMs in the specialized domainof healthcare, serving as a foundation for future research and the developmentof potential applications in healthcare settings."
Multi-Prompt with Depth Partitioned Cross-Modal Learning,"['Yingjie Tian', 'Yiqi Wang', 'Xianda Guo', 'Zheng Zhu', 'Long Chen']",http://arxiv.org/pdf/2305.06221v3.pdf,2023-05-10,"['cs.cv', 'cs.ai']","  In recent years, soft prompt learning methods have been proposed to fine-tunelarge-scale vision-language pre-trained models for various downstream tasks.These methods typically combine learnable textual tokens with class tokens asinput for models with frozen parameters. However, they often employ a singleprompt to describe class contexts, failing to capture categories' diverseattributes adequately. This study introduces the Partitioned Multi-modal Prompt(PMPO), a multi-modal prompting technique that extends the soft prompt from asingle learnable prompt to multiple prompts. Our method divides the visualencoder depths and connects learnable prompts to the separated visual depths,enabling different prompts to capture the hierarchical contextual depths ofvisual representations. Furthermore, to maximize the advantages of multi-promptlearning, we incorporate prior information from manually designed templates andlearnable multi-prompts, thus improving the generalization capabilities of ourapproach. We evaluate the effectiveness of our approach on three challengingtasks: new class generalization, cross-dataset evaluation, and domaingeneralization. For instance, our method achieves a $79.28$ harmonic mean,averaged over 11 diverse image recognition datasets ($+7.62$ compared to CoOp),demonstrating significant competitiveness compared to state-of-the-artprompting methods."
ONCE: Boosting Content-based Recommendation with Both Open- and  Closed-source Large Language Models,"['Qijiong Liu', 'Nuo Chen', 'Tetsuya Sakai', 'Xiao-Ming Wu']",http://arxiv.org/pdf/2305.06566v4.pdf,2023-05-11,"['cs.ir', 'cs.cl']","  Personalized content-based recommender systems have become indispensabletools for users to navigate through the vast amount of content available onplatforms like daily news websites and book recommendation services. However,existing recommenders face significant challenges in understanding the contentof items. Large language models (LLMs), which possess deep semanticcomprehension and extensive knowledge from pretraining, have proven to beeffective in various natural language processing tasks. In this study, weexplore the potential of leveraging both open- and closed-source LLMs toenhance content-based recommendation. With open-source LLMs, we utilize theirdeep layers as content encoders, enriching the representation of content at theembedding level. For closed-source LLMs, we employ prompting techniques toenrich the training data at the token level. Through comprehensive experiments,we demonstrate the high effectiveness of both types of LLMs and show thesynergistic relationship between them. Notably, we observed a significantrelative improvement of up to 19.32% compared to existing state-of-the-artrecommendation models. These findings highlight the immense potential of bothopen- and closed-source of LLMs in enhancing content-based recommendationsystems. We will make our code and LLM-generated data available for otherresearchers to reproduce our results."
OPT-R: Exploring the Role of Explanations in Finetuning and Prompting  for Reasoning Skills of Large Language Models,"['Badr AlKhamissi', 'Siddharth Verma', 'Ping Yu', 'Zhijing Jin', 'Asli Celikyilmaz', 'Mona Diab']",http://arxiv.org/pdf/2305.12001v2.pdf,2023-05-19,['cs.cl'],"  In this paper, we conduct a thorough investigation into the reasoningcapabilities of Large Language Models (LLMs), focusing specifically on the OpenPretrained Transformers (OPT) models as a representative of such models. Ourstudy entails finetuning three different sizes of OPT on a carefully curatedreasoning corpus, resulting in two sets of finetuned models: OPT-R, finetunedwithout explanations, and OPT-RE, finetuned with explanations. We then evaluateall models on 57 out-of-domain tasks drawn from the SUPER-NATURALINSTRUCTIONSbenchmark, covering 26 distinct reasoning skills, utilizing three promptingtechniques. Through a comprehensive grid of 27 configurations and 6,156 testevaluations, we investigate the dimensions of finetuning, prompting, and scaleto understand the role of explanations on different reasoning skills. Ourfindings reveal that having explanations in the fewshot exemplar has nosignificant impact on the model's performance when the model is finetuned,while positively affecting the non-finetuned counterpart. Moreover, we observea slight yet consistent increase in classification accuracy as we incorporateexplanations during prompting and finetuning, respectively. Finally, we offerinsights on which skills benefit the most from incorporating explanationsduring finetuning and prompting, such as Numerical (+20.4%) and Analogical(+13.9%) reasoning, as well as skills that exhibit negligible or negativeeffects."
The Utility of Large Language Models and Generative AI for Education  Research,"['Andrew Katz', 'Umair Shakir', 'Ben Chambers']",http://arxiv.org/pdf/2305.18125v1.pdf,2023-05-29,['cs.hc'],"  The use of natural language processing (NLP) techniques in engineeringeducation can provide valuable insights into the underlying processes involvedin generating text. While accessing these insights can be labor-intensive ifdone manually, recent advances in NLP and large language models have made it arealistic option for individuals. This study explores and evaluates acombination of clustering, summarization, and prompting techniques to analyzeover 1,000 student essays in which students discussed their career interests.The specific assignment prompted students to define and explain their careergoals as engineers. Using text embedding representations of student responses,we clustered the responses together to identify thematically similar statementsfrom students. The clustered responses were then summarized to quickly identifycareer interest themes. We also used a set of a priori codes about careersatisfaction and sectors to demonstrate an alternative approach to using thesegenerative text models to analyze student writing. The results of this studydemonstrate the feasibility and usefulness of NLP techniques in engineeringeducation research. By automating the initial analysis of student essays,researchers and educators can more efficiently and accurately identify keythemes and patterns in student writing. The methods presented in this paperhave broader applications for engineering education and research purposesbeyond analyzing student essays. By explaining these methods to the engineeringeducation community, readers can utilize them in their own contexts."
The FormAI Dataset: Generative AI in Software Security Through the Lens  of Formal Verification,"['Norbert Tihanyi', 'Tamas Bisztray', 'Ridhi Jain', 'Mohamed Amine Ferrag', 'Lucas C. Cordeiro', 'Vasileios Mavroeidis']",http://arxiv.org/pdf/2307.02192v2.pdf,2023-07-05,"['cs.db', 'cs.ai']","  This paper presents the FormAI dataset, a large collection of 112, 000AI-generated compilable and independent C programs with vulnerabilityclassification. We introduce a dynamic zero-shot prompting techniqueconstructed to spawn diverse programs utilizing Large Language Models (LLMs).The dataset is generated by GPT-3.5-turbo and comprises programs with varyinglevels of complexity. Some programs handle complicated tasks like networkmanagement, table games, or encryption, while others deal with simpler taskslike string manipulation. Every program is labeled with the vulnerabilitiesfound within the source code, indicating the type, line number, and vulnerablefunction name. This is accomplished by employing a formal verification methodusing the Efficient SMT-based Bounded Model Checker (ESBMC), which uses modelchecking, abstract interpretation, constraint programming, and satisfiabilitymodulo theories to reason over safety/security properties in programs. Thisapproach definitively detects vulnerabilities and offers a formal model knownas a counterexample, thus eliminating the possibility of generating falsepositive reports. We have associated the identified vulnerabilities with CommonWeakness Enumeration (CWE) numbers. We make the source code available for the112, 000 programs, accompanied by a separate file containing thevulnerabilities detected in each program, making the dataset ideal for trainingLLMs and machine learning algorithms. Our study unveiled that according toESBMC, 51.24% of the programs generated by GPT-3.5 contained vulnerabilities,thereby presenting considerable risks to software safety and security."
SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering  Dataset for Scientific Graphs,"['Shengzhi Li', 'Nima Tajbakhsh']",http://arxiv.org/pdf/2308.03349v1.pdf,2023-08-07,"['cs.cl', 'cs.ai', 'cs.cv']","  In this work, we present SciGraphQA, a synthetic multi-turn question-answerdataset related to academic graphs. SciGraphQA is 13 times larger thanChartVQA, the previously largest chart-visual question-answering dataset. It isalso the largest open-sourced chart VQA dataset with non-synthetic charts. Tobuild our dataset, we selected 290,000 Computer Science or Machine LearningArXiv papers published between 2010 and 2020, and then used Palm-2 to generate295K samples of open-vocabulary multi-turn question-answering dialogues aboutthe graphs. As context, we provided the text-only Palm-2 with paper title,abstract, paragraph mentioning the graph, and rich text contextual data fromthe graph itself, obtaining dialogues with an average 2.23 question-answerturns for each graph. We asked GPT-4 to assess the matching quality of ourquestion-answer turns given the paper's context, obtaining an average rating of8.7/10 on our 3K test set. We evaluated the 0-shot capability of the mostpopular MLLM models such as LLaVa, mPLUGowl, BLIP-2, and openFlamingo's on ourdataset, finding LLaVA-13B being the most performant with a CIDEr score of0.08. We further enriched the question prompts for LLAVA by including theserialized data tables extracted from the graphs using the DePlot model,boosting LLaVA's 0-shot CIDEr to 0.15. To verify the validity of our dataset,we also fine-tuned LLaVa using our dataset, reaching a substantially higherCIDEr score of 0.26. We anticipate further accuracy improvement by includingsegmentation mask tokens and leveraging larger LLM backbones coupled withemergent prompting techniques. Our code and data are open-sourced."
GOPro: Generate and Optimize Prompts in CLIP using Self-Supervised  Learning,"['Mainak Singha', 'Ankit Jha', 'Biplab Banerjee']",http://arxiv.org/pdf/2308.11605v1.pdf,2023-08-22,['cs.cv'],"  Large-scale foundation models, such as CLIP, have demonstrated remarkablesuccess in visual recognition tasks by embedding images in a semantically richspace. Self-supervised learning (SSL) has also shown promise in improvingvisual recognition by learning invariant features. However, the combination ofCLIP with SSL is found to face challenges due to the multi-task framework thatblends CLIP's contrastive loss and SSL's loss, including difficulties with lossweighting and inconsistency among different views of images in CLIP's outputspace. To overcome these challenges, we propose a prompt learning-based modelcalled GOPro, which is a unified framework that ensures similarity betweenvarious augmented views of input images in a shared image-text embedding space,using a pair of learnable image and text projectors atop CLIP, to promoteinvariance and generalizability. To automatically learn such prompts, weleverage the visual content and style primitives extracted from pre-trainedCLIP and adapt them to the target task. In addition to CLIP's cross-domaincontrastive loss, we introduce a visual contrastive loss and a novel promptconsistency loss, considering the different views of the images. GOPro istrained end-to-end on all three loss objectives, combining the strengths ofCLIP and SSL in a principled manner. Empirical evaluations demonstrate thatGOPro outperforms the state-of-the-art prompting techniques on threechallenging domain generalization tasks across multiple benchmarks by asignificant margin. Our code is available athttps://github.com/mainaksingha01/GOPro."
Spoken Language Intelligence of Large Language Models for Language  Learning,"['Linkai Peng', 'Baorian Nuchged', 'Yingming Gao']",http://arxiv.org/pdf/2308.14536v1.pdf,2023-08-28,"['cs.cl', 'cs.ai', 'cs.lg', 'cs.sd', 'eess.as']","  People have long hoped for a conversational system that can assist inreal-life situations, and recent progress on large language models (LLMs) isbringing this idea closer to reality. While LLMs are often impressive inperformance, their efficacy in real-world scenarios that demand expertknowledge remains unclear. LLMs are believed to hold the most potential andvalue in education, especially in the development of Artificial intelligence(AI) based virtual teachers capable of facilitating language learning. Ourfocus is centered on evaluating the efficacy of LLMs in the realm of education,specifically in the areas of spoken language learning which encompassphonetics, phonology, and second language acquisition. We introduce a newmultiple-choice question dataset to evaluate the effectiveness of LLMs in theaforementioned scenarios, including understanding and application of spokenlanguage knowledge. In addition, we investigate the influence of variousprompting techniques such as zero- and few-shot method (prepending the questionwith question-answer exemplars), chain-of-thought (CoT, think step-by-step),in-domain exampler and external tools (Google, Wikipedia). We conductedlarge-scale evaluation on popular LLMs (20 distinct models) using thesemethods. We achieved significant performance improvements compared to thezero-shot baseline in the practical questions reasoning (GPT-3.5, 49.1% ->63.1%; LLaMA2-70B-Chat, 42.2% -> 48.6%). We found that models of differentsizes have good understanding of concepts in phonetics, phonology, and secondlanguage acquisition, but show limitations in reasoning for real-worldproblems. Additionally, we also explore preliminary findings on conversationalcommunication."
Are Emergent Abilities in Large Language Models just In-Context  Learning?,"['Sheng Lu', 'Irina Bigoulaeva', 'Rachneet Sachdeva', 'Harish Tayyar Madabushi', 'Iryna Gurevych']",http://arxiv.org/pdf/2309.01809v1.pdf,2023-09-04,['cs.cl'],"  Large language models have exhibited emergent abilities, demonstratingexceptional performance across diverse tasks for which they were not explicitlytrained, including those that require complex reasoning abilities. Theemergence of such abilities carries profound implications for the futuredirection of research in NLP, especially as the deployment of such modelsbecomes more prevalent. However, one key challenge is that the evaluation ofthese abilities is often confounded by competencies that arise in modelsthrough alternative prompting techniques, such as in-context learning andinstruction following, which also emerge as the models are scaled up. In thisstudy, we provide the first comprehensive examination of these emergentabilities while accounting for various potentially biasing factors that caninfluence the evaluation of models. We conduct rigorous tests on a set of 18models, encompassing a parameter range from 60 million to 175 billionparameters, across a comprehensive set of 22 tasks. Through an extensive seriesof over 1,000 experiments, we provide compelling evidence that emergentabilities can primarily be ascribed to in-context learning. We find no evidencefor the emergence of reasoning abilities, thus providing valuable insights intothe underlying mechanisms driving the observed abilities and thus alleviatingsafety concerns regarding their use."
Unsupervised Contrast-Consistent Ranking with Language Models,"['Niklas Stoehr', 'Pengxiang Cheng', 'Jing Wang', 'Daniel Preotiuc-Pietro', 'Rajarshi Bhowmik']",http://arxiv.org/pdf/2309.06991v1.pdf,2023-09-13,"['cs.lg', 'cs.cl', 'stat.ml']","  Language models contain ranking-based knowledge and are powerful solvers ofin-context ranking tasks. For instance, they may have parametric knowledgeabout the ordering of countries by size or may be able to rank reviews bysentiment. Recent work focuses on pairwise, pointwise, and listwise promptingtechniques to elicit a language model's ranking knowledge. However, we findthat even with careful calibration and constrained decoding, prompting-basedtechniques may not always be self-consistent in the rankings they produce. Thismotivates us to explore an alternative approach that is inspired by anunsupervised probing method called Contrast-Consistent Search (CCS). The ideais to train a probing model guided by a logical constraint: a model'srepresentation of a statement and its negation must be mapped to contrastivetrue-false poles consistently across multiple statements. We hypothesize thatsimilar constraints apply to ranking tasks where all items are related viaconsistent pairwise or listwise comparisons. To this end, we extend the binaryCCS method to Contrast-Consistent Ranking (CCR) by adapting existing rankingmethods such as the Max-Margin Loss, Triplet Loss, and Ordinal Regressionobjective. Our results confirm that, for the same language model, CCR probingoutperforms prompting and even performs on a par with prompting much largerlanguage models."
S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking  in the Era of LLMs,"['Sarkar Snigdha Sarathi Das', 'Chirag Shah', 'Mengting Wan', 'Jennifer Neville', 'Longqi Yang', 'Reid Andersen', 'Georg Buscher', 'Tara Safavi']",http://arxiv.org/pdf/2309.08827v1.pdf,2023-09-16,"['cs.cl', 'cs.ai']","  The traditional Dialogue State Tracking (DST) problem aims to track userpreferences and intents in user-agent conversations. While sufficient fortask-oriented dialogue systems supporting narrow domain applications, theadvent of Large Language Model (LLM)-based chat systems has introduced manyreal-world intricacies in open-domain dialogues. These intricacies manifest inthe form of increased complexity in contextual interactions, extended dialoguesessions encompassing a diverse array of topics, and more frequent contextualshifts. To handle these intricacies arising from evolving LLM-based chatsystems, we propose joint dialogue segmentation and state tracking per segmentin open-domain dialogue systems. Assuming a zero-shot setting appropriate to atrue open-domain dialogue system, we propose S3-DST, a structured promptingtechnique that harnesses Pre-Analytical Recollection, a novel groundingmechanism we designed for improving long context tracking. To demonstrate theefficacy of our proposed approach in joint segmentation and state tracking, weevaluate S3-DST on a proprietary anonymized open-domain dialogue dataset, aswell as publicly available DST and segmentation datasets. Across all datasetsand settings, S3-DST consistently outperforms the state-of-the-art,demonstrating its potency and robustness the next generation of LLM-based chatsystems."
Scalable Multi-Robot Collaboration with Large Language Models:  Centralized or Decentralized Systems?,"['Yongchao Chen', 'Jacob Arkin', 'Yang Zhang', 'Nicholas Roy', 'Chuchu Fan']",http://arxiv.org/pdf/2309.15943v1.pdf,2023-09-27,['cs.ro'],"  A flurry of recent work has demonstrated that pre-trained large languagemodels (LLMs) can be effective task planners for a variety of single-robottasks. The planning performance of LLMs is significantly improved via promptingtechniques, such as in-context learning or re-prompting with state feedback,placing new importance on the token budget for the context window. Anunder-explored but natural next direction is to investigate LLMs as multi-robottask planners. However, long-horizon, heterogeneous multi-robot planningintroduces new challenges of coordination while also pushing up against thelimits of context window length. It is therefore critical to findtoken-efficient LLM planning frameworks that are also able to reason about thecomplexities of multi-robot coordination. In this work, we compare the tasksuccess rate and token efficiency of four multi-agent communication frameworks(centralized, decentralized, and two hybrid) as applied to fourcoordination-dependent multi-agent 2D task scenarios for increasing numbers ofagents. We find that a hybrid framework achieves better task success ratesacross all four tasks and scales better to more agents. We further demonstratethe hybrid frameworks in 3D simulations where the vision-to-text problem anddynamical errors are considered. See our project websitehttps://yongchao98.github.io/MIT-REALM-Multi-Robot/ for prompts, videos, andcode."
Adaptive-Solver Framework for Dynamic Strategy Selection in Large  Language Model Reasoning,"['Jianpeng Zhou', 'Wanjun Zhong', 'Yanlin Wang', 'Jiahai Wang']",http://arxiv.org/pdf/2310.01446v1.pdf,2023-10-01,"['cs.cl', 'cs.ai']","  Large Language Models (LLMs) are showcasing impressive ability in handlingcomplex reasoning tasks. In real-world situations, problems often span aspectrum of complexities. Humans inherently adjust their problem-solvingapproaches based on task complexity. However, most methodologies that leverageLLMs tend to adopt a uniform approach: utilizing consistent models, promptingmethods, and degrees of problem decomposition, regardless of the problemcomplexity. Inflexibility of them can bring unnecessary computational overheador sub-optimal performance. To address this problem, we introduce anAdaptive-Solver framework. It strategically modulates solving strategies basedon the difficulties of the problems. Given an initial solution, the frameworkfunctions with two primary modules. The initial evaluation module assesses theadequacy of the current solution. If improvements are needed, the subsequentadaptation module comes into play. Within this module, three key adaptationstrategies are employed: (1) Model Adaptation: Switching to a stronger LLM whena weaker variant is inadequate. (2) Prompting Method Adaptation: Alternatingbetween different prompting techniques to suit the problem's nuances. (3)Decomposition Granularity Adaptation: Breaking down a complex problem into morefine-grained sub-questions to enhance solvability. Through such dynamicadaptations, our framework not only enhances computational efficiency but alsoelevates the overall performance. This dual-benefit ensures both the efficiencyof the system for simpler tasks and the precision required for more complexquestions. Experimental results from complex reasoning tasks reveal that theprompting method adaptation and decomposition granularity adaptation enhanceperformance across all tasks. Furthermore, the model adaptation approachsignificantly reduces API costs (up to 50%) while maintaining superiorperformance."
Revisiting Large Language Models as Zero-shot Relation Extractors,"['Guozheng Li', 'Peng Wang', 'Wenjun Ke']",http://arxiv.org/pdf/2310.05028v4.pdf,2023-10-08,"['cs.ai', 'cs.cl']","  Relation extraction (RE) consistently involves a certain degree of labeled orunlabeled data even if under zero-shot setting. Recent studies have shown thatlarge language models (LLMs) transfer well to new tasks out-of-the-box simplygiven a natural language prompt, which provides the possibility of extractingrelations from text without any data and parameter tuning. This work focuses onthe study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.On the one hand, we analyze the drawbacks of existing RE prompts and attempt toincorporate recent prompt techniques such as chain-of-thought (CoT) to improvezero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, asimple prompt recursively using LLMs to transform RE inputs to the effectivequestion answering (QA) format. On the other hand, we conduct comprehensiveexperiments on various benchmarks and settings to investigate the capabilitiesof LLMs on zero-shot RE. Specifically, we have the following findings: (i)\textsc{SumAsk} consistently and significantly improves LLMs performance ondifferent model sizes, benchmarks and settings; (ii) Zero-shot prompting withChatGPT achieves competitive or superior results compared with zero-shot andfully supervised methods; (iii) LLMs deliver promising performance inextracting overlapping relations; (iv) The performance varies greatly regardingdifferent relations. Different from small language models, LLMs are effectivein handling challenge none-of-the-above (NoTA) relation."
Towards Training-free Open-world Segmentation via Image Prompting  Foundation Models,"['Lv Tang', 'Peng-Tao Jiang', 'Hao-Ke Xiao', 'Bo Li']",http://arxiv.org/pdf/2310.10912v1.pdf,2023-10-17,['cs.cv'],"  The realm of computer vision has witnessed a paradigm shift with the adventof foundational models, mirroring the transformative influence of largelanguage models in the domain of natural language processing. This paper delvesinto the exploration of open-world segmentation, presenting a novel approachcalled Image Prompt Segmentation (IPSeg) that harnesses the power of visionfoundational models. At the heart of IPSeg lies the principle of atraining-free paradigm, which capitalizes on image prompting techniques. IPSegutilizes a single image containing a subjective visual concept as a flexibleprompt to query vision foundation models like DINOv2 and Stable Diffusion. Ourapproach extracts robust features for the prompt image and input image, thenmatches the input representations to the prompt representations via a novelfeature interaction module to generate point prompts highlighting targetobjects in the input image. The generated point prompts are further utilized toguide the Segment Anything Model to segment the target object in the inputimage. The proposed method stands out by eliminating the need for exhaustivetraining sessions, thereby offering a more efficient and scalable solution.Experiments on COCO, PASCAL VOC, and other datasets demonstrate IPSeg'sefficacy for flexible open-world segmentation using intuitive image prompts.This work pioneers tapping foundation models for open-world understandingthrough visual concepts conveyed in images."
Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning  across Languages,"['Libo Qin', 'Qiguang Chen', 'Fuxuan Wei', 'Shijue Huang', 'Wanxiang Che']",http://arxiv.org/pdf/2310.14799v1.pdf,2023-10-23,"['cs.cl', 'cs.ai']","  Chain-of-thought (CoT) is capable of eliciting models to explicitly generatereasoning paths, thus promoting reasoning accuracy and attracting increasingattention. Specifically, zero-shot CoT achieves remarkable improvements in awide range of reasoning tasks by simply instructing the LLM with the prompt""Let's think step by step!"". Despite the success of zero-shot CoT, the existingzero-shot prompting techniques remain limited to a single language, making itchallenging to generalize to other languages and hindering global development.In this work, we introduce cross-lingual prompting (CLP), aiming to improvezero-shot CoT reasoning across languages. Specifically, CLP consists of twomain components: (1) cross-lingual alignment prompting and (2) task-specificsolver prompting. The cross-lingual alignment prompting is responsible foraligning representations across different languages, whereas the task-specificsolver prompting is used to generate the final chain of thoughts and resultsfor the reasoning task. In addition, we further introduce cross-lingualself-consistent prompting (CLSP) to ensemble different reasoning paths acrosslanguages. Our experimental evaluations on several benchmarks demonstrate thatCLP and CLSP significantly outperform the existing prompting methods andachieve state-of-the-art performance. We hope this work will inspire furtherbreakthroughs in cross-lingual CoT."
HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained  Heterogeneous Graph Neural Networks,"['Yihong Ma', 'Ning Yan', 'Jiayu Li', 'Masood Mortazavi', 'Nitesh V. Chawla']",http://arxiv.org/pdf/2310.15318v1.pdf,2023-10-23,"['cs.lg', 'cs.ai']","  Graphs have emerged as a natural choice to represent and analyze theintricate patterns and rich information of the Web, enabling applications suchas online page classification and social recommendation. The prevailing""pre-train, fine-tune"" paradigm has been widely adopted in graph machinelearning tasks, particularly in scenarios with limited labeled nodes. However,this approach often exhibits a misalignment between the training objectives ofpretext tasks and those of downstream tasks. This gap can result in the""negative transfer"" problem, wherein the knowledge gained from pre-trainingadversely affects performance in the downstream tasks. The surge inprompt-based learning within Natural Language Processing (NLP) suggests thepotential of adapting a ""pre-train, prompt"" paradigm to graphs as analternative. However, existing graph prompting techniques are tailored tohomogeneous graphs, neglecting the inherent heterogeneity of Web graphs. Tobridge this gap, we propose HetGPT, a general post-training prompting frameworkto improve the predictive performance of pre-trained heterogeneous graph neuralnetworks (HGNNs). The key is the design of a novel prompting function thatintegrates a virtual class prompt and a heterogeneous feature prompt, with theaim to reformulate downstream tasks to mirror pretext tasks. Moreover, HetGPTintroduces a multi-view neighborhood aggregation mechanism, capturing thecomplex neighborhood structure in heterogeneous graphs. Extensive experimentson three benchmark datasets demonstrate HetGPT's capability to enhance theperformance of state-of-the-art HGNNs on semi-supervised node classification."
Videoprompter: an ensemble of foundational models for zero-shot video  understanding,"['Adeel Yousaf', 'Muzammal Naseer', 'Salman Khan', 'Fahad Shahbaz Khan', 'Mubarak Shah']",http://arxiv.org/pdf/2310.15324v1.pdf,2023-10-23,['cs.cv'],"  Vision-language models (VLMs) classify the query video by calculating asimilarity score between the visual features and text-based class labelrepresentations. Recently, large language models (LLMs) have been used toenrich the text-based class labels by enhancing the descriptiveness of theclass names. However, these improvements are restricted to the text-basedclassifier only, and the query visual features are not considered. In thispaper, we propose a framework which combines pre-trained discriminative VLMswith pre-trained generative video-to-text and text-to-text models. We introducetwo key modifications to the standard zero-shot setting. First, we proposelanguage-guided visual feature enhancement and employ a video-to-text model toconvert the query video to its descriptive form. The resulting descriptionscontain vital visual cues of the query video, such as what objects are presentand their spatio-temporal interactions. These descriptive cues provideadditional semantic knowledge to VLMs to enhance their zeroshot performance.Second, we propose video-specific prompts to LLMs to generate more meaningfuldescriptions to enrich class label representations. Specifically, we introduceprompt techniques to create a Tree Hierarchy of Categories for class names,offering a higher-level action context for additional visual cues, Wedemonstrate the effectiveness of our approach in video understanding acrossthree different zero-shot settings: 1) video action recognition, 2)video-to-text and textto-video retrieval, and 3) time-sensitive video tasks.Consistent improvements across multiple benchmarks and with various VLMsdemonstrate the effectiveness of our proposed framework. Our code will be madepublicly available."
Improving Diversity of Demographic Representation in Large Language  Models via Collective-Critiques and Self-Voting,"['Preethi Lahoti', 'Nicholas Blumm', 'Xiao Ma', 'Raghavendra Kotikalapudi', 'Sahitya Potluri', 'Qijun Tan', 'Hansa Srinivasan', 'Ben Packer', 'Ahmad Beirami', 'Alex Beutel', 'Jilin Chen']",http://arxiv.org/pdf/2310.16523v1.pdf,2023-10-25,"['cs.cl', 'cs.ai']","  A crucial challenge for generative large language models (LLMs) is diversity:when a user's prompt is under-specified, models may follow implicit assumptionswhile generating a response, which may result in homogenization of theresponses, as well as certain demographic groups being under-represented oreven erased from the generated responses. In this paper, we formalize diversityof representation in generative LLMs. We present evaluation datasets andpropose metrics to measure diversity in generated responses along people andculture axes. We find that LLMs understand the notion of diversity, and thatthey can reason and critique their own responses for that goal. This findingmotivated a new prompting technique called collective-critique and self-voting(CCSV) to self-improve people diversity of LLMs by tapping into its diversityreasoning capabilities, without relying on handcrafted examples or prompttuning. Extensive empirical experiments with both human and automatedevaluations show that our proposed approach is effective at improving peopleand culture diversity, and outperforms all baseline methods by a large margin."
LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?,"['Zeyang Zhang', 'Xin Wang', 'Ziwei Zhang', 'Haoyang Li', 'Yijian Qin', 'Simin Wu', 'Wenwu Zhu']",http://arxiv.org/pdf/2310.17110v1.pdf,2023-10-26,['cs.lg'],"  In an era marked by the increasing adoption of Large Language Models (LLMs)for various tasks, there is a growing focus on exploring LLMs' capabilities inhandling web data, particularly graph data. Dynamic graphs, which capturetemporal network evolution patterns, are ubiquitous in real-world web data.Evaluating LLMs' competence in understanding spatial-temporal information ondynamic graphs is essential for their adoption in web applications, whichremains unexplored in the literature. In this paper, we bridge the gap viaproposing to evaluate LLMs' spatial-temporal understanding abilities on dynamicgraphs, to the best of our knowledge, for the first time. Specifically, wepropose the LLM4DyG benchmark, which includes nine specially designed tasksconsidering the capability evaluation of LLMs from both temporal and spatialdimensions. Then, we conduct extensive experiments to analyze the impacts ofdifferent data generators, data statistics, prompting techniques, and LLMs onthe model performance. Finally, we propose Disentangled Spatial-TemporalThoughts (DST2) for LLMs on dynamic graphs to enhance LLMs' spatial-temporalunderstanding abilities. Our main observations are: 1) LLMs have preliminaryspatial-temporal understanding abilities on dynamic graphs, 2) Dynamic graphtasks show increasing difficulties for LLMs as the graph size and densityincrease, while not sensitive to the time span and data generation mechanism,3) the proposed DST2 prompting method can help to improve LLMs'spatial-temporal understanding abilities on dynamic graphs for most tasks. Thedata and codes will be open-sourced at publication time."
Which is better? Exploring Prompting Strategy For LLM-based Metrics,"['Joonghoon Kim', 'Saeran Park', 'Kiyoon Jeong', 'Sangmin Lee', 'Seung Hun Han', 'Jiyoon Lee', 'Pilsung Kang']",http://arxiv.org/pdf/2311.03754v1.pdf,2023-11-07,['cs.cl'],"  This paper describes the DSBA submissions to the Prompting Large LanguageModels as Explainable Metrics shared task, where systems were submitted to twotracks: small and large summarization tracks. With advanced Large LanguageModels (LLMs) such as GPT-4, evaluating the quality of Natural LanguageGeneration (NLG) has become increasingly paramount. Traditionalsimilarity-based metrics such as BLEU and ROUGE have shown to misalign withhuman evaluation and are ill-suited for open-ended generation tasks. To addressthis issue, we explore the potential capability of LLM-based metrics,especially leveraging open-source LLMs. In this study, wide range of promptsand prompting techniques are systematically analyzed with three approaches:prompting strategy, score aggregation, and explainability. Our research focuseson formulating effective prompt templates, determining the granularity of NLGquality scores and assessing the impact of in-context examples on LLM-basedevaluation. Furthermore, three aggregation strategies are compared to identifythe most reliable method for aggregating NLG quality scores. To examineexplainability, we devise a strategy that generates rationales for the scoresand analyzes the characteristics of the explanation produced by the open-sourceLLMs. Extensive experiments provide insights regarding evaluation capabilitiesof open-source LLMs and suggest effective prompting strategies."
CompCodeVet: A Compiler-guided Validation and Enhancement Approach for  Code Dataset,"['Le Chen', 'Arijit Bhattacharjee', 'Nesreen K. Ahmed', 'Niranjan Hasabnis', 'Gal Oren', 'Bin Lei', 'Ali Jannesari']",http://arxiv.org/pdf/2311.06505v1.pdf,2023-11-11,['cs.lg'],"  Large language models (LLMs) have become increasingly prominent in academiaand industry due to their remarkable performance in diverse applications. Asthese models evolve with increasing parameters, they excel in tasks likesentiment analysis and machine translation. However, even models with billionsof parameters face challenges in tasks demanding multi-step reasoning. Codegeneration and comprehension, especially in C and C++, emerge as significantchallenges. While LLMs trained on code datasets demonstrate competence in manytasks, they struggle with rectifying non-compilable C and C++ code. Ourinvestigation attributes this subpar performance to two primary factors: thequality of the training dataset and the inherent complexity of the problemwhich demands intricate reasoning. Existing ""Chain of Thought"" (CoT) promptingtechniques aim to enhance multi-step reasoning. This approach, however, retainsthe limitations associated with the latent drawbacks of LLMs. In this work, wepropose CompCodeVet, a compiler-guided CoT approach to produce compilable codefrom non-compilable ones. Diverging from the conventional approach of utilizinglarger LLMs, we employ compilers as a teacher to establish a more robustzero-shot thought process. The evaluation of CompCodeVet on two open-sourcecode datasets shows that CompCodeVet has the ability to improve the trainingdataset quality for LLMs."
MacGyver: Are Large Language Models Creative Problem Solvers?,"['Yufei Tian', 'Abhilasha Ravichander', 'Lianhui Qin', 'Ronan Le Bras', 'Raja Marjieh', 'Nanyun Peng', 'Yejin Choi', 'Thomas L. Griffiths', 'Faeze Brahman']",http://arxiv.org/pdf/2311.09682v1.pdf,2023-11-16,"['cs.cl', 'cs.ai']","  We explore the creative problem-solving capabilities of modern large languagemodels (LLMs) in a constrained setting. The setting requires circumventing acognitive bias known in psychology as ''functional fixedness'' to use familiarobjects in innovative or unconventional ways. To this end, we create MacGyver,an automatically generated dataset consisting of 1,600 real-world problems thatdeliberately trigger functional fixedness and require thinking'out-of-the-box'. We then present our collection of problems to both LLMs andhumans to compare and contrast their problem-solving abilities. We show thatMacGyver is challenging for both groups, but in unique and complementary ways.For example, humans typically excel in solving problems that they are familiarwith but may struggle with tasks requiring domain-specific knowledge, leadingto a higher variance. On the other hand, LLMs, being exposed to a variety ofhighly specialized knowledge, attempt broader problems but are prone tooverconfidence and propose actions that are physically infeasible orinefficient. We also provide a detailed error analysis of LLMs, and demonstratethe potential of enhancing their problem-solving ability with novel promptingtechniques such as iterative step-wise reflection and divergent-convergentthinking. This work provides insight into the creative problem-solvingcapabilities of humans and AI and illustrates how psychological paradigms canbe extended into large-scale tasks for comparing humans and machines."
How Far Can We Extract Diverse Perspectives from Large Language Models?  Criteria-Based Diversity Prompting!,"['Shirley Anugrah Hayati', 'Minhwa Lee', 'Dheeraj Rajagopal', 'Dongyeop Kang']",http://arxiv.org/pdf/2311.09799v1.pdf,2023-11-16,['cs.cl'],"  Collecting diverse human data on subjective NLP topics is costly andchallenging. As Large Language Models (LLMs) have developed human-likecapabilities, there is a recent trend in collaborative efforts between humansand LLMs for generating diverse data, offering potential scalable and efficientsolutions. However, the extent of LLMs' capability to generate diverseperspectives on subjective topics remains an unexplored question. In thisstudy, we investigate LLMs' capacity for generating diverse perspectives andrationales on subjective topics, such as social norms and argumentative texts.We formulate this problem as diversity extraction in LLMs and propose acriteria-based prompting technique to ground diverse opinions and measureperspective diversity from the generated criteria words. Our results show thatmeasuring semantic diversity through sentence embeddings and distance metricsis not enough to measure perspective diversity. To see how far we can extractdiverse perspectives from LLMs, or called diversity coverage, we employ astep-by-step recall prompting for generating more outputs from the model in aniterative manner. As we apply our prompting method to other tasks (hate speechlabeling and story continuation), indeed we find that LLMs are able to generatediverse opinions according to the degree of task subjectivity."
Web News Timeline Generation with Extended Task Prompting,"['Sha Wang', 'Yuchen Li', 'Hanhua Xiao', 'Lambert Deng', 'Yanfei Dong']",http://arxiv.org/pdf/2311.11652v1.pdf,2023-11-20,['cs.ai'],"  The creation of news timeline is essential for a comprehensive and contextualunderstanding of events as they unfold over time. This approach aids indiscerning patterns and trends that might be obscured when news is viewed inisolation. By organizing news in a chronological sequence, it becomes easier totrack the development of stories, understand the interrelation of events, andgrasp the broader implications of news items. This is particularly helpful insectors like finance and insurance, where timely understanding of the eventdevelopment-ranging from extreme weather to political upheavals and healthcrises-is indispensable for effective risk management. While traditionalnatural language processing (NLP) techniques have had some success, they oftenfail to capture the news with nuanced relevance that are readily apparent todomain experts, hindering broader industry integration. The advance of LargeLanguage Models (LLMs) offers a renewed opportunity to tackle this challenge.However, direct prompting LLMs for this task is often ineffective. Our studyinvestigates the application of an extended task prompting technique to assesspast news relevance. We demonstrate that enhancing conventional prompts withadditional tasks boosts their effectiveness on various news dataset, renderingnews timeline generation practical for professional use. This work has beendeployed as a publicly accessible browser extension which is adopted within ournetwork."
Applications of Large Language Models in Data Processing: Innovative  Approaches to Segmenting and Renewing Information,"['Yu-Chen Lin', 'Akhilesh Kumar', 'Wen-Liang Zhang', 'Norman Chang', 'Muhammad Zakir', 'Rucha Apte', 'Chao Wang', 'Jyh-Shing Roger Jang']",http://arxiv.org/pdf/2311.16267v1.pdf,2023-11-27,"['cs.cl', 'cs.se']","  Our paper investigates effective methods for code generation in""specific-domain"" applications, including the use of Large Language Models(LLMs) for data segmentation and renewal, as well as stimulating deeperthinking in LLMs through prompt adjustments. Using a real company product as anexample, we provide user manuals, API documentation, and other data. The ideasdiscussed in this paper help segment and then convert this data into semanticvectors to better reflect their true positioning. Subsequently, userrequirements are transformed into vectors to retrieve the most relevantcontent, achieving about 70% accuracy in simple to medium-complexity tasksthrough various prompt techniques. This paper is the first to enhancespecific-domain code generation effectiveness from this perspective.Additionally, we experiment with generating more scripts from a limited numberusing llama2-based fine-tuning to test its effectiveness in professional domaincode generation. This is a challenging and promising field, and once achieved,it will not only lead to breakthroughs in LLM development across multipleindustries but also enable LLMs to understand and learn any new knowledgeeffectively."
Efficiently Programming Large Language Models using SGLang,"['Lianmin Zheng', 'Liangsheng Yin', 'Zhiqiang Xie', 'Jeff Huang', 'Chuyue Sun', 'Cody Hao Yu', 'Shiyi Cao', 'Christos Kozyrakis', 'Ion Stoica', 'Joseph E. Gonzalez', 'Clark Barrett', 'Ying Sheng']",http://arxiv.org/pdf/2312.07104v1.pdf,2023-12-12,"['cs.ai', 'cs.pl']","  Large language models (LLMs) are increasingly used for complex tasksrequiring multiple chained generation calls, advanced prompting techniques,control flow, and interaction with external environments. However, efficientsystems for programming and executing these applications are lacking. To bridgethis gap, we introduce SGLang, a Structured Generation Language for LLMs.SGLang is designed for the efficient programming of LLMs and incorporatesprimitives for common LLM programming patterns. We have implemented SGLang as adomain-specific language embedded in Python, and we developed an interpreter, acompiler, and a high-performance runtime for SGLang. These components worktogether to enable optimizations such as parallelism, batching, caching,sharing, and other compilation techniques. Additionally, we proposeRadixAttention, a novel technique that maintains a Least Recently Used (LRU)cache of the Key-Value (KV) cache for all requests in a radix tree, enablingautomatic KV cache reuse across multiple generation calls at runtime. SGLangsimplifies the writing of LLM programs and boosts execution efficiency. Ourexperiments demonstrate that SGLang can speed up common LLM tasks by up to 5x,while reducing code complexity and enhancing control."
Understanding and Improving Visual Prompting: A Label-Mapping  Perspective,"['Aochuan Chen', 'Yuguang Yao', 'Pin-Yu Chen', 'Yihua Zhang', 'Sijia Liu']",http://arxiv.org/pdf/2211.11635v5.pdf,2022-11-21,['cs.cv'],"  We revisit and advance visual prompting (VP), an input prompting techniquefor vision tasks. VP can reprogram a fixed, pre-trained source model toaccomplish downstream tasks in the target domain by simply incorporatinguniversal prompts (in terms of input perturbation patterns) into downstreamdata points. Yet, it remains elusive why VP stays effective even given aruleless label mapping (LM) between the source classes and the target classes.Inspired by the above, we ask: How is LM interrelated with VP? And how toexploit such a relationship to improve its accuracy on target tasks? We peerinto the influence of LM on VP and provide an affirmative answer that a better'quality' of LM (assessed by mapping precision and explanation) canconsistently improve the effectiveness of VP. This is in contrast to the priorart where the factor of LM was missing. To optimize LM, we propose a new VPframework, termed ILM-VP (iterative label mapping-based visual prompting),which automatically re-maps the source labels to the target labels andprogressively improves the target task accuracy of VP. Further, when using acontrastive language-image pretrained (CLIP) model, we propose to integrate anLM process to assist the text prompt selection of CLIP and to improve thetarget task accuracy. Extensive experiments demonstrate that our proposalsignificantly outperforms state-of-the-art VP methods. As highlighted below, weshow that when reprogramming an ImageNet-pretrained ResNet-18 to 13 targettasks, our method outperforms baselines by a substantial margin, e.g., 7.9% and6.7% accuracy improvements in transfer learning to the target Flowers102 andCIFAR100 datasets. Besides, our proposal on CLIP-based VP provides 13.7% and7.1% accuracy improvements on Flowers102 and DTD respectively. Our code isavailable at https://github.com/OPTML-Group/ILM-VP."
GPT-4 can pass the Korean National Licensing Examination for Korean  Medicine Doctors,"['Dongyeop Jang', 'Tae-Rim Yun', 'Choong-Yeol Lee', 'Young-Kyu Kwon', 'Chang-Eop Kim']",http://arxiv.org/pdf/2303.17807v2.pdf,2023-03-31,"['cs.cl', 'cs.lg', 'j.3']","  Traditional Korean medicine (TKM) emphasizes individualized diagnosis andtreatment. This uniqueness makes AI modeling difficult due to limited data andimplicit processes. Large language models (LLMs) have demonstrated impressivemedical inference, even without advanced training in medical texts. This studyassessed the capabilities of GPT-4 in TKM, using the Korean National LicensingExamination for Korean Medicine Doctors (K-NLEKMD) as a benchmark. TheK-NLEKMD, administered by a national organization, encompasses 12 majorsubjects in TKM. We optimized prompts with Chinese-term annotation, Englishtranslation for questions and instruction, exam-optimized instruction, andself-consistency. GPT-4 with optimized prompts achieved 66.18% accuracy,surpassing both the examination's average pass mark of 60% and the 40% minimumfor each subject. The gradual introduction of language-related prompts andprompting techniques enhanced the accuracy from 51.82% to its maximum accuracy.GPT-4 showed low accuracy in subjects including public health &medicine-related law, internal medicine (2) which are localized in Korea andTKM. The model's accuracy was lower for questions requiring TKM-specializedknowledge. It exhibited higher accuracy in diagnosis-based and recall-basedquestions than in intervention-based questions. A positive correlation wasobserved between the consistency and accuracy of GPT-4's responses. This studyunveils both the potential and challenges of applying LLMs to TKM. Thesefindings underline the potential of LLMs like GPT-4 in culturally adaptedmedicine, especially TKM, for tasks such as clinical assistance, medicaleducation, and research. But they also point towards the necessity for thedevelopment of methods to mitigate cultural bias inherent in large languagemodels and validate their efficacy in real-world clinical settings."
The Power of Large Language Models for Wireless Communication System  Development: A Case Study on FPGA Platforms,"['Yuyang Du', 'Soung Chang Liew', 'Kexin Chen', 'Yulin Shao']",http://arxiv.org/pdf/2307.07319v4.pdf,2023-07-14,['eess.sp'],"  Large language models (LLMs) have garnered significant attention acrossvarious research disciplines, including the wireless communication community.There have been several heated discussions on the intersection of LLMs andwireless technologies. While recent studies have demonstrated the ability ofLLMs to generate hardware description language (HDL) code for simplecomputation tasks, developing wireless prototypes and products via HDL posesfar greater challenges because of the more complex computation tasks involved.In this paper, we aim to address this challenge by investigating the role ofLLMs in FPGA-based hardware development for advanced wireless signalprocessing. We begin by exploring LLM-assisted code refactoring, reuse, andvalidation, using an open-source software-defined radio (SDR) project as a casestudy. Through the case study, we find that an LLM assistant can potentiallyyield substantial productivity gains for researchers and developers. We thenexamine the feasibility of using LLMs to generate HDL code for advancedwireless signal processing, using the Fast Fourier Transform (FFT) algorithm asan example. This task presents two unique challenges: the scheduling ofsubtasks within the overall task and the multi-step thinking required to solvecertain arithmetic problem within the task. To address these challenges, weemploy in-context learning (ICL) and Chain-of-Thought (CoT) promptingtechniques, culminating in the successful generation of a 64-point Verilog FFTmodule. Our results demonstrate the potential of LLMs for generalization andimitation, affirming their usefulness in writing HDL code for wirelesscommunication systems. Overall, this work contributes to understanding the roleof LLMs in wireless communication and motivates further exploration of theircapabilities."
Foundation Metrics: Quantifying Effectiveness of Healthcare  Conversations powered by Generative AI,"['Mahyar Abbasian', 'Elahe Khatibi', 'Iman Azimi', 'David Oniani', 'Zahra Shakeri Hossein Abad', 'Alexander Thieme', 'Ram Sriram', 'Zhongqi Yang', 'Yanshan Wang', 'Bryant Lin', 'Olivier Gevaert', 'Li-Jia Li', 'Ramesh Jain', 'Amir M. Rahmani']",http://arxiv.org/pdf/2309.12444v2.pdf,2023-09-21,['cs.cl'],"  Generative Artificial Intelligence is set to revolutionize healthcaredelivery by transforming traditional patient care into a more personalized,efficient, and proactive process. Chatbots, serving as interactiveconversational models, will probably drive this patient-centered transformationin healthcare. Through the provision of various services, including diagnosis,personalized lifestyle recommendations, and mental health support, theobjective is to substantially augment patient health outcomes, all the whilemitigating the workload burden on healthcare providers. The life-criticalnature of healthcare applications necessitates establishing a unified andcomprehensive set of evaluation metrics for conversational models. Existingevaluation metrics proposed for various generic large language models (LLMs)demonstrate a lack of comprehension regarding medical and health concepts andtheir significance in promoting patients' well-being. Moreover, these metricsneglect pivotal user-centered aspects, including trust-building, ethics,personalization, empathy, user comprehension, and emotional support. Thepurpose of this paper is to explore state-of-the-art LLM-based evaluationmetrics that are specifically applicable to the assessment of interactiveconversational models in healthcare. Subsequently, we present an comprehensiveset of evaluation metrics designed to thoroughly assess the performance ofhealthcare chatbots from an end-user perspective. These metrics encompass anevaluation of language processing abilities, impact on real-world clinicaltasks, and effectiveness in user-interactive conversations. Finally, we engagein a discussion concerning the challenges associated with defining andimplementing these metrics, with particular emphasis on confounding factorssuch as the target audience, evaluation methods, and prompt techniques involvedin the evaluation process."
Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward  Reasoning in Math Word Problems,"['Aniruddha Deb', 'Neeva Oza', 'Sarthak Singla', 'Dinesh Khandelwal', 'Dinesh Garg', 'Parag Singla']",http://arxiv.org/pdf/2310.01991v1.pdf,2023-10-03,"['cs.cl', 'cs.ai', 'cs.lg', 'i.2.3']","  While forward reasoning (i.e. find the answer given the question) has beenexplored extensively in the recent literature, backward reasoning is relativelyunexplored. We examine the backward reasoning capabilities of LLMs on Math WordProblems (MWPs): given a mathematical question and its answer, with somedetails omitted from the question, can LLMs effectively retrieve the missinginformation?  In this paper, we formally define the backward reasoning task on math wordproblems and modify three datasets to evaluate this task: GSM8k, SVAMP andMultiArith. Our findings show a significant drop in the accuracy of models onbackward reasoning compared to forward reasoning across four SOTA LLMs (GPT4,GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, wepropose three novel techniques that improve performance: Rephrase reformulatesthe given problem into a forward reasoning problem, PAL-Tools combines the ideaof Program-Aided LLMs to produce a set of equations that can be solved by anexternal solver, and Check your Work exploits the availability of naturalverifier of high accuracy in the forward direction, interleaving solving andverification steps. Finally, realizing that each of our base methods correctlysolves a different set of problems, we propose a novel Bayesian formulation forcreating an ensemble over these base methods aided by a verifier to furtherboost the accuracy by a significant margin. Extensive experimentationdemonstrates that our techniques successively improve the performance of LLMson the backward reasoning task, with the final ensemble-based method resultingin a substantial performance gain compared to the raw LLMs with standardprompting techniques such as chain-of-thought."
Autonomous Tree-search Ability of Large Language Models,"['Zheyu Zhang', 'Zhuorui Ye', 'Yikang Shen', 'Chuang Gan']",http://arxiv.org/pdf/2310.10686v1.pdf,2023-10-14,"['cs.cl', 'cs.ai']","  Large Language Models have excelled in remarkable reasoning capabilities withadvanced prompting techniques, but they fall short on tasks that requireexploration, strategic foresight, and sequential decision-making. Recent workspropose to utilize external programs to define search logic, such that LLMs canperform passive tree search to solve more challenging reasoning tasks. Thoughimpressive results have been achieved, there are several fundamentallimitations of these approaches. First, passive tree searches are not efficientas they usually require multiple rounds of LLM API calls to solve one singleproblem. Moreover, passive search methods are not flexible since they needtask-specific program designs. Then a natural question arises: can we maintainthe tree-search capability of LLMs without the aid of external programs, andcan still generate responses that clearly demonstrate the process of atree-structure search? To this end, we propose a new concept called autonomoustree-search ability of LLM, which can automatically generate a responsecontaining search trajectories for the correct answer. Concretely, we performsearch trajectories using capable LLM API via a fixed system prompt, allowingthem to perform autonomous tree-search (ATS) right out of the box. Experimentson 4 puzzle games demonstrate our method can achieve huge improvements. TheATS-BFS method outperforms the Chain of Thought approach by achieving anaverage accuracy improvement of 33%. Compared to Tree of Thoughts, it requires65.6% or 47.7% less GPT-api cost to attain a comparable level of accuracy.Moreover, we have collected data using the ATS prompt method and fine-tunedLLaMA. This approach yield a greater improvement compared to the onesfine-tuned on CoT data. Specifically, it outperforms CoT-tuned LLaMAs by anaverage of 40.6% and 38.5% for LLaMA2-7B and LLaMA2-13B, respectively."
On-the-Fly Fusion of Large Language Models and Machine Translation,"['Hieu Hoang', 'Huda Khayrallah', 'Marcin Junczys-Dowmunt']",http://arxiv.org/pdf/2311.08306v1.pdf,2023-11-14,['cs.cl'],"  We propose the on-the-fly ensembling of a machine translation model with anLLM, prompted on the same task and input. We perform experiments on 4 languagepairs (both directions) with varying data amounts. We find that a slightlyweaker-at-translation LLM can improve translations of a NMT model, andensembling with an LLM can produce better translations than ensembling twostronger MT models. We combine our method with various techniques from LLMprompting, such as in context learning and translation context."
In-Context Impersonation Reveals Large Language Models' Strengths and  Biases,"['Leonard Salewski', 'Stephan Alaniz', 'Isabel Rio-Torto', 'Eric Schulz', 'Zeynep Akata']",http://arxiv.org/pdf/2305.14930v2.pdf,2023-05-24,"['cs.ai', 'cs.cl', 'cs.lg']","  In everyday conversations, humans can take on different roles and adapt theirvocabulary to their chosen roles. We explore whether LLMs can take on, that isimpersonate, different roles when they generate text in-context. We ask LLMs toassume different personas before solving vision and language tasks. We do thisby prefixing the prompt with a persona that is associated either with a socialidentity or domain expertise. In a multi-armed bandit task, we find that LLMspretending to be children of different ages recover human-like developmentalstages of exploration. In a language-based reasoning task, we find that LLMsimpersonating domain experts perform better than LLMs impersonating non-domainexperts. Finally, we test whether LLMs' impersonations are complementary tovisual information when describing different categories. We find thatimpersonation can improve performance: an LLM prompted to be a bird expertdescribes birds better than one prompted to be a car expert. However,impersonation can also uncover LLMs' biases: an LLM prompted to be a mandescribes cars better than one prompted to be a woman. These findingsdemonstrate that LLMs are capable of taking on diverse roles and that thisin-context impersonation can be used to uncover their hidden strengths andbiases."
ROSGPT_Vision: Commanding Robots Using Only Language Models' Prompts,"['Bilel Benjdira', 'Anis Koubaa', 'Anas M. Ali']",http://arxiv.org/pdf/2308.11236v2.pdf,2023-08-22,"['cs.ro', 'cs.ai']","  In this paper, we argue that the next generation of robots can be commandedusing only Language Models' prompts. Every prompt interrogates separately aspecific Robotic Modality via its Modality Language Model (MLM). A central TaskModality mediates the whole communication to execute the robotic mission via aLarge Language Model (LLM). This paper gives this new robotic design patternthe name of: Prompting Robotic Modalities (PRM). Moreover, this paper appliesthis PRM design pattern in building a new robotic framework namedROSGPT_Vision. ROSGPT_Vision allows the execution of a robotic task using onlytwo prompts: a Visual and an LLM prompt. The Visual Prompt extracts, in naturallanguage, the visual semantic features related to the task under consideration(Visual Robotic Modality). Meanwhile, the LLM Prompt regulates the roboticreaction to the visual description (Task Modality). The framework automates allthe mechanisms behind these two prompts. The framework enables the robot toaddress complex real-world scenarios by processing visual data, making informeddecisions, and carrying out actions automatically. The framework comprises onegeneric vision module and two independent ROS nodes. As a test application, weused ROSGPT_Vision to develop CarMate, which monitors the driver's distractionon the roads and makes real-time vocal notifications to the driver. We showedhow ROSGPT_Vision significantly reduced the development cost compared totraditional methods. We demonstrated how to improve the quality of theapplication by optimizing the prompting strategies, without delving intotechnical details. ROSGPT_Vision is shared with the community (link:https://github.com/bilel-bj/ROSGPT_Vision) to advance robotic research in thisdirection and to build more robotic frameworks that implement the PRM designpattern and enables controlling robots using only prompts."
ProgPrompt: Generating Situated Robot Task Plans using Large Language  Models,"['Ishika Singh', 'Valts Blukis', 'Arsalan Mousavian', 'Ankit Goyal', 'Danfei Xu', 'Jonathan Tremblay', 'Dieter Fox', 'Jesse Thomason', 'Animesh Garg']",http://arxiv.org/pdf/2209.11302v1.pdf,2022-09-22,"['cs.ro', 'cs.ai', 'cs.cl', 'cs.lg']","  Task planning can require defining myriad domain knowledge about the world inwhich a robot needs to act. To ameliorate that effort, large language models(LLMs) can be used to score potential next actions during task planning, andeven generate action sequences directly, given an instruction in naturallanguage with no additional domain information. However, such methods eitherrequire enumerating all possible next steps for scoring, or generate free-formtext that may contain actions not possible on a given robot in its currentcontext. We present a programmatic LLM prompt structure that enables plangeneration functional across situated environments, robot capabilities, andtasks. Our key insight is to prompt the LLM with program-like specifications ofthe available actions and objects in an environment, as well as with exampleprograms that can be executed. We make concrete recommendations about promptstructure and generation constraints through ablation experiments, demonstratestate of the art success rates in VirtualHome household tasks, and deploy ourmethod on a physical robot arm for tabletop tasks. Website atprogprompt.github.io"
Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented  Large Language Models,"['Renat Aksitov', 'Chung-Ching Chang', 'David Reitter', 'Siamak Shakeri', 'Yunhsuan Sung']",http://arxiv.org/pdf/2302.05578v2.pdf,2023-02-11,"['cs.cl', 'cs.ai']","  Despite recent progress, it has been difficult to prevent semantichallucinations in generative Large Language Models. One common solution to thisis augmenting LLMs with a retrieval system and making sure that the generatedoutput is attributable to the retrieved information. Given this new addedconstraint, it is plausible to expect that the overall quality of the outputwill be affected, for example, in terms of fluency. Can scaling language modelshelp?  Here we examine the relationship between fluency and attribution in LLMsprompted with retrieved evidence in knowledge-heavy dialog settings. Ourexperiments were implemented with a set of auto-metrics that are aligned withhuman preferences. They were used to evaluate a large set of generations,produced under varying parameters of LLMs and supplied context.  We show that larger models tend to do much better in both fluency andattribution, and that (naively) using top-k retrieval versus top-1 retrievalimproves attribution but hurts fluency. We next propose a recipe that couldallow smaller models to both close the gap with larger models and preserve thebenefits of top-k retrieval while avoiding its drawbacks."
Dictionary-based Phrase-level Prompting of Large Language Models for  Machine Translation,"['Marjan Ghazvininejad', 'Hila Gonen', 'Luke Zettlemoyer']",http://arxiv.org/pdf/2302.07856v1.pdf,2023-02-15,"['cs.cl', 'cs.lg']","  Large language models (LLMs) demonstrate remarkable machine translation (MT)abilities via prompting, even though they were not explicitly trained for thistask. However, even given the incredible quantities of data they are trainedon, LLMs can struggle to translate inputs with rare words, which are common inlow resource or domain transfer scenarios. We show that LLM prompting canprovide an effective solution for rare words as well, by using prior knowledgefrom bilingual dictionaries to provide control hints in the prompts. We proposea novel method, DiPMT, that provides a set of possible translations for asubset of the input words, thereby enabling fine-grained phrase-level promptedcontrol of the LLM. Extensive experiments show that DiPMT outperforms thebaseline both in low-resource MT, as well as for out-of-domain MT. We furtherprovide a qualitative analysis of the benefits and limitations of thisapproach, including the overall level of controllability that is achieved."
UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and  Distillation of Rerankers,"['Jon Saad-Falcon', 'Omar Khattab', 'Keshav Santhanam', 'Radu Florian', 'Martin Franz', 'Salim Roukos', 'Avirup Sil', 'Md Arafat Sultan', 'Christopher Potts']",http://arxiv.org/pdf/2303.00807v3.pdf,2023-03-01,"['cs.ir', 'cs.cl']","  Many information retrieval tasks require large labeled datasets forfine-tuning. However, such datasets are often unavailable, and their utilityfor real-world applications can diminish quickly due to domain shifts. Toaddress this challenge, we develop and motivate a method for using largelanguage models (LLMs) to generate large numbers of synthetic queries cheaply.The method begins by generating a small number of synthetic queries using anexpensive LLM. After that, a much less expensive one is used to create largenumbers of synthetic queries, which are used to fine-tune a family of rerankermodels. These rerankers are then distilled into a single efficient retrieverfor use in the target domain. We show that this technique boosts zero-shotaccuracy in long-tail domains and achieves substantially lower latency thanstandard reranking methods."
LMCanvas: Object-Oriented Interaction to Personalize Large Language  Model-Powered Writing Environments,"['Tae Soo Kim', 'Arghya Sarkar', 'Yoonjoo Lee', 'Minsuk Chang', 'Juho Kim']",http://arxiv.org/pdf/2303.15125v1.pdf,2023-03-27,"['cs.hc', 'cs.cl']","  Large language models (LLMs) can enhance writing by automating or supportingspecific tasks in writers' workflows (e.g., paraphrasing, creating analogies).Leveraging this capability, a collection of interfaces have been developed thatprovide LLM-powered tools for specific writing tasks. However, these interfacesprovide limited support for writers to create personal tools for their ownunique tasks, and may not comprehensively fulfill a writer's needs -- requiringthem to continuously switch between interfaces during writing. In this work, weenvision LMCanvas, an interface that enables writers to create their ownLLM-powered writing tools and arrange their personal writing environment byinteracting with ""blocks"" in a canvas. In this interface, users can create textblocks to encapsulate writing and LLM prompts, model blocks for model parameterconfigurations, and connect these to create pipeline blocks that outputgenerations. In this workshop paper, we discuss the design for LMCanvas and ourplans to develop this concept."
SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting,"['Xiaoying Zhang', 'Baolin Peng', 'Kun Li', 'Jingyan Zhou', 'Helen Meng']",http://arxiv.org/pdf/2305.09067v1.pdf,2023-05-15,['cs.cl'],"  Building end-to-end task bots and maintaining their integration with newfunctionalities using minimal human efforts is a long-standing challenge indialog research. Recently large language models (LLMs) have demonstratedexceptional proficiency in conversational engagement and adherence toinstructions across various downstream tasks. In this work, we introduceSGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systemseffortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, weinstruct fixed LLMs to generate appropriate responses on novel tasks,circumventing the need for training data. Specifically, SGP-TOD comprises threecomponents: a LLM for engaging with users, a DST Prompter to aid the LLM withdialog state tracking, which is then used to retrieve database items, and aPolicy Prompter to elicit proper responses adhering to the provided dialogpolicy. Experimental results on Multiwoz, RADDLE and STAR datasets show thatour training-free strategy SGP-TOD, without any task-specific data, yieldsstate-of-the-art (SOTA) zero-shot performance, greatly surpasses the few-shotapproaches. In a domain-extension setting, SGP-TOD aptly adapts to newfunctionalities by merely adding supplementary schema rules. We make our codeand data publicly available."
TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks,"['Shubhra Kanti Karmaker Santu', 'Dongji Feng']",http://arxiv.org/pdf/2305.11430v2.pdf,2023-05-19,"['cs.ai', 'cs.cl', 'cs.ir', 'cs.lg', 'i.2.7']","  While LLMs have shown great success in understanding and generating text intraditional conversational settings, their potential for performing ill-definedcomplex tasks is largely under-studied. Indeed, we are yet to conductcomprehensive benchmarking studies with multiple LLMs that are exclusivelyfocused on a complex task. However, conducting such benchmarking studies ischallenging because of the large variations in LLMs' performance when differentprompt types/styles are used and different degrees of detail are provided inthe prompts. To address this issue, the paper proposes a general taxonomy thatcan be used to design prompts with specific properties in order to perform awide range of complex tasks. This taxonomy will allow future benchmarkingstudies to report the specific categories of prompts used as part of the study,enabling meaningful comparisons across different studies. Also, by establishinga common standard through this taxonomy, researchers will be able to draw moreaccurate conclusions about LLMs' performance on a specific complex task."
S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid  Question Answering,"['Fangyu Lei', 'Xiang Li', 'Yifan Wei', 'Shizhu He', 'Yiming Huang', 'Jun Zhao', 'Kang Liu']",http://arxiv.org/pdf/2305.11725v1.pdf,2023-05-19,['cs.cl'],"  Answering multi-hop questions over hybrid factual knowledge from the giventext and table (TextTableQA) is a challenging task. Existing models mainlyadopt a retriever-reader framework, which have several deficiencies, such asnoisy labeling in training retriever, insufficient utilization of heterogeneousinformation over text and table, and deficient ability for different reasoningoperations. In this paper, we propose a three-stage TextTableQA frameworkS3HQA, which comprises of retriever, selector, and reasoner. We use a retrieverwith refinement training to solve the noisy labeling problem. Then, a hybridselector considers the linked relationships between heterogeneous data toselect the most relevant factual knowledge. For the final stage, instead ofadapting a reading comprehension module like in previous methods, we employ ageneration-based reasoner to obtain answers. This includes two approaches: arow-wise generator and an LLM prompting generator~(first time used in thistask). The experimental results demonstrate that our method achievescompetitive results in the few-shot setting. When trained on the full dataset,our approach outperforms all baseline methods, ranking first on the HybridQAleaderboard."
LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain  Conversations with Large Language Models,"['Yen-Ting Lin', 'Yun-Nung Chen']",http://arxiv.org/pdf/2305.13711v1.pdf,2023-05-23,"['cs.cl', 'cs.ai']","  We propose LLM-Eval, a unified multi-dimensional automatic evaluation methodfor open-domain conversations with large language models (LLMs). Existingevaluation methods often rely on human annotations, ground-truth responses, ormultiple LLM prompts, which can be expensive and time-consuming. To addressthese issues, we design a single prompt-based evaluation method that leveragesa unified evaluation schema to cover multiple dimensions of conversationquality in a single model call. We extensively evaluate the performance ofLLM-Eval on various benchmark datasets, demonstrating its effectiveness,efficiency, and adaptability compared to state-of-the-art evaluation methods.Our analysis also highlights the importance of choosing suitable LLMs anddecoding strategies for accurate evaluation results. LLM-Eval offers aversatile and robust solution for evaluating open-domain conversation systems,streamlining the evaluation process and providing consistent performance acrossdiverse scenarios."
AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With  Large Language Models,"['Siqi Ouyang', 'Lei Li']",http://arxiv.org/pdf/2305.15064v3.pdf,2023-05-24,['cs.cl'],"  Recent large language models (LLMs) are promising for making decisions ingrounded environments. However, LLMs frequently fail in complex decision-makingtasks due to the misalignment between the pre-trained knowledge in LLMs and theactual rules in the environment. Existing methods require either costlygradient computation or lengthy in-context demonstrations. In this paper, wepropose AutoPlan, an approach to guide LLM-based agents to accomplishinteractive decision-making tasks. AutoPlan augments the LLM prompt with atask-solving plan and optimizes it through iterative experience collection andreflection. Our experiments show that AutoPlan, though using no in-contextdemonstrations, achieves success rates on par with the baselines usinghuman-written demonstrations on ALFWorld and even outperforms them by 8% onHotpotQA. The code is available at https://github.com/owaski/AutoPlan."
ChatGPT for PLC/DCS Control Logic Generation,"['Heiko Koziolek', 'Sten Gruener', 'Virendra Ashiwal']",http://arxiv.org/pdf/2305.15809v1.pdf,2023-05-25,"['cs.se', 'cs.ai', 'd.2.2']","  Large language models (LLMs) providing generative AI have become popular tosupport software engineers in creating, summarizing, optimizing, anddocumenting source code. It is still unknown how LLMs can support controlengineers using typical control programming languages in programming tasks.Researchers have explored GitHub CoPilot or DeepMind AlphaCode for source codegeneration but did not yet tackle control logic programming. The contributionof this paper is an exploratory study, for which we created 100 LLM prompts in10 representative categories to analyze control logic generation for of PLCsand DCS from natural language. We tested the prompts by generating answers withChatGPT using the GPT-4 LLM. It generated syntactically correct IEC 61131-3Structured Text code in many cases and demonstrated useful reasoning skillsthat could boost control engineer productivity. Our prompt collection is thebasis for a more formal LLM benchmark to test and compare such models forcontrol logic generation."
AdaPlanner: Adaptive Planning from Feedback with Language Models,"['Haotian Sun', 'Yuchen Zhuang', 'Lingkai Kong', 'Bo Dai', 'Chao Zhang']",http://arxiv.org/pdf/2305.16653v1.pdf,2023-05-26,"['cs.cl', 'cs.ai', 'cs.lg']","  Large language models (LLMs) have recently demonstrated the potential inacting as autonomous agents for sequential decision-making tasks. However, mostexisting methods either take actions greedily without planning or rely onstatic plans that are not adaptable to environmental feedback. Consequently,the sequential decision-making performance of LLM agents degenerates withproblem complexity and plan horizons increase. We propose a closed-loopapproach, AdaPlanner, which allows the LLM agent to refine its self-generatedplan adaptively in response to environmental feedback. In AdaPlanner, the LLMagent adaptively refines its plan from feedback with both in-plan andout-of-plan refinement strategies. To mitigate hallucination, we develop acode-style LLM prompt structure that facilitates plan generation across avariety of tasks, environments, and agent capabilities. Furthermore, we proposea skill discovery mechanism that leverages successful plans as few-shotexemplars, enabling the agent to plan and refine with fewer taskdemonstrations. Our experiments in the ALFWorld and MiniWoB++ environmentsdemonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and4.11% while utilizing 2x and 600x fewer samples, respectively."
Robot Task Planning Based on Large Language Model Representing Knowledge  with Directed Graph Structures,"['Yue Zhen', 'Sheng Bi', 'Lu Xing-tong', 'Pan Wei-qin', 'Shi Hai-peng', 'Chen Zi-rui', 'Fang Yi-shu']",http://arxiv.org/pdf/2306.05171v1.pdf,2023-06-08,"['cs.ro', 'cs.ai']","  Traditional robot task planning methods face challenges when dealing withhighly unstructured environments and complex tasks. We propose a task planningmethod that combines human expertise with an LLM and have designed an LLMprompt template, Think_Net_Prompt, with stronger expressive power to representstructured professional knowledge. We further propose a method to progressivelydecompose tasks and generate a task tree to reduce the planning volume for eachtask, and we have designed a strategy to decouple robot task planning. Bydividing different planning entities and separating the task from the actualmachine binding process, the task planning process becomes more flexible.Research results show that our method performs well in handling specified codeformats, understanding the relationship between tasks and subtasks, andextracting parameters from text descriptions. However, there are also problemssuch as limited complexity of task logic handling, ambiguity in the quantity ofparts and the precise location of assembly. Improving the precision of taskdescription and cognitive structure can bring certain improvements.https://github.com/NOMIzy/Think_Net_Prompt"
SayTap: Language to Quadrupedal Locomotion,"['Yujin Tang', 'Wenhao Yu', 'Jie Tan', 'Heiga Zen', 'Aleksandra Faust', 'Tatsuya Harada']",http://arxiv.org/pdf/2306.07580v3.pdf,2023-06-13,['cs.ro'],"  Large language models (LLMs) have demonstrated the potential to performhigh-level planning. Yet, it remains a challenge for LLMs to comprehendlow-level commands, such as joint angle targets or motor torques. This paperproposes an approach to use foot contact patterns as an interface that bridgeshuman commands in natural language and a locomotion controller that outputsthese low-level commands. This results in an interactive system for quadrupedalrobots that allows the users to craft diverse locomotion behaviors flexibly. Wecontribute an LLM prompt design, a reward function, and a method to expose thecontroller to the feasible distribution of contact patterns. The results are acontroller capable of achieving diverse locomotion patterns that can betransferred to real robot hardware. Compared with other design choices, theproposed approach enjoys more than 50% success rate in predicting the correctcontact patterns and can solve 10 more tasks out of a total of 30 tasks. Ourproject site is: https://saytap.github.io."
Large Language Models Enable Few-Shot Clustering,"['Vijay Viswanathan', 'Kiril Gashteovski', 'Carolin Lawrence', 'Tongshuang Wu', 'Graham Neubig']",http://arxiv.org/pdf/2307.00524v1.pdf,2023-07-02,['cs.cl'],"  Unlike traditional unsupervised clustering, semi-supervised clustering allowsusers to provide meaningful structure to the data, which helps the clusteringalgorithm to match the user's intent. Existing approaches to semi-supervisedclustering require a significant amount of feedback from an expert to improvethe clusters. In this paper, we ask whether a large language model can amplifyan expert's guidance to enable query-efficient, few-shot semi-supervised textclustering. We show that LLMs are surprisingly effective at improvingclustering. We explore three stages where LLMs can be incorporated intoclustering: before clustering (improving input features), during clustering (byproviding constraints to the clusterer), and after clustering (using LLMspost-correction). We find incorporating LLMs in the first two stages canroutinely provide significant improvements in cluster quality, and that LLMsenable a user to make trade-offs between cost and accuracy to produce desiredclusters. We release our code and LLM prompts for the public to use."
GEAR: Augmenting Language Models with Generalizable and Efficient Tool  Resolution,"['Yining Lu', 'Haoping Yu', 'Daniel Khashabi']",http://arxiv.org/pdf/2307.08775v1.pdf,2023-07-17,['cs.ai'],"  Augmenting large language models (LLM) to use external tools enhances theirperformance across a variety of tasks. However, prior works over-rely ontask-specific demonstration of tool use that limits their generalizability andcomputational cost due to making many calls to large-scale LLMs. We introduceGEAR, a computationally efficient query-tool grounding algorithm that isgeneralizable to various tasks that require tool use while not relying ontask-specific demonstrations. GEAR achieves better efficiency by delegatingtool grounding and execution to small language models (SLM) and LLM,respectively; while leveraging semantic and pattern-based evaluation at bothquestion and answer levels for generalizable tool grounding. We evaluate GEARon 14 datasets across 6 downstream tasks, demonstrating its stronggeneralizability to novel tasks, tools and different SLMs. Despite offeringmore efficiency, GEAR achieves higher precision in tool grounding compared toprior strategies using LLM prompting, thus improving downstream accuracy at areduced computational cost. For example, we demonstrate that GEAR-augmentedGPT-J and GPT-3 outperform counterpart tool-augmented baselines because ofbetter tool use."
Simple LLM Prompting is State-of-the-Art for Robust and Multilingual  Dialogue Evaluation,"['John Mendonça', 'Patrícia Pereira', 'Helena Moniz', 'João Paulo Carvalho', 'Alon Lavie', 'Isabel Trancoso']",http://arxiv.org/pdf/2308.16797v2.pdf,2023-08-31,['cs.cl'],"  Despite significant research effort in the development of automatic dialogueevaluation metrics, little thought is given to evaluating dialogues other thanin English. At the same time, ensuring metrics are invariant to semanticallysimilar responses is also an overlooked topic. In order to achieve the desiredproperties of robustness and multilinguality for dialogue evaluation metrics,we propose a novel framework that takes advantage of the strengths of currentevaluation models with the newly-established paradigm of prompting LargeLanguage Models (LLMs). Empirical results show our framework achieves state ofthe art results in terms of mean Spearman correlation scores across severalbenchmarks and ranks first place on both the Robust and Multilingual tasks ofthe DSTC11 Track 4 ""Automatic Evaluation Metrics for Open-Domain DialogueSystems"", proving the evaluation capabilities of prompted LLMs."
"MMHQA-ICL: Multimodal In-context Learning for Hybrid Question Answering  over Text, Tables and Images","['Weihao Liu', 'Fangyu Lei', 'Tongxu Luo', 'Jiahe Lei', 'Shizhu He', 'Jun Zhao', 'Kang Liu']",http://arxiv.org/pdf/2309.04790v1.pdf,2023-09-09,['cs.cl'],"  In the real world, knowledge often exists in a multimodal and heterogeneousform. Addressing the task of question answering with hybrid data types,including text, tables, and images, is a challenging task (MMHQA). Recently,with the rise of large language models (LLM), in-context learning (ICL) hasbecome the most popular way to solve QA problems. We propose MMHQA-ICLframework for addressing this problems, which includes stronger heterogeneousdata retriever and an image caption module. Most importantly, we propose aType-specific In-context Learning Strategy for MMHQA, enabling LLMs to leveragetheir powerful performance in this task. We are the first to use end-to-end LLMprompting method for this task. Experimental results demonstrate that ourframework outperforms all baselines and methods trained on the full dataset,achieving state-of-the-art results under the few-shot setting on theMultimodalQA dataset."
Empowering Private Tutoring by Chaining Large Language Models,"['Yulin Chen', 'Ning Ding', 'Hai-Tao Zheng', 'Zhiyuan Liu', 'Maosong Sun', 'Bowen Zhou']",http://arxiv.org/pdf/2309.08112v1.pdf,2023-09-15,['cs.hc'],"  Artificial intelligence has been applied in various aspects of onlineeducation to facilitate teaching and learning. However, few approaches has beenmade toward a complete AI-powered tutoring system. In this work, we explore thedevelopment of a full-fledged intelligent tutoring system powered bystate-of-the-art large language models (LLMs), covering automatic courseplanning and adjusting, tailored instruction, and flexible quiz evaluation. Tomake the system robust to prolonged interaction and cater to individualizededucation, the system is decomposed into three inter-connected coreprocesses-interaction, reflection, and reaction. Each process is implemented bychaining LLM-powered tools along with dynamically updated memory modules. Toolsare LLMs prompted to execute one specific task at a time, while memories aredata storage that gets updated during education process. Statistical resultsfrom learning logs demonstrate the effectiveness and mechanism of each toolusage. Subjective feedback from human users reveal the usability of eachfunction, and comparison with ablation systems further testify the benefits ofthe designed processes in long-term interaction."
Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for  Knowledge Graph Question Answering,"['Yike Wu', 'Nan Hu', 'Sheng Bi', 'Guilin Qi', 'Jie Ren', 'Anhuan Xie', 'Wei Song']",http://arxiv.org/pdf/2309.11206v2.pdf,2023-09-20,"['cs.cl', 'cs.ai']","  Despite their competitive performance on knowledge-intensive tasks, largelanguage models (LLMs) still have limitations in memorizing all world knowledgeespecially long tail knowledge. In this paper, we study the KG-augmentedlanguage model approach for solving the knowledge graph question answering(KGQA) task that requires rich world knowledge. Existing work has shown thatretrieving KG knowledge to enhance LLMs prompting can significantly improveLLMs performance in KGQA. However, their approaches lack a well-formedverbalization of KG knowledge, i.e., they ignore the gap between KGrepresentations and textual representations. To this end, we propose ananswer-sensitive KG-to-Text approach that can transform KG knowledge intowell-textualized statements most informative for KGQA. Based on this approach,we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task.Experiments on several KGQA benchmarks show that the proposed KG-to-Textaugmented LLMs approach outperforms previous KG-augmented LLMs approachesregarding answer accuracy and usefulness of knowledge statements."
LPML: LLM-Prompting Markup Language for Mathematical Reasoning,"['Ryutaro Yamauchi', 'Sho Sonoda', 'Akiyoshi Sannai', 'Wataru Kumagai']",http://arxiv.org/pdf/2309.13078v2.pdf,2023-09-21,"['cs.ai', 'cs.lg', 'cs.pl']","  In utilizing large language models (LLMs) for mathematical reasoning,addressing the errors in the reasoning and calculation present in the generatedtext by LLMs is a crucial challenge. In this paper, we propose a novelframework that integrates the Chain-of-Thought (CoT) method with an externaltool (Python REPL). We discovered that by prompting LLMs to generate structuredtext in XML-like markup language, we could seamlessly integrate CoT and theexternal tool and control the undesired behaviors of LLMs. With our approach,LLMs can utilize Python computation to rectify errors within CoT. We appliedour method to ChatGPT (GPT-3.5) to solve challenging mathematical problems anddemonstrated that combining CoT and Python REPL through the markup languageenhances the reasoning capability of LLMs. Our approach enables LLMs to writethe markup language and perform advanced mathematical reasoning using onlyzero-shot prompting."
HeaP: Hierarchical Policies for Web Actions using LLMs,"['Paloma Sodhi', 'S. R. K. Branavan', 'Ryan McDonald']",http://arxiv.org/pdf/2310.03720v1.pdf,2023-10-05,['cs.lg'],"  Large language models (LLMs) have demonstrated remarkable capabilities inperforming a range of instruction following tasks in few and zero-shotsettings. However, teaching LLMs to perform tasks on the web presentsfundamental challenges -- combinatorially large open-world tasks and variationsacross web interfaces. We tackle these challenges by leveraging LLMs todecompose web tasks into a collection of sub-tasks, each of which can be solvedby a low-level, closed-loop policy. These policies constitute a shared grammaracross tasks, i.e., new web tasks can be expressed as a composition of thesepolicies. We propose a novel framework, Hierarchical Policies for Web Actionsusing LLMs (HeaP), that learns a set of hierarchical LLM prompts fromdemonstrations for planning high-level tasks and executing them via a sequenceof low-level policies. We evaluate HeaP against a range of baselines on a suiteof web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well aslive website interactions, and show that it is able to outperform prior worksusing orders of magnitude less data."
OptiMUS: Optimization Modeling Using MIP Solvers and large language  models,"['Ali AhmadiTeshnizi', 'Wenzhi Gao', 'Madeleine Udell']",http://arxiv.org/pdf/2310.06116v2.pdf,2023-10-09,['cs.ai'],"  Optimization problems are pervasive across various sectors, frommanufacturing and distribution to healthcare. However, most such problems arestill solved heuristically by hand rather than optimally by state-of-the-artsolvers, as the expertise required to formulate and solve these problems limitsthe widespread adoption of optimization tools and techniques. We introduceOptiMUS, a Large Language Model (LLM)-based agent designed to formulate andsolve MILP problems from their natural language descriptions. OptiMUS iscapable of developing mathematical models, writing and debugging solver code,developing tests, and checking the validity of generated solutions. Tobenchmark our agent, we present NLP4LP, a novel dataset of linear programming(LP) and mixed integer linear programming (MILP) problems. Our experimentsdemonstrate that OptiMUS solves nearly twice as many problems as a basic LLMprompting strategy. OptiMUS code and NLP4LP dataset are available at\href{https://github.com/teshnizi/OptiMUS}{https://github.com/teshnizi/OptiMUS}"
A ML-LLM pairing for better code comment classification,['Hanna Abi Akl'],http://arxiv.org/pdf/2310.10275v1.pdf,2023-10-13,"['cs.se', 'cs.ai']","  The ""Information Retrieval in Software Engineering (IRSE)"" at FIRE 2023shared task introduces code comment classification, a challenging task thatpairs a code snippet with a comment that should be evaluated as either usefulor not useful to the understanding of the relevant code. We answer the codecomment classification shared task challenge by providing a two-foldevaluation: from an algorithmic perspective, we compare the performance ofclassical machine learning systems and complement our evaluations from adata-driven perspective by generating additional data with the help of largelanguage model (LLM) prompting to measure the potential increase inperformance. Our best model, which took second place in the shared task, is aNeural Network with a Macro-F1 score of 88.401% on the provided seed data and a1.5% overall increase in performance on the data generated by the LLM."
Multi-stage Large Language Model Correction for Speech Recognition,"['Jie Pu', 'Thai-Son Nguyen', 'Sebastian Stüker']",http://arxiv.org/pdf/2310.11532v1.pdf,2023-10-17,"['cs.cl', 'eess.as']","  In this paper, we investigate the usage of large language models (LLMs) toimprove the performance of competitive speech recognition systems. Differentfrom traditional language models that focus on one single data domain, the riseof LLMs brings us the opportunity to push the limit of state-of-the-art ASRperformance, and at the same time to achieve higher robustness and generalizeeffectively across multiple domains. Motivated by this, we propose a novelmulti-stage approach to combine traditional language model re-scoring and LLMprompting. Specifically, the proposed method has two stages: the first stageuses a language model to re-score an N-best list of ASR hypotheses and run aconfidence check; The second stage uses prompts to a LLM to perform ASR errorcorrection on less confident results from the first stage. Our experimentalresults demonstrate the effectiveness of the proposed method by showing a 10% ~20% relative improvement in WER over a competitive ASR system -- acrossmultiple test domains."
PromptInfuser: How Tightly Coupling AI and UI Design Impacts Designers'  Workflows,"['Savvas Petridis', 'Michael Terry', 'Carrie J. Cai']",http://arxiv.org/pdf/2310.15435v1.pdf,2023-10-24,"['cs.hc', 'cs.ai']","  Prototyping AI applications is notoriously difficult. While large languagemodel (LLM) prompting has dramatically lowered the barriers to AI prototyping,designers are still prototyping AI functionality and UI separately. Weinvestigate how coupling prompt and UI design affects designers' workflows.Grounding this research, we developed PromptInfuser, a Figma plugin thatenables users to create semi-functional mockups, by connecting UI elements tothe inputs and outputs of prompts. In a study with 14 designers, we comparePromptInfuser to designers' current AI-prototyping workflow. PromptInfuser wasperceived to be significantly more useful for communicating product ideas, morecapable of producing prototypes that realistically represent the envisionedartifact, more efficient for prototyping, and more helpful for anticipating UIissues and technical constraints. PromptInfuser encouraged iteration overprompt and UI together, which helped designers identify UI and promptincompatibilities and reflect upon their total solution. Together, thesefindings inform future systems for prototyping AI applications."
OmniFill: Domain-Agnostic Form Filling Suggestions Using Multi-Faceted  Context,"['Timothy J. Aveni', 'Armando Fox', 'Björn Hartmann']",http://arxiv.org/pdf/2310.17826v1.pdf,2023-10-27,['cs.hc'],"  Predictive suggestion systems offer contextually-relevant text entrycompletions. Existing approaches, like autofill, often excel innarrowly-defined domains but fail to generalize to arbitrary workflows. Weintroduce a conceptual framework to analyze the compound demands of aparticular suggestion context, yielding unique opportunities for large languagemodels (LLMs) to infer suggestions for a wide range of domain-agnosticform-filling tasks that were out of reach with prior approaches. We explorethese opportunities in OmniFill, a prototype that collects multi-facetedcontext including browsing and text entry activity to construct an LLM promptthat offers suggestions in situ for arbitrary structured text entry interfaces.Through a user study with 18 participants, we found that OmniFill offeredvaluable suggestions and we identified four themes that characterize users'behavior and attitudes: an ""opportunistic scrapbooking"" approach; a trustplaced in the system; value in partial success; and a need for visibility intoprompt context."
Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data  Generation with Large Language Models,"['Ran Xu', 'Hejie Cui', 'Yue Yu', 'Xuan Kan', 'Wenqi Shi', 'Yuchen Zhuang', 'Wei Jin', 'Joyce Ho', 'Carl Yang']",http://arxiv.org/pdf/2311.00287v1.pdf,2023-11-01,"['cs.cl', 'cs.ai', 'cs.lg', 'q-bio.qm']","  Clinical natural language processing requires methods that can addressdomain-specific challenges, such as complex medical terminology and clinicalcontexts. Recently, large language models (LLMs) have shown promise in thisdomain. Yet, their direct deployment can lead to privacy issues and areconstrained by resources. To address this challenge, we delve into syntheticclinical text generation using LLMs for clinical NLP tasks. We propose aninnovative, resource-efficient approach, ClinGen, which infuses knowledge intothe process. Our model involves clinical knowledge extraction andcontext-informed LLM prompting. Both clinical topics and writing styles aredrawn from external domain-specific knowledge graphs and LLMs to guide datageneration. Our extensive empirical study across 7 clinical NLP tasks and 16datasets reveals that ClinGen consistently enhances performance across varioustasks, effectively aligning the distribution of real datasets and significantlyenriching the diversity of generated training instances. We will publish ourcode and all the generated data in \url{https://github.com/ritaranx/ClinGen}."
Prompt Cache: Modular Attention Reuse for Low-Latency Inference,"['In Gim', 'Guojun Chen', 'Seung-seob Lee', 'Nikhil Sarda', 'Anurag Khandelwal', 'Lin Zhong']",http://arxiv.org/pdf/2311.04934v1.pdf,2023-11-07,"['cs.cl', 'cs.ai']","  We present Prompt Cache, an approach for accelerating inference for largelanguage models (LLM) by reusing attention states across different LLM prompts.Many input prompts have overlapping text segments, such as system messages,prompt templates, and documents provided for context. Our key insight is thatby precomputing and storing the attention states of these frequently occurringtext segments on the inference server, we can efficiently reuse them when thesesegments appear in user prompts. Prompt Cache employs a schema to explicitlydefine such reusable text segments, called prompt modules. The schema ensurespositional accuracy during attention state reuse and provides users with aninterface to access cached states in their prompt. Using a prototypeimplementation, we evaluate Prompt Cache across several LLMs. We show thatPrompt Cache significantly reduce latency in time-to-first-token, especiallyfor longer prompts such as document-based question answering andrecommendations. The improvements range from 8x for GPU-based inference to 60xfor CPU-based inference, all while maintaining output accuracy and without theneed for model parameter modifications."
Extrinsically-Focused Evaluation of Omissions in Medical Summarization,"['Elliot Schumacher', 'Daniel Rosenthal', 'Varun Nair', 'Luladay Price', 'Geoffrey Tso', 'Anitha Kannan']",http://arxiv.org/pdf/2311.08303v1.pdf,2023-11-14,"['cs.cl', 'cs.ai']","  The goal of automated summarization techniques (Paice, 1990; Kupiec et al,1995) is to condense text by focusing on the most critical information.Generative large language models (LLMs) have shown to be robust summarizers,yet traditional metrics struggle to capture resulting performance (Goyal et al,2022) in more powerful LLMs. In safety-critical domains such as medicine, morerigorous evaluation is required, especially given the potential for LLMs toomit important information in the resulting summary. We propose MED-OMIT, a newomission benchmark for medical summarization. Given a doctor-patientconversation and a generated summary, MED-OMIT categorizes the chat into a setof facts and identifies which are omitted from the summary. We further proposeto determine fact importance by simulating the impact of each fact on adownstream clinical task: differential diagnosis (DDx) generation. MED-OMITleverages LLM prompt-based approaches which categorize the importance of factsand cluster them as supporting or negating evidence to the diagnosis. Weevaluate MED-OMIT on a publicly-released dataset of patient-doctorconversations and find that MED-OMIT captures omissions better than alternativemetrics."
"Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits","['Johannes Schneider', 'Steffi Haag', 'Leona Chandra Kruse']",http://arxiv.org/pdf/2312.03720v1.pdf,2023-11-26,"['cs.cl', 'cs.ai']","  Large language models LLMs like ChatGPT have reached the 100 Mio user barrierin record time and might increasingly enter all areas of our life leading to adiverse set of interactions between those Artificial Intelligence models andhumans. While many studies have discussed governance and regulationsdeductively from first-order principles, few studies provide an inductive,data-driven lens based on observing dialogues between humans and LLMsespecially when it comes to non-collaborative, competitive situations that havethe potential to pose a serious threat to people. In this work, we conduct auser study engaging over 40 individuals across all age groups in pricenegotiations with an LLM. We explore how people interact with an LLM,investigating differences in negotiation outcomes and strategies. Furthermore,we highlight shortcomings of LLMs with respect to their reasoning capabilitiesand, in turn, susceptiveness to prompt hacking, which intends to manipulate theLLM to make agreements that are against its instructions or beyond anyrationality. We also show that the negotiated prices humans manage to achievespan a broad range, which points to a literacy gap in effectively interactingwith LLMs."
Promptagator: Few-shot Dense Retrieval From 8 Examples,"['Zhuyun Dai', 'Vincent Y. Zhao', 'Ji Ma', 'Yi Luan', 'Jianmo Ni', 'Jing Lu', 'Anton Bakalov', 'Kelvin Guu', 'Keith B. Hall', 'Ming-Wei Chang']",http://arxiv.org/pdf/2209.11755v1.pdf,2022-09-23,"['cs.cl', 'cs.ir']","  Much recent research on information retrieval has focused on how to transferfrom one task (typically with abundant supervised data) to various other taskswhere supervision is limited, with the implicit assumption that it is possibleto generalize from one task to all the rest. However, this overlooks the factthat there are many diverse and unique retrieval tasks, each targetingdifferent search intents, queries, and search domains. In this paper, wesuggest to work on Few-shot Dense Retrieval, a setting where each task comeswith a short description and a few examples. To amplify the power of a fewexamples, we propose Prompt-base Query Generation for Retriever (Promptagator),which leverages large language models (LLM) as a few-shot query generator, andcreates task-specific retrievers based on the generated data. Powered by LLM'sgeneralization ability, Promptagator makes it possible to create task-specificend-to-end retrievers solely based on a few examples {without} using NaturalQuestions or MS MARCO to train %question generators or dual encoders.Surprisingly, LLM prompting with no more than 8 examples allows dual encodersto outperform heavily engineered models trained on MS MARCO like ColBERT v2 bymore than 1.2 nDCG on average on 11 retrieval sets. Further trainingstandard-size re-rankers using the same generated data yields another 5.0 pointnDCG improvement. Our studies determine that query generation can be far moreeffective than previously observed, especially when a small amount oftask-specific knowledge is given."
Check Your Facts and Try Again: Improving Large Language Models with  External Knowledge and Automated Feedback,"['Baolin Peng', 'Michel Galley', 'Pengcheng He', 'Hao Cheng', 'Yujia Xie', 'Yu Hu', 'Qiuyuan Huang', 'Lars Liden', 'Zhou Yu', 'Weizhu Chen', 'Jianfeng Gao']",http://arxiv.org/pdf/2302.12813v3.pdf,2023-02-24,"['cs.cl', 'cs.ai']","  Large language models (LLMs), such as ChatGPT, are able to generatehuman-like, fluent responses for many downstream tasks, e.g., task-orienteddialog and question answering. However, applying LLMs to real-world,mission-critical applications remains challenging mainly due to their tendencyto generate hallucinations and their inability to use external knowledge. Thispaper proposes a LLM-Augmenter system, which augments a black-box LLM with aset of plug-and-play modules. Our system makes the LLM generate responsesgrounded in external knowledge, e.g., stored in task-specific databases. Italso iteratively revises LLM prompts to improve model responses using feedbackgenerated by utility functions, e.g., the factuality score of a LLM-generatedresponse. The effectiveness of LLM-Augmenter is empirically validated on twotypes of scenarios, task-oriented dialog and open-domain question answering.LLM-Augmenter significantly reduces ChatGPT's hallucinations withoutsacrificing the fluency and informativeness of its responses. We make thesource code and models publicly available."
AlpacaFarm: A Simulation Framework for Methods that Learn from Human  Feedback,"['Yann Dubois', 'Xuechen Li', 'Rohan Taori', 'Tianyi Zhang', 'Ishaan Gulrajani', 'Jimmy Ba', 'Carlos Guestrin', 'Percy Liang', 'Tatsunori B. Hashimoto']",http://arxiv.org/pdf/2305.14387v3.pdf,2023-05-22,"['cs.lg', 'cs.ai', 'cs.cl']","  Large language models (LLMs) such as ChatGPT have seen widespread adoptiondue to their ability to follow user instructions well. Developing these LLMsinvolves a complex yet poorly understood workflow requiring training with humanfeedback. Replicating and understanding this instruction-following processfaces three major challenges: the high cost of data collection, the lack oftrustworthy evaluation, and the absence of reference method implementations. Weaddress these challenges with AlpacaFarm, a simulator that enables research anddevelopment for learning from feedback at a low cost. First, we design LLMprompts to simulate human feedback that are 45x cheaper than crowdworkers anddisplay high agreement with humans. Second, we propose an automatic evaluationand validate it against human instructions obtained on real-world interactions.Third, we contribute reference implementations for several methods (PPO, DPO,best-of-n, expert iteration, and more) that learn from pairwise feedback.Finally, as an end-to-end validation of AlpacaFarm, we train and evaluateeleven models on 10k pairs of real human feedback and show that rankings ofmodels trained in AlpacaFarm match rankings of models trained on human data. Asa demonstration of the research possible in AlpacaFarm, we find that methodsthat use a reward model can substantially improve over supervised fine-tuningand that our reference PPO implementation leads to a +10% improvement inwin-rate against Davinci003. We release all components of AlpacaFarm athttps://github.com/tatsu-lab/alpaca_farm."
MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties  Grounded in Math Reasoning Problems,"['Jakub Macina', 'Nico Daheim', 'Sankalan Pal Chowdhury', 'Tanmay Sinha', 'Manu Kapur', 'Iryna Gurevych', 'Mrinmaya Sachan']",http://arxiv.org/pdf/2305.14536v2.pdf,2023-05-23,['cs.cl'],"  While automatic dialogue tutors hold great potential in making educationpersonalized and more accessible, research on such systems has been hampered bya lack of sufficiently large and high-quality datasets. Collecting suchdatasets remains challenging, as recording tutoring sessions raises privacyconcerns and crowdsourcing leads to insufficient data quality. To address this,we propose a framework to generate such dialogues by pairing human teacherswith a Large Language Model (LLM) prompted to represent common student errors.We describe how we use this framework to collect MathDial, a dataset of 3kone-to-one teacher-student tutoring dialogues grounded in multi-step mathreasoning problems. While models like GPT-3 are good problem solvers, they failat tutoring because they generate factually incorrect feedback or are prone torevealing solutions to students too early. To overcome this, we let teachersprovide learning opportunities to students by guiding them using variousscaffolding questions according to a taxonomy of teacher moves. We demonstrateMathDial and its extensive annotations can be used to finetune models to bemore effective tutors (and not just solvers). We confirm this by automatic andhuman evaluation, notably in an interactive setting that measures the trade-offbetween student solving success and telling solutions. The dataset is releasedpublicly."
SPRING: Studying the Paper and Reasoning to Play Games,"['Yue Wu', 'Shrimai Prabhumoye', 'So Yeon Min', 'Yonatan Bisk', 'Ruslan Salakhutdinov', 'Amos Azaria', 'Tom Mitchell', 'Yuanzhi Li']",http://arxiv.org/pdf/2305.15486v3.pdf,2023-05-24,"['cs.ai', 'cs.lg']","  Open-world survival games pose significant challenges for AI algorithms dueto their multi-tasking, deep exploration, and goal prioritization requirements.Despite reinforcement learning (RL) being popular for solving games, its highsample complexity limits its effectiveness in complex open-world games likeCrafter or Minecraft. We propose a novel approach, SPRING, to read the game'soriginal academic paper and use the knowledge learned to reason and play thegame through a large language model (LLM). Prompted with the LaTeX source asgame context and a description of the agent's current observation, our SPRINGframework employs a directed acyclic graph (DAG) with game-related questions asnodes and dependencies as edges. We identify the optimal action to take in theenvironment by traversing the DAG and calculating LLM responses for each nodein topological order, with the LLM's answer to final node directly translatingto environment actions. In our experiments, we study the quality of in-context""reasoning"" induced by different forms of prompts under the setting of theCrafter open-world environment. Our experiments suggest that LLMs, whenprompted with consistent chain-of-thought, have great potential in completingsophisticated high-level trajectories. Quantitatively, SPRING with GPT-4outperforms all state-of-the-art RL baselines, trained for 1M steps, withoutany training. Finally, we show the potential of games as a test bed for LLMs."
Flocks of Stochastic Parrots: Differentially Private Prompt Learning for  Large Language Models,"['Haonan Duan', 'Adam Dziedzic', 'Nicolas Papernot', 'Franziska Boenisch']",http://arxiv.org/pdf/2305.15594v1.pdf,2023-05-24,"['cs.lg', 'cs.cl', 'cs.cr']","  Large language models (LLMs) are excellent in-context learners. However, thesensitivity of data contained in prompts raises privacy concerns. Our workfirst shows that these concerns are valid: we instantiate a simple but highlyeffective membership inference attack against the data used to prompt LLMs. Toaddress this vulnerability, one could forego prompting and resort tofine-tuning LLMs with known algorithms for private gradient descent. However,this comes at the expense of the practicality and efficiency offered byprompting. Therefore, we propose to privately learn to prompt. We first showthat soft prompts can be obtained privately through gradient descent ondownstream data. However, this is not the case for discrete prompts. Thus, weorchestrate a noisy vote among an ensemble of LLMs presented with differentprompts, i.e., a flock of stochastic parrots. The vote privately transfers theflock's knowledge into a single public prompt. We show that LLMs prompted withour private algorithms closely match the non-private baselines. For example,using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on thesst2 dataset with ($\epsilon=0.147, \delta=10^{-6}$)-differential privacy vs.95.2% for the non-private baseline. Through our experiments, we also show thatour prompt-based approach is easily deployed with existing commercial APIs."
Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction,"['Salvatore Carta', 'Alessandro Giuliani', 'Leonardo Piano', 'Alessandro Sebastian Podda', 'Livio Pompianu', 'Sandro Gabriele Tiddia']",http://arxiv.org/pdf/2307.01128v1.pdf,2023-07-03,"['cs.cl', 'cs.ai']","  In the current digitalization era, capturing and effectively representingknowledge is crucial in most real-world scenarios. In this context, knowledgegraphs represent a potent tool for retrieving and organizing a vast amount ofinformation in a properly interconnected and interpretable structure. However,their generation is still challenging and often requires considerable humaneffort and domain expertise, hampering the scalability and flexibility acrossdifferent application fields. This paper proposes an innovative knowledge graphgeneration approach that leverages the potential of the latest generative largelanguage models, such as GPT-3.5, that can address all the main critical issuesin knowledge graph building. The approach is conveyed in a pipeline thatcomprises novel iterative zero-shot and external knowledge-agnostic strategiesin the main stages of the generation process. Our unique manifold approach mayencompass significant benefits to the scientific community. In particular, themain contribution can be summarized by: (i) an innovative strategy foriteratively prompting large language models to extract relevant components ofthe final graph; (ii) a zero-shot strategy for each prompt, meaning that thereis no need for providing examples for ""guiding"" the prompt result; (iii) ascalable solution, as the adoption of LLMs avoids the need for any externalresources or human expertise. To assess the effectiveness of our proposedmodel, we performed experiments on a dataset that covered a specific domain. Weclaim that our proposal is a suitable solution for scalable and versatileknowledge graph construction and may be applied to different and novelcontexts."
PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine,"['Chenrui Zhang', 'Lin Liu', 'Jinpeng Wang', 'Chuyuan Wang', 'Xiao Sun', 'Hongyu Wang', 'Mingchen Cai']",http://arxiv.org/pdf/2308.12033v1.pdf,2023-08-23,"['cs.cl', 'cs.ai']","  As an effective tool for eliciting the power of Large Language Models (LLMs),prompting has recently demonstrated unprecedented abilities across a variety ofcomplex tasks. To further improve the performance, prompt ensemble hasattracted substantial interest for tackling the hallucination and instabilityof LLMs. However, existing methods usually adopt a two-stage paradigm, whichrequires a pre-prepared set of prompts with substantial manual effort, and isunable to perform directed optimization for different weak learners. In thispaper, we propose a simple, universal, and automatic method named PREFER (PomptEnsemble learning via Feedback-Reflect-Refine) to address the statedlimitations. Specifically, given the fact that weak learners are supposed tofocus on hard examples during boosting, PREFER builds a feedback mechanism forreflecting on the inadequacies of existing weak learners. Based on this, theLLM is required to automatically synthesize new prompts for iterativerefinement. Moreover, to enhance stability of the prompt effect evaluation, wepropose a novel prompt bagging method involving forward and backward thinking,which is superior to majority voting and is beneficial for both feedback andweight calculation in boosting. Extensive experiments demonstrate that ourPREFER achieves state-of-the-art performance in multiple types of tasks by asignificant margin. We have made our code publicly available."
ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI  Co-Writing Tasks using Large Language Models,"['Mohi Reza', 'Nathan Laundry', 'Ilya Musabirov', 'Peter Dushniku', 'Zhi Yuan ""Michael"" Yu', 'Kashish Mittal', 'Tovi Grossman', 'Michael Liut', 'Anastasia Kuzminykh', 'Joseph Jay Williams']",http://arxiv.org/pdf/2310.00117v2.pdf,2023-09-29,"['cs.hc', 'cs.ai', 'cs.lg']","  Exploring alternative ideas by rewriting text is integral to the writingprocess. State-of-the-art large language models (LLMs) can simplify writingvariation generation. However, current interfaces pose challenges forsimultaneous consideration of multiple variations: creating new versionswithout overwriting text can be difficult, and pasting them sequentially canclutter documents, increasing workload and disrupting writers' flow. To tacklethis, we present ABScribe, an interface that supports rapid, yet visuallystructured, exploration of writing variations in human-AI co-writing tasks.With ABScribe, users can swiftly produce multiple variations using LLM prompts,which are auto-converted into reusable buttons. Variations are storedadjacently within text segments for rapid in-place comparisons using mouse-overinteractions on a context toolbar. Our user study with 12 writers shows thatABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhancesuser perceptions of the revision process (d = 2.41, p < 0.001) compared to apopular baseline workflow, and provides insights into how writers explorevariations using LLMs."
Knowledge Crosswords: Geometric Reasoning over Structured Knowledge with  Large Language Models,"['Wenxuan Ding', 'Shangbin Feng', 'Yuhan Liu', 'Zhaoxuan Tan', 'Vidhisha Balachandran', 'Tianxing He', 'Yulia Tsvetkov']",http://arxiv.org/pdf/2310.01290v1.pdf,2023-10-02,"['cs.cl', 'cs.ai']","  Large language models (LLMs) are widely adopted in knowledge-intensive tasksand have achieved impressive performance thanks to their knowledge abilities.While LLMs have demonstrated outstanding performance on atomic or linear(multi-hop) QA tasks, whether they can reason in knowledge-rich scenarios withinterweaving constraints remains an underexplored problem. In this work, wepropose geometric reasoning over structured knowledge, where pieces ofknowledge are connected in a graph structure and models need to fill in themissing information. Such geometric knowledge reasoning would require theability to handle structured knowledge, reason with uncertainty, verify facts,and backtrack when an error occurs. We propose Knowledge Crosswords, amulti-blank QA dataset where each problem consists of a natural languagequestion representing the geometric constraints of an incomplete entitynetwork, where LLMs are tasked with working out the missing entities whilemeeting all factual constraints. Knowledge Crosswords contains 2,101 individualproblems, covering various knowledge domains and further divided into threedifficulty levels. We conduct extensive experiments to evaluate existing LLMprompting approaches on the Knowledge Crosswords benchmark. We additionallypropose two new approaches, Staged Prompting and Verify-All, to augment LLMs'ability to backtrack and verify structured constraints. Our results demonstratethat while baseline approaches perform well on easier problems but strugglewith hard ones, our proposed Verify-All outperforms other methods by a largemargin and is more robust with hard problems. Further analysis reveals thatLLMs' ability of geometric reasoning over structured knowledge is still farfrom robust or perfect, susceptible to confounders such as the order ofoptions, certain structural patterns, assumption of existence of correctanswer, and more."
Retrieval-augmented Generation to Improve Math Question-Answering:  Trade-offs Between Groundedness and Human Preference,"['Zachary Levonian', 'Chenglu Li', 'Wangda Zhu', 'Anoushka Gade', 'Owen Henkel', 'Millie-Ellen Postle', 'Wanli Xing']",http://arxiv.org/pdf/2310.03184v2.pdf,2023-10-04,"['cs.cl', 'cs.hc']","  For middle-school math students, interactive question-answering (QA) withtutors is an effective way to learn. The flexibility and emergent capabilitiesof generative large language models (LLMs) has led to a surge of interest inautomating portions of the tutoring process - including interactive QA tosupport conceptual discussion of mathematical concepts. However, LLM responsesto math questions can be incorrect or mismatched to the educational context -such as being misaligned with a school's curriculum. One potential solution isretrieval-augmented generation (RAG), which involves incorporating a vettedexternal knowledge source in the LLM prompt to increase response quality. Inthis paper, we designed prompts that retrieve and use content from ahigh-quality open-source math textbook to generate responses to real studentquestions. We evaluate the efficacy of this RAG system for middle-schoolalgebra and geometry QA by administering a multi-condition survey, finding thathumans prefer responses generated using RAG, but not when responses are toogrounded in the textbook content. We argue that while RAG is able to improveresponse quality, designers of math QA systems must consider trade-offs betweengenerating responses preferred by students and responses closely matched tospecific educational resources."
Small Language Models Fine-tuned to Coordinate Larger Language Models  improve Complex Reasoning,"['Gurusha Juneja', 'Subhabrata Dutta', 'Soumen Chakrabarti', 'Sunny Manchanda', 'Tanmoy Chakraborty']",http://arxiv.org/pdf/2310.18338v1.pdf,2023-10-21,"['cs.cl', 'cs.ai']","  Large Language Models (LLMs) prompted to generate chain-of-thought (CoT)exhibit impressive reasoning capabilities. Recent attempts at promptdecomposition toward solving complex, multi-step reasoning problems depend onthe ability of the LLM to simultaneously decompose and solve the problem. Asignificant disadvantage is that foundational LLMs are typically not availablefor fine-tuning, making adaptation computationally prohibitive. We believe (anddemonstrate) that problem decomposition and solution generation are distinctcapabilites, better addressed in separate modules, than by one monolithic LLM.We introduce DaSLaM, which uses a decomposition generator to decompose complexproblems into subproblems that require fewer reasoning steps. These subproblemsare answered by a solver. We use a relatively small (13B parameters) LM as thedecomposition generator, which we train using policy gradient optimization tointeract with a solver LM (regarded as black-box) and guide it throughsubproblems, thereby rendering our method solver-agnostic. Evaluation onmultiple different reasoning datasets reveal that with our method, a 175billion parameter LM (text-davinci-003) can produce competitive or even betterperformance, compared to its orders-of-magnitude larger successor, GPT-4.Additionally, we show that DaSLaM is not limited by the solver's capabilitiesas a function of scale; e.g., solver LMs with diverse sizes give significantperformance improvement with our solver-agnostic decomposition technique.Exhaustive ablation studies evince the superiority of our modular finetuningtechnique over exorbitantly large decomposer LLMs, based on prompting alone."
Knowledge-Augmented Large Language Models for Personalized Contextual  Query Suggestion,"['Jinheon Baek', 'Nirupama Chandrasekaran', 'Silviu Cucerzan', 'Allen herring', 'Sujay Kumar Jauhar']",http://arxiv.org/pdf/2311.06318v1.pdf,2023-11-10,"['cs.ir', 'cs.ai', 'cs.cl', 'cs.lg']","  Large Language Models (LLMs) excel at tackling various natural languagetasks. However, due to the significant costs involved in re-training orfine-tuning them, they remain largely static and difficult to personalize.Nevertheless, a variety of applications could benefit from generations that aretailored to users' preferences, goals, and knowledge. Among them is web search,where knowing what a user is trying to accomplish, what they care about, andwhat they know can lead to improved search experiences. In this work, wepropose a novel and general approach that augments an LLM with relevant contextfrom users' interaction histories with a search engine in order to personalizeits outputs. Specifically, we construct an entity-centric knowledge store foreach user based on their search and browsing activities on the web, which isthen leveraged to provide contextually relevant LLM prompt augmentations. Thisknowledge store is light-weight, since it only produces user-specific aggregateprojections of interests and knowledge onto public knowledge graphs, andleverages existing search log infrastructure, thereby mitigating the privacy,compliance, and scalability concerns associated with building deep userprofiles for personalization. We then validate our approach on the task ofcontextual query suggestion, which requires understanding not only the user'scurrent search context but also what they historically know and care about.Through a number of experiments based on human evaluation, we show that ourapproach is significantly better than several other LLM-powered baselines,generating query suggestions that are contextually more relevant, personalized,and useful."
Hallucination-minimized Data-to-answer Framework for Financial  Decision-makers,"['Sohini Roychowdhury', 'Andres Alvarez', 'Brian Moore', 'Marko Krema', 'Maria Paz Gelpi', 'Federico Martin Rodriguez', 'Angel Rodriguez', 'Jose Ramon Cabrejas', 'Pablo Martinez Serrano', 'Punit Agrawal', 'Arijit Mukherjee']",http://arxiv.org/pdf/2311.07592v1.pdf,2023-11-09,"['cs.cl', 'cs.ai', 'cs.ir']","  Large Language Models (LLMs) have been applied to build several automationand personalized question-answering prototypes so far. However, scaling suchprototypes to robust products with minimized hallucinations or fake responsesstill remains an open challenge, especially in niche data-table heavy domainssuch as financial decision making. In this work, we present a novelLangchain-based framework that transforms data tables into hierarchical textualdata chunks to enable a wide variety of actionable question answering. First,the user-queries are classified by intention followed by automated retrieval ofthe most relevant data chunks to generate customized LLM prompts per query.Next, the custom prompts and their responses undergo multi-metric scoring toassess for hallucinations and response confidence. The proposed system isoptimized with user-query intention classification, advanced prompting, datascaling capabilities and it achieves over 90% confidence scores for a varietyof user-queries responses ranging from {What, Where, Why, How, predict, trend,anomalies, exceptions} that are crucial for financial decision makingapplications. The proposed data to answers framework can be extended to otheranalytical domains such as sales and payroll to ensure optimal hallucinationcontrol guardrails."
Interpreting User Requests in the Context of Natural Language Standing  Instructions,"['Nikita Moghe', 'Patrick Xia', 'Jacob Andreas', 'Jason Eisner', 'Benjamin Van Durme', 'Harsh Jhamtani']",http://arxiv.org/pdf/2311.09796v1.pdf,2023-11-16,"['cs.cl', 'cs.ai']","  Users of natural language interfaces, generally powered by Large LanguageModels (LLMs),often must repeat their preferences each time they make a similarrequest. To alleviate this, we propose including some of a user's preferencesand instructions in natural language -- collectively termed standinginstructions -- as additional context for such interfaces. For example, when auser states I'm hungry, their previously expressed preference for Persian foodwill be automatically added to the LLM prompt, so as to influence the searchfor relevant restaurants. We develop NLSI, a language-to-program datasetconsisting of over 2.4K dialogues spanning 17 domains, where each dialogue ispaired with a user profile (a set of users specific standing instructions) andcorresponding structured representations (API calls). A key challenge in NLSIis to identify which subset of the standing instructions is applicable to agiven dialogue. NLSI contains diverse phenomena, from simple preferences tointerdependent instructions such as triggering a hotel search whenever the useris booking tickets to an event. We conduct experiments on NLSI using promptingwith large language models and various retrieval approaches, achieving amaximum of 44.7% exact match on API prediction. Our results demonstrate thechallenges in identifying the relevant standing instructions and theirinterpretation into API calls."
Latent Skill Discovery for Chain-of-Thought Reasoning,"['Zifan Xu', 'Haozhu Wang', 'Dmitriy Bespalov', 'Peter Stone', 'Yanjun Qi']",http://arxiv.org/pdf/2312.04684v1.pdf,2023-12-07,"['cs.cl', 'cs.ai']","  Recent advances in Large Language Models (LLMs) have led to an emergentability of chain-of-thought (CoT) prompting, a prompt reasoning strategy thatadds intermediate rationale steps between questions and answers to constructprompts. Conditioned on these prompts, LLMs can effectively learn in context togenerate rationales that lead to more accurate answers than when answering thesame question directly. To design LLM prompts, one important setting, calleddemonstration selection, considers selecting demonstrations from an examplebank. Existing methods use various heuristics for this selection, but for CoTprompting, which involves unique rationales, it is essential to base theselection upon the intrinsic skills that CoT rationales need, for instance, theskills of addition or subtraction for math word problems.  To address this requirement, we introduce a novel approach named ReasoningSkill Discovery (RSD) that use unsupervised learning to create a latent spacerepresentation of rationales, called a reasoning skill. Simultaneously, RSDlearns a reasoning policy to determine the required reasoning skill for a givenquestion. This can then guide the selection of examples that demonstrate therequired reasoning skills. Our approach offers several desirable properties: itis (1) theoretically grounded, (2) sample-efficient, requiring no LLM inferenceor manual prompt design, and (3) LLM-agnostic. Empirically, RSD outperformsexisting methods by up to 6% in terms of the answer accuracy across multiplereasoning tasks."
Faithful Persona-based Conversational Dataset Generation with Large  Language Models,"['Pegah Jandaghi', 'XiangHai Sheng', 'Xinyi Bai', 'Jay Pujara', 'Hakim Sidahmed']",http://arxiv.org/pdf/2312.10007v1.pdf,2023-12-15,"['cs.cl', 'cs.lg']","  High-quality conversational datasets are essential for developing AI modelsthat can communicate with users. One way to foster deeper interactions betweena chatbot and its user is through personas, aspects of the user's characterthat provide insights into their personality, motivations, and behaviors.Training Natural Language Processing (NLP) models on a diverse andcomprehensive persona-based dataset can lead to conversational models thatcreate a deeper connection with the user, and maintain their engagement. Inthis paper, we leverage the power of Large Language Models (LLMs) to create alarge, high-quality conversational dataset from a seed dataset. We propose aGenerator-Critic architecture framework to expand the initial dataset, whileimproving the quality of its conversations. The Generator is an LLM prompted tooutput conversations. The Critic consists of a mixture of expert LLMs thatcontrol the quality of the generated conversations. These experts select thebest generated conversations, which we then use to improve the Generator. Werelease Synthetic-Persona-Chat, consisting of 20k conversations seeded fromPersona-Chat. We evaluate the quality of Synthetic-Persona-Chat and ourgeneration framework on different dimensions through extensive experiments, andobserve that the losing rate of Synthetic-Persona-Chat against Persona-Chatduring Turing test decreases from 17.2% to 8.8% over three iterations."
Universal Fuzzing via Large Language Models,"['Chunqiu Steven Xia', 'Matteo Paltenghi', 'Jia Le Tian', 'Michael Pradel', 'Lingming Zhang']",http://arxiv.org/pdf/2308.04748v1.pdf,2023-08-09,"['cs.se', 'cs.lg']","  Fuzzing has achieved tremendous success in discovering bugs andvulnerabilities in various software systems. Systems under test (SUTs) thattake in programming or formal language as inputs, e.g., compilers, runtimeengines, constraint solvers, and software libraries with accessible APIs, areespecially important as they are fundamental building blocks of softwaredevelopment. However, existing fuzzers for such systems often target a specificlanguage, and thus cannot be easily applied to other languages or even otherversions of the same language. Moreover, the inputs generated by existingfuzzers are often limited to specific features of the input language, and thuscan hardly reveal bugs related to other or new features. This paper presentsFuzz4All, the first fuzzer that is universal in the sense that it can targetmany different input languages and many different features of these languages.The key idea behind Fuzz4All is to leverage large language models (LLMs) as aninput generation and mutation engine, which enables the approach to producediverse and realistic inputs for any practically relevant language. To realizethis potential, we present a novel autoprompting technique, which creates LLMprompts that are wellsuited for fuzzing, and a novel LLM-powered fuzzing loop,which iteratively updates the prompt to create new fuzzing inputs. We evaluateFuzz4All on nine systems under test that take in six different languages (C,C++, Go, SMT2, Java and Python) as inputs. The evaluation shows, across all sixlanguages, that universal fuzzing achieves higher coverage than existing,language-specific fuzzers. Furthermore, Fuzz4All has identified 76 bugs inwidely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskitquantum computing platform, with 47 bugs already confirmed by developers aspreviously unknown."
AI Chains: Transparent and Controllable Human-AI Interaction by Chaining  Large Language Model Prompts,"['Tongshuang Wu', 'Michael Terry', 'Carrie J. Cai']",http://arxiv.org/pdf/2110.01691v3.pdf,2021-10-04,"['cs.hc', 'cs.cl']","  Although large language models (LLMs) have demonstrated impressive potentialon simple tasks, their breadth of scope, lack of transparency, and insufficientcontrollability can make them less effective when assisting humans on morecomplex tasks. In response, we introduce the concept of Chaining LLM stepstogether, where the output of one step becomes the input for the next, thusaggregating the gains per step. We first define a set of LLM primitiveoperations useful for Chain construction, then present an interactive systemwhere users can modify these Chains, along with their intermediate results, ina modular way. In a 20-person user study, we found that Chaining not onlyimproved the quality of task outcomes, but also significantly enhanced systemtransparency, controllability, and sense of collaboration. Additionally, we sawthat users developed new ways of interacting with LLMs through Chains: theyleveraged sub-tasks to calibrate model expectations, compared and contrastedalternative strategies by observing parallel downstream effects, and debuggedunexpected model outputs by ""unit-testing"" sub-components of a Chain. In twocase studies, we further explore how LLM Chains may be used in futureapplications"
PromptChainer: Chaining Large Language Model Prompts through Visual  Programming,"['Tongshuang Wu', 'Ellen Jiang', 'Aaron Donsbach', 'Jeff Gray', 'Alejandra Molina', 'Michael Terry', 'Carrie J Cai']",http://arxiv.org/pdf/2203.06566v1.pdf,2022-03-13,['cs.hc'],"  While LLMs can effectively help prototype single ML functionalities, manyreal-world applications involve complex tasks that cannot be easily handled viaa single run of an LLM. Recent work has found that chaining multiple LLM runstogether (with the output of one step being the input to the next) can helpusers accomplish these more complex tasks, and in a way that is perceived to bemore transparent and controllable. However, it remains unknown what users needwhen authoring their own LLM chains -- a key step for lowering the barriers fornon-AI-experts to prototype AI-infused applications. In this work, we explorethe LLM chain authoring process. We conclude from pilot studies find thatchaining requires careful scaffolding for transforming intermediate nodeoutputs, as well as debugging the chain at multiple granularities; to help withthese needs, we designed PromptChainer, an interactive interface for visuallyprogramming chains. Through case studies with four people, we show thatPromptChainer supports building prototypes for a range of applications, andconclude with open questions on scaling chains to complex tasks, and supportinglow-fi chain prototyping."
Few-shot Reranking for Multi-hop QA via Language Model Prompting,"['Muhammad Khalifa', 'Lajanugen Logeswaran', 'Moontae Lee', 'Honglak Lee', 'Lu Wang']",http://arxiv.org/pdf/2205.12650v3.pdf,2022-05-25,"['cs.cl', 'cs.ir']","  We study few-shot reranking for multi-hop QA with open-domain questions. Toalleviate the need for a large number of labeled question-document pairs forretriever training, we propose PromptRank, which relies on large languagemodels prompting for multi-hop path reranking. PromptRank first constructs aninstruction-based prompt that includes a candidate document path and thencomputes the relevance score between a given question and the path based on theconditional likelihood of the question given the path prompt according to alanguage model. PromptRank yields strong retrieval performance on HotpotQA withonly 128 training examples compared to state-of-the-art methods trained onthousands of examples -- 73.6 recall@10 by PromptRank vs. 77.8 by PathRetrieverand 77.5 by multi-hop dense retrieval. Code available athttps://github.com/mukhal/PromptRank"
Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency,"['Lingfeng Shen', 'Weiting Tan', 'Boyuan Zheng', 'Daniel Khashabi']",http://arxiv.org/pdf/2305.10713v2.pdf,2023-05-18,"['cs.cl', 'cs.lg']","  With growing capabilities of large language models, prompting them has becomethe dominant way to access them. This has motivated the development ofstrategies for automatically selecting effective language prompts. In thispaper, we introduce prompt flatness, a new metric to quantify the expectedutility of a language prompt. This metric is inspired by flatnessregularization in statistical learning that quantifies the robustness of themodel towards its parameter perturbations. We provide theoretical foundationsfor this metric and its relationship with other prompt selection metrics,providing a comprehensive understanding of existing methods. Empirically, weshow that combining prompt flatness with existing metrics improves bothperformance and sample efficiency. Our metric outperforms the previous promptselection metrics with an average increase of 5% in accuracy and 10% in Pearsoncorrelation across 6 classification benchmarks."
A Monte Carlo Language Model Pipeline for Zero-Shot Sociopolitical Event  Extraction,"['Erica Cai', ""Brendan O'Connor""]",http://arxiv.org/pdf/2305.15051v1.pdf,2023-05-24,['cs.cl'],"  We consider dyadic zero-shot event extraction (EE) to identify actionsbetween pairs of actors. The \emph{zero-shot} setting allows social scientistsor other non-computational researchers to extract any customized,user-specified set of events without training, resulting in a \emph{dyadic}event database, allowing insight into sociopolitical relational dynamics amongactors and the higher level organizations or countries they represent.Unfortunately, we find that current zero-shot EE methods perform poorly for thetask, with issues including word sense ambiguity, modality mismatch, andefficiency. Straightforward application of large language model promptingtypically performs even worse. We address these challenges with a newfine-grained, multi-stage generative question-answer method, using a MonteCarlo approach to exploit and overcome the randomness of generative outputs. Itperforms 90\% fewer queries than a previous approach, with strong performanceon the widely-used Automatic Content Extraction dataset. Finally, we extend ourmethod to extract affiliations of actor arguments and demonstrate our methodand findings on a dyadic international relations case study."
EvalLM: Interactive Evaluation of Large Language Model Prompts on  User-Defined Criteria,"['Tae Soo Kim', 'Yoonjoo Lee', 'Jamin Shin', 'Young-Ho Kim', 'Juho Kim']",http://arxiv.org/pdf/2309.13633v1.pdf,2023-09-24,"['cs.hc', 'cs.ai', 'cs.cl']","  By simply composing prompts, developers can prototype novel generativeapplications with Large Language Models (LLMs). To refine prototypes intoproducts, however, developers must iteratively revise prompts by evaluatingoutputs to diagnose weaknesses. Formative interviews (N=8) revealed thatdevelopers invest significant effort in manually evaluating outputs as theyassess context-specific and subjective criteria. We present EvalLM, aninteractive system for iteratively refining prompts by evaluating multipleoutputs on user-defined criteria. By describing criteria in natural language,users can employ the system's LLM-based evaluator to get an overview of whereprompts excel or fail, and improve these based on the evaluator's feedback. Acomparative study (N=12) showed that EvalLM, when compared to manualevaluation, helped participants compose more diverse criteria, examine twice asmany outputs, and reach satisfactory prompts with 59% fewer revisions. Beyondprompts, our work can be extended to augment model evaluation and alignment inspecific application contexts."
Terminology-Aware Translation with Constrained Decoding and Large  Language Model Prompting,"['Nikolay Bogoychev', 'Pinzhen Chen']",http://arxiv.org/pdf/2310.05824v1.pdf,2023-10-09,['cs.cl'],"  Terminology correctness is important in the downstream application of machinetranslation, and a prevalent way to ensure this is to inject terminologyconstraints into a translation system. In our submission to the WMT 2023terminology translation task, we adopt a translate-then-refine approach whichcan be domain-independent and requires minimal manual efforts. We annotaterandom source words with pseudo-terminology translations obtained from wordalignment to first train a terminology-aware model. Further, we explore twopost-processing methods. First, we use an alignment process to discover whethera terminology constraint has been violated, and if so, we re-decode with theviolating word negatively constrained. Alternatively, we leverage a largelanguage model to refine a hypothesis by providing it with terminologyconstraints. Results show that our terminology-aware model learns toincorporate terminologies effectively, and the large language model refinementprocess can further improve terminology recall."
Multi-Step Dialogue Workflow Action Prediction,"['Ramya Ramakrishnan', 'Ethan Elenberg', 'Hashan Narangodage', 'Ryan McDonald']",http://arxiv.org/pdf/2311.09593v1.pdf,2023-11-16,"['cs.cl', 'cs.ai']","  In task-oriented dialogue, a system often needs to follow a sequence ofactions, called a workflow, that complies with a set of guidelines in order tocomplete a task. In this paper, we propose the novel problem of multi-stepworkflow action prediction, in which the system predicts multiple futureworkflow actions. Accurate prediction of multiple steps allows for multi-turnautomation, which can free up time to focus on more complex tasks. We proposethree modeling approaches that are simple to implement yet lead to more actionautomation: 1) fine-tuning on a training dataset, 2) few-shot in-contextlearning leveraging retrieval and large language model prompting, and 3)zero-shot graph traversal, which aggregates historical action sequences into agraph for prediction. We show that multi-step action prediction producesfeatures that improve accuracy on downstream dialogue tasks like predictingtask success, and can increase automation of steps by 20% without requiring asmuch feedback from a human overseeing the system."
Prompter: Utilizing Large Language Model Prompting for a Data Efficient  Embodied Instruction Following,"['Yuki Inoue', 'Hiroki Ohashi']",http://arxiv.org/pdf/2211.03267v1.pdf,2022-11-07,"['cs.ro', 'cs.cv']","  Embodied Instruction Following (EIF) studies how mobile manipulator robotsshould be controlled to accomplish long-horizon tasks specified by naturallanguage instructions. While most research on EIF are conducted in simulators,the ultimate goal of the field is to deploy the agents in real life. As such,it is important to minimize the data cost required for training an agent, tohelp the transition from sim to real. However, many studies only focus on theperformance and overlook the data cost -- modules that require separatetraining on extra data are often introduced without a consideration ondeployability. In this work, we propose FILM++ which extends the existing workFILM with modifications that do not require extra data. While all data-drivenmodules are kept constant, FILM++ more than doubles FILM's performance.Furthermore, we propose Prompter, which replaces FILM++'s semantic searchmodule with language model prompting. Unlike FILM++'s implementation thatrequires training on extra sets of data, no training is needed for ourprompting based implementation while achieving better or at least comparableperformance. Prompter achieves 42.64% and 45.72% on the ALFRED benchmark withhigh-level instructions only and with step-by-step instructions, respectively,outperforming the previous state of the art by 6.57% and 10.31%."
FIRE: Food Image to REcipe generation,"['Prateek Chhikara', 'Dhiraj Chaurasia', 'Yifan Jiang', 'Omkar Masur', 'Filip Ilievski']",http://arxiv.org/pdf/2308.14391v1.pdf,2023-08-28,"['cs.cv', 'cs.cl']","  Food computing has emerged as a prominent multidisciplinary field of researchin recent years. An ambitious goal of food computing is to develop end-to-endintelligent systems capable of autonomously producing recipe information for afood image. Current image-to-recipe methods are retrieval-based and theirsuccess depends heavily on the dataset size and diversity, as well as thequality of learned embeddings. Meanwhile, the emergence of powerfulattention-based vision and language models presents a promising avenue foraccurate and generalizable recipe generation, which has yet to be extensivelyexplored. This paper proposes FIRE, a novel multimodal methodology tailored torecipe generation in the food computing domain, which generates the food title,ingredients, and cooking instructions based on input food images. FIREleverages the BLIP model to generate titles, utilizes a Vision Transformer witha decoder for ingredient extraction, and employs the T5 model to generaterecipes incorporating titles and ingredients as inputs. We showcase twopractical applications that can benefit from integrating FIRE with largelanguage model prompting: recipe customization to fit recipes to userpreferences and recipe-to-code transformation to enable automated cookingprocesses. Our experimental findings validate the efficacy of our proposedapproach, underscoring its potential for future advancements and widespreadadoption in food computing."
Large language models can accurately predict searcher preferences,"['Paul Thomas', 'Seth Spielman', 'Nick Craswell', 'Bhaskar Mitra']",http://arxiv.org/pdf/2309.10621v1.pdf,2023-09-19,"['cs.ir', 'cs.ai', 'cs.cl', 'cs.lg']","  Relevance labels, which indicate whether a search result is valuable to asearcher, are key to evaluating and optimising search systems. The best way tocapture the true preferences of users is to ask them for their careful feedbackon which results would be useful, but this approach does not scale to produce alarge number of labels. Getting relevance labels at scale is usually done withthird-party labellers, who judge on behalf of the user, but there is a risk oflow-quality data if the labeller doesn't understand user needs. To improvequality, one standard approach is to study real users through interviews, userstudies and direct feedback, find areas where labels are systematicallydisagreeing with users, then educate labellers about user needs through judgingguidelines, training and monitoring. This paper introduces an alternateapproach for improving label quality. It takes careful feedback from realusers, which by definition is the highest-quality first-party gold data thatcan be derived, and develops an large language model prompt that agrees withthat data.  We present ideas and observations from deploying language models forlarge-scale relevance labelling at Bing, and illustrate with data from TREC. Wehave found large language models can be effective, with accuracy as good ashuman labellers and similar capability to pick the hardest queries, best runs,and best groups. Systematic changes to the prompts make a difference inaccuracy, but so too do simple paraphrases. To measure agreement with realsearchers needs high-quality ``gold'' labels, but with these we find thatmodels produce better labels than third-party workers, for a fraction of thecost, and these labels let us train notably better rankers."
Meta-in-context learning in large language models,"['Julian Coda-Forno', 'Marcel Binz', 'Zeynep Akata', 'Matthew Botvinick', 'Jane X. Wang', 'Eric Schulz']",http://arxiv.org/pdf/2305.12907v1.pdf,2023-05-22,"['cs.cl', 'cs.ai', 'cs.lg']","  Large language models have shown tremendous performance in a variety oftasks. In-context learning -- the ability to improve at a task after beingprovided with a number of demonstrations -- is seen as one of the maincontributors to their success. In the present paper, we demonstrate that thein-context learning abilities of large language models can be recursivelyimproved via in-context learning itself. We coin this phenomenonmeta-in-context learning. Looking at two idealized domains, a one-dimensionalregression task and a two-armed bandit task, we show that meta-in-contextlearning adaptively reshapes a large language model's priors over expectedtasks. Furthermore, we find that meta-in-context learning modifies thein-context learning strategies of such models. Finally, we extend our approachto a benchmark of real-world regression problems where we observe competitiveperformance to traditional learning algorithms. Taken together, our workimproves our understanding of in-context learning and paves the way towardadapting large language models to the environment they are applied purelythrough meta-in-context learning rather than traditional finetuning."
How does representation impact in-context learning: A exploration on a  synthetic task,"['Jingwen Fu', 'Tao Yang', 'Yuwang Wang', 'Yan Lu', 'Nanning Zheng']",http://arxiv.org/pdf/2309.06054v1.pdf,2023-09-12,"['cs.lg', 'cs.cl', 'cs.cv']","  In-context learning, i.e., learning from in-context samples, is an impressiveability of Transformer. However, the mechanism driving the in-context learningis not yet fully understood. In this study, we aim to investigate from anunderexplored perspective of representation learning. The representation ismore complex for in-context learning senario, where the representation can beimpacted by both model weights and in-context samples. We refer the above twoconceptually aspects of representation as in-weight component and in-contextcomponent, respectively. To study how the two components affect in-contextlearning capabilities, we construct a novel synthetic task, making it possibleto device two probes, in-weights probe and in-context probe, to evaluate thetwo components, respectively. We demonstrate that the goodness of in-contextcomponent is highly related to the in-context learning performance, whichindicates the entanglement between in-context learning and representationlearning. Furthermore, we find that a good in-weights component can actuallybenefit the learning of the in-context component, indicating that in-weightslearning should be the foundation of in-context learning. To further understandthe the in-context learning mechanism and importance of the in-weightscomponent, we proof by construction that a simple Transformer, which usespattern matching and copy-past mechanism to perform in-context learning, canmatch the in-context learning performance with more complex, best tunedTransformer under the perfect in-weights component assumption. In short, thosediscoveries from representation learning perspective shed light on newapproaches to improve the in-context capacity."
MetaVL: Transferring In-Context Learning Ability From Language Models to  Vision-Language Models,"['Masoud Monajatipoor', 'Liunian Harold Li', 'Mozhdeh Rouhsedaghat', 'Lin F. Yang', 'Kai-Wei Chang']",http://arxiv.org/pdf/2306.01311v1.pdf,2023-06-02,['cs.cl'],"  Large-scale language models have shown the ability to adapt to a new task viaconditioning on a few demonstrations (i.e., in-context learning). However, inthe vision-language domain, most large-scale pre-trained vision-language (VL)models do not possess the ability to conduct in-context learning. How can weenable in-context learning for VL models? In this paper, we study aninteresting hypothesis: can we transfer the in-context learning ability fromthe language domain to VL domain? Specifically, we first meta-trains a languagemodel to perform in-context learning on NLP tasks (as in MetaICL); then wetransfer this model to perform VL tasks by attaching a visual encoder. Ourexperiments suggest that indeed in-context learning ability can be transferredcross modalities: our model considerably improves the in-context learningcapability on VL tasks and can even compensate for the size of the modelsignificantly. On VQA, OK-VQA, and GQA, our method could outperform thebaseline model while having 20 times fewer parameters."
Multimodal Prompt Learning for Product Title Generation with Extremely  Limited Labels,"['Bang Yang', 'Fenglin Liu', 'Zheng Li', 'Qingyu Yin', 'Chenyu You', 'Bing Yin', 'Yuexian Zou']",http://arxiv.org/pdf/2307.01969v1.pdf,2023-07-05,['cs.cv'],"  Generating an informative and attractive title for the product is a crucialtask for e-commerce. Most existing works follow the standard multimodal naturallanguage generation approaches, e.g., image captioning, and employ the largescale of human-labelled datasets to train desirable models. However, for novelproducts, especially in a different domain, there are few existing labelleddata. In this paper, we propose a prompt-based approach, i.e., the MultimodalPrompt Learning framework, to accurately and efficiently generate titles fornovel products with limited labels. We observe that the core challenges ofnovel product title generation are the understanding of novel productcharacteristics and the generation of titles in a novel writing style. To thisend, we build a set of multimodal prompts from different modalities to preservethe corresponding characteristics and writing styles of novel products. As aresult, with extremely limited labels for training, the proposed method canretrieve the multimodal prompts to generate desirable titles for novelproducts. The experiments and analyses are conducted on five novel productcategories under both the in-domain and out-of-domain experimental settings.The results show that, with only 1% of downstream labelled data for training,our proposed approach achieves the best few-shot results and even achievescompetitive results with fully-supervised methods trained on 100% of trainingdata; With the full labelled data for training, our method achievesstate-of-the-art results."
Mastering Robot Manipulation with Multimodal Prompts through Pretraining  and Multi-task Fine-tuning,"['Jiachen Li', 'Qiaozi Gao', 'Michael Johnston', 'Xiaofeng Gao', 'Xuehai He', 'Suhaila Shakiah', 'Hangjie Shi', 'Reza Ghanadan', 'William Yang Wang']",http://arxiv.org/pdf/2310.09676v1.pdf,2023-10-14,"['cs.ro', 'cs.ai']","  Prompt-based learning has been demonstrated as a compelling paradigmcontributing to large language models' tremendous success (LLMs). Inspired bytheir success in language tasks, existing research has leveraged LLMs inembodied instruction following and task planning. However, not much attentionhas been paid to embodied tasks with multimodal prompts, combining visionsignals with text descriptions. This type of task poses a major challenge torobots' capability to understand the interconnection and complementaritybetween vision and language signals. In this work, we introduce an effectiveframework that learns a policy to perform robot manipulation with multimodalprompts from multi-task expert trajectories. Our methods consist of a two-stagetraining pipeline that performs inverse dynamics pretraining and multi-taskfinetuning. To facilitate multimodal understanding, we design our multimodalprompt encoder by augmenting a pretrained LM with a residual connection to thevisual input and model the dependencies among action dimensions. Empirically,we evaluate the efficacy of our method on the VIMA-BENCH and establish a newstate-of-the-art (10% improvement in success rate). Moreover, we demonstratethat our model exhibits remarkable in-context learning ability."
Few-shot Joint Multimodal Aspect-Sentiment Analysis Based on Generative  Multimodal Prompt,"['Xiaocui Yang', 'Shi Feng', 'Daling Wang', 'Sun Qi', 'Wenfang Wu', 'Yifei Zhang', 'Pengfei Hong', 'Soujanya Poria']",http://arxiv.org/pdf/2305.10169v2.pdf,2023-05-17,['cs.mm'],"  We have witnessed the rapid proliferation of multimodal data on numeroussocial media platforms. Conventional studies typically require massive labeleddata to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA).However, collecting and annotating fine-grained multimodal data for MABSA istough. To alleviate the above issue, we perform three MABSA-related tasks withquite a small number of labeled multimodal samples. We first build diverse andcomprehensive multimodal few-shot datasets according to the data distribution.To capture the specific prompt for each aspect term in a few-shot scenario, wepropose a novel Generative Multimodal Prompt (GMP) model for MABSA, whichincludes the Multimodal Encoder module and the N-Stream Decoders module. Wefurther introduce a subtask to predict the number of aspect terms in eachinstance to construct the multimodal prompt. Extensive experiments on twodatasets demonstrate that our approach outperforms strong baselines on twoMABSA-related tasks in the few-shot setting."
VIMA: General Robot Manipulation with Multimodal Prompts,"['Yunfan Jiang', 'Agrim Gupta', 'Zichen Zhang', 'Guanzhi Wang', 'Yongqiang Dou', 'Yanjun Chen', 'Li Fei-Fei', 'Anima Anandkumar', 'Yuke Zhu', 'Linxi Fan']",http://arxiv.org/pdf/2210.03094v2.pdf,2022-10-06,"['cs.ro', 'cs.ai', 'cs.lg']","  Prompt-based learning has emerged as a successful paradigm in naturallanguage processing, where a single general-purpose language model can beinstructed to perform any task specified by input prompts. Yet taskspecification in robotics comes in various forms, such as imitating one-shotdemonstrations, following language instructions, and reaching visual goals.They are often considered different tasks and tackled by specialized models. Weshow that a wide spectrum of robot manipulation tasks can be expressed withmultimodal prompts, interleaving textual and visual tokens. Accordingly, wedevelop a new simulation benchmark that consists of thousands ofprocedurally-generated tabletop tasks with multimodal prompts, 600K+ experttrajectories for imitation learning, and a four-level evaluation protocol forsystematic generalization. We design a transformer-based robot agent, VIMA,that processes these prompts and outputs motor actions autoregressively. VIMAfeatures a recipe that achieves strong model scalability and data efficiency.It outperforms alternative designs in the hardest zero-shot generalizationsetting by up to $2.9\times$ task success rate given the same training data.With $10\times$ less training data, VIMA still performs $2.7\times$ better thanthe best competing variant. Code and video demos are available athttps://vimalabs.github.io/"
Delving into Multimodal Prompting for Fine-grained Visual Classification,"['Xin Jiang', 'Hao Tang', 'Junyao Gao', 'Xiaoyu Du', 'Shengfeng He', 'Zechao Li']",http://arxiv.org/pdf/2309.08912v2.pdf,2023-09-16,"['cs.cv', 'cs.mm']","  Fine-grained visual classification (FGVC) involves categorizing finesubdivisions within a broader category, which poses challenges due to subtleinter-class discrepancies and large intra-class variations. However, prevailingapproaches primarily focus on uni-modal visual concepts. Recent advancements inpre-trained vision-language models have demonstrated remarkable performance invarious high-level vision tasks, yet the applicability of such models to FGVCtasks remains uncertain. In this paper, we aim to fully exploit thecapabilities of cross-modal description to tackle FGVC tasks and propose anovel multimodal prompting solution, denoted as MP-FGVC, based on thecontrastive language-image pertaining (CLIP) model. Our MP-FGVC comprises amultimodal prompts scheme and a multimodal adaptation scheme. The formerincludes Subcategory-specific Vision Prompt (SsVP) and Discrepancy-aware TextPrompt (DaTP), which explicitly highlights the subcategory-specificdiscrepancies from the perspectives of both vision and language. The latteraligns the vision and text prompting elements in a common semantic space,facilitating cross-modal collaborative reasoning through a Vision-LanguageFusion Module (VLFM) for further improvement on FGVC. Moreover, we tailor atwo-stage optimization strategy for MP-FGVC to fully leverage the pre-trainedCLIP model and expedite efficient adaptation for FGVC. Extensive experimentsconducted on four FGVC datasets demonstrate the effectiveness of our MP-FGVC."
Multimodal Prompt Transformer with Hybrid Contrastive Learning for  Emotion Recognition in Conversation,"['Shihao Zou', 'Xianying Huang', 'Xudong Shen']",http://arxiv.org/pdf/2310.04456v1.pdf,2023-10-04,"['cs.cl', 'cs.sd', 'eess.as']","  Emotion Recognition in Conversation (ERC) plays an important role in drivingthe development of human-machine interaction. Emotions can exist in multiplemodalities, and multimodal ERC mainly faces two problems: (1) the noise problemin the cross-modal information fusion process, and (2) the prediction problemof less sample emotion labels that are semantically similar but differentcategories. To address these issues and fully utilize the features of eachmodality, we adopted the following strategies: first, deep emotion cuesextraction was performed on modalities with strong representation ability, andfeature filters were designed as multimodal prompt information for modalitieswith weak representation ability. Then, we designed a Multimodal PromptTransformer (MPT) to perform cross-modal information fusion. MPT embedsmultimodal fusion information into each attention layer of the Transformer,allowing prompt information to participate in encoding textual features andbeing fused with multi-level textual information to obtain better multimodalfusion features. Finally, we used the Hybrid Contrastive Learning (HCL)strategy to optimize the model's ability to handle labels with few samples.This strategy uses unsupervised contrastive learning to improve therepresentation ability of multimodal fusion and supervised contrastive learningto mine the information of labels with few samples. Experimental results showthat our proposed model outperforms state-of-the-art models in ERC on twobenchmark datasets."
2nd Place Winning Solution for the CVPR2023 Visual Anomaly and Novelty  Detection Challenge: Multimodal Prompting for Data-centric Anomaly Detection,"['Yunkang Cao', 'Xiaohao Xu', 'Chen Sun', 'Yuqi Cheng', 'Liang Gao', 'Weiming Shen']",http://arxiv.org/pdf/2306.09067v2.pdf,2023-06-15,['cs.cv'],"  This technical report introduces the winning solution of the team Segment AnyAnomaly for the CVPR2023 Visual Anomaly and Novelty Detection (VAND) challenge.Going beyond uni-modal prompt, e.g., language prompt, we present a novelframework, i.e., Segment Any Anomaly + (SAA$+$), for zero-shot anomalysegmentation with multi-modal prompts for the regularization of cascaded modernfoundation models. Inspired by the great zero-shot generalization ability offoundation models like Segment Anything, we first explore their assembly (SAA)to leverage diverse multi-modal prior knowledge for anomaly localization.Subsequently, we further introduce multimodal prompts (SAA$+$) derived fromdomain expert knowledge and target image context to enable the non-parameteradaptation of foundation models to anomaly segmentation. The proposed SAA$+$model achieves state-of-the-art performance on several anomaly segmentationbenchmarks, including VisA and MVTec-AD, in the zero-shot setting. We willrelease the code of our winning solution for the CVPR2023 VAN."
Multimodal Prompt Retrieval for Generative Visual Question Answering,"['Timothy Ossowski', 'Junjie Hu']",http://arxiv.org/pdf/2306.17675v1.pdf,2023-06-30,"['cs.cv', 'cs.ai']","  Recent years have witnessed impressive results of pre-trained vision-languagemodels on knowledge-intensive tasks such as visual question answering (VQA).Despite the recent advances in VQA, existing methods mainly adopt adiscriminative formulation that predicts answers within a pre-defined labelset, leading to easy overfitting on low-resource domains with limited labeleddata (e.g., medicine) and poor generalization under domain shift to anotherdataset. To tackle this limitation, we propose a novel generative modelenhanced by multimodal prompt retrieval (MPR) that integrates retrieved promptsand multimodal features to generate answers in free text. Our generative modelenables rapid zero-shot dataset adaptation to unseen data distributions andopen-set answer labels across datasets. Our experiments on medical VQA tasksshow that MPR outperforms its non-retrieval counterpart by up to 30% accuracypoints in a few-shot domain adaptation setting."
"Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and  Fidelity for All-in-One Image Restoration","['Yuang Ai', 'Huaibo Huang', 'Xiaoqiang Zhou', 'Jiexiang Wang', 'Ran He']",http://arxiv.org/pdf/2312.02918v1.pdf,2023-12-05,['cs.cv'],"  Despite substantial progress, all-in-one image restoration (IR) grapples withpersistent challenges in handling intricate real-world degradations. This paperintroduces MPerceiver: a novel multimodal prompt learning approach thatharnesses Stable Diffusion (SD) priors to enhance adaptiveness,generalizability and fidelity for all-in-one image restoration. Specifically,we develop a dual-branch module to master two types of SD prompts: textual forholistic representation and visual for multiscale detail representation. Bothprompts are dynamically adjusted by degradation predictions from the CLIP imageencoder, enabling adaptive responses to diverse unknown degradations. Moreover,a plug-in detail refinement module improves restoration fidelity via directencoder-to-decoder information transformation. To assess our method, MPerceiveris trained on 9 tasks for all-in-one IR and outperforms state-of-the-arttask-specific methods across most tasks. Post multitask pre-training,MPerceiver attains a generalized representation in low-level vision, exhibitingremarkable zero-shot and few-shot capabilities in unseen tasks. Extensiveexperiments on 16 IR tasks and 26 benchmarks underscore the superiority ofMPerceiver in terms of adaptiveness, generalizability and fidelity."
Zero-Shot and Few-Shot Video Question Answering with Multi-Modal Prompts,"['Deniz Engin', 'Yannis Avrithis']",http://arxiv.org/pdf/2309.15915v1.pdf,2023-09-27,['cs.cv'],"  Recent vision-language models are driven by large-scale pretrained models.However, adapting pretrained models on limited data presents challenges such asoverfitting, catastrophic forgetting, and the cross-modal gap between visionand language. We introduce a parameter-efficient method to address thesechallenges, combining multimodal prompt learning and a transformer-basedmapping network, while keeping the pretrained models frozen. Our experiments onseveral video question answering benchmarks demonstrate the superiority of ourapproach in terms of performance and parameter efficiency on both zero-shot andfew-shot settings. Our code is available at https://engindeniz.github.io/vitis."
Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting,"['Syed Talal Wasim', 'Muzammal Naseer', 'Salman Khan', 'Fahad Shahbaz Khan', 'Mubarak Shah']",http://arxiv.org/pdf/2304.03307v1.pdf,2023-04-06,"['cs.cv', 'eess.iv']","  Adopting contrastive image-text pretrained models like CLIP towards videoclassification has gained attention due to its cost-effectiveness andcompetitive performance. However, recent works in this area face a trade-off.Finetuning the pretrained model to achieve strong supervised performanceresults in low zero-shot generalization. Similarly, freezing the backbone toretain zero-shot capability causes significant drop in supervised accuracy.Because of this, recent works in literature typically train separate models forsupervised and zero-shot action recognition. In this work, we propose amultimodal prompt learning scheme that works to balance the supervised andzero-shot performance under a single unified training. Our prompting approachon the vision side caters for three aspects: 1) Global video-level prompts tomodel the data distribution; 2) Local frame-level prompts to provide per-framediscriminative conditioning; and 3) a summary prompt to extract a condensedvideo representation. Additionally, we define a prompting scheme on the textside to augment the textual context. Through this prompting scheme, we canachieve state-of-the-art zero-shot performance on Kinetics-600, HMDB51 andUCF101 while remaining competitive in the supervised setting. By keeping thepretrained backbone frozen, we optimize a much lower number of parameters andretain the existing general representation which helps achieve the strongzero-shot performance. Our codes/models are released athttps://github.com/TalalWasim/Vita-CLIP."
IMProv: Inpainting-based Multimodal Prompting for Computer Vision Tasks,"['Jiarui Xu', 'Yossi Gandelsman', 'Amir Bar', 'Jianwei Yang', 'Jianfeng Gao', 'Trevor Darrell', 'Xiaolong Wang']",http://arxiv.org/pdf/2312.01771v1.pdf,2023-12-04,['cs.cv'],"  In-context learning allows adapting a model to new tasks given a taskdescription at test time. In this paper, we present IMProv - a generative modelthat is able to in-context learn visual tasks from multimodal prompts. Given atextual description of a visual task (e.g. ""Left: input image, Right:foreground segmentation""), a few input-output visual examples, or both, themodel in-context learns to solve it for a new test input. We train a maskedgenerative transformer on a new dataset of figures from computer vision papersand their associated captions, together with a captioned large-scale image-textdataset. During inference time, we prompt the model with text and/or image taskexample(s) and have the model inpaint the corresponding output. We show thattraining our model with text conditioning and scaling the dataset size improvesin-context learning for computer vision tasks by over +10\% AP for ForegroundSegmentation, over +5\% gains in AP for Single Object Detection, and almost20\% lower LPIPS in Colorization. Our empirical results suggest that vision andlanguage prompts are complementary and it is advantageous to use both toachieve better in-context learning performance. Project page is available athttps://jerryxu.net/IMProv ."
Similarity-Aware Multimodal Prompt Learning for Fake News Detection,"['Ye Jiang', 'Xiaomin Yu', 'Yimin Wang', 'Xiaoman Xu', 'Xingyi Song', 'Diana Maynard']",http://arxiv.org/pdf/2304.04187v3.pdf,2023-04-09,['cs.cl'],"  The standard paradigm for fake news detection mainly utilizes textinformation to model the truthfulness of news. However, the discourse of onlinefake news is typically subtle and it requires expert knowledge to use textualinformation to debunk fake news. Recently, studies focusing on multimodal fakenews detection have outperformed text-only methods. Recent approaches utilizingthe pre-trained model to extract unimodal features, or fine-tuning thepre-trained model directly, have become a new paradigm for detecting fake news.Again, this paradigm either requires a large number of training instances, orupdates the entire set of pre-trained model parameters, making real-world fakenews detection impractical. Furthermore, traditional multimodal methods fusethe cross-modal features directly without considering that the uncorrelatedsemantic representation might inject noise into the multimodal features. Thispaper proposes a Similarity-Aware Multimodal Prompt Learning (SAMPLE)framework. First, we incorporate prompt learning into multimodal fake newsdetection. Prompt learning, which only tunes prompts with a frozen languagemodel, can reduce memory usage significantly and achieve comparableperformances, compared with fine-tuning. We analyse three prompt templates witha soft verbalizer to detect fake news. In addition, we introduce thesimilarity-aware fusing method to adaptively fuse the intensity of multimodalrepresentation and mitigate the noise injection via uncorrelated cross-modalfeatures. For evaluation, SAMPLE surpasses the F1 and the accuracies ofprevious works on two benchmark multimodal datasets, demonstrating theeffectiveness of the proposed method in detecting fake news. In addition,SAMPLE also is superior to other approaches regardless of few-shot anddata-rich settings."
Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning,"['Shaohua Dong', 'Yunhe Feng', 'Qing Yang', 'Yan Huang', 'Dongfang Liu', 'Heng Fan']",http://arxiv.org/pdf/2312.00360v2.pdf,2023-12-01,['cs.cv'],"  Multimodal (e.g., RGB-Depth/RGB-Thermal) fusion has shown great potential forimproving semantic segmentation in complex scenes (e.g., indoor/low-lightconditions). Existing approaches often fully fine-tune a dual-branchencoder-decoder framework with a complicated feature fusion strategy forachieving multimodal semantic segmentation, which is training-costly due to themassive parameter updates in feature extraction and fusion. To address thisissue, we propose a surprisingly simple yet effective dual-prompt learningnetwork (dubbed DPLNet) for training-efficient multimodal (e.g., RGB-D/T)semantic segmentation. The core of DPLNet is to directly adapt a frozenpre-trained RGB model to multimodal semantic segmentation, reducing parameterupdates. For this purpose, we present two prompt learning modules, comprisingmultimodal prompt generator (MPG) and multimodal feature adapter (MFA). MPGworks to fuse the features from different modalities in a compact manner and isinserted from shadow to deep stages to generate the multi-level multimodalprompts that are injected into the frozen backbone, while MPG adapts promptedmultimodal features in the frozen backbone for better multimodal semanticsegmentation. Since both the MPG and MFA are lightweight, only a few trainableparameters (3.88M, 4.4% of the pre-trained backbone parameters) are introducedfor multimodal feature fusion and learning. Using a simple decoder (3.27Mparameters), DPLNet achieves new state-of-the-art performance or is on a parwith other complex approaches on four RGB-D/T semantic segmentation datasetswhile satisfying parameter efficiency. Moreover, we show that DPLNet is generaland applicable to other multimodal tasks such as salient object detection andvideo semantic segmentation. Without special design, DPLNet outperforms manycomplicated models. Our code will be available atgithub.com/ShaohuaDong2021/DPLNet."
Draw Your Art Dream: Diverse Digital Art Synthesis with Multimodal  Guided Diffusion,"['Nisha Huang', 'Fan Tang', 'Weiming Dong', 'Changsheng Xu']",http://arxiv.org/pdf/2209.13360v2.pdf,2022-09-27,['cs.cv'],"  Digital art synthesis is receiving increasing attention in the multimediacommunity because of engaging the public with art effectively. Current digitalart synthesis methods usually use single-modality inputs as guidance, therebylimiting the expressiveness of the model and the diversity of generatedresults. To solve this problem, we propose the multimodal guided artworkdiffusion (MGAD) model, which is a diffusion-based digital artwork generationapproach that utilizes multimodal prompts as guidance to control theclassifier-free diffusion model. Additionally, the contrastive language-imagepretraining (CLIP) model is used to unify text and image modalities. Extensiveexperimental results on the quality and quantity of the generated digital artpaintings confirm the effectiveness of the combination of the diffusion modeland multimodal guidance. Code is available athttps://github.com/haha-lisa/MGAD-multimodal-guided-artwork-diffusion."
Multimodal Prompting with Missing Modalities for Visual Recognition,"['Yi-Lun Lee', 'Yi-Hsuan Tsai', 'Wei-Chen Chiu', 'Chen-Yu Lee']",http://arxiv.org/pdf/2303.03369v2.pdf,2023-03-06,['cs.cv'],"  In this paper, we tackle two challenges in multimodal learning for visualrecognition: 1) when missing-modality occurs either during training or testingin real-world situations; and 2) when the computation resources are notavailable to finetune on heavy transformer models. To this end, we propose toutilize prompt learning and mitigate the above two challenges together.Specifically, our modality-missing-aware prompts can be plugged into multimodaltransformers to handle general missing-modality cases, while only requiringless than 1% learnable parameters compared to training the entire model. Wefurther explore the effect of different prompt configurations and analyze therobustness to missing modality. Extensive experiments are conducted to show theeffectiveness of our prompt learning framework that improves the performanceunder various missing-modality cases, while alleviating the requirement ofheavy model re-training. Code is available."
Audio Visual Language Maps for Robot Navigation,"['Chenguang Huang', 'Oier Mees', 'Andy Zeng', 'Wolfram Burgard']",http://arxiv.org/pdf/2303.07522v2.pdf,2023-03-13,"['cs.ro', 'cs.ai', 'cs.cl', 'cs.cv', 'cs.lg']","  While interacting in the world is a multi-sensory experience, many robotscontinue to predominantly rely on visual perception to map and navigate intheir environments. In this work, we propose Audio-Visual-Language Maps(AVLMaps), a unified 3D spatial map representation for storing cross-modalinformation from audio, visual, and language cues. AVLMaps integrate theopen-vocabulary capabilities of multimodal foundation models pre-trained onInternet-scale data by fusing their features into a centralized 3D voxel grid.In the context of navigation, we show that AVLMaps enable robot systems toindex goals in the map based on multimodal queries, e.g., textual descriptions,images, or audio snippets of landmarks. In particular, the addition of audioinformation enables robots to more reliably disambiguate goal locations.Extensive experiments in simulation show that AVLMaps enable zero-shotmultimodal goal navigation from multimodal prompts and provide 50% betterrecall in ambiguous scenarios. These capabilities extend to mobile robots inthe real world - navigating to landmarks referring to visual, audio, andspatial concepts. Videos and code are available at: https://avlmaps.github.io."
Multitask Multimodal Prompted Training for Interactive Embodied Task  Completion,"['Georgios Pantazopoulos', 'Malvina Nikandrou', 'Amit Parekh', 'Bhathiya Hemanthage', 'Arash Eshghi', 'Ioannis Konstas', 'Verena Rieser', 'Oliver Lemon', 'Alessandro Suglia']",http://arxiv.org/pdf/2311.04067v1.pdf,2023-11-07,"['cs.lg', 'cs.ai', 'cs.cv']","  Interactive and embodied tasks pose at least two fundamental challenges toexisting Vision & Language (VL) models, including 1) grounding language intrajectories of actions and observations, and 2) referential disambiguation. Totackle these challenges, we propose an Embodied MultiModal Agent (EMMA): aunified encoder-decoder model that reasons over images and trajectories, andcasts action prediction as multimodal text generation. By unifying all tasks astext generation, EMMA learns a language of actions which facilitates transferacross tasks. Different to previous modular approaches with independentlytrained components, we use a single multitask model where each task contributesto goal completion. EMMA performs on par with similar models on several VLbenchmarks and sets a new state-of-the-art performance (36.81% success rate) onthe Dialog-guided Task Completion (DTC), a benchmark to evaluate dialog-guidedagents in the Alexa Arena"
Modality-invariant and Specific Prompting for Multimodal Human  Perception Understanding,"['Hao Sun', 'Ziwei Niu', 'Xinyao Yu', 'Jiaqing Liu', 'Yen-Wei Chen', 'Lanfen Lin']",http://arxiv.org/pdf/2311.10791v1.pdf,2023-11-17,"['cs.mm', 'cs.hc']","  Understanding human perceptions presents a formidable multimodal challengefor computers, encompassing aspects such as sentiment tendencies and sense ofhumor. While various methods have recently been introduced to extractmodality-invariant and specific information from diverse modalities, with thegoal of enhancing the efficacy of multimodal learning, few works emphasize thisaspect in large language models. In this paper, we introduce a novel multimodalprompt strategy tailored for tuning large language models. Our method assessesthe correlation among different modalities and isolates the modality-invariantand specific components, which are then utilized for prompt tuning. Thisapproach enables large language models to efficiently and effectivelyassimilate information from various modalities. Furthermore, our strategy isdesigned with scalability in mind, allowing the integration of features fromany modality into pretrained large language models. Experimental results onpublic datasets demonstrate that our proposed method significantly improvesperformance compared to previous methods."
MaPLe: Multi-modal Prompt Learning,"['Muhammad Uzair Khattak', 'Hanoona Rasheed', 'Muhammad Maaz', 'Salman Khan', 'Fahad Shahbaz Khan']",http://arxiv.org/pdf/2210.03117v3.pdf,2022-10-06,['cs.cv'],"  Pre-trained vision-language (V-L) models such as CLIP have shown excellentgeneralization ability to downstream tasks. However, they are sensitive to thechoice of input text prompts and require careful selection of prompt templatesto perform well. Inspired by the Natural Language Processing (NLP) literature,recent CLIP adaptation approaches learn prompts as the textual inputs tofine-tune CLIP for downstream tasks. We note that using prompting to adaptrepresentations in a single branch of CLIP (language or vision) is sub-optimalsince it does not allow the flexibility to dynamically adjust bothrepresentation spaces on a downstream task. In this work, we proposeMulti-modal Prompt Learning (MaPLe) for both vision and language branches toimprove alignment between the vision and language representations. Our designpromotes strong coupling between the vision-language prompts to ensure mutualsynergy and discourages learning independent uni-modal solutions. Further, welearn separate prompts across different early stages to progressively model thestage-wise feature relationships to allow rich context learning. We evaluatethe effectiveness of our approach on three representative tasks ofgeneralization to novel classes, new target datasets and unseen domain shifts.Compared with the state-of-the-art method Co-CoOp, MaPLe exhibits favorableperformance and achieves an absolute gain of 3.45% on novel classes and 2.72%on overall harmonic-mean, averaged over 11 diverse image recognition datasets.Our code and pre-trained models are available athttps://github.com/muzairkhattak/multimodal-prompt-learning."
Few-shot Multimodal Sentiment Analysis based on Multimodal Probabilistic  Fusion Prompts,"['Xiaocui Yang', 'Shi Feng', 'Daling Wang', 'Pengfei Hong', 'Soujanya Poria']",http://arxiv.org/pdf/2211.06607v2.pdf,2022-11-12,"['cs.cl', 'cs.mm']","  Multimodal sentiment analysis has gained significant attention due to theproliferation of multimodal content on social media. However, existing studiesin this area rely heavily on large-scale supervised data, which istime-consuming and labor-intensive to collect. Thus, there is a need to addressthe challenge of few-shot multimodal sentiment analysis. To tackle thisproblem, we propose a novel method called Multimodal Probabilistic FusionPrompts (MultiPoint) that leverages diverse cues from different modalities formultimodal sentiment detection in the few-shot scenario. Specifically, we startby introducing a Consistently Distributed Sampling approach called CDS, whichensures that the few-shot dataset has the same category distribution as thefull dataset. Unlike previous approaches primarily using prompts based on thetext modality, we design unified multimodal prompts to reduce discrepanciesbetween different modalities and dynamically incorporate multimodaldemonstrations into the context of each multimodal instance. To enhance themodel's robustness, we introduce a probabilistic fusion method to fuse outputpredictions from multiple diverse prompts for each input. Our extensiveexperiments on six datasets demonstrate the effectiveness of our approach.First, our method outperforms strong baselines in the multimodal few-shotsetting. Furthermore, under the same amount of data (1% of the full dataset),our CDS-based experimental results significantly outperform those based onpreviously sampled datasets constructed from the same number of instances ofeach class."
Multimodal Garment Designer: Human-Centric Latent Diffusion Models for  Fashion Image Editing,"['Alberto Baldrati', 'Davide Morelli', 'Giuseppe Cartella', 'Marcella Cornia', 'Marco Bertini', 'Rita Cucchiara']",http://arxiv.org/pdf/2304.02051v2.pdf,2023-04-04,"['cs.cv', 'cs.ai', 'cs.mm']","  Fashion illustration is used by designers to communicate their vision and tobring the design idea from conceptualization to realization, showing howclothes interact with the human body. In this context, computer vision can thusbe used to improve the fashion design process. Differently from previous worksthat mainly focused on the virtual try-on of garments, we propose the task ofmultimodal-conditioned fashion image editing, guiding the generation ofhuman-centric fashion images by following multimodal prompts, such as text,human body poses, and garment sketches. We tackle this problem by proposing anew architecture based on latent diffusion models, an approach that has notbeen used before in the fashion domain. Given the lack of existing datasetssuitable for the task, we also extend two existing fashion datasets, namelyDress Code and VITON-HD, with multimodal annotations collected in asemi-automatic manner. Experimental results on these new datasets demonstratethe effectiveness of our proposal, both in terms of realism and coherence withthe given multimodal inputs. Source code and collected multimodal annotationsare publicly available at:https://github.com/aimagelab/multimodal-garment-designer."
Parameter-efficient Tuning of Large-scale Multimodal Foundation Model,"['Haixin Wang', 'Xinlong Yang', 'Jianlong Chang', 'Dian Jin', 'Jinan Sun', 'Shikun Zhang', 'Xiao Luo', 'Qi Tian']",http://arxiv.org/pdf/2305.08381v3.pdf,2023-05-15,['cs.cv'],"  Driven by the progress of large-scale pre-training, parameter-efficienttransfer learning has gained immense popularity across different subfields ofArtificial Intelligence. The core is to adapt the model to downstream taskswith only a small set of parameters. Recently, researchers have leveraged suchproven techniques in multimodal tasks and achieve promising results. However,two critical issues remain unresolved: how to further reduce the complexitywith lightweight design and how to boost alignment between modalities underextremely low parameters. In this paper, we propose A graceful prompt frameworkfor cross-modal transfer (Aurora) to overcome these challenges. Considering theredundancy in existing architectures, we first utilize the mode approximationto generate 0.1M trainable parameters to implement the multimodal prompttuning, which explores the low intrinsic dimension with only 0.04% parametersof the pre-trained model. Then, for better modality alignment, we propose theInformative Context Enhancement and Gated Query Transformation module underextremely few parameters scenes. A thorough evaluation on six cross-modalbenchmarks shows that it not only outperforms the state-of-the-art but evenoutperforms the full fine-tuning approach. Our code is available at:https://github.com/WillDreamer/Aurora."
RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with  Progressive Reasoning Tasks,"['Pengzhen Ren', 'Kaidong Zhang', 'Hetao Zheng', 'Zixuan Li', 'Yuhang Wen', 'Fengda Zhu', 'Mas Ma', 'Xiaodan Liang']",http://arxiv.org/pdf/2306.11335v2.pdf,2023-06-20,"['cs.ro', 'cs.ai', 'cs.cv', 'cs.lg']","  Recently, the advent of pre-trained large-scale language models (LLMs) likeChatGPT and GPT-4 have significantly advanced the machine's natural languageunderstanding capabilities. This breakthrough has allowed us to seamlesslyintegrate these open-source LLMs into a unified robot simulator environment tohelp robots accurately understand and execute human natural languageinstructions. To this end, in this work, we introduce a realistic roboticmanipulation simulator and build a Robotic Manipulation with ProgressiveReasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRTbenchmark builds a new high-fidelity digital twin scene based on Unreal Engine5, which includes 782 categories, 2023 objects, and 15K natural languageinstructions generated by ChatGPT for a detailed evaluation of robotmanipulation. We propose a general pipeline for the RM-PRT benchmark that takesas input multimodal prompts containing natural language instructions andautomatically outputs actions containing the movement and position transitions.We set four natural language understanding tasks with progressive reasoninglevels and evaluate the robot's ability to understand natural languageinstructions in two modes of adsorption and grasping. In addition, we alsoconduct a comprehensive analysis and comparison of the differences andadvantages of 10 different LLMs in instruction understanding and generationquality. We hope the new simulator and benchmark will facilitate futureresearch on language-guided robotic manipulation. Project website:https://necolizer.github.io/RM-PRT/ ."
PromptMTopic: Unsupervised Multimodal Topic Modeling of Memes using  Large Language Models,"['Nirmalendu Prakash', 'Han Wang', 'Nguyen Khoi Hoang', 'Ming Shan Hee', 'Roy Ka-Wei Lee']",http://arxiv.org/pdf/2312.06093v1.pdf,2023-12-11,"['cs.cl', 'cs.cv', 'cs.mm', 'i.1.4; i.1.7']","  The proliferation of social media has given rise to a new form ofcommunication: memes. Memes are multimodal and often contain a combination oftext and visual elements that convey meaning, humor, and cultural significance.While meme analysis has been an active area of research, little work has beendone on unsupervised multimodal topic modeling of memes, which is important forcontent moderation, social media analysis, and cultural studies. We propose\textsf{PromptMTopic}, a novel multimodal prompt-based model designed to learntopics from both text and visual modalities by leveraging the language modelingcapabilities of large language models. Our model effectively extracts andclusters topics learned from memes, considering the semantic interactionbetween the text and visual modalities. We evaluate our proposed model throughextensive experiments on three real-world meme datasets, which demonstrate itssuperiority over state-of-the-art topic modeling baselines in learningdescriptive topics in memes. Additionally, our qualitative analysis shows that\textsf{PromptMTopic} can identify meaningful and culturally relevant topicsfrom memes. Our work contributes to the understanding of the topics and themesof memes, a crucial form of communication in today's society.\\\red{\textbf{Disclaimer: This paper contains sensitive content that may bedisturbing to some readers.}}"
VL-GPT: A Generative Pre-trained Transformer for Vision and Language  Understanding and Generation,"['Jinguo Zhu', 'Xiaohan Ding', 'Yixiao Ge', 'Yuying Ge', 'Sijie Zhao', 'Hengshuang Zhao', 'Xiaohua Wang', 'Ying Shan']",http://arxiv.org/pdf/2312.09251v1.pdf,2023-12-14,['cs.cv'],"  In this work, we introduce Vision-Language Generative Pre-trained Transformer(VL-GPT), a transformer model proficient at concurrently perceiving andgenerating visual and linguistic data. VL-GPT achieves a unified pre-trainingapproach for both image and text modalities by employing a straightforwardauto-regressive objective, thereby enabling the model to process image and textas seamlessly as a language model processes text. To accomplish this, weinitially propose a novel image tokenizer-detokenizer framework for visualdata, specifically designed to transform raw images into a sequence ofcontinuous embeddings and reconstruct them accordingly. In combination with theexisting text tokenizer and detokenizer, this framework allows for the encodingof interleaved image-text data into a multimodal sequence, which cansubsequently be fed into the transformer model. Consequently, VL-GPT canperform large-scale pre-training on multimodal corpora utilizing a unifiedauto-regressive objective (i.e., next-token prediction). Upon completion ofpre-training, VL-GPT exhibits remarkable zero-shot and few-shot performanceacross a diverse range of vision and language understanding and generationtasks, including image captioning, visual question answering, text-to-imagegeneration, and more. Additionally, the pre-trained model retrains in-contextlearning capabilities when provided with multimodal prompts. We further conductinstruction tuning on our VL-GPT, highlighting its exceptional potential formultimodal assistance. The source code and model weights shall be released."
Reframing Instructional Prompts to GPTk's Language,"['Swaroop Mishra', 'Daniel Khashabi', 'Chitta Baral', 'Yejin Choi', 'Hannaneh Hajishirzi']",http://arxiv.org/pdf/2109.07830v3.pdf,2021-09-16,"['cs.cl', 'cs.ai', 'cs.lg']","  What kinds of instructional prompts are easier to follow for Language Models(LMs)? We study this question by conducting extensive empirical analysis thatshed light on important features of successful instructional prompts.Specifically, we study several classes of reframing techniques for manualreformulation of prompts into more effective ones. Some examples includedecomposing a complex task instruction into multiple simpler tasks or itemizinginstructions into sequential steps. Our experiments compare the zero-shot andfew-shot performance of LMs prompted with reframed instructions on 12 NLP tasksacross 6 categories. Compared with original instructions, our reframedinstructions lead to significant improvements across LMs with different sizes.For example, the same reframed prompts boost few-shot performance ofGPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over alltasks. Furthermore, reframed instructions reduce the number of examplesrequired to prompt LMs in the few-shot setting. We hope theseempirically-driven techniques will pave the way towards more effective futureprompting algorithms."
Prompt-Based Learning for Thread Structure Prediction in Cybersecurity  Forums,"['Kazuaki Kashihara', 'Kuntal Kumar Pal', 'Chitta Baral', 'Robert P Trevino']",http://arxiv.org/pdf/2303.05400v1.pdf,2023-03-05,"['cs.cl', 'cs.ai', 'cs.cr']","  With recent trends indicating cyber crimes increasing in both frequency andcost, it is imperative to develop new methods that leverage data-rich hackerforums to assist in combating ever evolving cyber threats. Defininginteractions within these forums is critical as it facilitates identifyinghighly skilled users, which can improve prediction of novel threats and futurecyber attacks. We propose a method called Next Paragraph Prediction withInstructional Prompting (NPP-IP) to predict thread structures while grounded onthe context around posts. This is the first time to apply an instructionalprompting approach to the cybersecurity domain. We evaluate our NPP-IP with theReddit dataset and Hacker Forums dataset that has posts and thread structuresof real hacker forums' threads, and compare our method's performance withexisting methods. The experimental evaluation shows that our proposed methodcan predict the thread structure significantly better than existing methodsallowing for better social network prediction based on forum interactions."
Red Teaming Language Model Detectors with Language Models,"['Zhouxing Shi', 'Yihan Wang', 'Fan Yin', 'Xiangning Chen', 'Kai-Wei Chang', 'Cho-Jui Hsieh']",http://arxiv.org/pdf/2305.19713v2.pdf,2023-05-31,"['cs.cl', 'cs.lg']","  The prevalence and strong capability of large language models (LLMs) presentsignificant safety and ethical risks if exploited by malicious users. Toprevent the potentially deceptive usage of LLMs, recent works have proposedalgorithms to detect LLM-generated text and protect LLMs. In this paper, weinvestigate the robustness and reliability of these LLM detectors underadversarial attacks. We study two types of attack strategies: 1) replacingcertain words in an LLM's output with their synonyms given the context; 2)automatically searching for an instructional prompt to alter the writing styleof the generation. In both strategies, we leverage an auxiliary LLM to generatethe word replacements or the instructional prompt. Different from previousworks, we consider a challenging setting where the auxiliary LLM can also beprotected by a detector. Experiments reveal that our attacks effectivelycompromise the performance of all detectors in the study with plausiblegenerations, underscoring the urgent need to improve the robustness ofLLM-generated text detection systems."
Large Language Models Encode Clinical Knowledge,"['Karan Singhal', 'Shekoofeh Azizi', 'Tao Tu', 'S. Sara Mahdavi', 'Jason Wei', 'Hyung Won Chung', 'Nathan Scales', 'Ajay Tanwani', 'Heather Cole-Lewis', 'Stephen Pfohl', 'Perry Payne', 'Martin Seneviratne', 'Paul Gamble', 'Chris Kelly', 'Nathaneal Scharli', 'Aakanksha Chowdhery', 'Philip Mansfield', 'Blaise Aguera y Arcas', 'Dale Webster', 'Greg S. Corrado', 'Yossi Matias', 'Katherine Chou', 'Juraj Gottweis', 'Nenad Tomasev', 'Yun Liu', 'Alvin Rajkomar', 'Joelle Barral', 'Christopher Semturs', 'Alan Karthikesalingam', 'Vivek Natarajan']",http://arxiv.org/pdf/2212.13138v1.pdf,2022-12-26,['cs.cl'],"  Large language models (LLMs) have demonstrated impressive capabilities innatural language understanding and generation, but the quality bar for medicaland clinical applications is high. Today, attempts to assess models' clinicalknowledge typically rely on automated evaluations on limited benchmarks. Thereis no standard to evaluate model predictions and reasoning across a breadth oftasks. To address this, we present MultiMedQA, a benchmark combining sixexisting open question answering datasets spanning professional medical exams,research, and consumer queries; and HealthSearchQA, a new free-response datasetof medical questions searched online. We propose a framework for humanevaluation of model answers along multiple axes including factuality,precision, possible harm, and bias. In addition, we evaluate PaLM (a540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, onMultiMedQA. Using a combination of prompting strategies, Flan-PaLM achievesstate-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA,MedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (USMedical License Exam questions), surpassing prior state-of-the-art by over 17%.However, human evaluation reveals key gaps in Flan-PaLM responses. To resolvethis we introduce instruction prompt tuning, a parameter-efficient approach foraligning LLMs to new domains using a few exemplars. The resulting model,Med-PaLM, performs encouragingly, but remains inferior to clinicians. We showthat comprehension, recall of knowledge, and medical reasoning improve withmodel scale and instruction prompt tuning, suggesting the potential utility ofLLMs in medicine. Our human evaluations reveal important limitations of today'smodels, reinforcing the importance of both evaluation frameworks and methoddevelopment in creating safe, helpful LLM models for clinical applications."
Layout and Task Aware Instruction Prompt for Zero-shot Document Image  Question Answering,"['Wenjin Wang', 'Yunhao Li', 'Yixin Ou', 'Yin Zhang']",http://arxiv.org/pdf/2306.00526v4.pdf,2023-06-01,"['cs.cl', 'cs.ai', 'cs.cv']","  Layout-aware pre-trained models has achieved significant progress on documentimage question answering. They introduce extra learnable modules into existinglanguage models to capture layout information within document images from textbounding box coordinates obtained by OCR tools. However, extra modulesnecessitate pre-training on extensive document images. This prevents thesemethods from directly utilizing off-the-shelf instruction-tuning languagefoundation models, which have recently shown promising potential in zero-shotlearning. Instead, in this paper, we find that instruction-tuning languagemodels like Claude and ChatGPT can understand layout by spaces and line breaks.Based on this observation, we propose the LAyout and Task aware InstructionPrompt (LATIN-Prompt), which consists of layout-aware document content andtask-aware instruction. Specifically, the former uses appropriate spaces andline breaks to recover the layout information among text segments obtained byOCR tools, and the latter ensures that generated answers adhere to formattingrequirements. Moreover, we propose the LAyout and Task aware Instruction Tuning(LATIN-Tuning) to improve the performance of small instruction-tuning modelslike Alpaca. Experimental results show that LATIN-Prompt enables zero-shotperformance of Claude and ChatGPT to be comparable to the fine-tuningperformance of SOTAs on document image question answering, and LATIN-Tuningenhances the zero-shot performance of Alpaca significantly. For example,LATIN-Prompt improves the performance of Claude and ChatGPT on DocVQA by 263%and 20% respectively. LATIN-Tuning improves the performance of Alpaca on DocVQAby 87.7%. Quantitative and qualitative analyses demonstrate the effectivenessof LATIN-Prompt and LATIN-Tuning. We provide the code in supplementary and willrelease it to facilitate future research."
InstructUIE: Multi-task Instruction Tuning for Unified Information  Extraction,"['Xiao Wang', 'Weikang Zhou', 'Can Zu', 'Han Xia', 'Tianze Chen', 'Yuansen Zhang', 'Rui Zheng', 'Junjie Ye', 'Qi Zhang', 'Tao Gui', 'Jihua Kang', 'Jingsheng Yang', 'Siyuan Li', 'Chunsai Du']",http://arxiv.org/pdf/2304.08085v1.pdf,2023-04-17,"['cs.cl', 'cs.ai']","  Large language models have unlocked strong multi-task capabilities fromreading instructive prompts. However, recent studies have shown that existinglarge models still have difficulty with information extraction tasks. Forexample, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset,which is significantly lower than the state-of-the-art performance. In thispaper, we propose InstructUIE, a unified information extraction framework basedon instruction tuning, which can uniformly model various information extractiontasks and capture the inter-task dependency. To validate the proposed method,we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extractiondatasets in a unified text-to-text format with expert-written instructions.Experimental results demonstrate that our method achieves comparableperformance to Bert in supervised settings and significantly outperforms thestate-of-the-art and gpt3.5 in zero-shot settings."
Camoscio: an Italian Instruction-tuned LLaMA,"['Andrea Santilli', 'Emanuele Rodolà']",http://arxiv.org/pdf/2307.16456v1.pdf,2023-07-31,['cs.cl'],"  In recent years Large Language Models (LLMs) have increased the state of theart on several natural language processing tasks. However, their accessibilityis often limited to paid API services, posing challenges for researchers inconducting extensive investigations. On the other hand, while some open-sourcemodels have been proposed by the community, they are typically multilingual andnot specifically tailored for the Italian language. In an effort to democratizethe available and open resources for the Italian language, in this paper weintroduce Camoscio: a language model specifically tuned to follow users'prompts in Italian. Specifically, we finetuned the smallest variant of LLaMA(7b) with LoRA on a corpus of instruction prompts translated to Italian viaChatGPT. Results indicate that the model's zero-shot performance on variousdownstream tasks in Italian competes favorably with existing modelsspecifically finetuned for those tasks. All the artifacts (code, dataset,model) are released to the community at the following url:https://github.com/teelinsan/camoscio"
Self-Alignment with Instruction Backtranslation,"['Xian Li', 'Ping Yu', 'Chunting Zhou', 'Timo Schick', 'Luke Zettlemoyer', 'Omer Levy', 'Jason Weston', 'Mike Lewis']",http://arxiv.org/pdf/2308.06259v2.pdf,2023-08-11,['cs.cl'],"  We present a scalable method to build a high quality instruction followinglanguage model by automatically labelling human-written text with correspondinginstructions. Our approach, named instruction backtranslation, starts with alanguage model finetuned on a small amount of seed data, and a given webcorpus. The seed model is used to construct training examples by generatinginstruction prompts for web documents (self-augmentation), and then selectinghigh quality examples from among these candidates (self-curation). This data isthen used to finetune a stronger model. Finetuning LLaMa on two iterations ofour approach yields a model that outperforms all other LLaMa-based models onthe Alpaca leaderboard not relying on distillation data, demonstrating highlyeffective self-alignment."
Discrete Prompt Compression with Reinforcement Learning,"['Hoyoun Jung', 'Kyung-Joong Kim']",http://arxiv.org/pdf/2308.08758v1.pdf,2023-08-17,"['cs.cl', 'cs.ai']","  Instruction-tuned Language Models (LMs) are widely used by users to addressvarious problems with task-specific prompts. Constraints associated with thecontext window length and computational costs encourage the development ofcompressed prompts. Existing methods rely heavily on training embeddings, whichare designed to accommodate multiple token meanings. This presents challengesin terms of interpretability, a fixed number of embedding tokens, reusabilityacross different LMs, and inapplicability when interacting with black-box APIs.This study proposes prompt compression with reinforcement learning (PCRL), anovel discrete prompt compression method that addresses these issues. PCRLemploys a computationally efficient policy network that directly edits prompts.The PCRL training approach can be flexibly applied to various types of LMs, aswell as decoder-only and encoder-decoder architecture, and can be trainedwithout gradient access to LMs or labeled data. PCRL achieves an averagereduction of 24.6% in token count across various instruction prompts whilepreserving performance. Further, we demonstrate that the learned policy can betransferred to larger LMs, and through various analyses, we aid theunderstanding of token importance within prompts."
Casteist but Not Racist? Quantifying Disparities in Large Language Model  Bias between India and the West,"['Khyati Khandelwal', 'Manuel Tonneau', 'Andrew M. Bean', 'Hannah Rose Kirk', 'Scott A. Hale']",http://arxiv.org/pdf/2309.08573v1.pdf,2023-09-15,"['cs.cl', 'cs.cy']","  Large Language Models (LLMs), now used daily by millions of users, can encodesocietal biases, exposing their users to representational harms. A large bodyof scholarship on LLM bias exists but it predominantly adopts a Western-centricframe and attends comparatively less to bias levels and potential harms in theGlobal South. In this paper, we quantify stereotypical bias in popular LLMsaccording to an Indian-centric frame and compare bias levels between the Indianand Western contexts. To do this, we develop a novel dataset which we callIndian-BhED (Indian Bias Evaluation Dataset), containing stereotypical andanti-stereotypical examples for caste and religion contexts. We find that themajority of LLMs tested are strongly biased towards stereotypes in the Indiancontext, especially as compared to the Western context. We finally investigateInstruction Prompting as a simple intervention to mitigate such bias and findthat it significantly reduces both stereotypical and anti-stereotypical biasesin the majority of cases for GPT-3.5. The findings of this work highlight theneed for including more diverse voices when evaluating LLMs."
Harnessing Large Language Models' Empathetic Response Generation  Capabilities for Online Mental Health Counselling Support,"['Siyuan Brandon Loh', 'Aravind Sesagiri Raamkumar']",http://arxiv.org/pdf/2310.08017v1.pdf,2023-10-12,"['cs.cl', 'i.2']","  Large Language Models (LLMs) have demonstrated remarkable performance acrossvarious information-seeking and reasoning tasks. These computational systemsdrive state-of-the-art dialogue systems, such as ChatGPT and Bard. They alsocarry substantial promise in meeting the growing demands of mental health care,albeit relatively unexplored. As such, this study sought to examine LLMs'capability to generate empathetic responses in conversations that emulate thosein a mental health counselling setting. We selected five LLMs: version 3.5 andversion 4 of the Generative Pre-training (GPT), Vicuna FastChat-T5, PathwaysLanguage Model (PaLM) version 2, and Falcon-7B-Instruct. Based on a simpleinstructional prompt, these models responded to utterances derived from theEmpatheticDialogues (ED) dataset. Using three empathy-related metrics, wecompared their responses to those from traditional response generation dialoguesystems, which were fine-tuned on the ED dataset, along with human-generatedresponses. Notably, we discovered that responses from the LLMs were remarkablymore empathetic in most scenarios. We position our findings in light ofcatapulting advancements in creating empathetic conversational systems."
Instruct Me More! Random Prompting for Visual In-Context Learning,"['Jiahao Zhang', 'Bowen Wang', 'Liangzhi Li', 'Yuta Nakashima', 'Hajime Nagahara']",http://arxiv.org/pdf/2311.03648v1.pdf,2023-11-07,['cs.cv'],"  Large-scale models trained on extensive datasets, have emerged as thepreferred approach due to their high generalizability across various tasks.In-context learning (ICL), a popular strategy in natural language processing,uses such models for different tasks by providing instructive prompts butwithout updating model parameters. This idea is now being explored in computervision, where an input-output image pair (called an in-context pair) issupplied to the model with a query image as a prompt to exemplify the desiredoutput. The efficacy of visual ICL often depends on the quality of the prompts.We thus introduce a method coined Instruct Me More (InMeMo), which augmentsin-context pairs with a learnable perturbation (prompt), to explore itspotential. Our experiments on mainstream tasks reveal that InMeMo surpasses thecurrent state-of-the-art performance. Specifically, compared to the baselinewithout learnable prompt, InMeMo boosts mIoU scores by 7.35 and 15.13 forforeground segmentation and single object detection tasks, respectively. Ourfindings suggest that InMeMo offers a versatile and efficient way to enhancethe performance of visual ICL with lightweight training. Code is available athttps://github.com/Jackieam/InMeMo."
Few-shot Instruction Prompts for Pretrained Language Models to Detect  Social Biases,"['Shrimai Prabhumoye', 'Rafal Kocielnik', 'Mohammad Shoeybi', 'Anima Anandkumar', 'Bryan Catanzaro']",http://arxiv.org/pdf/2112.07868v2.pdf,2021-12-15,"['cs.cl', 'cs.ai']","  Detecting social bias in text is challenging due to nuance, subjectivity, anddifficulty in obtaining good quality labeled datasets at scale, especiallygiven the evolving nature of social biases and society. To address thesechallenges, we propose a few-shot instruction-based method for promptingpre-trained language models (LMs). We select a few class-balanced exemplarsfrom a small support repository that are closest to the query to be labeled inthe embedding space. We then provide the LM with instruction that consists ofthis subset of labeled exemplars, the query text to be classified, a definitionof bias, and prompt it to make a decision. We demonstrate that large LMs usedin a few-shot context can detect different types of fine-grained biases withsimilar and sometimes superior accuracy to fine-tuned models. We observe thatthe largest 530B parameter model is significantly more effective in detectingsocial bias compared to smaller models (achieving at least 13% improvement inAUC metric compared to other models). It also maintains a high AUC (droppingless than 2%) when the labeled repository is reduced to as few as $100$samples. Large pretrained language models thus make it easier and quicker tobuild new bias detectors."
"GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large  Language Models","['Archiki Prasad', 'Peter Hase', 'Xiang Zhou', 'Mohit Bansal']",http://arxiv.org/pdf/2203.07281v2.pdf,2022-03-14,"['cs.cl', 'cs.ai', 'cs.lg']","  Providing natural language instructions in prompts is a useful new paradigmfor improving task performance of large language models in a zero-shot setting.Recent work has aimed to improve such prompts via manual rewriting orgradient-based tuning. However, manual rewriting is time-consuming and requiressubjective interpretation, while gradient-based tuning can be extremelycomputationally demanding for large models and may not be feasible forAPI-based models. In this work, we introduce Gradient-free Instructional PromptSearch (GrIPS), a gradient-free, edit-based search approach for improving taskinstructions for large language models. GrIPS takes in instructions designedfor humans and automatically returns an improved, edited prompt, while allowingfor API-based tuning. With InstructGPT models, GrIPS improves the average taskperformance by up to 4.30 percentage points on eight classification tasks fromthe Natural Instructions dataset (with similar improvements for OPT, BLOOM, andFLAN-T5). We see improvements for both instruction-only prompts and instruction+ k-shot examples prompts. Notably, GrIPS outperforms manual rewriting andpurely example-based prompts while controlling for the available compute anddata budget. Further, performance of GrIPS is comparable to selectgradient-based tuning approaches. Qualitatively, we show our edits can simplifyinstructions and at times make them incoherent but nonetheless improveaccuracy. Our code is available at: https://github.com/archiki/GrIPS"
LINGUIST: Language Model Instruction Tuning to Generate Annotated  Utterances for Intent Classification and Slot Tagging,"['Andy Rosenbaum', 'Saleh Soltan', 'Wael Hamza', 'Yannick Versley', 'Markus Boese']",http://arxiv.org/pdf/2209.09900v1.pdf,2022-09-20,"['cs.cl', 'cs.ai', 'cs.lg']","  We present LINGUIST, a method for generating annotated data for IntentClassification and Slot Tagging (IC+ST), via fine-tuning AlexaTM 5B, a5-billion-parameter multilingual sequence-to-sequence (seq2seq) model, on aflexible instruction prompt. In a 10-shot novel intent setting for the SNIPSdataset, LINGUIST surpasses state-of-the-art approaches (Back-Translation andExample Extrapolation) by a wide margin, showing absolute improvement for thetarget intents of +1.9 points on IC Recall and +2.5 points on ST F1 Score. Inthe zero-shot cross-lingual setting of the mATIS++ dataset, LINGUISTout-performs a strong baseline of Machine Translation with Slot Alignment by+4.14 points absolute on ST F1 Score across 6 languages, while matchingperformance on IC. Finally, we verify our results on an internal large-scalemultilingual dataset for conversational agent IC+ST and show significantimprovements over a baseline which uses Back-Translation, Paraphrasing and SlotCatalog Resampling. To our knowledge, we are the first to demonstrateinstruction fine-tuning of a large-scale seq2seq model to control the outputsof multilingual intent- and slot-labeled data generation."
How Does In-Context Learning Help Prompt Tuning?,"['Simeng Sun', 'Yang Liu', 'Dan Iter', 'Chenguang Zhu', 'Mohit Iyyer']",http://arxiv.org/pdf/2302.11521v1.pdf,2023-02-22,['cs.cl'],"  Fine-tuning large language models is becoming ever more impractical due totheir rapidly-growing scale. This motivates the use of parameter-efficientadaptation methods such as prompt tuning (PT), which adds a small number oftunable embeddings to an otherwise frozen model, and in-context learning (ICL),in which demonstrations of the task are provided to the model in naturallanguage without any additional training. Recently, Singhal et al. (2022)propose ``instruction prompt tuning'' (IPT), which combines PT with ICL byconcatenating a natural language demonstration with learned prompt embeddings.While all of these methods have proven effective on different tasks, how theyinteract with each other remains unexplored. In this paper, we empiricallystudy when and how in-context examples improve prompt tuning by measuring theeffectiveness of ICL, PT, and IPT on five text generation tasks with multiplebase language models. We observe that (1) IPT does \emph{not} always outperformPT, and in fact requires the in-context demonstration to be semanticallysimilar to the test input to yield improvements; (2) PT is unstable andexhibits high variance, but combining PT and ICL (into IPT) consistentlyreduces variance across all five tasks; and (3) prompts learned for a specificsource task via PT exhibit positive transfer when paired with in-contextexamples of a different target task. Our results offer actionable insights onchoosing a suitable parameter-efficient adaptation method for a given task."
InferFix: End-to-End Program Repair with LLMs,"['Matthew Jin', 'Syed Shahriar', 'Michele Tufano', 'Xin Shi', 'Shuai Lu', 'Neel Sundaresan', 'Alexey Svyatkovskiy']",http://arxiv.org/pdf/2303.07263v1.pdf,2023-03-13,['cs.se'],"  Software development life cycle is profoundly influenced by bugs: theirintroduction, identification, and eventual resolution account for a significantportion of software cost. This has motivated software engineering researchersand practitioners to propose different approaches for automating theidentification and repair of software defects. Large language models have beenadapted to the program repair task through few-shot demonstration learning andinstruction prompting, treating this as an infilling task. However, thesemodels have only focused on learning general bug-fixing patterns foruncategorized bugs mined from public repositories. In this paper, we proposeInferFix: a transformer-based program repair framework paired with astate-of-the-art static analyzer to fix critical security and performance bugs.InferFix combines a Retriever -- transformer encoder model pretrained viacontrastive learning objective, which aims at searching for semanticallyequivalent bugs and corresponding fixes; and a Generator -- a large languagemodel (Codex Cushman) finetuned on supervised bug-fix data with promptsaugmented via bug type annotations and semantically similar fixes retrievedfrom an external non-parametric memory. To train and evaluate our approach, wecurated InferredBugs, a novel, metadata-rich dataset of bugs extracted byexecuting the Infer static analyzer on the change histories of thousands ofJava and C# repositories. Our evaluation demonstrates that InferFix outperformsstrong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C#and 76.8% in Java. We discuss the deployment of InferFix alongside Infer atMicrosoft which offers an end-to-end solution for detection, classification,and localization of bugs, as well as fixing and validation of candidatepatches, integrated in the continuous integration pipeline to automate thesoftware development workflow."
Text-based Person Search without Parallel Image-Text Data,"['Yang Bai', 'Jingyao Wang', 'Min Cao', 'Chen Chen', 'Ziqiang Cao', 'Liqiang Nie', 'Min Zhang']",http://arxiv.org/pdf/2305.12964v2.pdf,2023-05-22,['cs.cv'],"  Text-based person search (TBPS) aims to retrieve the images of the targetperson from a large image gallery based on a given natural languagedescription. Existing methods are dominated by training models with parallelimage-text pairs, which are very costly to collect. In this paper, we make thefirst attempt to explore TBPS without parallel image-text data ($\mu$-TBPS), inwhich only non-parallel images and texts, or even image-only data, can beadopted. Towards this end, we propose a two-stage framework,generation-then-retrieval (GTR), to first generate the corresponding pseudotext for each image and then perform the retrieval in a supervised manner. Inthe generation stage, we propose a fine-grained image captioning strategy toobtain an enriched description of the person image, which firstly utilizes aset of instruction prompts to activate the off-the-shelf pretrainedvision-language model to capture and generate fine-grained person attributes,and then converts the extracted attributes into a textual description via thefinetuned large language model or the hand-crafted template. In the retrievalstage, considering the noise interference of the generated texts for trainingmodel, we develop a confidence score-based training scheme by enabling morereliable texts to contribute more during the training. Experimental results onmultiple TBPS benchmarks (i.e., CUHK-PEDES, ICFG-PEDES and RSTPReid) show thatthe proposed GTR can achieve a promising performance without relying onparallel image-text data."
EDM3: Event Detection as Multi-task Text Generation,"['Ujjwala Anantheswaran', 'Himanshu Gupta', 'Mihir Parmar', 'Kuntal Kumar Pal', 'Chitta Baral']",http://arxiv.org/pdf/2305.16357v1.pdf,2023-05-25,['cs.cl'],"  Event detection refers to identifying event occurrences in a text andcomprises of two subtasks; event identification and classification. We presentEDM3, a novel approach for Event Detection that formulates three generativetasks: identification, classification, and combined detection. We show thatEDM3 helps to learn transferable knowledge that can be leveraged to performEvent Detection and its subtasks concurrently, mitigating the error propagationinherent in pipelined approaches. Unlike previous dataset- or domain-specificapproaches, EDM3 utilizes the existing knowledge of language models, allowingit to be trained over any classification schema. We evaluate EDM3 on multipleevent detection datasets: RAMS, WikiEvents, MAVEN, and MLEE, showing that EDM3outperforms 1) single-task performance by 8.4% on average and 2) multi-taskperformance without instructional prompts by 2.4% on average. We obtain SOTAresults on RAMS (71.3% vs. 65.1% F-1) and competitive performance on otherdatasets. We analyze our approach to demonstrate its efficacy in low-resourceand multi-sentence settings. We also show the effectiveness of this approach onnon-standard event configurations such as multi-word and multi-class eventtriggers. Overall, our results show that EDM3 is a promising approach for EventDetection that has the potential for real-world applications."
VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and  Dataset,"['Sihan Chen', 'Handong Li', 'Qunbo Wang', 'Zijia Zhao', 'Mingzhen Sun', 'Xinxin Zhu', 'Jing Liu']",http://arxiv.org/pdf/2305.18500v2.pdf,2023-05-29,"['cs.cv', 'cs.ai', 'cs.cl', 'cs.lg', 'eess.as']","  Vision and text have been fully explored in contemporary video-textfoundational models, while other modalities such as audio and subtitles invideos have not received sufficient attention. In this paper, we resort toestablish connections between multi-modality video tracks, including Vision,Audio, and Subtitle, and Text by exploring an automatically generatedlarge-scale omni-modality video caption dataset called VAST-27M. Specifically,we first collect 27 million open-domain video clips and separately train avision and an audio captioner to generate vision and audio captions. Then, weemploy an off-the-shelf Large Language Model (LLM) to integrate the generatedcaptions, together with subtitles and instructional prompts into omni-modalitycaptions. Based on the proposed VAST-27M dataset, we train an omni-modalityvideo-text foundational model named VAST, which can perceive and processvision, audio, and subtitle modalities from video, and better support varioustasks including vision-text, audio-text, and multi-modal video-text tasks(retrieval, captioning and QA). Extensive experiments have been conducted todemonstrate the effectiveness of our proposed VAST-27M corpus and VASTfoundation model. VAST achieves 22 new state-of-the-art results on variouscross-modality benchmarks. Code, model and dataset will be released athttps://github.com/TXH-mercury/VAST."
Mondrian: Prompt Abstraction Attack Against Large Language Models for  Cheaper API Pricing,"['Wai Man Si', 'Michael Backes', 'Yang Zhang']",http://arxiv.org/pdf/2308.03558v1.pdf,2023-08-07,"['cs.cr', 'cs.cl']","  The Machine Learning as a Service (MLaaS) market is rapidly expanding andbecoming more mature. For example, OpenAI's ChatGPT is an advanced largelanguage model (LLM) that generates responses for various queries withassociated fees. Although these models can deliver satisfactory performance,they are far from perfect. Researchers have long studied the vulnerabilitiesand limitations of LLMs, such as adversarial attacks and model toxicity.Inevitably, commercial ML models are also not exempt from such issues, whichcan be problematic as MLaaS continues to grow. In this paper, we discover a newattack strategy against LLM APIs, namely the prompt abstraction attack.Specifically, we propose Mondrian, a simple and straightforward method thatabstracts sentences, which can lower the cost of using LLM APIs. In thisapproach, the adversary first creates a pseudo API (with a lower establishedprice) to serve as the proxy of the target API (with a higher establishedprice). Next, the pseudo API leverages Mondrian to modify the user query,obtain the abstracted response from the target API, and forward it back to theend user. Our results show that Mondrian successfully reduces user queries'token length ranging from 13% to 23% across various tasks, including textclassification, generation, and question answering. Meanwhile, these abstractedqueries do not significantly affect the utility of task-specific and generallanguage models like ChatGPT. Mondrian also reduces instruction prompts' tokenlength by at least 11% without compromising output quality. As a result, theprompt abstraction attack enables the adversary to profit without bearing thecost of API development and deployment."
Large Language Models are Visual Reasoning Coordinators,"['Liangyu Chen', 'Bo Li', 'Sheng Shen', 'Jingkang Yang', 'Chunyuan Li', 'Kurt Keutzer', 'Trevor Darrell', 'Ziwei Liu']",http://arxiv.org/pdf/2310.15166v1.pdf,2023-10-23,"['cs.cv', 'cs.cl']","  Visual reasoning requires multimodal perception and commonsense cognition ofthe world. Recently, multiple vision-language models (VLMs) have been proposedwith excellent commonsense reasoning ability in various domains. However, howto harness the collective power of these complementary VLMs is rarely explored.Existing methods like ensemble still struggle to aggregate these models withthe desired higher-order communications. In this work, we propose Cola, a novelparadigm that coordinates multiple VLMs for visual reasoning. Our key insightis that a large language model (LLM) can efficiently coordinate multiple VLMsby facilitating natural language communication that leverages their distinctand complementary capabilities. Extensive experiments demonstrate that ourinstruction tuning variant, Cola-FT, achieves state-of-the-art performance onvisual question answering (VQA), outside knowledge VQA, visual entailment, andvisual spatial reasoning tasks. Moreover, we show that our in-context learningvariant, Cola-Zero, exhibits competitive performance in zero and few-shotsettings, without finetuning. Through systematic ablation studies andvisualizations, we validate that a coordinator LLM indeed comprehends theinstruction prompts as well as the separate functionalities of VLMs; it thencoordinates them to enable impressive visual reasoning capabilities."
ImpressionGPT: An Iterative Optimizing Framework for Radiology Report  Summarization with ChatGPT,"['Chong Ma', 'Zihao Wu', 'Jiaqi Wang', 'Shaochen Xu', 'Yaonai Wei', 'Zhengliang Liu', 'Xi Jiang', 'Lei Guo', 'Xiaoyan Cai', 'Shu Zhang', 'Tuo Zhang', 'Dajiang Zhu', 'Dinggang Shen', 'Tianming Liu', 'Xiang Li']",http://arxiv.org/pdf/2304.08448v2.pdf,2023-04-17,"['cs.cl', 'cs.ai']","  The 'Impression' section of a radiology report is a critical basis forcommunication between radiologists and other physicians, and it is typicallywritten by radiologists based on the 'Findings' section. However, writingnumerous impressions can be laborious and error-prone for radiologists.Although recent studies have achieved promising results in automatic impressiongeneration using large-scale medical text data for pre-training and fine-tuningpre-trained language models, such models often require substantial amounts ofmedical text data and have poor generalization performance. While largelanguage models (LLMs) like ChatGPT have shown strong generalizationcapabilities and performance, their performance in specific domains, such asradiology, remains under-investigated and potentially limited. To address thislimitation, we propose ImpressionGPT, which leverages the in-context learningcapability of LLMs by constructing dynamic contexts using domain-specific,individualized data. This dynamic prompt approach enables the model to learncontextual knowledge from semantically similar examples from existing data.Additionally, we design an iterative optimization algorithm that performsautomatic evaluation on the generated impression results and composes thecorresponding instruction prompts to further optimize the model. The proposedImpressionGPT model achieves state-of-the-art performance on both MIMIC-CXR andOpenI datasets without requiring additional training data or fine-tuning theLLMs. This work presents a paradigm for localizing LLMs that can be applied ina wide range of similar application scenarios, bridging the gap betweengeneral-purpose LLMs and the specific language processing needs of variousdomains."
Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive  Synthesis using Large Language Models and Satisfiability Solving,"['Sumit Kumar Jha', 'Susmit Jha', 'Patrick Lincoln', 'Nathaniel D. Bastian', 'Alvaro Velasquez', 'Rickard Ewetz', 'Sandeep Neema']",http://arxiv.org/pdf/2309.16436v1.pdf,2023-09-28,"['cs.ai', 'cs.lo']","  Generative large language models (LLMs) with instruct training such as GPT-4can follow human-provided instruction prompts and generate human-like responsesto these prompts. Apart from natural language responses, they have also beenfound to be effective at generating formal artifacts such as code, plans, andlogical specifications from natural language prompts. Despite their remarkablyimproved accuracy, these models are still known to produce factually incorrector contextually inappropriate results despite their syntactic coherence - aphenomenon often referred to as hallucination. This limitation makes itdifficult to use these models to synthesize formal artifacts that are used insafety-critical applications. Unlike tasks such as text summarization andquestion-answering, bugs in code, plan, and other formal artifacts produced byLLMs can be catastrophic. We posit that we can use the satisfiability modulotheory (SMT) solvers as deductive reasoning engines to analyze the generatedsolutions from the LLMs, produce counterexamples when the solutions areincorrect, and provide that feedback to the LLMs exploiting the dialogcapability of instruct-trained LLMs. This interaction between inductive LLMsand deductive SMT solvers can iteratively steer the LLM to generate the correctresponse. In our experiments, we use planning over the domain of blocks as oursynthesis task for evaluating our approach. We use GPT-4, GPT3.5 Turbo,Davinci, Curie, Babbage, and Ada as the LLMs and Z3 as the SMT solver. Ourmethod allows the user to communicate the planning problem in natural language;even the formulation of queries to SMT solvers is automatically generated fromnatural language. Thus, the proposed technique can enable non-expert users todescribe their problems in natural language, and the combination of LLMs andSMT solvers can produce provably correct solutions."
Benchmarking a foundation LLM on its ability to re-label structure names  in accordance with the AAPM TG-263 report,"['Jason Holmes', 'Lian Zhang', 'Yuzhen Ding', 'Hongying Feng', 'Zhengliang Liu', 'Tianming Liu', 'William W. Wong', 'Sujay A. Vora', 'Jonathan B. Ashman', 'Wei Liu']",http://arxiv.org/pdf/2310.03874v1.pdf,2023-10-05,"['physics.med-ph', 'cs.cl']","  Purpose: To introduce the concept of using large language models (LLMs) tore-label structure names in accordance with the American Association ofPhysicists in Medicine (AAPM) Task Group (TG)-263 standard, and to establish abenchmark for future studies to reference.  Methods and Materials: The Generative Pre-trained Transformer (GPT)-4application programming interface (API) was implemented as a Digital Imagingand Communications in Medicine (DICOM) storage server, which upon receiving astructure set DICOM file, prompts GPT-4 to re-label the structure names of bothtarget volumes and normal tissues according to the AAPM TG-263. Three diseasesites, prostate, head and neck, and thorax were selected for evaluation. Foreach disease site category, 150 patients were randomly selected for manuallytuning the instructions prompt (in batches of 50) and 50 patients were randomlyselected for evaluation. Structure names that were considered were those thatwere most likely to be relevant for studies utilizing structure contours formany patients.  Results: The overall re-labeling accuracy of both target volumes and normaltissues for prostate, head and neck, and thorax cases was 96.0%, 98.5%, and96.9% respectively. Re-labeling of target volumes was less accurate on averageexcept for prostate - 100%, 93.1%, and 91.1% respectively.  Conclusions: Given the accuracy of GPT-4 in re-labeling structure names ofboth target volumes and normal tissues as presented in this work, LLMs arepoised to be the preferred method for standardizing structure names inradiation oncology, especially considering the rapid advancements in LLMcapabilities that are likely to continue."
What Makes Pre-trained Language Models Better Zero-shot Learners?,"['Jinghui Lu', 'Dongsheng Zhu', 'Weidong Han', 'Rui Zhao', 'Brian Mac Namee', 'Fei Tan']",http://arxiv.org/pdf/2209.15206v3.pdf,2022-09-30,"['cs.cl', 'cs.ai']","  Current methods for prompt learning in zeroshot scenarios widely rely on adevelopment set with sufficient human-annotated data to select thebest-performing prompt template a posteriori. This is not ideal because in arealworld zero-shot scenario of practical relevance, no labelled data isavailable. Thus, we propose a simple yet effective method for screeningreasonable prompt templates in zero-shot text classification: PerplexitySelection (Perplection). We hypothesize that language discrepancy can be usedto measure the efficacy of prompt templates, and thereby develop asubstantiated perplexity-based scheme allowing for forecasting the performanceof prompt templates in advance. Experiments show that our method leads toimproved prediction performance in a realistic zero-shot setting, eliminatingthe need for any labelled examples."
IIE-NLP-NUT at SemEval-2020 Task 4: Guiding PLM with Prompt Template  Reconstruction Strategy for ComVE,"['Luxi Xing', 'Yuqiang Xie', 'Yue Hu', 'Wei Peng']",http://arxiv.org/pdf/2007.00924v1.pdf,2020-07-02,['cs.cl'],"  This paper introduces our systems for the first two subtasks of SemEvalTask4: Commonsense Validation and Explanation. To clarify the intention forjudgment and inject contrastive information for selection, we propose the inputreconstruction strategy with prompt templates. Specifically, we formalize thesubtasks into the multiple-choice question answering format and construct theinput with the prompt templates, then, the final prediction of questionanswering is considered as the result of subtasks. Experimental results showthat our approaches achieve significant performance compared with the baselinesystems. Our approaches secure the third rank on both official test sets of thefirst two subtasks with an accuracy of 96.4 and an accuracy of 94.3respectively."
GraphPrompt: Graph-Based Prompt Templates for Biomedical Synonym  Prediction,"['Hanwen Xu', 'Jiayou Zhang', 'Zhirui Wang', 'Shizhuo Zhang', 'Megh Manoj Bhalerao', 'Yucong Liu', 'Dawei Zhu', 'Sheng Wang']",http://arxiv.org/pdf/2112.03002v2.pdf,2021-11-13,"['cs.cl', 'cs.ai']","  In the expansion of biomedical dataset, the same category may be labeled withdifferent terms, thus being tedious and onerous to curate these terms.Therefore, automatically mapping synonymous terms onto the ontologies isdesirable, which we name as biomedical synonym prediction task. Unlikebiomedical concept normalization (BCN), no clues from context can be used toenhance synonym prediction, making it essential to extract graph features fromontology. We introduce an expert-curated dataset OBO-syn encompassing 70different types of concepts and 2 million curated concept-term pairs forevaluating synonym prediction methods. We find BCN methods perform weakly onthis task for not making full use of graph information. Therefore, we proposeGraphPrompt, a prompt-based learning approach that creates prompt templatesaccording to the graphs. GraphPrompt obtained 37.2\% and 28.5\% improvement onzero-shot and few-shot settings respectively, indicating the effectiveness ofthese graph-based prompt templates. We envision that our method GraphPrompt andOBO-syn dataset can be broadly applied to graph-based NLP tasks, and serve asthe basis for analyzing diverse and accumulating biomedical data. All the dataand codes are avalible at: https://github.com/HanwenXuTHU/GraphPrompt"
CCPrompt: Counterfactual Contrastive Prompt-Tuning for Many-Class  Classification,"['Yang Li', 'Canran Xu', 'Tao Shen', 'Jing Jiang', 'Guodong Long']",http://arxiv.org/pdf/2211.05987v1.pdf,2022-11-11,['cs.cl'],"  With the success of the prompt-tuning paradigm in Natural Language Processing(NLP), various prompt templates have been proposed to further stimulatespecific knowledge for serving downstream tasks, e.g., machine translation,text generation, relation extraction, and so on. Existing prompt templates aremainly shared among all training samples with the information of taskdescription. However, training samples are quite diverse. The sharing taskdescription is unable to stimulate the unique task-related information in eachtraining sample, especially for tasks with the finite-label space. To exploitthe unique task-related information, we imitate the human decision processwhich aims to find the contrastive attributes between the objective factual andtheir potential counterfactuals. Thus, we propose the \textbf{C}ounterfactual\textbf{C}ontrastive \textbf{Prompt}-Tuning (CCPrompt) approach for many-classclassification, e.g., relation classification, topic classification, and entitytyping. Compared with simple classification tasks, these tasks have morecomplex finite-label spaces and are more rigorous for prompts. First of all, weprune the finite label space to construct fact-counterfactual pairs. Then, weexploit the contrastive attributes by projecting training instances onto everyfact-counterfactual pair. We further set up global prototypes correspondingwith all contrastive attributes for selecting valid contrastive attributes asadditional tokens in the prompt template. Finally, a simple Siameserepresentation learning is employed to enhance the robustness of the model. Weconduct experiments on relation classification, topic classification, andentity typing tasks in both fully supervised setting and few-shot setting. Theresults indicate that our model outperforms former baselines."
Low-Resource Multi-Granularity Academic Function Recognition Based on  Multiple Prompt Knowledge,"['Jiawei Liu', 'Zi Xiong', 'Yi Jiang', 'Yongqiang Ma', 'Wei Lu', 'Yong Huang', 'Qikai Cheng']",http://arxiv.org/pdf/2305.03287v1.pdf,2023-05-05,"['cs.cl', 'cs.ai']","  Fine-tuning pre-trained language models (PLMs), e.g., SciBERT, generallyrequires large numbers of annotated data to achieve state-of-the-artperformance on a range of NLP tasks in the scientific domain. However,obtaining the fine-tune data for scientific NLP task is still challenging andexpensive. Inspired by recent advancement in prompt learning, in this paper, wepropose the Mix Prompt Tuning (MPT), which is a semi-supervised method toalleviate the dependence on annotated data and improve the performance ofmulti-granularity academic function recognition tasks with a small number oflabeled examples. Specifically, the proposed method provides multi-perspectiverepresentations by combining manual prompt templates with automatically learnedcontinuous prompt templates to help the given academic function recognitiontask take full advantage of knowledge in PLMs. Based on these prompt templatesand the fine-tuned PLM, a large number of pseudo labels are assigned to theunlabeled examples. Finally, we fine-tune the PLM using the pseudo trainingset. We evaluate our method on three academic function recognition tasks ofdifferent granularity including the citation function, the abstract sentencefunction, and the keyword function, with datasets from computer science domainand biomedical domain. Extensive experiments demonstrate the effectiveness ofour method and statistically significant improvements against strong baselines.In particular, it achieves an average increase of 5% in Macro-F1 score comparedwith fine-tuning, and 6% in Macro-F1 score compared with other semi-supervisedmethod under low-resource settings. In addition, MPT is a general method thatcan be easily applied to other low-resource scientific classification tasks."
AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models,"['Jan Hendrik Metzen', 'Piyapat Saranrittichai', 'Chaithanya Kumar Mummadi']",http://arxiv.org/pdf/2309.16414v2.pdf,2023-09-28,"['cs.cv', 'cs.ai', 'cs.lg']","  Classifiers built upon vision-language models such as CLIP have shownremarkable zero-shot performance across a broad range of image classificationtasks. Prior work has studied different ways of automatically creatingdescriptor sets for every class based on prompt templates, ranging frommanually engineered templates over templates obtained from a large languagemodel to templates built from random words and characters. Up until now,deriving zero-shot classifiers from the respective encoded class descriptorshas remained nearly unchanged, i.e., classify to the class that maximizescosine similarity between its averaged encoded class descriptors and the imageencoding. However, weighing all class descriptors equally can be suboptimalwhen certain descriptors match visual clues on a given image better thanothers. In this work, we propose AutoCLIP, a method for auto-tuning zero-shotclassifiers. AutoCLIP tunes per-image weights to each prompt template atinference time, based on statistics of class descriptor-image similarities.AutoCLIP is fully unsupervised, has very low computational overhead, and can beeasily implemented in few lines of code. We show that AutoCLIP outperformsbaselines across a broad range of vision-language models, datasets, and prompttemplates consistently and by up to 3 percent point accuracy."
Position-based Prompting for Health Outcome Generation,"['M. Abaho', 'D. Bollegala', 'P. Williamson', 'S. Dodd']",http://arxiv.org/pdf/2204.03489v1.pdf,2022-03-30,"['cs.cl', 'cs.lg']","  Probing Pre-trained Language Models (PLMs) using prompts has indirectlyimplied that language models (LMs) can be treated as knowledge bases. To thisend, this phenomena has been effective especially when these LMs are fine-tunedtowards not just data of a specific domain, but also to the style or linguisticpattern of the prompts themselves. We observe that, satisfying a particularlinguistic pattern in prompts is an unsustainable constraint that unnecessarilylengthens the probing task, especially because, they are often manuallydesigned and the range of possible prompt template patterns can vary dependingon the prompting objective and domain. We therefore explore an idea of using aposition-attention mechanism to capture positional information of each word ina prompt relative to the mask to be filled, hence avoiding the need tore-construct prompts when the prompts linguistic pattern changes. Using ourapproach, we demonstrate the ability of eliciting answers to rare prompttemplates (in a case study on health outcome generation) such as Postfix andMixed patterns whose missing information is respectively at the start and inmultiple random places of the prompt. More so, using various biomedical PLMs,our approach consistently outperforms a baseline in which the default masklanguage model (MLM) representation is used to predict masked tokens."
Prompting Large Language Models With the Socratic Method,['Edward Y. Chang'],http://arxiv.org/pdf/2303.08769v2.pdf,2023-02-17,"['cs.lg', 'i.2.7']","  This paper presents a systematic approach to using the Socratic method indeveloping prompt templates that effectively interact with large languagemodels, including GPT-3. Various methods are examined, and those that yieldprecise answers and justifications while fostering creativity and imaginationto enhance creative writing are identified. Techniques such as {\emdefinition}, {\em elenchus}, {\em dialectic}, {\em maieutics}, {\emgeneralization}, and {\em counterfactual reasoning} are discussed for theirapplication in engineering prompt templates and their connections to inductive,deductive, and abductive reasoning. Through examples, the effectiveness ofthese dialogue and reasoning methods is demonstrated. An interestingobservation is made that when the task's goal and user intent are conveyed toGPT-3 via ChatGPT before the start of a dialogue, the large language modelseems to connect to the external context expressed in the intent and performmore effectively."
Prompt Learning for News Recommendation,"['Zizhuo Zhang', 'Bang Wang']",http://arxiv.org/pdf/2304.05263v1.pdf,2023-04-11,"['cs.ir', 'cs.ai', 'h.3.3']","  Some recent \textit{news recommendation} (NR) methods introduce a Pre-trainedLanguage Model (PLM) to encode news representation by following the vanillapre-train and fine-tune paradigm with carefully-designedrecommendation-specific neural networks and objective functions. Due to theinconsistent task objective with that of PLM, we argue that their modelingparadigm has not well exploited the abundant semantic information andlinguistic knowledge embedded in the pre-training process. Recently, thepre-train, prompt, and predict paradigm, called \textit{prompt learning}, hasachieved many successes in natural language processing domain. In this paper,we make the first trial of this new paradigm to develop a \textit{PromptLearning for News Recommendation} (Prompt4NR) framework, which transforms thetask of predicting whether a user would click a candidate news as a cloze-stylemask-prediction task. Specifically, we design a series of prompt templates,including discrete, continuous, and hybrid templates, and construct theircorresponding answer spaces to examine the proposed Prompt4NR framework.Furthermore, we use the prompt ensembling to integrate predictions frommultiple prompt templates. Extensive experiments on the MIND dataset validatethe effectiveness of our Prompt4NR with a set of new benchmark results."
Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot  Classification,"['Han Wang', 'Canwen Xu', 'Julian McAuley']",http://arxiv.org/pdf/2204.06305v2.pdf,2022-04-13,"['cs.cl', 'cs.ai', 'cs.lg']","  Prompt-based learning (i.e., prompting) is an emerging paradigm forexploiting knowledge learned by a pretrained language model. In this paper, wepropose Automatic Multi-Label Prompting (AMuLaP), a simple yet effective methodto automatically select label mappings for few-shot text classification withprompting. Our method exploits one-to-many label mappings and astatistics-based algorithm to select label mappings given a prompt template.Our experiments demonstrate that AMuLaP achieves competitive performance on theGLUE benchmark without human effort or external resources."
CoCoMo: Computational Consciousness Modeling for Generative and Ethical  AI,['Edward Y. Chang'],http://arxiv.org/pdf/2304.02438v2.pdf,2023-03-17,"['cs.oh', 'i.2.7']","  The CoCoMo model proposes a computational solution to the challenge ofincorporating ethical and emotional intelligence considerations into AIsystems, with the aim of creating AI agents that combine knowledge withcompassion. To achieve this goal, CoCoMo prioritizes fairness, beneficence,non-maleficence, empathy, adaptability, transparency, and critical andexploratory thinking abilities. The model employs consciousness modeling,reinforcement learning, and prompt template formulation to support thesedesired traits. By incorporating ethical and emotional intelligenceconsiderations, a generative AI model can potentially lead to improvedfairness, reduced toxicity, and increased reliability."
PromptNER: Prompt Locating and Typing for Named Entity Recognition,"['Yongliang Shen', 'Zeqi Tan', 'Shuhui Wu', 'Wenqi Zhang', 'Rongsheng Zhang', 'Yadong Xi', 'Weiming Lu', 'Yueting Zhuang']",http://arxiv.org/pdf/2305.17104v1.pdf,2023-05-26,['cs.cl'],"  Prompt learning is a new paradigm for utilizing pre-trained language modelsand has achieved great success in many tasks. To adopt prompt learning in theNER task, two kinds of methods have been explored from a pair of symmetricperspectives, populating the template by enumerating spans to predict theirentity types or constructing type-specific prompts to locate entities. However,these methods not only require a multi-round prompting manner with a high timeoverhead and computational cost, but also require elaborate prompt templates,that are difficult to apply in practical scenarios. In this paper, we unifyentity locating and entity typing into prompt learning, and design a dual-slotmulti-prompt template with the position slot and type slot to prompt locatingand typing respectively. Multiple prompts can be input to the modelsimultaneously, and then the model extracts all entities by parallelpredictions on the slots. To assign labels for the slots during training, wedesign a dynamic template filling mechanism that uses the extended bipartitegraph matching between prompts and the ground-truth entities. We conductexperiments in various settings, including resource-rich flat and nested NERdatasets and low-resource in-domain and cross-domain datasets. Experimentalresults show that the proposed model achieves a significant performanceimprovement, especially in the cross-domain few-shot setting, which outperformsthe state-of-the-art model by +7.7% on average."
Large Language and Text-to-3D Models for Engineering Design Optimization,"['Thiago Rios', 'Stefan Menzel', 'Bernhard Sendhoff']",http://arxiv.org/pdf/2307.01230v1.pdf,2023-07-03,"['cs.cl', 'cs.lg', 'cs.ne']","  The current advances in generative AI for learning large neural networkmodels with the capability to produce essays, images, music and even 3D assetsfrom text prompts create opportunities for a manifold of disciplines. In thepresent paper, we study the potential of deep text-to-3D models in theengineering domain, with focus on the chances and challenges when integratingand interacting with 3D assets in computational simulation-based designoptimization. In contrast to traditional design optimization of 3D geometriesthat often searches for the optimum designs using numerical representations,such as B-Spline surface or deformation parameters in vehicle aerodynamicoptimization, natural language challenges the optimization framework byrequiring a different interpretation of variation operators while at the sametime may ease and motivate the human user interaction. Here, we propose andrealize a fully automated evolutionary design optimization framework usingShap-E, a recently published text-to-3D asset network by OpenAI, in the contextof aerodynamic vehicle optimization. For representing text prompts in theevolutionary optimization, we evaluate (a) a bag-of-words approach based onprompt templates and Wordnet samples, and (b) a tokenisation approach based onprompt templates and the byte pair encoding method from GPT4. Our main findingsfrom the optimizations indicate that, first, it is important to ensure that thedesigns generated from prompts are within the object class of application, i.e.diverse and novel designs need to be realistic, and, second, that more researchis required to develop methods where the strength of text prompt variations andthe resulting variations of the 3D designs share causal relations to somedegree to improve the optimization."
Zero-shot information extraction from radiological reports using ChatGPT,"['Danqing Hu', 'Bing Liu', 'Xiaofeng Zhu', 'Xudong Lu', 'Nan Wu']",http://arxiv.org/pdf/2309.01398v2.pdf,2023-09-04,['cs.cl'],"  Electronic health records contain an enormous amount of valuable information,but many are recorded in free text. Information extraction is the strategy totransform the sequence of characters into structured data, which can beemployed for secondary analysis. However, the traditional informationextraction components, such as named entity recognition and relationextraction, require annotated data to optimize the model parameters, which hasbecome one of the major bottlenecks in building information extraction systems.With the large language models achieving good performances on variousdownstream NLP tasks without parameter tuning, it becomes possible to use largelanguage models for zero-shot information extraction. In this study, we aim toexplore whether the most popular large language model, ChatGPT, can extractuseful information from the radiological reports. We first design the prompttemplate for the interested information in the CT reports. Then, we generatethe prompts by combining the prompt template with the CT reports as the inputsof ChatGPT to obtain the responses. A post-processing module is developed totransform the responses into structured extraction results. We conducted theexperiments with 847 CT reports collected from Peking University CancerHospital. The experimental results indicate that ChatGPT can achievecompetitive performances for some extraction tasks compared with the baselineinformation extraction system, but some limitations need to be furtherimproved."
Can Language Models be Biomedical Knowledge Bases?,"['Mujeen Sung', 'Jinhyuk Lee', 'Sean Yi', 'Minji Jeon', 'Sungdong Kim', 'Jaewoo Kang']",http://arxiv.org/pdf/2109.07154v1.pdf,2021-09-15,['cs.cl'],"  Pre-trained language models (LMs) have become ubiquitous in solving variousnatural language processing (NLP) tasks. There has been increasing interest inwhat knowledge these LMs contain and how we can extract that knowledge,treating LMs as knowledge bases (KBs). While there has been much work onprobing LMs in the general domain, there has been little attention to whetherthese powerful LMs can be used as domain-specific KBs. To this end, we createthe BioLAMA benchmark, which is comprised of 49K biomedical factual knowledgetriples for probing biomedical LMs. We find that biomedical LMs with recentlyproposed probing methods can achieve up to 18.51% Acc@5 on retrievingbiomedical knowledge. Although this seems promising given the task difficulty,our detailed analyses reveal that most predictions are highly correlated withprompt templates without any subjects, hence producing similar results on eachrelation and hindering their capabilities to be used as domain-specific KBs. Wehope that BioLAMA can serve as a challenging benchmark for biomedical factualprobing."
HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural  Language Processing,"['Sonish Sivarajkumar', 'Yanshan Wang']",http://arxiv.org/pdf/2203.05061v1.pdf,2022-03-09,"['cs.cl', 'cs.ai', 'cs.ir']","  Deep learning algorithms are dependent on the availability of large-scaleannotated clinical text datasets. The lack of such publicly available datasetsis the biggest bottleneck for the development of clinical Natural LanguageProcessing(NLP) systems. Zero-Shot Learning(ZSL) refers to the use of deeplearning models to classify instances from new classes of which no trainingdata have been seen before. Prompt-based learning is an emerging ZSL techniquewhere we define task-based templates for NLP tasks. We developed a novelprompt-based clinical NLP framework called HealthPrompt and applied theparadigm of prompt-based learning on clinical texts. In this technique, ratherthan fine-tuning a Pre-trained Language Model(PLM), the task definitions aretuned by defining a prompt template. We performed an in-depth analysis ofHealthPrompt on six different PLMs in a no-data setting. Our experiments provethat prompts effectively capture the context of clinical texts and performremarkably well without any training data."
RelationPrompt: Leveraging Prompts to Generate Synthetic Data for  Zero-Shot Relation Triplet Extraction,"['Yew Ken Chia', 'Lidong Bing', 'Soujanya Poria', 'Luo Si']",http://arxiv.org/pdf/2203.09101v1.pdf,2022-03-17,['cs.cl'],"  Despite the importance of relation extraction in building and representingknowledge, less research is focused on generalizing to unseen relations types.We introduce the task setting of Zero-Shot Relation Triplet Extraction(ZeroRTE) to encourage further research in low-resource relation extractionmethods. Given an input sentence, each extracted triplet consists of the headentity, relation label, and tail entity where the relation label is not seen atthe training stage. To solve ZeroRTE, we propose to synthesize relationexamples by prompting language models to generate structured texts. Concretely,we unify language model prompts and structured text approaches to design astructured prompt template for generating synthetic relation samples whenconditioning on relation label prompts (RelationPrompt). To overcome thelimitation for extracting multiple relation triplets in a sentence, we design anovel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSLdatasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shotrelation classification. Our code and data are available atgithub.com/declare-lab/RelationPrompt."
CUP: Curriculum Learning based Prompt Tuning for Implicit Event Argument  Extraction,"['Jiaju Lin', 'Qin Chen', 'Jie Zhou', 'Jian Jin', 'Liang He']",http://arxiv.org/pdf/2205.00498v2.pdf,2022-05-01,['cs.cl'],"  Implicit event argument extraction (EAE) aims to identify arguments thatcould scatter over the document. Most previous work focuses on learning thedirect relations between arguments and the given trigger, while the implicitrelations with long-range dependency are not well studied. Moreover, recentneural network based approaches rely on a large amount of labeled data fortraining, which is unavailable due to the high labelling cost. In this paper,we propose a Curriculum learning based Prompt tuning (CUP) approach, whichresolves implicit EAE by four learning stages. The stages are defined accordingto the relations with the trigger node in a semantic graph, which well capturesthe long-range dependency between arguments and the trigger. In addition, weintegrate a prompt-based encoder-decoder model to elicit related knowledge frompre-trained language models (PLMs) in each stage, where the prompt templatesare adapted with the learning progress to enhance the reasoning for arguments.Experimental results on two well-known benchmark datasets show the greatadvantages of our proposed approach. In particular, we outperform thestate-of-the-art models in both fully-supervised and low-data scenarios."
Let Me Check the Examples: Enhancing Demonstration Learning via Explicit  Imitation,"['Sirui Wang', 'Kaiwen Wei', 'Hongzhi Zhang', 'Yuntao Li', 'Wei Wu']",http://arxiv.org/pdf/2209.00455v1.pdf,2022-08-31,"['cs.lg', 'cs.ai']","  Demonstration learning aims to guide the prompt prediction via providinganswered demonstrations in the few shot settings. Despite achieving promisingresults, existing work only concatenates the answered examples asdemonstrations to the prompt template (including the raw context) without anyadditional operation, neglecting the prompt-demonstration dependencies.Besides, prior research found that randomly replacing the labels ofdemonstrations marginally hurts performance, illustrating that the model couldnot properly learn the knowledge brought by the demonstrations. Inspired by thehuman learning process, in this paper, we introduce Imitation DEMOnstrationLearning (Imitation-Demo) to strengthen demonstration learning via explicitlyimitating human review behaviour, which includes: (1) contrastive learningmechanism to concentrate on the similar demonstrations. (2) demonstration-labelre-prediction method to consolidate known knowledge. Experiment results showthat our proposed method achieves state-of-the-art performance on 11 out of 14classification corpora. Further studies also prove that Imitation-Demostrengthen the association between prompt and demonstrations, which couldprovide the basis for exploring how demonstration learning works."
A Few-shot Approach to Resume Information Extraction via Prompts,"['Chengguang Gan', 'Tatsunori Mori']",http://arxiv.org/pdf/2209.09450v2.pdf,2022-09-20,['cs.cl'],"  Prompt learning's fine-tune performance on text classification tasks hasattracted the NLP community. This paper applies it to resume informationextraction, improving existing methods for this task. We created manualtemplates and verbalizers tailored to resume texts and compared the performanceof Masked Language Model (MLM) and Seq2Seq PLMs. Also, we enhanced theverbalizer design for Knowledgeable Prompt-tuning, contributing to prompttemplate design across NLP tasks. We present the Manual KnowledgeableVerbalizer (MKV), a rule for constructing verbalizers for specificapplications. Our tests show that MKV rules yield more effective, robusttemplates and verbalizers than existing methods. Our MKV approach resolvedsample imbalance, surpassing current automatic prompt methods. This studyunderscores the value of tailored prompt learning for resume extraction,stressing the importance of custom-designed templates and verbalizers."
Distilling Task-specific Logical Rules from Large Pre-trained Models,"['Tao Chen', 'Luxin Liu', 'Xuepeng Jia', 'Baoliang Cui', 'Haihong Tang', 'Siliang Tang']",http://arxiv.org/pdf/2210.02768v1.pdf,2022-10-06,['cs.cl'],"  Logical rules, both transferable and explainable, are widely used as weaklysupervised signals for many downstream tasks such as named entity tagging. Toreduce the human effort of writing rules, previous researchers adopt aniterative approach to automatically learn logical rules from several seedrules. However, obtaining more seed rules can only be accomplished by extrahuman annotation with heavy costs. Limited by the size and quality of the seedrules, the model performance of previous systems is bounded. In this paper, wedevelop a novel framework STREAM to distill task-specific logical rules fromlarge pre-trained models. Specifically, we borrow recent prompt-based languagemodels as the knowledge expert to yield initial seed rules, and based on theformed high-quality instance pool that acts as an intermediary role, we keepteaching the expert to fit our task and learning task-specific logical rules.Experiments on three public named entity tagging benchmarks demonstrate theeffectiveness of our proposed framework. With several predefined prompttemplates, our system has gained significant improvements over previousstate-of-the-art methods."
CLIP model is an Efficient Continual Learner,"['Vishal Thengane', 'Salman Khan', 'Munawar Hayat', 'Fahad Khan']",http://arxiv.org/pdf/2210.03114v1.pdf,2022-10-06,['cs.cv'],"  The continual learning setting aims to learn new tasks over time withoutforgetting the previous ones. The literature reports several significantefforts to tackle this problem with limited or no access to previous task data.Among such efforts, typical solutions offer sophisticated techniques involvingmemory replay, knowledge distillation, model regularization, and dynamicnetwork expansion. The resulting methods have a retraining cost at eachlearning task, dedicated memory requirements, and setting-specific designchoices. In this work, we show that a frozen CLIP (Contrastive Language-ImagePretraining) model offers astounding continual learning performance without anyfine-tuning (zero-shot evaluation). We evaluate CLIP under a variety ofsettings including class-incremental, domain-incremental and task-agnosticincremental learning on five popular benchmarks (ImageNet-100 & 1K, CORe50,CIFAR-100, and TinyImageNet). Without any bells and whistles, the CLIP modeloutperforms the state-of-the-art continual learning approaches in the majorityof the settings. We show the effect on the CLIP model's performance by varyingtext inputs with simple prompt templates. To the best of our knowledge, this isthe first work to report the CLIP zero-shot performance in a continual setting.We advocate the use of this strong yet embarrassingly simple baseline forfuture comparisons in the continual learning tasks."
A Unified Framework for Multi-intent Spoken Language Understanding with  prompting,"['Feifan Song', 'Lianzhe Huang', 'Houfeng Wang']",http://arxiv.org/pdf/2210.03337v1.pdf,2022-10-07,"['cs.cl', 'cs.ai']","  Multi-intent Spoken Language Understanding has great potential for widespreadimplementation. Jointly modeling Intent Detection and Slot Filling in itprovides a channel to exploit the correlation between intents and slots.However, current approaches are apt to formulate these two sub-tasksdifferently, which leads to two issues: 1) It hinders models from effectiveextraction of shared features. 2) Pretty complicated structures are involved toenhance expression ability while causing damage to the interpretability offrameworks. In this work, we describe a Prompt-based Spoken LanguageUnderstanding (PromptSLU) framework, to intuitively unify two sub-tasks intothe same form by offering a common pre-trained Seq2Seq model. In detail, ID andSF are completed by concisely filling the utterance into task-specific prompttemplates as input, and sharing output formats of key-value pairs sequence.Furthermore, variable intents are predicted first, then naturally embedded intoprompts to guide slot-value pairs inference from a semantic perspective.Finally, we are inspired by prevalent multi-task learning to introduce anauxiliary sub-task, which helps to learn relationships among provided labels.Experiment results show that our framework outperforms several state-of-the-artbaselines on two public datasets."
UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical  Simplification?,"['Dennis Aumiller', 'Michael Gertz']",http://arxiv.org/pdf/2301.01764v2.pdf,2023-01-04,['cs.cl'],"  Previous state-of-the-art models for lexical simplification consist ofcomplex pipelines with several components, each of which requires deeptechnical knowledge and fine-tuned interaction to achieve its full potential.As an alternative, we describe a frustratingly simple pipeline based onprompted GPT-3 responses, beating competing approaches by a wide margin insettings with few training instances. Our best-performing submission to theEnglish language track of the TSAR-2022 shared task consists of an ``ensemble''of six different prompt templates with varying context levels. As alate-breaking result, we further detail a language transfer technique thatallows simplification in languages other than English. Applied to the Spanishand Portuguese subset, we achieve state-of-the-art results with only minormodification to the original prompts. Aside from detailing the implementationand setup, we spend the remainder of this work discussing the particularitiesof prompting and implications for future work. Code for the experiments isavailable online at https://github.com/dennlinger/TSAR-2022-Shared-Task"
Prompting Large Language Model for Machine Translation: A Case Study,"['Biao Zhang', 'Barry Haddow', 'Alexandra Birch']",http://arxiv.org/pdf/2301.07069v2.pdf,2023-01-17,"['cs.cl', 'cs.lg']","  Research on prompting has shown excellent performance with little or even nosupervised training across many tasks. However, prompting for machinetranslation is still under-explored in the literature. We fill this gap byoffering a systematic study on prompting strategies for translation, examiningvarious factors for prompt template and demonstration example selection. Wefurther explore the use of monolingual data and the feasibility ofcross-lingual, cross-domain, and sentence-to-document transfer learning inprompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as thetestbed show that 1) the number and the quality of prompt examples matter,where using suboptimal examples degenerates translation; 2) several features ofprompt examples, such as semantic similarity, show significant Spearmancorrelation with their prompting performance; yet, none of the correlations arestrong enough; 3) using pseudo parallel prompt examples constructed frommonolingual data via zero-shot prompting could improve translation; and 4)improved performance is achievable by transferring knowledge from promptexamples selected in other settings. We finally provide an analysis on themodel outputs and discuss several problems that prompting still suffers from."
Global Constraints with Prompting for Zero-Shot Event Argument  Classification,"['Zizheng Lin', 'Hongming Zhang', 'Yangqiu Song']",http://arxiv.org/pdf/2302.04459v1.pdf,2023-02-09,['cs.cl'],"  Determining the role of event arguments is a crucial subtask of eventextraction. Most previous supervised models leverage costly annotations, whichis not practical for open-domain applications. In this work, we propose to useglobal constraints with prompting to effectively tackles event argumentclassification without any annotation and task-specific training. Specifically,given an event and its associated passage, the model first creates several newpassages by prefix prompts and cloze prompts, where prefix prompts indicateevent type and trigger span, and cloze prompts connect each candidate role withthe target argument span. Then, a pre-trained language model scores the newpassages, making the initial prediction. Our novel prompt templates can easilyadapt to all events and argument types without manual effort. Next, the modelregularizes the prediction by global constraints exploiting cross-task,cross-argument, and cross-event relations. Extensive experiments demonstrateour model's effectiveness: it outperforms the best zero-shot baselines by 12.5%and 10.9% F1 on ACE and ERE with given argument spans and by 4.3% and 3.3% F1,respectively, without given argument spans. We have made our code publiclyavailable."
Large Language Models Are State-of-the-Art Evaluators of Translation  Quality,"['Tom Kocmi', 'Christian Federmann']",http://arxiv.org/pdf/2302.14520v2.pdf,2023-02-28,['cs.cl'],"  We describe GEMBA, a GPT-based metric for assessment of translation quality,which works both with a reference translation and without. In our evaluation,we focus on zero-shot prompting, comparing four prompt variants in two modes,based on the availability of the reference. We investigate nine versions of GPTmodels, including ChatGPT and GPT-4. We show that our method for translationquality assessment only works with GPT~3.5 and larger models. Comparing toresults from WMT22's Metrics shared task, our method achieves state-of-the-artaccuracy in both modes when compared to MQM-based human labels. Our results arevalid on the system level for all three WMT22 Metrics shared task languagepairs, namely English into German, English into Russian, and Chinese intoEnglish. This provides a first glimpse into the usefulness of pre-trained,generative large language models for quality assessment of translations. Wepublicly release all our code and prompt templates used for the experimentsdescribed in this work, as well as all corresponding scoring results, to allowfor external validation and reproducibility."
The Prompt Artists,"['Minsuk Chang', 'Stefania Druga', 'Alex Fiannaca', 'Pedro Vergani', 'Chinmay Kulkarni', 'Carrie Cai', 'Michael Terry']",http://arxiv.org/pdf/2303.12253v1.pdf,2023-03-22,['cs.hc'],"  This paper examines the art practices, artwork, and motivations of prolificusers of the latest generation of text-to-image models. Through interviews,observations, and a user survey, we present a sampling of the artistic stylesand describe the developed community of practice around generative AI. We findthat: 1) the text prompt and the resulting image can be considered collectivelyas an art piece prompts as art and 2) prompt templates (prompts with ``slots''for others to fill in with their own words) are developed to create generativeart styles. We discover that the value placed by this community on uniqueoutputs leads to artists seeking specialized vocabulary to produce distinctiveart pieces (e.g., by reading architectural blogs to find phrases to describeimages). We also find that some artists use ""glitches"" in the model that can beturned into artistic styles of their own right. From these findings, we outlinespecific implications for design regarding future prompting and image editingoptions."
WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation,"['Jongheon Jeong', 'Yang Zou', 'Taewan Kim', 'Dongqing Zhang', 'Avinash Ravichandran', 'Onkar Dabeer']",http://arxiv.org/pdf/2303.14814v1.pdf,2023-03-26,"['cs.cv', 'cs.ai', 'cs.cl']","  Visual anomaly classification and segmentation are vital for automatingindustrial quality inspection. The focus of prior research in the field hasbeen on training custom models for each quality inspection task, which requirestask-specific images and annotation. In this paper we move away from thisregime, addressing zero-shot and few-normal-shot anomaly classification andsegmentation. Recently CLIP, a vision-language model, has shown revolutionarygenerality with competitive zero-/few-shot performance in comparison tofull-supervision. But CLIP falls short on anomaly classification andsegmentation tasks. Hence, we propose window-based CLIP (WinCLIP) with (1) acompositional ensemble on state words and prompt templates and (2) efficientextraction and aggregation of window/patch/image-level features aligned withtext. We also propose its few-normal-shot extension WinCLIP+, which usescomplementary information from normal images. In MVTec-AD (and VisA), withoutfurther tuning, WinCLIP achieves 91.8%/85.1% (78.1%/79.6%) AUROC in zero-shotanomaly classification and segmentation while WinCLIP+ does 93.1%/95.2%(83.8%/96.4%) in 1-normal-shot, surpassing state-of-the-art by large margins."
MetricPrompt: Prompting Model as a Relevance Metric for Few-shot Text  Classification,"['Hongyuan Dong', 'Weinan Zhang', 'Wanxiang Che']",http://arxiv.org/pdf/2306.08892v1.pdf,2023-06-15,['cs.cl'],"  Prompting methods have shown impressive performance in a variety of textmining tasks and applications, especially few-shot ones. Despite the promisingprospects, the performance of prompting model largely depends on the design ofprompt template and verbalizer. In this work, we propose MetricPrompt, whicheases verbalizer design difficulty by reformulating few-shot textclassification task into text pair relevance estimation task. MetricPromptadopts prompting model as the relevance metric, further bridging the gapbetween Pre-trained Language Model's (PLM) pre-training objective and textclassification task, making possible PLM's smooth adaption. Taking a trainingsample and a query one simultaneously, MetricPrompt captures cross-samplerelevance information for accurate relevance estimation. We conduct experimentson three widely used text classification datasets across four few-shotsettings. Results show that MetricPrompt outperforms manual verbalizer andother automatic verbalizer design methods across all few-shot settings,achieving new state-of-the-art (SOTA) performance."
TrustGPT: A Benchmark for Trustworthy and Responsible Large Language  Models,"['Yue Huang', 'Qihui Zhang', 'Philip S. Y', 'Lichao Sun']",http://arxiv.org/pdf/2306.11507v1.pdf,2023-06-20,"['cs.cl', 'cs.ai']","  Large Language Models (LLMs) such as ChatGPT, have gained significantattention due to their impressive natural language processing capabilities. Itis crucial to prioritize human-centered principles when utilizing these models.Safeguarding the ethical and moral compliance of LLMs is of utmost importance.However, individual ethical issues have not been well studied on the latestLLMs. Therefore, this study aims to address these gaps by introducing a newbenchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs inthree crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPTexamines toxicity in language models by employing toxic prompt templatesderived from social norms. It then quantifies the extent of bias in models bymeasuring quantifiable toxicity values across different groups. Lastly,TrustGPT assesses the value of conversation generation models from both activevalue-alignment and passive value-alignment tasks. Through the implementationof TrustGPT, this research aims to enhance our understanding of the performanceof conversation generation models and promote the development of languagemodels that are more ethical and socially responsible."
DAPrompt: Deterministic Assumption Prompt Learning for Event Causality  Identification,"['Wei Xiang', 'Chuanhong Zhan', 'Bang Wang']",http://arxiv.org/pdf/2307.09813v1.pdf,2023-07-19,['cs.cl'],"  Event Causality Identification (ECI) aims at determining whether there is acausal relation between two event mentions. Conventional prompt learningdesigns a prompt template to first predict an answer word and then maps it tothe final decision. Unlike conventional prompts, we argue that predicting ananswer word may not be a necessary prerequisite for the ECI task. Instead, wecan first make a deterministic assumption on the existence of causal relationbetween two events and then evaluate its rationality to either accept or rejectthe assumption. The design motivation is to try the most utilization of theencyclopedia-like knowledge embedded in a pre-trained language model. In lightof such considerations, we propose a deterministic assumption prompt learningmodel, called DAPrompt, for the ECI task. In particular, we design a simpledeterministic assumption template concatenating with the input event pair,which includes two masks as predicted events' tokens. We use the probabilitiesof predicted events to evaluate the assumption rationality for the final eventcausality decision. Experiments on the EventStoryLine corpus andCausal-TimeBank corpus validate our design objective in terms of significantperformance improvements over the state-of-the-art algorithms."
DiffuGen: Adaptable Approach for Generating Labeled Image Datasets using  Stable Diffusion Models,"['Michael Shenoda', 'Edward Kim']",http://arxiv.org/pdf/2309.00248v1.pdf,2023-09-01,"['cs.cv', 'cs.ai']","  Generating high-quality labeled image datasets is crucial for trainingaccurate and robust machine learning models in the field of computer vision.However, the process of manually labeling real images is often time-consumingand costly. To address these challenges associated with dataset generation, weintroduce ""DiffuGen,"" a simple and adaptable approach that harnesses the powerof stable diffusion models to create labeled image datasets efficiently. Byleveraging stable diffusion models, our approach not only ensures the qualityof generated datasets but also provides a versatile solution for labelgeneration. In this paper, we present the methodology behind DiffuGen, whichcombines the capabilities of diffusion models with two distinct labelingtechniques: unsupervised and supervised. Distinctively, DiffuGen employs prompttemplating for adaptable image generation and textual inversion to enhancediffusion model capabilities."
Mitigating Word Bias in Zero-shot Prompt-based Classifiers,"['Adian Liusie', 'Potsawee Manakul', 'Mark J. F. Gales']",http://arxiv.org/pdf/2309.04992v1.pdf,2023-09-10,['cs.cl'],"  Prompt-based classifiers are an attractive approach for zero-shotclassification. However, the precise choice of the prompt template and labelwords can largely influence performance, with semantically equivalent settingsoften showing notable performance difference. This discrepancy can be partlyattributed to word biases, where the classifier may be biased towards classes.To address this problem, it is possible to optimise classification thresholdson a labelled data set, however, this mitigates some of the advantages ofprompt-based classifiers. This paper instead approaches this problem byexamining the expected marginal probabilities of the classes. Here,probabilities are reweighted to have a uniform prior over classes, in anunsupervised fashion. Further, we draw a theoretical connection between theclass priors and the language models' word prior, and offer the ability to seta threshold in a zero-resource fashion. We show that matching class priorscorrelates strongly with the oracle upper bound performance and demonstratelarge consistent performance gains for prompt settings over a range of NLPtasks."
Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of  Text-To-Image Models,"['Mor Ventura', 'Eyal Ben-David', 'Anna Korhonen', 'Roi Reichart']",http://arxiv.org/pdf/2310.01929v2.pdf,2023-10-03,"['cs.cl', 'cs.ai', 'cs.lg']","  Text-To-Image (TTI) models, such as DALL-E and StableDiffusion, havedemonstrated remarkable prompt-based image generation capabilities.Multilingual encoders may have a substantial impact on the cultural agency ofthese models, as language is a conduit of culture. In this study, we explorethe cultural perception embedded in TTI models by characterizing culture acrossthree hierarchical tiers: cultural dimensions, cultural domains, and culturalconcepts. Based on this ontology, we derive prompt templates to unlock thecultural knowledge in TTI models, and propose a comprehensive suite ofevaluation techniques, including intrinsic evaluations using the CLIP space,extrinsic evaluations with a Visual-Question-Answer (VQA) model and humanassessments, to evaluate the cultural content of TTI-generated images. Tobolster our research, we introduce the CulText2I dataset, derived from fourdiverse TTI models and spanning ten languages. Our experiments provide insightsregarding Do, What, Which and How research questions about the nature ofcultural encoding in TTI models, paving the way for cross-cultural applicationsof these models."
LLM4DV: Using Large Language Models for Hardware Test Stimuli Generation,"['Zixi Zhang', 'Greg Chadwick', 'Hugo McNally', 'Yiren Zhao', 'Robert Mullins']",http://arxiv.org/pdf/2310.04535v1.pdf,2023-10-06,"['cs.lg', 'cs.ar']","  Test stimuli generation has been a crucial but labor-intensive task inhardware design verification. In this paper, we revolutionize this process byharnessing the power of large language models (LLMs) and present a novelbenchmarking framework, LLM4DV. This framework introduces a prompt template forinteractively eliciting test stimuli from the LLM, along with four innovativeprompting improvements to support the pipeline execution and further enhanceits performance. We compare LLM4DV to traditional constrained-random testing(CRT), using three self-designed design-under-test (DUT) modules. Experimentsdemonstrate that LLM4DV excels in efficiently handling straightforward DUTscenarios, leveraging its ability to employ basic mathematical reasoning andpre-trained knowledge. While it exhibits reduced efficiency in complex tasksettings, it still outperforms CRT in relative terms. The proposed frameworkand the DUT modules used in our experiments will be open-sourced uponpublication."
The Limits of ChatGPT in Extracting Aspect-Category-Opinion-Sentiment  Quadruples: A Comparative Analysis,"['Xiancai Xu', 'Jia-Dong Zhang', 'Rongchang Xiao', 'Lei Xiong']",http://arxiv.org/pdf/2310.06502v1.pdf,2023-10-10,['cs.cl'],"  Recently, ChatGPT has attracted great attention from both industry andacademia due to its surprising abilities in natural language understanding andgeneration. We are particularly curious about whether it can achieve promisingperformance on one of the most complex tasks in aspect-based sentimentanalysis, i.e., extracting aspect-category-opinion-sentiment quadruples fromtexts. To this end, in this paper we develop a specialized prompt template thatenables ChatGPT to effectively tackle this complex quadruple extraction task.Further, we propose a selection method on few-shot examples to fully exploitthe in-context learning ability of ChatGPT and uplift its effectiveness on thiscomplex task. Finally, we provide a comparative evaluation on ChatGPT againstexisting state-of-the-art quadruple extraction models based on four publicdatasets and highlight some important findings regarding the capabilityboundaries of ChatGPT in the quadruple extraction."
Estimating Uncertainty in Multimodal Foundation Models using Public  Internet Data,"['Shiladitya Dutta', 'Hongbo Wei', 'Lars van der Laan', 'Ahmed M. Alaa']",http://arxiv.org/pdf/2310.09926v2.pdf,2023-10-15,['cs.ai'],"  Foundation models are trained on vast amounts of data at scale usingself-supervised learning, enabling adaptation to a wide range of downstreamtasks. At test time, these models exhibit zero-shot capabilities through whichthey can classify previously unseen (user-specified) categories. In this paper,we address the problem of quantifying uncertainty in these zero-shotpredictions. We propose a heuristic approach for uncertainty estimation inzero-shot settings using conformal prediction with web data. Given a set ofclasses at test time, we conduct zero-shot classification with CLIP-stylemodels using a prompt template, e.g., ""an image of a <category>"", and use thesame template as a search query to source calibration data from the open web.Given a web-based calibration set, we apply conformal prediction with a novelconformity score that accounts for potential errors in retrieved web data. Weevaluate the utility of our proposed method in Biomedical foundation models;our preliminary results show that web-based conformal prediction sets achievethe target coverage with satisfactory efficiency on a variety of biomedicaldatasets."
A Search for Prompts: Generating Structured Answers from Contracts,"['Adam Roegiest', 'Radha Chitta', 'Jonathan Donnelly', 'Maya Lash', 'Alexandra Vtyurina', 'François Longtin']",http://arxiv.org/pdf/2310.10141v1.pdf,2023-10-16,['cs.cv'],"  In many legal processes being able to action on the concrete implication of alegal question can be valuable to automating human review or signalling certainconditions (e.g., alerts around automatic renewal). To support such tasks, wepresent a form of legal question answering that seeks to return one (or more)fixed answers for a question about a contract clause. After showing thatunstructured generative question answering can have questionable outcomes forsuch a task, we discuss our exploration methodology for legal questionanswering prompts using OpenAI's \textit{GPT-3.5-Turbo} and provide a summaryof insights.  Using insights gleaned from our qualitative experiences, we compare ourproposed template prompts against a common semantic matching approach and findthat our prompt templates are far more accurate despite being less reliable inthe exact response return. With some additional tweaks to prompts and the useof in-context learning, we are able to further improve the performance of ourproposed strategy while maximizing the reliability of responses as best we can."
Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring  Fine-Grained Relevance Labels,"['Honglei Zhuang', 'Zhen Qin', 'Kai Hui', 'Junru Wu', 'Le Yan', 'Xuanhui Wang', 'Michael Bendersky']",http://arxiv.org/pdf/2310.14122v2.pdf,2023-10-21,['cs.ir'],"  Zero-shot text rankers powered by recent LLMs achieve remarkable rankingperformance by simply prompting. Existing prompts for pointwise LLM rankersmostly ask the model to choose from binary relevance labels like ""Yes"" and""No"". However, the lack of intermediate relevance label options may cause theLLM to provide noisy or biased answers for documents that are partiallyrelevant to the query. We propose to incorporate fine-grained relevance labelsinto the prompt for LLM rankers, enabling them to better differentiate amongdocuments with different levels of relevance to the query and thus derive amore accurate ranking. We study two variants of the prompt template, coupledwith different numbers of relevance levels. Our experiments on 8 BEIR data setsshow that adding fine-grained relevance labels significantly improves theperformance of LLM rankers."
"Large Language Models can Share Images, Too!","['Young-Jun Lee', 'Jonghwan Hyeon', 'Ho-Jin Choi']",http://arxiv.org/pdf/2310.14804v1.pdf,2023-10-23,"['cs.cv', 'cs.ai', 'cs.cl']","  This paper explores the image-sharing capability of Large Language Models(LLMs), such as InstructGPT, ChatGPT, and GPT-4, in a zero-shot setting,without the help of visual foundation models. Inspired by the two-stage processof image-sharing in human dialogues, we propose a two-stage framework thatallows LLMs to predict potential image-sharing turns and generate related imagedescriptions using our effective restriction-based prompt template. Withextensive experiments, we unlock the \textit{image-sharing} capability of LLMsin zero-shot prompting, with GPT-4 achieving the best performance.Additionally, we uncover the emergent \textit{image-sharing} ability inzero-shot prompting, demonstrating the effectiveness of restriction-basedprompts in both stages of our framework. Based on this framework, we augmentthe PhotoChat dataset with images generated by Stable Diffusion at predictedturns, namely PhotoChat++. To our knowledge, this is the first study to assessthe image-sharing ability of LLMs in a zero-shot setting without visualfoundation models. The source code and the dataset will be released afterpublication."
AllTogether: Investigating the Efficacy of Spliced Prompt for Web  Navigation using Large Language Models,"['Jiarun Liu', 'Wentao Hu', 'Chunhong Zhang']",http://arxiv.org/pdf/2310.18331v2.pdf,2023-10-20,"['cs.cl', 'cs.ai', 'cs.lg']","  Large Language Models (LLMs) have emerged as promising agents for webnavigation tasks, interpreting objectives and interacting with web pages.However, the efficiency of spliced prompts for such tasks remainsunderexplored. We introduces AllTogether, a standardized prompt template thatenhances task context representation, thereby improving LLMs' performance inHTML-based web navigation. We evaluate the efficacy of this approach throughprompt learning and instruction finetuning based on open-source Llama-2 andAPI-accessible GPT models. Our results reveal that models like GPT-4 outperformsmaller models in web navigation tasks. Additionally, we find that the lengthof HTML snippet and history trajectory significantly influence performance, andprior step-by-step instructions prove less effective than real-timeenvironmental feedback. Overall, we believe our work provides valuable insightsfor future research in LLM-driven web agents."
GistScore: Learning Better Representations for In-Context Example  Selection with Gist Bottlenecks,"['Shivanshu Gupta', 'Clemens Rosenbaum', 'Ethan R. Elenberg']",http://arxiv.org/pdf/2311.09606v1.pdf,2023-11-16,['cs.cl'],"  Large language models (LLMs) have the ability to perform in-context learning(ICL) of new tasks by conditioning on prompts comprising a few task examples.This work studies the problem of selecting the best examples given a candidatepool to improve ICL performance on given a test input. Existing approacheseither require training with feedback from a much larger LLM or arecomputationally expensive. We propose a novel metric, GistScore, based onExample Gisting, a novel approach for training example retrievers for ICL usingan attention bottleneck via Gisting, a recent technique for compressing taskinstructions. To tradeoff performance with ease of use, we experiment with bothfine-tuning gist models on each dataset and multi-task training a single modelon a large collection of datasets. On 21 diverse datasets spanning 9 tasks, weshow that our fine-tuned models get state-of-the-art ICL performance with 20%absolute average gain over off-the-shelf retrievers and 7% over the best priormethods. Our multi-task model generalizes well out-of-the-box to new taskcategories, datasets, and prompt templates with retrieval speeds that areconsistently thousands of times faster than the best prior training-freemethod."
HKUST at SemEval-2023 Task 1: Visual Word Sense Disambiguation with  Context Augmentation and Visual Assistance,"['Zhuohao Yin', 'Xin Huang']",http://arxiv.org/pdf/2311.18273v1.pdf,2023-11-30,"['cs.cv', 'cs.mm']","  Visual Word Sense Disambiguation (VWSD) is a multi-modal task that aims toselect, among a batch of candidate images, the one that best entails the targetword's meaning within a limited context. In this paper, we propose amulti-modal retrieval framework that maximally leverages pretrainedVision-Language models, as well as open knowledge bases and datasets. Oursystem consists of the following key components: (1) Gloss matching: apretrained bi-encoder model is used to match contexts with proper senses of thetarget words; (2) Prompting: matched glosses and other textual information,such as synonyms, are incorporated using a prompting template; (3) Imageretrieval: semantically matching images are retrieved from large open datasetsusing prompts as queries; (4) Modality fusion: contextual information fromdifferent modalities are fused and used for prediction. Although our systemdoes not produce the most competitive results at SemEval-2023 Task 1, we arestill able to beat nearly half of the teams. More importantly, our experimentsreveal acute insights for the field of Word Sense Disambiguation (WSD) andmulti-modal learning. Our code is available on GitHub."
ICL Markup: Structuring In-Context Learning using Soft-Token Tags,"['Marc-Etienne Brunet', 'Ashton Anderson', 'Richard Zemel']",http://arxiv.org/pdf/2312.07405v1.pdf,2023-12-12,"['cs.cl', 'cs.lg']","  Large pretrained language models (LLMs) can be rapidly adapted to a widevariety of tasks via a text-to-text approach, where the instruction and inputare fed to the model in natural language. Combined with in-context learning(ICL), this paradigm is impressively flexible and powerful. However, it alsoburdens users with an overwhelming number of choices, many of them arbitrary.Inspired by markup languages like HTML, we contribute a method of usingsoft-token tags to compose prompt templates. This approach reduces arbitrarydecisions and streamlines the application of ICL. Our method is a form ofmeta-learning for ICL; it learns these tags in advance during aparameter-efficient fine-tuning ``warm-up'' process. The tags can subsequentlybe used in templates for ICL on new, unseen tasks without any additionalfine-tuning. Our experiments with this approach yield promising initialresults, improving LLM performance on important enterprise applications such asfew-shot and open-world intent detection, as well as text classification innews and legal domains."
KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization  for Relation Extraction,"['Xiang Chen', 'Ningyu Zhang', 'Xin Xie', 'Shumin Deng', 'Yunzhi Yao', 'Chuanqi Tan', 'Fei Huang', 'Luo Si', 'Huajun Chen']",http://arxiv.org/pdf/2104.07650v7.pdf,2021-04-15,"['cs.cl', 'cs.ai', 'cs.ir', 'cs.lg']","  Recently, prompt-tuning has achieved promising results for specific few-shotclassification tasks. The core idea of prompt-tuning is to insert text pieces(i.e., templates) into the input and transform a classification task into amasked language modeling problem. However, for relation extraction, determiningan appropriate prompt template requires domain expertise, and it is cumbersomeand time-consuming to obtain a suitable label word. Furthermore, there existsabundant semantic and prior knowledge among the relation labels that cannot beignored. To this end, we focus on incorporating knowledge among relation labelsinto prompt-tuning for relation extraction and propose a Knowledge-awarePrompt-tuning approach with synergistic optimization (KnowPrompt).Specifically, we inject latent knowledge contained in relation labels intoprompt construction with learnable virtual type words and answer words. Then,we synergistically optimize their representation with structured constraints.Extensive experimental results on five datasets with standard and low-resourcesettings demonstrate the effectiveness of our approach. Our code and datasetsare available in https://github.com/zjunlp/KnowPrompt for reproducibility."
Prompt-based Zero-shot Relation Extraction with Semantic Knowledge  Augmentation,"['Jiaying Gong', 'Hoda Eldardiry']",http://arxiv.org/pdf/2112.04539v2.pdf,2021-12-08,['cs.cl'],"  In relation triplet extraction (RTE), recognizing unseen (new) relations forwhich there are no training instances is a challenging task. Efforts have beenmade to recognize unseen relations based on question-answering models orrelation descriptions. However, these approaches miss the semantic informationabout connections between seen and unseen relations. In this paper, We proposea prompt-based model with semantic knowledge augmentation (ZS-SKA) to recognizeunseen relations under the zero-shot setting. We present a new word-levelanalogy-based sentence translation rule and generate augmented instances withunseen relations from instances with seen relations using that new rule. Wedesign prompts with weighted virtual label construction based on an externalknowledge graph to integrate semantic knowledge information learned from seenrelations. Instead of using the actual label sets in the prompt template, weconstruct weighted virtual label words. We learn the representations of bothseen and unseen relations with augmented instances and prompts. We thencalculate the distance between the generated representations using prototypicalnetworks to predict unseen relations. Extensive experiments conducted on threepublic datasets FewRel, Wiki-ZSL, and NYT, show that ZS-SKA outperformsstate-of-the-art methods under the zero-shot scenarios. Our experimentalresults also demonstrate the effectiveness and robustness of ZS-SKA."
Ground-Truth Labels Matter: A Deeper Look into Input-Label  Demonstrations,"['Kang Min Yoo', 'Junyeob Kim', 'Hyuhng Joon Kim', 'Hyunsoo Cho', 'Hwiyeol Jo', 'Sang-Woo Lee', 'Sang-goo Lee', 'Taeuk Kim']",http://arxiv.org/pdf/2205.12685v2.pdf,2022-05-25,"['cs.cl', 'cs.ai', 'cs.lg']","  Despite recent explosion of interests in in-context learning, the underlyingmechanism and the precise impact of the quality of demonstrations remainelusive. Intuitively, ground-truth labels should have as much impact inin-context learning (ICL) as supervised learning, but recent work reported thatthe input-label correspondence is significantly less important than previouslythought. Intrigued by this counter-intuitive observation, we re-examine theimportance of ground-truth labels in in-context learning. With the introductionof two novel metrics, namely Label-Correctness Sensitivity and Ground-truthLabel Effect Ratio (GLER), we were able to conduct quantifiable analysis on theimpact of ground-truth label demonstrations. Through extensive analyses, wefind that the correct input-label mappings can have varying impacts on thedownstream in-context learning performances, depending on the experimentalconfiguration. Through additional studies, we identify key components, such asthe verbosity of prompt templates and the language model size, as thecontrolling factor to achieve more noise-resilient ICL."
DynaMaR: Dynamic Prompt with Mask Token Representation,"['Xiaodi Sun', 'Sunny Rajagopalan', 'Priyanka Nigam', 'Weiyi Lu', 'Yi Xu', 'Belinda Zeng', 'Trishul Chilimbi']",http://arxiv.org/pdf/2206.02982v1.pdf,2022-06-07,"['cs.cl', 'cs.lg']","  Recent research has shown that large language models pretrained usingunsupervised approaches can achieve significant performance improvement on manydownstream tasks. Typically when adapting these language models to downstreamtasks, like a classification or regression task, we employ a fine-tuningparadigm in which the sentence representation from the language model is inputto a task-specific head; the model is then fine-tuned end-to-end. However, withthe emergence of models like GPT-3, prompt-based fine-tuning has been proven tobe a successful approach for few-shot tasks. Inspired by this work, we studydiscrete prompt technologies in practice. There are two issues that arise withthe standard prompt approach. First, it can overfit on the prompt template.Second, it requires manual effort to formulate the downstream task as alanguage model problem. In this paper, we propose an improvement toprompt-based fine-tuning that addresses these two issues. We refer to ourapproach as DynaMaR -- Dynamic Prompt with Mask Token Representation. Resultsshow that DynaMaR can achieve an average improvement of 10% in few-shotsettings and improvement of 3.7% in data-rich settings over the standardfine-tuning approach on four e-commerce applications."
Rethinking the Event Coding Pipeline with Prompt Entailment,"['Clément Lefebvre', 'Niklas Stoehr']",http://arxiv.org/pdf/2210.05257v2.pdf,2022-10-11,"['cs.cl', 'cs.hc', 'cs.lg']","  For monitoring crises, political events are extracted from the news. Thelarge amount of unstructured full-text event descriptions makes a case-by-caseanalysis unmanageable, particularly for low-resource humanitarian aidorganizations. This creates a demand to classify events into event types, atask referred to as event coding. Typically, domain experts craft an event typeontology, annotators label a large dataset and technical experts develop asupervised coding system. In this work, we propose PR-ENT, a new event codingapproach that is more flexible and resource-efficient, while maintainingcompetitive accuracy: first, we extend an event description such as ""Militaryinjured two civilians'' by a template, e.g. ""People were [Z]"" and prompt apre-trained (cloze) language model to fill the slot Z. Second, we select answercandidates Z* = {""injured'', ""hurt""...} by treating the event description aspremise and the filled templates as hypothesis in a textual entailment task.This allows domain experts to draft the codebook directly as labeled promptsand interpretable answer candidates. This human-in-the-loop process is guidedby our interactive codebook design tool. We evaluate PR-ENT in severalrobustness checks: perturbing the event description and prompt template,restricting the vocabulary and removing contextual information."
Visual Prompting for Adversarial Robustness,"['Aochuan Chen', 'Peter Lorenz', 'Yuguang Yao', 'Pin-Yu Chen', 'Sijia Liu']",http://arxiv.org/pdf/2210.06284v4.pdf,2022-10-12,"['cs.cv', 'cs.cr', 'cs.lg']","  In this work, we leverage visual prompting (VP) to improve adversarialrobustness of a fixed, pre-trained model at testing time. Compared toconventional adversarial defenses, VP allows us to design universal (i.e.,data-agnostic) input prompting templates, which have plug-and-play capabilitiesat testing time to achieve desired model performance without introducing muchcomputation overhead. Although VP has been successfully applied to improvingmodel generalization, it remains elusive whether and how it can be used todefend against adversarial attacks. We investigate this problem and show thatthe vanilla VP approach is not effective in adversarial defense since auniversal input prompt lacks the capacity for robust learning againstsample-specific adversarial perturbations. To circumvent it, we propose a newVP method, termed Class-wise Adversarial Visual Prompting (C-AVP), to generateclass-wise visual prompts so as to not only leverage the strengths of ensembleprompts but also optimize their interrelations to improve model robustness. Ourexperiments show that C-AVP outperforms the conventional VP method, with 2.1Xstandard accuracy gain and 2X robust accuracy gain. Compared to classicaltest-time defenses, C-AVP also yields a 42X inference time speedup."
Continuous Prompt Tuning Based Textual Entailment Model for E-commerce  Entity Typing,"['Yibo Wang', 'Congying Xia', 'Guan Wang', 'Philip Yu']",http://arxiv.org/pdf/2211.02483v1.pdf,2022-11-04,['cs.cl'],"  The explosion of e-commerce has caused the need for processing and analysisof product titles, like entity typing in product titles. However, the rapidactivity in e-commerce has led to the rapid emergence of new entities, which isdifficult to be solved by general entity typing. Besides, product titles ine-commerce have very different language styles from text data in generaldomain. In order to handle new entities in product titles and address thespecial language styles problem of product titles in e-commerce domain, wepropose our textual entailment model with continuous prompt tuning basedhypotheses and fusion embeddings for e-commerce entity typing. First, wereformulate the entity typing task into a textual entailment problem to handlenew entities that are not present during training. Second, we design a model toautomatically generate textual entailment hypotheses using a continuous prompttuning method, which can generate better textual entailment hypotheses withoutmanual design. Third, we utilize the fusion embeddings of BERT embedding andCharacterBERT embedding with a two-layer MLP classifier to solve the problemthat the language styles of product titles in e-commerce are different fromthat of general domain. To analyze the effect of each contribution, we comparethe performance of entity typing and textual entailment model, and conductablation studies on continuous prompt tuning and fusion embeddings. We alsoevaluate the impact of different prompt template initialization for thecontinuous prompt tuning. We show our proposed model improves the average F1score by around 2% compared to the baseline BERT entity typing model."
Multi-label Few-shot ICD Coding as Autoregressive Generation with Prompt,"['Zhichao Yang', 'Sunjae Kwon', 'Zonghai Yao', 'Hong Yu']",http://arxiv.org/pdf/2211.13813v2.pdf,2022-11-24,"['cs.cl', 'cs.ai']","  Automatic International Classification of Diseases (ICD) coding aims toassign multiple ICD codes to a medical note with an average of 3,000+ tokens.This task is challenging due to the high-dimensional space of multi-labelassignment (155,000+ ICD code candidates) and the long-tail challenge - ManyICD codes are infrequently assigned yet infrequent ICD codes are importantclinically. This study addresses the long-tail challenge by transforming thismulti-label classification task into an autoregressive generation task.Specifically, we first introduce a novel pretraining objective to generate freetext diagnoses and procedure using the SOAP structure, the medical logicphysicians use for note documentation. Second, instead of directly predictingthe high dimensional space of ICD codes, our model generates the lowerdimension of text descriptions, which then infer ICD codes. Third, we designeda novel prompt template for multi-label classification. We evaluate ourGeneration with Prompt model with the benchmark of all code assignment(MIMIC-III-full) and few shot ICD code assignment evaluation benchmark(MIMIC-III-few). Experiments on MIMIC-III-few show that our model performs witha marco F1 30.2, which substantially outperforms the previous MIMIC-III-fullSOTA model (marco F1 4.3) and the model specifically designed for few/zero shotsetting (marco F1 18.7). Finally, we design a novel ensemble learner, a crossattention reranker with prompts, to integrate previous SOTA and our bestfew-shot coding predictions. Experiments on MIMIC-III-full show that ourensemble learner substantially improves both macro and micro F1, from 10.4 to14.6 and from 58.2 to 59.1, respectively."
LabelPrompt: Effective Prompt-based Learning for Relation Classification,"['Wenjie Zhang', 'Xiaoning Song', 'Zhenhua Feng', 'Tianyang Xu', 'Xiaojun Wu']",http://arxiv.org/pdf/2302.08068v2.pdf,2023-02-16,"['cs.cl', 'cs.ai', 'cs.ir', 'cs.lg']","  Recently, prompt-based learning has gained popularity across many naturallanguage processing (NLP) tasks by reformulating them into a cloze-style formatto better align pre-trained language models (PLMs) with downstream tasks.However, applying this approach to relation classification poses uniquechallenges. Specifically, associating natural language words that fill themasked token with semantic relation labels (\textit{e.g.}\textit{``org:founded\_by}'') is difficult. To address this challenge, thispaper presents a novel prompt-based learning method, namely LabelPrompt, forthe relation classification task. Motivated by the intuition to ``GIVE MODELCHOICES!'', we first define additional tokens to represent relation labels,which regard these tokens as the verbaliser with semantic initialisation andexplicitly construct them with a prompt template method. Then, to mitigateinconsistency between predicted relations and given entities, we implement anentity-aware module with contrastive learning. Last, we conduct an attentionquery strategy within the self-attention layer to differentiates prompt tokensand sequence tokens. Together, these strategies enhance the adaptability ofprompt-based learning, especially when only small labelled datasets isavailable. Comprehensive experiments on benchmark datasets demonstrate thesuperiority of our method, particularly in the few-shot scenario."
Adapting Prompt for Few-shot Table-to-Text Generation,"['Zhixin Guo', 'Minyxuan Yan', 'Jiexing Qi', 'Jianping Zhou', 'Ziwei He', 'Zhouhan Lin', 'Guanjie Zheng', 'Xinbing Wang']",http://arxiv.org/pdf/2302.12468v2.pdf,2023-02-24,['cs.cl'],"  Pretrained language models (PLMs) have made remarkable progress intable-to-text generation tasks. However, the lack of domain-specific knowledgemakes it challenging to bridge the topological gap between tabular data andtext, especially in real-world applications with limited resources. To mitigatethe limitation of insufficient labeled data, we propose a novel framework:Adapt-Prompt-to-Generate (AdaPTGen). The core insight of AdaPTGen is to adaptprompt templates of domain-specific knowledge into the model, which brings atleast three benefits: (1) it injects representation of normal table-relateddescriptions to bridge the topological gap between tabular data and texts; (2)it enables us to use large amounts of unlabeled domain-specific knowledgefully, which can alleviate the PLMs' inherent shortcomings of lacking domainknowledge; (3) it allows us to design various tasks to explore thedomain-specific knowledge. Extensive experiments and analyses are conducted onthree open-domain few-shot natural language generation (NLG) data sets: Humans,Songs, and Books. Compared to previous state-of-the-art approaches, our modelachieves superior performance in terms of both fluency and accuracy."
Model-tuning Via Prompts Makes NLP Models Adversarially Robust,"['Mrigank Raman', 'Pratyush Maini', 'J. Zico Kolter', 'Zachary C. Lipton', 'Danish Pruthi']",http://arxiv.org/pdf/2303.07320v2.pdf,2023-03-13,"['cs.cl', 'cs.lg']","  In recent years, NLP practitioners have converged on the following practice:(i) import an off-the-shelf pretrained (masked) language model; (ii) append amultilayer perceptron atop the CLS token's hidden representation (with randomlyinitialized weights); and (iii) fine-tune the entire model on a downstream task(MLP-FT). This procedure has produced massive gains on standard NLP benchmarks,but these models remain brittle, even to mild adversarial perturbations. Inthis work, we demonstrate surprising gains in adversarial robustness enjoyed byModel-tuning Via Prompts (MVP), an alternative method of adapting to downstreamtasks. Rather than appending an MLP head to make output prediction, MVP appendsa prompt template to the input, and makes prediction via textinfilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3different models, MVP improves performance against adversarial substitutions byan average of 8% over standard methods and even outperforms adversarialtraining-based state-of-art defenses by 3.5%. By combining MVP with adversarialtraining, we achieve further improvements in adversarial robustness whilemaintaining performance on unperturbed examples. Finally, we conduct ablationsto investigate the mechanism underlying these gains. Notably, we find that themain causes of vulnerability of MLP-FT can be attributed to the misalignmentbetween pre-training and fine-tuning tasks, and the randomly initialized MLPparameters."
"PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using  Visual Analytics for Large Language Models","['Aditi Mishra', 'Utkarsh Soni', 'Anjana Arunkumar', 'Jinbin Huang', 'Bum Chul Kwon', 'Chris Bryan']",http://arxiv.org/pdf/2304.01964v2.pdf,2023-04-04,['cs.hc'],"  Large Language Models (LLMs) have gained widespread popularity due to theirability to perform ad-hoc Natural Language Processing (NLP) tasks with a simplenatural language prompt. Part of the appeal for LLMs is their approachabilityto the general public, including individuals with no prior technical experiencein NLP techniques. However, natural language prompts can vary significantly interms of their linguistic structure, context, and other semantics. Modifyingone or more of these aspects can result in significant differences in taskperformance. Non-expert users may find it challenging to identify the changesneeded to improve a prompt, especially when they lack domain-specific knowledgeand lack appropriate feedback. To address this challenge, we present PromptAid,a visual analytics system designed to interactively create, refine, and testprompts through exploration, perturbation, testing, and iteration. PromptAiduses multiple, coordinated visualizations which allow users to improve promptsby using the three strategies: keyword perturbations, paraphrasingperturbations, and obtaining the best set of in-context few-shot examples.PromptAid was designed through an iterative prototyping process involving NLPexperts and was evaluated through quantitative and qualitative assessments forLLMs. Our findings indicate that PromptAid helps users to iterate over prompttemplate alterations with less cognitive overhead, generate diverse promptswith help of recommendations, and analyze the performance of the generatedprompts while surpassing existing state-of-the-art prompting interfaces inperformance."
FashionSAP: Symbols and Attributes Prompt for Fine-grained Fashion  Vision-Language Pre-training,"['Yunpeng Han', 'Lisai Zhang', 'Qingcai Chen', 'Zhijian Chen', 'Zhonghua Li', 'Jianxin Yang', 'Zhao Cao']",http://arxiv.org/pdf/2304.05051v1.pdf,2023-04-11,"['cs.cv', 'cs.cl']","  Fashion vision-language pre-training models have shown efficacy for a widerange of downstream tasks. However, general vision-language pre-training modelspay less attention to fine-grained domain features, while these features areimportant in distinguishing the specific domain tasks from general tasks. Wepropose a method for fine-grained fashion vision-language pre-training based onfashion Symbols and Attributes Prompt (FashionSAP) to model fine-grainedmulti-modalities fashion attributes and characteristics. Firstly, we proposethe fashion symbols, a novel abstract fashion concept layer, to representdifferent fashion items and to generalize various kinds of fine-grained fashionfeatures, making modelling fine-grained attributes more effective. Secondly,the attributes prompt method is proposed to make the model learn specificattributes of fashion items explicitly. We design proper prompt templatesaccording to the format of fashion data. Comprehensive experiments areconducted on two public fashion benchmarks, i.e., FashionGen and FashionIQ, andFashionSAP gets SOTA performances for four popular fashion tasks. The ablationstudy also shows the proposed abstract fashion symbols, and the attributeprompt method enables the model to acquire fine-grained semantics in thefashion domain effectively. The obvious performance gains from FashionSAPprovide a new baseline for future fashion task research."
"A study on Prompt Design, Advantages and Limitations of ChatGPT for Deep  Learning Program Repair","['Jialun Cao', 'Meiziniu Li', 'Ming Wen', 'Shing-chi Cheung']",http://arxiv.org/pdf/2304.08191v1.pdf,2023-04-17,['cs.se'],"  ChatGPT has revolutionized many research and industrial fields. ChatGPT hasshown great potential in software engineering to boost various traditionaltasks such as program repair, code understanding, and code generation. However,whether automatic program repair (APR) applies to deep learning (DL) programsis still unknown. DL programs, whose decision logic is not explicitly encodedin the source code, have posed unique challenges to APR. While to repair DLprograms, an APR approach needs to not only parse the source code syntacticallybut also needs to understand the code intention. With the best prior work, theperformance of fault localization is still far less than satisfactory (onlyabout 30\%). Therefore, in this paper, we explore ChatGPT's capability for DLprogram repair by asking three research questions. (1) Can ChatGPT debug DLprograms effectively? (2) How can ChatGPT's repair performance be improved byprompting? (3) In which way can dialogue help facilitate the repair? On top ofthat, we categorize the common aspects useful for prompt design for DL programrepair. Also, we propose various prompt templates to facilitate the performanceand summarize the advantages and disadvantages of ChatGPT's abilities such asdetecting bad code smell, code refactoring, and detecting APImisuse/deprecation."
Prompt-Learning for Cross-Lingual Relation Extraction,"['Chiaming Hsu', 'Changtong Zan', 'Liang Ding', 'Longyue Wang', 'Xiaoting Wang', 'Weifeng Liu', 'Fu Lin', 'Wenbin Hu']",http://arxiv.org/pdf/2304.10354v1.pdf,2023-04-20,['cs.cl'],"  Relation Extraction (RE) is a crucial task in Information Extraction, whichentails predicting relationships between entities within a given sentence.However, extending pre-trained RE models to other languages is challenging,particularly in real-world scenarios where Cross-Lingual Relation Extraction(XRE) is required. Despite recent advancements in Prompt-Learning, whichinvolves transferring knowledge from Multilingual Pre-trained Language Models(PLMs) to diverse downstream tasks, there is limited research on the effectiveuse of multilingual PLMs with prompts to improve XRE. In this paper, we presenta novel XRE algorithm based on Prompt-Tuning, referred to as Prompt-XRE. Toevaluate its effectiveness, we design and implement several prompt templates,including hard, soft, and hybrid prompts, and empirically test theirperformance on competitive multilingual PLMs, specifically mBART. Our extensiveexperiments, conducted on the low-resource ACE05 benchmark across multiplelanguages, demonstrate that our Prompt-XRE algorithm significantly outperformsboth vanilla multilingual PLMs and other existing models, achievingstate-of-the-art performance in XRE. To further show the generalization of ourPrompt-XRE on larger data scales, we construct and release a new XRE dataset-WMT17-EnZh XRE, containing 0.9M English-Chinese pairs extracted from WMT 2017parallel corpus. Experiments on WMT17-EnZh XRE also show the effectiveness ofour Prompt-XRE against other competitive baselines. The code and newlyconstructed dataset are freely available at\url{https://github.com/HSU-CHIA-MING/Prompt-XRE}."
CitePrompt: Using Prompts to Identify Citation Intent in Scientific  Papers,"['Avishek Lahiri', 'Debarshi Kumar Sanyal', 'Imon Mukherjee']",http://arxiv.org/pdf/2304.12730v2.pdf,2023-04-25,['cs.cl'],"  Citations in scientific papers not only help us trace the intellectuallineage but also are a useful indicator of the scientific significance of thework. Citation intents prove beneficial as they specify the role of thecitation in a given context. In this paper, we present CitePrompt, a frameworkwhich uses the hitherto unexplored approach of prompt-based learning forcitation intent classification. We argue that with the proper choice of thepretrained language model, the prompt template, and the prompt verbalizer, wecan not only get results that are better than or comparable to those obtainedwith the state-of-the-art methods but also do it with much less exteriorinformation about the scientific document. We report state-of-the-art resultson the ACL-ARC dataset, and also show significant improvement on the SciCitedataset over all baseline models except one. As suitably large labelleddatasets for citation intent classification can be quite hard to find, in afirst, we propose the conversion of this task to the few-shot and zero-shotsettings. For the ACL-ARC dataset, we report a 53.86% F1 score for thezero-shot setting, which improves to 63.61% and 66.99% for the 5-shot and10-shot settings, respectively."
Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner,"['Zhengxiang Shi', 'Aldo Lipani']",http://arxiv.org/pdf/2305.01711v4.pdf,2023-05-02,['cs.cl'],"  Language models (LMs) trained on vast quantities of unlabelled data havegreatly advanced the field of natural language processing (NLP). In this study,we re-visit the widely accepted notion in NLP that continued pre-training LMson task-related texts improves the performance of fine-tuning (FT) indownstream tasks. Through experiments on eight single-sentence tasks and eightsentence-pair tasks in both semi-supervised and fully-supervised settings, wefind that conventional continued pre-training does not consistently providebenefits and can even be detrimental for sentence-pair tasks or whenprompt-based FT is used. To tackle these issues, we propose Prompt-basedContinued Pre-training (PCP), which combines the idea of instruction tuningwith conventional continued pre-training. Our approach aims to improve theperformance of prompt-based FT by presenting both task-related texts and prompttemplates to LMs through unsupervised pre-training objectives beforefine-tuning for the target task. Our empirical evaluations on 21 benchmarksdemonstrate that the PCP consistently improves the performance ofstate-of-the-art prompt-based FT approaches (up to 20.1% absolute) in bothsemi-supervised and fully-supervised settings, even with only hundreds ofunlabelled examples. Additionally, prompt-based FT with the PCP outperformsstate-of-the-art semi-supervised approaches with greater simplicity,eliminating the need for an iterative process and extra data augmentation. Ourfurther analysis explores the performance lower bound of the PCP and revealsthat the advantages of PCP persist across different sizes of models anddatasets."
Large Language Models are Zero-Shot Rankers for Recommender Systems,"['Yupeng Hou', 'Junjie Zhang', 'Zihan Lin', 'Hongyu Lu', 'Ruobing Xie', 'Julian McAuley', 'Wayne Xin Zhao']",http://arxiv.org/pdf/2305.08845v1.pdf,2023-05-15,"['cs.ir', 'cs.cl']","  Recently, large language models (LLMs) (e.g. GPT-4) have demonstratedimpressive general-purpose task-solving abilities, including the potential toapproach recommendation tasks. Along this line of research, this work aims toinvestigate the capacity of LLMs that act as the ranking model for recommendersystems. To conduct our empirical study, we first formalize the recommendationproblem as a conditional ranking task, considering sequential interactionhistories as conditions and the items retrieved by the candidate generationmodel as candidates. We adopt a specific prompting approach to solving theranking task by LLMs: we carefully design the prompting template by includingthe sequential interaction history, the candidate items, and the rankinginstruction. We conduct extensive experiments on two widely-used datasets forrecommender systems and derive several key findings for the use of LLMs inrecommender systems. We show that LLMs have promising zero-shot rankingabilities, even competitive to or better than conventional recommendationmodels on candidates retrieved by multiple candidate generators. We alsodemonstrate that LLMs struggle to perceive the order of historical interactionsand can be affected by biases like position bias, while these issues can bealleviated via specially designed prompting and bootstrapping strategies. Thecode to reproduce this work is available athttps://github.com/RUCAIBox/LLMRank."
TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse  Relation Recognition,"['Wei Xiang', 'Chao Liang', 'Bang Wang']",http://arxiv.org/pdf/2305.10866v1.pdf,2023-05-18,['cs.cl'],"  Implicit Discourse Relation Recognition (IDRR) aims at classifying therelation sense between two arguments without an explicit connective. Recently,the ConnPrompt~\cite{Wei.X:et.al:2022:COLING} has leveraged the powerful promptlearning for IDRR based on the fusion of multi-prompt decisions from threedifferent yet much similar connective prediction templates. Instead ofmulti-prompt ensembling, we propose to design auxiliary tasks with enlightenedprompt learning for the IDRR task. Although an auxiliary task is not used todirectly output final prediction, we argue that during the joint training someof its learned features can be useful to boost the main task. In light of suchmotivations, we propose a task enlightenment prompt learning model, calledTEPrompt, to fuse learned features from three related tasks for IDRR. Inparticular, the TEPrompt contains three tasks, viz., Discourse RelationRecognition (DRR), Sense Semantics Classification (SSC) and AnnotatedConnective Prediction (ACP), each with a unique prompt template and an answerspace. In the training phase, we jointly train three prompt learning tasks withshared argument representation. In the testing phase, we only take the DRRoutput with fused features as the final IDRR decision. Experiments with thesame conditions have shown that the proposed TEPrompt outperforms theConnPrompt. This can be attributed to the promoted decision features andlanguage models benefited from joint-training of auxiliary tasks."
Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition  with Auxiliary Refined Knowledge,"['Jinyuan Li', 'Han Li', 'Zhuo Pan', 'Di Sun', 'Jiahao Wang', 'Wenkun Zhang', 'Gang Pan']",http://arxiv.org/pdf/2305.12212v2.pdf,2023-05-20,['cs.cl'],"  Multimodal Named Entity Recognition (MNER) on social media aims to enhancetextual entity prediction by incorporating image-based clues. Existing studiesmainly focus on maximizing the utilization of pertinent image information orincorporating external knowledge from explicit knowledge bases. However, thesemethods either neglect the necessity of providing the model with externalknowledge, or encounter issues of high redundancy in the retrieved knowledge.In this paper, we present PGIM -- a two-stage framework that aims to leverageChatGPT as an implicit knowledge base and enable it to heuristically generateauxiliary knowledge for more efficient entity prediction. Specifically, PGIMcontains a Multimodal Similar Example Awareness module that selects suitableexamples from a small number of predefined artificial samples. These examplesare then integrated into a formatted prompt template tailored to the MNER andguide ChatGPT to generate auxiliary refined knowledge. Finally, the acquiredknowledge is integrated with the original text and fed into a downstream modelfor further processing. Extensive experiments show that PGIM outperformsstate-of-the-art methods on two classic MNER datasets and exhibits a strongerrobustness and generalization capability."
"Paradigm Shift in Sustainability Disclosure Analysis: Empowering  Stakeholders with CHATREPORT, a Language Model-Based Tool","['Jingwei Ni', 'Julia Bingler', 'Chiara Colesanti-Senni', 'Mathias Kraus', 'Glen Gostlow', 'Tobias Schimanski', 'Dominik Stammbach', 'Saeid Ashraf Vaghefi', 'Qian Wang', 'Nicolas Webersinke', 'Tobias Wekhof', 'Tingyu Yu', 'Markus Leippold']",http://arxiv.org/pdf/2306.15518v2.pdf,2023-06-27,['cs.cl'],"  This paper introduces a novel approach to enhance Large Language Models(LLMs) with expert knowledge to automate the analysis of corporatesustainability reports by benchmarking them against the Task Force forClimate-Related Financial Disclosures (TCFD) recommendations. Corporatesustainability reports are crucial in assessing organizations' environmentaland social risks and impacts. However, analyzing these reports' vast amounts ofinformation makes human analysis often too costly. As a result, only a fewentities worldwide have the resources to analyze these reports, which couldlead to a lack of transparency. While AI-powered tools can automaticallyanalyze the data, they are prone to inaccuracies as they lack domain-specificexpertise. This paper introduces a novel approach to enhance LLMs with expertknowledge to automate the analysis of corporate sustainability reports. Wechristen our tool CHATREPORT, and apply it in a first use case to assesscorporate climate risk disclosures following the TCFD recommendations.CHATREPORT results from collaborating with experts in climate science, finance,economic policy, and computer science, demonstrating how domain experts can beinvolved in developing AI tools. We make our prompt templates, generated data,and scores available to the public to encourage transparency."
TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation,"['Paul Grimal', 'Hervé Le Borgne', 'Olivier Ferret', 'Julien Tourille']",http://arxiv.org/pdf/2307.05134v1.pdf,2023-07-11,"['cs.cv', 'cs.ai', 'cs.cl', 'cs.lg']","  The progress in the generation of synthetic images has made it crucial toassess their quality. While several metrics have been proposed to assess therendering of images, it is crucial for Text-to-Image (T2I) models, whichgenerate images based on a prompt, to consider additional aspects such as towhich extent the generated image matches the important content of the prompt.Moreover, although the generated images usually result from a random startingpoint, the influence of this one is generally not considered. In this article,we propose a new metric based on prompt templates to study the alignmentbetween the content specified in the prompt and the corresponding generatedimages. It allows us to better characterize the alignment in terms of the typeof the specified objects, their number, and their color. We conducted a studyon several recent T2I models about various aspects. An additional interestingresult we obtained with our approach is that image quality can vary drasticallydepending on the latent noise used as a seed for the images. We also quantifythe influence of the number of concepts in the prompt, their order as well astheir (color) attributes. Finally, our method allows us to identify some latentseeds that produce better images than others, opening novel directions ofresearch on this understudied topic."
LLM-FuncMapper: Function Identification for Interpreting Complex Clauses  in Building Codes via LLM,"['Zhe Zheng', 'Ke-Yin Chen', 'Xin-Yu Cao', 'Xin-Zheng Lu', 'Jia-Rui Lin']",http://arxiv.org/pdf/2308.08728v1.pdf,2023-08-17,"['cs.ai', 'cs.cl']","  As a vital stage of automated rule checking (ARC), rule interpretation ofregulatory texts requires considerable effort. However, interpreting regulatoryclauses with implicit properties or complex computational logic is stillchallenging due to the lack of domain knowledge and limited expressibility ofconventional logic representations. Thus, LLM-FuncMapper, an approach toidentifying predefined functions needed to interpret various regulatory clausesbased on the large language model (LLM), is proposed. First, by systematicallyanalysis of building codes, a series of atomic functions are defined to captureshared computational logics of implicit properties and complex constraints,creating a database of common blocks for interpreting regulatory clauses. Then,a prompt template with the chain of thought is developed and further enhancedwith a classification-based tuning strategy, to enable common LLMs foreffective function identification. Finally, the proposed approach is validatedwith statistical analysis, experiments, and proof of concept. Statisticalanalysis reveals a long-tail distribution and high expressibility of thedeveloped function database, with which almost 100% of computer-processibleclauses can be interpreted and represented as computer-executable codes.Experiments show that LLM-FuncMapper achieve promising results in identifyingrelevant predefined functions for rule interpretation. Further proof of conceptin automated rule interpretation also demonstrates the possibility ofLLM-FuncMapper in interpreting complex regulatory clauses. To the best of ourknowledge, this study is the first attempt to introduce LLM for understandingand interpreting complex regulatory clauses, which may shed light on furtheradoption of LLM in the construction domain."
Prompt-Based Length Controlled Generation with Reinforcement Learning,"['Renlong Jie', 'Xiaojun Meng', 'Lifeng Shang', 'Xin Jiang', 'Qun Liu']",http://arxiv.org/pdf/2308.12030v2.pdf,2023-08-23,"['cs.cl', 'cs.ai', 'cs.lg']","  Large language models (LLMs) like ChatGPT and GPT-4 have attracted greatattention given their surprising performance on a wide range of NLP tasks.Length controlled generation of LLMs emerges as an important topic, whichenables users to fully leverage the capability of LLMs in more real-worldscenarios like generating a proper answer or essay of a desired length. Inaddition, the autoregressive generation in LLMs is extremely time-consuming,while the ability of controlling this generated length can reduce the inferencecost by limiting the length. Therefore, we propose a prompt-based lengthcontrol method to achieve high-accuracy length controlled generation. Inparticular, we adopt reinforcement learning with the reward signal given byeither trainable or rule-based reward models, which further enhances thelength-control ability of LLMs by rewarding outputs that follows pre-definedcontrol instruction. To enable rule-based inference, we also introduce standardprompt extractor to collect the standard control information from users' input.Experiments show that our method significantly improves the accuracy ofprompt-based length control for summarization task on popular datasets likeCNNDM and NYT. Both the standard prompt extractor and the RL-tuned model haveshow strong generalization ability to unseen control prompt templates."
LLM Powered Sim-to-real Transfer for Traffic Signal Control,"['Longchao Da', 'Minchiuan Gao', 'Hao Mei', 'Hua Wei']",http://arxiv.org/pdf/2308.14284v3.pdf,2023-08-28,"['cs.ai', 'h.4.0']","  Numerous solutions are proposed for the Traffic Signal Control (TSC) tasksaiming to provide efficient transportation and mitigate congestion waste. Inrecent, promising results have been attained by Reinforcement Learning (RL)methods through trial and error in simulators, bringing confidence in solvingcities' congestion headaches. However, there still exist performance gaps whensimulator-trained policies are deployed to the real world. This issue is mainlyintroduced by the system dynamic difference between the training simulator andthe real-world environments. The Large Language Models (LLMs) are trained onmass knowledge and proved to be equipped with astonishing inference abilities.In this work, we leverage LLMs to understand and profile the system dynamics bya prompt-based grounded action transformation. Accepting the cloze prompttemplate, and then filling in the answer based on accessible context, thepre-trained LLM's inference ability is exploited and applied to understand howweather conditions, traffic states, and road types influence traffic dynamics,being aware of this, the policies' action is taken and grounded based onrealistic dynamics, thus help the agent learn a more realistic policy. Weconduct experiments using DQN to show the effectiveness of the proposedPromptGAT's ability in mitigating the performance gap from simulation toreality (sim-to-real)."
AnoVL: Adapting Vision-Language Models for Unified Zero-shot Anomaly  Localization,"['Hanqiu Deng', 'Zhaoxiang Zhang', 'Jinan Bao', 'Xingyu Li']",http://arxiv.org/pdf/2308.15939v1.pdf,2023-08-30,['cs.cv'],"  Contrastive Language-Image Pre-training (CLIP) models have shown promisingperformance on zero-shot visual recognition tasks by learning visualrepresentations under natural language supervision. Recent studies attempt theuse of CLIP to tackle zero-shot anomaly detection by matching images withnormal and abnormal state prompts. However, since CLIP focuses on buildingcorrespondence between paired text prompts and global image-levelrepresentations, the lack of patch-level vision to text alignment limits itscapability on precise visual anomaly localization. In this work, we introduce atraining-free adaptation (TFA) framework of CLIP for zero-shot anomalylocalization. In the visual encoder, we innovate a training-free value-wiseattention mechanism to extract intrinsic local tokens of CLIP for patch-levellocal description. From the perspective of text supervision, we particularlydesign a unified domain-aware contrastive state prompting template. On top ofthe proposed TFA, we further introduce a test-time adaptation (TTA) mechanismto refine anomaly localization results, where a layer of trainable parametersin the adapter is optimized using TFA's pseudo-labels and syntheticnoise-corrupted tokens. With both TFA and TTA adaptation, we significantlyexploit the potential of CLIP for zero-shot anomaly localization anddemonstrate the effectiveness of our proposed methods on various datasets."
Investigating the Applicability of Self-Assessment Tests for Personality  Measurement of Large Language Models,"['Akshat Gupta', 'Xiaoyang Song', 'Gopala Anumanchipalli']",http://arxiv.org/pdf/2309.08163v1.pdf,2023-09-15,"['cs.cl', 'cs.ai']","  As large language models (LLM) evolve in their capabilities, various recentstudies have tried to quantify their behavior using psychological tools createdto study human behavior. One such example is the measurement of ""personality""of LLMs using personality self-assessment tests. In this paper, we take threesuch studies on personality measurement of LLMs that use personalityself-assessment tests created to study human behavior. We use the prompts usedin these three different papers to measure the personality of the same LLM. Wefind that all three prompts lead very different personality scores. This simpletest reveals that personality self-assessment scores in LLMs depend on thesubjective choice of the prompter. Since we don't know the ground truth valueof personality scores for LLMs as there is no correct answer to such questions,there's no way of claiming if one prompt is more or less correct than theother. We then introduce the property of option order symmetry for personalitymeasurement of LLMs. Since most of the self-assessment tests exist in the formof multiple choice question (MCQ) questions, we argue that the scores shouldalso be robust to not just the prompt template but also the order in which theoptions are presented. This test unsurprisingly reveals that the answers to theself-assessment tests are not robust to the order of the options. These simpletests, done on ChatGPT and Llama2 models show that self-assessment personalitytests created for humans are not appropriate for measuring personality in LLMs."
InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision  Generalists,"['Yulu Gan', 'Sungwoo Park', 'Alexander Schubert', 'Anthony Philippakis', 'Ahmed M. Alaa']",http://arxiv.org/pdf/2310.00390v1.pdf,2023-09-30,['cs.cv'],"  Recent advances in generative diffusion models have enabled text-controlledsynthesis of realistic and diverse images with impressive quality. Despitethese remarkable advances, the application of text-to-image generative modelsin computer vision for standard visual recognition tasks remains limited. Thecurrent de facto approach for these tasks is to design model architectures andloss functions that are tailored to the task at hand. In this paper, we developa unified language interface for computer vision tasks that abstracts awaytask-specific design choices and enables task execution by following naturallanguage instructions. Our approach involves casting multiple computer visiontasks as text-to-image generation problems. Here, the text represents aninstruction describing the task, and the resulting image is a visually-encodedtask output. To train our model, we pool commonly-used computer vision datasetscovering a range of tasks, including segmentation, object detection, depthestimation, and classification. We then use a large language model toparaphrase prompt templates that convey the specific tasks to be conducted oneach image, and through this process, we create a multi-modal and multi-tasktraining dataset comprising input and output images along with annotatedinstructions. Following the InstructPix2Pix architecture, we applyinstruction-tuning to a text-to-image diffusion model using our constructeddataset, steering its functionality from a generative model to aninstruction-guided multi-task vision learner. Experiments demonstrate that ourmodel, dubbed InstructCV, performs competitively compared to other generalistand task-specific vision models. Moreover, it exhibits compellinggeneralization capabilities to unseen data, categories, and user instructions."
Revisit Input Perturbation Problems for LLMs: A Unified Robustness  Evaluation Framework for Noisy Slot Filling Task,"['Guanting Dong', 'Jinxu Zhao', 'Tingfeng Hui', 'Daichi Guo', 'Wenlong Wan', 'Boqi Feng', 'Yueyan Qiu', 'Zhuoma Gongque', 'Keqing He', 'Zechen Wang', 'Weiran Xu']",http://arxiv.org/pdf/2310.06504v1.pdf,2023-10-10,"['cs.cl', 'cs.ai', 'cs.lg']","  With the increasing capabilities of large language models (LLMs), thesehigh-performance models have achieved state-of-the-art results on a wide rangeof natural language processing (NLP) tasks. However, the models' performance oncommonly-used benchmark datasets often fails to accurately reflect theirreliability and robustness when applied to real-world noisy data. To addressthese challenges, we propose a unified robustness evaluation framework based onthe slot-filling task to systematically evaluate the dialogue understandingcapability of LLMs in diverse input perturbation scenarios. Specifically, weconstruct a input perturbation evaluation dataset, Noise-LLM, which containsfive types of single perturbation and four types of mixed perturbation data.Furthermore, we utilize a multi-level data augmentation method (character,word, and sentence levels) to construct a candidate data pool, and carefullydesign two ways of automatic task demonstration construction strategies(instance-level and entity-level) with various prompt templates. Our aim is toassess how well various robustness methods of LLMs perform in real-world noisyscenarios. The experiments have demonstrated that the current open-source LLMsgenerally achieve limited perturbation robustness performance. Based on theseexperimental observations, we make some forward-looking suggestions to fuel theresearch in this direction."
Do Language Models Learn about Legal Entity Types during Pretraining?,"['Claire Barale', 'Michael Rovatsos', 'Nehal Bhuta']",http://arxiv.org/pdf/2310.13092v1.pdf,2023-10-19,['cs.cl'],"  Language Models (LMs) have proven their ability to acquire diverse linguisticknowledge during the pretraining phase, potentially serving as a valuablesource of incidental supervision for downstream tasks. However, there has beenlimited research conducted on the retrieval of domain-specific knowledge, andspecifically legal knowledge. We propose to explore the task of Entity Typing,serving as a proxy for evaluating legal knowledge as an essential aspect oftext comprehension, and a foundational task to numerous downstream legal NLPapplications. Through systematic evaluation and analysis and two types ofprompting (cloze sentences and QA-based templates) and to clarify the nature ofthese acquired cues, we compare diverse types and lengths of entities bothgeneral and domain-specific entities, semantics or syntax signals, anddifferent LM pretraining corpus (generic and legal-oriented) and architectures(encoder BERT-based and decoder-only with Llama2). We show that (1) Llama2performs well on certain entities and exhibits potential for substantialimprovement with optimized prompt templates, (2) law-oriented LMs showinconsistent performance, possibly due to variations in their training corpus,(3) LMs demonstrate the ability to type entities even in the case ofmulti-token entities, (4) all models struggle with entities belonging tosub-domains of the law (5) Llama2 appears to frequently overlook syntacticcues, a shortcoming less present in BERT-based architectures."
LlamaRec: Two-Stage Recommendation using Large Language Models for  Ranking,"['Zhenrui Yue', 'Sara Rabhi', 'Gabriel de Souza Pereira Moreira', 'Dong Wang', 'Even Oldridge']",http://arxiv.org/pdf/2311.02089v1.pdf,2023-10-25,"['cs.ir', 'cs.ai', 'cs.cl']","  Recently, large language models (LLMs) have exhibited significant progress inlanguage understanding and generation. By leveraging textual features,customized LLMs are also applied for recommendation and demonstrateimprovements across diverse recommendation scenarios. Yet the majority ofexisting methods perform training-free recommendation that heavily relies onpretrained knowledge (e.g., movie recommendation). In addition, inference onLLMs is slow due to autoregressive generation, rendering existing methods lesseffective for real-time recommendation. As such, we propose a two-stageframework using large language models for ranking-based recommendation(LlamaRec). In particular, we use small-scale sequential recommenders toretrieve candidates based on the user interaction history. Then, both historyand retrieved items are fed to the LLM in text via a carefully designed prompttemplate. Instead of generating next-item titles, we adopt a verbalizer-basedapproach that transforms output logits into probability distributions over thecandidate items. Therefore, the proposed LlamaRec can efficiently rank itemswithout generating long text. To validate the effectiveness of the proposedframework, we compare against state-of-the-art baseline methods on benchmarkdatasets. Our experimental results demonstrate the performance of LlamaRec,which consistently achieves superior performance in both recommendationperformance and efficiency."
GPT Struct Me: Probing GPT Models on Narrative Entity Extraction,"['Hugo Sousa', 'Nuno Guimarães', 'Alípio Jorge', 'Ricardo Campos']",http://arxiv.org/pdf/2311.14583v1.pdf,2023-11-24,"['cs.cl', 'cs.ai', 'cs.ir']","  The importance of systems that can extract structured information fromtextual data becomes increasingly pronounced given the ever-increasing volumeof text produced on a daily basis. Having a system that can effectively extractsuch information in an interoperable manner would be an asset for severaldomains, be it finance, health, or legal. Recent developments in naturallanguage processing led to the production of powerful language models that can,to some degree, mimic human intelligence. Such effectiveness raises a pertinentquestion: Can these models be leveraged for the extraction of structuredinformation? In this work, we address this question by evaluating thecapabilities of two state-of-the-art language models -- GPT-3 and GPT-3.5,commonly known as ChatGPT -- in the extraction of narrative entities, namelyevents, participants, and temporal expressions. This study is conducted on theText2Story Lusa dataset, a collection of 119 Portuguese news articles whoseannotation framework includes a set of entity structures along with severaltags and attribute values. We first select the best prompt template through anablation study over prompt components that provide varying degrees ofinformation on a subset of documents of the dataset. Subsequently, we use thebest templates to evaluate the effectiveness of the models on the remainingdocuments. The results obtained indicate that GPT models are competitive without-of-the-box baseline systems, presenting an all-in-one alternative forpractitioners with limited resources. By studying the strengths and limitationsof these models in the context of information extraction, we offer insightsthat can guide future improvements and avenues to explore in this field."
LAMM: Label Alignment for Multi-Modal Prompt Learning,"['Jingsheng Gao', 'Jiacheng Ruan', 'Suncheng Xiang', 'Zefang Yu', 'Ke Ji', 'Mingye Xie', 'Ting Liu', 'Yuzhuo Fu']",http://arxiv.org/pdf/2312.08212v1.pdf,2023-12-13,['cs.cv'],"  With the success of pre-trained visual-language (VL) models such as CLIP invisual representation tasks, transferring pre-trained models to downstreamtasks has become a crucial paradigm. Recently, the prompt tuning paradigm,which draws inspiration from natural language processing (NLP), has madesignificant progress in VL field. However, preceding methods mainly focus onconstructing prompt templates for text and visual inputs, neglecting the gap inclass label representations between the VL models and downstream tasks. Toaddress this challenge, we introduce an innovative label alignment method named\textbf{LAMM}, which can dynamically adjust the category embeddings ofdownstream datasets through end-to-end training. Moreover, to achieve a moreappropriate label distribution, we propose a hierarchical loss, encompassingthe alignment of the parameter space, feature space, and logits space. Weconduct experiments on 11 downstream vision datasets and demonstrate that ourmethod significantly improves the performance of existing multi-modal promptlearning models in few-shot scenarios, exhibiting an average accuracyimprovement of 2.31(\%) compared to the state-of-the-art methods on 16 shots.Moreover, our methodology exhibits the preeminence in continual learningcompared to other prompt tuning methods. Importantly, our method is synergisticwith existing prompt tuning methods and can boost the performance on top ofthem. Our code and dataset will be publicly available athttps://github.com/gaojingsheng/LAMM."
Prompting Multilingual Large Language Models to Generate Code-Mixed  Texts: The Case of South East Asian Languages,"['Zheng-Xin Yong', 'Ruochen Zhang', 'Jessica Zosa Forde', 'Skyler Wang', 'Arjun Subramonian', 'Holy Lovenia', 'Samuel Cahyawijaya', 'Genta Indra Winata', 'Lintang Sutawika', 'Jan Christian Blaise Cruz', 'Yin Lin Tan', 'Long Phan', 'Rowena Garcia', 'Thamar Solorio', 'Alham Fikri Aji']",http://arxiv.org/pdf/2303.13592v4.pdf,2023-03-23,"['cs.cl', 'cs.ai']","  While code-mixing is a common linguistic practice in many parts of the world,collecting high-quality and low-cost code-mixed data remains a challenge fornatural language processing (NLP) research. The recent proliferation of LargeLanguage Models (LLMs) compels one to ask: how capable are these systems ingenerating code-mixed data? In this paper, we explore prompting multilingualLLMs in a zero-shot manner to generate code-mixed data for seven languages inSouth East Asia (SEA), namely Indonesian, Malay, Chinese, Tagalog, Vietnamese,Tamil, and Singlish. We find that publicly available multilingualinstruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable ofproducing texts with phrases or clauses from different languages. ChatGPTexhibits inconsistent capabilities in generating code-mixed texts, wherein itsperformance varies depending on the prompt template and language pairing. Forinstance, ChatGPT generates fluent and natural Singlish texts (an English-basedcreole spoken in Singapore), but for English-Tamil language pair, the systemmostly produces grammatically incorrect or semantically meaningless utterances.Furthermore, it may erroneously introduce languages not specified in theprompt. Based on our investigation, existing multilingual LLMs exhibit a widerange of proficiency in code-mixed data generation for SEA languages. As such,we advise against using LLMs in this context without extensive human checks."
"Reason for Future, Act for Now: A Principled Framework for Autonomous  LLM Agents with Provable Sample Efficiency","['Zhihan Liu', 'Hao Hu', 'Shenao Zhang', 'Hongyi Guo', 'Shuqi Ke', 'Boyi Liu', 'Zhaoran Wang']",http://arxiv.org/pdf/2309.17382v2.pdf,2023-09-29,"['cs.ai', 'cs.lg']","  Large language models (LLMs) demonstrate impressive reasoning abilities, buttranslating reasoning into actions in the real world remains challenging. Inparticular, it remains unclear how to complete a given task provably within aminimum number of interactions with the external environment, e.g., through aninternal mechanism of reasoning. To this end, we propose a principled frameworkwith provable regret guarantees to orchestrate reasoning and acting, which wecall ""reason for future, act for now"" (\texttt{RAFA}). Specifically, we designa prompt template for reasoning that learns from the memory buffer and plans afuture trajectory over a long horizon (""reason for future""). At each step, theLLM agent takes the initial action of the planned trajectory (""act for now""),stores the collected feedback in the memory buffer, and reinvokes the reasoningroutine to replan the future trajectory from the new state.  The key idea is to cast reasoning in LLMs as learning and planning inBayesian adaptive Markov decision processes (MDPs). Correspondingly, we promptLLMs to form an updated posterior of the unknown environment from the memorybuffer (learning) and generate an optimal trajectory for multiple future stepsthat maximizes a value function (planning). The learning and planningsubroutines are performed in an ""in-context"" manner to emulate the actor-criticupdate for MDPs. Our theoretical analysis proves that the novel combination oflong-term reasoning and short-term acting achieves a $\sqrt{T}$ regret. Inparticular, the regret bound highlights an intriguing interplay between theprior knowledge obtained through pretraining and the uncertainty reductionachieved by reasoning and acting. Our empirical validation shows that itoutperforms various existing frameworks and achieves nearly perfect scores on afew benchmarks."
ClickPrompt: CTR Models are Strong Prompt Generators for Adapting  Language Models to CTR Prediction,"['Jianghao Lin', 'Bo Chen', 'Hangyu Wang', 'Yunjia Xi', 'Yanru Qu', 'Xinyi Dai', 'Kangning Zhang', 'Ruiming Tang', 'Yong Yu', 'Weinan Zhang']",http://arxiv.org/pdf/2310.09234v2.pdf,2023-10-13,"['cs.ir', 'cs.ai']","  Click-through rate (CTR) prediction has become increasingly indispensable forvarious Internet applications. Traditional CTR models convert the multi-fieldcategorical data into ID features via one-hot encoding, and extract thecollaborative signals among features. Such a paradigm suffers from the problemof semantic information loss. Another line of research explores the potentialof pretrained language models (PLMs) for CTR prediction by converting inputdata into textual sentences through hard prompt templates. Although semanticsignals are preserved, they generally fail to capture the collaborativeinformation (e.g., feature interactions, pure ID features), not to mention theunacceptable inference overhead brought by the huge model size. In this paper,we aim to model both the semantic knowledge and collaborative knowledge foraccurate CTR estimation, and meanwhile address the inference inefficiencyissue. To benefit from both worlds and close their gaps, we propose a novelmodel-agnostic framework (i.e., ClickPrompt), where we incorporate CTR modelsto generate interaction-aware soft prompts for PLMs. We design aprompt-augmented masked language modeling (PA-MLM) pretraining task, where PLMhas to recover the masked tokens based on the language context, as well as thesoft prompts generated by CTR model. The collaborative and semantic knowledgefrom ID and textual features would be explicitly aligned and interacted via theprompt interface. Then, we can either tune the CTR model with PLM for superiorperformance, or solely tune the CTR model without PLM for inference efficiency.Experiments on four real-world datasets validate the effectiveness ofClickPrompt compared with existing baselines."
FLIP: Towards Fine-grained Alignment between ID-based Models and  Pretrained Language Models for CTR Prediction,"['Hangyu Wang', 'Jianghao Lin', 'Xiangyang Li', 'Bo Chen', 'Chenxu Zhu', 'Ruiming Tang', 'Weinan Zhang', 'Yong Yu']",http://arxiv.org/pdf/2310.19453v2.pdf,2023-10-30,"['cs.ir', 'cs.ai']","  Click-through rate (CTR) prediction plays as a core function module invarious personalized online services. The traditional ID-based models for CTRprediction take as inputs the one-hot encoded ID features of tabular modality,which capture the collaborative signals via feature interaction modeling. Butthe one-hot encoding discards the semantic information conceived in theoriginal feature texts. Recently, the emergence of Pretrained Language Models(PLMs) has given rise to another paradigm, which takes as inputs the sentencesof textual modality obtained by hard prompt templates and adopts PLMs toextract the semantic knowledge. However, PLMs generally tokenize the input textdata into subword tokens and ignore field-wise collaborative signals.Therefore, these two lines of research focus on different characteristics ofthe same input data (i.e., textual and tabular modalities), forming a distinctcomplementary relationship with each other. In this paper, we propose toconduct Fine-grained feature-level ALignment between ID-based Models andPretrained Language Models (FLIP) for CTR prediction. We design a novel jointreconstruction pretraining task for both masked language and tabular modeling.Specifically, the masked data of one modality (i.e., tokens or features) has tobe recovered with the help of the other modality, which establishes thefeature-level interaction and alignment via sufficient mutual informationextraction between dual modalities. Moreover, we propose to jointly finetunethe ID-based model and PLM for downstream CTR prediction tasks, thus achievingsuperior performance by combining the advantages of both models. Extensiveexperiments on three real-world datasets demonstrate that FLIP outperforms SOTAbaselines, and is highly compatible for various ID-based models and PLMs."
